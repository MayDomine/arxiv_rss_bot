# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-05 12:54:49 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)

**Authors**: Xiumei Deng, Zehui Xiong, Binbin Chen, Dong In Kim, Merouane Debbah, H. Vincent Poor  
**Category**: cs.DC  
**Published**: 2025-11-05  
**Score**: 13.5  
**Type**: new  
**ArXiv ID**: 2511.02647v1  

Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computat...

---

### 2. [Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term Memory Access](https://arxiv.org/abs/2504.16795)

**Authors**: Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 13.0  
**Type**: replace-cross  
**ArXiv ID**: 2504.16795v2  

A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms...

---

### 3. [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)

**Authors**: Jungyeon Koh, Hyun Jong Yang  
**Category**: cs.LG  
**Published**: 2025-11-05  
**Score**: 11.0  
**Type**: replace  
**ArXiv ID**: 2511.01695v2  

The growing demand for on-device large language model (LLM) inference highlights the need for efficient mobile edge computing (MEC) solutions, especially in resource-constrained settings. Speculative decoding offers a promising solution by partitioning token generation between a lightweight draft mo...

---

### 4. [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)

**Authors**: Peng Xia, Junbiao Pang, Tianyang Cai  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 10.5  
**Type**: cross  
**ArXiv ID**: 2511.01462v1  

Post-training quantization (PTQ) for vision transformers (ViTs) has garnered significant attention due to its efficiency in compressing models. However, existing methods typically overlook the relationship between a well-trained NN and the quantized model, leading to considerable quantization error ...

---

### 5. [KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems](https://arxiv.org/abs/2510.12872)

**Authors**: Hancheng Ye, Zhengqi Gao, Mingyuan Ma, Qinsi Wang, Yuzhe Fu, Ming-Yu Chung, Yueqian Lin, Zhijian Liu, Jianyi Zhang, Danyang Zhuo, Yiran Chen  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 10.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.12872v2  

Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typica...

---

### 6. [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)

**Authors**: Octavian Alexandru Trifan, Karthik Sangaiah, Muhammad Awad, Muhammad Osama, Sumanth Gudaparthi, Alexandru Nicolau, Alexander Veidenbaum, Ganesh Dasika  
**Category**: cs.DC  
**Published**: 2025-11-05  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2511.02168v1  

As large language models (LLMs) continue to scale, their workloads increasingly rely on distributed execution across multiple GPUs. However, the conventional bulk synchronous parallel~(BSP) model used in such settings introduces significant performance inefficiencies. To characterize these bottlenec...

---

### 7. [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)

**Authors**: Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Regina Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Yulun Wang, Bruce Wu, Xinfeng Xie, Jingyi Yang, Mingran Yang, Kenny Yu, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Prashanth Kannan, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Srikanth Sundaresan, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng  
**Category**: cs.DC  
**Published**: 2025-11-05  
**Score**: 10.5  
**Type**: replace  
**ArXiv ID**: 2510.20171v3  

The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hin...

---

### 8. [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)

**Authors**: Jin Huang, Yuchao Jin, Le An, Josh Park  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2506.07416v2  

This paper introduces an efficient Vision-Language Model (VLM) pipeline specifically optimized for deployment on embedded devices, such as those used in robotics and autonomous driving. The pipeline significantly reduces the computational overhead by jointly leveraging patch selection to filter irre...

---

### 9. [Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.00294)

**Authors**: Shutong Wu, Jiawei Zhang  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.00294v2  

Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges lik...

---

### 10. [Multi-Personality Generation of LLMs at Decoding-time](https://arxiv.org/abs/2511.01891)

**Authors**: Rongxin Chen, Yunfan Li, Yige Yuan, Bingbing Xu, Huawei Shen  
**Category**: cs.CL  
**Published**: 2025-11-05  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2511.01891v1  

Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibili...

---

### 11. [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884)

**Authors**: Zijian Zhang, Rong Wang, Shiyang Li, Yuebo Luo, Mingyi Hong, Caiwen Ding  
**Category**: cs.CL  
**Published**: 2025-11-05  
**Score**: 10.0  
**Type**: cross  
**ArXiv ID**: 2511.01884v1  

Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, ...

---

### 12. [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)

**Authors**: Zicheng Xu, Guanchu Wang, Yu-Neng Chuang, Guangyao Zheng, Alexander S. Szalay, Zirui Liu, Vladimir Braverman  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.00640v1  

Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reas...

---

### 13. [Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits](https://arxiv.org/abs/2511.00321)

**Authors**: Dowon Kim, MinJae Lee, Janghyeon Kim, HyuckSung Kwon, Hyeonggyu Jeong, Sang-Soo Park, Minyong Yoon, Si-Dong Roh, Yongsuk Kwon, Jinin So, Jungwook Choi  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2511.00321v1  

The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to sca...

---

### 14. [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)

**Authors**: Chengying Huan, Ziheng Meng, Yongchao Liu, Zhengyi Yang, Yun Zhu, Yue Yun, Shipeng Li, Rong Gu, Xiabao Wu, Haitao Zhang, Chuntao Hong, Shaonan Ma, Guihai Chen, Chen Tian  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2511.01633v1  

Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context...

---

### 15. [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)

**Authors**: Fali Wang, Jihai Chen, Shuhua Yang, Runxue Bao, Tianxiang Zhao, Zhiwei Zhang, Xianfeng Tang, Hui Liu, Qi He, Suhang Wang  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2511.00086v1  

Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking...

---

### 16. [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)

**Authors**: Mahmut Selman Gokmen, Cody Bumgardner  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.01610v1  

Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a m...

---

### 17. [Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning](https://arxiv.org/abs/2502.03304)

**Authors**: Qitao Tan, Jun Liu, Zheng Zhan, Caiwei Ding, Yanzhi Wang, Xiaolong Ma, Jaewoo Lee, Jin Lu, Geng Yuan  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2502.03304v4  

Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes...

---

### 18. [Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding](https://arxiv.org/abs/2503.01422)

**Authors**: Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, Rui Wang  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2503.01422v3  

Test-time scaling enhances large language model performance by allocating additional compute resources during inference. Best-of-N (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However...

---

### 19. [Multi-head Temporal Latent Attention](https://arxiv.org/abs/2505.13544)

**Authors**: Keqi Deng, Philip C. Woodland  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.13544v3  

While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Mu...

---

### 20. [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)

**Authors**: Jameson Sandler, Jacob K. Christopher, Thomas Hartvigsen, Ferdinando Fioretto  
**Category**: cs.CL  
**Published**: 2025-11-05  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2511.00606v2  

Speculative decoding has become the standard approach for accelerating Large Language Model (LLM) inference. It exploits a lossless draft-then-verify procedure to circumvent the latency of autoregressive decoding, achieving impressive speed-ups. Yet, current speculative decoding approaches remain li...

---

### 21. [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)

**Authors**: Johansell Villalobos, Josef Ruzicka, Silvio Rizzi  
**Category**: cs.DC  
**Published**: 2025-11-05  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.02655v1  

Scientific computing in the exascale era demands increased computational power to solve complex problems across various domains. With the rise of heterogeneous computing architectures the need for vendor-agnostic, performance portability frameworks has been highlighted. Libraries like Kokkos have be...

---

### 22. [Federated Quantum Kernel Learning for Anomaly Detection in Multivariate IoT Time-Series](https://arxiv.org/abs/2511.02301)

**Authors**: Kuan-Cheng Chen, Samuel Yen-Chi Chen, Chen-Yu Liu, Kin K. Leung  
**Category**: cs.LG  
**Published**: 2025-11-05  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.02301v1  

The rapid growth of industrial Internet of Things (IIoT) systems has created new challenges for anomaly detection in high-dimensional, multivariate time-series, where privacy, scalability, and communication efficiency are critical. Classical federated learning approaches mitigate privacy concerns by...

---

### 23. [Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning](https://arxiv.org/abs/2511.02314)

**Authors**: Jueye Zhang, Chao Yang, Youfang Lai, Kai-Wen Li, Wenting Yan, Yunzhou Xia, Haimei Zhang, Jingjing Zhou, Gen Yang, Chen Lin, Tian Li, Yibao Zhang  
**Category**: cs.LG  
**Published**: 2025-11-05  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.02314v1  

Head-and-neck cancer (HNC) planning is difficult because multiple critical organs-at-risk (OARs) are close to complex targets. Intensity-modulated carbon-ion therapy (IMCT) offers superior dose conformity and OAR sparing but remains slow due to relative biological effectiveness (RBE) modeling, leadi...

---

### 24. [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)

**Authors**: Ujjwal Sharma, Stevan Rudinac, Ana Mi\'ckovi\'c, Willemijn van Dolen, Marcel Worring  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.01550v1  

In this work, we introduce a multimodal analysis pipeline that leverages large foundation models in vision and language to analyze corporate social media content, with a focus on sustainability-related communication. Addressing the challenges of evolving, multimodal, and often ambiguous corporate me...

---

### 25. [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)

**Authors**: Yuchen Zhang, Hanyue Du, Chun Cao, Jingwei Xu  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.00101v1  

Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning ...

---

### 26. [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)

**Authors**: Long Li, Jiajia Li, Dong Chen, Lina Pu, Haibo Yao, Yanbo Huang  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.00269v1  

Accurate classification plays a pivotal role in smart agriculture, enabling applications such as crop monitoring, fruit recognition, and pest detection. However, conventional centralized training often requires large-scale data collection, which raises privacy concerns, while standard federated lear...

---

### 27. [Proactive DDoS Detection and Mitigation in Decentralized Software-Defined Networking via Port-Level Monitoring and Zero-Training Large Language Models](https://arxiv.org/abs/2511.00460)

**Authors**: Mohammed N. Swileh, Shengli Zhang  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.00460v1  

Centralized Software-Defined Networking (cSDN) offers flexible and programmable control of networks but suffers from scalability and reliability issues due to its reliance on centralized controllers. Decentralized SDN (dSDN) alleviates these concerns by distributing control across multiple local con...

---

### 28. [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)

**Authors**: Qiang Li, Jin Niu, Lina Yu  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.00549v1  

Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to captu...

---

### 29. [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)

**Authors**: Juan Gabriel Kostelec, Qinghai Guo  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.00576v1  

Transformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we pre...

---

### 30. [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)

**Authors**: Min Fang, Zhihui Fu, Qibin Zhao, Jun Wang  
**Category**: cs.AI  
**Published**: 2025-11-05  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.01282v1  

Speculative decoding (SD) has emerged as an effective technique to accelerate large language model (LLM) inference without compromising output quality. However, the achievable speedup largely depends on the effectiveness of the drafting model. While model-based methods like EAGLE-2 are accurate but ...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
