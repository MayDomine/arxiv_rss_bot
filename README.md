# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-23 12:55:06 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [dInfer: An Efficient Inference Framework for Diffusion Language Models](https://arxiv.org/abs/2510.08666)

**Authors**: Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, Xinyuan Zhang, Zhen Tao, Haibo Feng, Ziyun Jiang, Ying Xu, Zenan Huang, Yihong Zhuang, Haokai Xu, Jiaqi Hu, Zhenzhong Lan, Junbo Zhao, Jianguo Li, Da Zheng  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 12.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.08666v3  

Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack o...

---

### 2. [ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices](https://arxiv.org/abs/2510.19482)

**Authors**: Xin Nie, Liang Dong, HaiCheng Zhang, JiaWang Xiao, G. Sun  
**Category**: cs.LG  
**Published**: 2025-10-23  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2510.19482v1  

The deployment of Large Language Models (LLMs) on CPU-based edge devices is crucial for enabling on-device intelligence and expanding AI accessibility. However, it remains challenging due to limited memory and computational resources. During edge inference, memory usage and latency are the primary b...

---

### 3. [ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable Multimodal Model Serving](https://arxiv.org/abs/2502.00937)

**Authors**: Haoran Qiu, Anish Biswas, Zihan Zhao, Jayashree Mohan, Alind Khare, Esha Choukse, \'I\~nigo Goiri, Zeyu Zhang, Haiying Shen, Chetan Bansal, Ramachandran Ramjee, Rodrigo Fonseca  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2502.00937v3  

Large multimodal models (LMMs) demonstrate impressive capabilities in understanding images, videos, and audio beyond text. However, efficiently serving LMMs in production environments poses significant challenges due to their complex architectures and heterogeneous characteristics across their multi...

---

### 4. [RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.19262)

**Authors**: Heng Xu, Zhiwei Yu, Chengze Du, Ying Zhou, Letian Li, Haojie Wang, Weiqiang Cheng, Jialong Li  
**Category**: cs.DC  
**Published**: 2025-10-23  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2510.19262v1  

Training Mixture-of-Experts (MoE) models introduces sparse and highly imbalanced all-to-all communication that dominates iteration time. Conventional load-balancing methods fail to exploit the deterministic topology of Rail architectures, leaving multi-NIC bandwidth underutilized. We present RailS, ...

---

### 5. [Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment](https://arxiv.org/abs/2509.20214)

**Authors**: Deokjae Lee, Hyun Oh Song  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2509.20214v2  

We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-bat...

---

### 6. [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)

**Authors**: Ziheng Deng, Xue Liu, Jiantong Jiang, Yankai Li, Qingxu Deng, Xiaochun Yang  
**Category**: cs.DC  
**Published**: 2025-10-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2510.19301v1  

The Viterbi algorithm is a key operator for structured sequence inference in modern data systems, with applications in trajectory analysis, online recommendation, and speech recognition. As these workloads increasingly migrate to resource-constrained edge platforms, standard Viterbi decoding remains...

---

### 7. [Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud](https://arxiv.org/abs/2510.19617)

**Authors**: Eric Ding  
**Category**: cs.DC  
**Published**: 2025-10-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2510.19617v1  

Collaborative Machine Learning is a paradigm in the field of distributed machine learning, designed to address the challenges of data privacy, communication overhead, and model heterogeneity. There have been significant advancements in optimization and communication algorithm design and ML hardware ...

---

### 8. [Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation](https://arxiv.org/abs/2510.19689)

**Authors**: Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2510.19689v1  

Industrial and government organizations increasingly depend on data-driven analytics for workforce, finance, and regulated decision processes, where timeliness, cost efficiency, and compliance are critical. Distributed frameworks such as Spark and Flink remain effective for massive-scale batch or st...

---

### 9. [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)

**Authors**: Yongshun Zhang, Zhongyi Fan, Yonghang Zhang, Zhangzikang Li, Weifeng Chen, Zhongwei Feng, Chaoyue Wang, Peng Hou, Anxiang Zeng  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.17519v2  

In recent years, large-scale generative models for visual content (\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable progress. However, training large-scale video generation models remains particularly challenging and resource-intensive due to cross-modal text-video alignmen...

---

### 10. [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)

**Authors**: Yongji Wu, Xueshen Liu, Haizhong Zheng, Juncheng Gu, Beidi Chen, Z. Morley Mao, Arvind Krishnamurthy, Ion Stoica  
**Category**: cs.DC  
**Published**: 2025-10-23  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2510.19225v1  

Reinforcement learning (RL) has become essential for unlocking advanced reasoning capabilities in large language models (LLMs). RL workflows involve interleaving rollout and training stages with fundamentally different resource requirements. Rollout typically dominates overall execution time, yet sc...

---

### 11. [AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators](https://arxiv.org/abs/2510.18897)

**Authors**: Jacopo Tagliabue  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.18897v1  

We explore AI-driven distributed-systems policy design by combining stochastic code generation from large language models (LLMs) with deterministic verification in a domain-specific simulator. Using a Function-as-a-Service runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we...

---

### 12. [BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.18927)

**Authors**: Zhiheng Xi, Xin Guo, Yang Nan, Enyu Zhou, Junrui Shen, Wenxiang Chen, Jiaqi Liu, Jixuan Huang, Zhihao Zhang, Honglin Guo, Xun Deng, Zhikai Lei, Miao Zheng, Guoteng Wang, Shuo Zhang, Peng Sun, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.18927v1  

Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy decli...

---

### 13. [Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning](https://arxiv.org/abs/2510.19338)

**Authors**: Ling Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, Jun Zhou  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.19338v1  

In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a...

---

### 14. [An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks](https://arxiv.org/abs/2502.02197)

**Authors**: Linus Aronsson, Morteza Haghir Chehreghani  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2502.02197v3  

Signed networks, where edges are labeled as positive or negative to represent friendly or antagonistic interactions, provide a natural framework for analyzing polarization, trust, and conflict in social systems. Detecting meaningful group structures in such networks is crucial for understanding onli...

---

### 15. [CATransformers: Carbon Aware Transformers Through Joint Model-Hardware Optimization](https://arxiv.org/abs/2505.01386)

**Authors**: Irene Wang, Newsha Ardalani, Mostafa Elhoushi, Daniel Jiang, Samuel Hsia, Ekin Sumbul, Divya Mahajan, Carole-Jean Wu, Bilge Acun  
**Category**: cs.LG  
**Published**: 2025-10-23  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2505.01386v3  

Machine learning solutions are rapidly adopted to enable a variety of key use cases, from conversational AI assistants to scientific discovery. This growing adoption is expected to increase the associated lifecycle carbon footprint, including both \emph{operational carbon} from training and inferenc...

---

### 16. [Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks](https://arxiv.org/abs/2510.19322)

**Authors**: Changbo Wu, Zhuolong Yu, Gongming Zhao, Hongli Xu  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.19322v1  

Collective communication (CC) is widely adopted for large-scale distributed machine learning (DML) training workloads. DML's predictable traffic pattern provides a great oppotunity for applying optical network technology. Existing optical interconnects-based CC schemes adopt ``one-shot network recon...

---

### 17. [HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission](https://arxiv.org/abs/2510.19470)

**Authors**: Weihao Yang, Hao Huang, Donglei Wu, Ningke Li, Yanqi Pan, Qiyang Zheng, Wen Xia, Shiyi Li, Qiang Wang  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.19470v1  

Mixture-of-Experts (MoE) has become a popular architecture for scaling large models. However, the rapidly growing scale outpaces model training on a single DC, driving a shift toward a more flexible, cross-DC training paradigm. Under this, Expert Parallelism (EP) of MoE faces significant scalability...

---

### 18. [DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference](https://arxiv.org/abs/2510.19669)

**Authors**: Xiang Liu, Xuming Hu, Xiaowen Chu, Eunsol Choi  
**Category**: cs.CL  
**Published**: 2025-10-23  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.19669v1  

Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their efficiency, enabling them to reach high performance without overthinking. First, we analyze the entropy of t...

---

### 19. [CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](https://arxiv.org/abs/2510.19670)

**Authors**: Hasan Akgul, Mari Eplik, Javier Rojas, Aina Binti Abdullah, Pieter van der Merwe  
**Category**: cs.CL  
**Published**: 2025-10-23  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.19670v1  

We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy...

---

### 20. [Learn More by Using Less: Distributed Learning with Energy-Constrained Devices](https://arxiv.org/abs/2412.02289)

**Authors**: Roberto Pereira, Cristian J. Vaca-Rubio, Luis Blanco  
**Category**: cs.DC  
**Published**: 2025-10-23  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2412.02289v2  

Federated Learning (FL) has emerged as a solution for distributed model training across decentralized, privacy-preserving devices, but the different energy capacities of participating devices (system heterogeneity) constrain real-world implementations. These energy limitations not only reduce model ...

---

### 21. [AmorLIP: Efficient Language-Image Pretraining via Amortization](https://arxiv.org/abs/2505.18983)

**Authors**: Haotian Sun, Yitong Li, Yuchen Zhuang, Niao He, Hanjun Dai, Bo Dai  
**Category**: cs.LG  
**Published**: 2025-10-23  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2505.18983v2  

Contrastive Language-Image Pretraining (CLIP) has demonstrated strong zero-shot performance across diverse downstream text-image tasks. Existing CLIP methods typically optimize a contrastive objective using negative samples drawn from each minibatch. To achieve robust representation learning, these ...

---

### 22. [CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation](https://arxiv.org/abs/2510.18895)

**Authors**: Santhosh Kumar Ravindran  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.18895v1  

We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL) architecture that integrates affective signals to enhance code generation in large language models (LLMs). Motivated by human and animal learning where embarrassment from mistakes drives rapid correction, as observed in trai...

---

### 23. [No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence](https://arxiv.org/abs/2510.19212)

**Authors**: Ernest Fokou\'e  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.19212v1  

The rapid ascent of artificial intelligence (AI) is often portrayed as a revolution born from computer science and engineering. This narrative, however, obscures a fundamental truth: the theoretical and methodological core of AI is, and has always been, statistical. This paper systematically argues ...

---

### 24. [SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes](https://arxiv.org/abs/2510.19241)

**Authors**: Xuyuan Xiong, Pedro Chumpitaz-Flores, Kaixun Hua, Cheng Hua  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.19241v1  

Interpretable reinforcement learning policies are essential for high-stakes decision-making, yet optimizing decision tree policies in Markov Decision Processes (MDPs) remains challenging. We propose SPOT, a novel method for computing decision tree policies, which formulates the optimization problem ...

---

### 25. [PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization](https://arxiv.org/abs/2409.03811)

**Authors**: Federico Berto, Chuanbo Hua, Laurin Luttmann, Jiwoo Son, Junyoung Park, Kyuree Ahn, Changhyun Kwon, Lin Xie, Jinkyoo Park  
**Category**: cs.AI  
**Published**: 2025-10-23  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2409.03811v3  

Combinatorial optimization problems involving multiple agents are notoriously challenging due to their NP-hard nature and the necessity for effective agent coordination. Despite advancements in learning-based methods, existing approaches often face critical limitations, including suboptimal agent co...

---

### 26. [DiSRouter: Distributed Self-Routing for LLM Selections](https://arxiv.org/abs/2510.19208)

**Authors**: Hang Zheng, Hongshen Xu, Yongkai Lin, Shuai Fan, Lu Chen, Kai Yu  
**Category**: cs.CL  
**Published**: 2025-10-23  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2510.19208v1  

The proliferation of Large Language Models (LLMs) has created a diverse ecosystem of models with highly varying performance and costs, necessitating effective query routing to balance performance and expense. Current routing systems often rely on a centralized external router trained on a fixed set ...

---

### 27. [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)

**Authors**: Yiqun Zhang, Hao Li, Jianhao Chen, Hangfan Zhang, Peng Ye, Lei Bai, Shuyue Hu  
**Category**: cs.CL  
**Published**: 2025-10-23  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2508.12631v2  

Balancing performance and efficiency is a central challenge in large language model (LLM) advancement. GPT-5 addresses this with test-time routing, dynamically assigning queries to either an efficient or a high-capacity model during inference. In this work, we present Avengers-Pro, a test-time routi...

---

### 28. [Efficient and scalable atmospheric dynamics simulations using non-conforming meshes](https://arxiv.org/abs/2408.08129)

**Authors**: Giuseppe Orlando, Tommaso Benacchio, Luca Bonaventura  
**Category**: cs.DC  
**Published**: 2025-10-23  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2408.08129v3  

We present the massively parallel performance of a $h$-adaptive solver for atmosphere dynamics that allows for non-conforming mesh refinement. The numerical method is based on a Discontinuous Galerkin (DG) spatial discretization, highly scalable thanks to its data locality properties, and on a secon...

---

### 29. [ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression](https://arxiv.org/abs/2505.06252)

**Authors**: Zirui Wang, Tingfeng Lan, Zhaoyuan Su, Juncheng Yang, Yue Cheng  
**Category**: cs.DC  
**Published**: 2025-10-23  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.06252v2  

Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with eac...

---

### 30. [g-DPO: Scalable Preference Optimization for Protein Language Models](https://arxiv.org/abs/2510.19474)

**Authors**: Constance Ferragu, Jonathan D. Ziegler, Nicolas Deutschmann, Arthur Lindoulsi, Eli Bixby, Cradle ML Team  
**Category**: cs.LG  
**Published**: 2025-10-23  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2510.19474v1  

Direct Preference Optimization (DPO) is an effective approach for aligning protein language models with experimental design goals. However, DPO faces a scalability bottleneck: the number of possible training pairs grows quadratically with the number of labeled sequences, leading to prohibitive train...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
