# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-09 12:53:11 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference](https://arxiv.org/abs/2505.11329)

**Authors**: Raja Gond, Nipun Kwatra, Ramachandran Ramjee  
**Category**: cs.DC  
**Published**: 2025-10-09  
**Score**: 12.0

arXiv:2505.11329v3 Announce Type: replace 
Abstract: Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLink. Multiple techniques have been proposed to mitigate these overheads by decomposing comp...

---

### 2. [ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training](https://arxiv.org/abs/2510.00606)

**Authors**: Xueze Kang, Guangyu Xiang, Yuxin Wang, Hao Zhang, Yuchu Fang, Yuhang Zhou, Zhenheng Tang, Youhui Lv, Eliran Maman, Mark Wasserman, Alon Zameret, Zhipeng Bian, Shushu Chen, Zhiyou Yu, Jin Wang, Xiaoyu Wu, Yang Zheng, Chen Tian, Xiaowen Chu  
**Category**: cs.DC  
**Published**: 2025-10-09  
**Score**: 10.5

arXiv:2510.00606v3 Announce Type: replace 
Abstract: Large-scale LLM pretraining now runs across $10^5$--$10^6$ accelerators, making failures routine and elasticity mandatory. We posit that an elastic-native training system must jointly deliver (i) parameter consistency, (ii) low mean time to recove...

---

### 3. [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)

**Authors**: Fanheng Kong, Jingyuan Zhang, Yahui Liu, Zirui Wu, Yu Tian, Victoria W., Guorui Zhou  
**Category**: cs.CL  
**Published**: 2025-10-09  
**Score**: 10.0

arXiv:2510.07081v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) represent a significant advancement in text generation, offering parallel token decoding capabilities. However, existing open-source implementations suffer from quality-speed trade-offs that impede their practic...

---

### 4. [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)

**Authors**: Shuang Cheng, Yihan Bian, Dawei Liu, Yuhua Jiang, Yihao Liu, Linfeng Zhang, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, Bowen Zhou  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 9.5

arXiv:2510.06303v1 Announce Type: cross 
Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies the training efficiency of autoregressive models with the parallel inference capability of diffusion. Instead of costly end-to-end diffusion training, SDAR performs a ligh...

---

### 5. [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)

**Authors**: Ziyan Wang, Zheng Wang, Jie Fu, Xingwei Qu, Qi Cheng, Shengpu Tang, Minjia Zhang, Xiaoming Huo  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 9.5

arXiv:2510.04072v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from low-qualit...

---

### 6. [A Novel Collaborative Framework for Efficient Synchronization in Split Federated Learning over Wireless Networks](https://arxiv.org/abs/2503.15559)

**Authors**: Haoran Gao, Samuel D. Okegbile, Jun Cai  
**Category**: cs.LG  
**Published**: 2025-10-09  
**Score**: 9.5

arXiv:2503.15559v2 Announce Type: replace 
Abstract: Split Federated Learning (SFL) offers a promising approach for distributed model training in wireless networks, combining the layer-partitioning advantages of split learning with the federated aggregation that ensures global convergence. However, ...

---

### 7. [SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications](https://arxiv.org/abs/2411.04975)

**Authors**: Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 9.0

arXiv:2411.04975v3 Announce Type: replace-cross 
Abstract: Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present ...

---

### 8. [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)

**Authors**: Guochen Yan, Luyuan Xie, Qingni Shen, Yuejian Fang, Zhonghai Wu  
**Category**: cs.CL  
**Published**: 2025-10-09  
**Score**: 9.0

arXiv:2510.04601v2 Announce Type: replace 
Abstract: The current paradigm of training large language models (LLMs) on publicly available Web data is becoming unsustainable, with high-quality data sources in specialized domains nearing exhaustion. Federated Learning (FL) emerges as a practical soluti...

---

### 9. [Accelerating Sparse Ternary GEMM for Quantized LLM inference on Apple Silicon](https://arxiv.org/abs/2510.06957)

**Authors**: Baraq Lipshitz (ETH Zurich), Alessio Melone (ETH Zurich), Charalampos Maraziaris (ETH Zurich), Muhammed Bilal (ETH Zurich)  
**Category**: cs.LG  
**Published**: 2025-10-09  
**Score**: 8.5

arXiv:2510.06957v1 Announce Type: cross 
Abstract: Sparse Ternary General Matrix-Matrix Multiplication (GEMM) remains under-optimized in existing libraries for Apple Silicon CPUs. We present a Sparse Ternary GEMM kernel optimized specifically for Apple's M-series processors. We propose a set of arch...

---

### 10. [Scalable In-context Ranking with Generative Models](https://arxiv.org/abs/2510.05396)

**Authors**: Nilesh Gupta, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Inderjit Dhillon, Felix Yu  
**Category**: cs.LG  
**Published**: 2025-10-09  
**Score**: 8.5

arXiv:2510.05396v2 Announce Type: replace-cross 
Abstract: In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR), which leverages contextual understanding of LLMs by directly incorporating the task description, candidate documents, and the query into the model's input promp...

---

### 11. [Sustainable LSTM-Based Precoding for RIS-Aided mmWave MIMO Systems with Implicit CSI](https://arxiv.org/abs/2509.12658)

**Authors**: Po-Heng Chou, Jiun-Jia Wu, Wan-Jen Huang, Ronald Y. Chang  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 8.0

arXiv:2509.12658v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a sustainable long short-term memory (LSTM)-based precoding framework for reconfigurable intelligent surface (RIS)-assisted millimeter-wave (mmWave) MIMO systems. Instead of explicit channel state information (CSI) ...

---

### 12. [Nonparametric Bellman Mappings for Value Iteration in Distributed Reinforcement Learning](https://arxiv.org/abs/2503.16192)

**Authors**: Yuki Akiyama, Konstantinos Slavakis  
**Category**: cs.LG  
**Published**: 2025-10-09  
**Score**: 8.0

arXiv:2503.16192v2 Announce Type: replace 
Abstract: This paper introduces novel Bellman mappings (B-Maps) for value iteration (VI) in distributed reinforcement learning (DRL), where agents are deployed over an undirected, connected graph/network with arbitrary topology -- but without a centralized ...

---

### 13. [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)

**Authors**: Zhepeng Cen, Haolin Chen, Shiyu Wang, Zuxin Liu, Zhiwei Liu, Ding Zhao, Silvio Savarese, Caiming Xiong, Huan Wang, Weiran Yao  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 7.5

arXiv:2510.06499v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient ...

---

### 14. [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)

**Authors**: Arjun Krishnakumar, Rhea Sanjay Sukthanker, Hannan Javed Mahadik, Gabriela Kadlecov\'a, Vladyslav Moroshan, Timur Carstensen, Frank Hutter, Aaron Klein  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 7.5

arXiv:2510.07227v1 Announce Type: cross 
Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings...

---

### 15. [KunServe: Parameter-centric Memory Management for Efficient Memory Overloading Handling in LLM Serving](https://arxiv.org/abs/2412.18169)

**Authors**: Rongxin Cheng, Yuxin Lai, Xingda Wei, Rong Chen, Haibo Chen  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 7.5

arXiv:2412.18169v5 Announce Type: replace-cross 
Abstract: Serving LLMs with a cluster of GPUs is common nowadays, where the serving system must meet strict latency SLOs required by applications. However, the stateful nature of LLM serving requires maintaining huge states (i.e., KVCache) in limited ...

---

### 16. [Structure-Aware Compound-Protein Affinity Prediction via Graph Neural Network with Group Lasso Regularization](https://arxiv.org/abs/2507.03318)

**Authors**: Zanyu Shi, Yang Wang, Pathum Weerawarna, Jie Zhang, Timothy Richardson, Yijie Wang, Kun Huang  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 7.5

arXiv:2507.03318v2 Announce Type: replace-cross 
Abstract: Explainable artificial intelligence (XAI) approaches have been increasingly applied in drug discovery to learn molecular representations and identify substructures driving property predictions. However, building end-to-end explainable models...

---

### 17. [Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions](https://arxiv.org/abs/2509.25775)

**Authors**: Amber Srivastava, Salar Basiri, Srinivasa Salapaka  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 7.5

arXiv:2509.25775v3 Announce Type: replace-cross 
Abstract: Clustering arises in a wide range of problem formulations, yet most existing approaches assume that the entities under clustering are passive and strictly conform to their assigned groups. In reality, entities often exhibit local autonomy, o...

---

### 18. [PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction](https://arxiv.org/abs/2506.15556)

**Authors**: Shufan Li, Aditya Grover  
**Category**: cs.CL  
**Published**: 2025-10-09  
**Score**: 7.5

arXiv:2506.15556v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely used in real-time voice chat applications, typically in combination with text-to-speech (TTS) systems to generate audio responses. However, their large size often leads to noticeable latency between the end ...

---

### 19. [POME: Post Optimization Model Edit via Muon-style Projection](https://arxiv.org/abs/2510.06627)

**Authors**: Yong Liu, Di Fu, Yang Luo, Zirui Zhu, Minhao Cheng, Cho-Jui Hsieh, Yang You  
**Category**: cs.LG  
**Published**: 2025-10-09  
**Score**: 7.5

arXiv:2510.06627v1 Announce Type: new 
Abstract: We introduce Post-Optimization Model Edit (POME), a new algorithm that enhances the performance of fine-tuned large language models using only their pretrained and fine-tuned checkpoints, without requiring extra data or further optimization. The core ...

---

### 20. [Toward Uncertainty-Aware and Generalizable Neural Decoding for Quantum LDPC Codes](https://arxiv.org/abs/2510.06257)

**Authors**: Xiangjun Mi, Frank Mueller  
**Category**: cs.LG  
**Published**: 2025-10-09  
**Score**: 7.5

arXiv:2510.06257v1 Announce Type: cross 
Abstract: Quantum error correction (QEC) is essential for scalable quantum computing, yet decoding errors via conventional algorithms result in limited accuracy (i.e., suppression of logical errors) and high overheads, both of which can be alleviated by infer...

---

### 21. [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)

**Authors**: Miao Lu, Weiwei Sun, Weihua Du, Zhan Ling, Xuesong Yao, Kang Liu, Jiecao Chen  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 7.0

arXiv:2510.06727v1 Announce Type: cross 
Abstract: We study reinforcement learning (RL) fine-tuning of large language model (LLM) agents for long-horizon multi-turn tool use, where context length quickly becomes a fundamental bottleneck. Existing RL pipelines can suffer from degraded instruction fol...

---

### 22. [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)

**Authors**: Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 7.0

arXiv:2510.07318v1 Announce Type: cross 
Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cogniti...

---

### 23. [SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models](https://arxiv.org/abs/2509.15174)

**Authors**: Huy Nghiem, Advik Sachdeva, Hal Daum\'e III  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 7.0

arXiv:2509.15174v2 Announce Type: replace-cross 
Abstract: WARNING: This paper contains examples of offensive materials. To address the proliferation of toxic content on social media, we introduce SMARTER, we introduce SMARTER, a data-efficient two-stage framework for explainable content moderation ...

---

### 24. [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)

**Authors**: Manish Nagaraj, Sakshi Choudhary, Utkarsh Saxena, Deepak Ravikumar, Kaushik Roy  
**Category**: cs.CL  
**Published**: 2025-10-09  
**Score**: 7.0

arXiv:2510.07118v1 Announce Type: new 
Abstract: Instruction tuning is essential for aligning large language models (LLMs) to downstream tasks and commonly relies on large, diverse corpora. However, small, high-quality subsets, known as coresets, can deliver comparable or superior results, though cu...

---

### 25. [The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives](https://arxiv.org/abs/2510.06096)

**Authors**: Matthieu Bou, Nyal Patel, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo  
**Category**: cs.CL  
**Published**: 2025-10-09  
**Score**: 7.0

arXiv:2510.06096v2 Announce Type: replace-cross 
Abstract: The objectives that Large Language Models (LLMs) implicitly optimize remain dangerously opaque, making trustworthy alignment and auditing a grand challenge. While Inverse Reinforcement Learning (IRL) can infer reward functions from behaviour...

---

### 26. [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)

**Authors**: Aochong Oliver Li, Tanya Goyal  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 6.5

arXiv:2510.06410v1 Announce Type: new 
Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding strong gains on complex tasks. This transparency also opens a promising direction: multiple reasoners can directly collaborate on each other's thinking within a shared trajector...

---

### 27. [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)

**Authors**: Wenxun Wu, Yuanyang Li, Guhan Chen, Linyue Wang, Hongyang Chen  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 6.5

arXiv:2510.07038v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have popularized test-time scaling, where models generate additional reasoning tokens before producing final answers. These approaches have demonstrated significant performance improvements on benchmarks...

---

### 28. [HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution](https://arxiv.org/abs/2510.06564)

**Authors**: Qiongyang Hu, Wenyang Liu, Wenbin Zou, Yuejiao Su, Lap-Pui Chau, Yi Wang  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 6.5

arXiv:2510.06564v1 Announce Type: cross 
Abstract: Existing deep learning approaches for image super-resolution, particularly those based on CNNs and attention mechanisms, often suffer from structural inflexibility. Although graph-based methods offer greater representational adaptability, they are f...

---

### 29. [h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning](https://arxiv.org/abs/2510.07312)

**Authors**: Sumeet Ramesh Motwani, Alesia Ivanova, Ziyang Cai, Philip Torr, Riashat Islam, Shital Shah, Christian Schroeder de Witt, Charles London  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 6.5

arXiv:2510.07312v1 Announce Type: cross 
Abstract: Large language models excel at short-horizon reasoning tasks, but performance drops as reasoning horizon lengths increase. Existing approaches to combat this rely on inference-time scaffolding or costly step-level supervision, neither of which scale...

---

### 30. [Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward](https://arxiv.org/abs/2209.12164)

**Authors**: Yolo Yunlong Tang, Siting Xu, Teng Wang, Qin Lin, Qinglin Lu, Feng Zheng  
**Category**: cs.AI  
**Published**: 2025-10-09  
**Score**: 6.5

arXiv:2209.12164v2 Announce Type: replace-cross 
Abstract: Advertisement video editing aims to automatically edit advertising videos into shorter videos while retaining coherent content and crucial information conveyed by advertisers. It mainly contains two stages: video segmentation and segment ass...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
