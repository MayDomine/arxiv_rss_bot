# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-01 12:53:53 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference](https://arxiv.org/abs/2405.18628)

**Authors**: Hao Mark Chen, Wayne Luk, Ka Fai Cedric Yiu, Rui Li, Konstantin Mishchenko, Stylianos I. Venieris, Hongxiang Fan  
**Category**: cs.CL  
**Published**: 2025-10-01  
**Score**: 14.0

arXiv:2405.18628v3 Announce Type: replace-cross 
Abstract: The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these eff...

---

### 2. [Fast-dLLM v2: Efficient Block-Diffusion LLM](https://arxiv.org/abs/2509.26328)

**Authors**: Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, Enze Xie  
**Category**: cs.CL  
**Published**: 2025-10-01  
**Score**: 12.5

arXiv:2509.26328v1 Announce Type: new 
Abstract: Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully...

---

### 3. [SlimPack: Fine-Grained Asymmetric Packing for Balanced and Efficient Variable-Length LLM Training](https://arxiv.org/abs/2509.26246)

**Authors**: Yuliang Liu, Guohao Wu, Shenglong Zhang, Wei Zhang, Qianchao Zhu, Zhouyang Li, Chenyu Wang  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 11.5

arXiv:2509.26246v1 Announce Type: new 
Abstract: The efficient distributed training of Large Language Models (LLMs) is severely hampered by the extreme variance in context lengths. This data heterogeneity, amplified by conventional packing strategies and asymmetric forward-backward costs, leads to c...

---

### 4. [OPPO: Accelerating PPO-based RLHF via Pipeline Overlap](https://arxiv.org/abs/2509.25762)

**Authors**: Kaizhuo Yan (University of Illinois Urbana-Champaign), Yingjie Yu (University of Illinois Urbana-Champaign), Yifan Yu (University of Illinois Urbana-Champaign), Haizhong Zheng (Carnegie Mellon University), Fan Lai (University of Illinois Urbana-Champaign)  
**Category**: cs.LG  
**Published**: 2025-10-01  
**Score**: 11.5

arXiv:2509.25762v1 Announce Type: new 
Abstract: Proximal Policy Optimization (PPO)-based reinforcement learning from human feedback (RLHF) is a widely adopted paradigm for aligning large language models (LLMs) with human preferences. However, its training pipeline suffers from substantial inefficie...

---

### 5. [Ban&Pick: Ehancing Performance and Efficiency of MoE-LLMs via Smarter Routing](https://arxiv.org/abs/2509.06346)

**Authors**: Yuanteng Chen, Peisong Wang, Yuantian Shao, Nanxin Zeng, Chang Xu, Jian Cheng  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 10.5

arXiv:2509.06346v2 Announce Type: replace-cross 
Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling large language models (LLMs) efficiently. Recent fine-grained MoE designs introduce hundreds of experts per layer, with multiple experts activated per token, enabling ...

---

### 6. [Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution](https://arxiv.org/abs/2509.25301)

**Authors**: Tianrui Qin, Qianben Chen, Sinuo Wang, He Xing, King Zhu, He Zhu, Dingfeng Shi, Xinxin Liu, Ge Zhang, Jiaheng Liu, Yuchen Eleanor Jiang, Xitong Gao, Wangchunshu Zhou  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 10.0

arXiv:2509.25301v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly...

---

### 7. [Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation](https://arxiv.org/abs/2509.26219)

**Authors**: Chenyang Jiang, Zhengcen Li, Hang Zhao, Qiben Shan, Shaocong Wu, Jingyong Su  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 10.0

arXiv:2509.26219v1 Announce Type: cross 
Abstract: Dataset distillation has emerged as a promising paradigm that synthesizes compact, informative datasets capable of retaining the knowledge of large-scale counterparts, thereby addressing the substantial computational and storage burdens of modern mo...

---

### 8. [AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size](https://arxiv.org/abs/2509.26432)

**Authors**: Guanxi Lu (Mark),  Hao (Mark),  Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 10.0

arXiv:2509.26432v1 Announce Type: cross 
Abstract: Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-A...

---

### 9. [dParallel: Learnable Parallel Decoding for dLLMs](https://arxiv.org/abs/2509.26488)

**Authors**: Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang  
**Category**: cs.CL  
**Published**: 2025-10-01  
**Score**: 9.5

arXiv:2509.26488v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parall...

---

### 10. [CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models](https://arxiv.org/abs/2509.25996)

**Authors**: Weiyu Huang, Yuezhou Hu, Jun Zhu, Jianfei Chen  
**Category**: cs.CL  
**Published**: 2025-10-01  
**Score**: 9.5

arXiv:2509.25996v1 Announce Type: cross 
Abstract: Sparsity-aware training is an effective approach for transforming large language models (LLMs) into hardware-friendly sparse patterns, thereby reducing latency and memory consumption during inference. In this paper, we propose Continuous Adaptive Sp...

---

### 11. [Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models](https://arxiv.org/abs/2509.26628)

**Authors**: Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai  
**Category**: cs.CL  
**Published**: 2025-10-01  
**Score**: 9.5

arXiv:2509.26628v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing P...

---

### 12. [FastCoder: Accelerating Repository-level Code Generation via Efficient Retrieval and Verification](https://arxiv.org/abs/2502.17139)

**Authors**: Qianhui Zhao, Li Zhang, Fang Liu, Xiaoli Lian, Qiaoyuanhe Meng, Ziqian Jiao, Zetong Zhou, Jia Li, Lin Shi  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 9.0

arXiv:2502.17139v2 Announce Type: replace 
Abstract: Code generation is a latency-sensitive task that demands high timeliness. However, with the growing interest and inherent difficulty in repository-level code generation, most existing code generation studies focus on improving the correctness of g...

---

### 13. [Hierarchical Balance Packing: Towards Efficient Supervised Fine-tuning for Long-Context LLM](https://arxiv.org/abs/2503.07680)

**Authors**: Yongqiang Yao, Jingru Tan, Kaihuan Liang, Feizhao Zhang, Jiahao Hu, Shuo Wu, Yazhe Niu, Ruihao Gong, Dahua Lin, Ningyi Xu  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 9.0

arXiv:2503.07680v2 Announce Type: replace-cross 
Abstract: Training Long-Context Large Language Models (LLMs) is challenging, as hybrid training with long-context and short-context data often leads to workload imbalances. Existing works mainly use data packing to alleviate this issue, but fail to co...

---

### 14. [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)

**Authors**: Feilong Chen, Yijiang Liu, Yi Huang, Hao Wang, Miren Tian, Ya-Qi Yu, Minghui Liao, Jihao Wu  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 9.0

arXiv:2509.11662v3 Announce Type: replace-cross 
Abstract: We propose MindVL, a multimodal large language model (MLLMs) trained on Ascend NPUs. The training of state-of-the-art MLLMs is often confined to a limited set of hardware platforms and relies heavily on massive, undisclosed data recipes, whi...

---

### 15. [DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search](https://arxiv.org/abs/2509.25454)

**Authors**: Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin ChoiRetry  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 8.5

arXiv:2509.25454v1 Announce Type: new 
Abstract: Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performa...

---

### 16. [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966)

**Authors**: Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 8.5

arXiv:2507.07966v4 Announce Type: replace-cross 
Abstract: We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical component...

---

### 17. [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)

**Authors**: Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 8.5

arXiv:2508.19982v2 Announce Type: replace-cross 
Abstract: Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive model...

---

### 18. [Parallax: Efficient LLM Inference Service over Decentralized Environment](https://arxiv.org/abs/2509.26182)

**Authors**: Chris Tong, Youhe Jiang, Gufeng Chen, Tianyi Zhao, Sibian Lu, Wenjie Qu, Eric Yang, Lynn Ai, Binhang Yuan  
**Category**: cs.DC  
**Published**: 2025-10-01  
**Score**: 8.5

arXiv:2509.26182v1 Announce Type: new 
Abstract: Deploying a large language model (LLM) inference service remains costly because centralized serving depends on specialized GPU clusters and high-bandwidth interconnects in datacenters. An appealing alternative is to leverage collaborative decentralize...

---

### 19. [Cascadia: An Efficient Cascade Serving System for Large Language Models](https://arxiv.org/abs/2506.04203)

**Authors**: Youhe Jiang, Fangcheng Fu, Wanru Zhao, Stephan Rabanser, Jintao Zhang, Nicholas D. Lane, Binhang Yuan  
**Category**: cs.DC  
**Published**: 2025-10-01  
**Score**: 8.5

arXiv:2506.04203v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have intensified the need to deliver both rapid responses and high-quality outputs. More powerful models yield better results but incur higher inference latency, whereas smaller models are faster yet...

---

### 20. [Fast training of accurate physics-informed neural networks without gradient descent](https://arxiv.org/abs/2405.20836)

**Authors**: Chinmay Datar, Taniya Kapoor, Abhishek Chandra, Qing Sun, Erik Lien Bolager, Iryna Burak, Anna Veselovska, Massimo Fornasier, Felix Dietrich  
**Category**: cs.LG  
**Published**: 2025-10-01  
**Score**: 8.5

arXiv:2405.20836v2 Announce Type: replace-cross 
Abstract: Solving time-dependent Partial Differential Equations (PDEs) is one of the most critical problems in computational science. While Physics-Informed Neural Networks (PINNs) offer a promising framework for approximating PDE solutions, their acc...

---

### 21. [Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning](https://arxiv.org/abs/2509.25267)

**Authors**: Jiexi Xu  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 8.0

arXiv:2509.25267v1 Announce Type: cross 
Abstract: The performance of Large Language Models (LLMs) depends heavily on the chosen prompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or Chain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly accurate strategies l...

---

### 22. [Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning](https://arxiv.org/abs/2509.25300)

**Authors**: Zelin Tan, Hejia Geng, Mulei Zhang, Xiaohang Yu, Guancheng Wan, Yifan Zhou, Qiang He, Xiangyuan Xue, Heng Zhou, Yutao Fan, Zhongzhi Li, Zaibin Zhang, Guibin Zhang, Chen Zhang, Zhenfei Yin, Lei Bai  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 8.0

arXiv:2509.25300v1 Announce Type: cross 
Abstract: While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigatio...

---

### 23. [Linear Attention for Efficient Bidirectional Sequence Modeling](https://arxiv.org/abs/2502.16249)

**Authors**: Arshia Afzal, Elias Abad Rocamora, Leyla Naz Candogan, Pol Puigdemont, Francesco Tonin, Yongtao Wu, Mahsa Shoaran, Volkan Cevher  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 8.0

arXiv:2502.16249v2 Announce Type: replace-cross 
Abstract: Linear Transformers and State Space Models have emerged as efficient alternatives to softmax Transformers for causal sequence modeling, enabling parallel training via matrix multiplication and efficient RNN-style inference. However, despite ...

---

### 24. [Neural Optimal Transport Meets Multivariate Conformal Prediction](https://arxiv.org/abs/2509.25444)

**Authors**: Vladimir Kondratyev, Alexander Fishkov, Nikita Kotelevskii, Mahmoud Hegazy, Remi Flamary, Maxim Panov, Eric Moulines  
**Category**: cs.LG  
**Published**: 2025-10-01  
**Score**: 8.0

arXiv:2509.25444v1 Announce Type: cross 
Abstract: We propose a framework for conditional vector quantile regression (CVQR) that combines neural optimal transport with amortized optimization, and apply it to multivariate conformal prediction. Classical quantile regression does not extend naturally t...

---

### 25. [Metis: Training LLMs with FP4 Quantization](https://arxiv.org/abs/2509.00404)

**Authors**: Hengjie Cao, Mengyi Chen, Yifeng Yang, Ruijun Huang, Fang Dong, Jixian Zhou, Anrui Chen, Mingzhi Dong, Yujiang Wang, Jinlong Hou, Yuan Cheng, Fan Wu, Fan Yang, Tun Lu, Ning Gu, Li Shang  
**Category**: cs.LG  
**Published**: 2025-10-01  
**Score**: 8.0

arXiv:2509.00404v4 Announce Type: replace 
Abstract: This work identifies anisotropy in the singular value spectra of parameters, activations, and gradients as the fundamental barrier to low-bit training of large language models (LLMs). These spectra are dominated by a small fraction of large singul...

---

### 26. [SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation](https://arxiv.org/abs/2509.25672)

**Authors**: Hasan Alp Cafero\u{g}lu, Mehmet Serhat \c{C}elik, \"Ozg\"ur Ulusoy  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 7.5

arXiv:2509.25672v1 Announce Type: new 
Abstract: Translating natural language questions into SQL has become a core challenge in enabling non-technical users to query databases. While recent work has explored large-scale synthetic data generation to improve model performance through post-training, mo...

---

### 27. [Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs](https://arxiv.org/abs/2509.25779)

**Authors**: Siyu Zhu, Yanbin Jiang, Hejian Sang, Shao Tang, Qingquan Song, Biao He, Rohit Jain, Zhipeng Wang, Alborz Geramifard  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 7.5

arXiv:2509.25779v1 Announce Type: new 
Abstract: We investigated Agentic RL with large language models on the \textsc{TravelPlanner} benchmark. Our approach, \textsc{Planner-R1}, achieved a \textbf{56.9\%} final-pass rate with only 180 training queries, a $2.7\times$ improvement over GPT-5's $21.2\%...

---

### 28. [Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework](https://arxiv.org/abs/2509.26534)

**Authors**: Jovan Stojkovic, Chaojie Zhang, \'I\~nigo Goiri, Ricardo Bianchini  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 7.5

arXiv:2509.26534v1 Announce Type: new 
Abstract: The rapid rise of large language models (LLMs) has been driving an enormous demand for AI inference infrastructure, mainly powered by high-end GPUs. While these accelerators offer immense computational power, they incur high capital and operational co...

---

### 29. [Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications](https://arxiv.org/abs/2509.25736)

**Authors**: Chenhua Shi, Gregor Macdonald, Bhavika Jalli, Wanlu Lei, John Zou, Mridul Jain, Joji Philip  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 7.5

arXiv:2509.25736v1 Announce Type: cross 
Abstract: The success of large language models (LLMs) depends heavily on large-scale, high-quality instruction-following and reinforcement datasets. However, generating such data through human annotation is prohibitively time-consuming particularly for domain...

---

### 30. [Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions](https://arxiv.org/abs/2509.25775)

**Authors**: Amber Srivastava, Salar Basiri, Srinivasa Salapaka  
**Category**: cs.AI  
**Published**: 2025-10-01  
**Score**: 7.5

arXiv:2509.25775v1 Announce Type: cross 
Abstract: Clustering arises in a wide range of problem formulations, yet most existing approaches assume that the entities under clustering are passive and strictly conform to their assigned groups. In reality, entities often exhibit local autonomy, overridin...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
