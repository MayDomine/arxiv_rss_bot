# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-10-13 12:52:52 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [dInfer: An Efficient Inference Framework for Diffusion Language Models](https://arxiv.org/abs/2510.08666)

**Authors**: Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, Xinyuan Zhang, Zhen Tao, Haibo Feng, Ziyun Jiang, Ying Xu, Zenan Huang, Yihong Zhuang, Haokai Xu, Jiaqi Hu, Zhenzhong Lan, Junbo Zhao, Jianguo Li, Da Zheng  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 12.5

arXiv:2510.08666v1 Announce Type: cross 
Abstract: Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their wi...

---

### 2. [Automated Evolutionary Optimization for Resource-Efficient Neural Network Training](https://arxiv.org/abs/2510.09566)

**Authors**: Ilia Revin, Leon Strelkov, Vadim A. Potemkin, Ivan Kireev, Andrey Savchenko  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 12.0

arXiv:2510.09566v1 Announce Type: new 
Abstract: There are many critical challenges in optimizing neural network models, including distributed computing, compression techniques, and efficient training, regardless of their application to specific tasks. Solving such problems is crucial because the ne...

---

### 3. [Slim Scheduler: A Runtime-Aware RL and Scheduler System for Efficient CNN Inference](https://arxiv.org/abs/2510.09018)

**Authors**: Ian Harshbarger, Calvin Chidambaram  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 11.5

arXiv:2510.09018v1 Announce Type: new 
Abstract: Most neural network scheduling research focuses on optimizing static, end-to-end models of fixed width, overlooking dynamic approaches that adapt to heterogeneous hardware and fluctuating runtime conditions. We present Slim Scheduler, a hybrid schedul...

---

### 4. [Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference](https://arxiv.org/abs/2510.09309)

**Authors**: Jianuo Huang, Yaojie Zhang, Yicun Yang, Benhao Huang, Biqing Qi, Dongrui Liu, Linfeng Zhang  
**Category**: cs.CL  
**Published**: 2025-10-13  
**Score**: 10.5

arXiv:2510.09309v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) present a promising alternative to dominant autoregressive models (ARMs) by the ability of parallel decoding at the expense of substantial computation and memory costs. Specifically, the cache mechanism for bidi...

---

### 5. [LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and Hardware Co-design](https://arxiv.org/abs/2502.15260)

**Authors**: Renjie Wei, Songqiang Xu, Linfeng Zhong, Zebin Yang, Qingyu Guo, Yuan Wang, Runsheng Wang, Meng Li  
**Category**: cs.CL  
**Published**: 2025-10-13  
**Score**: 9.5

arXiv:2502.15260v2 Announce Type: replace 
Abstract: State space models (SSMs) like Mamba have recently attracted much attention. Compared to Transformer-based large language models (LLMs), Mamba achieves linear computation complexity with the sequence length and demonstrates superior performance. H...

---

### 6. [Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning](https://arxiv.org/abs/2507.20424)

**Authors**: Tolga Dimlioglu, Anna Choromanska  
**Category**: cs.DC  
**Published**: 2025-10-13  
**Score**: 9.0

arXiv:2507.20424v2 Announce Type: replace-cross 
Abstract: We study centralized distributed data parallel training of deep neural networks (DNNs), aiming to improve the trade-off between communication efficiency and model performance of the local gradient methods. To this end, we revisit the flat-mi...

---

### 7. [Partition Generative Modeling: Masked Modeling Without Masks](https://arxiv.org/abs/2505.18883)

**Authors**: Justin Deschenaux, Lan Tran, Caglar Gulcehre  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 9.0

arXiv:2505.18883v2 Announce Type: replace 
Abstract: Masked generative models (MGMs) are widely used to capture complex data and enable faster generation than autoregressive models (AR) through parallel decoding. However, MGMs typically operate on fixed-length inputs, which can be inefficient: early...

---

### 8. [The Enduring Dominance of Deep Neural Networks: A Critical Analysis of the Fundamental Limitations of Quantum Machine Learning and Spiking Neural Networks](https://arxiv.org/abs/2510.08591)

**Authors**: Takehiro Ishikawa  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 8.5

arXiv:2510.08591v1 Announce Type: cross 
Abstract: Recent advancements in QML and SNNs have generated considerable excitement, promising exponential speedups and brain-like energy efficiency to revolutionize AI. However, this paper argues that they are unlikely to displace DNNs in the near term. QML...

---

### 9. [FLRC: Fine-grained Low-Rank Compressor for Efficient LLM Inference](https://arxiv.org/abs/2510.09332)

**Authors**: Yu-Chen Lu, Chong-Yan Chen, Chi-Chih Chang, Yu-Fang Hu, Kai-Chiang Wu  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 8.5

arXiv:2510.09332v1 Announce Type: cross 
Abstract: Although large language models (LLM) have achieved remarkable performance, their enormous parameter counts hinder deployment on resource-constrained hardware. Low-rank compression can reduce both memory usage and computational demand, but applying a...

---

### 10. [DeepOHeat-v1: Efficient Operator Learning for Fast and Trustworthy Thermal Simulation and Optimization in 3D-IC Design](https://arxiv.org/abs/2504.03955)

**Authors**: Xinling Yu, Ziyue Liu, Hai Li, Yixing Li, Xin Ai, Zhiyu Zeng, Ian Young, Zheng Zhang  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 8.5

arXiv:2504.03955v2 Announce Type: replace-cross 
Abstract: Thermal analysis is crucial in 3D-IC design due to increased power density and complex heat dissipation paths. Although operator learning frameworks such as DeepOHeat~\cite{liu2023deepoheat} have demonstrated promising preliminary results in...

---

### 11. [Collaborative Unlabeled Data Optimization](https://arxiv.org/abs/2505.14117)

**Authors**: Xinyi Shang, Peng Sun, Fengyuan Liu, Tao Lin  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 8.5

arXiv:2505.14117v2 Announce Type: replace-cross 
Abstract: This paper pioneers a novel data-centric paradigm to maximize the utility of unlabeled data, tackling a critical question: How can we enhance the efficiency and sustainability of deep learning training by optimizing the data itself? We begin...

---

### 12. [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)

**Authors**: Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, Tong Zhang  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 8.5

arXiv:2510.04996v2 Announce Type: replace-cross 
Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this...

---

### 13. [Getting Your Indices in a Row: Full-Text Search for LLM Training Data for Real World](https://arxiv.org/abs/2510.09471)

**Authors**: Ines Altemir Marinas, Anastasiia Kucherenko, Alexander Sternfeld, Andrei Kucharavy  
**Category**: cs.CL  
**Published**: 2025-10-13  
**Score**: 8.5

arXiv:2510.09471v1 Announce Type: new 
Abstract: The performance of Large Language Models (LLMs) is determined by their training data. Despite the proliferation of open-weight LLMs, access to LLM training data has remained limited. Even for fully open LLMs, the scale of the data makes it all but ins...

---

### 14. [FAST: An Efficient Scheduler for All-to-All GPU Communication](https://arxiv.org/abs/2505.09764)

**Authors**: Yiran Lei, Dongjoo Lee, Liangyu Zhao, Daniar Kurniawan, Chanmyeong Kim, Heetaek Jeong, Changsu Kim, Hyeonseong Choi, Liangcheng Yu, Arvind Krishnamurthy, Justine Sherry, Eriko Nurvitadhi  
**Category**: cs.DC  
**Published**: 2025-10-13  
**Score**: 8.5

arXiv:2505.09764v2 Announce Type: replace 
Abstract: All-to-All(v) communication is a critical primitive in modern machine learning workloads, particularly mixture-of-experts (MoE) models. Unfortunately, efficient scheduling is challenging due to workload skew, heterogeneous two-tier fabrics, and in...

---

### 15. [Efficient Resource-Constrained Training of Vision Transformers via Subspace Optimization](https://arxiv.org/abs/2510.09160)

**Authors**: Le-Trung Nguyen, Enzo Tartaglione, Van-Tam Nguyen  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 8.5

arXiv:2510.09160v1 Announce Type: new 
Abstract: As AI increasingly shapes daily life, energy consumption and data privacy have become pressing concerns. On-device learning trains models directly on edge devices, cutting energy consumption and safeguarding data privacy. However, the expanding scale ...

---

### 16. [K-ASTRO: Structure-Aware Adaptation of LLMs for Code Vulnerability Detection](https://arxiv.org/abs/2208.08067)

**Authors**: Yifan Zhang, Michael Sandborn, Stefan Larson, Yu Huang, Kevin Leach  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 8.5

arXiv:2208.08067v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are transforming software engineering tasks, including code vulnerability detection-a critical area of software security. However, existing methods often rely on resource-intensive models or graph-based technique...

---

### 17. [Localist LLMs -- A Mathematical Framework for Dynamic Locality Control](https://arxiv.org/abs/2510.09338)

**Authors**: Joachim Diederich  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 8.0

arXiv:2510.09338v1 Announce Type: new 
Abstract: We present a novel framework for training large language models with continuously adjustable internal representations that span the full spectrum from localist (interpretable, rule-based) to distributed (generalizable, efficient) encodings. The key in...

---

### 18. [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)

**Authors**: Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 8.0

arXiv:2507.01752v2 Announce Type: replace-cross 
Abstract: Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, exposing gradients during training can leak sensitive information about the underlying data, raising privac...

---

### 19. [Neural Beam Field for Spatial Beam RSRP Prediction](https://arxiv.org/abs/2508.06956)

**Authors**: Keqiang Guo, Yuheng Zhong, Xin Tong, Jiangbin Lyu, Rui Zhang  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 8.0

arXiv:2508.06956v2 Announce Type: replace-cross 
Abstract: Accurately predicting beam-level reference signal received power (RSRP) is essential for beam management in dense multi-user wireless networks, yet challenging due to high measurement overhead and fast channel variations. This paper proposes...

---

### 20. [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)

**Authors**: Chien Van Nguyen, Ruiyi Zhang, Hanieh Deilamsalehy, Puneet Mathur, Viet Dac Lai, Haoliang Wang, Jayakumar Subramanian, Ryan A. Rossi, Trung Bui, Nikos Vlassis, Franck Dernoncourt, Thien Huu Nguyen  
**Category**: cs.CL  
**Published**: 2025-10-13  
**Score**: 8.0

arXiv:2507.09025v3 Announce Type: replace 
Abstract: We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into subquadratic architectures. Transformers faces severe computational and memory bottlenecks with long sequences due to the q...

---

### 21. [AdaPM: a Partial Momentum Algorithm for LLM Training](https://arxiv.org/abs/2510.09103)

**Authors**: Yimu Zhang, Yuanshi Liu, Cong Fang  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 8.0

arXiv:2510.09103v1 Announce Type: new 
Abstract: In the training of large language models, momentum is widely used and often demonstrated to achieve significant acceleration. However, storing momentum typically presents memory challenges. In this paper, we propose AdaPM, an adaptive training strateg...

---

### 22. [Efficient Autoregressive Inference for Transformer Probabilistic Models](https://arxiv.org/abs/2510.09477)

**Authors**: Conor Hassan, Nasrulloh Loka, Cen-You Li, Daolang Huang, Paul E. Chang, Yang Yang, Francesco Silvestrin, Samuel Kaski, Luigi Acerbi  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 8.0

arXiv:2510.09477v1 Announce Type: cross 
Abstract: Transformer-based models for amortized probabilistic inference, such as neural processes, prior-fitted networks, and tabular foundation models, excel at single-pass marginal prediction. However, many real-world applications, from signal interpolatio...

---

### 23. [Filtering out mislabeled training instances using black-box optimization and quantum annealing](https://arxiv.org/abs/2501.06916)

**Authors**: Makoto Otsuka, Kento Kodama, Keisuke Morita, Masayuki Ohzeki  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 8.0

arXiv:2501.06916v2 Announce Type: replace 
Abstract: This study proposes an approach for removing mislabeled instances from contaminated training datasets by combining surrogate model-based black-box optimization (BBO) with postprocessing and quantum annealing. Mislabeled training instances, a commo...

---

### 24. [GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare](https://arxiv.org/abs/2510.08872)

**Authors**: Siqi Zhu, David Zhang, Pedro Cisneros-Velarde, Jiaxuan You  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 7.5

arXiv:2510.08872v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable progress in reasoning, yet sometimes produce responses that are suboptimal for users in tasks such as writing, information seeking, or providing practical guidance. Conventional alignment practices...

---

### 25. [Guiding Exploration in Reinforcement Learning Through LLM-Augmented Observations](https://arxiv.org/abs/2510.08779)

**Authors**: Vaibhav Jain, Gerrit Grossmann  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 7.5

arXiv:2510.08779v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) agents often struggle in sparse-reward environments where traditional exploration strategies fail to discover effective action sequences. Large Language Models (LLMs) possess procedural knowledge and reasoning capabilitie...

---

### 26. [Enabling Population-Level Parallelism in Tree-Based Genetic Programming for GPU Acceleration](https://arxiv.org/abs/2501.17168)

**Authors**: Zhihong Wu, Lishuang Wang, Kebin Sun, Zhuozhao Li, Ran Cheng  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 7.5

arXiv:2501.17168v5 Announce Type: replace-cross 
Abstract: Tree-based Genetic Programming (TGP) is a widely used evolutionary algorithm for tasks such as symbolic regression, classification, and robotic control. Due to the intensive computational demands of running TGP, GPU acceleration is crucial f...

---

### 27. [Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting](https://arxiv.org/abs/2510.09152)

**Authors**: Suming Qiu, Jing Li, Zhicheng Zhou, Junjie Huang, Linyuan Qiu, Zhijie Sun  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 7.5

arXiv:2510.09152v1 Announce Type: new 
Abstract: Large language models (LLMs) often face a trade-off in post-training: improvements on specialized domains frequently come at the expense of general capabilities. Existing solutions attempt to mitigate this tension via regularization, selective paramet...

---

### 28. [COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs](https://arxiv.org/abs/2502.17410)

**Authors**: Liming Liu, Zhenghao Xu, Zixuan Zhang, Hao Kang, Zichong Li, Chen Liang, Weizhu Chen, Tuo Zhao  
**Category**: cs.LG  
**Published**: 2025-10-13  
**Score**: 7.5

arXiv:2502.17410v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable success across various domains, yet their optimization remains a significant challenge due to the complex and high-dimensional loss landscapes they inhabit. While adaptive optimizers such a...

---

### 29. [Deep Multimodal Subspace Clustering Networks](https://arxiv.org/abs/1804.06498)

**Authors**: Mahdi Abavisani, Vishal M. Patel  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 7.0

arXiv:1804.06498v3 Announce Type: cross 
Abstract: We present convolutional neural network (CNN) based approaches for unsupervised multimodal subspace clustering. The proposed framework consists of three main stages - multimodal encoder, self-expressive layer, and multimodal decoder. The encoder tak...

---

### 30. [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)

**Authors**: Wenjun Wang, Shuo Cai, Congkai Xie, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Hongxia Yang  
**Category**: cs.AI  
**Published**: 2025-10-13  
**Score**: 7.0

arXiv:2509.22536v3 Announce Type: replace-cross 
Abstract: The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation. While FP8 training offers a promising solution with significant theoretical efficiency gains, its widespread adoption has been hi...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
