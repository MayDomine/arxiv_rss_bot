# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-27 12:55:07 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding](https://arxiv.org/abs/2505.13254)

**Authors**: Siran Liu, Yang Ye, Qianchao Zhu, Zane Cao, Yongchao He  
**Category**: cs.CL  
**Published**: 2025-10-27  
**Score**: 12.5  
**Type**: replace  
**ArXiv ID**: 2505.13254v2  

Autoregressive decoding inherently limits the inference throughput of Large Language Model (LLM) due to its sequential dependency. Speculative decoding mitigates this by verifying multiple predicted tokens in parallel, but its efficiency remains constrained by what we identify as verification hetero...

---

### 2. [Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression](https://arxiv.org/abs/2510.20984)

**Authors**: Xi Zhang, Xiaolin Wu, Jiamang Wang, Weisi Lin  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 10.5  
**Type**: cross  
**ArXiv ID**: 2510.20984v1  

Large Language Models (LLMs) have demonstrated remarkable capabilities but typically require extensive computational resources and memory for inference. Post-training quantization (PTQ) can effectively reduce these demands by storing weights in lower bit-width formats. However, standard uniform quan...

---

### 3. [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)

**Authors**: Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Regina Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Yulun Wang, Bruce Wu, Xinfeng Xie, Jingyi Yang, Mingran Yang, Kenny Yu, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Prashanth Kannan, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Srikanth Sundaresan, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 10.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.20171v2  

The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hin...

---

### 4. [ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](https://arxiv.org/abs/2505.17495)

**Authors**: Landon Butler, Abhineet Agarwal, Justin Singh Kang, Yigit Efe Erginbas, Bin Yu, Kannan Ramchandran  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.17495v2  

Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the nu...

---

### 5. [FlexLLM: Token-Level Co-Serving of LLM Inference and Finetuning with SLO Guarantees](https://arxiv.org/abs/2402.18789)

**Authors**: Gabriele Oliaro, Xupeng Miao, Xinhao Cheng, Vineeth Kada, Mengdi Wu, Ruohan Gao, Yingyi Huang, Remi Delacourt, April Yang, Yingcheng Wang, Colin Unger, Zhihao Jia  
**Category**: cs.CL  
**Published**: 2025-10-27  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2402.18789v3  

Finetuning large language models (LLMs) is essential for task adaptation, yet today's serving stacks isolate inference and finetuning on separate GPU clusters -- wasting resources and under-utilizing hardware. We introduce FlexLLM, the first system to co-serve LLM inference and PEFT-based finetuning...

---

### 6. [BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning](https://arxiv.org/abs/2506.06072)

**Authors**: Hongyi Zhou, Weiran Liao, Xi Huang, Yucheng Tang, Fabian Otto, Xiaogang Jia, Xinkai Jiang, Simon Hilber, Ge Li, Qian Wang, \"Omer Erdin\c{c} Ya\u{g}murlu, Nils Blank, Moritz Reuss, Rudolf Lioutikov  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2506.06072v3  

We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel action tokenizer that encodes action sequences into compact discrete or continuous tokens using B-splines. In contrast to existing action tokenizers based on vector quantization or byte pair encoding, BEAST requires no separa...

---

### 7. [Sparser Block-Sparse Attention via Token Permutation](https://arxiv.org/abs/2510.21270)

**Authors**: Xinghao Wang, Pengyu Wang, Dong Zhang, Chenkun Tan, Shaojun Zhou, Zhaoxiang Liu, Shiguo Lian, Fangxu Liu, Kai Song, Xipeng Qiu  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2510.21270v1  

Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose $O(N^2)$ complexity with respect to sequence length presents a major bottleneck for both memory and latency. ...

---

### 8. [Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning](https://arxiv.org/abs/2506.09498)

**Authors**: Jaesik Yoon, Hyeonseo Cho, Yoshua Bengio, Sungjin Ahn  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2506.09498v4  

Diffusion models have recently emerged as a powerful approach for trajectory planning. However, their inherently non-sequential nature limits their effectiveness in long-horizon reasoning tasks at test time. The recently proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by combi...

---

### 9. [Approximating Signed Distance Fields of Implicit Surfaces with Sparse Ellipsoidal Radial Basis Function Networks](https://arxiv.org/abs/2505.02350)

**Authors**: Bobo Lian, Dandan Wang, Chenjian Wu, Minxin Chen  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.02350v3  

Accurate and compact representation of signed distance functions (SDFs) of implicit surfaces is crucial for efficient storage, computation, and downstream processing of 3D geometry. In this work, we propose a general learning method for approximating precomputed SDF fields of implicit surfaces by a ...

---

### 10. [Efficient semantic uncertainty quantification in language models via diversity-steered sampling](https://arxiv.org/abs/2510.21310)

**Authors**: Ji Won Park, Kyunghyun Cho  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2510.21310v1  

Accurately estimating semantic aleatoric and epistemic uncertainties in large language models (LLMs) is particularly challenging in free-form question answering (QA), where obtaining stable estimates often requires many expensive generations. We introduce a diversity-steered sampler that discourages...

---

### 11. [Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models](https://arxiv.org/abs/2407.04656)

**Authors**: Yongji Wu, Wenjie Qu, Xueshen Liu, Tianyang Tao, Yifan Qiao, Zhuang Wang, Wei Bai, Yuan Tian, Jiaheng Zhang, Z. Morley Mao, Matthew Lentz, Danyang Zhuo, Ion Stoica  
**Category**: cs.DC  
**Published**: 2025-10-27  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2407.04656v2  

Sparsely-activated Mixture-of-Experts (MoE) architecture has increasingly been adopted to further scale large language models (LLMs). However, frequent failures still pose significant challenges as training scales. The cost of even a single failure is significant, as all GPUs need to idle wait until...

---

### 12. [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)

**Authors**: Yongji Wu, Xueshen Liu, Haizhong Zheng, Juncheng Gu, Beidi Chen, Z. Morley Mao, Arvind Krishnamurthy, Ion Stoica  
**Category**: cs.DC  
**Published**: 2025-10-27  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2510.19225v2  

Reinforcement learning (RL) has become essential for unlocking advanced reasoning capabilities in large language models (LLMs). RL workflows involve interleaving rollout and training stages with fundamentally different resource requirements. Rollout typically dominates overall execution time, yet sc...

---

### 13. [KOALA++: Efficient Kalman-Based Optimization with Gradient-Covariance Products](https://arxiv.org/abs/2506.04432)

**Authors**: Zixuan Xia, Aram Davtyan, Paolo Favaro  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2506.04432v3  

We propose KOALA++, a scalable Kalman-based optimization algorithm that explicitly models structured gradient uncertainty in neural network training. Unlike second-order methods, which rely on expensive second order gradient calculation, our method directly estimates the parameter covariance matrix ...

---

### 14. [FORLA: Federated Object-centric Representation Learning with Slot Attention](https://arxiv.org/abs/2506.02964)

**Authors**: Guiqiu Liao, Matjaz Jogan, Eric Eaton, Daniel A. Hashimoto  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2506.02964v2  

Learning efficient visual representations across heterogeneous unlabeled datasets remains a central challenge in federated learning. Effective federated representations require features that are jointly informative across clients while disentangling domain-specific factors without supervision. We in...

---

### 15. [A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization](https://arxiv.org/abs/2510.21314)

**Authors**: Xuan Tang, Jichu Li, Difan Zou  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.21314v1  

The rapid scaling of large language models (LLMs) has made low-precision training essential for reducing memory, improving efficiency, and enabling larger models and datasets. Existing convergence theories for adaptive optimizers, however, assume all components are exact and neglect hardware-aware q...

---

### 16. [Reinforced Latent Reasoning for LLM-based Recommendation](https://arxiv.org/abs/2505.19092)

**Authors**: Yang Zhang, Wenxin Xu, Xiaoyan Zhao, Wenjie Wang, Fuli Feng, Xiangnan He, Tat-Seng Chua  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2505.19092v2  

Large Language Models (LLMs) have demonstrated impressive reasoning capabilities in complex problem-solving tasks, sparking growing interest in their application to preference reasoning in recommendation systems. Existing methods typically rely on fine-tuning with explicit chain-of-thought (CoT) dat...

---

### 17. [Exploring the Limitations of Layer Synchronization in Spiking Neural Networks](https://arxiv.org/abs/2408.05098)

**Authors**: Roel Koopman, Amirreza Yousefzadeh, Mahyar Shahsavari, Guangzhi Tang, Manolis Sifalakis  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2408.05098v2  

Neural-network processing in machine learning applications relies on layer synchronization. This is practiced even in artificial Spiking Neural Networks (SNNs), which are touted as consistent with neurobiology, in spite of processing in the brain being in fact asynchronous. A truly asynchronous syst...

---

### 18. [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485)

**Authors**: Ziyang Wang, Jaehong Yoon, Shoubin Yu, Md Mohaiminul Islam, Gedas Bertasius, Mohit Bansal  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2507.06485v2  

Despite advances in reinforcement learning (RL)-based video reasoning with large language models (LLMs), data collection and fine-tuning remain significant challenges. These methods often rely on large-scale supervised fine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT) annot...

---

### 19. [Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space](https://arxiv.org/abs/2505.13181)

**Authors**: Zhengrui Ma, Yang Feng, Chenze Shao, Fandong Meng, Jie Zhou, Min Zhang  
**Category**: cs.CL  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2505.13181v2  

We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by...

---

### 20. [Online AUC Optimization Based on Second-order Surrogate Loss](https://arxiv.org/abs/2510.21202)

**Authors**: JunRu Luo, Difei Cheng, Bo Zhang  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.21202v1  

The Area Under the Curve (AUC) is an important performance metric for classification tasks, particularly in class-imbalanced scenarios. However, minimizing the AUC presents significant challenges due to the non-convex and discontinuous nature of pairwise 0/1 losses, which are difficult to optimize, ...

---

### 21. [Amortized Variational Inference for Partial-Label Learning: A Probabilistic Approach to Label Disambiguation](https://arxiv.org/abs/2510.21300)

**Authors**: Tobias Fuchs, Nadja Klein  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.21300v1  

Real-world data is frequently noisy and ambiguous. In crowdsourcing, for example, human annotators may assign conflicting class labels to the same instances. Partial-label learning (PLL) addresses this challenge by training classifiers when each instance is associated with a set of candidate labels,...

---

### 22. [Compositional Monte Carlo Tree Diffusion for Extendable Planning](https://arxiv.org/abs/2510.21361)

**Authors**: Jaesik Yoon, Hyeonseo Cho, Sungjin Ahn  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.21361v1  

Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured tree search to enable effective trajectory exploration through stepwise reasoning. However, MCTD remains fundamentally limited by training trajectory lengths. While periodic replanning allows plan concatenation for longer ...

---

### 23. [A Stable Whitening Optimizer for Efficient Neural Network Training](https://arxiv.org/abs/2506.07254)

**Authors**: Kevin Frans, Sergey Levine, Pieter Abbeel  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2506.07254v3  

In this work, we take an experimentally grounded look at neural network optimization. Building on the Shampoo family of algorithms, we identify and alleviate three key issues, resulting in the proposed SPlus method. First, we find that naive Shampoo is prone to divergence when matrix-inverses are ca...

---

### 24. [RockNet: Distributed Learning on Ultra-Low-Power Devices](https://arxiv.org/abs/2510.13320)

**Authors**: Alexander Gr\"afe, Fabian Mager, Marco Zimmerling, Sebastian Trimpe  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2510.13320v2  

As Machine Learning (ML) becomes integral to Cyber-Physical Systems (CPS), there is growing interest in shifting training from traditional cloud-based to on-device processing (TinyML), for example, due to privacy and latency concerns. However, CPS often comprise ultra-low-power microcontrollers, who...

---

### 25. [STACI: Spatio-Temporal Aleatoric Conformal Inference](https://arxiv.org/abs/2505.21658)

**Authors**: Brandon R. Feng, David Keetae Park, Xihaier Luo, Arantxa Urdangarin, Shinjae Yoo, Brian J. Reich  
**Category**: cs.LG  
**Published**: 2025-10-27  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.21658v2  

Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty quantification for estimation of spatio-temporal fields. Spatio-temporal deep learning models, while scalable, typically assume a simplistic independent covariance matrix for the response, failing to capture the underlying...

---

### 26. [Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning](https://arxiv.org/abs/2510.21398)

**Authors**: Ravindra Aribowo Tarunokusumo, Rafael Fernandes Cunha  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2510.21398v1  

Test-time scaling methods have seen a rapid increase in popularity for its computational efficiency and parameter-independent training to improve reasoning performance on Large Language Models. One such method is called budget forcing, a decoding intervention strategy which allocates extra compute b...

---

### 27. [Incentivizing Consistent, Effective and Scalable Reasoning Capability in Audio LLMs via Reasoning Process Rewards](https://arxiv.org/abs/2510.20867)

**Authors**: Jiajun Fan, Roger Ren, Jingyuan Li, Rahul Pandey, Prashanth Gurunath Shivakumar, Ivan Bulyko, Ankur Gandhe, Ge Liu, Yile Gu  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.20867v1  

The role of reasoning in Audio Large Language Models remains widely underexplored, as introducing a reasoning process often degrades rather than improves performance during inference, a phenomenon we term test-time inverse scaling, where longer reasoning chains yield progressively worse results. We ...

---

### 28. [Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference](https://arxiv.org/abs/2510.20930)

**Authors**: Soham Hans, Stacy Marsella, Sophia Hirschmann, Nikolos Gurney  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.20930v1  

Understanding adversarial behavior in cybersecurity has traditionally relied on high-level intelligence reports and manual interpretation of attack chains. However, real-time defense requires the ability to infer attacker intent and cognitive strategy directly from low-level system telemetry such as...

---

### 29. [DEEDEE: Fast and Scalable Out-of-Distribution Dynamics Detection](https://arxiv.org/abs/2510.21638)

**Authors**: Tala Aljaafari, Varun Kanade, Philip Torr, Christian Schroeder de Witt  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.21638v1  

Deploying reinforcement learning (RL) in safety-critical settings is constrained by brittleness under distribution shift. We study out-of-distribution (OOD) detection for RL time series and introduce DEEDEE, a two-statistic detector that revisits representation-heavy pipelines with a minimal alterna...

---

### 30. [Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay](https://arxiv.org/abs/2506.05316)

**Authors**: Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang  
**Category**: cs.AI  
**Published**: 2025-10-27  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2506.05316v2  

Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
