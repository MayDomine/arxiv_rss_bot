# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-06 12:53:06 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference](https://arxiv.org/abs/2510.02675)

**Authors**: Shubham Negi, Kaushik Roy  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 12.0

arXiv:2510.02675v1 Announce Type: cross 
Abstract: The rapid adoption of Large Language Models (LLMs) has driven a growing demand for efficient inference, particularly in latency-sensitive applications such as chatbots and personalized assistants. Unlike traditional deep neural networks, LLM inferen...

---

### 2. [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)

**Authors**: Peijun Zhu, Ning Yang, Jiayu Wei, Jinghang Wu, Haijun Zhang  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 11.5

arXiv:2510.02345v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address thes...

---

### 3. [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)

**Authors**: Nii Osae Osae Dade, Moinul Hossain Rahat  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 11.5

arXiv:2510.02483v1 Announce Type: cross 
Abstract: Training Large Language Models (LLMs) is plagued by long training times and massive energy consumption, with modern models requiring months of computation and gigawatt-hours of electricity. In light of these challenges,we introduce Litespark, a nove...

---

### 4. [Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training](https://arxiv.org/abs/2510.00183)

**Authors**: Ween Yang, Jason Liu, Suli Wang, Xinyuan Song, Lynn Ai, Eric Yang, Bill Shi  
**Category**: cs.DC  
**Published**: 2025-10-06  
**Score**: 10.5

arXiv:2510.00183v2 Announce Type: replace 
Abstract: The rapid expansion of distributed Artificial Intelligence (AI) workloads beyond centralized data centers creates a demand for new communication substrates. These substrates must operate reliably in heterogeneous and permissionless environments, w...

---

### 5. [To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration](https://arxiv.org/abs/2510.02676)

**Authors**: Zeyu Yang, Tianyi Zhang, Jianwen Xie, Chuan Li, Zhaozhuo Xu, Anshumali Shrivastava  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 10.0

arXiv:2510.02676v1 Announce Type: cross 
Abstract: The scaling of Generative AI (GenAI) models into the hundreds of billions of parameters makes low-precision computation indispensable for efficient deployment. We argue that the fundamental solution lies in developing low-precision floating-point fo...

---

### 6. [CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration](https://arxiv.org/abs/2510.03038)

**Authors**: Tianqi Liu, Kairui Fu, Shengyu Zhang, Wenyan Fan, Zhaocheng Du, Jieming Zhu, Fan Wu, Fei Wu  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 9.0

arXiv:2510.03038v1 Announce Type: cross 
Abstract: With the advancement of mobile device capabilities, deploying reranking models directly on devices has become feasible, enabling real-time contextual recommendations. When migrating models from cloud to devices, resource heterogeneity inevitably nec...

---

### 7. [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)

**Authors**: Vaibhav Singh, Zafir Khalid, Edouard Oyallon, Eugene Belilovsky  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 9.0

arXiv:2507.09029v4 Announce Type: replace-cross 
Abstract: Pre-training large neural networks at scale imposes heavy memory demands on accelerators and often requires costly communication. We introduce Subnetwork Data Parallelism (SDP), a distributed training framework that partitions a model into s...

---

### 8. [Learning to Parallel: Accelerating Diffusion Large Language Models via Learnable Parallel Decoding](https://arxiv.org/abs/2509.25188)

**Authors**: Wenrui Bao, Zhiben Chen, Dan Xu, Yuzhang Shang  
**Category**: cs.CL  
**Published**: 2025-10-06  
**Score**: 9.0

arXiv:2509.25188v2 Announce Type: replace 
Abstract: Autoregressive decoding in large language models (LLMs) requires $\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting inference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token generation through iterative ...

---

### 9. [ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models](https://arxiv.org/abs/2510.02613)

**Authors**: Gursimran Singh (Huawei Technologies Canada), Timothy Yu (Huawei Technologies Canada), Haley Li (Huawei Technologies Canada), Cheng Chen (Huawei Technologies Canada), Hanieh Sadri (Huawei Technologies Canada), Qintao Zhang (Huawei Technologies China), Yu Zhang (Huawei Technologies China), Ying Xiong (Huawei Technologies Canada), Yong Zhang (Huawei Technologies Canada), Zhenan Fan (Huawei Technologies Canada)  
**Category**: cs.DC  
**Published**: 2025-10-06  
**Score**: 9.0

arXiv:2510.02613v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models promise efficient scaling of large language models (LLMs) by activating only a small subset of experts per token, but their parallelized inference pipelines make elastic serving challenging. Existing strategies fall sho...

---

### 10. [Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization](https://arxiv.org/abs/2509.24932)

**Authors**: Fardis Nadimi, Payam Abdisarabshali, Jacob Chakareski, Nicholas Mastronarde, Seyyedali Hosseinalipour  
**Category**: cs.DC  
**Published**: 2025-10-06  
**Score**: 9.0

arXiv:2509.24932v2 Announce Type: replace 
Abstract: We introduce Fed-Span, a novel federated/distributed learning framework designed for low Earth orbit satellite constellations. Fed-Span aims to address critical challenges inherent to distributed learning in dynamic satellite networks, including i...

---

### 11. [FTTE: Federated Learning on Resource-Constrained Devices](https://arxiv.org/abs/2510.03165)

**Authors**: Irene Tenison, Anna Murphy, Charles Beauville, Lalana Kagal  
**Category**: cs.LG  
**Published**: 2025-10-06  
**Score**: 9.0

arXiv:2510.03165v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy, but deployment on resource-constrained edge nodes remains challenging due to limited memory, energy, and communication bandwidth. Tr...

---

### 12. [Highly Efficient and Effective LLMs with Multi-Boolean Architectures](https://arxiv.org/abs/2505.22811)

**Authors**: Ba-Hien Tran, Van Minh Nguyen  
**Category**: cs.LG  
**Published**: 2025-10-06  
**Score**: 9.0

arXiv:2505.22811v2 Announce Type: replace-cross 
Abstract: Weight binarization has emerged as a promising strategy to reduce the complexity of large language models (LLMs). Existing approaches fall into post-training binarization, which is simple but causes severe performance loss, and training-awar...

---

### 13. [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)

**Authors**: Haojie Ouyang, Jianwei Lv, Lei Ren, Chen Wei, Xiaojie Wang, Fangxiang Feng  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 8.5

arXiv:2510.02361v1 Announce Type: cross 
Abstract: Transformer-based large models excel in natural language processing and computer vision, but face severe computational inefficiencies due to the self-attention's quadratic complexity with input tokens. Recently, researchers have proposed a series of...

---

### 14. [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)

**Authors**: Makram Chahine, Philipp Nazari, Daniela Rus, T. Konstantin Rusch  
**Category**: cs.LG  
**Published**: 2025-10-06  
**Score**: 8.5

arXiv:2510.02823v1 Announce Type: new 
Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks efficiently, offer both parallelizable training and fast inference. At their core are recurrent dynamical systems that maintain a hidden state, with update costs scaling with ...

---

### 15. [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)

**Authors**: Guanghao Li, Zhihui Fu, Min Fang, Qibin Zhao, Ming Tang, Chun Yuan, Jun Wang  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 8.0

arXiv:2510.02358v1 Announce Type: cross 
Abstract: As large language models (LLMs) scale up, accuracy improves, but the autoregressive (AR) nature of decoding increases latency since each token requires a serial forward pass. Speculative decoding addresses this by employing a fast drafter to propose...

---

### 16. [A Study of Neural Polar Decoders for Communication](https://arxiv.org/abs/2510.03069)

**Authors**: Rom Hirsch, Ziv Aharoni, Henry D. Pfister, Haim H. Permuter  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 8.0

arXiv:2510.03069v1 Announce Type: cross 
Abstract: In this paper, we adapt and analyze Neural Polar Decoders (NPDs) for end-to-end communication systems. While prior work demonstrated the effectiveness of NPDs on synthetic channels, this study extends the NPD to real-world communication systems. The...

---

### 17. [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)

**Authors**: Hao Zhang, Zhenjia Li, Runfeng Bao, Yifan Gao, Xi Xiao, Bo Huang, Yuhang Wu, Tianyang Wang, Hao Xu  
**Category**: cs.CL  
**Published**: 2025-10-06  
**Score**: 8.0

arXiv:2510.02630v1 Announce Type: cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), has emerged as a promising approach to fine-tuning large language models(LLMs) while reducing computational and memory overhead. However, LoRA assumes a uniform rank \tex...

---

### 18. [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)

**Authors**: Kanghoon Yoon, Minsub Kim, Sungjae Lee, Joonhyung Lee, Sunghyeon Woo, Yeonjun In, Se Jung Kwon, Chanyoung Park, Dongsoo Lee  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2510.02329v1 Announce Type: cross 
Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens from a draft model against a larger target model. Recent judge decoding boosts this process by relaxing verification criteria by accepting draft tokens that may exhibit min...

---

### 19. [A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks](https://arxiv.org/abs/2510.02711)

**Authors**: Tarun Kumar Biswas, Ashrafun Zannat, Waqas Ishtiaq, Md. Alamgir Hossain  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2510.02711v1 Announce Type: cross 
Abstract: The growing integration of drones across commercial, industrial, and civilian domains has introduced significant cybersecurity challenges, particularly due to the susceptibility of drone networks to a wide range of cyberattacks. Existing intrusion d...

---

### 20. [Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs](https://arxiv.org/abs/2502.14837)

**Authors**: Tao Ji, Bin Guo, Yuanbin Wu, Qipeng Guo, Lixing Shen, Zhan Chen, Xipeng Qiu, Qi Zhang, Tao Gui  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2502.14837v2 Announce Type: replace-cross 
Abstract: Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector. Compared to MLA, standard...

---

### 21. [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116)

**Authors**: Juyi Lin, Amir Taherin, Arash Akbari, Arman Akbari, Lei Lu, Guangyu Chen, Taskin Padir, Xiaomeng Yang, Weiwei Chen, Yiqian Li, Xue Lin, David Kaeli, Pu Zhao, Yanzhi Wang  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2507.05116v4 Announce Type: replace-cross 
Abstract: Recent large-scale Vision Language Action (VLA) models have shown superior performance in robotic manipulation tasks guided by natural language. However, current VLA models suffer from two drawbacks: (i) generation of massive tokens leading ...

---

### 22. [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)

**Authors**: Wen Huang, Jiarui Yang, Tao Dai, Jiawei Li, Shaoxiong Zhan, Bin Wang, Shu-Tao Xia  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2508.09459v2 Announce Type: replace-cross 
Abstract: Visual manipulation localization (VML) aims to identify tampered regions in images and videos, a task that has become increasingly challenging with the rise of advanced editing tools. Existing methods face two main issues: resolution diversi...

---

### 23. [KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting](https://arxiv.org/abs/2510.02084)

**Authors**: Kuiye Ding, Fanda Fan, Zheya Wang, Hongxiao Li, Yifan Wang, Lei Wang, Chunjie Luo, Jianfeng Zhan  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2510.02084v2 Announce Type: replace-cross 
Abstract: In the World Wide Web, reliable time series forecasts provide the forward-looking signals that drive resource planning, cache placement, and anomaly response, enabling platforms to operate efficiently as user behavior and content distributio...

---

### 24. [ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2502.00299)

**Authors**: Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Yue Liu, Bo Li, Xuming Hu, Xiaowen Chu  
**Category**: cs.CL  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2502.00299v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) require significant GPU memory when processing long texts, with the key value (KV) cache consuming up to 70\% of total memory during inference. Although existing compression methods reduce memory by evaluating the impo...

---

### 25. [On the energy efficiency of sparse matrix computations on multi-GPU clusters](https://arxiv.org/abs/2510.02878)

**Authors**: Massimo Bernaschi, Alessandro Celestini, Pasqua D'Ambra, Giorgio Richelli  
**Category**: cs.DC  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2510.02878v1 Announce Type: new 
Abstract: We investigate the energy efficiency of a library designed for parallel computations with sparse matrices. The library leverages high-performance, energy-efficient Graphics Processing Unit (GPU) accelerators to enable large-scale scientific applicatio...

---

### 26. [Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions](https://arxiv.org/abs/2510.02882)

**Authors**: Adhitya Bhawiyuga, Serkan Girgin, Rolf A. de By, Raul Zurita-Milla  
**Category**: cs.DC  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2510.02882v1 Announce Type: new 
Abstract: Earth observation (EO) data volumes are rapidly increasing. While cloud computing are now used for processing large EO datasets, the energy efficiency aspects of such a processing have received much less attention. This issue is notable given the incr...

---

### 27. [Action Deviation-Aware Inference for Low-Latency Wireless Robots](https://arxiv.org/abs/2510.02851)

**Authors**: Jeyoung Park, Yeonsub Lim, Seungeun Oh, Jihong Park, Jinho Choi, Seong-Lyun Kim  
**Category**: cs.DC  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2510.02851v1 Announce Type: cross 
Abstract: To support latency-sensitive AI applications ranging from autonomous driving to industrial robot manipulation, 6G envisions distributed ML, connecting distributed computational resources in edge and cloud over hyper-reliable low-latency communicatio...

---

### 28. [An Encoder-Decoder Network for Beamforming over Sparse Large-Scale MIMO Channels](https://arxiv.org/abs/2510.02355)

**Authors**: Yubo Zhang, Jeremy Johnston, Xiaodong Wang  
**Category**: cs.LG  
**Published**: 2025-10-06  
**Score**: 7.5

arXiv:2510.02355v1 Announce Type: cross 
Abstract: We develop an end-to-end deep learning framework for downlink beamforming in large-scale sparse MIMO channels. The core is a deep EDN architecture with three modules: (i) an encoder NN, deployed at each user end, that compresses estimated downlink c...

---

### 29. [Dissecting Transformers: A CLEAR Perspective towards Green AI](https://arxiv.org/abs/2510.02810)

**Authors**: Hemang Jain, Shailender Goyal, Divyansh Pandey, Karthik Vaidhyanathan  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 7.0

arXiv:2510.02810v1 Announce Type: cross 
Abstract: The rapid adoption of Large Language Models (LLMs) has raised significant environmental concerns. Unlike the one-time cost of training, LLM inference occurs continuously at a global scale and now dominates the AI energy footprint. Yet, most sustaina...

---

### 30. [Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language](https://arxiv.org/abs/2508.01918)

**Authors**: Jaskaranjeet Singh, Rakesh Thakur  
**Category**: cs.AI  
**Published**: 2025-10-06  
**Score**: 7.0

arXiv:2508.01918v2 Announce Type: replace-cross 
Abstract: Despite rapid advances in large language models (LLMs), low-resource languages remain excluded from NLP, limiting digital access for millions. We present PunGPT2, the first fully open-source Punjabi generative model suite, trained on a 35GB ...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
