# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-01-27 05:59:48 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2601.16991)

**Authors**: Longteng Zhang, Sen Wu, Shuai Hou, Zhengyu Qing, Zhuo Zheng, Danning Ke, Qihong Lin, Qiang Wang, Shaohuai Shi, Xiaowen Chu  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2601.16991v1  

#### Abstract
Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight update...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**SALR: Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜æ•ˆå¾®è°ƒä¸­ï¼Œä¸»æµæ–¹æ³•å¦‚ **LoRA** è™½ç„¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œä½†ä»ä¾èµ–äº**å¯†é›†æƒé‡æ›´æ–°**ï¼Œå¯¼è‡´å­˜å‚¨å’Œè®¡ç®—å¼€é”€é«˜ã€‚è€Œç›´æ¥å¯¹ LoRA æ¨¡å‹è¿›è¡Œå‰ªæï¼ˆpruningï¼‰è™½ç„¶èƒ½å¼•å…¥ç¨€ç–æ€§ä»¥å‹ç¼©æ¨¡å‹ï¼Œä½†é€šå¸¸ä¼šä¸¥é‡æŸå®³æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å½“å‰ªæç­–ç•¥ç ´åäº†ä½ç§©å­ç©ºé—´æ—¶ã€‚

æ­¤å¤–ï¼Œè®¸å¤šå£°ç§°â€œå®ç°ç¨€ç–â€çš„æ–¹æ³•å¹¶æœªçœŸæ­£å®ç°**æ¨¡å‹å¤§å°å‹ç¼©**æˆ–**æ¨ç†åŠ é€Ÿ**ï¼Œå› ä¸ºç¼ºä¹é«˜æ•ˆçš„ç¨€ç–æ ¼å¼æ”¯æŒå’Œç¡¬ä»¶ä¼˜åŒ–ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šSALR

æœ¬æ–‡æå‡º **SALR (Sparsity-Aware Low-Rank Representation)**ï¼Œä¸€ç§å°†**ç¨€ç–å‰ªæ**ä¸**ä½ç§©é€‚åº”**ç»Ÿä¸€çš„æ–°å‹å¾®è°ƒèŒƒå¼ï¼Œå…¶æ ¸å¿ƒæ€æƒ³åŒ…æ‹¬ï¼š

1. **é™æ€å‰ªæå†»ç»“çš„åŸºæƒé‡ $W_0$**  
   ç†è®ºè¯æ˜ï¼šä»…å¯¹é¢„è®­ç»ƒçš„å†»ç»“æƒé‡ $W_0$ åº”ç”¨é™æ€æ©ç ï¼ˆstatic maskï¼‰å¯æœ€å°åŒ–å‰ªæè¯¯å·®ä¸Šç•Œï¼ˆMSE boundï¼‰ï¼Œä¼˜äºåŠ¨æ€å‰ªæç­–ç•¥ã€‚

2. **æ®‹å·®ä¿¡æ¯ä¿ç•™æœºåˆ¶ï¼ˆSparsity Preservation Pruningï¼‰**  
   å°†è¢«å‰ªæçš„æƒé‡æ®‹å·® $E = W_0 - \tilde{W}_0$ é€šè¿‡æˆªæ–­ SVD åˆ†è§£ä¸ºä¸€ä¸ªä½ç§©é€‚é…å™¨ï¼ˆlow-rank adapterï¼‰ï¼Œä»è€Œæ¢å¤ä¸¢å¤±çš„ä¿¡æ¯ã€‚è¯¥æ“ä½œç†è®ºä¸Šå¯å°†æ¯é¡¹ MSE å‡å°‘ $(1 - r/\min(d,k))$ å€ã€‚

3. **å¤šé€‚é…å™¨èåˆï¼ˆAdapter Concatenationï¼‰**  
   å°†åŸå§‹ LoRA é€‚é…å™¨ä¸å¤šä¸ªæ®‹å·®é€‚é…å™¨æ²¿ç§©ç»´åº¦æ‹¼æ¥ï¼Œåˆå¹¶ä¸ºå•ä¸ª GEMM æ“ä½œï¼Œæ˜¾è‘—é™ä½å†…æ ¸å¯åŠ¨å¼€é”€å¹¶æå‡ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚

4. **çœŸæ­£çš„æ¨¡å‹å‹ç¼©ä¸æ¨ç†åŠ é€Ÿè®¾è®¡**  
   - ä½¿ç”¨ **bitmap ç¼–ç ** å­˜å‚¨ç¨€ç–æƒé‡ï¼Œå®ç°ç´§å‡‘è¡¨ç¤ºï¼›
   - è®¾è®¡ **ä¸¤é˜¶æ®µæµæ°´çº¿ï¼ˆdecoding + GEMMï¼‰**ï¼šç¬¬ä¸€é˜¶æ®µè§£ç ç¨€ç–å­çŸ©é˜µï¼Œç¬¬äºŒé˜¶æ®µç”± Tensor Core æ‰§è¡Œå¯†é›†ä¹˜æ³•ï¼Œä¿æŒè®¡ç®—å¯†åº¦ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | æ€§èƒ½ | æ¨¡å‹ç¨€ç–æ€§ | æ¨ç†åŠ é€Ÿ |
|------|------|------------|----------|
| LoSA (Huang et al., 2025) | ä½ | ç¨€ç– | æ˜¯ |
| SparseLoRA (Khaki et al., 2025) | é«˜ | å¯†é›† | å¦ |
| **SALR (Ours)** | **é«˜** | **ç¨€ç–** | **æ˜¯** |

SALR æˆåŠŸå®ç°äº†ä¸‰è€…å…¼é¡¾ï¼š**é«˜æ€§èƒ½ã€çœŸå®ç¨€ç–ã€å®é™…åŠ é€Ÿ**ï¼Œçªç ´äº†ä»¥å¾€æ–¹æ³•åªèƒ½å–å…¶äºŒçš„ç“¶é¢ˆã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

- **MetaMath**ï¼šç”¨äºæ•°å­¦é¢†åŸŸç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œé€‚é… MATH ç±»ä»»åŠ¡ã€‚
- **ARC, MC-TEST, OBQA, RACE ç­‰å¤šé€‰é¢˜æ•°æ®é›†**ï¼šç”¨äºè·¨å­¦ç§‘çŸ¥è¯†è¯„ä¼°ï¼ˆMMLUï¼‰ã€‚
- å¾®è°ƒååœ¨ä»¥ä¸‹åŸºå‡†æµ‹è¯•é›¶æ ·æœ¬æ€§èƒ½ï¼š
  - **GSM8K**ï¼šå°å­¦æ•°å­¦åº”ç”¨é¢˜ï¼Œè¯„ä¼°æ¨ç†èƒ½åŠ›ã€‚
  - **MMLU**ï¼šæ¶µç›–57ä¸ªå­¦ç§‘çš„å¤šä»»åŠ¡ç†è§£èƒ½åŠ›ã€‚

### âš™ï¸ å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

| é¡¹ç›® | è®¾ç½® |
|------|------|
| æ¨¡å‹ | Llama2-7B, Llama3-8B, Mixtral-8x7B, DeepSeek-V2-Lite |
| å‰ªæç‡ | å…¨å±€ 50% ç¨€ç–åº¦ï¼ˆé™¤éç‰¹åˆ«è¯´æ˜ï¼‰ |
| LoRA ç§© | $r=64$ |
| è¯„ä¼°æ–¹å¼ | Zero-shot accuracyï¼ˆGSM8Kï¼‰ã€5-shot accuracyï¼ˆMMLUï¼‰ |
| ç¡¬ä»¶å¹³å° | 1x RTX 4090ï¼ˆæ¨ç†é€Ÿåº¦æµ‹è¯•ï¼‰ |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

- **Pretrained**ï¼šæœªå¾®è°ƒçš„åŸå§‹æ¨¡å‹
- **LoRA**ï¼šæ ‡å‡†ä½ç§©å¾®è°ƒï¼Œä½œä¸ºæ€§èƒ½ä¸Šé™
- **LoSA**ï¼šåŠ¨æ€ä½ç§©ç¨€ç–é€‚é…ï¼Œæ”¯æŒç¨€ç–ä½†æ€§èƒ½ä¸‹é™æ˜æ˜¾
- **SparseLoRA**ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¨€ç–ï¼Œè®­ç»ƒå¿«ä½†éƒ¨ç½²ä»ä¸ºå¯†é›†æ¨¡å‹
- **DeepSparse**ï¼šæ·±åº¦ç¨€ç–å¾®è°ƒæ–¹æ³•ï¼Œæ€§èƒ½æŸå¤±å¤§

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 2ï¼‰

| æ–¹æ³• | Llama2-7B (MMLU/GSM8K) | Llama3-8B (MMLU/GSM8K) | Mixtral-8x7B (MMLU/GSM8K) |
|------|------------------------|-------------------------|----------------------------|
| LoRA | 56.0 / 56.8 | 69.2 / 79.5 | 71.0 / 79.2 |
| LoSA | 45.0 / 34.2 | 64.4 / 71.4 | 69.2 / 57.9 |
| SparseLoRA | 56.0 / 37.6 | 69.0 / 72.0 | 70.9 / 78.0 |
| **SALR (Ours)** | **56.0 / 56.7** | **68.2 / 79.5** | **71.4 / 79.1** |

âœ… **ç»“è®º**ï¼šSALR åœ¨ **50% ç¨€ç–åº¦ä¸‹å‡ ä¹å®Œå…¨åŒ¹é… LoRA çš„æ€§èƒ½**ï¼Œè¿œè¶…å…¶ä»–å‰ªææ–¹æ³•ã€‚

### â±ï¸ æ¨ç†é€Ÿåº¦ä¸ç³»ç»Ÿæ•ˆç‡ï¼ˆTable 4ï¼‰

| æ–¹æ³•ï¼ˆSparsityï¼‰ | GSM8K å‡†ç¡®ç‡ | æ¨ç†ååï¼ˆtokens/sï¼‰ | åŠ é€Ÿæ¯” |
|------------------|---------------|------------------------|--------|
| LoRA (N/A) | 79.5 | 60.1 | 1.0x |
| LoSA (2:4) | 69.4 | 113.5 | 1.9x |
| **SALR (2:4)** | **78.9** | **104.9** | **1.7x** |

ğŸ“Œ SALR å®ç°äº†é«˜è¾¾ **1.7Ã— æ¨ç†åŠ é€Ÿ**ï¼ŒåŒæ—¶å‡†ç¡®ç‡ä»…è½»å¾®ä¸‹é™ï¼ˆ79.5 â†’ 78.9ï¼‰ï¼Œæ˜¾è‘—ä¼˜äº LoSAï¼ˆä¸‹é™è‡³ 69.4ï¼‰ã€‚

### ğŸ’¾ æ¨¡å‹å‹ç¼©æ•ˆæœï¼ˆFigure 1 & Table 3ï¼‰

- å¯¹ Llama3-8Bï¼š
  - åŸå§‹ LoRA æ¨¡å‹å¤§å°ï¼š15.5 GB
  - SALR å‹ç¼©åï¼š**7.98 GB**ï¼ˆæ¥è¿‘ **2Ã— å‹ç¼©**ï¼‰
- Fine-tuning å†…å­˜ä» LoSA çš„ 27.1 GB é™è‡³ **19.2 GB**ï¼ŒTFLOPS æå‡è‡³ **89.2**ï¼ˆvs. LoSA çš„ 74.5ï¼‰

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰æ˜¯å¦æ›´æ–°æ®‹å·®é€‚é…å™¨ï¼ˆTable 5ï¼‰

| æ–¹æ³• | Llama2-7B (MMLU) | Llama3-8B (MMLU) |
|------|------------------|------------------|
| LoRA | 56.0 | 69.2 |
| SALRï¼ˆæ®‹å·®å†»ç»“ï¼‰ | 54.2 | 66.8 |
| **SALRï¼ˆæ®‹å·®å¯è®­ç»ƒï¼‰** | **56.0** | **68.2** |

â¡ï¸ **ç»“è®º**ï¼šå…è®¸æ®‹å·®é€‚é…å™¨å‚ä¸å¾®è°ƒè‡³å…³é‡è¦ï¼Œå¦åˆ™æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒç¨€ç–åº¦çš„å½±å“ï¼ˆTable 7ï¼‰

| ç¨€ç–åº¦ | GSM8K å‡†ç¡®ç‡ |
|--------|--------------|
| 10% | 79.5 |
| 30% | **80.1** âœ… |
| 50% | 79.5 |

â¡ï¸ **å‘ç°**ï¼šé€‚åº¦ç¨€ç–ï¼ˆå¦‚ 30%ï¼‰ç”šè‡³å¯èƒ½èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨ï¼Œç•¥å¾®æå‡æ€§èƒ½ã€‚

#### ï¼ˆ3ï¼‰ç»“åˆé‡åŒ–ï¼ˆTable 6ï¼‰â€”â€”Quantized SALR (QSALR)

| æ–¹æ³• | DeepSeek-V2-Lite (Size â†“) | Mixtral-8x7B (Size â†“) | æ€§èƒ½å½±å“ |
|------|----------------------------|------------------------|----------|
| LoRA | 31.8 GB | 93.9 GB | â€” |
| **QSALR (20% sparsity + NF4)** | **6.5 GB** (~5Ã—) | **19.2 GB** (~5Ã—) | -0.6 æˆ–æ— æŸ |

â¡ï¸ å¯è¿›ä¸€æ­¥ä¸ **NF4 é‡åŒ–** ç»“åˆï¼Œåœ¨æå°å†…å­˜ä¸‹éƒ¨ç½²è¶…å¤§è§„æ¨¡æ¨¡å‹ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **ç†è®ºæŒ‡å¯¼å®è·µ**ï¼šé™æ€å‰ªæå†»ç»“æƒé‡ $W_0$ æ˜¯æœ€ä¼˜é€‰æ‹©ï¼Œèƒ½æœ€å°åŒ– MSE ä¸Šç•Œã€‚
2. **æ®‹å·®ä½ç§©è¡¥å¿æœ‰æ•ˆ**ï¼šé€šè¿‡ SVD æå–å¹¶å¾®è°ƒå‰ªææ®‹å·®ï¼Œå¯æ˜¾è‘—æ¢å¤æ€§èƒ½æŸå¤±ã€‚
3. **ç¡¬ä»¶æ„è¯†è®¾è®¡å¿…è¦**ï¼šadapter concatenation å’Œ bitmap æµæ°´çº¿æ˜¯å®ç°**çœŸå®å‹ç¼©ä¸åŠ é€Ÿ**çš„å…³é”®ã€‚
4. **SALR å®ç°å¸•ç´¯æ‰˜å‰æ²¿çªç ´**ï¼šé¦–æ¬¡åœ¨â€œæ€§èƒ½-ç¨€ç–-é€Ÿåº¦â€ä¸‰è§’ä¸­è¾¾æˆå…¨é¢ä¼˜åŠ¿ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

- å½“å‰ SALR ä¸»è¦é’ˆå¯¹ **magnitude-based pruning**ï¼Œå¯¹ç»“æ„åŒ–æˆ–éå‡åŒ€ç¨€ç–çš„æ”¯æŒæœ‰å¾…æ‰©å±•ã€‚
- å¤šé€‚é…å™¨èåˆè™½æå‡æ•ˆç‡ï¼Œä½†åœ¨æç«¯ç¨€ç–åœºæ™¯ä¸‹å¯èƒ½å¢åŠ ç®¡ç†å¤æ‚åº¦ã€‚
- å½“å‰ bitmap è§£ç ä¾èµ–ç‰¹å®šç¡¬ä»¶ä¼˜åŒ–ï¼ˆå¦‚ Tensor Coreï¼‰ï¼Œé€šç”¨æ€§éœ€è¿›ä¸€æ­¥éªŒè¯ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

- å°† SALR æ‰©å±•åˆ° **MoE æ¶æ„** å’Œ **é•¿åºåˆ—å»ºæ¨¡** åœºæ™¯ã€‚
- æ¢ç´¢ **è‡ªåŠ¨åŒ–ç¨€ç–åº¦åˆ†é…**ï¼ˆper-layer æˆ– per-headï¼‰ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚
- ç»“åˆ **åŠ¨æ€ç¨€ç–æ¿€æ´»** æœºåˆ¶ï¼Œåœ¨è¾“å…¥å±‚é¢å®ç°æ¡ä»¶ç¨€ç–æ‰§è¡Œã€‚
- æ¨åŠ¨ SALR åœ¨ç§»åŠ¨ç«¯ã€è¾¹ç¼˜è®¾å¤‡ç­‰èµ„æºæåº¦å—é™ç¯å¢ƒä¸­çš„è½åœ°ã€‚

---

## æ€»ç»“

> **SALR æ˜¯é¦–ä¸ªå°†ç¨€ç–å‰ªæä¸ä½ç§©å¾®è°ƒä»ç†è®ºåˆ°ç³»ç»Ÿå®Œæ•´ç»Ÿä¸€çš„æ–¹æ³•ï¼Œåœ¨ä¸ç‰ºç‰² LoRA æ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾ 50% ç¨€ç–åº¦ã€2Ã— æ¨¡å‹å‹ç¼©å’Œ 1.7Ã— æ¨ç†åŠ é€Ÿã€‚å®ƒä¸ä»…æä¾›äº†æ–°çš„ç†è®ºè§†è§’ï¼Œæ›´é€šè¿‡ç¡¬ä»¶å‹å¥½çš„è®¾è®¡æ¨åŠ¨äº†ç¨€ç–åŒ–åœ¨ LLM éƒ¨ç½²ä¸­çš„å®ç”¨åŒ–è¿›ç¨‹ã€‚**

</details>

---

### 2. [JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers](https://arxiv.org/abs/2601.17215)

**Authors**: Ruoqing Zheng, Chang Sun, Qibin Liu, Lauri Laatu, Arianna Cox, Benedikt Maier, Alexander Tapper, Jose G. F. Coutinho, Wayne Luk, Zhiqiang Que  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2601.17215v1  

#### Abstract
We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of j...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šJetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
åœ¨é«˜èƒ½ç‰©ç†ï¼ˆHEPï¼‰é¢†åŸŸï¼Œ**jet tagging** æ˜¯è¯†åˆ«ç”±ç‰¹å®šç²’å­ï¼ˆå¦‚ Higgsã€top quarkï¼‰å¼•å‘çš„å–·æ³¨çš„å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é«˜æ€§èƒ½æ¨¡å‹ï¼ˆå¦‚ ParTï¼‰è™½ç„¶åœ¨ç¦»çº¿åˆ†æä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç”±äºå…¶**è®¡ç®—å¤æ‚åº¦é«˜ã€å‚æ•°é‡å¤§**ï¼Œéš¾ä»¥éƒ¨ç½²åˆ°å¯¹å»¶è¿Ÿè¦æ±‚æé«˜çš„åœ¨çº¿è§¦å‘ç³»ç»Ÿï¼ˆå¦‚ CMS Level-1 Trigger, L1Tï¼‰ï¼Œå°¤å…¶æ˜¯åŸºäº FPGA çš„ç¡¬ä»¶å¹³å°ã€‚

æ­¤å¤–ï¼Œå½“å‰ä¸»æµå·¥å…·é“¾ï¼ˆå¦‚ hls4mlï¼‰å¯¹ Transformer æ¶æ„æ”¯æŒæœ‰é™ï¼Œè¿›ä¸€æ­¥é˜»ç¢äº†å…ˆè¿›æ¨¡å‹çš„å®æ—¶éƒ¨ç½²ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
æœ¬æ–‡æå‡ºäº† **JetFormer** â€”â€” ä¸€ç§**å¯æ‰©å±•ä¸”é«˜æ•ˆçš„ encoder-only Transformer æ¶æ„**ï¼Œä¸“ä¸ºä»ç¦»çº¿åˆ†æåˆ° FPGA è§¦å‘å™¨çš„å…¨åœºæ™¯ jet tagging è®¾è®¡ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **ç»Ÿä¸€æ¶æ„è®¾è®¡**ï¼šåŒä¸€å¥—æ¨¡å‹æ¡†æ¶æ—¢å¯ç”¨äºé«˜ç²¾åº¦ç¦»çº¿åˆ†æï¼Œä¹Ÿå¯é€šè¿‡å‹ç¼©é€‚é…è¶…ä½å»¶è¿Ÿï¼ˆsub-microsecondï¼‰çš„ FPGA è§¦å‘ç³»ç»Ÿã€‚
- **ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–æµç¨‹**ï¼šæå‡ºäº†ä¸€å¥—å®Œæ•´çš„ç¡¬ä»¶éƒ¨ç½²æµæ°´çº¿ï¼Œç»“åˆ **multi-objective Hyperparameter Optimization (HPO)**ã€**structured pruning** å’Œ **1-bit quantization**ï¼Œå®ç°æ¨¡å‹å°å‹åŒ–ä¸é«˜æ•ˆæ¨ç†ã€‚
- **Allo æ¡†æ¶æ‰©å±•**ï¼šé’ˆå¯¹ç°æœ‰ HLS å·¥å…·é“¾ä¸æ”¯æŒ Transformer çš„é—®é¢˜ï¼Œä½œè€…æ‰©å±•äº† **Allo** ç¼–è¯‘æ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿç«¯åˆ°ç«¯åœ°å°† JetFormer è½¬æ¢ä¸ºå¯åœ¨ FPGA ä¸Šè¿è¡Œçš„ HLS C++ ä»£ç ã€‚
- **ç¡¬ä»¶å‹å¥½å‹ç»“æ„æ”¹è¿›**ï¼š
  - ç”¨ **BatchNorm æ›¿ä»£ LayerNorm**ï¼Œä¾¿äºæ¨ç†æ—¶å‚æ•°å›ºåŒ–ï¼›
  - ç”¨ **ReLU æ›¿ä»£ SiLU**ï¼Œé¿å…æŒ‡æ•°è¿ç®—ï¼Œæå‡ç¡¬ä»¶æ‰§è¡Œæ•ˆç‡ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ€§èƒ½ vs æ•ˆç‡** | åœ¨ JETCLASS æ•°æ®é›†ä¸Šï¼Œå‡†ç¡®ç‡æ¥è¿‘ SOTA çš„ ParTï¼ˆä»…å·® 0.7%ï¼‰ï¼Œä½† **FLOPs å‡å°‘ 37.4%**ï¼Œæ˜¾è‘—æ›´é«˜æ•ˆã€‚ |
| **é€šç”¨æ€§** | æ”¯æŒä»å°è§„æ¨¡ï¼ˆJetFormer-tinyï¼‰åˆ°å¤§è§„æ¨¡æ¨¡å‹çš„æ— ç¼ç¼©æ”¾ï¼Œé€‚ç”¨äºä¸åŒå»¶è¿Ÿä¸ç²¾åº¦éœ€æ±‚ã€‚ |
| **å¯éƒ¨ç½²æ€§** | æˆåŠŸåœ¨ Allo ä¸­å®ç°ç«¯åˆ°ç«¯ FPGA åˆæˆï¼ŒéªŒè¯äº† Transformer åœ¨ L1T ç³»ç»Ÿä¸­çš„å¯è¡Œæ€§ã€‚ |
| **å‹ç¼©èƒ½åŠ›** | å¯é€šè¿‡ç»“æ„åŒ–å‰ªæå’Œ 1-bit é‡åŒ–å¤§å¹…å‹ç¼©æ¨¡å‹ï¼Œä»…é€ æˆè½»å¾®ç²¾åº¦æŸå¤±ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
| æ•°æ®é›† | æè¿° |
|-------|------|
| **HLS4ML 150P Dataset** | åŒ…å« 620K è®­ç»ƒæ ·æœ¬ï¼Œæ¯ jet æœ€å¤š 150 ä¸ªç²’å­ï¼Œæ¯ä¸ªç²’å­æœ‰ 16 ä¸ªè¿åŠ¨å­¦ç‰¹å¾ï¼›ç±»åˆ«å¹³è¡¡ï¼ˆq, g, W, Z, tï¼‰ã€‚ç”¨äºå°è§„æ¨¡ benchmark æµ‹è¯•ã€‚ |
| **JETCLASS Dataset** | å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…± 100M jetsï¼Œæ¶µç›– 10 ç±»ç²’å­ï¼Œæ¯ç²’å­ 17 ä¸ªç‰¹å¾ï¼ˆå« PID å’Œè½¨è¿¹ä½ç§»ç­‰ï¼‰ã€‚æ˜¯ç›®å‰æœ€å¤æ‚çš„ jet tagging benchmarkã€‚æœ¬æ–‡ä½¿ç”¨å…¶ä¸­ 2M å­é›†è¿›è¡Œè®­ç»ƒä¸è¯„ä¼°ã€‚ |

> âš ï¸ æ³¨æ„ï¼šJetFormer **æœªä½¿ç”¨ pairwise interaction features**ï¼Œè€Œ ParT ä½¿ç”¨äº†è¿™äº›é¢å¤–ç‰¹å¾ï¼Œå› æ­¤ JetFormer æ˜¯åœ¨â€œä¿¡æ¯æ›´å°‘â€çš„è¾“å…¥ä¸‹è¾¾åˆ°ç›¸è¿‘æ€§èƒ½ã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
| é¡¹ç›® | è®¾ç½® |
|-----|------|
| **è®­ç»ƒé…ç½®** | ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨ï¼Œåˆå§‹å­¦ä¹ ç‡ 0.001ï¼›é‡‡ç”¨ OneCycleLR å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ•ˆæœæœ€ä¼˜ï¼‰ï¼›batch size åˆ†åˆ«ä¸º 256ï¼ˆHLS4MLï¼‰å’Œ 128ï¼ˆJETCLASSï¼‰ã€‚ |
| **HPO æ–¹æ³•** | ä½¿ç”¨ Optuna è¿›è¡Œå¤šç›®æ ‡è¶…å‚æœç´¢ï¼Œç›®æ ‡ä¸ºæœ€å¤§åŒ– accuracy å¹¶æœ€å°åŒ– FLOPsï¼›æ¯”è¾ƒä¸‰ç§ samplerï¼ˆNSGAIISamplerã€TPESamplerã€BoTorchSamplerï¼‰ã€‚ |
| **å‹ç¼©ç­–ç•¥** |
| - Pruning | ç»“æ„åŒ–å‰ªæï¼ˆtorch-pruningï¼‰ï¼Œå…¨å±€å‰ªææ¯”ä¾‹ 50%ï¼Œåˆ†æ­¥å¾®è°ƒæ¢å¤ç²¾åº¦ã€‚ |
| - Quantization | é‡‡ç”¨ BitNet æ¡†æ¶è¿›è¡Œ **1-bit æƒé‡é‡åŒ– + 8-bit æ¿€æ´»é‡åŒ–**ï¼Œè®­ç»ƒé˜¶æ®µä½¿ç”¨ QATï¼ˆQuantization-Aware Trainingï¼‰ã€‚ |
| **ç¡¬ä»¶éƒ¨ç½²** | ä½¿ç”¨ **Allo** æ¡†æ¶å°†æ¨¡å‹ç¼–è¯‘ä¸º HLS C++ï¼Œå¹¶åœ¨ CPUã€sw_emuã€hw_emu ä¸ŠéªŒè¯åŠŸèƒ½æ­£ç¡®æ€§ã€‚ |
| **è¯„ä¼°æŒ‡æ ‡** | Accuracyã€AUCã€FLOPsã€Parametersã€Model Sizeã€Inference Latencyã€FPGA Resource Utilizationï¼ˆBRAM, DSP, FF, LUTï¼‰ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ¨¡å‹ | ç±»å‹ | æ˜¯å¦ç¡¬ä»¶å‹å¥½ |
|------|------|-------------|
| MLP / Deep Sets (DS) / Interaction Network (IN) | ä¼ ç»Ÿè½»é‡æ¨¡å‹ | âœ… å¹¿æ³›ç”¨äº FPGA éƒ¨ç½² |
| JEDI-net / JEDI-linear | GNN-basedï¼Œå½“å‰ FPGA SOTA | âœ… æ”¯æŒ hls4mlï¼Œä»£è¡¨å…ˆè¿›æ°´å¹³ |
| ParT | Transformer-based SOTA | âŒ æ¨¡å‹è¿‡å¤§ï¼Œéš¾éƒ¨ç½²äº L1T |
| ParticleNet | DGCNN-based | âŒ ä¸é€‚åˆ FPGA å®æ—¶æ¨ç† |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### ï¼ˆ1ï¼‰åœ¨ JETCLASS æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆTable 3ï¼‰
| Model | Accuracy | AUC | Params | FLOPs |
|-------|----------|-----|--------|-------|
| ParT | 0.836 | 0.9834 | 2.14M | 340M |
| **JetFormer** | **0.829** | **0.9827** | **1.66M** | **213M** |

âœ… **ç»“è®º**ï¼š  
- å‡†ç¡®ç‡ä»…æ¯” ParT ä½ **0.7%**ï¼ŒAUC ä»…ä½ 0.07%ï¼Œä½† **FLOPs å‡å°‘äº† 37.4%**ï¼Œå‚æ•°å‡å°‘ 22.4%ï¼Œä½“ç°å‡ºæ›´å¼ºçš„è®¡ç®—æ•ˆç‡ã€‚

#### ï¼ˆ2ï¼‰åœ¨ HLS4ML 150P æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆTable 1 & 2ï¼‰
| Constituents | Model | Accuracy â†‘ | FLOPs â†“ |
|--------------|-------|------------|---------|
| 8 particles | JetFormer | **67.1%** | 933k |
|             | IN (best prior) | 64.9% | 37k |
| 32 particles | JetFormer | **79.9%** | 4M |
|              | IN | 75.8% | 110k |

âœ… **ç»“è®º**ï¼š  
- åœ¨æ‰€æœ‰ç²’å­æ•°é…ç½®ä¸‹ï¼ŒJetFormer **å‡†ç¡®ç‡é¢†å…ˆ 3â€“4%**ï¼Œå°½ç®¡ FLOPs æ›´é«˜ï¼Œä½†å¯é€šè¿‡å‹ç¼©è§£å†³ã€‚

#### ï¼ˆ3ï¼‰HPO ä¸ JetFormer-tiny é€‰æ‹©ï¼ˆTable 4ï¼‰
- é€‰å‡ºæœ€ä¼˜å°æ¨¡å‹ **JetFormer-tiny**ï¼ˆ4 transformer blocks, embed_dim=8, heads=2ï¼‰ï¼ŒFLOPs=26,168ï¼Œval_acc=0.6525ã€‚
- NSGAIISampler è¡¨ç°æœ€ä½³ï¼Œåœ¨æ—©æœŸå³æ”¶æ•›è‡³é«˜ hypervolumeï¼Œä¼˜äº TPE å’Œ BoTorchã€‚

#### ï¼ˆ4ï¼‰ç»“æ„åŒ–å‰ªæç»“æœï¼ˆTable 5ï¼‰
| æŒ‡æ ‡ | å‰ªæå‰ | å‰ªæå | å˜åŒ– |
|------|--------|--------|------|
| FLOPs | 26,168 | 13,784 | â†“47.3% |
| Parameters | 3,085 | 1,997 | â†“35.3% |
| Accuracy | 0.656 | 0.653 | â†“0.49% |
| Inference Time (GPU) | 3.517ms | 2.902ms | â†“17.5% |

âœ… **ç»“è®º**ï¼šå‰ªæå¯**å‡åŠè®¡ç®—æˆæœ¬**ï¼Œç²¾åº¦æŸå¤±æå°ï¼ˆ<0.5%ï¼‰ï¼Œæ¨ç†é€Ÿåº¦æå‡æ˜æ˜¾ã€‚

#### ï¼ˆ5ï¼‰1-bit é‡åŒ–ç»“æœï¼ˆTable 6ï¼‰
| Particles | Model Size Before â†’ After | Reduction | Accuracy Drop |
|----------|----------------------------|-----------|----------------|
| 8 | 404KB â†’ 31KB | **92.2%** | 1.49% |
| 16 | 414KB â†’ 41KB | **90.1%** | 2.15% |
| 32 | 451KB â†’ 79KB | **82.6%** | 3.50% |

âœ… **ç»“è®º**ï¼š  
- æ¨¡å‹ä½“ç§¯å‹ç¼©è¾¾ **82â€“92%**ï¼Œé€‚åˆåµŒå…¥èµ„æºå—é™è®¾å¤‡ï¼›
- ç²¾åº¦ä¸‹é™å¯æ§ï¼ˆ1.5â€“3.5%ï¼‰ï¼Œå°¤å…¶åœ¨è¾“å…¥ç»´åº¦è¾ƒä½æ—¶è¡¨ç°æ›´å¥½ã€‚

#### ï¼ˆ6ï¼‰FPGA ç¡¬ä»¶è¯„ä¼°ï¼ˆTable 8ï¼‰
| æ¨¡å‹ | Batch Size | Latency | BRAM/DSP/LUT åˆ©ç”¨ç‡ |
|------|------------|--------|---------------------|
| JetFormer-tiny (original) | 16 | 4.767ms | ~9.5%/1.07%/8.05% |
| JetFormer-tiny (pruned) | 16 | 2.705ms | ~9.8%/0.74%/7.5% |
| JetFormer-tiny (pruned) | **2** | **0.404ms** | **~2.92%/0.61%/6.95%** |

âœ… **ç»“è®º**ï¼š  
- å½“ batch size é™è‡³ 2 æ—¶ï¼Œå»¶è¿Ÿè¿›å…¥ **äºšæ¯«ç§’çº§ï¼ˆ0.404msï¼‰**ï¼Œå·²æ¥è¿‘ sub-microsecond ç›®æ ‡ï¼›
- èµ„æºåˆ©ç”¨ç‡ä»è¿œä½äºä¸Šé™ï¼Œè¯´æ˜å¯é€šè¿‡**æ›´æ·±çš„æµæ°´çº¿ï¼ˆpipeliningï¼‰æˆ–å¹¶è¡Œå¤åˆ¶**è¿›ä¸€æ­¥å‹ä½å»¶è¿Ÿã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **Transformer å¯ä»¥å…¼é¡¾æ€§èƒ½ä¸æ•ˆç‡**ï¼šJetFormer åœ¨ä¸ä¾èµ– pairwise interaction features çš„å‰æä¸‹ï¼Œå®ç°äº†ä¸ ParT æ¥è¿‘çš„æ€§èƒ½ï¼ŒåŒæ—¶è®¡ç®—å¼€é”€æ›´ä½ã€‚
2. âœ… **ç»Ÿä¸€æ¶æ„å¯è¡Œ**ï¼šä¸€ä¸ªåŸºç¡€æ¨¡å‹å¯é€šè¿‡ç¼©æ”¾ä¸å‹ç¼©ï¼Œçµæ´»æœåŠ¡äºä»ç¦»çº¿åˆ†æåˆ° FPGA è§¦å‘çš„ä¸åŒåœºæ™¯ã€‚
3. âœ… **ç¡¬ä»¶å‹ç¼©æœ‰æ•ˆ**ï¼š**structured pruning + 1-bit quantization** å¯ä½¿æ¨¡å‹ç¼©å°ä¸€ä¸ªæ•°é‡çº§ä»¥ä¸Šï¼Œç²¾åº¦æŸå¤±å°äº 3.5%ã€‚
4. âœ… **FPGA éƒ¨ç½²è·¯å¾„æ‰“é€š**ï¼šå€ŸåŠ© Allo æ¡†æ¶æ‰©å±•ï¼ŒæˆåŠŸå®ç° JetFormer åœ¨ FPGA ä¸Šçš„åŠŸèƒ½åˆæˆï¼ŒéªŒè¯äº† Transformer åœ¨ L1T ä¸­çš„éƒ¨ç½²æ½œåŠ›ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å½“å‰å»¶è¿Ÿå°šæœªå®Œå…¨è¾¾æ ‡**ï¼šå°½ç®¡å·²å®ç° 0.4ms å»¶è¿Ÿï¼Œä½†ä»ç•¥é«˜äºä¸¥æ ¼çš„ **sub-microsecondï¼ˆ<1Î¼sï¼‰** è¦æ±‚ã€‚
- **ç¼ºä¹ fully pipelined å®ç°**ï¼šå®éªŒä¸­æœªæ‰‹åŠ¨æ·»åŠ æ·±åº¦æµæ°´çº¿æˆ–å¾ªç¯å±•å¼€ï¼Œå¯¼è‡´èµ„æºåˆ©ç”¨ä¸è¶³ï¼Œå­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚
- **é‡åŒ–ç­–ç•¥ä»æœ‰æ”¹è¿›ç©ºé—´**ï¼š1-bit é‡åŒ–åœ¨é«˜ç»´è¾“å…¥ä¸‹è¯¯å·®ç´¯ç§¯è¾ƒæ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦æ›´ç²¾ç»†çš„ mixed-precision æˆ– error compensation æŠ€æœ¯ã€‚
- **Allo å¯¹é‡åŒ–æ”¯æŒä¸è¶³**ï¼šç›®å‰æ— æ³•ç›´æ¥éƒ¨ç½²é‡åŒ–åçš„æ¨¡å‹åˆ° Vitis HLS åç«¯ï¼Œé™åˆ¶äº†æœ€ç»ˆç¡¬ä»¶æ€§èƒ½æµ‹è¯•ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ·±å…¥ç¡¬ä»¶ä¼˜åŒ–**ï¼šå¼•å…¥è‡ªåŠ¨æˆ–æ‰‹åŠ¨çš„ **loop pipeliningã€unrollingã€spatial replication** ç­‰æŠ€æœ¯ï¼Œå……åˆ†æŒ–æ˜ FPGA å¹¶è¡Œèƒ½åŠ›ï¼Œé™ä½å»¶è¿Ÿè‡³ Î¼s çº§ã€‚
2. **è”åˆå»ºæ¨¡ä¸ç¡¬ä»¶ååŒè®¾è®¡ï¼ˆco-designï¼‰**ï¼šæ¢ç´¢æ¨¡å‹ç»“æ„ä¸ç¡¬ä»¶æ¶æ„çš„ä¸€ä½“åŒ–ä¼˜åŒ–ï¼Œä¾‹å¦‚å®šåˆ¶ç¨€ç–æ¨¡å¼ã€ä¸“ç”¨ attention å•å…ƒã€‚
3. **æ‰©å±•è‡³å…¶ä»– HEP ä»»åŠ¡**ï¼šå°† JetFormer æ¡†æ¶æ¨å¹¿è‡³é¡¶å¤¸å…‹é‡å»ºã€MET ä¼°è®¡ã€å¼‚å¸¸æ£€æµ‹ç­‰ä»»åŠ¡ã€‚
4. **å¼€å‘æ›´é²æ£’çš„ä½æ¯”ç‰¹è®­ç»ƒæ–¹æ³•**ï¼šç ”ç©¶é€‚ç”¨äºç‰©ç†æ¨¡å‹çš„æ–°å‹ QAT ç­–ç•¥ï¼Œç¼“è§£æç«¯é‡åŒ–ä¸‹çš„æ€§èƒ½é€€åŒ–ã€‚
5. **æ¨åŠ¨ Allo ç”Ÿæ€å‘å±•**ï¼šå®Œå–„å¯¹é‡åŒ–ã€åŠ¨æ€ shapeã€å¤æ‚ control flow çš„æ”¯æŒï¼Œå¢å¼ºå…¶ä½œä¸º HEP é¢†åŸŸ HLS å·¥å…·çš„èƒ½åŠ›ã€‚

---

> ğŸ”— **å¼€æºåœ°å€**ï¼šhttps://github.com/walkieq/JetFormer  
> ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼šJetFormer æä¾›äº†ä¸€ä¸ª**ä»é«˜æ€§èƒ½å»ºæ¨¡åˆ°å®é™…ç¡¬ä»¶éƒ¨ç½²çš„å®Œæ•´é—­ç¯æ–¹æ¡ˆ**ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯æ˜äº† Transformer æ¶æ„åœ¨ LHC å®æ—¶è§¦å‘ç³»ç»Ÿä¸­çš„å¯è¡Œæ€§ä¸ä¼˜è¶Šæ€§ã€‚

</details>

---

### 3. [Multi-Agent Deep Reinforcement Learning Under Constrained Communications](https://arxiv.org/abs/2601.17069)

**Authors**: Shahil Shaik, Jonathon M. Smereka, Yue Wang  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2601.17069v1  

#### Abstract
Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡ã€ŠMulti-Agent Deep Reinforcement Learning Under Constrained Communicationsã€‹æ€»ç»“**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ™®éé‡‡ç”¨**é›†ä¸­å¼è®­ç»ƒã€å»ä¸­å¿ƒåŒ–æ‰§è¡Œ**ï¼ˆCTDEï¼‰èŒƒå¼ï¼Œå…¶åœ¨è®­ç»ƒé˜¶æ®µä¾èµ–å…¨å±€çŠ¶æ€ä¿¡æ¯ï¼ˆglobal stateï¼‰ï¼Œå¯¼è‡´ä»¥ä¸‹é—®é¢˜ï¼š
- **å¯æ‰©å±•æ€§å·®**ï¼šå¤§è§„æ¨¡ç³»ç»Ÿä¸­éš¾ä»¥è·å–å’Œä¼ è¾“å…¨å±€ä¿¡æ¯ï¼›
- **é²æ£’æ€§å¼±**ï¼šè®­ç»ƒæ—¶ä¾èµ–å…¨å±€ä¿¡æ¯ï¼Œè€Œæ‰§è¡Œæ—¶ä»…èƒ½è®¿é—®å±€éƒ¨è§‚æµ‹ï¼Œé€ æˆ**è®­ç»ƒ-æµ‹è¯•ä¸åŒ¹é…**ï¼ˆtrain-test mismatchï¼‰ï¼›
- **é€šä¿¡å—é™åœºæ™¯ä¸‹å¤±æ•ˆ**ï¼šåœ¨æ— çº¿ç½‘ç»œã€è¶Šé‡æœºå™¨äººã€æœæ•‘ç­‰å®é™…åº”ç”¨ä¸­ï¼Œé•¿è·ç¦»ä½å»¶è¿Ÿé€šä¿¡éš¾ä»¥ç»´æŒã€‚

æœ¬æ–‡æ—¨åœ¨è®¾è®¡ä¸€ç§**å®Œå…¨å»ä¸­å¿ƒåŒ–çš„MARLæ¡†æ¶**ï¼Œæ— éœ€ä»»ä½•é›†ä¸­å¼ç»„ä»¶æˆ–å…¨å±€å¯è§‚æµ‹æ€§ï¼Œä»…é€šè¿‡**å±€éƒ¨è§‚æµ‹å’Œå¯¹ç­‰é€šä¿¡**ï¼ˆpeer-to-peer communicationï¼‰å®ç°é«˜æ•ˆåä½œã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**
ä½œè€…æå‡ºäº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

#### âœ… **D-GATï¼ˆDistributed Graph Attention Networkï¼‰**
- ä¸€ç§**åˆ†å¸ƒå¼å›¾æ³¨æ„åŠ›ç½‘ç»œ**ï¼Œå…è®¸æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹ç»´æŠ¤å’Œæ›´æ–°è‡ªå·±çš„æ³¨æ„åŠ›å‚æ•°ï¼›
- åˆ©ç”¨**å¤šè·³é€šä¿¡**ï¼ˆmulti-hop communicationï¼‰è¿›è¡Œ**å…¨å±€çŠ¶æ€æ¨ç†**ï¼ˆglobal state inferenceï¼‰ï¼›
- å¼•å…¥**å…±è¯†æ­£åˆ™åŒ–æŸå¤±**ï¼ˆconsensus lossï¼‰å’Œ**å»ä¸­å¿ƒåŒ–éšæœºæ¢¯åº¦ä¸‹é™**ï¼ˆD-SGDï¼‰æœºåˆ¶ï¼Œä¿ƒè¿›é‚»å±…é—´è¡¨ç¤ºä¸€è‡´æ€§ï¼Œæå‡åè°ƒèƒ½åŠ›ã€‚

#### âœ… **DG-MAPPOï¼ˆDistributed Graph-Attention MAPPOï¼‰**
- ä¸€ä¸ª**å®Œå…¨åˆ†å¸ƒå¼çš„MARLæ¡†æ¶**ï¼Œç»“åˆD-GATä¸MAPPOæ¶æ„ï¼›
- æ™ºèƒ½ä½“åŸºäº**æœ¬åœ°è§‚æµ‹ + D-GATæ¨æ–­çš„å…¨å±€çŠ¶æ€è¿‘ä¼¼ + å¹³å‡å›¢é˜Ÿå¥–åŠ±**æ¥ä¼˜åŒ–ç­–ç•¥ï¼›
- æ‰€æœ‰å­¦ä¹ è¿‡ç¨‹ï¼ˆç­–ç•¥ã€ä»·å€¼å‡½æ•°ã€å›¾ç½‘ç»œå‚æ•°ï¼‰å‡åœ¨æœ¬åœ°å®Œæˆï¼Œ**æ— ä¸­å¤®æ§åˆ¶å™¨æˆ–å…±äº«å‚æ•°**ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | CTDEæ–¹æ³•ï¼ˆå¦‚MAPPO, HAPPOï¼‰ | DG-MAPPO |
|------|----------------------------|----------|
| **è®­ç»ƒæ¨¡å¼** | é›†ä¸­å¼è®­ç»ƒï¼ˆéœ€å…¨å±€çŠ¶æ€ï¼‰ | å®Œå…¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆä»…å±€éƒ¨ä¿¡æ¯ï¼‰ |
| **é€šä¿¡éœ€æ±‚** | è®­ç»ƒæœŸé«˜å¸¦å®½æ±‡èšï¼Œæ‰§è¡ŒæœŸé€šå¸¸â€œé™é»˜â€ | è®­ç»ƒä¸æ‰§è¡Œå‡ä½¿ç”¨è½»é‡çº§å¤šè·³é€šä¿¡ |
| **æ³›åŒ–æ€§** | å­˜åœ¨train-test mismatch | æ›´å¼ºé²æ£’æ€§ï¼Œé€‚åº”åŠ¨æ€æ‹“æ‰‘å˜åŒ– |
| **å¯æ‰©å±•æ€§** | å—é™äºä¸­å¿ƒèŠ‚ç‚¹åå | çº¿æ€§å¯æ‰©å±•ï¼Œé€‚åˆå¤§å›¢é˜Ÿ |
| **éšç§/å®‰å…¨** | å…¨å±€ä¿¡æ¯æš´éœ²é£é™© | ä»…äº¤æ¢åŠ å¯†ç‰¹å¾å‘é‡ |

> ğŸ”¥ **é¦–æ¬¡å®ç°äº†å®Œå…¨æ¶ˆé™¤å¯¹ç‰¹æƒé›†ä¸­ä¿¡æ¯ä¾èµ–çš„MARLæ–¹æ³•**ï¼ŒçœŸæ­£å®ç°â€œå­¦ä¸è¡Œçš†å»ä¸­å¿ƒåŒ–â€ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†ä¸ç¯å¢ƒ**
åœ¨ä¸‰ä¸ªä¸»æµMARLåŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼š
1. **StarCraftII Multi-Agent Challenge (SMAC)**  
   - å¤šç§æˆ˜æ–—ä»»åŠ¡ï¼ˆå¦‚3m, 8m vs 9m, MMM2ç­‰ï¼‰
   - åŒ…å«åŒæ„/å¼‚æ„å›¢é˜Ÿã€ä¸åŒéš¾åº¦ç­‰çº§
2. **Google Research Football (GFootball)**  
   - è¶³çƒå¯¹æŠ—ä»»åŠ¡ï¼ˆAcademy 3 vs 1 with Keeperï¼‰
   - è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œå¼ºè°ƒå®æ—¶åä½œ
3. **Multi-Agent MuJoCo (MA-MuJoCo)**  
   - ç‰©ç†æ§åˆ¶ä»»åŠ¡ï¼ˆå¦‚Multi-HalfCheetahï¼‰
   - å›ºå®šé‚»æ¥å…³ç³»ï¼Œæ¨¡æ‹ŸçœŸå®æœºå™¨äººå…³èŠ‚è¿æ¥

---

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**

| è®¾ç½®é¡¹ | æè¿° |
|-------|------|
| **è§‚æµ‹èŒƒå›´é™åˆ¶** | åœ¨SMACä¸­å°†é»˜è®¤è§†é‡ä»9é™è‡³4ï¼Œæ¨¡æ‹Ÿç¨€ç–é€šä¿¡ |
| **é€šä¿¡æ¨¡å‹** | åŠ¨æ€å›¾ $ G=(N,E) $ï¼Œè¾¹ç”±è·ç¦»å†³å®šï¼›æ”¯æŒ1-hopåˆ°N-hopæ¶ˆæ¯ä¼ é€’ |
| **è¯„ä¼°æŒ‡æ ‡** | - èƒœç‡ï¼ˆWin Rateï¼‰<br>- å¹³å‡å›åˆå¾—åˆ†ï¼ˆAverage Episode Scoreï¼‰<br>- æ”¶æ•›é€Ÿåº¦ã€ç¨³å®šæ€§ |
| **è®­ç»ƒæ–¹å¼** | æ¯ä¸ªagentæ‹¥æœ‰ç‹¬ç«‹bufferå’Œç½‘ç»œï¼Œä»…é€šè¿‡D-GATé€šä¿¡åŒæ­¥è¡¨å¾ |
| **è¶…å‚æ•°** | ä½¿ç”¨æ ‡å‡†PPOé…ç½®ï¼ŒD-GATå­¦ä¹ ç‡5e-4ï¼Œconsensus lossæƒé‡20ï¼ˆè§é™„å½•ï¼‰ |

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
ä¸å¤šä¸ªSOTA CTDEæ–¹æ³•æ¯”è¾ƒï¼š
- **MAPPO**ï¼šç»å…¸çš„CTDE-PPOæ–¹æ³•
- **HAPPO**ï¼šåˆ†å±‚æ³¨æ„åŠ›PPOï¼Œæ”¯æŒå¼‚æ„agent
- **MAT-Dec**ï¼šåŸºäºåºåˆ—å»ºæ¨¡çš„å»ä¸­å¿ƒåŒ–å˜ä½“ï¼ˆä½†ä»ä¸ºCTDEï¼‰

> âš ï¸ æ³¨æ„ï¼šDG-MAPPO**ä»…ä½¿ç”¨å±€éƒ¨è§‚æµ‹**ï¼Œè€Œæ‰€æœ‰baselineå‡å¯è®¿é—®å…¨å±€çŠ¶æ€ã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ªTable 1 & Figure 3ï¼‰**

#### ğŸ“Š **SMAC èƒœç‡è¡¨ç°ï¼ˆ% Â± stdï¼‰**
| ä»»åŠ¡ | MAPPO | HAPPO | DG-MAPPO |
|------|--------|--------|-----------|
| 3m (Easy) | 100.0(0.4) | 100.0(1.2) | **100.0(1.4)** |
| 8m (Easy) | 96.8(2.9) | 97.5(1.1) | **100.0(1.4)** |
| MMM (Easy) | 95.6(4.5) | 81.2(22.9) | **100.0(1.6)** |
| 5m vs 6m (Hard) | 88.2(6.2) | 77.5(7.2) | **88.7(4.7)** |
| 10m vs 11m (Hard) | 96.3(5.8) | 87.5(6.7) | **100.0(1.4)** |
| 25m (Hard) | 100.0(2.7) | 95.0(2.0) | **95.3(3.1)** |
| MMM2 (Hard+) | 81.8(10.1) | 88.8(2.0) | **98.9(1.2)** |
| 6h vs 8z (Hard+) | 88.4(5.7) | 76.2(3.1) | **95.0(2.7)** |

âœ… **ç»“è®º**ï¼šDG-MAPPOåœ¨ç»å¤§å¤šæ•°ä»»åŠ¡ä¸Š**æŒå¹³ç”šè‡³è¶…è¶ŠCTDEæ–¹æ³•**ï¼Œå°¤å…¶åœ¨å¤æ‚å¼‚æ„ä»»åŠ¡ä¸­ä¼˜åŠ¿æ˜æ˜¾ã€‚

---

#### ğŸ“ˆ **ç¨€ç–é€šä¿¡ä¸‹çš„æ€§èƒ½ï¼ˆTable 2, Sight Range=4ï¼‰**
| ä»»åŠ¡ | 1-hop | 3-hops | N-hops |
|------|--------|--------|--------|
| 6h vs 8z | 77.08% | 83.68% | **83.75%** |
| MMM2 | 90.62% | 92.7% | **93.1%** |

ğŸ“Œ å³ä½¿åœ¨æç«¯ç¨€ç–é€šä¿¡ä¸‹ï¼ˆå¹³å‡åº¦â‰ˆ3ï¼‰ï¼Œ**ä»…1-hopå³å¯è¾¾åˆ°è‰¯å¥½æ€§èƒ½**ï¼Œå¢åŠ hopæ•°å¸¦æ¥è¾¹é™…å¢ç›Šã€‚

---

#### ğŸƒâ€â™‚ï¸ **è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆMA-MuJoCoï¼‰**
- åœ¨ **Multi-HalfCheetah (6Ã—1)** ä¸Šï¼š
  - K=1ï¼šæ¥è¿‘CTDEæ”¶æ•›æ›²çº¿
  - K=3ï¼šå‡ ä¹é‡åˆ
  - K=6ï¼š**è¶…è¿‡CTDE baseline**
- è¡¨æ˜ï¼š**å°‘é‡å¤šè·³é€šä¿¡è¶³ä»¥æ”¯æ’‘é«˜è´¨é‡åä½œ**

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰**

#### ğŸ”¹ **A. æ¶ˆæ¯èšåˆæ–¹å¼å¯¹æ¯”ï¼ˆMean vs Attentionï¼‰**
- åœ¨ç®€å•ä»»åŠ¡ï¼ˆMMM2, 10m vs 11mï¼‰ä¸­ï¼Œmean aggregationè¡¨ç°æ¥è¿‘attentionï¼›
- åœ¨å›°éš¾ä»»åŠ¡ï¼ˆ6h vs 8zï¼‰ä¸­ï¼Œ**attentionæ˜¾è‘—ä¼˜äºmean**ï¼Œå°¤å…¶æ˜¯åœ¨ä½hopè®¾ç½®ä¸‹ï¼›
- ç»“è®ºï¼š**attentionæœºåˆ¶åœ¨æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­æœ‰æ›´å¼ºè¡¨è¾¾åŠ›**ã€‚

#### ğŸ”¹ **B. Hopæ•°é‡çš„å½±å“**
- æ€§èƒ½éšhopå¢åŠ è€Œæå‡ï¼Œä½†å‘ˆ**äºšçº¿æ€§å¢é•¿**ï¼›
- **K=1 å·²å¯è·å¾—è¾ƒå¼ºæ€§èƒ½**ï¼ŒK=N/2åŸºæœ¬è¾¾åˆ°é¥±å’Œï¼›
- å®è·µå»ºè®®ï¼šä»å°hopå¼€å§‹ï¼ŒæŒ‰éœ€æ‰©å±•ä»¥èŠ‚çœé€šä¿¡å¼€é”€ã€‚

#### ğŸ”¹ **C. Consensus Lossçš„ä½œç”¨**
- åœ¨é«˜hopè®¾ç½®ä¸‹ï¼ˆå¦‚6-hopï¼‰ï¼ŒåŠ å…¥consensus lossæ˜¾è‘—åŠ å¿«æ”¶æ•›å¹¶æé«˜æœ€ç»ˆæ€§èƒ½ï¼›
- åœ¨1-hopè®¾ç½®ä¸‹æ•ˆæœè¾ƒå°ä½†ä»æœ‰ç›Šï¼›
- ç»“è®ºï¼šè¯¥æŸå¤±æœ‰åŠ©äºç¨³å®šè®­ç»ƒï¼Œå°¤å…¶åœ¨ä¿¡æ¯ä¼ æ’­å……åˆ†æ—¶æ›´æœ‰æ•ˆã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **ç»“æ„åŒ–å±€éƒ¨é€šä¿¡è¶³ä»¥æ”¯æŒé«˜è´¨é‡å¤šæ™ºèƒ½ä½“åä½œ**  
   > æ— éœ€å…¨å±€çŠ¶æ€ï¼Œä»…é D-GATçš„å¤šè·³æ³¨æ„åŠ›é€šä¿¡å³å¯å®ç°åª²ç¾ç”šè‡³è¶…è¶ŠCTDEçš„è¡¨ç°ã€‚

2. âœ… **DG-MAPPOå…·æœ‰ä¼˜å¼‚çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§**  
   > åœ¨å¤šè¾¾25ä¸ªagentçš„ä»»åŠ¡ä¸­ä»ä¿æŒé«˜æ€§èƒ½ï¼Œä¸”åœ¨ç¨€ç–é€šä¿¡ä¸‹ä¾ç„¶ç¨³å¥ã€‚

3. âœ… **è®­ç»ƒä¸æ‰§è¡Œçš„ä¸€è‡´æ€§æå‡äº†æ³›åŒ–èƒ½åŠ›**  
   > ç”±äºè®­ç»ƒå’Œæ‰§è¡Œéƒ½ä¾èµ–ç›¸åŒé€šä¿¡æœºåˆ¶ï¼Œé¿å…äº†CTDEçš„train-test mismatché—®é¢˜ã€‚

4. âœ… **è½»é‡çº§è®¾è®¡å…·å¤‡å®ç”¨æ½œåŠ›**  
   > å°‘é‡hopï¼ˆå¦‚1~3ï¼‰å³å¯å–å¾—è‰¯å¥½æ€§èƒ½ï¼Œé€‚åˆéƒ¨ç½²äºèµ„æºå—é™çš„çœŸå®ç³»ç»Ÿï¼ˆå¦‚æ— äººæœºç¾¤ã€æ— äººè½¦ç¼–é˜Ÿï¼‰ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- â— **é€šä¿¡é¢‘ç‡è¾ƒé«˜**ï¼šæ‰§è¡Œé˜¶æ®µä»éœ€æŒç»­é€šä¿¡ï¼Œä¸é€‚åˆâ€œé™é»˜éƒ¨ç½²â€åœºæ™¯ï¼›
- â— **å¯¹åˆå§‹è¿é€šæ€§æ•æ„Ÿ**ï¼šè‹¥é€šä¿¡å›¾é•¿æœŸä¸è¿é€šï¼Œå¯èƒ½å½±å“å…¨å±€çŠ¶æ€æ¨ç†è´¨é‡ï¼›
- â— **D-GATå‚æ•°é‡ç•¥å¢**ï¼šæ¯ä¸ªagentç‹¬ç«‹ç»´æŠ¤GATå‚æ•°ï¼Œå†…å­˜å ç”¨é«˜äºå‚æ•°å…±äº«ç‰ˆæœ¬ï¼›
- â— **æœªè€ƒè™‘æ¶æ„èŠ‚ç‚¹æˆ–æ¬ºéª—è¡Œä¸º**ï¼šå‡è®¾æ‰€æœ‰agentåˆä½œè¯šå®ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **å¼•å…¥è‡ªé€‚åº”é€šä¿¡æœºåˆ¶**ï¼šè®©agentè‡ªä¸»å†³å®šä½•æ—¶/å‘è°å‘é€æ¶ˆæ¯ï¼ˆç±»ä¼¼ATOCï¼Œä½†å»ä¸­å¿ƒåŒ–å®ç°ï¼‰ï¼›
2. **æ¢ç´¢å¼‚æ­¥é€šä¿¡ä¸å»¶è¿Ÿå®¹å¿æœºåˆ¶**ï¼šé€‚ç”¨äºçœŸå®æ— çº¿ç½‘ç»œä¸­çš„ä¸¢åŒ…ä¸å»¶è¿Ÿï¼›
3. **ç»“åˆé€šä¿¡å‹ç¼©ä¸é‡åŒ–æŠ€æœ¯**ï¼šé™ä½å¸¦å®½éœ€æ±‚ï¼Œä¾¿äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼›
4. **æ‹“å±•è‡³æ··åˆåˆä½œ-ç«äº‰ç¯å¢ƒ**ï¼šç ”ç©¶åœ¨éå®Œå…¨åˆä½œä¸‹çš„ç¨³å®šæ€§ï¼›
5. **ç¡¬ä»¶åŸå‹éªŒè¯**ï¼šåœ¨çœŸå®æœºå™¨äººå¹³å°ï¼ˆUGV/UAVï¼‰ä¸Šéƒ¨ç½²æµ‹è¯•ã€‚

---

## âœ… **æ€»ç»“ä¸€å¥è¯**
> **DG-MAPPOæ˜¯é¦–ä¸ªå®Œå…¨æ‘†è„±é›†ä¸­å¼è®­ç»ƒä¾èµ–çš„MARLæ¡†æ¶ï¼Œè¯æ˜äº†ä»…å‡­å±€éƒ¨è§‚æµ‹ä¸å¤šè·³æ³¨æ„åŠ›é€šä¿¡ï¼Œå°±èƒ½åœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸­å®ç°ä¸CTDEç›¸å½“ç”šè‡³æ›´ä¼˜çš„åä½œæ€§èƒ½ï¼Œä¸ºç°å®ä¸–ç•Œä¸­å¯æ‰©å±•ã€é²æ£’çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ã€‚**

</details>

---

### 4. [Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts](https://arxiv.org/abs/2601.17111)

**Authors**: Xuan-Phi Nguyen, Shrey Pandit, Austin Xu, Caiming Xiong, Shafiq Joty  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2601.17111v1  

#### Abstract
Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirabl...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜
- **MoEæ¨¡å‹åœ¨å®é™…è¿è¡Œä¸­å­˜åœ¨ä¸¥é‡çš„ä¸“å®¶è·¯ç”±ä¸å¹³è¡¡**ï¼ˆimbalanced routingï¼‰ï¼Œå³ä½¿åœ¨é¢„è®­ç»ƒé˜¶æ®µå¼•å…¥äº†è´Ÿè½½å‡è¡¡æœºåˆ¶ï¼ˆå¦‚auxiliary lossï¼‰ï¼Œåè®­ç»ƒï¼ˆpost-trainingï¼‰æˆ–æ¨ç†é˜¶æ®µä»ä¼šå‡ºç°å¤§é‡tokené›†ä¸­åˆ°å°‘æ•°ä¸“å®¶çš„æƒ…å†µã€‚
- **æ ‡å‡†Expert Parallelismï¼ˆEPï¼‰å‡è®¾è´Ÿè½½å‡è¡¡**ï¼Œä½†åœ¨æç«¯ä¸å¹³è¡¡åœºæ™¯ä¸‹ä¼šå¯¼è‡´ï¼š
  - å°‘æ•°GPUä¸¥é‡è¿‡è½½ï¼ˆcompute- and memory-boundï¼‰
  - å‡ºç° **Out-of-Memory (OOM)** é”™è¯¯
  - æ•´ä½“å»¶è¿Ÿæ˜¾è‘—ä¸Šå‡
- ç°æœ‰ç¼“è§£ç­–ç•¥ï¼ˆå¦‚é™ä½batch sizeã€å¤åˆ¶å†—ä½™ä¸“å®¶ï¼‰ä¼šç‰ºç‰²ååé‡æˆ–å¢åŠ å†…å­˜å¼€é”€ã€‚

### ğŸš€ æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æå‡º **Least-Loaded Expert Parallelism (LLEP)** â€”â€” ä¸€ç§åŠ¨æ€è´Ÿè½½å‡è¡¡çš„EPç®—æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- **åŠ¨æ€é‡è·¯ç”±**ï¼šå½“æŸä¸ªGPUä¸Šçš„ä¸“å®¶è´Ÿè½½è¶…è¿‡å®¹é‡é˜ˆå€¼æ—¶ï¼Œå°†è¶…é¢çš„tokensåŠå…¶å¯¹åº”çš„expert weightsè½¬å‘è‡³å½“å‰æœ€ç©ºé—²çš„GPUè¿›è¡Œâ€œå…±äº«è®¡ç®—â€ã€‚
- **åŒç»´åº¦ä¼˜åŒ–**ï¼šåŒæ—¶è€ƒè™‘ **compute load** å’Œ **memory usage**ï¼Œå¹¶æƒè¡¡é€šä¿¡å¼€é”€ä¸æœ¬åœ°è®¡ç®—æˆæœ¬ã€‚
- **ç²¾ç¡®è®¡ç®—ä¿éšœ**ï¼šLLEPæ˜¯ä¸€ä¸ª**exact MoE computation algorithm**ï¼Œä¸æ”¹å˜åŸå§‹MoEçš„æ•°å­¦è¡Œä¸ºï¼ˆå¦‚ä¸ä¿®æ”¹gateè¾“å‡ºæˆ–è·¯ç”±é€»è¾‘ï¼‰ã€‚
- æ”¯æŒ**å‰å‘ä¸åå‘ä¼ æ’­**ï¼Œå¯ç”¨äºè®­ç»ƒå’Œæ¨ç†ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | æ˜¯å¦æ”¹å˜æ¨¡å‹è¡Œä¸º | æ˜¯å¦æ”¯æŒè®­ç»ƒ | å†…å­˜æ•ˆç‡ | æç«¯ä¸å¹³è¡¡ä¸‹ç¨³å®šæ€§ |
|------|------------------|-------------|----------|---------------------|
| æ ‡å‡†EP | å¦ | æ˜¯ | å·®ï¼ˆOOMé£é™©é«˜ï¼‰ | âŒ |
| é™ä½batch size | å¦ | æ˜¯ | ä¸­ï¼ˆååä¸‹é™ï¼‰ | âš ï¸ |
| å¤åˆ¶ä¸“å®¶ï¼ˆEPLBï¼‰ | å¦ | å¦ï¼ˆä»…æ¨ç†ï¼‰ | å·®ï¼ˆé¢å¤–å‰¯æœ¬ï¼‰ | âš ï¸ï¼ˆä»å¯èƒ½OOMï¼‰ |
| LLEPï¼ˆæœ¬æ–‡ï¼‰ | **å¦** | **æ˜¯** | **ä¼˜ï¼ˆå³°å€¼å†…å­˜ç¨³å®šï¼‰** | âœ… |

> âœ… LLEPåœ¨ä¿æŒæ•°å­¦ç­‰ä»·æ€§çš„å‰æä¸‹ï¼Œå®ç°äº†ç³»ç»Ÿçº§çš„åŠ¨æ€è´Ÿè½½å‡è¡¡ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†
- **åˆæˆä¸å¹³è¡¡è´Ÿè½½æ¨¡æ‹Ÿ**ï¼šäººå·¥æ„é€ ä¸åŒçº§åˆ«çš„ä¸å¹³è¡¡åœºæ™¯ï¼ˆ30% ~ 95% tokensé›†ä¸­åœ¨1/4/16ä¸ªä¸“å®¶ä¸Šï¼‰
- **çœŸå®ä»»åŠ¡æ•°æ®**ï¼š
  - `Megatron-Math` æ•°æ®é›†ï¼ˆç”¨äºend-to-endæµ‹è¯•ï¼‰
  - å¯¹è¯æ•°æ®æ¥è‡ª `DeepScaleR`ï¼Œå“åº”ç”±gpt-ossæ¨¡å‹è‡ªç”Ÿæˆ
- æ¨¡å‹æœ¬èº«ä¸ºé¢„è®­ç»ƒå¥½çš„ **gpt-oss-20b** å’Œ **gpt-oss-120b**

### âš™ï¸ å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
| è®¾ç½®é¡¹ | é…ç½® |
|-------|------|
| ç¡¬ä»¶å¹³å° | 8Ã— H200 GPUsï¼ˆP=8ï¼‰ |
| MoEé…ç½® | è¦†ç›–gpt-oss-120bã€DeepSeek-V3ã€Kimi-K2ç­‰å¤šç§æ¶æ„ |
| Batch size per GPU | 32Kï¼ˆgpt-ossï¼‰ã€16Kï¼ˆå…¶ä»–ï¼‰ |
| è¯„ä¼°æŒ‡æ ‡ | 
| - Speedupï¼ˆç›¸å¯¹æ ‡å‡†EPçš„åŠ é€Ÿæ¯”ï¼‰ |
| - Peak memory per GPUï¼ˆå³°å€¼æ˜¾å­˜å ç”¨ï¼‰ |
| - Full-model throughputï¼ˆæ•´æ¨¡å‹ååï¼‰ |
| - End-to-end wall-clock timeï¼ˆç«¯åˆ°ç«¯è€—æ—¶ï¼‰ |
| - Accuracy vs timeï¼ˆå¾®è°ƒæ”¶æ•›é€Ÿåº¦ï¼‰ |

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Standard EP**ï¼šä¼ ç»ŸExpert Parallelismå®ç°ï¼ˆåŸºäºAll-to-Allé€šä¿¡ï¼‰
- **EPLB**ï¼ˆExpert Parallelism Load Balancerï¼‰ï¼šé€šè¿‡å¤åˆ¶çƒ­é—¨ä¸“å®¶æå‡è´Ÿè½½èƒ½åŠ›ï¼ˆä»…é€‚ç”¨äºæ¨ç†ï¼‰
- æ‰€æœ‰å¯¹æ¯”å‡åœ¨åŒä¸€ç¡¬ä»¶å’Œè½¯ä»¶æ ˆä¸‹å®Œæˆï¼ˆPyTorch + NCCLï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰MoEå±‚çº§åˆ«æ€§èƒ½ï¼ˆå›¾1ã€å›¾4ï¼‰
| åœºæ™¯ | æœ€å¤§åŠ é€Ÿæ¯”ï¼ˆSpeedupï¼‰ | å³°å€¼å†…å­˜å‡å°‘ |
|------|------------------------|--------------|
| æç«¯ä¸å¹³è¡¡ï¼ˆ95% tokens â†’ 1 expertï¼‰ | **é«˜è¾¾6.11Ã—**ï¼ˆgpt-oss-120bï¼‰ | **æœ€é«˜è¾¾5Ã—é™ä½** |
| å¹³è¡¡æƒ…å†µ | ~1Ã—ï¼ˆæ— æŸå¤±ï¼‰ | æ˜¾å­˜ç¨³å®š |
| ä¸åŒæ¨¡å‹å¹³å‡åŠ é€Ÿ | **4â€“5Ã—** | æ˜¾è‘—æŠ‘åˆ¶æ˜¾å­˜ spikes |

> ğŸ’¡ LLEPåœ¨è¶Šä¸å¹³è¡¡çš„æƒ…å†µä¸‹ä¼˜åŠ¿è¶Šæ˜æ˜¾ï¼›è€Œåœ¨å¹³è¡¡æƒ…å†µä¸‹å‡ ä¹æ— é¢å¤–å¼€é”€ã€‚

#### ï¼ˆ2ï¼‰ç«¯åˆ°ç«¯å…¨æ¨¡å‹ååï¼ˆå›¾1cï¼‰
| æ¨¡å‹ | LLEPåŠ é€Ÿæ¯” |
|------|------------|
| gpt-oss-20b | **2.2Ã—** |
| gpt-oss-120b | **1.9Ã—** |

> å³ä½¿å—éMoEå±‚ï¼ˆå¦‚attentionï¼‰å½±å“ï¼ŒMoEå±‚çš„ä¼˜åŒ–ä»å¸¦æ¥æ˜¾è‘—ç«¯åˆ°ç«¯æ”¶ç›Šã€‚

#### ï¼ˆ3ï¼‰è®­ç»ƒæ•ˆç‡ï¼ˆå›¾5ï¼‰
- åœ¨å¯¹gpt-oss-20bè¿›è¡ŒSFTï¼ˆSupervised Fine-Tuningï¼‰æ—¶ï¼š
  - LLEPæ¯”æ ‡å‡†EP **å¿«1.25Ã—**
  - æ”¶æ•›ç²¾åº¦ä¸€è‡´ï¼ˆaccuracy on AIMEâ€™25ç›¸åŒï¼‰
  - å—ç›Šäºæ›´é«˜æ•ˆçš„MoEå¤„ç†ï¼Œå‡å°‘äº†ç­‰å¾…æ—¶é—´

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ï¼ˆ1ï¼‰Batch Size å½±å“ï¼ˆå›¾6aï¼‰
- **Batchè¶Šå¤§ï¼ŒåŠ é€Ÿè¶Šæ˜æ˜¾**ï¼šå¤§batchæ›´å®¹æ˜“è§¦å‘GPUå®¹é‡é¥±å’Œï¼Œä½¿å¾—LLEPçš„è´Ÿè½½å‡è¡¡æ•ˆæœæ›´çªå‡ºã€‚
- åŸå› ï¼šå¤§batchä¸‹é€šä¿¡/è®¡ç®—å¼€é”€å æ¯”æ›´é«˜ï¼ŒLLEPçš„åŠ¨æ€åˆ†é…æ›´èƒ½å‘æŒ¥ä¼˜åŠ¿ã€‚

#### ï¼ˆ2ï¼‰è¶…å‚æ•° $ \alpha $ï¼ˆæ¯GPUæœ€å¤§è´Ÿè½½å› å­ï¼‰ï¼ˆå›¾6bï¼‰
- $ \alpha $ è¶Šå°ï¼ˆé™åˆ¶è¶Šä¸¥æ ¼ï¼‰ï¼Œé€Ÿåº¦æå‡è¶Šé«˜
- ä½†å¤ªå°å¯èƒ½å¯¼è‡´é¢‘ç¹é€šä¿¡ â†’ å­˜åœ¨ä¸€ä¸ª**trade-off**

#### ï¼ˆ3ï¼‰è‡ªé€‚åº”é˜ˆå€¼ $ \lambda $ï¼ˆå›¾7aï¼‰
- å½“è·¯ç”±æ¥è¿‘å¹³è¡¡æ—¶ï¼ˆimbalance ratio < $ \lambda $ï¼‰ï¼Œè‡ªåŠ¨åˆ‡æ¢å›æ ‡å‡†EP
- è®¾ç½® $ \lambda = 1.3 $ å¯é¿å…ä¸å¿…è¦çš„weight transferå¼€é”€ï¼Œåœ¨è½»åº¦ä¸å¹³è¡¡æ—¶æ›´é«˜æ•ˆ

#### ï¼ˆ4ï¼‰Hidden Size å½±å“ï¼ˆå›¾7bï¼‰
- **éšç€D/Hå¢å¤§ï¼ŒLLEPå¢ç›Šå¢å¼º**
- åŸå› ï¼šcomputeæ•ˆç‡æå‡å¿«äºcommunication overheadå¢é•¿ï¼Œä½¿å¾—è´Ÿè½½å‡è¡¡å¸¦æ¥çš„æ”¶ç›Šæ›´å¤§

#### ï¼ˆ5ï¼‰ä¸“å®¶æ•°é‡Nçš„å½±å“ï¼ˆé™„å½•å›¾9ï¼‰
- Nè¶Šå¤§ï¼ŒLLEPä¼˜åŠ¿è¶Šæ˜æ˜¾
- æ›´å¤šä¸“å®¶æ„å‘³ç€æ›´é«˜çš„æ½œåœ¨ä¸å¹³è¡¡é£é™©ï¼Œä¹Ÿæä¾›äº†æ›´å¤šâ€œç©ºé—²è®¾å¤‡â€ä¾›å¸è½½ä½¿ç”¨

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Imbalanced routingæ˜¯MoEæ¨¡å‹çš„è‡ªç„¶ä¸”ç†æƒ³ç°è±¡**  
   - è¡¨æ˜éƒ¨åˆ†ä¸“å®¶å·²ä¸“ä¸šåŒ–ï¼ˆspecializationï¼‰ï¼Œè€Œéè®­ç»ƒå¤±è´¥ã€‚
   - å¼ºè¡Œå¼ºåˆ¶å‡è¡¡ï¼ˆå¦‚åŠ lossï¼‰å¯èƒ½ç ´åè¿™ç§æœ‰ç›Šçš„ä¸“ä¸šåŒ–ã€‚

2. **ç³»ç»Ÿçº§è´Ÿè½½å‡è¡¡ä¼˜äºæ¨¡å‹çº§å¹²é¢„**  
   - LLEPåœ¨ä¸æ”¹åŠ¨æ¨¡å‹çš„å‰æä¸‹ï¼Œä»**ç³»ç»Ÿè°ƒåº¦å±‚é¢**è§£å†³äº†è´Ÿè½½ä¸å‡é—®é¢˜ã€‚
   - æ˜¯é¦–ä¸ªæ”¯æŒ**è®­ç»ƒ+æ¨ç†é€šç”¨**ã€ä¸”ä¿è¯**æ•°å­¦ç­‰ä»·æ€§**çš„åŠ¨æ€EPæ–¹æ¡ˆã€‚

3. **LLEPå®ç°äº†â€œæœ€å°é›†ä½“å»¶è¿Ÿ + æœ€ä½å³°å€¼å†…å­˜â€åŒé‡ç›®æ ‡**  
   - æ‰€æœ‰GPUå°½å¯èƒ½åŒæ—¶å®Œæˆä»»åŠ¡ï¼ˆworkload balancingï¼‰
   - æ˜¾å­˜ä½¿ç”¨è¶‹äºå¹³ç¨³ï¼Œé¿å…OOM

4. **é€‚ç”¨äºå¤§è§„æ¨¡éƒ¨ç½²åœºæ™¯**  
   - ç‰¹åˆ«é€‚åˆdomain-specific fine-tuning / inferenceï¼Œè¿™äº›åœºæ™¯å¤©ç„¶å¯¼è‡´è·¯ç”±åæ–œã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **å¼•å…¥é¢å¤–é€šä¿¡å¼€é”€**ï¼šéœ€è¦P2Pä¼ è¾“expert weightsï¼Œè™½ç„¶è¢«è®¾è®¡ä¸ºä»…åœ¨â€œåˆ’ç®—æ—¶â€æ‰è§¦å‘ã€‚
- **ä¾èµ–è¿è¡Œæ—¶è´Ÿè½½é¢„æµ‹**ï¼šéœ€æå‰è·å–å…¨å±€expert loadä¿¡æ¯ï¼Œå¢åŠ äº†è°ƒåº¦å¤æ‚æ€§ã€‚
- **å½“å‰å®ç°åŸºäºPythonè°ƒåº¦**ï¼šLLAç®—æ³•ç”¨Pythonå®ç°ï¼Œå­˜åœ¨ä¸€å®šæ§åˆ¶å¼€é”€ï¼Œæœªæ¥å¯é€šè¿‡kernel fusionè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
- **å¤šèŠ‚ç‚¹ç¯å¢ƒä¸‹éœ€æ³¨æ„è·¨èŠ‚ç‚¹é€šä¿¡ä»£ä»·**ï¼šç›®å‰æœªæ·±å…¥ä¼˜åŒ–inter-node spillingç­–ç•¥ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **Kernel-levelèåˆå®ç°**  
   - å°†dispatch + weight transfer + grouped-GEMMèåˆä¸ºå•ä¸€kernelï¼ˆç±»ä¼¼DeepEPæˆ–Triton-Distributedé£æ ¼ï¼‰
2. **æ”¯æŒå¼‚æ„ç¡¬ä»¶è°ƒåº¦**  
   - æ ¹æ®GPUæ€§èƒ½å·®å¼‚åŠ¨æ€é€‰æ‹©â€œleast-loadedâ€è®¾å¤‡
3. **ç»“åˆä¸“å®¶ç¼“å­˜ä¸é¢„åŠ è½½æœºåˆ¶**  
   - å¯¹é«˜é¢‘ä¸“å®¶åšç¼“å­˜ï¼Œå‡å°‘é‡å¤ä¼ è¾“
4. **æ‰©å±•è‡³Tensor Parallelism + EPæ··åˆæ¶æ„**
5. **è‡ªåŠ¨åŒ–è¶…å‚æ•°è°ƒä¼˜æ¡†æ¶**  
   - è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜çš„ $ \alpha, m, \lambda $ ç»„åˆä»¥é€‚é…ä¸åŒæ¨¡å‹ä¸ç¡¬ä»¶

---

## æ€»ç»“ä¸€å¥è¯
> **LLEPé€šè¿‡åŠ¨æ€åœ°å°†è¿‡è½½ä¸“å®¶çš„ä»»åŠ¡â€œå¸è½½â€ç»™ç©ºé—²GPUï¼Œåœ¨ä¸æ”¹å˜MoEè¯­ä¹‰çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾6Ã—åŠ é€Ÿå’Œ5Ã—æ˜¾å­˜èŠ‚çœï¼Œä¸ºå¤§è§„æ¨¡MoEæ¨¡å‹åœ¨çœŸå®ä¸å¹³è¡¡åœºæ™¯ä¸‹çš„é«˜æ•ˆè®­ç»ƒä¸æ¨ç†æä¾›äº†å¯é è§£å†³æ–¹æ¡ˆã€‚**

ğŸ”— ä»£ç å¼€æºåœ°å€ï¼š[github.com/SalesforceAIResearch/LeastLoadedEP](https://github.com/SalesforceAIResearch/LeastLoadedEP)

</details>

---

### 5. [Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning](https://arxiv.org/abs/2601.17275)

**Authors**: Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.17275v1  

#### Abstract
While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠLatent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoningã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ Large Language Modelsï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚å¤šæ­¥æ¨ç†ä»»åŠ¡æ—¶ï¼Œå¾€å¾€ä¾èµ–â€œç»Ÿè®¡æ‹Ÿåˆâ€è€Œéç³»ç»Ÿæ€§çš„é€»è¾‘æ¨å¯¼ï¼Œå¯¼è‡´ä¸­é—´æ­¥éª¤å­˜åœ¨**å±€éƒ¨åˆç†ä½†å…¨å±€é”™è¯¯**çš„ç°è±¡ã€‚ä¼ ç»Ÿ Reinforcement Learningï¼ˆRLï¼‰æ–¹æ³•ï¼ˆå¦‚ GRPOï¼‰è™½èƒ½å¼•å…¥æ­£ç¡®æ€§å¥–åŠ±æ¥å¼•å¯¼æ¨ç†ï¼Œä½†åœ¨**ç¦»æ•£ token ç©ºé—´**ä¸­ç›´æ¥è¿›è¡Œç­–ç•¥ä¼˜åŒ–é¢ä¸´ä¸‰å¤§ç“¶é¢ˆï¼š

- **Sample Inefficiency**ï¼šæ¯æ¬¡ç­–ç•¥æ›´æ–°éœ€ç”Ÿæˆå®Œæ•´ token åºåˆ—ï¼Œåé¦ˆç¨€ç–ä¸”è®¡ç®—ä»£ä»·é«˜æ˜‚ã€‚
- **High Gradient Variance**ï¼šé•¿åºåˆ—çš„é‡è¦æ€§é‡‡æ ·è¯¯å·®ç´¯ç§¯ä¸¥é‡ï¼Œè®­ç»ƒä¸ç¨³å®šã€‚
- **Catastrophic Forgetting**ï¼šå¯¹ä¸»æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒæ˜“ç ´åé¢„è®­ç»ƒé˜¶æ®µè·å¾—çš„è¯­è¨€ä¸çŸ¥è¯†èƒ½åŠ›ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šDeepLatent Reasoningï¼ˆDLRï¼‰
ä½œè€…æå‡º **DeepLatent Reasoningï¼ˆDLRï¼‰** â€”â€”ä¸€ç§åŸºäº**è¿ç»­æ½œåœ¨ç©ºé—´**çš„åŒå‘å¯¹æ¯”å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°â€œ**å…ˆæ€åè¨€**â€ï¼ˆthink-before-speakï¼‰çš„é«˜æ•ˆæ¨ç†èŒƒå¼ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†è¯•é”™è¿‡ç¨‹ä»é«˜ç»´ç¦»æ•£çš„ token ç©ºé—´è¿ç§»è‡³è¯­ä¹‰ä¸°å¯Œçš„è¿ç»­ latent manifold ä¸­ã€‚

#### æ ¸å¿ƒæ¶æ„è®¾è®¡ï¼š
- **Assistant Model**ï¼šè½»é‡çº§å¯è®­ç»ƒæ¨¡å‹ï¼Œåœ¨ latent space ä¸­é‡‡æ · $K$ æ¡å€™é€‰æ¨ç†è½¨è¿¹ï¼ˆlatent reasoning trajectoriesï¼‰ï¼Œè¡¨ç¤ºä¸ºâ€œè½¯æ€ç»´â€ï¼ˆsoft thoughtsï¼‰ã€‚
- **Frozen Main Model**ï¼šå†»ç»“å‚æ•°çš„å¤§æ¨¡å‹ï¼Œä»…è´Ÿè´£å°†ç­›é€‰åçš„é«˜è´¨é‡ latent è½¨è¿¹è§£ç ä¸ºè‡ªç„¶è¯­è¨€è¾“å‡ºã€‚
- **Latent Filtering & Selective Decoding**ï¼šé€šè¿‡ dual reward æœºåˆ¶åœ¨ latent å±‚é¢é¢„ç­›ï¼Œåªè§£ç é«˜ä»·å€¼è·¯å¾„ï¼Œå¤§å¹…å‡å°‘æ˜‚è´µçš„å‰å‘ä¼ æ’­æ¬¡æ•°ã€‚
- **Contrastive Learning Objective**ï¼šå¼•å…¥å¯¹æ¯”æŸå¤±é¼“åŠ± latent è½¨è¿¹å¤šæ ·æ€§ï¼Œé¿å…æ¨¡å¼åç¼©ï¼ˆmode collapseï¼‰ï¼Œå®ç°**å®šå‘æ¢ç´¢**ï¼ˆdirected explorationï¼‰ã€‚
- **Zero-Forgetting Updates**ï¼šæ¢¯åº¦æ›´æ–°ä»…ä½œç”¨äº Assistant å’ŒæŠ•å½±å±‚ï¼Œä¸»æ¨¡å‹å®Œå…¨å†»ç»“ï¼Œä»æ ¹æœ¬ä¸Šæ¶ˆé™¤ç¾éš¾æ€§é—å¿˜ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ GRPOã€DeepSeekMathï¼‰ | DLR |
|------|-------------------------------|-----|
| æ¢ç´¢ç©ºé—´ | Discrete token space | Continuous latent space |
| é‡‡æ ·æ•ˆç‡ | é«˜æˆæœ¬å…¨åºåˆ— rollout | ä½æˆæœ¬ latent å‘é‡é‡‡æ · |
| æ¢¯åº¦æ–¹å·® | é«˜ï¼ˆéšåºåˆ—é•¿åº¦æŒ‡æ•°å¢é•¿ï¼‰ | æ˜¾è‘—é™ä½ï¼ˆå• latent å¯¹è±¡å»ºæ¨¡ï¼‰ |
| å¿˜è®°é£é™© | å­˜åœ¨ï¼ˆä¸»æ¨¡å‹å¾®è°ƒï¼‰ | æ•°å­¦ä¸Šæ¶ˆé™¤ï¼ˆä¸»æ¨¡å‹å†»ç»“ï¼‰ |
| æ¢ç´¢è´¨é‡ | æ— æ–¹å‘éšæœºæ‰°åŠ¨ | è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å¤šæ ·åŒ–æ¢ç´¢ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **GSM8K**ï¼šçº¦ 8.5K å°å­¦æ•°å­¦åº”ç”¨é¢˜ï¼Œæ ‡å‡†åˆ’åˆ†ï¼š7,473 è®­ç»ƒ / 1,319 æµ‹è¯•ã€‚
- **MATH**ï¼šçº¦ 12.5K ç«èµ›çº§æ•°å­¦é—®é¢˜ï¼Œé‡‡ç”¨ 7,500 è®­ç»ƒ / 5,000 æµ‹è¯•çš„æ ‡å‡†æ‹†åˆ†ã€‚

### å®éªŒè®¾ç½®
- **ä¸»æ¨¡å‹ï¼ˆMain Modelï¼‰**ï¼š`LLaMA-2-7B`ï¼Œ**å…¨ç¨‹å†»ç»“**ã€‚
- **åŠ©ç†æ¨¡å‹ï¼ˆAssistant Modelï¼‰**ï¼š`LLaMA-2-1.3B`ï¼Œç”¨äº latent æ¨ç†é‡‡æ ·ã€‚
- **ç¡¬ä»¶å¹³å°**ï¼š8Ã— NVIDIA A100 GPUsã€‚
- **Latent Dimension**ï¼š512
- **Group Size (G)**ï¼š64
- **æœ€å¤§ latent æ­¥æ•°**ï¼š32
- **è®­ç»ƒè½®æ¬¡**ï¼š3 epochs

### è¯„ä¼°æŒ‡æ ‡
- **Pass@1 Accuracy**ï¼šæ¨¡å‹ä¸€æ¬¡æ€§ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹ï¼Œä½œä¸ºä¸»è¦æ€§èƒ½æŒ‡æ ‡ã€‚
- **Main Model Forward Pass Ratio**ï¼šè¡¡é‡è®¡ç®—å¼€é”€çš„å…³é”®æ•ˆç‡æŒ‡æ ‡ã€‚
- **Qualitative Analysis**ï¼šäººå·¥æ£€æŸ¥ 100 ä¸ª GSM8K è§£ç­”ä¸­çš„å¹»è§‰ä¸­é—´æ­¥éª¤æ¯”ä¾‹ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Base LLaMA-2-7B**ï¼šé›¶å¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚
- **GRPO (token-level)**ï¼šæ ‡å‡†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œæ“ä½œäº token ç©ºé—´ã€‚
- **DeepSeekMath-RL**ï¼šä»£è¡¨æ€§çš„ GRPO å˜ä½“ï¼Œå…·å¤‡åºåˆ—çº§ KL æ­£åˆ™åŒ–ç­‰æ”¹è¿›ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 3ï¼‰

| Method | #Forward | GSM8K (Pass@1) | MATH (Pass@1) |
|--------|----------|----------------|---------------|
| Base LLaMA-2-7B | â€” | 14.8% | 3.9% |
| GRPO (token-level) | 100% | 46.2% | 15.7% |
| DeepSeekMath-RL | 100% | 51.7% | 18.3% |
| **DLR (Ours)** | **18%** | **55.4%** | **22.1%** |

> âœ… **ç»“è®º**ï¼šDLR åœ¨ä»…ä½¿ç”¨ **18% çš„ä¸»æ¨¡å‹å‰å‘ä¼ æ’­**ä¸‹ï¼Œä»æ˜¾è‘—è¶…è¶Šæ‰€æœ‰ baselineï¼Œåœ¨ä¸¤ä¸ª benchmark ä¸Šå‡è¾¾åˆ° **SOTA æ€§èƒ½**ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **æ€§èƒ½æå‡**ï¼š
  - GSM8K ä¸Šæ¯” DeepSeekMath æå‡ **+3.7%**ï¼›
  - MATH ä¸Šæå‡ **+3.8%**ï¼Œæ˜¾ç¤ºå…¶åœ¨æ›´éš¾ä»»åŠ¡ä¸Šçš„æ³›åŒ–ä¼˜åŠ¿ã€‚
- **æ•ˆç‡é£è·ƒ**ï¼š
  - ä¸»æ¨¡å‹è®¡ç®—é‡ä¸‹é™è‡³ **1/5.6**ï¼ˆè§ Section 11ï¼‰ï¼Œæå¤§é™ä½éƒ¨ç½²æˆæœ¬ã€‚
- **ç¨³å®šæ€§å¢å¼º**ï¼š
  - è®­ç»ƒæ”¶æ•›æ›´å¹³æ»‘ï¼Œgradient variance æ›´ä½ï¼ˆç†è®ºåˆ†ææ”¯æŒï¼‰ã€‚
- **æ¨ç†è´¨é‡æ›´é«˜**ï¼š
  - å®šæ€§åˆ†æè¡¨æ˜ï¼ŒDLR å°†å¹»è§‰ä¸­é—´æ­¥éª¤ä» **27%ï¼ˆGRPOï¼‰é™è‡³ 9%**ï¼ŒéªŒè¯ latent è¿‡æ»¤çš„æœ‰æ•ˆæ€§ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰å†»ç»“ä¸»æ¨¡å‹çš„å½±å“ï¼ˆTable 4ï¼‰
| è®¾ç½® | å‡†ç¡®ç‡ | ç¨³å®šæ€§ |
|------|--------|--------|
| ä¸»æ¨¡å‹å¯è®­ç»ƒ | 56.1% | ä½ï¼ˆå‡ºç°éœ‡è¡ä¸é—å¿˜ï¼‰ |
| ä¸»æ¨¡å‹å†»ç»“ï¼ˆDLRï¼‰ | 55.4% | é«˜ï¼ˆç¨³å®šæ”¶æ•›ï¼‰ |

> ğŸ” è™½å³°å€¼ç•¥ä½ï¼Œä½†**ç¨³å®šæ€§ä¸é€šç”¨èƒ½åŠ›ä¿ç•™**è¿œèƒœè¿‡å¾®è°ƒæ–¹æ¡ˆã€‚

#### ï¼ˆ2ï¼‰ç§»é™¤ Contrastive Loss çš„å½±å“
- åœ¨ MATH ä¸Šé€ æˆ **4.7% çš„ç»å¯¹æ€§èƒ½ä¸‹é™**ï¼›
- latent è½¨è¿¹è¶‹äºé›†ä¸­ï¼Œç¼ºä¹å¤šæ ·æ€§ï¼Œtest-time scaling æ•ˆæœå˜å·®ã€‚

#### ï¼ˆ3ï¼‰è®¡ç®—æˆæœ¬åˆ†æï¼ˆEquation 13ï¼‰
- GRPO æˆæœ¬ï¼š$O(G \cdot C_M)$
- DLR æˆæœ¬ï¼š$O(G \cdot C_A + K \cdot C_M)$ï¼Œå…¶ä¸­ $K \ll G$
- å®é™… $K/G \approx 0.18$ï¼Œå¸¦æ¥ **5.6Ã— ä¸»æ¨¡å‹è®¡ç®—èŠ‚çœ**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Latent Space æ˜¯æ›´ä¼˜çš„æ¨ç†ä¼˜åŒ–åœºæ‰€**ï¼šç›¸æ¯”ç¦»æ•£ token ç©ºé—´ï¼Œè¿ç»­ latent manifold æä¾›æ›´å¹³æ»‘ã€è¯­ä¹‰è¿è´¯çš„ä¼˜åŒ–åœ°å½¢ï¼Œæ˜¾è‘—é™ä½ gradient variance å¹¶æé«˜ sample efficiencyã€‚
2. **Reasoning ä¸ Generation åº”è§£è€¦**ï¼šå°†â€œæ€è€ƒâ€äº¤ç»™è½»é‡ Assistant åœ¨ latent ç©ºé—´å®Œæˆï¼Œè€Œâ€œè¡¨è¾¾â€ç”±å†»ç»“å¤§æ¨¡å‹æ‰§è¡Œï¼Œæ˜¯ä¸€ç§é«˜æ•ˆä¸”å®‰å…¨çš„è®¾è®¡èŒƒå¼ã€‚
3. **Contrastive Learning å¯é©±åŠ¨æœ‰æ„ä¹‰çš„æ¢ç´¢**ï¼šé€šè¿‡å¯¹æ¯”æŸå¤±å¼ºåˆ¶ latent ç¼–ç åˆ†æ•£ï¼Œå¯åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹è‡ªå‘æ¶Œç°å‡ºå¤šè·¯å¾„ã€å¤šè§’åº¦çš„æ¨ç†èƒ½åŠ›ã€‚
4. **Zero-Forgetting æ˜¯å¯è¡Œä¸”å¿…è¦çš„**ï¼šå†»ç»“ä¸»æ¨¡å‹ä¸ä»…ç†è®ºä¸Šé˜²æ­¢ catastrophic forgettingï¼Œåœ¨å®è·µä¸­ä¹Ÿä¿éšœäº†è¯­è¨€é€šé¡ºæ€§å’ŒçŸ¥è¯†å®Œæ•´æ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æç«¯ç¬¦å·åŒ–ä»»åŠ¡é€‚åº”æ€§æœ‰é™**ï¼šå¯¹äºå½¢å¼åŒ–è¯æ˜ç­‰è¦æ±‚æ¯ä¸€æ­¥ä¸¥æ ¼å¯¹é½ token çš„ä»»åŠ¡ï¼Œlatent-to-token è§£ç å¯èƒ½å­˜åœ¨ç²’åº¦ä¸åŒ¹é…é—®é¢˜ã€‚
- **Reward Attribution è¾ƒç²—ç²’åº¦**ï¼šå½“å‰å¥–åŠ±ä»åŸºäºæ•´ä¸ª latent è½¨è¿¹æ‰“åˆ†ï¼Œç¼ºä¹ step-level çš„ç»†ç²’åº¦ç›‘ç£ä¿¡å·ã€‚
- **ä¾èµ–é«˜è´¨é‡ frozen decoder**ï¼šè‹¥ä¸»æ¨¡å‹æœ¬èº«è§£ç èƒ½åŠ›å¼±ï¼Œåˆ™ latent è¡¨ç¤ºå†ä¼˜ä¹Ÿæ— æ³•ä½“ç°ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³å…¶ä»–æ¨ç†å¯†é›†å‹ä»»åŠ¡ï¼šå¦‚ **code generation**ã€**commonsense reasoning**ã€**formal verification**ã€‚
- æ¢ç´¢ **latent space çš„ scaling laws**ï¼šç ”ç©¶ latent dimensionã€trajectory length ä¸æ€§èƒ½çš„å…³ç³»ã€‚
- å¼•å…¥ **finer-grained auxiliary supervision**ï¼šä¾‹å¦‚ step-wise reward modeling æˆ– latent consistency constraintsã€‚
- æ„å»º **self-evolution + self-correction** æœºåˆ¶ï¼šåˆ©ç”¨ DLR æ¡†æ¶å®ç° LLM è‡ªä¸»è¿­ä»£ä¼˜åŒ–æ¨ç†ç­–ç•¥ã€‚

--- 

> ğŸ“Œ **æ€»ä½“è¯„ä»·**ï¼š  
> DLR ä¸ä»…æ˜¯ä¸€é¡¹æŠ€æœ¯æ”¹è¿›ï¼Œæ›´æ˜¯å¯¹ LLM æ¨ç†èŒƒå¼çš„æ ¹æœ¬é‡æ„â€”â€”ä»â€œåœ¨è¯æµ·ä¸­ç›²ç›®æœç´¢â€è½¬å‘â€œåœ¨æ€æƒ³ç©ºé—´ä¸­è§„åˆ’è·¯å¾„â€ã€‚å®ƒä¸ºæ„å»º**ç¨³å®šã€é«˜æ•ˆã€å¯æŒç»­è¿›åŒ–**çš„ LLM reasoning ç³»ç»Ÿæä¾›äº†åšå®çš„æ–°è·¯å¾„ã€‚

</details>

---

### 6. [BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation](https://arxiv.org/abs/2601.17625)

**Authors**: Yuhan Xie, Jinhan Liu, Xiaoyong Ni, Fei Tan, Icare Sakr, Thibault Collin, Shiqi Sun, Alejandro Rodriguez Guajardo, Demon Fanny, Charles-francois Vincent Latchoumane, Henri Lorach, Jocelyne Bloch, Gregoire Courtine, Mahsa Shoaran  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.17625v1  

#### Abstract
Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hin...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šBrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation**

---

## **1. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å½“å‰åŸºäº **Transformer** çš„ç¥ç»è§£ç å™¨è™½ç„¶åœ¨è„‘æœºæ¥å£ï¼ˆBCIï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå…¶åºå¤§çš„å‚æ•°é‡å’Œé«˜è®¡ç®—å¼€é”€ï¼Œéš¾ä»¥éƒ¨ç½²åœ¨åŠŸè€—å—é™çš„**å¯æ¤å…¥ç³»ç»Ÿ**ï¼ˆimplantable BCIï¼‰ä¸­ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillation, KDï¼‰æ–¹æ³•è¯•å›¾å®Œæ•´ä¿ç•™æ•™å¸ˆæ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºï¼Œä½†åœ¨å­¦ç”Ÿæ¨¡å‹å®¹é‡è¿œå°äºæ•™å¸ˆæ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆè¿ç§»å…³é”®ä»»åŠ¡ä¿¡æ¯ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯**
æœ¬æ–‡æå‡ºäº† **BrainDistill**ï¼Œä¸€ä¸ªé¢å‘å¯æ¤å…¥ç³»ç»Ÿçš„æ–°å‹è¿åŠ¨æ„å›¾è§£ç æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯ **Task-Specific Knowledge Distillation (TSKD)** å’Œè½»é‡åŒ–ç¥ç»è§£ç å™¨ **Implantable Neural Decoder (IND)**ã€‚

#### **ä¸»è¦åˆ›æ–°ç‚¹ï¼š**
- **TSKDï¼ˆä»»åŠ¡ç‰¹å®šçŸ¥è¯†è’¸é¦ï¼‰**ï¼š
  - ä¸åŒäºä¼ ç»Ÿ KD ä¸­ç›´æ¥å¯¹é½æ•™å¸ˆä¸å­¦ç”Ÿç‰¹å¾ç©ºé—´çš„æ–¹æ³•ï¼ŒTSKD é€šè¿‡ä¸€ä¸ª**æœ‰ç›‘ç£çš„æŠ•å½±å™¨**ï¼ˆsupervised projectorï¼‰ï¼Œå°†æ•™å¸ˆæ¨¡å‹çš„é«˜ç»´åµŒå…¥å‹ç¼©åˆ°ä¸€ä¸ª**ä»»åŠ¡ç›¸å…³çš„å­ç©ºé—´**ã€‚
  - è¯¥æ–¹æ³•æ˜¾å¼åœ°ä¼˜å…ˆä¿ç•™å¯¹è§£ç ä»»åŠ¡æœ€å…³é”®çš„ç‰¹å¾ï¼Œä»è€Œå¼¥è¡¥å­¦ç”Ÿæ¨¡å‹å®¹é‡ä¸è¶³çš„é—®é¢˜ã€‚
  
- **Task-Specific Ratio (TSR)**ï¼š
  - æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ **TSR**ï¼Œç”¨äºé‡åŒ–æŠ•å½±åç‰¹å¾ä¸­ä¿ç•™çš„ä»»åŠ¡ç›¸å…³ä¿¡æ¯æ¯”ä¾‹ã€‚
  - è¯¥æŒ‡æ ‡å¯ç”¨äºé€‰æ‹©æœ€ä¼˜çš„æŠ•å½±æ–¹å¼ï¼Œå¹¶è¯æ˜å…¶ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½é«˜åº¦ç›¸å…³ï¼ˆç›¸å…³ç³»æ•° >0.9ï¼‰ï¼Œè€Œä¼ ç»ŸæŒ‡æ ‡å¦‚é‡æ„è¯¯å·®æˆ–äº’ä¿¡æ¯åˆ™å‡ ä¹æ— ç›¸å…³æ€§ã€‚

- **IND æ¶æ„è®¾è®¡**ï¼š
  - è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§ã€é€‚åˆç¡¬ä»¶éƒ¨ç½²çš„ Transformer è§£ç å™¨ï¼Œé‡‡ç”¨ **Continuous Wavelet Transform (CWT)** è¿›è¡Œç‰¹å¾åˆ†è¯ï¼Œç»“åˆ **Linear Attention** ç»“æ„ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ã€‚
  - æ‰€æœ‰çº¿æ€§å±‚ä»…å«æƒé‡çŸ©é˜µï¼ˆæ— åç½®ï¼‰ï¼Œæœ‰åˆ©äºåç»­æ•´æ•°é‡åŒ–ã€‚

- **Quantization-Aware Training (QAT) with Learnable Clipping Ranges**ï¼š
  - æå‡ºä¸€ç§æ”¯æŒ**æ•´æ•°ä»…æ¨ç†**ï¼ˆinteger-only inferenceï¼‰çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹æ¡ˆã€‚
  - å¼•å…¥**å¯å­¦ä¹ çš„æ¿€æ´»è£å‰ªèŒƒå›´**ï¼ˆlearnable clipping rangesï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è”åˆä¼˜åŒ–ï¼Œä»¥æœ€å°åŒ–é‡åŒ–å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| æ–¹é¢ | BrainDistill çš„ä¼˜åŠ¿ |
|------|---------------------|
| **æ¨¡å‹å¤§å°ä¸æ•ˆç‡** | IND ä»… 30K å‚æ•°ï¼Œè¿œå°äºä¸»æµåŸºç¡€æ¨¡å‹ï¼ˆå¦‚ 100M å‚æ•° Transformerï¼‰ |
| **è’¸é¦æœ‰æ•ˆæ€§** | åœ¨å°æ ·æœ¬æ ¡å‡†åœºæ™¯ä¸‹ï¼ŒTSKD æ˜¾è‘—ä¼˜äº KDã€SimKDã€VkDã€TOFDã€TED ç­‰åŸºçº¿æ–¹æ³• |
| **ç¡¬ä»¶å‹å¥½æ€§** | æ”¯æŒæ•´æ•°ä»…æ¨ç†ï¼ŒåŠŸè€—ä½è‡³ **5.66 mW**ï¼Œæ»¡è¶³å¯æ¤å…¥è®¾å¤‡çš„å®‰å…¨è¦æ±‚ï¼ˆ<15â€“40 mWï¼‰ |
| **æ³›åŒ–èƒ½åŠ›** | å¯è·¨æ¨¡æ€ï¼ˆECoGã€EEGã€spikeï¼‰ã€è·¨ä»»åŠ¡ã€è·¨è¢«è¯•ç¨³å®šå·¥ä½œ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
| æ•°æ®é›† | ç±»å‹ | è¢«è¯•æ•° | ä»»åŠ¡æè¿° |
|--------|------|--------|----------|
| **Human-C** | ECoG | 1 | å¤šç±»ä¸Šè‚¢åŠ¨ä½œåˆ†ç±»ï¼ˆ6ç±»ï¼šä¼‘æ¯ã€è‚©å±ˆã€è‚˜ä¼¸ç­‰ï¼‰ |
| **Monkey-R** | ECoG | 1 | ä¸Šè‚¢å…³èŠ‚è½¨è¿¹å›å½’ï¼ˆè…•éƒ¨ä½ç½®é¢„æµ‹ï¼‰ |
| **Human-D** | ECoG | 12 | äºŒåˆ†ç±»ï¼šæ‰‹è‡‚ç§»åŠ¨ vs é™æ­¢ |
| **BCIC-2A / BCIC-2B** | EEG | 9 | è¿åŠ¨æƒ³è±¡åˆ†ç±»ï¼ˆ4ç±» / 2ç±»ï¼‰ |
| **FALCON-M1** | Spikes | 1 | EMG ä¿¡å·å›å½’ï¼ˆ16é€šé“ï¼‰ |

### **å®éªŒè®¾ç½®**
- **è®­ç»ƒèŒƒå¼**ï¼š
  - **Scratch**ï¼šç›´æ¥åœ¨å°è§„æ¨¡æ ¡å‡†æ•°æ® $ \mathcal{X}_{\text{recalib}} $ ä¸Šè®­ç»ƒã€‚
  - **Distillation**ï¼šä»é¢„è®­ç»ƒçš„å¤§æ¨¡å‹ï¼ˆteacherï¼‰è’¸é¦çŸ¥è¯†åˆ° INDã€‚
- **æ•™å¸ˆæ¨¡å‹**ï¼š
  - Human-Cï¼š100M å‚æ•° Transformer
  - BCIC-2A/Bï¼šEEGPTï¼ˆ101Mï¼‰
  - FALCON-M1ï¼šNDT2ï¼ˆ3.7Mï¼‰
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - åˆ†ç±»ä»»åŠ¡ï¼š**F1 Score**ã€**Average Recall**
  - å›å½’ä»»åŠ¡ï¼š**RÂ²**
  - è·¨è¢«è¯•ï¼š**Accuracy**ã€**AUROC**
  - ç¡¬ä»¶ï¼š**åŠŸè€—ä¼°è®¡ï¼ˆmWï¼‰**

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| ç±»åˆ« | å¯¹æ¯”æ–¹æ³• |
|------|--------|
| **è§£ç å™¨æ¶æ„** | EEGNet, EEGConformer, ATCNet, CTNet, LaBraM |
| **çŸ¥è¯†è’¸é¦æ–¹æ³•** | KD (Hinton et al.), SimKD, VkD, RdimKD, TOFD, TED |
| **é‡åŒ–æ–¹æ³•** | I-ViT (W8A8) |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **(1) Human-C å¤šç±»åˆ†ç±»ä»»åŠ¡ï¼ˆF1 Scoreï¼‰**
| æ–¹æ³• | 1-1 | 4-4 | 4-5 | 4-6 | 16-17 |
|------|-----|-----|-----|-----|-------|
| **Teacher (100M)** | 77.6 | 73.3 | 74.5 | 77.3 | 80.6 |
| **IND (Scratch)** | 69.1 | 64.9 | 61.5 | 70.6 | 72.3 |
| **IND + TSKD** | **73.9** | **73.3** | **75.0** | **77.3** | **70.4** |
| IND + TSKD(CE) | 73.4 | 68.1 | 65.1 | 71.7 | **69.8** |
| IND + RdimKD | 72.3 | 68.6 | 63.1 | 72.3 | 65.9 |
| IND + TED | 71.2 | 67.9 | 61.5 | 71.9 | 69.1 |

> âœ… **TSKD åœ¨å¤šä¸ªä¼šè¯ä¸­è¾¾åˆ°ç”šè‡³è¶…è¿‡æ•™å¸ˆæ¨¡å‹çš„è¡¨ç°**ï¼Œå°¤å…¶åœ¨ 4-5 å’Œ 4-6 ä¼šè¯ä¸­è¡¨ç°æœ€ä½³ã€‚

#### **(2) è·¨æ¨¡æ€è’¸é¦æ•ˆæœï¼ˆF1/AUROC/RÂ²ï¼‰**
| æ•°æ®é›† | æ–¹æ³• | æ€§èƒ½ |
|--------|------|------|
| **BCIC-2A (EEG)** | IND + TSKD | **26.73** (vs. 24.22 scratch) |
| **BCIC-2B (EEG)** | IND + TSKD | **69.78** AUROC (vs. 68.79 scratch) |
| **FALCON-M1 (Spikes)** | IND + TSKD | **43.52** RÂ² (vs. 37.59 scratch) |

> âœ… **TSKD åœ¨ EEG å’Œ spike æ•°æ®ä¸Šå‡æœ‰æ•ˆæå‡æ€§èƒ½**ï¼ŒéªŒè¯äº†å…¶è·¨æ¨¡æ€é€šç”¨æ€§ã€‚

#### **(3) ç¡¬ä»¶éƒ¨ç½²æ€§èƒ½ï¼ˆHuman-Cï¼‰**
| æ¨¡å‹ | F1 (avg) | åŠŸè€— (mW) |
|------|---------|-----------|
| FP32 (Full Precision) | 73.9 | 22.84 |
| I-ViT (W8A8) | 72.4 | 5.66 |
| **Ours (W8A8)** | **73.5** | **5.66** |

> âœ… **é‡åŒ–åæ€§èƒ½æŸå¤± <3%ï¼ŒåŠŸè€—ä»…ä¸ºå…¨ç²¾åº¦æ¨¡å‹çš„ 1/4**ï¼Œä¸”æ˜¾è‘—ä¼˜äº I-ViTã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

#### **(1) æŠ•å½±æ–¹æ³•å¯¹æ¯”ï¼ˆHuman-Cï¼‰**
| æŠ•å½±æ–¹å¼ | F1 (4-5) | Avg Recall (4-5) | TSR |
|----------|---------|------------------|-----|
| **TSKD (ours)** | **75.0** | **60.9** | **0.936** |
| PCA | 72.5 | 56.0 | 0.749 |
| Random Orthogonal | 73.8 | 57.9 | 0.694 |
| Inverse Projection | 71.7 | 55.2 | â€“ |

> âœ… **TSKD æŠ•å½±æ˜¾è‘—ä¼˜äº PCAã€éšæœºæŠ•å½±å’Œé€†æŠ•å½±**ï¼Œä¸” TSR ä¸æ€§èƒ½å¼ºç›¸å…³ã€‚

#### **(2) IND æ¶æ„æ¶ˆèï¼ˆMonkey-Rï¼‰**
| æ¨¡å‹ | RÂ² (Day 5) | RÂ² (Day 15) |
|------|------------|-------------|
| **IND (CWT + Linear Attention)** | **0.6835** | **0.5448** |
| CWT + RNN | 0.5511 | 0.3342 |
| CWT + CNN | 0.5195 | 0.2222 |
| IND (STFT) | 0.3815 | 0.4509 |
| IND (Spectrogram) | -2.276 | -0.0542 |

> âœ… **CWT ç‰¹å¾ + Linear Attention æ¶æ„è¡¨ç°æœ€ä¼˜**ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„é•¿æœŸç¨³å®šæ€§ã€‚

#### **(3) æ³¢æ®µé‡è¦æ€§åˆ†æï¼ˆHuman-Dï¼‰**
ç§»é™¤ä¸åŒé¢‘ç‡æ³¢æ®µåçš„æ€§èƒ½å˜åŒ–ï¼ˆAcc / AUROCï¼‰ï¼š
| ç§»é™¤é¢‘ç‡ | Acc â†“ | AUROC â†“ |
|--------|------|--------|
| é»˜è®¤ï¼ˆ10â€“125Hzï¼‰ | 77.1 | 85.0 |
| /{10Hz,20Hz} | 74.5 | 85.2 |
| /{40Hz,60Hz} | 76.0 | 83.7 |
| /{100Hz,125Hz} | 76.1 | 84.3 |

> âœ… **æ‰€æœ‰é¢‘æ®µï¼ˆå°¤å…¶æ˜¯ 30Hz å’Œé«˜é¢‘æ®µï¼‰å‡å¯¹è§£ç æœ‰è´¡çŒ®**ï¼Œè¡¨æ˜å¤šé¢‘å¸¦èåˆçš„é‡è¦æ€§ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **TSKD æ˜¯ä¸€ç§é«˜æ•ˆçš„è’¸é¦ç­–ç•¥**ï¼š
   - åœ¨å­¦ç”Ÿæ¨¡å‹å®¹é‡è¿œå°äºæ•™å¸ˆæ—¶ï¼Œä¼ ç»Ÿ KD æ•ˆæœæœ‰é™ï¼Œè€Œ TSKD é€šè¿‡**ä»»åŠ¡å¯¼å‘çš„ç‰¹å¾å‹ç¼©**ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°è¿ç§»å…³é”®ä¿¡æ¯ã€‚
   - **TSR æŒ‡æ ‡å¯å‡†ç¡®é¢„æµ‹è’¸é¦æ€§èƒ½**ï¼Œä¸ºæŠ•å½±å™¨è®¾è®¡æä¾›ç†è®ºæŒ‡å¯¼ã€‚

2. **IND æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”å¯éƒ¨ç½²çš„è§£ç å™¨**ï¼š
   - å°½ç®¡å‚æ•°å°‘ï¼ˆ30Kï¼‰ï¼ŒIND åœ¨å¤šç§ä»»åŠ¡ä¸Šä»èƒ½å–å¾—æ¥è¿‘å¤§æ¨¡å‹çš„æ€§èƒ½ã€‚
   - CWT åˆ†è¯ + Linear Attention æ¶æ„å…¼å…·**å¯è§£é‡Šæ€§**ä¸**è®¡ç®—æ•ˆç‡**ã€‚

3. **æ•´æ•°ä»…æ¨ç†å¯è¡Œä¸”é«˜æ•ˆ**ï¼š
   - é€šè¿‡å¯å­¦ä¹ è£å‰ªèŒƒå›´çš„ QATï¼Œå®ç°äº† W8A8 ä¸‹çš„æ•´æ•°æ¨ç†ï¼Œ**æ€§èƒ½æŸå¤±æå°ï¼ˆ<3%ï¼‰**ï¼ŒåŠŸè€—é™è‡³ **5.66 mW**ï¼Œå®Œå…¨æ»¡è¶³å¯æ¤å…¥éœ€æ±‚ã€‚

4. **æ–¹æ³•å…·å¤‡å¼ºæ³›åŒ–èƒ½åŠ›**ï¼š
   - æˆåŠŸåº”ç”¨äº ECoGã€EEGã€spike å¤šç§ä¿¡å·æ¨¡æ€ã€‚
   - åœ¨è·¨ä¼šè¯ã€è·¨è¢«è¯•ä»»åŠ¡ä¸­ä¿æŒç¨³å®šæ€§èƒ½ã€‚

### **å±€é™æ€§**
- å½“å‰ TSKD ä¾èµ–äºä¸€ä¸ªå›ºå®šçš„æ•™å¸ˆæ¨¡å‹ï¼Œè‹¥æ•™å¸ˆæœ¬èº«æ€§èƒ½ä¸ä½³ï¼Œåˆ™è’¸é¦æ•ˆæœå—é™ã€‚
- IND æ¶æ„ç›®å‰é’ˆå¯¹çŸ­æ—¶åºä¿¡å·è®¾è®¡ï¼Œåœ¨å¤„ç†é•¿åºåˆ—ä»»åŠ¡æ—¶å¯èƒ½éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
- å®éªŒä¸»è¦åŸºäºç¦»çº¿æ•°æ®åˆ†æï¼ŒçœŸå®åœ¨çº¿æ¤å…¥ç¯å¢ƒä¸‹çš„é²æ£’æ€§æœ‰å¾…éªŒè¯ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- æ¢ç´¢**åŠ¨æ€æ›´æ–°çš„æ•™å¸ˆæ¨¡å‹**æœºåˆ¶ï¼Œå®ç°é•¿æœŸè‡ªé€‚åº”ã€‚
- å°† BrainDistill æ¡†æ¶æ‰©å±•è‡³**é—­ç¯ç¥ç»è°ƒæ§ç³»ç»Ÿ**ï¼ˆå¦‚è„‘è„Šæ¥å£ï¼‰ã€‚
- å¼€å‘ä¸“ç”¨ ASIC èŠ¯ç‰‡ï¼Œé›†æˆ ADCã€IND è§£ç å™¨ä¸åˆºæ¿€æ¨¡å—ï¼Œæ„å»ºå®Œæ•´çš„ SoC ç³»ç»Ÿã€‚
- æ¨åŠ¨ä¸´åºŠè¯•éªŒï¼ŒéªŒè¯å…¶åœ¨ç˜«ç—ªæ‚£è€…ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

> **æ€»ç»“**ï¼š  
> **BrainDistill** æä¾›äº†ä¸€ä¸ªä»ç®—æ³•åˆ°ç¡¬ä»¶éƒ¨ç½²çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œé¦–æ¬¡å®ç°äº†é«˜æ€§èƒ½ã€å°ä½“ç§¯ã€ä½åŠŸè€—çš„å¯æ¤å…¥ç¥ç»è§£ç ç³»ç»Ÿã€‚å®ƒä¸ä»…æ¨åŠ¨äº† BCI åŸºç¡€æ¨¡å‹çš„å®ç”¨åŒ–è¿›ç¨‹ï¼Œä¹Ÿä¸ºæœªæ¥æ™ºèƒ½ç¥ç»å‡ä½“çš„å‘å±•å¥ å®šäº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚

</details>

---

### 7. [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)

**Authors**: Zihan Wang, Cheng Tang, Lei Gong, Cheng Li, Chao Wang, teng wang, Wenqi Lou, Xuehai Zhou  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.16986v1  

#### Abstract
Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly importa...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
åœ¨ **Chain-of-Thought (CoT)** æ¨ç†ä¸­ï¼ŒLLM éœ€è¦ç”Ÿæˆå¤§é‡ä¸­é—´â€œæ€è€ƒâ€tokenï¼ˆthink-stage tokensï¼‰ï¼Œè¿™äº› token è¢«å­˜å‚¨åœ¨ **KV Cache** ä¸­ä»¥ä¾›åç»­ attention ä½¿ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›ä¸­é—´ token å¤§éƒ¨åˆ†å¯¹æœ€ç»ˆç­”æ¡ˆæ— ç›´æ¥è´¡çŒ®ï¼Œå´å ç”¨äº†å·¨å¤§çš„æ˜¾å­˜ï¼ˆHBMï¼‰èµ„æºï¼Œå¯¼è‡´ï¼š
- å†…å­˜å¼€é”€å‰§å¢ï¼ˆä¾‹å¦‚ 10GB+ï¼‰
- ååé‡ä¸‹é™
- ç”¨æˆ·å“åº”å»¶è¿Ÿå‡é«˜

ä¼ ç»Ÿ KV Cache å‹ç¼©æ–¹æ³•ï¼ˆå¦‚ SnapKVã€H2Oã€StreamingLLMï¼‰åŸºäº **token-uniform principle**ï¼Œå³å¹³ç­‰å¯¹å¾…æ‰€æœ‰è¾“å‡º tokenï¼Œè¿™åœ¨ CoT åœºæ™¯ä¸‹ä¸é€‚ç”¨â€”â€”å› ä¸ºç”¨æˆ·åªå…³å¿ƒ **final answer**ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ ¸å¿ƒæ€æƒ³
æœ¬æ–‡æå‡º **Crystal-KV**ï¼Œä¸€ä¸ªä¸“ä¸º CoT æ¨ç†è®¾è®¡çš„é«˜æ•ˆ KV Cache ç®¡ç†æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯ **Answer-First Principle** â€”â€” åªä¿ç•™é‚£äº›çœŸæ­£æœ‰åŠ©äºç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„ KV entriesã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š

1. **é¦–æ¬¡è¯†åˆ«å¹¶åŒºåˆ†ä¸¤ç§ç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å¼ï¼ˆUnified Attention Patternsï¼‰**ï¼š
   - **CrystalKV**ï¼šé—´æ­‡æ€§è¢«å…³æ³¨ä½†åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­æŒç»­å½±å“æœ€ç»ˆç­”æ¡ˆçš„ KV entriesã€‚å®ƒä»¬æ˜¯â€œå…³é”®è·¯å¾„â€ä¸Šçš„è®°å¿†ã€‚
   - **SlipKV**ï¼šå‘ˆç°æµå¼æ³¨æ„åŠ›æ¨¡å¼ï¼ˆstreaming patternï¼‰ï¼Œä¸»è¦ç”¨äºç»´æŒæ¨ç†æµç¨‹ï¼Œä½†å¯èƒ½å¼•å…¥è¯¯å¯¼ä¿¡æ¯ã€‚

2. **Attention-based LRFU ç®—æ³•**ï¼š
   - åŸºäº **Least Recently Frequently Used (LRFU)** æ€æƒ³ï¼Œç»“åˆ attention åˆ†æ•°åŠ¨æ€è®¡ç®—æ¯ä¸ª KV entry çš„é‡è¦æ€§ï¼ˆCRF Scoreï¼‰ã€‚
   - ä¸ä¸»åŠ¨é”å®š CrystalKVï¼Œè€Œæ˜¯é€šè¿‡æ¼”åŒ–è§†è§’å°†æ–°æ¥çš„ KV è§†ä¸º **PotentialKV**ï¼Œé€æ­¥æ·˜æ±°æ¼”å˜ä¸º SlipKV çš„æ¡ç›®ï¼Œè‡ªç„¶ç•™ä¸‹ CrystalKVã€‚
   - åˆ©ç”¨ Top-p é‡‡æ ·æ„å»º hit maskï¼Œå®ç°è½»é‡çº§ attention æ•è·ã€‚

3. **è‡ªé€‚åº”ç¼“å­˜é¢„ç®—åˆ†é…ç®—æ³•ï¼ˆAdaptive Budget Allocationï¼‰**ï¼š
   - åŠ¨æ€è°ƒæ•´æ¯å±‚/æ¯å¤´çš„ KV Cache é¢„ç®—ã€‚
   - åŸºäº CRF å¾—åˆ†ä¼°ç®—å„ layer/head çš„ **cache utilization**ï¼Œä¼˜å…ˆæ”¾å¤§é«˜åˆ©ç”¨ç‡ç»„ä»¶çš„é¢„ç®—ï¼Œæå‡æ•´ä½“ç©ºé—´åˆ©ç”¨æ•ˆç‡ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ H2O/SnapKVï¼‰ | R-KV / RaaS | Crystal-KV |
|------|--------------------------|-------------|------------|
| æ˜¯å¦éµå¾ª Answer-First | âŒ | âŒï¼ˆä»è§†ä¸º LCG ç‰¹ä¾‹ï¼‰ | âœ… |
| æ˜¯å¦åŒºåˆ† CrystalKV vs SlipKV | âŒ | âŒ | âœ… |
| æ˜¯å¦æ”¯æŒåŠ¨æ€é¢„ç®—è°ƒæ•´ | âŒ | âŒ | âœ… |
| åœ¨ä½é¢„ç®—ä¸‹èƒ½å¦ä¿æŒç”šè‡³è¶…è¶Š FullKV å‡†ç¡®ç‡ | âŒ | âŒ | âœ…ï¼ˆå¯è¾¾ 105% FullKV accuracyï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **ç¼–ç¨‹ä»»åŠ¡**ï¼š`CodeForces` benchmarkï¼ˆ10K ç«èµ›çº§ç¼–ç¨‹é¢˜ï¼‰ï¼Œéš¾åº¦é™åˆ¶åœ¨ <1500 ratingï¼Œé¿å…å‡†ç¡®ç‡é¥±å’Œæˆ–å½’é›¶ã€‚
- **æ•°å­¦ä»»åŠ¡**ï¼š`MATH-500`ï¼ˆé«˜çº§ç«èµ›çº§æ•°å­¦é¢˜ï¼‰ï¼Œæ¶µç›–ä»£æ•°ã€å‡ ä½•ã€ç»„åˆç­‰å¤æ‚æ¨ç†åœºæ™¯ã€‚

### âš™ï¸ æ¨¡å‹ä¸ç¡¬ä»¶å¹³å°
- **æ¨¡å‹**ï¼šä¸‰ä¸ªå¼€æºçš„ DeepSeek-R1-distilled æ¨¡å‹ï¼š
  - Llama-8B
  - Qwen-14B
  - Qwen-32B
- **ç¡¬ä»¶**ï¼šä¸‰å— NVIDIA RTX PRO 6000 Blackwell GPU
- **æ¨ç†é•¿åº¦**ï¼šå¹³å‡ 9,350ï¼ˆ8K-levelï¼‰ å’Œ 18,700ï¼ˆ16K-levelï¼‰tokens

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Accuracy** | å¹³å‡æ­£ç¡®ç‡ï¼Œé‡‡ç”¨ `k=8` ä¸ªæ ·æœ¬æŠ•ç¥¨ï¼Œå¾—åˆ† = $\sum p_i$ï¼Œå…¶ä¸­ $p_i$ æ˜¯ç¬¬ i ä¸ªç­”æ¡ˆæ˜¯å¦æ­£ç¡® |
| **HBM Saving (%)** | æ˜¾å­˜èŠ‚çœæ¯”ä¾‹ |
| **Throughput (tok/s)** | å•ä½æ—¶é—´å¤„ç† token æ•°é‡ |
| **Batch Size (max)** | æœ€å¤§å¹¶å‘ batch æ•° |
| **Response Latency Speedup** | ç”¨æˆ·çº§å“åº”å»¶è¿ŸåŠ é€Ÿæ¯” |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æ˜¯å¦é€‚é… CoT | æ˜¯å¦æ”¯æŒåŠ¨æ€é¢„ç®— |
|------|------|---------------|------------------|
| FullKV | æ— å‹ç¼© | â€” | âŒ |
| StreamingLLM | LCG å‹ç¼© | âŒ | âŒ |
| H2O | LCG å‹ç¼© | âŒ | âŒ |
| SnapKV | LCG å‹ç¼© | âŒ | âŒ |
| R-KV | CoT-oriented | âœ…ï¼ˆä½†ä»æ˜¯ LCG æ‰©å±•ï¼‰ | âŒ |
| RaaS | CoT-oriented | âœ… | âŒ |
| **Crystal-KV.Lite** | æœ¬æ–‡å˜ä½“ï¼ˆå…³é—­åŠ¨æ€é¢„ç®—ï¼‰ | âœ… | âŒ |
| **Ada-SnapKV** | SnapKV + åŠ¨æ€é¢„ç®—ï¼ˆäººå·¥å¢å¼ºï¼‰ | âœ… | âœ…ï¼ˆä»…æ­¤ä¸€ä¸ªå¯æ¯”ï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

| æŒ‡æ ‡ | Crystal-KV è¡¨ç° |
|------|----------------|
| **å¹³å‡æ˜¾å­˜èŠ‚çœ** | **90.89%** |
| **å¹³å‡ååé‡æå‡** | **7.57Ã—** |
| **æœ€å¤§å“åº”å»¶è¿ŸåŠ é€Ÿ** | **1.24Ã—** |
| **æœ€ä½é¢„ç®—ä¸‹å‡†ç¡®ç‡ï¼ˆvs FullKVï¼‰** | è¾¾åˆ°ç”šè‡³è¶…è¿‡ FullKVï¼ˆæœ€é«˜è¾¾ **105%**ï¼‰ |
| **æœ€å¤§å¹¶å‘ batch æå‡** | æœ€é«˜è¾¾ **8.07Ã—**ï¼ˆ8K-levelï¼‰ å’Œ **15.16Ã—**ï¼ˆ16K-levelï¼‰ |

### ğŸ”¬ è¯¦ç»†å¯¹æ¯”ç»“æœï¼ˆå›¾4 & è¡¨Iï¼‰

#### âœ… å‡†ç¡®ç‡ä¼˜åŠ¿ï¼ˆFig. 4ï¼‰
- åœ¨ç›¸åŒ KV budget ä¸‹ï¼ŒCrystal-KV æ˜¾è‘—ä¼˜äºæ‰€æœ‰ baselineï¼š
  - **ç¼–ç¨‹ä»»åŠ¡**ï¼šå¹³å‡é«˜å‡º 7.30% ~ 18.98%
  - **æ•°å­¦ä»»åŠ¡**ï¼šå¹³å‡é«˜å‡º 7.97% ~ 23.87%
- å­˜åœ¨ä¸€ä¸ªâ€œå³°å€¼ç‚¹â€ï¼šå½“ budget â‰ˆ CrystalKV æ€»é‡æ—¶ï¼Œå‡†ç¡®ç‡**è¶…è¿‡ FullKV**ï¼Œè¯´æ˜å»é™¤ SlipKV å¯å‡å°‘å¹²æ‰°ã€‚
- è‡ªé€‚åº”é¢„ç®—ï¼ˆadaptive allocationï¼‰è¿›ä¸€æ­¥æå‡äº†æä½é¢„ç®—ä¸‹çš„è¡¨ç°ã€‚

#### ğŸ’¾ æ•ˆç‡ä¸èµ„æºåˆ©ç”¨ï¼ˆTable Iï¼‰
| è®¾ç½® | æ–¹æ³• | Batch Max | Throughput (tok/s) | Tokens Concurrent |
|------|------|-----------|---------------------|--------------------|
| 8K-level | FullKV | 51 | 458.67 | 476,850 |
| 8K-level | Crystal-KV (fixed-1024) | **379** | **2,297.37** (+5.01Ã—) | **3,543,650** (+7.43Ã—) |
| 8K-level | Crystal-KV (ratio-10%) | **412** | **2,478.39** (+5.40Ã—) | **3,852,200** (+8.07Ã—) |
| 16K-level | FullKV | 25 | 174.95 | 467,500 |
| 16K-level | Crystal-KV (fixed-1024) | **379** | **2,141.44** (+12.24Ã—) | **7,087,300** (+15.16Ã—) |

> ğŸ’¡ ç»“è®ºï¼šéšç€åºåˆ—å¢é•¿ï¼ŒCrystal-KV çš„ä¼˜åŠ¿æ„ˆå‘æ˜æ˜¾ï¼Œå°¤å…¶é€‚åˆé•¿é“¾å¤æ‚æ¨ç†ã€‚

### ğŸ” æ¶ˆèå®éªŒä¸å‚æ•°åˆ†æï¼ˆFig. 5ï¼‰

- **Decay Rate Î»**ï¼š
  - æœ€ä½³èŒƒå›´ï¼šä»£ç ä»»åŠ¡ [0.6â€“0.7]ï¼Œæ•°å­¦ä»»åŠ¡ [0.5â€“0.6]
  - Î» è¿‡å° â†’ CrystalKV å›  attention gap è¢«è¯¯åˆ 
  - Î» è¿‡å¤§ â†’ SlipKV è¢«è¿‡åº¦ä¿ç•™ï¼ŒæŒ¤å ç©ºé—´
- **Top-p threshold**ï¼š
  - æ¨èä½¿ç”¨ **top-p=0.9**ï¼Œèƒ½æ›´å¥½ä¿ç•™å…³é”®ä¸Šä¸‹æ–‡
  - p < 0.7 å¯¼è‡´ attention æ³¢åŠ¨å‰§çƒˆï¼Œæ€§èƒ½ä¸ç¨³å®š

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Answer-First Principle æˆç«‹**ï¼š
   - æœ€ç»ˆç­”æ¡ˆç¡®å®åªä¾èµ–å°‘æ•°é—´æ­‡æ€§è¢«å…³æ³¨çš„ KV entriesï¼ˆCrystalKVï¼‰ï¼Œè€Œéå…¨éƒ¨ think-stage å†…å®¹ã€‚
   - æŠ•å½± answer-query åˆ° think-stage attention map å¯æœ‰æ•ˆè¯†åˆ« CrystalKVã€‚

2. **SlipKV å¯å®‰å…¨ç§»é™¤**ï¼š
   - å°½ç®¡ SlipKV ç»´æŒçŸ­æœŸæ¨ç†è¿è´¯æ€§ï¼Œä½†é•¿æœŸæ¥çœ‹å¹¶éå¿…è¦ï¼›æ¸…é™¤ååè€Œå¯èƒ½æé«˜å‡†ç¡®ç‡ï¼ˆæ¶ˆé™¤å™ªå£°ï¼‰ã€‚

3. **åŠ¨æ€é¢„ç®—åˆ†é…æ˜¾è‘—å¢ç›Š**ï¼š
   - ä¸åŒ layer/head å¯¹æ¨ç†è´¡çŒ®å¼‚è´¨æ€§å¼ºï¼Œé™æ€åˆ†é…æµªè´¹ä¸¥é‡ã€‚
   - è‡ªé€‚åº”æœºåˆ¶å¯åœ¨æä½æ€»é¢„ç®—ä¸‹æœ€å¤§åŒ–å…³é”®ç»„ä»¶å®¹é‡ã€‚

4. **æè‡´å‹ç¼©ä»å¯ lossless ç”šè‡³ better-than-full**ï¼š
   - ä»…ç”¨ **10% KV budget** å³å¯è¾¾åˆ° FullKV ç”šè‡³æ›´é«˜å‡†ç¡®ç‡ï¼Œæ‰“ç ´â€œè¶Šå¤šè¶Šå¥½â€çš„ç›´è§‰ã€‚

### âš ï¸ å±€é™æ€§
- å½“å‰æ–¹æ³•ä¾èµ– attention score è¿‘ä¼¼ï¼ˆvia Top-pï¼‰ï¼Œæœªè€ƒè™‘ query evolution çš„éçº¿æ€§å½±å“ã€‚
- å¯¹æåº¦åˆ†æ•£çš„çŸ¥è¯†ç±»ä»»åŠ¡ï¼ˆå¦‚æŸäº›æ•°å­¦é¢˜ï¼‰æ•ˆæœç•¥é€Šï¼Œå›  CrystalKV åˆ†å¸ƒæ›´å¹¿ã€‚
- å‚æ•°æ•æ„Ÿæ€§å­˜åœ¨ï¼ˆÎ» å’Œ top-pï¼‰ï¼Œéœ€åˆç†è°ƒå‚ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- å½¢å¼åŒ–å»ºæ¨¡ CrystalKV ä¸ SlipKV çš„ç”Ÿæˆæœºåˆ¶ï¼Œç•Œå®š KV å‹ç¼©çš„èƒ½åŠ›è¾¹ç•Œã€‚
- æ¢ç´¢è®­ç»ƒé˜¶æ®µå¼•å¯¼æ¨¡å‹å½¢æˆæ›´ç´§å‡‘çš„ CrystalKV ç»“æ„ã€‚
- å°†è¯¥èŒƒå¼æ‰©å±•è‡³å¤šè·³æ£€ç´¢ã€agent planning ç­‰æ›´å¤æ‚çš„ reasoning pipelineã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> *Crystal-KV* é€šè¿‡ **Answer-First Principle** é‡æ–°å®šä¹‰äº† CoT åœºæ™¯ä¸‹çš„ KV Cache ç®¡ç†èŒƒå¼ï¼Œå®ç°äº† **è¶…é«˜å‹ç¼©æ¯” + æ›´é«˜å‡†ç¡®ç‡ + æå¤§ååæå‡**ï¼Œä¸ºå¤§è§„æ¨¡æ¨ç†ç³»ç»Ÿçš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†æ–°è·¯å¾„ã€‚

</details>

---

### 8. [CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation](https://arxiv.org/abs/2601.17774)

**Authors**: Zizhao Zhang, Yihan Xue, Haotian Zhu, Sijia Li, Zhijun Wang, Yujie Xiao  
**Category**: cs.DC  
**Published**: 2026-01-27  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.17774v1  

#### Abstract
Distributed Graph Neural Network (GNN) training suffers from substantial communication overhead due to the inherent neighborhood dependency in graph-structured data. This neighbor explosion problem requires workers to frequently exchange boundary node features across partitions, creating a communica...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šCondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
åˆ†å¸ƒå¼ Graph Neural Networkï¼ˆGNNï¼‰è®­ç»ƒé¢ä¸´ä¸¥é‡çš„**é€šä¿¡å¼€é”€ç“¶é¢ˆ**ï¼Œä¸»è¦æºäºå›¾æ•°æ®ä¸­çš„é‚»åŸŸä¾èµ–æ€§ï¼ˆneighborhood dependencyï¼‰ã€‚åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œå›¾è¢«åˆ’åˆ†åˆ°å¤šä¸ª worker ä¸Šï¼Œä½äºåˆ†åŒºè¾¹ç•Œçš„èŠ‚ç‚¹ï¼ˆboundary nodesï¼‰éœ€è¦é¢‘ç¹ä»è¿œç¨‹ worker è·å–é‚»å±…ç‰¹å¾ï¼Œå¯¼è‡´è·¨èŠ‚ç‚¹é€šä¿¡é‡å·¨å¤§ï¼Œå è®­ç»ƒæ€»æ—¶é—´çš„ 50â€“90%ã€‚è¿™ä¸€â€œ**é‚»å±…çˆ†ç‚¸**â€ï¼ˆneighbor explosionï¼‰é—®é¢˜ä¸¥é‡é™åˆ¶äº†è®­ç»ƒçš„å¯æ‰©å±•æ€§ã€‚

ç°æœ‰æ–¹æ³•å¦‚é™æ€å›¾åˆ’åˆ†ï¼ˆå¦‚ METISï¼‰æ— æ³•é€‚åº”åŠ¨æ€ç½‘ç»œæ¡ä»¶ï¼Œè€Œé€šç”¨æ¢¯åº¦å‹ç¼©æŠ€æœ¯æœªèƒ½å……åˆ†åˆ©ç”¨å›¾æ•°æ®çš„ç»“æ„ç‰¹æ€§ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**
è®ºæ–‡æå‡ºäº† **CondenseGraph**ï¼Œä¸€ç§é¢å‘åˆ†å¸ƒå¼ GNN è®­ç»ƒçš„**é€šä¿¡é«˜æ•ˆæ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **On-the-Fly Graph Condensationï¼ˆè¿è¡Œæ—¶å›¾å‹ç¼©ï¼‰**  
  åœ¨é€šä¿¡å‰ï¼Œå°†è¾¹ç•ŒèŠ‚ç‚¹çš„ç‰¹å¾åŠ¨æ€èšåˆæˆç´§å‡‘çš„â€œ**super node**â€ï¼ˆè¶…èŠ‚ç‚¹ï¼‰ï¼Œæ˜¾è‘—å‡å°‘ä¼ è¾“æ•°æ®é‡ã€‚è¯¥è¿‡ç¨‹æ˜¯è½»é‡çº§ã€å®æ—¶æ‰§è¡Œçš„ï¼Œä¸åŒäºç¦»çº¿å›¾è’¸é¦ï¼ˆgraph distillationï¼‰æ–¹æ³•ã€‚

- **Gradient-based Error Compensationï¼ˆåŸºäºæ¢¯åº¦çš„è¯¯å·®è¡¥å¿æœºåˆ¶ï¼‰**  
  ä¸ºç¼“è§£å‹ç¼©å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ï¼Œå¼•å…¥è¯¯å·®ç´¯ç§¯å™¨ï¼ˆerror accumulatorï¼‰ï¼Œå°†æœªæˆåŠŸä¼ è¾“çš„å‹ç¼©è¯¯å·®åé¦ˆåˆ°åç»­è¿­ä»£ä¸­ï¼Œç¡®ä¿é‡è¦æ¢¯åº¦ä¿¡æ¯ä¸ä¸¢å¤±ï¼Œç»´æŒæ”¶æ•›æ€§ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| æ–¹é¢ | CondenseGraph | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ METISã€AdaQPã€BNS-GCNï¼‰ |
|------|---------------|-------------------------------|
| **é€‚åº”æ€§** | åŠ¨æ€é€‚åº”ç½‘ç»œå˜åŒ– | é™æ€ç­–ç•¥ï¼Œæ— æ³•åŠ¨æ€è°ƒæ•´ |
| **ç»“æ„åˆ©ç”¨** | åˆ©ç”¨å›¾åŒè´¨æ€§ï¼ˆhomophilyï¼‰è¿›è¡Œç‰¹å¾å‹ç¼© | é€šç”¨å‹ç¼©ï¼Œå¿½ç•¥å›¾ç»“æ„ |
| **ç²¾åº¦ä¿æŒ** | é€šè¿‡è¯¯å·®åé¦ˆç»´æŒé«˜ç²¾åº¦ | å‹ç¼©æ˜“å¯¼è‡´ç²¾åº¦ä¸‹é™ |
| **å…¼å®¹æ€§** | å¯ä¸å›¾åˆ’åˆ†ã€é‡åŒ–ç­‰æ–¹æ³•ç»“åˆ | å¤šä¸ºå•ä¸€ä¼˜åŒ–è·¯å¾„ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
å®éªŒåœ¨å››ä¸ªæ ‡å‡† benchmark æ•°æ®é›†ä¸Šè¿›è¡Œï¼š

| æ•°æ®é›† | èŠ‚ç‚¹æ•° | è¾¹æ•° | ç‰¹å¾ç»´åº¦ | ç±»åˆ«æ•° |
|--------|--------|------|----------|--------|
| **Reddit** | 232,965 | 114.6M | 602 | 41 |
| **ogbn-arxiv** | 169,343 | 1.17M | 128 | 40 |
| **ogbn-products** | 2.45M | 61.9M | 100 | 47 |
| **Flickr** | 89,250 | 899,756 | 500 | 7 |

> æ³¨ï¼šReddit ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬ï¼ˆçº¦ 1.14 äº¿æ¡è¾¹ï¼‰ï¼Œéç®€åŒ–ç‰ˆã€‚

---

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**

- **å®ç°å¹³å°**ï¼šåŸºäº **DistDGL** æ„å»ºï¼Œä½¿ç”¨ 8 ä¸ª workerï¼Œæ¯ä¸ªé…å¤‡ NVIDIA V100 GPUã€‚
- **æ¨¡å‹æ¶æ„**ï¼š2-layer **GraphSAGE**ï¼Œéšè—å±‚ç»´åº¦ 256ã€‚
- **å‹ç¼©ç‡**ï¼šé»˜è®¤è®¾ç½®ä¸º 50%ï¼ˆå³é€šä¿¡é‡å‡å°‘ä¸€åŠï¼‰ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **é€šä¿¡é‡ï¼ˆCommunication Volumeï¼‰**
  - **è®­ç»ƒæ—¶é—´åˆ†è§£**ï¼ˆé€šä¿¡ vs è®¡ç®—ï¼‰
  - **æµ‹è¯•å‡†ç¡®ç‡ï¼ˆTest Accuracyï¼‰**
  - **æ”¶æ•›é€Ÿåº¦**ï¼ˆConvergence Curveï¼‰
  - **æ¶ˆèå®éªŒ**ï¼ˆAblation Studyï¼‰

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **Baseline**ï¼šæ ‡å‡†åˆ†å¸ƒå¼ GNN è®­ç»ƒï¼Œå…¨ç²¾åº¦é€šä¿¡
- **AdaQP**ï¼šè‡ªé€‚åº”é‡åŒ– + å¹¶è¡ŒåŒ–
- **BNS-GCN**ï¼šè¾¹ç•ŒèŠ‚ç‚¹é‡‡æ ·æ–¹æ³•

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… **é€šä¿¡æ•ˆç‡æå‡**
- åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå®ç° **40â€“60% çš„é€šä¿¡é‡å‡å°‘**ã€‚
- åœ¨ `ogbn-products` ä¸Šï¼Œæ¯ epoch é€šä¿¡é‡ä» **15.2 GB é™è‡³ 6.2 GB**ï¼ˆâ†“59%ï¼‰ã€‚
- é€šä¿¡æ—¶é—´å æ¯”ä» **55% é™è‡³ 22%**ï¼Œæ˜¾è‘—ç¼“è§£é€šä¿¡ç“¶é¢ˆã€‚

#### âœ… **æ¨¡å‹ç²¾åº¦ä¿æŒ**
| æ–¹æ³• | Reddit | arxiv | products | Flickr |
|------|--------|-------|----------|--------|
| Baseline | 96.2% | 71.5% | 78.3% | 51.8% |
| **Ours (attention)** | **96.1%** | **71.4%** | **78.1%** | **51.6%** |

- æ‰€æœ‰å˜ä½“ç²¾åº¦æŸå¤±å‡ **< 0.5%**ï¼Œä¼˜äº AdaQP å’Œ BNS-GCNã€‚
- **Attention-based condensation** è¡¨ç°æœ€ä½³ï¼Œå°¤å…¶åœ¨å¼‚æ„åº¦é«˜çš„å›¾ä¸Šã€‚

#### âœ… **æ”¶æ•›æ€§åˆ†æ**
- æ”¶æ•›é€Ÿåº¦ä¸ Baseline å‡ ä¹ä¸€è‡´ï¼ŒéªŒè¯ç†è®ºåˆ†æã€‚
- **æ— è¯¯å·®è¡¥å¿æ—¶**ï¼Œåœ¨ 50% å‹ç¼©ç‡ä¸‹ç²¾åº¦ä¸‹é™ **2â€“3%**ï¼Œè¯´æ˜è¯¯å·®åé¦ˆæœºåˆ¶è‡³å…³é‡è¦ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

| å®éªŒé¡¹ | å‘ç° |
|--------|------|
| **å‹ç¼©ç‡å½±å“** | 40â€“60% åŒºé—´å†…ç²¾åº¦ç¨³å®šï¼ˆæŸå¤± <1%ï¼‰ï¼›è¶…è¿‡ 60% åæ€§èƒ½æ˜æ˜¾ä¸‹é™ |
| **è¯¯å·®è¡¥å¿ä½œç”¨** | ç§»é™¤åç²¾åº¦ä¸‹é™ 2.1%ï¼ˆogbn-productsï¼‰ï¼Œä½†å­˜å‚¨å¼€é”€å¯å¿½ç•¥ï¼ˆ<1%ï¼‰ |
| **å‹ç¼©æ–¹å¼æ¯”è¾ƒ** | <ul><li>**Mean**ï¼šæœ€ç®€å•ï¼Œç²¾åº¦ç•¥ä½</li><li>**Weighted**ï¼šè€ƒè™‘èŠ‚ç‚¹åº¦ï¼Œå¹³è¡¡æ€§å¥½</li><li>**Attention**ï¼šç²¾åº¦æœ€é«˜ï¼Œè®¡ç®—å¼€é”€ +5%</li></ul> |

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦ç»“è®º**
1. **CondenseGraph æ˜¾è‘—é™ä½é€šä¿¡å¼€é”€**ï¼ˆ40â€“60%ï¼‰ï¼ŒåŒæ—¶å‡ ä¹ä¸ç‰ºç‰²æ¨¡å‹ç²¾åº¦ï¼ˆ<0.5% å·®è·ï¼‰ã€‚
2. **è¿è¡Œæ—¶å›¾å‹ç¼© + è¯¯å·®åé¦ˆæœºåˆ¶** æ˜¯å®ç°é«˜æ•ˆä¸”ç¨³å®šè®­ç»ƒçš„å…³é”®ã€‚
3. æ‰€ææ–¹æ³•å…·æœ‰**è‰¯å¥½é€šç”¨æ€§**ï¼Œå¯ä¸ç°æœ‰å›¾åˆ’åˆ†ã€é‡åŒ–ç­‰æŠ€æœ¯ç»“åˆä½¿ç”¨ã€‚
4. å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨å¤§è§„æ¨¡å›¾ï¼ˆå¦‚ ogbn-productsï¼‰ä¸Šä»èƒ½ä¿æŒé«˜æ•ˆä¸ç¨³å®šã€‚

---

### **å±€é™æ€§**
1. **è¾¹ç•ŒèŠ‚ç‚¹åˆ†ç»„ç­–ç•¥** å½“å‰ä¸ºå¯å‘å¼ï¼ˆåŸºäºç»“æ„ä¸ç‰¹å¾ç›¸ä¼¼æ€§ï¼‰ï¼Œå¯èƒ½å› æ•°æ®é›†è€Œå¼‚ï¼Œç¼ºä¹ç»Ÿä¸€æœ€ä¼˜ç­–ç•¥ã€‚
2. å°šæœªæ‰©å±•è‡³ **heterogeneous graphs**ï¼ˆå¼‚æ„å›¾ï¼‰ï¼Œå¤šç±»å‹èŠ‚ç‚¹çš„å‹ç¼©æ›´å…·æŒ‘æˆ˜ã€‚
3. ä¸åŒ GNN æ¶æ„ï¼ˆå¦‚ GATã€GCNã€GINï¼‰å¯¹å‹ç¼©çš„æ•æ„Ÿæ€§æœ‰å¾…è¿›ä¸€æ­¥ç ”ç©¶ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. è®¾è®¡**å¯å­¦ä¹ çš„è¾¹ç•ŒèŠ‚ç‚¹åˆ†ç»„ç­–ç•¥**ï¼ˆlearnable groupingï¼‰ã€‚
2. æ‰©å±•è‡³ **dynamic graphs** ä¸ **heterogeneous graphs** åœºæ™¯ã€‚
3. æ¢ç´¢ä¸å…¶ä»–å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚é‡åŒ–ã€ç¨€ç–åŒ–ï¼‰çš„è”åˆä¼˜åŒ–ã€‚
4. åœ¨çœŸå®äº‘ç¯å¢ƒï¼ˆå¸¦å®½æ³¢åŠ¨ã€å»¶è¿Ÿå˜åŒ–ï¼‰ä¸­éƒ¨ç½²å¹¶è¯„ä¼°è‡ªé€‚åº”èƒ½åŠ›ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **CondenseGraph é€šè¿‡è¿è¡Œæ—¶å›¾å‹ç¼©ä¸è¯¯å·®åé¦ˆï¼Œåœ¨ä¸ç‰ºç‰²ç²¾åº¦çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾ 60% çš„é€šä¿¡å‰Šå‡ï¼Œä¸ºå¤§è§„æ¨¡åˆ†å¸ƒå¼ GNN è®­ç»ƒæä¾›äº†é«˜æ•ˆã€å®ç”¨çš„æ–°èŒƒå¼ã€‚**

</details>

---

### 9. [A Universal Load Balancing Principle and Its Application to Large Language Model Serving](https://arxiv.org/abs/2601.17855)

**Authors**: Zixi Chen, Tianci Bu, Chendong Song, Xin Lu, Yinyu Ye, Zijie Zhou  
**Category**: cs.DC  
**Published**: 2026-01-27  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.17855v1  

#### Abstract
Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (L...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šA Universal Load Balancing Principle and Its Application to Large Language Model Serving

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
æœ¬æ–‡é’ˆå¯¹ **Large Language Model (LLM) Serving** ä¸­ä¸€ä¸ªæ—¥ç›Šä¸¥é‡çš„æ•ˆç‡ç“¶é¢ˆâ€”â€”**Decode é˜¶æ®µçš„è´Ÿè½½ä¸å¹³è¡¡é—®é¢˜**ã€‚

åœ¨ç°ä»£ LLM æ¨ç†ä¸­ï¼Œè§£ç ï¼ˆdecodeï¼‰é˜¶æ®µé‡‡ç”¨ **Data Parallelism (DP)**ï¼Œæ¯ä¸ª GPU å·¥ä½œèŠ‚ç‚¹ç»´æŠ¤å…¶è¯·æ±‚æ‰¹æ¬¡çš„ KV Cacheã€‚ç”±äº **KV Cache æ— æ³•è½»æ˜“è¿ç§»**ï¼ˆsticky assignmentï¼‰ï¼Œä¸”æ‰€æœ‰èŠ‚ç‚¹å¿…é¡»é€šè¿‡ **Barrier Synchronization** ç­‰å¾…æœ€æ…¢çš„èŠ‚ç‚¹å®Œæˆï¼Œå¯¼è‡´â€œstragglerâ€é—®é¢˜ä¸¥é‡ã€‚ç”Ÿäº§ç¯å¢ƒä¸­çš„è¿½è¸ªæ•°æ®æ˜¾ç¤ºï¼Œ**Barrier-induced idle time è¶…è¿‡ 40%**ï¼Œé€ æˆäº†å·¨å¤§çš„è®¡ç®—èµ„æºå’Œèƒ½æºæµªè´¹ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„è´Ÿè½½å‡è¡¡åŸåˆ™ï¼š**Balance Future with Integer Optimization (BF-IO)**ã€‚

è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- **ä¸ä¾èµ–å¯¹ä»»åŠ¡æ€»æ—¶é•¿çš„ç²¾ç¡®é¢„æµ‹**ï¼ˆè¿™åœ¨ LLM ä¸­æéš¾å®ç°ï¼‰ã€‚
- è½¬è€Œåˆ©ç”¨ **Short Lookahead** ä¿¡æ¯ï¼Œå³é¢„æµ‹æ´»è·ƒè¯·æ±‚åœ¨æœªæ¥å‡ æ­¥å†…æ˜¯å¦ä¼šå®Œæˆã€‚
- åœ¨æ¯ä¸€æ­¥åˆ†é…æ–°è¯·æ±‚æ—¶ï¼Œé€šè¿‡æ±‚è§£ä¸€ä¸ª **æœ‰é™æ—¶é—´çª—å£å†…çš„æ•´æ•°ä¼˜åŒ–é—®é¢˜ (Integer Optimization, IO)**ï¼Œé€‰æ‹©èƒ½ä½¿æœªæ¥å‡ æ­¥å†…ç³»ç»Ÿæ€»ä¸å¹³è¡¡åº¦æœ€å°çš„åˆ†é…æ–¹æ¡ˆã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **ç†è®ºä¿è¯å¼º**ï¼šåœ¨å¯¹æŠ—æ€§åˆ°è¾¾æ¨¡å¼ä¸‹ï¼Œè¯æ˜äº† BF-IO ç›¸å¯¹äº FCFS ç­‰åŸºçº¿ç­–ç•¥ï¼Œèƒ½å°†é•¿æœŸä¸å¹³è¡¡åº¦é™ä½ $\Omega(\sqrt{B \log G})$ å€ï¼Œå…¶ä¸­ $B$ æ˜¯æ‰¹å¤§å°ï¼Œ$G$ æ˜¯ GPU æ•°é‡ã€‚è¿™æ„å‘³ç€**ç³»ç»Ÿè§„æ¨¡è¶Šå¤§ï¼Œæ”¶ç›Šè¶Šæ˜¾è‘—**ã€‚
- **æ™®é€‚æ€§å¼º**ï¼šè¯¥åŸåˆ™ä¸ä»…é€‚ç”¨äº LLM Servingï¼Œè¿˜è¢«æ¨å¹¿åˆ°ä¸€ç±»æ›´å¹¿æ³›çš„ **non-decreasing workload drift** é—®é¢˜ã€‚
- **æ•ˆæœæ˜¾è‘—**ï¼šç›¸æ¯” Round-Robinã€JSQ ç­‰ä¼ ç»Ÿç­–ç•¥ï¼ŒBF-IO èƒ½æœ‰æ•ˆé¿å…å› è¯·æ±‚é•¿åº¦å·®å¼‚å¯¼è‡´çš„è´Ÿè½½å€¾æ–œï¼Œä»æ ¹æœ¬ä¸Šè§£å†³ Barrier åŒæ­¥ä¸‹çš„æ•ˆç‡æŸå¤±ã€‚

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
- **å…¬å¼€æ•°æ®é›†**ï¼š`BurstGPT` æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ•æ‰äº†çœŸå®ç”Ÿäº§çº§åˆ«çš„ LLM æœåŠ¡æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯è¾“å‡ºé•¿åº¦çš„é‡å°¾åˆ†å¸ƒç‰¹å¾ã€‚
- **ç§æœ‰å·¥ä¸šå·¥ä½œè´Ÿè½½**ï¼šç”¨äºéªŒè¯æ–¹æ³•åœ¨å®é™…éƒ¨ç½²ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡æ‹Ÿå™¨**ï¼šæ„å»ºäº†ä¸€ä¸ªåŸºäº GPU çš„è¯·æ±‚å¤„ç†æ¨¡æ‹Ÿå™¨ï¼Œæ¨¡æ‹Ÿåœ¨çº¿è°ƒåº¦åœºæ™¯ã€‚
- **é…ç½®**ï¼šé»˜è®¤ä½¿ç”¨ `G=32` ä¸ª GPUï¼Œæ¯ä¸ª GPU æ‰¹å¤§å° `B=72`ï¼Œæ­ç¤ºç›®æ ‡ `R=128`ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Average Imbalance**ï¼šè¡¡é‡å„ GPU è´Ÿè½½çš„å¹³å‡ä¸å¹³è¡¡åº¦ã€‚
  - **Throughput**ï¼šæ¯ç§’ç”Ÿæˆçš„ token æ•°é‡ã€‚
  - **Time Per Output Token (TPOT)**ï¼šç”¨æˆ·æ„ŸçŸ¥çš„å»¶è¿Ÿï¼Œå³æ¯ä¸ªè¾“å‡º token çš„å¹³å‡è€—æ—¶ã€‚
  - **Energy Consumption**ï¼šæ€»èƒ½è€—ï¼ˆç„¦è€³ï¼‰ï¼Œé€šè¿‡ç¬æ—¶åŠŸè€—å¯¹æ—¶é—´ç§¯åˆ†å¾—åˆ°ã€‚åŠŸè€—æ¨¡å‹ä¸º $P(mfu) = P_{idle} + (P_{max} - P_{idle}) \cdot (mfu / mfu_{sat})^\gamma$ï¼Œå…¶ä¸­ $P_{idle}=100W$, $P_{max}=400W$, $mfu_{sat}=0.45$, $\gamma=0.7$ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **FCFS (First-Come-First-Serve)**ï¼šå…ˆæ¥å…ˆæœåŠ¡ï¼Œä½œä¸ºä¸»è¦åŸºçº¿ã€‚
- **JSQ (Join-Shortest-Queue)**ï¼šé€‰æ‹©é˜Ÿåˆ—é•¿åº¦æœ€çŸ­çš„ GPUï¼Œä½†é˜Ÿåˆ—é•¿åº¦ä¸ç­‰äºå®é™…è´Ÿè½½ã€‚
- **BF-IO (H=0)**ï¼šæ— å‰ç»çª—å£çš„ BF-IOï¼Œä»…å¹³è¡¡å½“å‰æ­¥ã€‚
- **BF-IO (H=20)**ï¼šä½¿ç”¨ 20 æ­¥å‰ç»çª—å£çš„ BF-IOã€‚

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ
| Strategy | Avg Imbalance | Throughput (10â´ tok/s) | TPOT (10â»Â² s/tok) | Energy (kJ) |
| :--- | :--- | :--- | :--- | :--- |
| **FCFS** | 27.9 | 8.00 | 1.42 | 396 |
| **JSQ** | 28.2 | 7.99 | 1.42 | 396 |
| **BF-IO (H=0)** | 2.92 | 9.03 | 1.26 | 386 |
| **BF-IO (H=20)** | **1.65** | **9.13** | **1.25** | **383** |

- **è´Ÿè½½ä¸å¹³è¡¡åº¦ (Imbalance)**ï¼šBF-IO (H=20) å°†å¹³å‡ä¸å¹³è¡¡åº¦ä» 27.9 é™è‡³ **1.65**ï¼Œå‡å°‘äº† **17å€**ã€‚
- **ååé‡ (Throughput)**ï¼šç›¸æ¯” FCFSï¼ŒBF-IO (H=20) çš„ååé‡æå‡äº† **14%**ã€‚
- **å»¶è¿Ÿ (TPOT)**ï¼šç›¸æ¯” FCFSï¼ŒBF-IO (H=20) çš„ TPOT é™ä½äº† **13%**ã€‚
- **èƒ½è€— (Energy)**ï¼šç›¸æ¯” FCFSï¼ŒBF-IO (H=20) çš„æ€»èƒ½è€—ä» 396kJ é™è‡³ 383kJï¼Œå®ç°äº† **3.3%** çš„èŠ‚èƒ½ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **H=0 vs H=20**ï¼šBF-IO (H=0) å·²ç»å–å¾—äº†å·¨å¤§æå‡ï¼ˆä¸å¹³è¡¡åº¦é™è‡³ 2.92ï¼‰ã€‚å¼•å…¥ H=20 çš„å‰ç»çª—å£åï¼Œä¸å¹³è¡¡åº¦è¿›ä¸€æ­¥ä¸‹é™äº† **44%**ï¼Œå¹¶å¸¦æ¥äº†ååé‡ã€å»¶è¿Ÿå’Œèƒ½è€—çš„æŒç»­ä¼˜åŒ–ã€‚è¿™è¯æ˜äº† **Short Lookahead ä¿¡æ¯çš„æœ‰æ•ˆæ€§**ã€‚
- **JSQ è¡¨ç°ä¸ä½³**ï¼šJSQ çš„è¡¨ç°å‡ ä¹ä¸ FCFS ç›¸åŒï¼Œç”šè‡³ç•¥å·®ã€‚è¿™æ˜¯å› ä¸º JSQ ä½¿ç”¨â€œé˜Ÿåˆ—é•¿åº¦â€ä½œä¸ºä»£ç†ï¼Œè€Œå¿½ç•¥äº†è¯·æ±‚çš„å®é™… KV Cache å¤§å°ï¼Œå› æ­¤æ— æ³•æœ‰æ•ˆå¹³è¡¡çœŸå®è´Ÿè½½ã€‚

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Decode é˜¶æ®µçš„è´Ÿè½½ä¸å¹³è¡¡æ˜¯ LLM Serving çš„ä¸»è¦æ•ˆç‡ç“¶é¢ˆ**ï¼Œç”± **sticky assignment** å’Œ **barrier synchronization** å…±åŒå¯¼è‡´ã€‚
2. **BF-IO åŸåˆ™éå¸¸æœ‰æ•ˆ**ï¼šé€šè¿‡æœ€å°åŒ–çŸ­æœŸæœªæ¥è´Ÿè½½ä¸å¹³è¡¡çš„æ•´æ•°ä¼˜åŒ–ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç³»ç»Ÿæ•ˆç‡ã€‚
3. **ç†è®ºä¸å®è·µä¸€è‡´**ï¼šç†è®ºè¯æ˜çš„ $\Omega(\sqrt{B \log G})$ æ”¹è¿›å› å­åœ¨å®éªŒä¸­å¾—åˆ°äº†éªŒè¯ï¼Œä¸”ç³»ç»Ÿè§„æ¨¡è¶Šå¤§ï¼Œç›¸å¯¹æ”¶ç›Šè¶Šæ˜æ˜¾ã€‚
4. **èŠ‚èƒ½æ½œåŠ›å·¨å¤§**ï¼šå³ä½¿åœ¨å•ä¸ªæœåŠ¡å®ä¾‹ä¸ŠèŠ‚çœå‡ ä¸ªç™¾åˆ†ç‚¹ï¼Œè€ƒè™‘åˆ° LLM æœåŠ¡çš„åºå¤§è§„æ¨¡ï¼ˆå¦‚ OpenAI æ¯å¤©å¤„ç† 25 äº¿æ¬¡è¯·æ±‚ï¼‰ï¼Œä¹Ÿèƒ½è½¬åŒ–ä¸ºå·¨å¤§çš„èƒ½æºå’Œæˆæœ¬èŠ‚çº¦ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **é›†ä¸­å¼å‡è®¾**ï¼šæœ€å¼ºçš„ç†è®ºä¿è¯å»ºç«‹åœ¨ä¸­å¤®ç­‰å¾…æ± ï¼ˆcentralized waiting-poolï¼‰çš„å‡è®¾ä¸Šã€‚ä¸€äº›æœåŠ¡å¼•æ“é‡‡ç”¨å³æ—¶åˆ†å‘ï¼ˆinstant-dispatchï¼‰åˆ°æ¯ä¸ª GPU çš„æœ¬åœ°é˜Ÿåˆ—ï¼Œè¿™ä¼šå‰Šå¼± BF-IO çš„æ•ˆæœã€‚
- **æ¨¡å‹èŒƒå›´**ï¼šç†è®ºä¸»è¦è¦†ç›–äº† **non-decreasing workload drift** è¿‡ç¨‹ã€‚å¯¹äºè´Ÿè½½å¯èƒ½å‡å°‘æˆ–é«˜åº¦éå•è°ƒçš„ç³»ç»Ÿï¼ˆå¦‚ aggressive pruning æˆ–è‡ªé€‚åº”å‹ç¼©ï¼‰ï¼Œè¯¥ç†è®ºæ¡†æ¶å°šä¸é€‚ç”¨ã€‚
- **å¤šç›®æ ‡ä¼˜åŒ–**ï¼šç°å®éƒ¨ç½²éœ€è¦åŒæ—¶ä¼˜åŒ–å¤šä¸ªç›®æ ‡ï¼ˆå°¾å»¶è¿Ÿã€ç§Ÿæˆ·å…¬å¹³æ€§ã€SLO åˆè§„æ€§ã€èƒ½è€—ä¸Šé™ï¼‰ã€‚BF-IO ä¸»è¦é’ˆå¯¹ Barrier Idle Reductionï¼Œå¦‚ä½•å°†å…¶æ‰©å±•åˆ°å¤šç›®æ ‡ä¼˜åŒ–ä»éœ€ç ”ç©¶ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **ç ”ç©¶â€œå³æ—¶åˆ†å‘â€æ¥å£**ï¼šä¸ºè·¯ç”±åˆ°æ¯ä¸ª GPU çš„ FIFO é˜Ÿåˆ—çš„æ¶æ„å¼€å‘å®Œæ•´çš„ç†è®ºï¼Œå¹¶é‡åŒ–çŸ­è§†ä¿¡å·éšæ¿€æ´»å»¶è¿Ÿå¢é•¿è€Œé€€åŒ–çš„ç¨‹åº¦ã€‚
2. **æ‰©å±•ç†è®ºæ¨¡å‹**ï¼šå°†æœ€åæƒ…å†µç†è®ºæ‰©å±•åˆ°è´Ÿè½½å‡å°‘æˆ–é«˜åº¦éå•è°ƒçš„ç³»ç»Ÿï¼Œå¹¶åˆ»ç”»å•è°ƒæ€§ä½•æ—¶æ˜¯å¿…éœ€çš„ã€‚
3. **å¤šç›®æ ‡æœåŠ¡ä¼˜åŒ–**ï¼šå°†å…¬å¹³æ€§å’Œ SLO çº¦æŸé›†æˆåˆ°çŸ­è§†ä¼˜åŒ–æ¡†æ¶ä¸­ï¼ŒåŒæ—¶ä¿æŒæ¯«ç§’çº§çš„å†³ç­–é¢„ç®—ã€‚
4. **æ¢ç´¢æ›´å¤æ‚çš„å‰ç»æ¨¡å‹**ï¼šç ”ç©¶å¦‚ä½•ç»“åˆå­¦ä¹ æ¨¡å‹æ¥æä¾›æ›´å‡†ç¡®çš„çŸ­æœŸå®Œæˆé¢„æµ‹ï¼Œä»¥è¿›ä¸€æ­¥æå‡ BF-IO çš„æ€§èƒ½ã€‚

</details>

---

### 10. [Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment](https://arxiv.org/abs/2601.17329)

**Authors**: Tiejin Chen, Xiaoou Liu, Vishnu Nandam, Kuan-Ru Liou, Hua Wei  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.17329v1  

#### Abstract
Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \emph{answers} b...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šConformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment**

---

## 1. **è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### âœ… **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
å½“å‰åŸºäºåå¥½çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½æ–¹æ³•ï¼ˆå¦‚ RLHFã€DPOï¼‰ä¾èµ–äºæˆå¯¹åå¥½æ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ ‡ç­¾é€šå¸¸å­˜åœ¨**å™ªå£°å’Œä¸ä¸€è‡´æ€§**ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ AI ç”Ÿæˆåé¦ˆï¼ˆRLAIFï¼‰æ—¶æ›´ä¸ºä¸¥é‡ã€‚ç°æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ–¹æ³•ä¸»è¦å…³æ³¨**åå¥½å±‚é¢çš„ä¸ç¡®å®šæ€§**ï¼ˆå³â€œè¿™ä¸ªåå¥½åˆ¤æ–­æ˜¯å¦å¯é â€ï¼‰ï¼Œå´å¿½ç•¥äº†æ›´æ ¹æœ¬çš„é—®é¢˜ï¼š**ç­”æ¡ˆæœ¬èº«çš„å¯é æ€§**ã€‚

å¦‚æœä¸¤ä¸ªè¢«æ¯”è¾ƒçš„ç­”æ¡ˆæœ¬èº«è´¨é‡ä½ã€ç½®ä¿¡åº¦å·®ï¼Œé‚£ä¹ˆæ— è®ºæ€æ ·å»ºæ¨¡åå¥½ä¸ç¡®å®šæ€§ï¼Œè¯¥æ¯”è¾ƒæ‰€æä¾›çš„å­¦ä¹ ä¿¡å·éƒ½ç¼ºä¹æ„ä¹‰ã€‚å› æ­¤ï¼Œä½œè€…æŒ‡å‡ºä¸€ä¸ªè¢«å¿½è§†çš„å…³é”®ç»´åº¦â€”â€”**answer-level reliabilityï¼ˆç­”æ¡ˆçº§å¯é æ€§ï¼‰**ã€‚

---

### ğŸš€ **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
æå‡º **Conformal Feedback Alignment (CFA)**ï¼Œä¸€ç§å°† **Conformal Prediction (CP)** å¼•å…¥ LLM å¯¹é½çš„æ–°æ¡†æ¶ï¼Œç”¨äºé‡åŒ–ç­”æ¡ˆçº§åˆ«çš„å¯é æ€§ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè®­ç»ƒä¸­çš„åŠ æƒä¿¡å·ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- åˆ©ç”¨ **Conformal Prediction** æ„å»ºå…·æœ‰ç»Ÿè®¡ä¿è¯çš„é¢„æµ‹é›†ï¼ˆprediction setsï¼‰ï¼Œè¡¡é‡æ¯ä¸ªæ¨¡å‹è¾“å‡ºçš„å¯é æ€§ã€‚
- å®šä¹‰ **set-wise uncertainty aggregation function**ï¼Œå°†ä¸¤ä¸ªç­”æ¡ˆæ‰€å±çš„ conformal é›†åˆæ˜ å°„ä¸ºä¸€ä¸ªç»Ÿä¸€çš„ä¸ç¡®å®šæ€§æƒé‡ $ u \in [0,1] $ã€‚
- å°†æ­¤æƒé‡åº”ç”¨äº **PPO-style å’Œ DPO-style** çš„è®­ç»ƒç›®æ ‡ä¸­ï¼Œå½¢æˆ **uncertainty-weighted loss**ï¼Œä½¿æ¨¡å‹ä¼˜å…ˆå­¦ä¹ æ¥è‡ªé«˜ç½®ä¿¡åº¦ç­”æ¡ˆå¯¹çš„åå¥½ã€‚

#### æŠ€æœ¯äº®ç‚¹ï¼š
- æ”¯æŒ **black-box å’Œ white-box è®¾ç½®**ï¼ˆåˆ†åˆ«ä½¿ç”¨ä¸åŒ nonconformity scoreï¼‰ã€‚
- ç¬¬ä¸€æ¬¡å°† CP åº”ç”¨äº AI-generated feedback åœºæ™¯ä¸‹çš„ LLM alignmentã€‚
- æƒé‡æœºåˆ¶å®ç°äº†ä»â€œç¡¬æ ‡ç­¾â€åˆ°â€œè½¯è¯æ®â€çš„è½¬å˜ï¼ˆsoft evidenceï¼‰ï¼Œæå‡é²æ£’æ€§ã€‚

---

### ğŸ” **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ WPO, IW-DPOï¼‰ | CFA |
|------|----------------------------|-----|
| ä¸ç¡®å®šæ€§å»ºæ¨¡å±‚çº§ | Preference-levelï¼ˆåå¥½æ˜¯å¦å¯ä¿¡ï¼‰ | **Answer-levelï¼ˆç­”æ¡ˆæœ¬èº«æ˜¯å¦å¯ä¿¡ï¼‰** |
| æ•°æ®æºåˆ©ç”¨ | å¿½ç•¥ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡å·®å¼‚ | æ˜¾å¼å»ºæ¨¡ç­”æ¡ˆå¯é æ€§ |
| æ³›åŒ–èƒ½åŠ› | å¤šæ•°ä¾èµ– reward model æˆ–ç‰¹å®šæ¶æ„ | é€‚ç”¨äº DPO/PPOã€é»‘ç™½ç›’é€šç”¨ |
| å­¦ä¹ æ•ˆç‡ | å¯èƒ½æ”¾å¤§ä½è´¨é‡åé¦ˆå½±å“ | è‡ªåŠ¨æŠ‘åˆ¶ä½å¯é æ€§æ¯”è¾ƒçš„å½±å“ |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šCFA åœ¨æºå¤´ä¸Šè¿‡æ»¤ä¸å¯é çš„å­¦ä¹ ä¿¡å·ï¼Œæå‡äº†å¯¹é½è¿‡ç¨‹çš„æ•°æ®æ•ˆç‡ä¸é²æ£’æ€§ã€‚

---

## 2. **æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### ğŸ“š **ä½¿ç”¨çš„æ•°æ®é›†**
åœ¨ä¸‰ä¸ªæ ‡å‡†åå¥½å¯¹é½æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼š
1. **WebGPT Comparisons**ï¼šåŸºäºç½‘é¡µæœç´¢çš„å›ç­”ï¼Œå¼ºè°ƒäº‹å®æ€§å’Œä¿¡æ¯å®Œæ•´æ€§ã€‚
2. **Synthetic Instruct GPT-J Pairwise (Pairwise)**ï¼šåˆæˆæŒ‡ä»¤éµå¾ªæ•°æ®é›†ï¼Œè¦†ç›–å¹¿æ³›ä»»åŠ¡ã€‚
3. **Summarize from Feedback (Summarize)**ï¼šReddit æ–‡ç« æ‘˜è¦ä»»åŠ¡ï¼Œä¾§é‡å†…å®¹å‹ç¼©ä¸è¯­ä¹‰ä¿ç•™ã€‚

---

### âš™ï¸ **å®éªŒè®¾ç½®**
- **æ¨¡å‹**ï¼š
  - `Llama2-7B`, `Llama3.1-8B`, `Qwen2.5-7B`ï¼ˆä¸»å®éªŒï¼‰
  - `Qwen3` ç³»åˆ—ï¼ˆ0.6Bâ€“8Bï¼‰ç”¨äºå¯æ‰©å±•æ€§åˆ†æ
- **è®­ç»ƒæµç¨‹**ï¼š
  1. å…ˆè¿›è¡Œ Supervised Fine-Tuning (SFT)
  2. å†åº”ç”¨ CFA è¿›è¡Œ preference-based alignment
- **CP è®¾ç½®**ï¼š
  - ç™½ç›’ï¼ˆwhite-boxï¼‰ï¼šä½¿ç”¨ token-level log-probabilities ä½œä¸º nonconformity score
  - é»‘ç›’ï¼ˆblack-boxï¼‰ï¼šé€šè¿‡é‡‡æ ·é¢‘ç‡ã€ç›¸ä¼¼åº¦ç­‰ä¼°è®¡ score
  - è¦†ç›–ç‡è®¾å®šï¼š$ \alpha_1 = 0.5, \alpha_2 = 0.8 $
- **ä¼˜åŒ–å™¨**ï¼šAdamWï¼Œbatch size=1ï¼Œlearning rate=1e-6
- **ç¡¬ä»¶**ï¼š4Ã—NVIDIA A100 GPU

---

### ğŸ“Š **è¯„ä¼°æŒ‡æ ‡**
é‡‡ç”¨ **LLM-as-a-Judge** æ–¹æ³•ï¼Œä½¿ç”¨ **GPT-4o** å¯¹ç”Ÿæˆç»“æœæ‰“åˆ†ï¼Œè¯„ä¼°å››ä¸ªç»´åº¦ï¼ˆæ¯é¡¹æ»¡åˆ†100ï¼‰ï¼š
1. **Accuracy (Acc)**ï¼šå†…å®¹å‡†ç¡®æ€§
2. **Relevance (Rel)**ï¼šä¸»é¢˜ç›¸å…³æ€§
3. **Completeness (Comp)**ï¼šä¿¡æ¯å®Œæ•´æ€§
4. **Expression (Expr)**ï¼šè¡¨è¾¾æ¸…æ™°åº¦

æœ€ç»ˆæŠ¥å‘Šå››é¡¹å¹³å‡å¾—åˆ†ï¼ˆOverall Scoreï¼‰ï¼Œprompt è§åŸæ–‡ Figure 5ã€‚

---

### ğŸ”€ **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| ç±»å‹ | æ–¹æ³• |
|------|------|
| åŸºç¡€è®­ç»ƒ | SFTï¼ˆä»…ç›‘ç£å¾®è°ƒï¼‰ |
| ä¸»æµå¯¹é½ | PPO, DPOï¼ˆæ— ä¸ç¡®å®šæ€§åŠ æƒï¼‰ |
| ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ–¹æ³• | WPO (Zhou et al., 2024), IW-DPO with `Direct Ask`, `p(true)` |
| æ¶ˆèå®éªŒ | ä¸åŒ coverage å‚æ•°ã€é»‘ç™½ç›’è®¾ç½®å¯¹æ¯” |

---

## 3. **ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### ğŸ“ˆ **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1ï¼‰**
åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šï¼ŒCFA å‡æ˜¾è‘—ä¼˜äº baselineï¼š

| Model | Dataset | Base (DPO/PPO) | CFA | Improvement |
|-------|--------|----------------|-----|-------------|
| Llama2-7B | Summarize | 65.68 (DPO) / 65.88 (PPO) | **67.30** / **67.39** | **+1.62 / +1.51** |
| Llama2-7B | Pairwise | 90.88 (DPO) / 91.15 (PPO) | **92.12** / **91.89** | **+1.24 / +0.74** |
| Llama2-7B | WebGPT | 71.68 (DPO) / 72.25 (PPO) | **72.25** / **72.78** | **+0.57 / +0.53** |

> ğŸ’¡ æ”¹è¿›å¹…åº¦æ¥è¿‘ç”šè‡³è¶…è¿‡ base æ–¹æ³•ç›¸å¯¹äº SFT çš„å¢ç›Šï¼ˆä¾‹å¦‚ base DPO vs SFT æå‡çº¦ +1.77ï¼‰

---

### ğŸ†š **ä¸å…¶ä»–ä¸ç¡®å®šæ€§æ–¹æ³•å¯¹æ¯”ï¼ˆRQ3ï¼‰**
åœ¨ `Llama2-7B + Summarize` ä¸Šæ¯”è¾ƒï¼š
- **IW-DPO (Direct Ask)**ï¼šè¡¨ç° **ä½äº base DPO** â†’ è¡¨æ˜ LLM è‡ªæˆ‘è¯„ä¼°çš„ç½®ä¿¡åº¦ä¸å¯é 
- **IW-DPO (p(true))** å’Œ **WPO**ï¼šç•¥æœ‰æå‡ï¼Œä½†ä» **ä¸å¦‚ CFA**
- **CFA æ˜æ˜¾èƒœå‡º** â†’ è¯´æ˜ answer-level reliability æ›´å…·åˆ¤åˆ«åŠ›

> å›¾4æ˜¾ç¤ºï¼šCFA å®ç°æœ€é«˜ overall scoreï¼ˆ~67.3ï¼‰ï¼Œè¿œè¶…å…¶ä»–æ–¹æ³•ã€‚

---

### ğŸ” **æ¶ˆèå®éªŒç»“æœ**

#### ï¼ˆ1ï¼‰**ä¸åŒæ¨¡å‹è§„æ¨¡çš„å½±å“ï¼ˆFigure 2ï¼‰**
- åœ¨ Qwen3 ç³»åˆ—ï¼ˆ0.6Bâ€“8Bï¼‰ä¸Šæµ‹è¯• CFA + DPO
- å‘ç°ï¼š**å°æ¨¡å‹æ”¶ç›Šæ›´å¤§** â†’ è¡¨æ˜ CFA ç‰¹åˆ«æœ‰åŠ©äºèµ„æºå—é™åœºæ™¯ä¸‹çš„å¯¹é½

#### ï¼ˆ2ï¼‰**ä¸åŒè®­ç»ƒæ ·æœ¬é‡çš„å½±å“ï¼ˆFigure 3ï¼‰**
- ä½¿ç”¨ Llama2-7Bï¼Œåœ¨ Summarize æ•°æ®é›†ä¸­æŠ½å– 5%~100% æ•°æ®
- ç»“æœï¼šå³ä½¿åœ¨ **5% æ•°æ®ä¸‹ï¼ŒCFA ä»ä¼˜äº base DPO**
- éšç€æ•°æ®å¢åŠ ï¼Œæ€§èƒ½å·®è·è¿›ä¸€æ­¥æ‹‰å¤§ â†’ å±•ç°å‡ºä¼˜ç§€çš„ **data efficiency**

#### ï¼ˆ3ï¼‰**é»‘ç™½ç›’ CP å¯¹æ¯”ï¼ˆFigure 7ï¼‰**
- ç™½ç›’ CP æ•ˆæœæ›´å¥½ï¼ˆ68.04 vs 67.40ï¼‰â†’ å› å…¶èƒ½æ›´ç²¾ç¡®æ•æ‰ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒ
- ä½†é»‘ç›’ä¹Ÿæœ‰æ•ˆ â†’ è¯æ˜ CFA å¯éƒ¨ç½²äº API æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰

#### ï¼ˆ4ï¼‰**ä¸åŒ coverage å‚æ•°å½±å“ï¼ˆFigure 8ï¼‰**
- æœ€ä½³ performance å‡ºç°åœ¨ $ \alpha_2 = 0.7â€“0.8 $
- å½“ $ \alpha_2 = 0.6 $ï¼ˆå¤ªæ¥è¿‘ $ \alpha_1 = 0.5 $ï¼‰æ—¶æ•ˆæœä¸‹é™ â†’ åŒºåˆ†åº¦ä¸è¶³å¯¼è‡´æƒé‡å¤±æ•ˆ
- ä½†æ‰€æœ‰è®¾ç½®ä¸‹ CFA å‡ä¼˜äº base â†’ æ–¹æ³•ç¨³å¥

---

### ğŸ† **èƒœç‡å¯¹æ¯”ï¼ˆWin Rate, Table 3ï¼‰**
ä½¿ç”¨ GPT-4o åˆ¤æ–­ CFA ä¸ Base DPO è¾“å‡ºå­°ä¼˜ï¼š

| Model       | Win Rate (CFA wins) |
|------------|--------------------|
| Llama2-7B  | 56.18%             |
| Llama3.1-8B| 64.33%             |
| Qwen2.5-7B | 57.61%             |

> âœ… æ‰€æœ‰æ¨¡å‹ä¸Š CFA å‡å–å¾—å¤šæ•°èƒœåˆ©ï¼Œå°¤å…¶åœ¨æ›´å¤§æ¨¡å‹ä¸Šä¼˜åŠ¿æ›´æ˜æ˜¾ã€‚

---

## 4. **å…³é”®ç»“è®ºå’Œå‘ç°**

### âœ… **ä¸»è¦å‘ç°**
1. **Answer-level reliability æ˜¯å½±å“å¯¹é½è´¨é‡çš„å…³é”®å› ç´ **ï¼Œä¸åº”è¢«å¿½ç•¥ã€‚
2. **CFA æˆåŠŸå°† Conformal Prediction å¼•å…¥ preference learning**ï¼Œæä¾›äº†ä¸€ç§ principled çš„æ–¹å¼æ¥é‡åŒ–å¹¶åˆ©ç”¨ç­”æ¡ˆå¯é æ€§ã€‚
3. CFA åœ¨å¤šç§æ¨¡å‹ã€æ•°æ®é›†ã€è®­ç»ƒèŒƒå¼ï¼ˆDPO/PPOï¼‰ã€è®¿é—®æ¨¡å¼ï¼ˆé»‘ç™½ç›’ï¼‰ä¸‹å‡å¸¦æ¥ç¨³å®šå¢ç›Šã€‚
4. æ–¹æ³•ç‰¹åˆ«é€‚åˆ **ä½æ•°æ®ã€å°æ¨¡å‹ã€é«˜å™ªå£°åé¦ˆï¼ˆå¦‚ RLAIFï¼‰** åœºæ™¯ï¼Œæ˜¾è‘—æå‡ **data efficiency å’Œ robustness**ã€‚
5. **DPO + CFA æ•ˆæœä¼˜äº PPO + CFA** â†’ æ¨æµ‹å›  PPO ä¸­ reward model ä»…ä¸ºä¸­é—´ç»„ä»¶ï¼Œéš¾ä»¥å®Œå…¨ä¼ é€’å¯é æ€§ä¿¡å·ã€‚

---

### âš ï¸ **å±€é™æ€§**
1. **ä¾èµ– calibration set çš„è´¨é‡**ï¼šå½“æ ¡å‡†é›†è¾ƒå°æ—¶ï¼ŒCP çš„ coverage ä¿è¯å¯èƒ½å¤±æ•ˆã€‚
2. **æœªç»“åˆ preference-level ä¸ answer-level ä¸ç¡®å®šæ€§**ï¼šç›®å‰ä»…å»ºæ¨¡å‰è€…æˆ–åè€…ï¼Œæœªæ¥å¯è”åˆå»ºæ¨¡ã€‚
3. **å±€é™äºæ–‡æœ¬æ¨¡æ€**ï¼šå°šæœªæ‰©å±•è‡³å¤šæ¨¡æ€ LLMï¼ˆå¦‚å›¾æ–‡ã€è¯­éŸ³ï¼‰ã€‚
4. **è®¡ç®—å¼€é”€å¢åŠ **ï¼šéœ€é¢å¤–ç”Ÿæˆ conformal é›†åˆå¹¶è®¡ç®— nonconformity scoresã€‚

---

### ğŸ”® **æœªæ¥å·¥ä½œæ–¹å‘**
1. **è”åˆå»ºæ¨¡ answer-level ä¸ preference-level uncertainty**
2. **æ‰©å±•è‡³ multi-modal feedback alignment**
3. **å¼•å…¥ human-in-the-loop è¿›è¡ŒåŠ¨æ€ calibration**
4. **æ¢ç´¢æ›´é«˜æ•ˆçš„ black-box CP approximation æ–¹æ³•**
5. **åº”ç”¨äº real-world åœ¨çº¿å¯¹é½ç³»ç»Ÿï¼ˆonline RLHFï¼‰**

---

## âœ… æ€»ç»“ä¸€å¥è¯
> **CFA é¦–æ¬¡å°† Conformal Prediction ç”¨äºé‡åŒ– LLM å¯¹é½ä¸­çš„ answer-level reliabilityï¼Œé€šè¿‡æ„å»ºç»Ÿè®¡å¯é çš„é¢„æµ‹é›†å¹¶åŠ æƒåå¥½æŸå¤±ï¼Œåœ¨å¤šç§è®¾ç½®ä¸‹å®ç°äº†æ›´é²æ£’ã€æ›´é«˜æ•ˆçš„å¤§æ¨¡å‹å¯¹é½ï¼Œå®éªŒè¯æ˜å…¶æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŠç°æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ–¹æ³•ã€‚**

</details>

---

### 11. [AttenMIA: LLM Membership Inference Attack through Attention Signals](https://arxiv.org/abs/2601.18110)

**Authors**: Pedram Zaree, Md Abdullah Al Mamun, Yue Dong, Ihsen Alouani, Nael Abu-Ghazaleh  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.18110v1  

#### Abstract
Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inf...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*AttenMIA: LLM Membership Inference Attack through Attention Signals*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½**è¿‡åº¦è®°å¿†ï¼ˆmemorizeï¼‰å…¶è®­ç»ƒæ•°æ®**ï¼Œè¿™å¸¦æ¥äº†ä¸¥é‡çš„éšç§å’ŒçŸ¥è¯†äº§æƒé£é™©ã€‚**æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMembership Inference Attack, MIAï¼‰** æ˜¯ä¸€ç§å…³é”®å¨èƒï¼Œæ—¨åœ¨åˆ¤æ–­æŸä¸ªæ ·æœ¬æ˜¯å¦å±äºæ¨¡å‹çš„è®­ç»ƒé›†ã€‚ç°æœ‰çš„ MIA æ–¹æ³•ä¸»è¦ä¾èµ–äºè¾“å‡ºç½®ä¿¡åº¦ï¼ˆå¦‚ perplexityã€log-likelihoodï¼‰æˆ–åµŒå…¥ç‰¹å¾ï¼Œä½†è¿™äº›ä¿¡å·å¾€å¾€**è„†å¼±ä¸”æ³›åŒ–èƒ½åŠ›å·®**ï¼Œå°¤å…¶æ˜¯åœ¨ä½è¯¯æŠ¥ç‡ï¼ˆlow-FPRï¼‰çš„å…³é”®åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **AttenMIA**ï¼Œè¿™æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§åœ°åˆ©ç”¨ **Transformer æ¨¡å‹å†…éƒ¨è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰ä¿¡å·** è¿›è¡Œ LLM æˆå‘˜æ¨æ–­çš„ç™½ç›’ï¼ˆwhite-boxï¼‰æ¡†æ¶ã€‚

å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºï¼š
- **ä»æ³¨æ„åŠ›æœºåˆ¶ä¸­æå–ç‰¹å¾**ï¼šè®¤ä¸ºæ³¨æ„åŠ›æ¨¡å¼ï¼ˆå¦‚é›†ä¸­åº¦ã€ç¨³å®šæ€§ã€è·¨å±‚ä¸€è‡´æ€§ï¼‰èƒ½åæ˜ æ¨¡å‹å¯¹è®­ç»ƒæ ·æœ¬çš„â€œç†Ÿæ‚‰åº¦â€ã€‚
- **è®¾è®¡ä¸¤ç±»äº’è¡¥çš„æ³¨æ„åŠ›ç‰¹å¾**ï¼š
  1. **Transitional Featuresï¼ˆè¿‡æ¸¡ç‰¹å¾ï¼‰**ï¼šé‡åŒ–æ³¨æ„åŠ›æ¨¡å¼åœ¨ä¸åŒå±‚ï¼ˆlayersï¼‰å’Œå¤´ï¼ˆheadsï¼‰ä¹‹é—´çš„å˜åŒ–ï¼Œä¾‹å¦‚ç›¸å…³æ€§ï¼ˆCorrï¼‰ã€Frobenius è·ç¦»ã€KL æ•£åº¦å’Œè´¨å¿ƒæ¼‚ç§»ï¼ˆbarycenter driftï¼‰ã€‚
  2. **Perturbation-Based Featuresï¼ˆæ‰°åŠ¨ç‰¹å¾ï¼‰**ï¼šé€šè¿‡ä¸»åŠ¨ä¿®æ”¹è¾“å…¥ï¼ˆå¦‚åˆ é™¤ã€æ›¿æ¢ token æˆ–æ·»åŠ éæˆå‘˜å‰ç¼€ï¼‰ï¼Œè§‚å¯Ÿæ³¨æ„åŠ›åˆ†å¸ƒçš„å˜åŒ–ï¼ˆKL shiftï¼‰ã€‚å‡è®¾æˆå‘˜æ ·æœ¬è¢«æ‰°åŠ¨åä¼šä»â€œæˆå‘˜â€å˜ä¸ºâ€œéæˆå‘˜â€ï¼Œå¯¼è‡´æ³¨æ„åŠ›æ¨¡å¼å‘ç”Ÿæ›´å¤§åç§»ã€‚
- **ç»“åˆå¤šå¤´æ³¨æ„åŠ›ç»“æ„**ï¼šå……åˆ†åˆ©ç”¨ Transformer çš„å¤šå¤´å¹¶è¡Œç»“æ„ï¼Œèšåˆæ¥è‡ªå¤šä¸ª head å’Œ layer çš„ç»†ç²’åº¦ä¿¡å·ï¼Œå½¢æˆæ›´ä¸°å¯Œã€æ›´å…·åˆ¤åˆ«åŠ›çš„ç‰¹å¾å‘é‡ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´é«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§**ï¼šåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶ŠåŸºäºè¾“å‡ºç½®ä¿¡åº¦çš„åŸºçº¿æ–¹æ³•ã€‚
- **åœ¨ä½è¯¯æŠ¥ç‡ï¼ˆlow-FPRï¼‰ä¸‹è¡¨ç°å“è¶Š**ï¼šè¿™å¯¹äºå®é™…çš„éšç§å®¡è®¡è‡³å…³é‡è¦ï¼Œå› ä¸ºé«˜è¯¯æŠ¥ç‡ä¼šå¯¼è‡´å¤§é‡æ— æ•ˆè­¦æŠ¥ã€‚
- **æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›**ï¼šæ³¨æ„åŠ›ä¿¡å·åœ¨ä¸åŒæ¨¡å‹æ¶æ„ï¼ˆLLaMA, Pythia, OPTï¼‰å’Œæ•°æ®é›†é—´è¡¨ç°å‡ºè‰¯å¥½çš„å¯è¿ç§»æ€§ï¼Œè€Œè®¸å¤šè¾“å‡ºåŸºçº¿éœ€è¦é’ˆå¯¹ç‰¹å®šè®¾ç½®è¿›è¡Œæ ¡å‡†ã€‚
- **æ— éœ€å½±å­æ¨¡å‹ï¼ˆshadow modelsï¼‰**ï¼šæ˜¯å‚è€ƒæ— å…³ï¼ˆreference-freeï¼‰çš„æ–¹æ³•ï¼Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **WikiMIA**ï¼šä¸€ä¸ªæ ‡å‡†çš„ MIA åŸºå‡†ï¼Œæä¾›å·²çŸ¥æˆå‘˜/éæˆå‘˜æ ‡ç­¾çš„ç»´åŸºç™¾ç§‘æ–‡æœ¬ã€‚
- **MIMIR**ï¼šå¦ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„ MIA æ•°æ®é›†ï¼ŒåŒ…å«å¤šä¸ªå­é›†ï¼ˆå¦‚ arXiv, GitHub, PubMed, HackerNews ç­‰ï¼‰ï¼Œç”¨äºè¯„ä¼°æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **ç›®æ ‡æ¨¡å‹**ï¼šåœ¨å¤šä¸ªå¼€æº LLM ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬ **LLaMA-2** (13B, 30B), **Pythia** (1.4B, 6.9B), **OPT** (6.7B, 66B), **NeoX-20B**ã€‚
- **å¨èƒæ¨¡å‹**ï¼š**ç™½ç›’æ”»å‡»**ï¼Œæ”»å‡»è€…å¯ä»¥è®¿é—®æ¨¡å‹çš„æ‰€æœ‰å†…éƒ¨çŠ¶æ€ï¼ˆç‰¹åˆ«æ˜¯æ‰€æœ‰å±‚å’Œå¤´çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼‰ï¼Œä½†**ä¸è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®æˆ–ä¼˜åŒ–å™¨çŠ¶æ€**ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **ROC AUC**ï¼šè¡¡é‡æ•´ä½“çš„åˆ†ç±»åŒºåˆ†èƒ½åŠ›ã€‚
  - **TPR@1%FPR**ï¼šåœ¨ 1% çš„å‡é˜³æ€§ç‡ä¸‹çš„çœŸé˜³æ€§ç‡ï¼Œè¿™æ˜¯éšç§å®¡è®¡ä¸­æœ€å…³é”®çš„æŒ‡æ ‡ï¼Œå¼ºè°ƒåœ¨æä½è¯¯æŠ¥å‰æä¸‹çš„æ£€æµ‹èƒ½åŠ›ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
è®ºæ–‡å°† AttenMIA ä¸å¤šç§å…ˆè¿›çš„ MIA åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼š
- **PPL / Loss**ï¼šåŸºäºå›°æƒ‘åº¦æˆ–æŸå¤±å€¼ã€‚
- **Zlib**ï¼šåŸºäºå‹ç¼©ç†µã€‚
- **Min-K% / Min-K%++**ï¼šåŸºäºæœ€ä½é¢„æµ‹æ¦‚ç‡ã€‚
- **Neighbor / PETAL**ï¼šåŸºäºé‚»è¿‘ token çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚
- **RECALL**ï¼šåŸºäºå‰ç¼€æ‰°åŠ¨åçš„æ¡ä»¶ä¼¼ç„¶å˜åŒ–ã€‚
- **LiRA / Reference**ï¼šåŸºäºå½±å­æ¨¡å‹æˆ–å‚è€ƒæ¨¡å‹çš„æ–¹æ³•ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
- åœ¨ **WikiMIA-32** åŸºå‡†ä¸Šï¼Œä½¿ç”¨ **Llama2-13b** æ¨¡å‹ï¼š
  - **AttenMIA (Perturbed)** è¾¾åˆ°äº† **0.996 ROC AUC** å’Œ **87.9% TPR@1%FPR**ï¼Œåˆ›ä¸‹äº†è¯¥åŸºå‡†ä¸Šçš„æ–°çºªå½•ã€‚
- åœ¨ **MIMIR** åŸºå‡†ä¸Šï¼Œå¹³å‡æ€§èƒ½ï¼š
  - AttenMIA å°† ROC AUC ç›¸æ¯” RECALL æå‡äº† **+55%**ï¼Œç›¸æ¯” PETAL æå‡äº† **+27%**ã€‚
  - åœ¨ TPR@1%FPR ä¸Šï¼ŒAttenMIA å¹³å‡è¾¾åˆ° **47.8%**ï¼Œè¿œè¶… RECALL (19.6%) å’Œ PETAL (13.3%)ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **å…¨é¢è¶…è¶Š**ï¼šåœ¨æ‰€æœ‰æµ‹è¯•çš„æ¨¡å‹ã€æ•°æ®é›†å’Œåºåˆ—é•¿åº¦ä¸Šï¼ŒAttenMIA çš„ä¸¤ç§å˜ä½“ï¼ˆTransitional å’Œ Perturbedï¼‰åœ¨ ROC AUC å’Œ TPR@1%FPR ä¸Šå‡**ä¸€è‡´åœ°ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•**ã€‚
- **ä¼˜åŠ¿æ˜¾è‘—**ï¼šç‰¹åˆ«æ˜¯åœ¨ **TPR@1%FPR** æŒ‡æ ‡ä¸Šï¼ŒAttenMIA çš„ä¼˜åŠ¿æœ€ä¸ºæ˜æ˜¾ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…éšç§å®¡è®¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚
- **æ¶ˆèå®éªŒ**ï¼šé€šè¿‡ **Hellinger Distance (HD)** åˆ†æï¼Œè¯æ˜äº† AttenMIA çš„å•ä¸ªæ³¨æ„åŠ›ç‰¹å¾ï¼ˆå¦‚ Consistency-Corr, Consistency-KLï¼‰çš„ HD å€¼ï¼ˆ~0.70ï¼‰è¿œé«˜äºæœ€ä½³åŸºçº¿ï¼ˆ~0.56ï¼‰ï¼Œè¡¨æ˜å…¶ç‰¹å¾å…·æœ‰æ›´å¼ºçš„åˆ†ç¦»æ€§ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **æ‰°åŠ¨ç­–ç•¥çš„å½±å“**ï¼šå•ç‹¬ä½¿ç”¨ token droppingã€replacement æˆ– prefix insertion éƒ½èƒ½å–å¾—è‰¯å¥½æ•ˆæœï¼Œä½†**ç»„åˆä½¿ç”¨æ‰€æœ‰ç­–ç•¥**èƒ½å¸¦æ¥æœ€ä½³æ€§èƒ½ï¼Œè¯´æ˜å®ƒä»¬æ•æ‰äº†äº’è¡¥çš„ä¿¡å·ã€‚
- **å±‚æ•°çš„å½±å“**ï¼šåˆ†æè¡¨æ˜ï¼Œ**æ¯ä¸€å±‚éƒ½åŒ…å«æœ‰ç”¨çš„æˆå‘˜ä¿¡æ¯**ï¼Œä½†æ€§èƒ½éšå±‚æ•°å¢åŠ è€Œæå‡ã€‚èšåˆæ‰€æœ‰å±‚çš„ç‰¹å¾ï¼ˆAll Layersï¼‰æ¯”ä»…ç”¨å•å±‚æˆ–åˆ†ç»„å±‚æ•ˆæœæ›´å¥½ï¼Œè¯´æ˜è®°å¿†æ˜¯åˆ†å¸ƒåœ¨å…¨ç½‘ç»œçš„ã€‚
- **åºåˆ—é•¿åº¦çš„å½±å“**ï¼šè¾ƒçŸ­çš„åºåˆ—ï¼ˆå¦‚ 32 tokensï¼‰é€šå¸¸èƒ½äº§ç”Ÿæ›´å¼ºçš„æˆå‘˜ä¿¡å·ï¼Œéšç€åºåˆ—å¢é•¿ï¼Œæ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œä½† AttenMIA ä»ä¿æŒé¢†å…ˆã€‚
- **é˜²å¾¡æœ‰æ•ˆæ€§**ï¼šæµ‹è¯•äº†**è®­ç»ƒæ•°æ®å»é‡ï¼ˆdeduplicationï¼‰** è¿™ä¸€å¸¸è§é˜²å¾¡æ‰‹æ®µï¼Œå‘ç°å…¶å¯¹ AttenMIA çš„æ”»å‡»æ€§èƒ½å½±å“**å¾®ä¹å…¶å¾®**ï¼ˆAUC å˜åŒ– < 0.03ï¼‰ï¼Œè¡¨æ˜è¯¥é˜²å¾¡ä¸è¶³ä»¥æŠµå¾¡åŸºäºæ³¨æ„åŠ›çš„é«˜çº§æ”»å‡»ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **æ³¨æ„åŠ›æ˜¯å¼ºå¤§çš„éšç§æ³„éœ²æº**ï¼šTransformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ä»…æ˜¯ä¸ºäº†è§£é‡Šæ¨¡å‹è¡Œä¸ºï¼Œä¹Ÿ**æ— æ„ä¸­æ”¾å¤§äº†éšç§é£é™©**ã€‚å…¶å†…éƒ¨åŠ¨æ€ï¼ˆå¦‚é›†ä¸­åº¦ã€ç¨³å®šæ€§å’Œå¯¹æ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼‰æ˜¯åˆ¤æ–­æˆå‘˜èº«ä»½çš„å¼ºå¤§ä¿¡å·ã€‚
2. **AttenMIA æ€§èƒ½å“è¶Š**ï¼šæ‰€æå‡ºçš„æ¡†æ¶åœ¨å¤šä¸ªç»´åº¦ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯åœ¨ä½è¯¯æŠ¥ç‡è¿™ä¸€å…³é”®æŒ‡æ ‡ä¸Šï¼Œè¯æ˜äº†å…¶ä½œä¸ºå…ˆè¿›éšç§å®¡è®¡å·¥å…·çš„æœ‰æ•ˆæ€§ã€‚
3. **è®°å¿†æ˜¯åˆ†å¸ƒå¼ä¸”ç¨³å¥çš„**ï¼šæˆå‘˜ä¿¡æ¯å¹¶éå±€é™äºç‰¹å®šå±‚ï¼Œè€Œæ˜¯åˆ†å¸ƒåœ¨æ¨¡å‹çš„å¤šä¸ªå±‚å’Œå¤´ä¸­ã€‚æ­¤å¤–ï¼Œç®€å•çš„å»é‡ç­‰é˜²å¾¡æªæ–½æ— æ³•æœ‰æ•ˆç¼“è§£è¿™ç§åŸºäºå†…éƒ¨çŠ¶æ€çš„æ”»å‡»ã€‚
4. **å¯ç”¨äºå¢å¼ºæ•°æ®æå–**ï¼šå°† AttenMIA ç”¨äºè®­ç»ƒæ•°æ®æå–æ¡†æ¶æ—¶ï¼Œå…¶ç”Ÿæˆçš„æ’åä¸çœŸå®è®°å¿†å†…å®¹çš„ç›¸ä¼¼åº¦ï¼ˆROUGE-Lï¼‰çš„ç›¸å…³æ€§ï¼ˆPearson r = 0.48ï¼‰è¿œè¶…åŸºäºå›°æƒ‘åº¦ç­‰çš„ä¼ ç»ŸæŒ‡æ ‡ï¼ˆæœ€é«˜ r = 0.32ï¼‰ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…æ”»å‡»ä¸­çš„å®ç”¨æ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ç™½ç›’å‡è®¾**ï¼šAttenMIA éœ€è¦å®Œå…¨è®¿é—®æ¨¡å‹çš„å†…éƒ¨æ³¨æ„åŠ›çŠ¶æ€ï¼Œè¿™åœ¨å•†ä¸šé—­æºæ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰ä¸­éš¾ä»¥å®ç°ã€‚å…¶åœ¨é»‘ç›’æˆ–APIè®¿é—®åœºæ™¯ä¸‹çš„é€‚ç”¨æ€§æœ‰å¾…ç ”ç©¶ã€‚
- **è®¡ç®—å¼€é”€**ï¼šè™½ç„¶æ¯”è®­ç»ƒå½±å­æ¨¡å‹ä¾¿å®œï¼Œä½†æå–å’Œå¤„ç†æ‰€æœ‰å±‚å’Œå¤´çš„æ³¨æ„åŠ›ç‰¹å¾ä»ç„¶æ¯”ç®€å•çš„è¾“å‡ºç»Ÿè®¡æ–¹æ³•æ›´è€—èµ„æºã€‚
- **é€šç”¨æ€§è¾¹ç•Œ**ï¼šå°½ç®¡åœ¨å¤šç§æ¶æ„ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å…¶åœ¨é Transformer æ¶æ„æˆ–å…¶ä»–æ¨¡æ€ï¼ˆå¦‚è§†è§‰ï¼‰æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§æœªçŸ¥ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **å¼€å‘é’ˆå¯¹æ€§çš„é˜²å¾¡æœºåˆ¶**ï¼šé‰´äºæ³¨æ„åŠ›æ˜¯æ–°çš„éšç§æ³„éœ²é€šé“ï¼Œéœ€è¦ç ”ç©¶å¦‚ä½•åœ¨ä¸å½±å“æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œå¯¹æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ­£åˆ™åŒ–æˆ–æ‰°åŠ¨ä»¥ä¿æŠ¤éšç§ï¼ˆä¾‹å¦‚ï¼Œæ³¨æ„åŠ›æ©ç ã€å·®åˆ†éšç§æ³¨å…¥ï¼‰ã€‚
- **æ¢ç´¢é»‘ç›’ç‰ˆæœ¬**ï¼šç ”ç©¶å¦‚ä½•é€šè¿‡ API æŸ¥è¯¢çš„è¾“å‡ºé—´æ¥æ¨æ–­æ³¨æ„åŠ›æ¨¡å¼ï¼Œæˆ–å°† AttenMIA çš„æ´è§è¿ç§»åˆ°é»‘ç›’æ”»å‡»ä¸­ã€‚
- **ç†è§£è®°å¿†çš„æ ¹æº**ï¼šæ·±å…¥åˆ†æä¸ºä½•æŸäº›æ³¨æ„åŠ›å¤´å’Œå±‚å¯¹æˆå‘˜ä¿¡æ¯æ›´æ•æ„Ÿï¼Œä»è€Œæ›´å¥½åœ°ç†è§£ LLM çš„è®°å¿†æœºåˆ¶ã€‚
- **è´Ÿè´£ä»»çš„å®¡è®¡å·¥å…·**ï¼šå°† AttenMIA å‘å±•ä¸ºå¼€å‘è€…å¯ç”¨çš„å·¥å…·ï¼Œç”¨äºåœ¨æ¨¡å‹å‘å¸ƒå‰ä¸»åŠ¨æ£€æµ‹å’Œå‡è½»æ½œåœ¨çš„è®°å¿†é£é™©ã€‚

</details>

---

### 12. [Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers](https://arxiv.org/abs/2601.17367)

**Authors**: Zecheng Tang, Quantong Qiu, Yi Yang, Zhiyi Hong, Haiya Xiang, Kebin Liu, Qingqing Dang, Juntao Li, Min Zhang  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.17367v1  

#### Abstract
The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically empl...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šElastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

æ ‡å‡†çš„ **Full Attention (FA)** åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹å…·æœ‰ $O(n^2)$ çš„è®¡ç®—å¤æ‚åº¦ï¼Œä¸¥é‡é™åˆ¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„å¯æ‰©å±•æ€§ã€‚è™½ç„¶ç°æœ‰çš„ **hybrid attention** æ–¹æ³•é€šè¿‡ç»“åˆç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attention, SAï¼‰å’Œå…¨æ³¨æ„åŠ›æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸é‡‡ç”¨**é™æ€çš„ç¨€ç–æ¯”ä¾‹**ï¼ˆå³å›ºå®šçš„ SA ä¸ FA å¤´æ•°æ¯”ä¾‹ï¼‰ï¼Œæ— æ³•åœ¨æ¨ç†é˜¶æ®µæ ¹æ®ä»»åŠ¡åŠ¨æ€è°ƒæ•´ã€‚

è¿™å¯¼è‡´ï¼š
- å¯¹äº**ç¨€ç–æ•æ„Ÿä»»åŠ¡**ï¼ˆå¦‚é—®ç­”ï¼‰ï¼Œè¿‡åº¦ç¨€ç–ä¼šæ˜¾è‘—é™ä½æ€§èƒ½ï¼›
- å¯¹äº**ç¨€ç–é²æ£’ä»»åŠ¡**ï¼ˆå¦‚æ‘˜è¦ï¼‰ï¼Œä¸å¿…è¦çš„å…¨æ³¨æ„åŠ›åˆ™æµªè´¹è®¡ç®—èµ„æºã€‚

å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç°**æµ‹è¯•æ—¶è‡ªé€‚åº”çš„ç¨€ç–ç­–ç•¥**ï¼Œæ˜¯å½“å‰çš„å…³é”®æŒ‘æˆ˜ã€‚

---

### âœ… æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

æœ¬æ–‡æå‡º **Elastic Attention**ï¼Œä¸€ç§**æµ‹è¯•æ—¶è‡ªé€‚åº”ç¨€ç–æ¯”ç‡**çš„æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> **è®©æ¨¡å‹åœ¨æ¨ç†å‰è‡ªåŠ¨è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼Œå¹¶æ®æ­¤åŠ¨æ€åˆ†é…æ³¨æ„åŠ›å¤´çš„è®¡ç®—æ¨¡å¼ï¼ˆFA æˆ– SAï¼‰ã€‚**

å…·ä½“åˆ›æ–°ç‚¹å¦‚ä¸‹ï¼š

1. **å¼•å…¥è½»é‡çº§çš„ Attention Router æ¨¡å—**  
   - æ’å…¥åˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œ**ä¸ä¿®æ”¹ä¸»å¹²å‚æ•°**ï¼ˆbackbone-freezeï¼‰ã€‚
   - æ¥æ”¶è¾“å…¥çš„ Key éšè—çŠ¶æ€ï¼Œè¾“å‡ºæ¯ä¸ª attention head åº”ä½¿ç”¨çš„è®¡ç®—æ¨¡å¼ï¼ˆFA æˆ– SAï¼‰ã€‚

2. **åŸºäº Mixture-of-Experts (MoE) çš„è·¯ç”±æœºåˆ¶**  
   - ç±»ä¼¼ MoE çš„é—¨æ§æœºåˆ¶ï¼Œå¯¹æ¯ä¸ª head è¿›è¡ŒäºŒåˆ†ç±»å†³ç­–ã€‚
   - ä½¿ç”¨ **Gumbel-Softmax + STE (Straight-Through Estimator)** å®ç°ç«¯åˆ°ç«¯è®­ç»ƒï¼Œè§£å†³ç¦»æ•£è·¯ç”±å¸¦æ¥çš„æ¢¯åº¦ä¸å¯å¯¼é—®é¢˜ã€‚

3. **åŒºåˆ†ä¸¤ç±»ä»»åŠ¡ä»¥ç®€åŒ–é…ç½®ç©ºé—´**  
   - å°†ä¸‹æ¸¸ä»»åŠ¡åˆ’åˆ†ä¸ºä¸¤ç±»ï¼š
     - **Sparsity-Robust Tasks**ï¼ˆå¦‚ Summarizationï¼‰ï¼šå¯å®¹å¿é«˜ç¨€ç–ã€‚
     - **Sparsity-Sensitive Tasks**ï¼ˆå¦‚ QAï¼‰ï¼šéœ€ä¿ç•™æ›´å¤š FA å¤´ã€‚
   - æ¨¡å‹åªéœ€åˆ¤æ–­ä»»åŠ¡ç±»åˆ«å¹¶åˆ†é…ç›¸åº”ç¨€ç–æ°´å¹³ï¼Œè€Œéä¸ºæ¯é¡¹ä»»åŠ¡å•ç‹¬è°ƒå‚ã€‚

4. **é«˜æ•ˆçš„èåˆå†…æ ¸ï¼ˆFused Kernelï¼‰è®¾è®¡**  
   - æ”¯æŒåœ¨åŒä¸€å±‚ä¸­æ··åˆæ‰§è¡Œ FA å’Œ SA headï¼Œé¿å…å¤šæ¬¡ kernel launch å’Œå†…å­˜æ‹·è´ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚

---

### âœ… ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | Elastic Attention | ä¼ ç»Ÿ Hybrid æ–¹æ³•ï¼ˆå¦‚ DuoAttention, PruLongï¼‰ |
|------|-------------------|---------------------------------------------|
| **ç¨€ç–ç­–ç•¥** | åŠ¨æ€ã€è¾“å…¥æ„ŸçŸ¥ï¼ˆtest-time adaptiveï¼‰ | é™æ€ã€å›ºå®šæ¯”ä¾‹ï¼ˆstatic ratioï¼‰ |
| **æ³›åŒ–èƒ½åŠ›** | è‡ªåŠ¨é€‚é…ä¸åŒä»»åŠ¡ç±»å‹ | éœ€é’ˆå¯¹æ¯ä¸ªä»»åŠ¡æ‰‹åŠ¨è°ƒä¼˜ |
| **éƒ¨ç½²çµæ´»æ€§** | æ— éœ€é‡æ–°è®­ç»ƒå³å¯åº”å¯¹æ–°ä»»åŠ¡ | æ–°ä»»åŠ¡å¯èƒ½éœ€è¦é‡æ–°å¾®è°ƒ |
| **å‚æ•°å¼€é”€** | æä½ï¼ˆæ¯å±‚ä»…å¢åŠ çº¦ 0.27M å‚æ•°ï¼‰ | é€šå¸¸éœ€æ›´æ–°éƒ¨åˆ†ä¸»å¹²å‚æ•° |
| **è®­ç»ƒæˆæœ¬** | ä»…éœ€ 12 å°æ—¶ï¼ˆ8Ã—A800ï¼‰ | ç±»ä¼¼æˆ–æ›´é«˜ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

- **LongBench-E** (Bai et al., 2024)ï¼šçœŸå®ä¸–ç•Œé•¿ä¸Šä¸‹æ–‡åŸºå‡†ï¼Œæ¶µç›– 6 ç±»ä»»åŠ¡ï¼š
  - å•æ–‡æ¡£ QAï¼ˆS-Doc QAï¼‰
  - å¤šè·³ QAï¼ˆM-Doc QAï¼‰
  - æ‘˜è¦ï¼ˆSummarizationï¼‰
  - ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learningï¼‰
  - åˆæˆæ£€ç´¢ï¼ˆSyntheticï¼‰
  - ä»£ç è¡¥å…¨ï¼ˆCodeï¼‰

- **RULER** (Hsieh et al., 2024)ï¼šç”¨äºè¯„ä¼°**é•¿åº¦å¤–æ¨èƒ½åŠ›**ï¼Œæµ‹è¯•ä» 8K åˆ° 256K tokens ä¸ç­‰çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚

- **LongBench-v2** (Bai et al., 2025)ï¼šæ›´å¤æ‚çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†åŸºå‡†ï¼ŒåŒ…å«é«˜è¾¾ 2M words çš„ä¸Šä¸‹æ–‡ã€‚

- **è®­ç»ƒæ•°æ®**ï¼šæ¥è‡ª ChatQA2-Long-SFT-dataã€MuSiQueã€CoLT-132Kã€GovReportã€XSumï¼Œè¦†ç›–ç¨€ç–æ•æ„Ÿä¸é²æ£’ä»»åŠ¡ï¼Œæ€» token æ•°çº¦ 0.74Bã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### ä¸»å¹²æ¨¡å‹ï¼ˆBackbone Modelsï¼‰
- Qwen3-4B / Qwen3-8B
- Llama-3.1-8B-Instruct

#### ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼ˆSA Patternsï¼‰
- **Streaming Sparse Attention (SSA)** (Xiao et al., 2024b)
- **XAttention (XA)** (Xu et al., 2025)

#### è¯„ä¼°æŒ‡æ ‡
- **Perf.**ï¼šå„é¡¹ä»»åŠ¡çš„å¹³å‡å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
- **Î©MsR (Model Sparsity Ratio)**ï¼šä½¿ç”¨ SA æ¨¡å¼çš„ attention head æ¯”ä¾‹ï¼ˆè¶Šé«˜è¶Šé«˜æ•ˆï¼‰
- **Î©EsR (Effective Sparsity Ratio)**ï¼šå®é™…å…³æ³¨ token çš„æ¯”ä¾‹ï¼ˆåæ˜ æœ‰æ•ˆç¨€ç–ç¨‹åº¦ï¼‰
- **Speedup**ï¼šæ¨ç†åŠ é€Ÿæ¯”
- **Avg. Perf.**ï¼šè·¨ä»»åŠ¡å¹³å‡æ€§èƒ½

#### è®­ç»ƒç»†èŠ‚
- ä½¿ç”¨ 8Ã—A800 GPUï¼Œè®­ç»ƒæ—¶é—´ â‰¤12 å°æ—¶
- Backbone å‚æ•°å†»ç»“ï¼Œä»…è®­ç»ƒ **Attention Router**
- ä½¿ç”¨ **Gumbel-Softmax + STE** è¿›è¡Œç¦»æ•£è·¯ç”±è®­ç»ƒ
- ä¼˜åŒ–ç›®æ ‡åŒ…å«è¯­è¨€å»ºæ¨¡æŸå¤± + ç¨€ç–æ­£åˆ™é¡¹ï¼ˆå¸¦å¯å­¦ä¹  Lagrange multiplierï¼‰

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿æ–¹æ³• | ç‰¹ç‚¹ |
|--------|------|
| **InfLLM-V2** | å¯†é›†-ç¨€ç–åˆ‡æ¢æœºåˆ¶ï¼Œä¾èµ–ä¸Šä¸‹æ–‡é•¿åº¦é˜ˆå€¼ |
| **DuoAttention** | å›ºå®šæ¯”ä¾‹çš„ FA + SSA æ··åˆå¤´ |
| **PruLong** | å­¦ä¹ é‡è¦ token å¹¶ä¿ç•™ |
| **MoBA / NSA** | åŸºäº block çš„ç¨€ç–æ³¨æ„åŠ›ï¼Œæœ‰ç¡¬ä»¶çº¦æŸ |
| **XAttention** | æ— è®­ç»ƒçš„å¯å‘å¼ç¨€ç–æ¨¡å¼ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 & Table 2ï¼‰

#### åœ¨ **LongBench-E** ä¸Šçš„è¡¨ç°ï¼ˆQwen3-8B ä¸»å¹²ï¼‰

| æ–¹æ³• | Avg. Perf. | Î©MsR |
|------|-----------|-------|
| Qwen3-8B (Full FA) | 52.16 | â€“ |
| + InfLLM-V2 | 49.03 | â€“ |
| + PruLong | 48.66 | 0.70 |
| + Elastic Attention (FA-SSA) | **51.51** | 0.69 |
| + Elastic Attention (FA-XA) | **51.66** | 0.80 |

âœ… **ç»“è®º**ï¼šElastic Attention åœ¨ä¿æŒç”šè‡³è¶…è¶Šå¤šæ•°åŸºçº¿æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æ›´é«˜çš„ç¨€ç–ç‡ï¼ˆÎ©MsR è¾¾ 0.8ï¼‰ï¼Œè¯´æ˜å…¶èƒ½æ™ºèƒ½åœ°èŠ‚çœè®¡ç®—ã€‚

---

#### åœ¨ **RULER** ä¸Šçš„é•¿åº¦å¤–æ¨è¡¨ç°ï¼ˆæœ€é•¿è¾¾ 256K tokensï¼‰

| æ–¹æ³• | 256K æ€§èƒ½ | åŠ é€Ÿæ¯” |
|------|----------|--------|
| MoBA / NSA | å‡ ä¹å´©æºƒï¼ˆæ¥è¿‘ 0ï¼‰ | <1.0Ã— |
| InfLLM-V2 | å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰ | â€“ |
| **Elastic Attention (FA-XA)** | **68.51** | **1.51Ã—** |
| **Elastic Attention (XA-SSA)** | **47.68** | **3.28Ã—** |

âœ… **ç»“è®º**ï¼šElastic Attention æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œåœ¨æç«¯é•¿åº¦ä¸‹ä»ä¿æŒå¯ç”¨æ€§ï¼Œå¹¶ä¸”å¯é€šè¿‡é…ç½®ï¼ˆFA-XA vs XA-SSAï¼‰çµæ´»æƒè¡¡**ç²¾åº¦ä¸é€Ÿåº¦**ã€‚

---

#### åœ¨ **LongBench-v2** ä¸Šçš„ç»“æœ

| æ–¹æ³• | Easy | Hard | Avg. Perf. | Î©MsR |
|------|------|------|------------|-------|
| Qwen3-8B | 39.33 | 27.82 | 31.97 | â€“ |
| + Elastic Attention (FA-SSA) | 37.33 | **31.20** | **33.41** | 0.66 |
| + Elastic Attention (FA-XA) | **30.68** | 30.77 | 30.74 | 0.78 |

âœ… **ç»“è®º**ï¼šå°¤å…¶åœ¨ Hard è®¾ç½®ä¸‹ï¼ŒElastic Attention æ˜æ˜¾ä¼˜äºåŸå§‹æ¨¡å‹å’Œå…¶ä»–åŸºçº¿ï¼Œè¯æ˜å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ï¼ˆ1ï¼‰Target Sparsity $t$ çš„å½±å“ï¼ˆFigure 7ï¼‰

- è®¾å®š sparsity-sensitive ä»»åŠ¡çš„ç›®æ ‡ç¨€ç–åº¦ $t_{sen}=0.7$
- ç»“æœæ˜¾ç¤ºæ¨¡å‹èƒ½è‡ªåŠ¨å­¦ä¹ å·®å¼‚åŒ–åˆ†é…ï¼š
  - QA ç±»ä»»åŠ¡ $\Omega_{MsR} \approx 0.68$
  - Code ç±»ä»»åŠ¡ $\Omega_{MsR} \approx 0.85$
- è¡¨æ˜æ¨¡å‹æˆåŠŸè¯†åˆ«äº†ä»»åŠ¡æ•æ„Ÿæ€§å·®å¼‚ã€‚

#### ï¼ˆ2ï¼‰Attention Router è®¾è®¡åˆ†æï¼ˆTable 3ï¼‰

| MLP éšè—å±‚å¤§å° | Avg. Perf. |
|---------------|-----------|
| 2Ã—dâ€™ | 45.22 |
| 4Ã—dâ€™ (**é»˜è®¤**) | **45.92** |
| 8Ã—dâ€™ | 46.40ï¼ˆç•¥é«˜ä½†å‚æ•°ç¿»å€ï¼‰ |

â¡ï¸ é»˜è®¤è®¾ç½®å·²åœ¨æ€§èƒ½ä¸å‚æ•°é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚

#### ï¼ˆ3ï¼‰è·¯ç”±è¡Œä¸ºå¯è§†åŒ–ï¼ˆFigure 6ï¼‰

- å‘ç°æŸäº› head å§‹ç»ˆè¢«åˆ†é…ä¸º FAï¼ˆç±»ä¼¼ retrieval headsï¼‰
- éƒ¨åˆ† head æ ¹æ®ä»»åŠ¡åŠ¨æ€åˆ‡æ¢
- éªŒè¯äº†â€œå›ºå®š retrieval head + åŠ¨æ€ç¨€ç–â€æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **ä»»åŠ¡å¤©ç„¶å¯åˆ†ä¸ºç¨€ç–é²æ£’ä¸ç¨€ç–æ•æ„Ÿä¸¤ç±»**  
   - åªéœ€åŒºåˆ†è¿™ä¸¤ç±»å³å¯å®ç°æœ‰æ•ˆçš„ç¨€ç–æ§åˆ¶ï¼Œæ— éœ€ä¸ºæ¯ä¸ªä»»åŠ¡å•ç‹¬è°ƒå‚ã€‚

2. **Elastic Attention èƒ½åœ¨æµ‹è¯•æ—¶åŠ¨æ€è°ƒæ•´ç¨€ç–æ¯”ä¾‹**  
   - é€šè¿‡è½»é‡çº§ **Attention Router** å®ç°è¾“å…¥æ„ŸçŸ¥çš„ head åˆ†é…ã€‚
   - åœ¨æ¨ç†å‰ Prefill é˜¶æ®µå®Œæˆå†³ç­–ï¼Œä¸å½±å“ç”Ÿæˆæ•ˆç‡ã€‚

3. **æ€§èƒ½ä¸æ•ˆç‡å…¼å¾—**  
   - åœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡åŸºå‡†ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡åŸºçº¿æ€§èƒ½ã€‚
   - å®ç°é«˜è¾¾ 3.28Ã— çš„æ¨ç†åŠ é€Ÿï¼ˆXA-SSA é…ç½®ï¼‰ã€‚
   - é€‚ç”¨äºä¸åŒè§„æ¨¡æ¨¡å‹ï¼ˆ4B~8Bï¼‰å’Œå¤šç§ SA æ¨¡å¼ï¼ˆSSA/XAï¼‰ã€‚

4. **æä½è®­ç»ƒæˆæœ¬ä¸éƒ¨ç½²å‹å¥½**  
   - ä»…éœ€ 12 å°æ—¶è®­ç»ƒï¼Œä¸ä¿®æ”¹ä¸»å¹²å‚æ•°ã€‚
   - èåˆå†…æ ¸è®¾è®¡æ¶ˆé™¤è°ƒåº¦å¼€é”€ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

1. **ä¾èµ–äº retrieval head çš„å­˜åœ¨å‡è®¾**  
   - æ–¹æ³•å‡è®¾æ¨¡å‹ä¸­å­˜åœ¨å¯ç”¨äºå…¨å±€ä¿¡æ¯æå–çš„ FA headï¼Œè‹¥ä¸»å¹²æ¨¡å‹æœ¬èº«ä¸å…·å¤‡æ­¤ç‰¹æ€§ï¼Œæ•ˆæœå¯èƒ½å—é™ã€‚

2. **å¯¹è¾“å…¥æˆªæ–­æ•æ„Ÿ**  
   - å½“å‰é‡‡ç”¨è¾¹ç•Œ poolingï¼ˆé¦–å°¾å„å– 100 tokenï¼‰ä½œä¸ºä»»åŠ¡è¡¨ç¤ºï¼Œè‹¥å…³é”®æŒ‡ä»¤ä¸åœ¨è¾¹ç•ŒåŒºåŸŸï¼Œå¯èƒ½å¯¼è‡´è¯¯åˆ¤ã€‚

3. **ç›®å‰ä»…æ”¯æŒå•å±‚ç»Ÿä¸€è·¯ç”±å†³ç­–**  
   - æ‰€æœ‰å±‚å…±äº«ç›¸åŒçš„ Î©MsR æ§åˆ¶é€»è¾‘ï¼Œæœªæ¢ç´¢é€å±‚å·®å¼‚åŒ–è·¯ç”±çš„å¯èƒ½æ€§ã€‚

4. **å°šæœªéªŒè¯åœ¨å¤šæ¨¡æ€æˆ–éæ–‡æœ¬ä»»åŠ¡ä¸Šçš„æ³›åŒ–æ€§**  
   - å½“å‰å®éªŒé›†ä¸­åœ¨çº¯æ–‡æœ¬é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **æ‰©å±•è‡³å¤šç²’åº¦è·¯ç”±æœºåˆ¶**  
   - å¦‚æ”¯æŒæŒ‰å±‚ã€æŒ‰ä½ç½®ã€æŒ‰ token åŠ¨æ€é€‰æ‹©ç¨€ç–æ¨¡å¼ã€‚

2. **æ¢ç´¢å®Œå…¨å…è®­ç»ƒçš„å¼¹æ€§è·¯ç”±**  
   - åˆ©ç”¨ prompt engineering æˆ– probing æŠ€æœ¯æ›¿ä»£è½»é‡è®­ç»ƒã€‚

3. **åº”ç”¨äº MoE æˆ– State Space Models (SSMs)**  
   - å°† elastic routing æ€æƒ³è¿ç§»åˆ°å…¶ä»–é«˜æ•ˆæ¶æ„ä¸­ã€‚

4. **æ„å»ºé€šç”¨çš„ Sparsity Controller API**  
   - æä¾›å¯æ’æ‹”æ¨¡å—ï¼Œæ”¯æŒä»»æ„ LLM å¿«é€Ÿé›†æˆã€‚

5. **ç ”ç©¶ç¨€ç–æ¨¡å¼ä¸æ¨¡å‹å¹»è§‰çš„å…³ç³»**  
   - æ¢ç´¢æ˜¯å¦åˆç†çš„ç¨€ç–ç­–ç•¥å¯ä»¥æŠ‘åˆ¶é•¿ä¸Šä¸‹æ–‡ä¸­å› ä¿¡æ¯ä¸¢å¤±å¼•å‘çš„é”™è¯¯æ¨ç†ã€‚

---

> **ä¸€å¥è¯æ€»ç»“**ï¼š  
> Elastic Attention æå‡ºäº†ä¸€ç§**è½»é‡ã€é«˜æ•ˆã€è‡ªé€‚åº”**çš„ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶ï¼Œé€šè¿‡ä¸€ä¸ªå¯è®­ç»ƒçš„ **Attention Router** å®ç°æµ‹è¯•æ—¶åŠ¨æ€ç¨€ç–æ§åˆ¶ï¼Œåœ¨å‡ ä¹é›¶ä¸»å¹²æ”¹åŠ¨çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†é•¿ä¸Šä¸‹æ–‡ LLM çš„æ¨ç†æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚

</details>

---

### 13. [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)

**Authors**: Henry Bell, Caroline Zhang, Mohammed Mobasserul Haque, Dhaval Potdar, Samia Zaman, Brandon Fain  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.18730v1  

#### Abstract
The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF)...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ä¸»æµçš„ **alignment** æŠ€æœ¯ï¼ˆå¦‚ RLHFã€DPOï¼‰ä¾èµ–äºå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œå¾®è°ƒï¼ˆparameter fine-tuningï¼‰ï¼Œå­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š
- **è®¡ç®—æˆæœ¬é«˜**ï¼šéœ€è¦å¤§é‡è®­ç»ƒèµ„æºã€‚
- **ä¾èµ–æ ‡æ³¨æ•°æ®**ï¼šéœ€æ”¶é›†äººç±»æˆ– AI ç”Ÿæˆçš„åå¥½æ•°æ®ï¼Œéš¾ä»¥è·å–ä¸”æˆæœ¬é«˜æ˜‚ã€‚
- **ç¼ºä¹çµæ´»æ€§**ï¼šéš¾ä»¥å¿«é€Ÿé€‚é…æ–°çš„ã€ç‰¹å®šåœºæ™¯ä¸‹çš„ä»·å€¼åŸåˆ™ï¼ˆä¾‹å¦‚ä¸åŒæ–‡åŒ–ã€é¢†åŸŸæˆ–ç”¨æˆ·ç¾¤ä½“çš„éœ€æ±‚ï¼‰ã€‚
- **é»‘ç®±åŒ–ä¸¥é‡**ï¼šåŸºäºå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ç¼ºä¹é€æ˜åº¦ï¼Œéš¾ä»¥è§£é‡Šä¸ºä½•æŸä¸ªè¾“å‡ºè¢«åˆ¤å®šä¸ºâ€œå¯¹é½â€ã€‚

å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨è§£å†³å¦‚ä½•åœ¨**ä¸è¿›è¡Œä»»ä½•å‚æ•°å¾®è°ƒã€æ— éœ€é¢å¤–è®­ç»ƒæ•°æ®**çš„å‰æä¸‹ï¼Œå®ç°å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡Œä¸ºçš„åŠ¨æ€ã€å¯è§£é‡Šã€å¤šåŸåˆ™çš„å¯¹é½æ§åˆ¶ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šREFLECT
ä½œè€…æå‡º **REFLECT** â€”â€” ä¸€ç§å®Œå…¨åœ¨æ¨ç†æ—¶ï¼ˆinference-timeï¼‰æ‰§è¡Œçš„ã€åŸºäºè‡ªç„¶è¯­è¨€åŸåˆ™å¼•å¯¼çš„é€æ˜æ¨ç†æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæµç¨‹åˆ†ä¸ºå››ä¸ªé˜¶æ®µï¼š

1. **Constitution-Conditioned Base Response (åˆå§‹å“åº”)**  
   åœ¨ç³»ç»Ÿæç¤ºä¸­ä¼ å…¥ä¸€ç»„ç”¨è‡ªç„¶è¯­è¨€ä¹¦å†™çš„ **principles**ï¼ˆæ„æˆä¸€ä¸ªâ€œå®ªæ³•â€ Constitutionï¼‰ï¼ŒæŒ‡å¯¼æ¨¡å‹ç”Ÿæˆåˆæ­¥å›åº”ã€‚

2. **Self-Evaluation (è‡ªæˆ‘è¯„ä¼°)**  
   æ¨¡å‹å¯¹è‡ªèº«ç”Ÿæˆçš„å“åº”é€æ¡è¯„ä¼°æ˜¯å¦ç¬¦åˆæ¯é¡¹åŸåˆ™ï¼Œå¹¶ç»™å‡º 1â€“5 åˆ†çš„ Likert è¯„åˆ†ã€‚

3. **Self-Critique (è‡ªæˆ‘æ‰¹åˆ¤)**  
   å¯¹å¾—åˆ†ä½äºé˜ˆå€¼çš„åŸåˆ™è¿›è¡Œæ·±å…¥åˆ†æï¼ŒæŒ‡å‡ºå…·ä½“è¿åä¹‹å¤„åŠæ”¹è¿›å»ºè®®ã€‚

4. **Final Revision (æœ€ç»ˆä¿®è®¢)**  
   åŸºäºæ‰¹åˆ¤å†…å®¹ç”Ÿæˆæ”¹è¿›åçš„æœ€ç»ˆè¾“å‡ºã€‚

è¯¥è¿‡ç¨‹å®Œå…¨é€šè¿‡ **in-context reasoning** å®Œæˆï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ RLHF/DPOï¼‰ | REFLECT |
|------|------------------------|-------|
| æ˜¯å¦éœ€è¦è®­ç»ƒ | âœ… éœ€è¦å‚æ•°å¾®è°ƒ | âŒ ä¸éœ€è¦ï¼Œçº¯æ¨ç†æ—¶æ“ä½œ |
| æ˜¯å¦éœ€è¦æ ‡æ³¨æ•°æ® | âœ… éœ€è¦åå¥½æ•°æ® | âŒ åªéœ€æä¾›åŸåˆ™æ–‡æœ¬ |
| çµæ´»æ€§ | ä½ï¼ˆå›ºå®šå¯¹é½ç›®æ ‡ï¼‰ | é«˜ï¼ˆå¯å³æ’å³ç”¨åˆ‡æ¢ä¸åŒå®ªæ³•ï¼‰ |
| é€æ˜æ€§ | ä½ï¼ˆé»‘ç®±å¥–åŠ±æ¨¡å‹ï¼‰ | é«˜ï¼ˆæ˜¾å¼è‡ªç„¶è¯­è¨€æ¨ç†è½¨è¿¹ï¼‰ |
| å°¾éƒ¨å®‰å…¨è¡¨ç° | å¹³å‡è¡¨ç°å¥½ï¼Œå°¾éƒ¨é£é™©ä»å­˜ | æ˜¾è‘—é™ä½ä¸¥é‡è¿è§„ç‡ï¼ˆtail-case violationsï¼‰ |
| æ•°æ®å¤ç”¨èƒ½åŠ› | â€”â€” | âœ… è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ SFT/DPO è®­ç»ƒæ•°æ® |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **PKU-Safe-RLHF Dataset**  
  åŒ…å« 500 ä¸ªå•è½®å¯¹æŠ—æ€§æç¤ºï¼ˆred-teaming promptsï¼‰ï¼Œç”¨äºæµ‹è¯•å®‰å…¨æ€§å¯¹é½ã€‚
- **Anthropic HH-RLHF Dataset**  
  åŒ…å«å¤šè½®å¯¹è¯æç¤ºï¼Œå…± 500 æ¡ï¼Œç”¨äºè¯„ä¼°é€šç”¨å¯¹é½èƒ½åŠ›ã€‚
- **LMSYS-Chat-1M Dataset**  
  ä»ä¸­é‡‡æ · 2000 æ¡çœŸå®èŠå¤©è®°å½•ï¼Œç”¨äºæ¨¡æ‹Ÿéƒ¨ç½²å¹¶ç”Ÿæˆè‡ªç›‘ç£å¾®è°ƒæ•°æ®ã€‚

### å®éªŒè®¾ç½®
- **æ¨¡å‹å®¶æ—**ï¼š
  - å•†ä¸šå…ˆè¿›æ¨¡å‹ï¼š`GPT-4.1-Mini`, `Claude-Haiku-3.5`
  - å¼€æºè¾ƒå°æ¨¡å‹ï¼š`Mistral-7B-Instruct-v0.3`
- **è¯„ä¼°æ–¹å¼**ï¼šé‡‡ç”¨ **LLM-as-a-judge** èŒƒå¼ï¼Œä½¿ç”¨æ›´å¼ºçš„ `GPT-4.1` ä½œä¸ºè£åˆ¤æ¨¡å‹ï¼Œå¯¹æ¯ä¸ªè¾“å‡ºæŒ‰æ¯æ¡ principle æ‰“åˆ†ï¼ˆ1â€“5 åˆ†ï¼‰ã€‚
- **äºŒå…ƒç¼–ç **ï¼šå°†å¾—åˆ† 1â€“2 è§†ä¸ºâ€œè¿ååŸåˆ™â€ï¼Œ3â€“5 è§†ä¸ºâ€œæœªè¿åâ€ã€‚
- **äººå·¥éªŒè¯**ï¼šæ‹›å‹Ÿ 50 å Prolific ç”¨æˆ·è¿›è¡Œäººå·¥æ‰“åˆ†ï¼ŒéªŒè¯ LLM åˆ¤å®˜çš„ä¸€è‡´æ€§ï¼ˆhuman-human MAD â‰ˆ 0.94â€“1.09ï¼›LLM-human MAD â‰ˆ 1.03â€“1.71ï¼‰ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Baseline**ï¼šä»…ä½¿ç”¨ constitution-conditioned promptingï¼ˆCCBaseï¼‰ï¼Œæ— åç»­åæ€æœºåˆ¶ã€‚
- **REFLECT**ï¼šå®Œæ•´å››æ­¥æµç¨‹ã€‚
- **CA_SELFREFINE**ï¼šå¯¹ Self-Refine æ–¹æ³•è¿›è¡Œæ”¹é€ ä»¥æ”¯æŒå®ªæ³•åŸåˆ™ï¼Œä½œä¸ºæ›¿ä»£ self-correction æ–¹æ³•çš„å¯¹ç…§ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 3ï¼‰
| Dataset | Model | Avg. Likert Score (CCBase â†’ REFLECT) | Principle Violation Rate â†“ |
|--------|--------|-------------------------------|--------------------------|
| SafeRLHF | GPT-4.1-Mini | 4.55 â†’ **4.65** (+0.098) | 7.7% â†’ **3.45%** (-4.27pp) |
| SafeRLHF | Claude-Haiku-3.5 | 2.89 â†’ **4.15** (+1.26) | 47.3% â†’ **13.3%** (-34.0pp) |
| HH-RLHF | GPT-4.1-Mini | 4.22 â†’ **4.60** (+0.37) | 10.4% â†’ **2.0%** (-8.36pp) |

> æ³¨ï¼šæ‰€æœ‰æå‡å‡ç» **paired bootstrap test** éªŒè¯ï¼Œp < 0.0001ï¼Œç»Ÿè®¡æ˜¾è‘—ã€‚

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **ç›¸æ¯” CCBaseï¼ˆä»…æç¤ºå·¥ç¨‹ï¼‰**ï¼š
  - åœ¨æ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡æ˜¾è‘—æå‡å¹³å‡å¯¹é½åˆ†æ•°ã€‚
  - ç‰¹åˆ«æ˜¯åœ¨åŸæœ¬å¯¹é½è¾ƒå·®çš„æ¨¡å‹ï¼ˆå¦‚ Claudeï¼‰ä¸Šæ•ˆæœæœ€æ˜æ˜¾ã€‚
  - å¤§å¹…å‡å°‘â€œä¸¥é‡è¿ååŸåˆ™â€çš„æ¡ˆä¾‹ï¼ˆtail-case mitigationï¼‰ã€‚
- **ç›¸æ¯” CA_SELFREFINE**ï¼ˆè§ Table 5ï¼‰ï¼š
  - å®ç°ç›¸è¿‘ç”šè‡³æ›´å¥½çš„å¯¹é½æ•ˆæœã€‚
  - **Token å¼€é”€ä»…ä¸ºåè€…çš„ 1/3 è‡³ 1/10**ï¼ˆREFLECT: 3.7â€“5.0Ã— baselineï¼›CA_SELFREFINE: 28.2â€“58.2Ã—ï¼‰ã€‚
  - æ›´é«˜æ•ˆåœ°è·³è¿‡å·²è‰¯å¥½å¯¹é½çš„å“åº”ï¼ˆé€šè¿‡ self-evaluation æå‰ç»ˆæ­¢ï¼‰ã€‚

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰è·¨æ¨¡å‹è§„æ¨¡æœ‰æ•ˆæ€§ï¼ˆSection 4.5ï¼‰
åœ¨ GPT-4.1 ç³»åˆ—ä¸åŒå°ºå¯¸æ¨¡å‹ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼š
- å³ä½¿æ˜¯å°å‹æ¨¡å‹ï¼ˆå¦‚ GPT-4.1-Nanoï¼‰ï¼ŒREFLECT ä¹Ÿèƒ½å¸¦æ¥æ˜¾è‘—æå‡ï¼ˆ+0.51 åˆ†ï¼‰ã€‚
- è¡¨æ˜ REFLECT å¯¹æ¨¡å‹å¤§å°é²æ£’ï¼Œå°æ¨¡å‹ä¹Ÿå¯å—ç›Šã€‚

#### ï¼ˆ2ï¼‰äº‹å®æ¨ç†ä¿ç•™èƒ½åŠ›ï¼ˆSection 4.4ï¼‰
åœ¨ **GSM8K** å’Œ **MMLU** ä¸Šæµ‹è¯• REFLECT å¯¹äº‹å®æ¨ç†çš„å½±å“ï¼š
| Benchmark | Base | CCBase | REFLECT |
|---------|------|--------|--------|
| GSM8K | 94.69% | 95.00% | **95.07%** |
| MMLU (test) | 87.60% | 85.80% | **85.80%** |

> ç»“è®ºï¼šREFLECT **ä¸å½±å“ç”šè‡³è½»å¾®æå‡** äº‹å®å‡†ç¡®æ€§ï¼Œè¯´æ˜å…¶ä¸ä¼šå› å¼ºè°ƒåŸåˆ™è€Œç‰ºç‰²ä»»åŠ¡æ€§èƒ½ã€‚

#### ï¼ˆ3ï¼‰ç”Ÿæˆè®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ï¼ˆSection 4.3ï¼‰
åˆ©ç”¨ REFLECT è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ `(base response, revised response)` å¯¹è¿›è¡Œå¾®è°ƒï¼š
- **SFT å¾®è°ƒæ¨¡å‹**ï¼šåœ¨æ–°æ•°æ®ä¸Šè¾¾åˆ°å¹³å‡ **4.60** åˆ†ï¼Œæ¥è¿‘å®Œæ•´ REFLECT æµç¨‹çš„è¡¨ç°ã€‚
- **DPO å¾®è°ƒæ¨¡å‹**ï¼šä¹Ÿæœ‰ä¸€å®šæå‡ï¼Œä½†ä¸å¦‚ SFT æ˜æ˜¾ã€‚
- è¯æ˜ REFLECT å¯ä½œä¸º**ä½æˆæœ¬æ•°æ®å¼•æ“**ï¼Œæ¨åŠ¨é•¿æœŸéƒ¨ç½²ä¸­çš„è¿­ä»£ä¼˜åŒ–ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **REFLECT æ˜¾è‘—æå‡å¤šåŸåˆ™å¯¹é½æ°´å¹³**ï¼Œå°¤å…¶åœ¨åŸæœ¬æœªé’ˆå¯¹è¿™äº›åŸåˆ™å¾®è°ƒè¿‡çš„æ¨¡å‹ä¸Šæ•ˆæœçªå‡ºã€‚
2. âœ… **ç‰¹åˆ«æ“…é•¿ç¼“è§£å°¾éƒ¨é£é™©**ï¼šå³ä½¿å¹³å‡å¾—åˆ†å˜åŒ–ä¸å¤§ï¼Œä¹Ÿèƒ½å¤§å¹…é™ä½ä¸¥é‡è¿è§„çš„å‘ç”Ÿé¢‘ç‡ï¼Œè¿™å¯¹å®‰å…¨å…³é”®åœºæ™¯è‡³å…³é‡è¦ã€‚
3. âœ… **ä¿æŒäº‹å®æ¨ç†èƒ½åŠ›ä¸å˜**ï¼šå¼•å…¥å¤æ‚åŸåˆ™ä¸ä¼šæŸå®³æ¨¡å‹çš„åŸºç¡€è®¤çŸ¥èƒ½åŠ›ã€‚
4. âœ… **å…·å¤‡è‰¯å¥½çš„æ³›åŒ–æ€§å’Œæ•ˆç‡**ï¼šé€‚ç”¨äºä¸åŒè§„æ¨¡æ¨¡å‹ï¼Œä¸”è®¡ç®—å¼€é”€å¯æ§ã€‚
5. âœ… **å¤©ç„¶ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®**ï¼šå½¢æˆâ€œæ¨ç†æ—¶å¯¹é½ â†’ æ•°æ®äº§å‡º â†’ å‚æ•°å¾®è°ƒ â†’ æ›´å¼ºåŸºç¡€æ¨¡å‹â€çš„æ­£å‘åé¦ˆå¾ªç¯ã€‚
6. âŒ **æ— æ³•ç”¨äºâ€œè¶Šç‹±â€æˆ–â€œå»å¯¹é½â€**ï¼šå°è¯•ç”¨ â€œharmfulnessâ€ åŸåˆ™ä¹Ÿæ— æ³•è®©æ¨¡å‹çœŸæ­£æœ‰å®³ï¼Œè¯´æ˜ REFLECT æœ¬èº«å…·æœ‰å†…åœ¨ç¨³å®šæ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–æ¨¡å‹è‡ªèº«çš„åˆ¤æ–­åŠ›**ï¼šself-evaluation æ­¥éª¤å¯èƒ½è¯¯åˆ¤ï¼ˆfalse positives/negativesï¼‰ï¼Œå½±å“ä¿®è®¢è´¨é‡ã€‚
- **ä¸èƒ½ä¿è¯å®Œç¾åˆè§„**ï¼šä»æœ‰å¯èƒ½åœ¨ä¿®è®¢è¿‡ç¨‹ä¸­å¼•å…¥æ–°çš„åŸåˆ™è¿åã€‚
- **è¯„ä¼°ä¾èµ– LLM Judge**ï¼šè™½ç„¶ç»è¿‡äººå·¥æ ¡å‡†ï¼Œä½†ä»éç»å¯¹å®¢è§‚æ ‡å‡†ã€‚
- **ä»…æµ‹è¯•æœ‰é™åŸåˆ™é›†åˆ**ï¼šå°šæœªæ¢ç´¢æç«¯å¯¹æŠ—æ€§æˆ–æ¨¡ç³Šä¼¦ç†æƒ…å¢ƒä¸‹çš„è¡¨ç°ã€‚
- **æ¨ç†é“¾å¯èƒ½è¯¯å¯¼**ï¼šç”Ÿæˆçš„ reasoning trace å¯èƒ½æ˜¯ post-hoc rationalizationï¼Œè€ŒéçœŸå®å†³ç­–è·¯å¾„ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. è®¾è®¡æ›´å¯é çš„ self-evaluation æœºåˆ¶ï¼Œå‡å°‘è¯¯åˆ¤ã€‚
2. æ¢ç´¢å¦‚ä½•è®­ç»ƒä¸“é—¨ç”¨äº critique å’Œ revision çš„ reasoning modelã€‚
3. ç ”ç©¶ REFLECT åœ¨å¯¹æŠ—æ”»å‡»ï¼ˆadversarial jailbreakï¼‰ä¸‹çš„é²æ£’æ€§ã€‚
4. æ‰©å±•è‡³æ›´å¤šå…ƒçš„æ–‡åŒ–ä»·å€¼è§‚ç©ºé—´ï¼Œæ”¯æŒçœŸæ­£çš„ pluralistic alignmentã€‚
5. æ„å»ºç«¯åˆ°ç«¯çš„â€œå¯¹é½é—­ç¯ç³»ç»Ÿâ€ï¼šå°† REFLECT è¾“å‡ºæŒç»­ç”¨äºç¦»çº¿å¾®è°ƒï¼Œå®ç°è‡ªåŠ¨åŒ–æ¼”è¿›ã€‚

--- 

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> REFLECT æ˜¯ä¸€ç§è½»é‡çº§ã€é€æ˜ã€æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶å¯¹é½æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ˜¾å¼çš„ in-context è‡ªæˆ‘åæ€æœºåˆ¶ï¼Œåœ¨ä¸ç‰ºç‰²äº‹å®å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº† LLM å¯¹å¤æ‚å¤šå…ƒåŸåˆ™çš„éµå¾ªç¨‹åº¦ï¼Œå°¤å…¶æœ‰æ•ˆæŠ‘åˆ¶äº†ç½•è§ä½†ä¸¥é‡çš„å¯¹é½å¤±è´¥ï¼ŒåŒæ—¶è¿˜èƒ½åå“ºè®­ç»ƒæ•°æ®ç”Ÿäº§ï¼Œä¸ºæ„å»ºå¯æŒç»­è¿›åŒ–çš„è´Ÿè´£ä»» AI æä¾›äº†ä¸€æ¡æ–°è·¯å¾„ã€‚

</details>

---

### 14. [FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices](https://arxiv.org/abs/2601.17063)

**Authors**: Byeongju Kim, Jungwan Lee, Donghyeon Han, Hoi-Jun Yoo, Sangyeob Kim  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.17063v1  

#### Abstract
Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠFlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devicesã€‹æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
- **MoEæ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å†…å­˜ç“¶é¢ˆ**ï¼šå°½ç®¡Mixture-of-Experts (MoE) æ¨¡å‹é€šè¿‡ç¨€ç–æ¿€æ´»å®ç°äº†é«˜æ•ˆæ¨ç†ï¼Œä½†å…¶æ€»å‚æ•°é‡å¯è¾¾æ•°ç™¾GBï¼Œè¿œè¶…å…¸å‹è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚æ¡Œé¢çº§PCï¼‰çš„DRAMå®¹é‡ï¼ˆé€šå¸¸ä¸º16â€“64GBï¼‰ã€‚
- **ä¼ ç»Ÿoffloadingæ–¹æ¡ˆä¸é€‚ç”¨äºSSDå±‚çº§å­˜å‚¨**ï¼šç°æœ‰ç³»ç»Ÿï¼ˆå¦‚Fiddlerã€DAOPï¼‰ä¾èµ–å°†ä¸“å®¶å¸è½½åˆ°DRAMï¼Œå‡è®¾æ•´ä¸ªæ¨¡å‹è‡³å°‘å¯é©»ç•™åœ¨ä¸»å­˜ä¸­ï¼Œè¿™åœ¨å®é™…è¾¹ç¼˜åœºæ™¯ä¸‹å·²ä¸å¯è¡Œã€‚
- **é€šç”¨ç¼“å­˜ç­–ç•¥ï¼ˆLRU/LFUï¼‰æ— æ³•æœ‰æ•ˆæ•æ‰MoEçš„è·¯ç”±ç‰¹æ€§**ï¼šä¸“å®¶è®¿é—®å…·æœ‰æ—¶é—´å±€éƒ¨æ€§å’Œé¢‘ç‡æ¨¡å¼ï¼Œä½†LRUæ˜“é€ æˆâ€œEviction Delayâ€å’Œâ€œEvicting Hot Expertsâ€ï¼Œå¯¼è‡´é«˜ä»£ä»·çš„SSD I/Oã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
- **FlashMoEç³»ç»Ÿæ¶æ„**ï¼š
  - å°†**expertä¸non-expertæƒé‡åˆ†ç¦»å­˜å‚¨**ï¼Œä»…åœ¨åˆå§‹åŒ–æ—¶åŠ è½½è½»é‡çš„non-expertéƒ¨åˆ†ï¼ˆAttentionã€Normalizationã€Routing Gateç­‰ï¼‰ï¼Œæ˜¾è‘—é™ä½å¯åŠ¨å»¶è¿Ÿã€‚
  - æ”¯æŒ**æŒ‰éœ€ä»SSDåŠ è½½expert**ï¼Œå®ç°ç»†ç²’åº¦çš„layer-levelå’Œunit-levelä¸“å®¶åŠ è½½ã€‚
- **ML-Based Cache Replacement Policy**ï¼š
  - è®¾è®¡äº†ä¸€ä¸ªåŸºäº**Feed-Forward Network (FFN)** çš„ç¼“å­˜æ›¿æ¢ç­–ç•¥ï¼ŒèåˆLRUï¼ˆrecencyï¼‰å’ŒLFUï¼ˆfrequencyï¼‰ä¿¡å·ã€‚
  - è¾“å…¥ç‰¹å¾åŒ…æ‹¬å½’ä¸€åŒ–çš„**Recency Score**ï¼ˆä¸Šæ¬¡è®¿é—®æ­¥æ•°å€’æ•°ï¼‰å’Œ**Frequency Score**ï¼ˆå†å²è°ƒç”¨æ¬¡æ•°ï¼‰ï¼Œè¾“å‡ºé¢„æµ‹ä¸‹ä¸€ä¸ªä½¿ç”¨è·ç¦»ï¼ŒæŒ‡å¯¼æœ€ä¼˜é©±é€å†³ç­–ã€‚
  - åˆ©ç”¨Beladyâ€™s optimal policyç”Ÿæˆè®­ç»ƒæ ‡ç­¾ï¼Œé€¼è¿‘ç†è®ºæœ€ä¼˜æ›¿æ¢æ—¶æœºã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | FlashMoEä¼˜åŠ¿ |
|------|--------------|
| **å†…å­˜æ•ˆç‡** | ä¸è¦æ±‚å…¨æ¨¡å‹é©»ç•™DRAMï¼Œæ”¯æŒSSDä½œä¸ºlast-level cacheï¼Œé€‚ç”¨äºå†…å­˜å—é™è¾¹ç¼˜è®¾å¤‡ |
| **åŠ è½½å»¶è¿Ÿ** | åˆå§‹åŒ–ä»…åŠ è½½<2GB non-expertæ¨¡å—ï¼Œæ¯”å®Œæ•´åŠ è½½å¿«4â€“6.8Ã— |
| **ç¼“å­˜å‘½ä¸­ç‡** | ML-basedç­–ç•¥ç›¸æ¯”LRUæå‡é«˜è¾¾51%ï¼Œæ˜¾è‘—å‡å°‘SSD I/O |
| **ç«¯åˆ°ç«¯æ€§èƒ½** | å®ç°æœ€é«˜è¾¾2.6Ã—çš„æ¨ç†é€Ÿåº¦åŠ é€Ÿ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **TriviaQA [7]**ï¼šç”¨äºè®­ç»ƒMLç¼“å­˜æ¨¡å‹å’Œè¯„ä¼°æ¨ç†æ€§èƒ½çš„å¤§è§„æ¨¡é—®ç­”æ•°æ®é›†ã€‚
  - è®­ç»ƒé›†ç”¨äºæå–routing traceä»¥è¯†åˆ«â€œpopular expertsâ€å¹¶è®­ç»ƒML cacheã€‚
  - æµ‹è¯•å­é›†ç”¨äºè§£ç é€Ÿåº¦å’Œç¼“å­˜å‘½ä¸­ç‡è¯„ä¼°ï¼Œç¡®ä¿æ— æ•°æ®æ³„éœ²ã€‚

### å®éªŒè®¾ç½®
- **ç¡¬ä»¶å¹³å°**ï¼ˆè§Table 2ï¼‰ï¼š
  - CPU: AMD Ryzen 9 9600X (Zen 5)
  - GPU: NVIDIA RTX 5070 Ti (16GB GDDR7)
  - SSD: SK hynix P51 NVMe (PCIe 5.0, è¯»å–7.4 GB/s)
  - RAM: 64GB DDR5 (ä¿ç•™çº¦1GBå¯ç”¨ï¼Œæ¨¡æ‹Ÿèµ„æºç´§å¼ ç¯å¢ƒ)
  - è½¯ä»¶æ ˆ: Python 3.11 + PyTorch 2.7 + CUDA 12.9

- **è¯„ä¼°æ¨¡å‹**ï¼š
  - **OLMoE-1B-7B**ï¼šå­¦æœ¯MoEæ¨¡å‹
  - **Qwen3-30B-A3B [12]**ï¼šå·¥ä¸šçº§å¤§è§„æ¨¡MoEæ¨¡å‹ï¼ˆæ€»30.3Bå‚æ•°ï¼Œæ¿€æ´»3.3Bï¼‰

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Initial Model Loading Time** | ä»ç£ç›˜åŠ è½½éä¸“å®¶æ¨¡å—çš„æ—¶é—´ |
| **Prefill Latency** | é¦–ä¸ªtokenå¤„ç†é˜¶æ®µçš„æ€»å»¶è¿Ÿï¼ˆå«ä¸“å®¶åŠ è½½ï¼‰ |
| **Cache Hit Rate** | ç¼“å­˜å‘½ä¸­æ¯”ä¾‹ï¼Œç›´æ¥å½±å“SSD I/Oé¢‘æ¬¡ |
| **Token Generation Speed (Throughput)** | å•ä½æ—¶é—´å†…ç”Ÿæˆçš„tokenæ•°é‡ï¼ˆtokens/secï¼‰ |
| **End-to-End Inference Latency** | å®Œæ•´è¯·æ±‚å“åº”æ—¶é—´ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **llama.cpp**ï¼šä¼ ç»ŸLLMæ¨ç†æ¡†æ¶ï¼Œæœªé’ˆå¯¹MoEä¼˜åŒ–
- **Fiddler [8]**ï¼šCPU-GPUååŒè°ƒåº¦MoEæ¨ç†ç³»ç»Ÿ
- **DAOP [13]**ï¼šæ•°æ®æ„ŸçŸ¥å¸è½½ä¸é¢„è®¡ç®—æ¡†æ¶
- **ç»å…¸ç¼“å­˜ç­–ç•¥**ï¼š
  - LRUï¼ˆLeast Recently Usedï¼‰
  - LFUï¼ˆLeast Frequently Usedï¼‰
  - LIFO
- **å…ˆè¿›å­¦ä¹ å‹ç¼“å­˜**ï¼š
  - ARCï¼ˆAdaptive Replacement Cacheï¼‰
  - LeCaRï¼ˆLearning-based Cache Replacementï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… ç¼“å­˜å‘½ä¸­ç‡ï¼ˆFig. 6ï¼‰
| æ¨¡å‹ | æ–¹æ³• | æå‡å¹…åº¦ï¼ˆvs LRUï¼‰ |
|------|------|------------------|
| OLMoE-1B-7B | FlashMoE (ML Cache) | **+21%** |
| Qwen3-30B-A3B | FlashMoE (ML Cache) | **+51%** |
- åŒæ—¶ä¼˜äºLFUï¼ˆ+51%ï¼‰ã€ARCï¼ˆ+28%ï¼‰ã€LeCaRï¼ˆ+21%ï¼‰
- æ˜¾è‘—é™ä½I/Oï¼šå‘½ä¸­ç‡æå‡å¸¦æ¥**22â€“35%çš„I/Oå‡å°‘**

#### âœ… æ¨ç†ååé‡ï¼ˆFig. 7b, 7eï¼‰
| æ¨¡å‹ | æ–¹æ³• | æ€§èƒ½æå‡ï¼ˆvs LRUï¼‰ |
|------|------|------------------|
| OLMoE-1B-7B | FlashMoE | **+22%** |
| Qwen3-30B-A3B | FlashMoE | **+7%** |
- åœ¨é«˜è´Ÿè½½ä¸‹ä»ä¿æŒä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨é•¿åºåˆ—è¾“å…¥æ—¶è¡¨ç°ç¨³å®š

#### âœ… ç³»ç»Ÿçº§ç«¯åˆ°ç«¯åŠ é€Ÿï¼ˆFig. 7c, 7fï¼‰
| å¯¹æ¯”é¡¹ | åŠ é€Ÿæ¯” |
|--------|-------|
| vs Fiddler / DAOP | æœ€é«˜ **2.6Ã— speedup** |
| vs llama.cpp | æœ€é«˜ **4.1Ã— speedup**ï¼ˆprefillé˜¶æ®µï¼‰ |

#### âœ… åˆå§‹åŠ è½½ä¸Prefillå»¶è¿Ÿ
| é¡¹ç›® | FlashMoEä¼˜åŠ¿ |
|------|-------------|
| Initial Load | æ¯”llama.cppå¿« **4Ã—**ï¼Œæ¯”Fiddler/DAOPå¿« **6.8Ã—** |
| Prefill Latency | å³ä½¿è®¡å…¥ä¸“å®¶åŠ è½½ï¼Œä»å¿« **2.5Ã—ï¼ˆvs llama.cppï¼‰**, **4.1Ã—ï¼ˆvs Fiddler/DAOPï¼‰** |
- åŸå› ï¼šä»…åŠ è½½<2GB non-expertæ¨¡å—ï¼Œè€Œéæ•°åGBå®Œæ•´æ¨¡å‹

### æ¶ˆèå®éªŒç»“æœï¼ˆéšå«åˆ†æï¼‰
- **ML Cacheæœ‰æ•ˆæ€§éªŒè¯**ï¼š
  - LRU vs Beladyæœ€ä¼˜ç­–ç•¥å¯¹æ¯”æ˜¾ç¤ºï¼šLRUæœ‰**34.2%çš„è¢«é©±é€ä¸“å®¶åœ¨åç»­5æ­¥å†…è¢«é‡ç”¨**ï¼Œè€ŒBeladyä»…ä¸º0.1%ï¼Œè¯´æ˜LRUå­˜åœ¨ä¸¥é‡è¯¯é©±é€ã€‚
  - åœ¨Qwen3ä¸Šï¼ŒLRUä¸LFUé€‰æ‹©æ›´ä¼˜çš„æ¯”ä¾‹åˆ†åˆ«ä¸º56% vs 44%ï¼Œè¡¨æ˜ä¸¤è€…å„æœ‰ä¼˜åŠ¿ï¼Œ**èåˆäºŒè€…ä¿¡æ¯æ˜¯å¿…è¦ä¸”æœ‰æ•ˆçš„**ã€‚
- **ç¼“å­˜å¤§å°å½±å“**ï¼ˆFig. 6ï¼‰ï¼š
  - æ‰€æœ‰ç­–ç•¥éšç¼“å­˜å¢å¤§å‘½ä¸­ç‡ä¸Šå‡ï¼Œä½†FlashMoEåœ¨å„ç§ç¼“å­˜é…ç½®ä¸‹å‡é¢†å…ˆï¼Œä½“ç°å…¶é²æ£’æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **MoEæ¨¡å‹å¯åœ¨è¾¹ç¼˜è®¾å¤‡è¿è¡Œ**ï¼šé€šè¿‡å°†inactive experts offloadè‡³SSDï¼Œå¹¶ç»“åˆæ™ºèƒ½ç¼“å­˜ç®¡ç†ï¼Œå¯åœ¨ä»…16â€“64GB RAMç¯å¢ƒä¸‹é«˜æ•ˆè¿è¡Œç™¾GBçº§MoEæ¨¡å‹ã€‚
2. **æ–‡ä»¶ç»„ç»‡æ–¹å¼è‡³å…³é‡è¦**ï¼šå°†expertä¸non-expertåˆ†ç¦»å­˜å‚¨ï¼Œå¹¶ä»¥`.pt`æ–‡ä»¶å½¢å¼åˆ†å±‚ä¿å­˜ï¼Œæå¤§æå‡äº†åŠ è½½çµæ´»æ€§å’Œæ•ˆç‡ã€‚
3. **ä¼ ç»Ÿç¼“å­˜ç­–ç•¥ä¸è¶³ä»¥åº”å¯¹MoEè·¯ç”±åŠ¨æ€æ€§**ï¼šLRUå› recency biaså¯¼è‡´é¢‘ç¹é©±é€å³å°†ä½¿ç”¨çš„â€œhot expertsâ€ï¼Œè€ŒLFUå¯¹çªå‘è®¿é—®ä¸æ•æ„Ÿã€‚
4. **è½»é‡çº§MLæ¨¡å‹å³å¯æ˜¾è‘—æå‡ç¼“å­˜æ€§èƒ½**ï¼š
   - ä½¿ç”¨æ¯å±‚ä¸€ä¸ªå°å‹FFNï¼ˆ~113KBï¼‰å³å¯å»ºæ¨¡å¤æ‚è®¿é—®æ¨¡å¼ã€‚
   - è®­ç»ƒå¼€é”€ä½ï¼šåœ¨A100ä¸Šçº¦2å°æ—¶å†…å®Œæˆå…¨æµç¨‹ï¼ˆtraceæ”¶é›† â†’ ç‰¹å¾æ„å»º â†’ è®­ç»ƒï¼‰ã€‚
5. **å¼‚æ­¥æ‰§è¡Œæ©ç›–è®¡ç®—å¼€é”€**ï¼š
   - FlashMoEåˆ©ç”¨SSDåŠ è½½å»¶è¿Ÿï¼ˆ~3msï¼‰å¹¶è¡Œæ‰§è¡ŒML Cacheæ¨ç†ï¼ˆ~0.1msï¼‰ï¼Œå®Œå…¨éšè—é¢å¤–è®¡ç®—æˆæœ¬ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–å†å²è·¯ç”±æ¨¡å¼æ³›åŒ–èƒ½åŠ›**ï¼šè‹¥æµ‹è¯•ä»»åŠ¡ä¸è®­ç»ƒåˆ†å¸ƒå·®å¼‚å¤§ï¼ˆå¦‚é¢†åŸŸè¿ç§»ï¼‰ï¼ŒML Cacheå¯èƒ½å¤±æ•ˆã€‚
- **éœ€è¦é¢„å…ˆé‡‡é›†routing traceè¿›è¡Œè®­ç»ƒ**ï¼šå¢åŠ äº†éƒ¨ç½²å‰å‡†å¤‡æ­¥éª¤ï¼Œå¯¹å®æ—¶è‡ªé€‚åº”èƒ½åŠ›æœ‰ä¸€å®šé™åˆ¶ã€‚
- **å½“å‰ML Cacheä¸ºé™æ€è®­ç»ƒæ¨¡å‹**ï¼šæœªå®ç°åœ¨æ¨ç†è¿‡ç¨‹ä¸­æŒç»­åœ¨çº¿å­¦ä¹ æ›´æ–°ã€‚
- **å¯¹SSDå¸¦å®½é«˜åº¦ä¾èµ–**ï¼šè‹¥SSDæ€§èƒ½ä¸‹é™ï¼ˆå¦‚QLCé¢—ç²’è€åŒ–ï¼‰ï¼Œæ•´ä½“æ”¶ç›Šä¼šå‡å¼±ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **Online Learning for Cache Policy**ï¼šå¼€å‘å¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´çš„è‡ªé€‚åº”MLç¼“å­˜æœºåˆ¶ã€‚
- **è·¨å±‚è”åˆç¼“å­˜å†³ç­–**ï¼šå½“å‰ç­–ç•¥ä¸ºper-layerç‹¬ç«‹å†³ç­–ï¼Œæœªæ¥å¯æ¢ç´¢å…¨å±€è”åˆä¼˜åŒ–ã€‚
- **æ”¯æŒæ›´å¤šè®¾å¤‡ç»„åˆ**ï¼šæ‰©å±•è‡³ç§»åŠ¨ç«¯SoCï¼ˆå¦‚æ‰‹æœºNPU+UFSï¼‰ã€IoTè®¾å¤‡ç­‰æ›´ä½åŠŸè€—å¹³å°ã€‚
- **é›†æˆPrefetchingæœºåˆ¶**ï¼šåŸºäºMLé¢„æµ‹æå‰åŠ è½½æ½œåœ¨è¢«è°ƒç”¨çš„expertï¼Œè¿›ä¸€æ­¥é™ä½missæƒ©ç½šã€‚

--- 

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> FlashMoEé¦–æ¬¡å®ç°äº†åœ¨æ™®é€šæ¡Œé¢çº§ç¡¬ä»¶ä¸Šé«˜æ•ˆè¿è¡Œè¶…å¤§è§„æ¨¡MoEæ¨¡å‹ï¼Œå…¶æ ¸å¿ƒåœ¨äº**ç»“æ„åŒ–æ¨¡å‹æ‹†åˆ† + MLé©±åŠ¨çš„æ™ºèƒ½ç¼“å­˜**ï¼Œè§£å†³äº†SSD I/Oç“¶é¢ˆè¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œä¸ºè¾¹ç¼˜ä¾§éƒ¨ç½²ç™¾äº¿å‚æ•°MoEæä¾›äº†å¯è¡Œè·¯å¾„ã€‚

</details>

---

### 15. [HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models](https://arxiv.org/abs/2601.18200)

**Authors**: Chenyu Zhang, Xinchen Lyu, Chenshan Ren, Shuhan Liu, Qimei Cui, Xiaofeng Tao  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.18200v1  

#### Abstract
Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šHeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰æ— çº¿ä¿¡é“åŸºç¡€æ¨¡å‹ï¼ˆWireless Foundation Modelï¼‰åœ¨å¤„ç† **Channel State Information (CSI)** æ•°æ®æ—¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š
- **Scale Heterogeneity**ï¼šä¸åŒåœºæ™¯ä¸‹ CSI å¼ é‡çš„ç»´åº¦å·®å¼‚å·¨å¤§ï¼ˆå¦‚æ—¶é—´çª—å£ $T$ã€å­è½½æ³¢æ•° $K$ã€å¤©çº¿é˜µåˆ—å¤§å° $A$ ä¸åŒï¼‰ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚
- **Scenario Heterogeneity**ï¼šä¸åŒä¼ æ’­ç¯å¢ƒï¼ˆå¦‚ RMaã€UMaã€UMiã€Indoorï¼‰ä¸‹çš„ç»Ÿè®¡åˆ†å¸ƒå·®å¼‚å¤§ï¼Œå½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ WiFoï¼‰é€šå¸¸é‡‡ç”¨ **scale- and scenario-isolated training**ï¼Œå³æ¯ä¸ª mini-batch åªåŒ…å«å•ä¸€å°ºåº¦æˆ–å•ä¸€åœºæ™¯çš„æ•°æ®ï¼Œé™åˆ¶äº†è·¨å°ºåº¦å’Œè·¨åœºæ™¯çš„çŸ¥è¯†è¿ç§»ï¼Œæ— æ³•å®ç°çœŸæ­£çš„é€šç”¨æ— çº¿åŸºç¡€æ¨¡å‹ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ ¸å¿ƒæ€æƒ³

ä½œè€…æå‡º **HeterCSI** â€”â€” ä¸€ç§é€šé“è‡ªé€‚åº”çš„å¼‚æ„ CSI é¢„è®­ç»ƒæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ´å¯Ÿæ˜¯ï¼š

> **Scale heterogeneity æ˜¯æ¢¯åº¦å†²çªï¼ˆgradient conflictï¼‰çš„ä¸»è¦æ¥æºï¼Œè€Œ scenario diversity å®é™…ä¸Šæœ‰åŠ©äºæ¢¯åº¦å¯¹é½ï¼ˆconstructive gradient alignmentï¼‰ã€‚**

åŸºäºæ­¤ï¼ŒHeterCSI çš„è®¾è®¡åŒ…å«ä¸¤ä¸ªå…³é”®æœºåˆ¶ï¼š

#### ï¼ˆ1ï¼‰**Scale-Aware Adaptive Batching Strategyï¼ˆå°ºåº¦æ„ŸçŸ¥è‡ªé€‚åº”æ‰¹å¤„ç†ç­–ç•¥ï¼‰**
- å°†æ‰€æœ‰ CSI æ ·æœ¬æŒ‰åºåˆ—é•¿åº¦æ’åºååˆ’åˆ†ä¸ºå¤šä¸ªâ€œbucketâ€ï¼ˆæ¡¶ï¼‰ï¼›
- æ¯ä¸ª bucket å†…éƒ¨æ ·æœ¬å…·æœ‰ç›¸ä¼¼çš„å°ºåº¦ï¼ˆminimize padding overheadï¼‰ï¼›
- åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»ä¸åŒ bucket ä¸­éšæœºé‡‡æ · mini-batchï¼Œä¿è¯å…¨å±€å¤šæ ·æ€§ï¼ˆpreserve scenario diversityï¼‰ï¼›
- å½¢æˆâ€œ**æ¡¶å†…åŒè´¨ã€æ¡¶é—´å¼‚è´¨**â€çš„ä¼˜åŒ–ç»“æ„ã€‚

#### ï¼ˆ2ï¼‰**Double-Masking Mechanismï¼ˆåŒé‡æ©ç æœºåˆ¶ï¼‰**
- **MAE-style éšæœºæ©ç **ï¼šç”¨äºè‡ªç›‘ç£å­¦ä¹ ï¼Œæå‡è¡¨ç¤ºèƒ½åŠ›ï¼›
- **Attention Masking**ï¼šå±è”½é›¶å¡«å……ï¼ˆzero-paddingï¼‰ä½ç½®çš„æ³¨æ„åŠ›æƒé‡ï¼Œé˜²æ­¢æ— æ•ˆ token å¹²æ‰°æœ‰æ•ˆä¿¡å·å»ºæ¨¡ã€‚

è¯¥æ¡†æ¶å®ç°äº†ï¼š
- è·¨å°ºåº¦ã€è·¨åœºæ™¯çš„ç»Ÿä¸€é¢„è®­ç»ƒï¼›
- é«˜æ•ˆä¸”ç¨³å®šçš„ä¼˜åŒ–è¿‡ç¨‹ï¼›
- å¼ºå¤§çš„ zero-shot æ³›åŒ–èƒ½åŠ›ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹é¢ | HeterCSI | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ WiFoï¼‰ |
|------|----------|---------------------|
| è¾“å…¥çµæ´»æ€§ | æ”¯æŒä»»æ„å°ºåº¦ CSI è¾“å…¥ | å¤šæ•°éœ€å›ºå®šå°ºå¯¸æˆ–éš”ç¦»è®­ç»ƒ |
| æ‰¹å¤„ç†æ–¹å¼ | è‡ªé€‚åº”æ··åˆå¤šå°ºåº¦ + å¤šåœºæ™¯ | å•ä¸€å°ºåº¦/åœºæ™¯éš”ç¦»è®­ç»ƒ |
| æ¢¯åº¦ç¨³å®šæ€§ | æ˜¾è‘—å‡å°‘ destructive gradient interference | å­˜åœ¨ä¸¥é‡æ¢¯åº¦å†²çªé£é™© |
| æ³›åŒ–èƒ½åŠ› | zero-shot ä¸‹ä¼˜äº full-shot æ¨¡å‹ | ä¾èµ–å¾®è°ƒæˆ–ç‰¹å®šåœºæ™¯è®­ç»ƒ |
| è®­ç»ƒæ•ˆç‡ | å‡å°‘çº¦ 53% è®­ç»ƒå»¶è¿Ÿ | padding å¼€é”€é«˜ï¼Œæ•ˆç‡ä½ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **ç”Ÿæˆå·¥å…·**ï¼šä½¿ç”¨ç¬¦åˆ 3GPP æ ‡å‡†çš„ä¿¡é“ä»¿çœŸå™¨ **QuaDRiGa** ç”Ÿæˆ CSI æ•°æ®ï¼›
- **ç³»ç»Ÿé…ç½®**ï¼šMISO-OFDM æ¶æ„ï¼ŒåŸºç«™é…å¤‡ UPA å¤©çº¿é˜µåˆ—ï¼Œç»ˆç«¯å•å¤©çº¿ï¼›
- **é¢‘ç‡èŒƒå›´**ï¼šæ¶µç›– Sub-6GHz åˆ° mmWaveï¼ˆæœ€é«˜è¾¾ 42 GHzï¼‰ï¼›
- **åœºæ™¯ç±»å‹**ï¼šåŒ…æ‹¬ Indoorã€RMaã€UMaã€UMi ç­‰å¤šç§å…¸å‹ä¼ æ’­ç¯å¢ƒã€‚

#### è®­ç»ƒä¸æµ‹è¯•åˆ’åˆ†ï¼š
- **è®­ç»ƒé›†**ï¼š40 ä¸ªå¼‚æ„æ•°æ®é›†ï¼Œå…± 480,000 ä¸ªæ ·æœ¬ï¼ˆæ¯ä¸ªæ•°æ®é›† 12,000ï¼‰ï¼›
- **æµ‹è¯•é›†**ï¼š12 ä¸ª**æœªè§è¿‡çš„ zero-shot åœºæ™¯**ï¼ˆD1â€“D12ï¼‰ï¼Œç”¨äºè¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼›
- æ‰€æœ‰æ•°æ®æ·»åŠ  20 dB çš„å¤é«˜æ–¯å™ªå£°ä»¥æ¨¡æ‹ŸçœŸå®æ¡ä»¶ã€‚

---

### ğŸ“Š å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### è¯„ä¼°ä»»åŠ¡ï¼š
1. **CSI Reconstruction**ï¼šä»éƒ¨åˆ†è§‚æµ‹ä¸­æ¢å¤å®Œæ•´ CSIï¼›
2. **Time-Domain Prediction**ï¼šåŸºäºå†å²æ—¶éš™é¢„æµ‹æœªæ¥ CSIï¼›
3. **Frequency-Domain Prediction**ï¼šåŸºäºä½é¢‘æ®µå¤–æ¨é«˜é¢‘æ®µ CSIã€‚

#### æ€§èƒ½æŒ‡æ ‡ï¼š
- **NMSE (Normalized Mean Squared Error)**ï¼Œå•ä½ä¸º dBï¼š
  $$
  \text{NMSE (dB)} = 10 \log_{10} \left( \frac{\| \hat{H} - H \|_F^2}{\| H \|_F^2} \right)
  $$
  æ•°å€¼è¶Šå°è¶Šå¥½ã€‚

#### åŸºçº¿æ–¹æ³•å¯¹æ¯”ï¼š
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **Full-shot æ¨¡å‹**ï¼ˆåœ¨ç›®æ ‡åŸŸè®­ç»ƒï¼‰ | LSTM, Transformer, LLM4CP, BERT4MIMO |
| **Zero-shot æ¨¡å‹**ï¼ˆæ— éœ€ç›®æ ‡åŸŸè®­ç»ƒï¼‰ | PAD (Prony-based), WiFo (SOTA) |
| **è®­ç»ƒèŒƒå¼å¯¹æ¯”** | Sequential, Alternating, Global Shuffling |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆå¹³å‡ NMSE å¯¹æ¯”ï¼‰

| æ–¹æ³• | CSI Recon. â†“ | Time Pred. â†“ | Freq. Pred. â†“ |
|------|---------------|----------------|------------------|
| **WiFo (SOTA zero-shot)** | -14.74 dB | -4.60 dB | -9.34 dB |
| **Proposed (HeterCSI)** | **-15.91 dB** | **-6.18 dB** | **-12.48 dB** |
| **æå‡å¹…åº¦** | **+1.17 dB** | **+1.58 dB** | **+3.14 dB** |

> ğŸ’¡ ç‰¹åˆ«åœ°ï¼Œç›¸æ¯” SOTA zero-shot æ¨¡å‹ **WiFo**ï¼ŒHeterCSI åœ¨ä¸‰é¡¹ä»»åŠ¡ä¸Šåˆ†åˆ«é™ä½ NMSE è¾¾ï¼š
> - **7.19 dB**ï¼ˆé‡å»ºï¼‰
> - **4.08 dB**ï¼ˆæ—¶åŸŸé¢„æµ‹ï¼‰
> - **5.27 dB**ï¼ˆé¢‘åŸŸé¢„æµ‹ï¼‰

---

### âš–ï¸ ä¸ Full-shot æ¨¡å‹çš„æ¯”è¾ƒ

å°½ç®¡ HeterCSI æ˜¯ä¸€ä¸ª **zero-shot æ¨¡å‹**ï¼ˆæœªåœ¨æµ‹è¯•åœºæ™¯è®­ç»ƒï¼‰ï¼Œä½†åœ¨å¹³å‡æ€§èƒ½ä¸Šä»ä¼˜äºå¤§å¤šæ•° full-shot æ¨¡å‹ï¼š

| æŒ‡æ ‡ | ç›¸æ¯”æœ€ä½³ baseline æå‡ |
|------|------------------------|
| CSI Reconstruction | +1.33 dB |
| Time-Domain Prediction | +1.58 dB |
| Frequency-Domain Prediction | +3.14 dB |

> âœ… åœ¨ 36 é¡¹æµ‹è¯•ä¸­ï¼ŒHeterCSI åœ¨ **80.6% çš„æƒ…å†µä¸‹å–å¾—æœ€ä¼˜æ€§èƒ½**ï¼›
> âœ… å…¶ä½™æƒ…å†µå‡è¿›å…¥ top-2ï¼Œæœ€å¤§å·®è·ä¸è¶…è¿‡ 1.8 dBï¼Œè¡¨ç°å‡ºæå¼ºçš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒä¸å…³é”®åˆ†æ

#### ï¼ˆ1ï¼‰**Bucket Granularity å½±å“**
- ä½¿ç”¨ä¸åŒæ•°é‡çš„ bucketï¼ˆB=4 vs B=8ï¼‰è¿›è¡Œå®éªŒï¼›
- ç»“æœæ˜¾ç¤ºï¼šæ›´é«˜çš„å¼‚æ„æ€§éœ€è¦æ›´ç»†ç²’åº¦çš„ bucket åˆ†å‰²æ¥å¹³è¡¡æ•ˆç‡ä¸æ³›åŒ–ï¼›
- å½“ B=8 æ—¶ï¼Œpadding ratio æ§åˆ¶åœ¨ 13.58%ï¼Œè®­ç»ƒæ—¶é—´ä»…ä¸º Global æ–¹æ³•çš„ **47%**ã€‚

#### ï¼ˆ2ï¼‰**è®­ç»ƒèŒƒå¼å¯¹æ¯”ï¼ˆScalabilityï¼‰**
éšç€è®­ç»ƒæ•°æ®é›†ä» 8 å¢åŠ åˆ° 40ï¼š
- **Global Shuffling** å‡ºç°æ˜æ˜¾æ€§èƒ½ä¸‹é™ï¼ˆå°¤å…¶åœ¨ 32 æ•°æ®é›†æ—¶ï¼‰ï¼Œå½’å› äºæ¢¯åº¦å†²çªï¼›
- **HeterCSI** è¡¨ç°å‡ºå•è°ƒä¸Šå‡çš„ scaling lawï¼ŒéªŒè¯äº†å…¶å¯æ‰©å±•æ€§ã€‚

#### ï¼ˆ3ï¼‰**æ•ˆç‡åˆ†æ**
| æ–¹æ³• | 40 datasets ä¸‹è®­ç»ƒæ—¶é—´ | Padding Ratio |
|------|-------------------------|----------------|
| Global | 30.55 å°æ—¶ | 58.91% |
| Alternating | 9.6 å°æ—¶ | 0% |
| **HeterCSI (B=8)** | **11.61 å°æ—¶** | **13.58%** |

> âœ… HeterCSI åœ¨ä¿æŒæ¥è¿‘ Alternating æ•ˆç‡çš„åŒæ—¶ï¼Œè·å¾—äº† Global çº§åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **Scale heterogeneity æ˜¯æ¢¯åº¦å†²çªçš„æ ¹æœ¬åŸå› **ï¼Œè€Œé scenario diversityï¼›
2. **Scenario diversity å®é™…ä¿ƒè¿›æ¢¯åº¦å¯¹é½**ï¼Œåº”è¢«ä¿ç•™è€Œéè§„é¿ï¼›
3. é€šè¿‡ **scale-aware batching + double masking** å¯æœ‰æ•ˆè§£è€¦æ•ˆç‡ä¸æ³›åŒ–ä¹‹é—´çš„çŸ›ç›¾ï¼›
4. HeterCSI æˆåŠŸæ„å»ºäº†ä¸€ä¸ªæ— éœ€å¾®è°ƒå³å¯é€‚ç”¨äºå¤šç§æœªçŸ¥åœºæ™¯çš„ **generalized wireless foundation model**ï¼›
5. æ¨¡å‹åœ¨ zero-shot è®¾ç½®ä¸‹è¶…è¶Š full-shot æ¨¡å‹ï¼Œæ ‡å¿—ç€å‘é€šç”¨æ— çº¿æ™ºèƒ½è¿ˆå‡ºå…³é”®ä¸€æ­¥ã€‚

---

### âš ï¸ å±€é™æ€§

1. **ä¾èµ–é«˜è´¨é‡åˆæˆæ•°æ®**ï¼šç›®å‰å®éªŒåŸºäº QuaDRiGa ä»¿çœŸæ•°æ®ï¼Œå°šæœªåœ¨çœŸå®è®¾å¤‡é‡‡é›†æ•°æ®ä¸ŠéªŒè¯ï¼›
2. **bucket æ•°é‡ä¸ºè¶…å‚æ•°**ï¼šéœ€æ‰‹åŠ¨è®¾å®šï¼Œç¼ºä¹è‡ªåŠ¨è°ƒèŠ‚æœºåˆ¶ï¼›
3. **æœªè€ƒè™‘åŠ¨æ€æ‹“æ‰‘å˜åŒ–**ï¼šå¦‚ç§»åŠ¨ç”¨æˆ·è½¨è¿¹ã€çªå‘å¹²æ‰°ç­‰å¤æ‚åŠ¨æ€è¡Œä¸ºï¼›
4. **è®¡ç®—èµ„æºè¦æ±‚è¾ƒé«˜**ï¼šè™½ç„¶æ•ˆç‡æå‡æ˜¾è‘—ï¼Œä½†ä»éœ€å¤š GPU æ”¯æŒå¤§è§„æ¨¡è®­ç»ƒã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **å¼•å…¥ç‰©ç†å…ˆéªŒçŸ¥è¯†**ï¼šæ¢ç´¢ physics-aware positional encodingï¼Œèåˆç”µç£ä¼ æ’­è§„å¾‹ï¼›
2. **è½»é‡åŒ–æ¶æ„è®¾è®¡**ï¼šå¼€å‘é€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²çš„å°å‹åŒ–ç‰ˆæœ¬ï¼›
3. **çœŸå®æ•°æ®é—­ç¯éªŒè¯**ï¼šç»“åˆå®æµ‹æ•°æ®è¿›è¡Œç«¯åˆ°ç«¯ç³»ç»Ÿçº§æµ‹è¯•ï¼›
4. **å¤šæ¨¡æ€æ‰©å±•**ï¼šé›†æˆ GPSã€IMUã€åœ°å›¾ç­‰è¾…åŠ©ä¿¡æ¯ï¼Œæ„å»ºæ›´å¼ºå¤§çš„æ— çº¿æ„ŸçŸ¥åŸºç¡€æ¨¡å‹ï¼›
5. **åœ¨çº¿è‡ªé€‚åº”æœºåˆ¶**ï¼šç ”ç©¶å¦‚ä½•åœ¨è¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´ bucket ç­–ç•¥ä»¥åº”å¯¹çªå‘æµé‡æˆ–æ–°åœºæ™¯ã€‚

---

> ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> HeterCSI é¦–æ¬¡æ­ç¤ºäº† CSI å¼‚æ„æ€§ä¸­â€œå°ºåº¦â€ä¸â€œåœºæ™¯â€çš„ä¸åŒä½œç”¨ï¼Œå¹¶é€šè¿‡åˆ›æ–°çš„ scale-aware batching å’Œ double masking æœºåˆ¶ï¼Œåœ¨ä¸ç‰ºç‰²æ•ˆç‡çš„å‰æä¸‹å®ç°äº†å‰æ‰€æœªæœ‰çš„ zero-shot æ³›åŒ–èƒ½åŠ›ï¼Œä¸º 6G æ—¶ä»£çš„é€šç”¨æ— çº¿åŸºç¡€æ¨¡å‹æä¾›äº†å¯è¡Œè·¯å¾„ã€‚

</details>

---

### 16. [Superlinear Multi-Step Attention](https://arxiv.org/abs/2601.18401)

**Authors**: Yufeng Huang  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.18401v1  

#### Abstract
In this paper, we propose \textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \textbf{random context access} (a.k.a.\ structural non-exclusion): no eligible token position is structurally exclud...

---

### 17. [UrduLM: A Resource-Efficient Monolingual Urdu Language Model](https://arxiv.org/abs/2601.17664)

**Authors**: Syed Muhammad Ali, Hammad Sajid, Zainab Haider, Ali Muhammad Asad, Haya Fatima, Abdul Samad  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.17664v1  

#### Abstract
Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To ad...

---

### 18. [ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning](https://arxiv.org/abs/2601.17135)

**Authors**: Jakob Karalus, Friedhelm Schwenker  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.17135v1  

#### Abstract
Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking w...

---

### 19. [PUNCH: Physics-informed Uncertainty-aware Network for Coronary Hemodynamics](https://arxiv.org/abs/2601.17192)

**Authors**: Sukirt Thakur, Marcus Roper, Yang Zhou, Reza Akbarian Bafghi, Brahmajee K. Nallamothu, C. Alberto Figueroa, Srinivas Paruchuri, Scott Burger, Maziar Raissi  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.17192v1  

#### Abstract
Coronary microvascular dysfunction (CMD) affects millions worldwide yet remains underdiagnosed because gold-standard physiological measurements are invasive and variably reproducible. We introduce a non-invasive, uncertainty-aware framework for estimating coronary flow reserve (CFR) directly from st...

---

### 20. [Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations](https://arxiv.org/abs/2601.17407)

**Authors**: Prajwal Chauhan, Salah Eddine Choutri, Saif Eddin Jabari  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.17407v1  

#### Abstract
Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and slu...

---

### 21. [Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning](https://arxiv.org/abs/2601.17616)

**Authors**: Fatema Siddika, Md Anwar Hossen, Tanwi Mallick, Ali Jannesari  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.17616v1  

#### Abstract
Continual learning in Large Language Models (LLMs) is hindered by the plasticity-stability dilemma, where acquiring new capabilities often leads to catastrophic forgetting of previous knowledge. Existing methods typically treat parameters uniformly, failing to distinguish between specific task knowl...

---

### 22. [Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates](https://arxiv.org/abs/2601.18510)

**Authors**: Yibo Li, Zijie Lin, Ailin Deng, Xuan Zhang, Yufei He, Shuo Ji, Tri Cao, Bryan Hooi  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.18510v1  

#### Abstract
While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting...

---

### 23. [Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic](https://arxiv.org/abs/2601.18783)

**Authors**: Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani  
**Category**: cs.LG  
**Published**: 2026-01-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.18783v1  

#### Abstract
Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their t...

---

### 24. [When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability in Multi-Agent LLM Systems](https://arxiv.org/abs/2601.16280)

**Authors**: Donghao Huang, Gauri Malwe, Zhaoxia Wang  
**Category**: cs.AI  
**Published**: 2026-01-27  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.16280v1  

#### Abstract
Multi-agent systems powered by large language models (LLMs) are transforming enterprise automation, yet systematic evaluation methodologies for assessing tool-use reliability remain underdeveloped. We introduce a comprehensive diagnostic framework that leverages big data analytics to evaluate proced...

---

### 25. [LongCat-Flash-Thinking-2601 Technical Report](https://arxiv.org/abs/2601.16725)

**Authors**: Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chen Gao, Chen Zhang, Chengcheng Han, Chenhui Yang, Chuyu Zhang, Cong Chen, Cunguang Wang, Daoru Pan, Defei Bu, Dengchang Zhao, Di Xiu, Dishan Liu, Dongyu Ru, Dunwei Tu, Fan Wu, Fengcheng Yuan, Fengcun Li, Gang Xu, Guanyu Wu, Guoyuan Lin, Haibin Wang, Hansi Yang, Hao Yang, Haonan Yan, Haoxiang Ma, Haoxing Wen, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiacheng Zhang, Jiahong Zhou, Jiahuan Li, Jiaming Wang, Jian Yang, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiapeng Zhu, Jiaqi Sun, Jiarong Shi, Jiarui Zhao, Jingang Wang, Jinluan Yang, Jinrui Ding, Jinwei Xiao, Jiyuan He, Juncan Xu, Kefeng Zhang, Keheng Wang, Li Wei, Lianhui Ma, Lin Qiu, Lingbing Kong, Lingchuan Liu, Linsen Guo, Mengshen Zhu, Mengxia Shen, Mingyang Zhu, Peiguang Li, Peng Pei, Pengcheng Jia, Pengtao Zhang, Peng Zhao, Qi Gu, Qiong Huang, Qiyuan Duan, Quanchi Weng, Rongxiang Weng, Rongzhi Zhang, Rumei Li, Shanglin Lei, Shengnan An, Shijun Dai, Shuaikang Liu, Shuang Zhou, Shuo Wang, Songyuan Zhao, Tao Liang, Tianhao Hu, Tianze Chen, Wei Liu, Wei Shi, Wei Wang, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Wentao Chen, Wentao Shi, Xi Su, Xiangcheng Liu, Xiandi Ma, Xiangyu Xi, Xiangyuan Liu, Xiangzhou Huang, Xiao Liu, Xiaodong Cai, Xiaolong Chen, Xiaowei Shi, Xiaoyu Li, Xin Chen, Xingchen Liu, Xuan Huang, Xuezhi Cao, Xunliang Cai, Yan Chen, Yang Bai, Yang Liu, Yang Yang, Yang Zheng, Yaoming Wang, Yaoming Zhu, Yaqi Huo, Yanyu Chen, Yaorui Shi, Yerui Sun, Yi Zhang, Yihao Chen, Yi-Kai Zhang, Yifan Lu, Yifan Zhao, Yitao Zhai, Yongjing Yin, Yongwei Zhou, Youshao Xiao, Yuchuan Dai, Yuchen Xie, Yuchen Yu, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunke Zhao, Yuwei Jiang, Yuxin Bian, Yuxin Chen, Yuxin Liu, Yue Xu, Yueqing Sun, Zeyang Yu, Zhao Yang, Zhengsheng Huang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhimin Lin, Zhiyuan Yao, Zhuofan Chen, Zhuowen Han, Zijian Zhang, Ziran Li, Ziwen Wang, Ziyuan Zhuang  
**Category**: cs.AI  
**Published**: 2026-01-27  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.16725v1  

#### Abstract
We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, includi...

---

### 26. [Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis](https://arxiv.org/abs/2601.17387)

**Authors**: Toshiki Nakai, Varsha Suresh, Vera Demberg  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.17387v1  

#### Abstract
Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three compl...

---

### 27. [A System for Name and Address Parsing with Large Language Models](https://arxiv.org/abs/2601.18014)

**Authors**: Adeeba Tarannum, Muzakkiruddin Ahmed Mohammed, Mert Can Cakmak, Shames Al Mandalawi, John Talburt  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.18014v1  

#### Abstract
Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large la...

---

### 28. [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)

**Authors**: Peng Sun, Xiangyu Zhang, Duan Wu  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.18253v1  

#### Abstract
Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrap...

---

### 29. [When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs](https://arxiv.org/abs/2601.18350)

**Authors**: Junyi Zou  
**Category**: cs.CL  
**Published**: 2026-01-27  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.18350v1  

#### Abstract
Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline:...

---

### 30. [Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs](https://arxiv.org/abs/2601.17136)

**Authors**: Julian Bellavita, Matthew Rubino, Nakul Iyer, Andrew Chang, Aditya Devarakonda, Flavio Vella, Giulia Guidi  
**Category**: cs.DC  
**Published**: 2026-01-27  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.17136v1  

#### Abstract
Clustering is an important tool in data analysis, with K-means being popular for its simplicity and versatility. However, it cannot handle non-linearly separable clusters. Kernel K-means addresses this limitation but requires a large kernel matrix, making it computationally and memory intensive. Pri...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
