# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2026-01-22 05:55:19 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design](https://arxiv.org/abs/2601.14549)

**Authors**: Nilesh Prasad Pandey, Jangseon Park, Onat Gungor, Flavio Ponzina, Tajana Rosing  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2601.14549v1  

#### Abstract
Deploying Small Language Models (SLMs) on edge platforms is critical for real-time, privacy-sensitive generative AI, yet constrained by memory, latency, and energy budgets. Quantization reduces model size and cost but suffers from device noise in emerging non-volatile memories, while conventional me...

---

### 2. [Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed Learning](https://arxiv.org/abs/2601.14942)

**Authors**: Hang Zhao, Hongru Li, Dongfang Xu, Shenghui Song, Khaled B. Letaief  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.14942v1  

#### Abstract
Semantic communication is emerging as a key enabler for distributed edge intelligence due to its capability to convey task-relevant meaning. However, achieving communication-efficient training and robust inference over wireless links remains challenging. This challenge is further exacerbated for mul...

---

### 3. [Mixture-of-Experts Models in Vision: Routing, Optimization, and Generalization](https://arxiv.org/abs/2601.15021)

**Authors**: Adam Rokah, Daniel Veress, Caleb Caulk, Sourav Sharan  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.15021v1  

#### Abstract
Mixture-of-Experts (MoE) architectures enable conditional computation by routing inputs to multiple expert subnetworks and are often motivated as a mechanism for scaling large language models. In this project, we instead study MoE behavior in an image classification setting, focusing on predictive p...

---

### 4. [Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree](https://arxiv.org/abs/2601.14523)

**Authors**: Leyi Zhao, Weijie Huang, Yitong Guo, Jiang Bian, Chenghong Wang, Xuhong Zhang  
**Category**: cs.AI  
**Published**: 2026-01-22  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.14523v1  

#### Abstract
Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for auto...

---

### 5. [DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs](https://arxiv.org/abs/2601.14711)

**Authors**: Mingxuan Song, Yusen Huo, Bohan Zhou, Shenglin Yin, Zhen Xiao, Jieyi Long, Zhilin Zhang, Chuan Yu  
**Category**: cs.AI  
**Published**: 2026-01-22  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.14711v1  

#### Abstract
Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-sh...

---

### 6. [LoRAP: Low-Rank Aggregation Prompting for Quantized Graph Neural Networks Training](https://arxiv.org/abs/2601.15079)

**Authors**: Chenyu Liu, Haige Li, Luca Rossi  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.15079v1  

#### Abstract
Graph Neural Networks (GNNs) are neural networks that aim to process graph data, capturing the relationships and interactions between nodes using the message-passing mechanism. GNN quantization has emerged as a promising approach for reducing model size and accelerating inference in resource-constra...

---

### 7. [Parallel Collaborative ADMM Privacy Computing and Adaptive GPU Acceleration for Distributed Edge Networks](https://arxiv.org/abs/2601.14980)

**Authors**: Mengchun Xia, Zhicheng Dong, Donghong Cai, Fang Fang, Lisheng Fan, Pingzhi Fan  
**Category**: cs.DC  
**Published**: 2026-01-22  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.14980v1  

#### Abstract
Distributed computing has been widely applied in distributed edge networks for reducing the processing burden of high-dimensional data centralization, where a high-dimensional computational task is decomposed into multiple low-dimensional collaborative processing tasks or multiple edge nodes use dis...

---

### 8. [Adaptive Exponential Integration for Stable Gaussian Mixture Black-Box Variational Inference](https://arxiv.org/abs/2601.14855)

**Authors**: Baojun Che, Yifan Chen, Daniel Zhengyu Huang, Xinying Mao, Weijie Wang  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.14855v1  

#### Abstract
Black-box variational inference (BBVI) with Gaussian mixture families offers a flexible approach for approximating complex posterior distributions without requiring gradients of the target density. However, standard numerical optimization methods often suffer from instability and inefficiency. We de...

---

### 9. [Semantic-Guided Unsupervised Video Summarization](https://arxiv.org/abs/2601.14773)

**Authors**: Haizhou Liu, Haodong Jin, Yiming Wang, Hui Yu  
**Category**: cs.AI  
**Published**: 2026-01-22  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.14773v1  

#### Abstract
Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe ...

---

### 10. [Typhoon OCR: Open Vision-Language Model For Thai Document Extraction](https://arxiv.org/abs/2601.14722)

**Authors**: Surapon Nonesung, Natapong Nitarach, Teetouch Jaknamon, Pittawat Taveekitworachai, Kunat Pipatanakul  
**Category**: cs.CL  
**Published**: 2026-01-22  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.14722v1  

#### Abstract
Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of hi...

---

### 11. [RefProtoFL: Communication-Efficient Federated Learning via External-Referenced Prototype Alignment](https://arxiv.org/abs/2601.14746)

**Authors**: Hongyue Wu, Hangyu Li, Guodong Fan, Haoran Zhu, Shizhan Chen, Zhiyong Feng  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.14746v1  

#### Abstract
Federated learning (FL) enables collaborative model training without sharing raw data in edge environments, but is constrained by limited communication bandwidth and heterogeneous client data distributions. Prototype-based FL mitigates this issue by exchanging class-wise feature prototypes instead o...

---

### 12. [Robustness of Mixtures of Experts to Feature Noise](https://arxiv.org/abs/2601.14792)

**Authors**: Dong Sun, Rahul Nittala, Rebekka Burkholz  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.14792v1  

#### Abstract
Despite their practical success, it remains unclear why Mixture of Experts (MoE) models can outperform dense networks beyond sheer parameter scaling. We study an iso-parameter regime where inputs exhibit latent modular structure but are corrupted by feature noise, a proxy for noisy internal activati...

---

### 13. [CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning](https://arxiv.org/abs/2601.15141)

**Authors**: Tianshi Xu, Yuteng Chen, Meng Li  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.15141v1  

#### Abstract
Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajec...

---

### 14. [Which Quantization Should I Use? A Unified Evaluation of llama.cpp Quantization on Llama-3.1-8B-Instruct](https://arxiv.org/abs/2601.14277)

**Authors**: Uygar Kurt  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.14277v1  

#### Abstract
Quantization is a practical technique for making large language models easier to deploy by reducing the precision used to store and operate on model weights. This can lower memory use and improve runtime feasibility on constrained hardware, which is especially relevant for users running models local...

---

### 15. [Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation](https://arxiv.org/abs/2601.14590)

**Authors**: Shovito Barua Soumma, Asiful Arefeen, Stephanie M. Carpenter, Melanie Hingle, Hassan Ghasemzadeh  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.14590v1  

#### Abstract
Counterfactual explanations (CFEs) provide human-centric interpretability by identifying the minimal, actionable changes required to alter a machine learning model's prediction. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust mod...

---

### 16. [Field-Space Autoencoder for Scalable Climate Emulators](https://arxiv.org/abs/2601.15102)

**Authors**: Johannes Meuer, Maximilian Witte, \'Eti\'enne Pl\'esiat, Thomas Ludwig, Christopher Kadow  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.15102v1  

#### Abstract
Kilometer-scale Earth system models are essential for capturing local climate change. However, these models are computationally expensive and produce petabyte-scale outputs, which limits their utility for applications such as probabilistic risk assessment. Here, we present the Field-Space Autoencode...

---

### 17. [IB-GRPO: Aligning LLM-based Learning Path Recommendation with Educational Objectives via Indicator-Based Group Relative Policy Optimization](https://arxiv.org/abs/2601.14686)

**Authors**: Shuai Wang, Yaoming Yang, Bingdong Li, Hao Hao, Aimin Zhou  
**Category**: cs.AI  
**Published**: 2026-01-22  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.14686v1  

#### Abstract
Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendati...

---

### 18. [Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation](https://arxiv.org/abs/2601.14955)

**Authors**: Hanqi Jin, Gaoming Yang, Zhangming Chan, Yapeng Yuan, Longbin Li, Fei Sun, Yeqiu Yang, Jian Wu, Yuning Jiang, Bo Zheng  
**Category**: cs.AI  
**Published**: 2026-01-22  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.14955v1  

#### Abstract
User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for understanding evolving preferen...

---

### 19. [Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning](https://arxiv.org/abs/2601.15160)

**Authors**: Yuval Kansal, Niraj K. Jha  
**Category**: cs.AI  
**Published**: 2026-01-22  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.15160v1  

#### Abstract
Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are ...

---

### 20. [Efficient Imputation for Patch-based Missing Single-cell Data via Cluster-regularized Optimal Transport](https://arxiv.org/abs/2601.14653)

**Authors**: Yuyu Liu, Jiannan Yang, Ziyang Yu, Weishen Pan, Fei Wang, Tengfei Ma  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.14653v1  

#### Abstract
Missing data in single-cell sequencing datasets poses significant challenges for extracting meaningful biological insights. However, existing imputation approaches, which often assume uniformity and data completeness, struggle to address cases with large patches of missing data. In this paper, we pr...

---

### 21. [Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems](https://arxiv.org/abs/2601.14662)

**Authors**: Shuhua Yang, Jiahao Zhang, Yilong Wang, Dongwon Lee, Suhang Wang  
**Category**: cs.AI  
**Published**: 2026-01-22  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.14662v1  

#### Abstract
Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph struct...

---

### 22. [Rewarding How Models Think Pedagogically: Integrating Pedagogical Reasoning and Thinking Rewards for LLMs in Education](https://arxiv.org/abs/2601.14560)

**Authors**: Unggi Lee, Jiyeong Bae, Jaehyeon Park, Haeun Park, Taejun Park, Younghoon Jeon, Sungmin Cho, Junbo Koh, Yeil Jeong, Gyeonggeon Lee  
**Category**: cs.CL  
**Published**: 2026-01-22  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.14560v1  

#### Abstract
Large language models (LLMs) are increasingly deployed as intelligent tutoring systems, yet research on optimizing LLMs specifically for educational contexts remains limited. Recent works have proposed reinforcement learning approaches for training LLM tutors, but these methods focus solely on optim...

---

### 23. [AdaTIR: Adaptive Tool-Integrated Reasoning via Difficulty-Aware Policy Optimization](https://arxiv.org/abs/2601.14696)

**Authors**: Zhaiyu Fang, Ruipeng Sun  
**Category**: cs.CL  
**Published**: 2026-01-22  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.14696v1  

#### Abstract
Tool-Integrated Reasoning (TIR) has significantly enhanced the capabilities of Large Language Models (LLMs), yet current agents tend to exhibit cognitive offloading, redundantly invoking external tools even for simple tasks. In this paper, we suggest that true agentic intelligence requires not just ...

---

### 24. [The Effect of Scripts and Formats on LLM Numeracy](https://arxiv.org/abs/2601.15251)

**Authors**: Varshini Reddy, Craig W. Schmidt, Seth Ebner, Adam Wiemerslage, Yuval Pinter, Chris Tanner  
**Category**: cs.CL  
**Published**: 2026-01-22  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.15251v1  

#### Abstract
Large language models (LLMs) have achieved impressive proficiency in basic arithmetic, rivaling human-level performance on standard numerical tasks. However, little attention has been given to how these models perform when numerical expressions deviate from the prevailing conventions present in thei...

---

### 25. [Fine-Grained Traceability for Transparent ML Pipelines](https://arxiv.org/abs/2601.14971)

**Authors**: Liping Chen, Mujie Liu, Haytham Fayek  
**Category**: cs.LG  
**Published**: 2026-01-22  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.14971v1  

#### Abstract
Modern machine learning systems are increasingly realised as multistage pipelines, yet existing transparency mechanisms typically operate at a model level: they describe what a system is and why it behaves as it does, but not how individual data samples are operationally recorded, tracked, and verif...

---

### 26. [On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL](https://arxiv.org/abs/2601.14456)

**Authors**: Valerio Belcamino, Nicholas Attolino, Alessio Capitanelli, Fulvio Mastrogiovanni  
**Category**: cs.AI  
**Published**: 2026-01-22  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.14456v1  

#### Abstract
Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domai...

---

### 27. [MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks](https://arxiv.org/abs/2601.14652)

**Authors**: Zixuan Ke, Yifei Ming, Austin Xu, Ryan Chin, Xuan-Phi Nguyen, Prathyusha Jwalapuram, Semih Yavuz, Caiming Xiong, Shafiq Joty  
**Category**: cs.AI  
**Published**: 2026-01-22  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.14652v1  

#### Abstract
While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution...

---

### 28. [From Chaos to Clarity: Schema-Constrained AI for Auditable Biomedical Evidence Extraction from Full-Text PDFs](https://arxiv.org/abs/2601.14267)

**Authors**: Pouria Mortezaagha, Joseph Shaw, Bowen Sun, Arya Rahgozar  
**Category**: cs.CL  
**Published**: 2026-01-22  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.14267v1  

#### Abstract
Biomedical evidence synthesis relies on accurate extraction of methodological, laboratory, and outcome variables from full-text research articles, yet these variables are embedded in complex scientific PDFs that make manual abstraction time-consuming and difficult to scale. Existing document AI syst...

---

### 29. [Large Language Models for Large-Scale, Rigorous Qualitative Analysis in Applied Health Services Research](https://arxiv.org/abs/2601.14478)

**Authors**: Sasha Ronaghi, Emma-Louise Aveling, Maria Levis, Rachel Lauren Ross, Emily Alsentzer, Sara Singer  
**Category**: cs.CL  
**Published**: 2026-01-22  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.14478v1  

#### Abstract
Large language models (LLMs) show promise for improving the efficiency of qualitative analysis in large, multi-site health-services research. Yet methodological guidance for LLM integration into qualitative analysis and evidence of their impact on real-world research methods and outcomes remain limi...

---

### 30. [Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning](https://arxiv.org/abs/2601.14750)

**Authors**: Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang, Zheng Wei  
**Category**: cs.CL  
**Published**: 2026-01-22  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.14750v1  

#### Abstract
Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
