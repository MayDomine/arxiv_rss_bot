# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-08-28 12:51:37 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)

**Authors**: Ethan Li (Taoyi), Anders Boesen Lindbo Larsen (Taoyi), Chen Zhang (Taoyi), Xiyou Zhou (Taoyi), Jun Qin (Taoyi), Dian Ang Yap (Taoyi), Narendran Raghavan (Taoyi), Xuankai Chang (Taoyi), Margit Bowler (Taoyi), Eray Yildiz (Taoyi), John Peebles (Taoyi), Hannah Gillis Coleman (Taoyi), Matteo Ronchi (Taoyi), Peter Gray (Taoyi), Keen You (Taoyi), Anthony Spalvieri-Kruse (Taoyi), Ruoming Pang (Taoyi), Reed Li (Taoyi), Yuli Yang (Taoyi), Emad Soroush (Taoyi), Zhiyun Lu (Taoyi), Crystal Xiao (Taoyi), Rong Situ (Taoyi), Jordan Huffaker (Taoyi), David Griffiths (Taoyi), Zaid Ahmed (Taoyi), Peng Zhang (Taoyi), Daniel Parilla (Taoyi), Asaf Liberman (Taoyi), Jennifer Mallalieu (Taoyi), Parsa Mazaheri (Taoyi), Qibin Chen (Taoyi), Manjot Bilkhu (Taoyi), Aonan Zhang (Taoyi), Eric Wang (Taoyi), Dave Nelson (Taoyi), Michael FitzMaurice (Taoyi), Thomas Voice (Taoyi), Jeremy Liu (Taoyi), Josh Shaffer (Taoyi), Shiwen Zhao (Taoyi), Prasanth Yadla (Taoyi), Farzin Rasteh (Taoyi), Pengsheng Guo (Taoyi), Arsalan Farooq (Taoyi), Jeremy Snow (Taoyi), Stephen Murphy (Taoyi), Tao Lei (Taoyi), Minsik Cho (Taoyi), George Horrell (Taoyi), Sam Dodge (Taoyi), Lindsay Hislop (Taoyi), Sumeet Singh (Taoyi), Alex Dombrowski (Taoyi), Aiswarya Raghavan (Taoyi), Sasha Sirovica (Taoyi), Mandana Saebi (Taoyi), Faye Lao (Taoyi), Max Lam (Taoyi), TJ Lu (Taoyi), Zhaoyang Xu (Taoyi), Karanjeet Singh (Taoyi), Marc Kirchner (Taoyi), David Mizrahi (Taoyi), Rajat Arora (Taoyi), Haotian Zhang (Taoyi), Henry Mason (Taoyi), Lawrence Zhou (Taoyi), Yi Hua (Taoyi), Ankur Jain (Taoyi), Felix Bai (Taoyi), Joseph Astrauskas (Taoyi), Floris Weers (Taoyi), Josh Gardner (Taoyi), Mira Chiang (Taoyi), Yi Zhang (Taoyi), Pulkit Agrawal (Taoyi), Tony Sun (Taoyi), Quentin Keunebroek (Taoyi), Matthew Hopkins (Taoyi), Bugu Wu (Taoyi), Tao Jia (Taoyi), Chen Chen (Taoyi), Xingyu Zhou (Taoyi), Nanzhu Wang (Taoyi), Peng Liu (Taoyi), Ruixuan Hou (Taoyi), Rene Rauch (Taoyi), Yuan Gao (Taoyi), Afshin Dehghan (Taoyi), Jonathan Janke (Taoyi), Zirui Wang (Taoyi), Cha Chen (Taoyi), Xiaoyi Ren (Taoyi), Feng Nan (Taoyi), Josh Elman (Taoyi), Dong Yin (Taoyi), Yusuf Goren (Taoyi), Jeff Lai (Taoyi), Yiran Fei (Taoyi), Syd Evans (Taoyi), Muyang Yu (Taoyi), Guoli Yin (Taoyi), Yi Qin (Taoyi), Erin Feldman (Taoyi), Isha Garg (Taoyi), Aparna Rajamani (Taoyi), Karla Vega (Taoyi), Walker Cheng (Taoyi), TJ Collins (Taoyi), Hans Han (Taoyi), Raul Rea Menacho (Taoyi), Simon Yeung (Taoyi), Sophy Lee (Taoyi), Phani Mutyala (Taoyi), Ying-Chang Cheng (Taoyi), Zhe Gan (Taoyi), Sprite Chu (Taoyi), Justin Lazarow (Taoyi), Alessandro Pappalardo (Taoyi), Federico Scozzafava (Taoyi), Jing Lu (Taoyi), Erik Daxberger (Taoyi), Laurent Duchesne (Taoyi), Jen Liu (Taoyi), David G\"uera (Taoyi), Stefano Ligas (Taoyi), Mary Beth Kery (Taoyi), Brent Ramerth (Taoyi), Ciro Sannino (Taoyi), Marcin Eichner (Taoyi), Haoshuo Huang (Taoyi), Rui Qian (Taoyi), Moritz Schwarzer-Becker (Taoyi), David Riazati (Taoyi), Mingfei Gao (Taoyi), Bailin Wang (Taoyi), Jack Cackler (Taoyi), Yang Lu (Taoyi), Ransen Niu (Taoyi), John Dennison (Taoyi), Guillaume Klein (Taoyi), Jeffrey Bigham (Taoyi), Deepak Gopinath (Taoyi), Navid Shiee (Taoyi), Darren Botten (Taoyi), Guillaume Tartavel (Taoyi), Alex Guillen Garcia (Taoyi), Sam Xu (Taoyi), Victoria M\"onchJuan Haladjian (Taoyi), Zi-Yi Dou (Taoyi), Matthias Paulik (Taoyi), Adolfo Lopez Mendez (Taoyi), Zhen Li (Taoyi), Hong-You Chen (Taoyi), Chao Jia (Taoyi), Dhaval Doshi (Taoyi), Zhengdong Zhang (Taoyi), Raunak Manjani (Taoyi), Aaron Franklin (Taoyi), Zhile Ren (Taoyi), David Chen (Taoyi), Artsiom Peshko (Taoyi), Nandhitha Raghuram (Taoyi), Hans Hao (Taoyi), Jiulong Shan (Taoyi), Kavya Nerella (Taoyi), Ramsey Tantawi (Taoyi), Vivek Kumar (Taoyi), Saiwen Wang (Taoyi), Brycen Wershing (Taoyi), Bhuwan Dhingra (Taoyi), Dhruti Shah (Taoyi), Ob Adaranijo (Taoyi), Xin Zheng (Taoyi), Tait Madsen (Taoyi), Hadas Kotek (Taoyi), Chang Liu (Taoyi), Yin Xia (Taoyi), Hanli Li (Taoyi), Suma Jayaram (Taoyi), Yanchao Sun (Taoyi), Ahmed Fakhry (Taoyi), Vasileios Saveris (Taoyi), Dustin Withers (Taoyi), Yanghao Li (Taoyi), Alp Aygar (Taoyi), Andres Romero Mier Y Teran (Taoyi), Kaiwei Huang (Taoyi), Mark Lee (Taoyi), Xiujun Li (Taoyi), Yuhong Li (Taoyi), Tyler Johnson (Taoyi), Jay Tang (Taoyi), Joseph Yitan Cheng (Taoyi), Futang Peng (Taoyi), Andrew Walkingshaw (Taoyi), Lucas Guibert (Taoyi), Abhishek Sharma (Taoyi), Cheng Shen (Taoyi), Piotr Maj (Taoyi), Yasutaka Tanaka (Taoyi), You-Cyuan Jhang (Taoyi), Vivian Ma (Taoyi), Tommi Vehvilainen (Taoyi), Kelvin Zou (Taoyi), Jeff Nichols (Taoyi), Matthew Lei (Taoyi), David Qiu (Taoyi), Yihao Qian (Taoyi), Gokul Santhanam (Taoyi), Wentao Wu (Taoyi), Yena Han (Taoyi), Dominik Moritz (Taoyi), Haijing Fu (Taoyi), Mingze Xu (Taoyi), Vivek Rathod (Taoyi), Jian Liu (Taoyi), Louis D'hauwe (Taoyi), Qin Ba (Taoyi), Haitian Sun (Taoyi), Haoran Yan (Taoyi), Philipp Dufter (Taoyi), Anh Nguyen (Taoyi), Yihao Feng (Taoyi), Emma Wang (Taoyi), Keyu He (Taoyi), Rahul Nair (Taoyi), Sanskruti Shah (Taoyi), Jiarui Lu (Taoyi), Patrick Sonnenberg (Taoyi), Jeremy Warner (Taoyi), Yuanzhi Li (Taoyi), Bowen Pan (Taoyi), Ziyi Zhong (Taoyi), Joe Zhou (Taoyi), Sam Davarnia (Taoyi), Olli Saarikivi (Taoyi), Irina Belousova (Taoyi), Rachel Burger (Taoyi), Shang-Chen Wu (Taoyi), Di Feng (Taoyi), Bas Straathof (Taoyi), James Chou (Taoyi), Yuanyang Zhang (Taoyi), Marco Zuliani (Taoyi), Eduardo Jimenez (Taoyi), Abhishek Sundararajan (Taoyi), Xianzhi Du (Taoyi), Chang Lan (Taoyi), Nilesh Shahdadpuri (Taoyi), Peter Grasch (Taoyi), Sergiu Sima (Taoyi), Josh Newnham (Taoyi), Varsha Paidi (Taoyi), Jianyu Wang (Taoyi), Kaelen Haag (Taoyi), Alex Braunstein (Taoyi), Daniele Molinari (Taoyi), Richard Wei (Taoyi), Brenda Yang (Taoyi), Nicholas Lusskin (Taoyi), Joanna Arreaza-Taylor (Taoyi), Meng Cao (Taoyi), Nicholas Seidl (Taoyi), Simon Wang (Taoyi), Jiaming Hu (Taoyi), Yiping Ma (Taoyi), Mengyu Li (Taoyi), Kieran Liu (Taoyi), Hang Su (Taoyi), Sachin Ravi (Taoyi), Chong Wang (Taoyi), Xin Wang (Taoyi), Kevin Smith (Taoyi), Haoxuan You (Taoyi), Binazir Karimzadeh (Taoyi), Rui Li (Taoyi), Jinhao Lei (Taoyi), Wei Fang (Taoyi), Alec Doane (Taoyi), Sam Wiseman (Taoyi), Ismael Fernandez (Taoyi), Jane Li (Taoyi), Andrew Hansen (Taoyi), Javier Movellan (Taoyi), Christopher Neubauer (Taoyi), Hanzhi Zhou (Taoyi), Chris Chaney (Taoyi), Nazir Kamaldin (Taoyi), Valentin Wolf (Taoyi), Fernando Berm\'udez-Medina (Taoyi), Joris Pelemans (Taoyi), Peter Fu (Taoyi), Howard Xing (Taoyi), Xiang Kong (Taoyi), Wayne Shan (Taoyi), Gabriel Jacoby-Cooper (Taoyi), Dongcai Shen (Taoyi), Tom Gunter (Taoyi), Guillaume Seguin (Taoyi), Fangping Shi (Taoyi), Shiyu Li (Taoyi), Yang Xu (Taoyi), Areeba Kamal (Taoyi), Dan Masi (Taoyi), Saptarshi Guha (Taoyi), Qi Zhu (Taoyi), Jenna Thibodeau (Taoyi), Changyuan Zhang (Taoyi), Rebecca Callahan (Taoyi), Charles Maalouf (Taoyi), Wilson Tsao (Taoyi), Boyue Li (Taoyi), Qingqing Cao (Taoyi), Naomy Sabo (Taoyi), Cheng Leong (Taoyi), Yi Wang (Taoyi), Anupama Mann Anupama (Taoyi), Colorado Reed (Taoyi), Kenneth Jung (Taoyi), Zhifeng Chen (Taoyi), Mohana Prasad Sathya Moorthy (Taoyi), Yifei He (Taoyi), Erik Hornberger (Taoyi), Devi Krishna (Taoyi), Senyu Tong (Taoyi),  Michael (Taoyi),  Lee, David Haldimann, Yang Zhao, Bowen Zhang, Chang Gao, Chris Bartels, Sushma Rao, Nathalie Tran, Simon Lehnerer, Co Giang, Patrick Dong, Junting Pan, Biyao Wang, Dongxu Li, Mehrdad Farajtabar, Dongseong Hwang, Grace Duanmu, Eshan Verma, Sujeeth Reddy, Qi Shan, Hongbin Gao, Nan Du, Pragnya Sridhar, Forrest Huang, Yingbo Wang, Nikhil Bhendawade, Diane Zhu, Sai Aitharaju, Fred Hohman, Lauren Gardiner, Chung-Cheng Chiu, Yinfei Yang, Alper Kokmen, Frank Chu, Ke Ye, Kaan Elgin, Oron Levy, John Park, Donald Zhang, Eldon Schoop, Nina Wenzel, Michael Booker, Hyunjik Kim, Chinguun Erdenebileg, Nan Dun, Eric Liang Yang, Priyal Chhatrapati, Vishaal Mahtani, Haiming Gang, Kohen Chia, Deepa Seshadri, Donghan Yu, Yan Meng, Kelsey Peterson, Zhen Yang, Yongqiang Wang, Carina Peng, Doug Kang, Anuva Agarwal, Albert Antony, Juan Lao Tebar, Albin Madappally Jose, Regan Poston, Andy De Wang, Gerard Casamayor, Elmira Amirloo, Violet Yao, Wojciech Kryscinski, Kun Duan, Lezhi L  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 6.0

arXiv:2507.13575v3 Announce Type: replace-cross 
Abstract: We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations suc...

---

### 2. [Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)](https://arxiv.org/abs/2508.19428)

**Authors**: Aleksandra Beliaeva, Temurbek Rahmatullaev  
**Category**: cs.CL  
**Published**: 2025-08-28  
**Score**: 6.0

arXiv:2508.19428v1 Announce Type: new 
Abstract: We present a comprehensive system for addressing Tasks A, B, and C of the LLMs4OL 2025 challenge, which together span the full ontology construction pipeline: term extraction, typing, and taxonomy discovery. Our approach combines retrieval-augmented p...

---

### 3. [AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios](https://arxiv.org/abs/2508.19988)

**Authors**: Lisa Alazraki, Lihu Chen, Ana Brassard, Joe Stacey, Hossein A. Rahmani, Marek Rei  
**Category**: cs.CL  
**Published**: 2025-08-28  
**Score**: 5.5

arXiv:2508.19988v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved high accuracy on complex commonsense and mathematical problems that involve the composition of multiple reasoning steps. However, current compositional benchmarks testing these skills tend to focus on either ...

---

### 4. [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)

**Authors**: Sheryl Mathew, N Harshit  
**Category**: cs.LG  
**Published**: 2025-08-28  
**Score**: 5.5

arXiv:2508.19567v1 Announce Type: new 
Abstract: In reinforcement learning with human feedback (RLHF), reward models can efficiently learn and amplify latent biases within multimodal datasets, which can lead to imperfect policy optimization through flawed reward signals and decreased fairness. Bias ...

---

### 5. [RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting](https://arxiv.org/abs/2508.19286)

**Authors**: Zhan Shi, Yefeng Yuan, Yuhong Liu, Liang Cheng, Yi Fang  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 5.0

arXiv:2508.19286v1 Announce Type: cross 
Abstract: The performance of modern machine learning systems depends on access to large, high-quality datasets, often sourced from user-generated content or proprietary, domain-specific corpora. However, these rich datasets inherently contain sensitive person...

---

### 6. [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)

**Authors**: Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Xiaoqiang Liu, Pengfei Wan  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 5.0

arXiv:2508.19320v1 Announce Type: cross 
Abstract: Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to exis...

---

### 7. [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)

**Authors**: Lotte Gross, Rebecca Walter, Nicole Zoppi, Adrien Justus, Alessandro Gambetti, Qiwei Han, Maximilian Kaiser  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 5.0

arXiv:2508.20013v1 Announce Type: cross 
Abstract: This study addresses critical industrial challenges in e-commerce product categorization, namely platform heterogeneity and the structural limitations of existing taxonomies, by developing and deploying a multimodal hierarchical classification frame...

---

### 8. [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)

**Authors**: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji  
**Category**: cs.CL  
**Published**: 2025-08-28  
**Score**: 5.0

arXiv:2508.19740v1 Announce Type: new 
Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify i...

---

### 9. [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)

**Authors**: Sining Zhoubian, Dan Zhang, Yuxiao Dong, Jie Tang  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 4.5

arXiv:2508.19576v1 Announce Type: new 
Abstract: With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from diffi...

---

### 10. [Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices](https://arxiv.org/abs/2508.19078)

**Authors**: Fahao Chen, Jie Wan, Peng Li, Zhou Su, Dongxiao Yu  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 4.5

arXiv:2508.19078v1 Announce Type: cross 
Abstract: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models (LLMs) is challenging due to their massive computational requirements and the resource constraints of participants. Existing working attempts to fill this gap through mode...

---

### 11. [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)

**Authors**: Hejia Liu, Mochen Yang, Gediminas Adomavicius  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 4.5

arXiv:2508.19563v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are being applied in a wide array of settings, well beyond the typical language-oriented use cases. In particular, LLMs are increasingly used as a plug-and-play method for fitting data and generating predictions. Prior w...

---

### 12. [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)

**Authors**: Jiaji Zhang, Ruichao Sun, Hailiang Zhao, Jiaju Wu, Peng Chen, Hao Li, Yuying Liu, Kingsum Chow, Gang Xiong, Shuiguang Deng  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 4.5

arXiv:2507.14811v4 Announce Type: replace-cross 
Abstract: Diffusion models have demonstrated exceptional generative capabilities but are computationally intensive, posing significant challenges for deployment in resource-constrained or latency-sensitive environments. Quantization offers an effectiv...

---

### 13. [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)

**Authors**: Chenxi Zhou, Pengfei Cao, Jiang Li, Jun Zhao, Kang Liu  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 4.5

arXiv:2508.18609v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) present significant deployment challenges due to their scale, with post-training quantization (PTQ) emerging as a practical compression solution. However, a comprehensive understanding of how PTQ precisely impact...

---

### 14. [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)

**Authors**: Rongzhi Li, Ruogu Du, Zefang Chu, Sida Zhao, Chunlei Han, Zuocheng Shi, Yiwen Shao, Huanle Han, Long Huang, Zherui Liu, Shufan Liu  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 4.0

arXiv:2508.19559v1 Announce Type: cross 
Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operati...

---

### 15. [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)

**Authors**: Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 4.0

arXiv:2508.19982v1 Announce Type: cross 
Abstract: Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, prima...

---

### 16. [Input-Time Scaling](https://arxiv.org/abs/2508.13654)

**Authors**: Rapheal Huang (Yuming), Weilong Guo  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 4.0

arXiv:2508.13654v3 Announce Type: replace-cross 
Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Inp...

---

### 17. [Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning](https://arxiv.org/abs/2508.19828)

**Authors**: Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich Sch\"utze, Volker Tresp, Yunpu Ma  
**Category**: cs.CL  
**Published**: 2025-08-28  
**Score**: 4.0

arXiv:2508.19828v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this...

---

### 18. [A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation](https://arxiv.org/abs/2508.19591)

**Authors**: Jiakui Shen, Yunqi Mi, Guoshuai Zhao, Jialie Shen, Xueming Qian  
**Category**: cs.DC  
**Published**: 2025-08-28  
**Score**: 4.0

arXiv:2508.19591v1 Announce Type: cross 
Abstract: Centralized recommender systems encounter privacy leakage due to the need to collect user behavior and other private data. Hence, federated recommender systems (FedRec) have become a promising approach with an aggregated global model on the server. ...

---

### 19. [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)

**Authors**: Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, Shixin Wu, Songfang Huang, Huiling Duan  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2508.19257v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while i...

---

### 20. [Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset](https://arxiv.org/abs/2508.19467)

**Authors**: Sumon Kanti Dey, Jeanne M. Powell, Azra Ismail, Jeanmarie Perrone, Abeed Sarker  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2508.19467v1 Announce Type: cross 
Abstract: Nonmedical opioid use is an urgent public health challenge, with far-reaching clinical and social consequences that are often underreported in traditional healthcare settings. Social media platforms, where individuals candidly share first-person exp...

---

### 21. [Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study](https://arxiv.org/abs/2508.19481)

**Authors**: Manuel Mosquera, Melissa Robles, Johan Rodriguez, Ruben Manrique  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2508.19481v1 Announce Type: cross 
Abstract: Low-resource machine translation remains a significant challenge for large language models (LLMs), which often lack exposure to these languages during pretraining and have limited parallel data for fine-tuning. We propose a novel approach that enhan...

---

### 22. [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)

**Authors**: Chao Huang, Zefeng Zhang, Juewei Yue, Quangang Li, Chuang Zhang, Tingwen Liu  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2508.19697v1 Announce Type: cross 
Abstract: Current safety alignment for large language models(LLMs) continues to present vulnerabilities, given that adversarial prompting can effectively bypass their safety measures.Our investigation shows that these safety mechanisms predominantly depend on...

---

### 23. [Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding](https://arxiv.org/abs/2508.19529)

**Authors**: Bowen Sun, Yujun Cai, Ming-Hsuan Yang, Yiwei Wang  
**Category**: cs.CL  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2508.19529v1 Announce Type: new 
Abstract: Discrete diffusion language models have shown strong potential for text generation, yet standard supervised fine-tuning (SFT) misaligns with their semi-autoregressive inference: training randomly masks tokens across the entire response, while inferenc...

---

### 24. [Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2508.19919)

**Authors**: Jingyu Guo, Yingying Xu  
**Category**: cs.CL  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2508.19919v1 Announce Type: new 
Abstract: While stereotypes are well-documented in human social interactions, AI systems are often presumed to be less susceptible to such biases. Previous studies have focused on biases inherited from training data, but whether stereotypes can emerge spontaneo...

---

### 25. [Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks](https://arxiv.org/abs/2508.20038)

**Authors**: Sheng Liu, Qiang Sheng, Danding Wang, Yang Li, Guang Yang, Juan Cao  
**Category**: cs.CL  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2508.20038v1 Announce Type: new 
Abstract: Despite advances in improving large language model(LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpor...

---

### 26. [On Domain-Adaptive Post-Training for Multimodal Large Language Models](https://arxiv.org/abs/2411.19930)

**Authors**: Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang  
**Category**: cs.CL  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2411.19930v4 Announce Type: replace 
Abstract: Adapting general multimodal large language models (MLLMs) to specific domains, such as scientific and industrial fields, is highly significant in promoting their practical applications. This paper systematically investigates domain adaptation of M...

---

### 27. [HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference](https://arxiv.org/abs/2508.19373)

**Authors**: Haoran Lin, Xianzhi Yu, Kang Zhao, Han Bao, Zongyuan Zhan, Ting Hu, Wulong Liu, Zekun Yin, Xin Li, Weiguo Liu  
**Category**: cs.DC  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2508.19373v1 Announce Type: new 
Abstract: Current inference systems for Mixture-of-Experts (MoE) models primarily employ static parallelization strategies. However, these static approaches cannot consistently achieve optimal performance across different inference scenarios, as they lack the f...

---

### 28. [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)

**Authors**: Zhiwei Li, Yong Hu, Wenqing Wang  
**Category**: cs.LG  
**Published**: 2025-08-28  
**Score**: 3.5

arXiv:2508.19598v1 Announce Type: new 
Abstract: The functionality of Large Language Model (LLM) agents is primarily determined by two capabilities: action planning and answer summarization. The former, action planning, is the core capability that dictates an agent's performance. However, prevailing...

---

### 29. [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)

**Authors**: Xifeng Yao, Chengyuan Ma, Dongyu Lang, Yinhao Ni, Zhiwei Xu, Huarui Xie, Zihao Chen, Guang Shen, Dandan Tu, Yi Bai, Changzheng Zhang  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 3.0

arXiv:2508.19502v1 Announce Type: new 
Abstract: In recent months, substantial progress has been made in complex reasoning of Large Language Models, particularly through the application of test-time scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When responding to a query, these ...

---

### 30. [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)

**Authors**: Anat Heilper, Doron Singer  
**Category**: cs.AI  
**Published**: 2025-08-28  
**Score**: 3.0

arXiv:2508.19263v1 Announce Type: cross 
Abstract: As deep learning models grow and deployment becomes more widespread, reducing the storage and transmission costs of neural network weights has become increasingly important. While prior work such as ZipNN has shown that lossless compression methods ...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative Decoding

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
