# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-12 06:46:51 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.10965)

**Authors**: Yupu Gu, Rongzhe Wei, Andy Zhu, Pan Li  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.10965v1  

#### Abstract
Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable L...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs â€”â€” æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰ä¸»æµçš„ **Knowledge Editing (KE)** æ–¹æ³•å¤§å¤šé’ˆå¯¹ **dense Transformer æ¶æ„** è®¾è®¡ï¼Œè€Œç°ä»£å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ Qwenã€GPT-OSSï¼‰å¹¿æ³›é‡‡ç”¨ **Mixture-of-Experts (MoE)** æ¶æ„ã€‚ç›´æ¥å°†ä¼ ç»Ÿ KE æ–¹æ³•åº”ç”¨äº MoE æ¨¡å‹é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š
1. **è®¡ç®—æˆæœ¬é«˜**ï¼šéœ€æ›´æ–°æ‰€æœ‰ä¸“å®¶å‚æ•°ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼ˆä¾‹å¦‚ 128 ä¸ªä¸“å®¶ â†’ æˆæœ¬ Ã—128ï¼‰ã€‚
2. **ä¸“å®¶è€¦åˆï¼ˆExpert Couplingï¼‰**ï¼šè¾“å‡ºæ˜¯å¤šä¸ªä¸“å®¶åŠ æƒç»„åˆçš„ç»“æœï¼Œå•ä¸ªä¸“å®¶ä¿®æ”¹å¯èƒ½è¢«ç¨€é‡Šæˆ–å¼•å‘å‰¯ä½œç”¨ã€‚
3. **è·¯ç”±åˆ†å¸ƒåç§»ï¼ˆRouting Distribution Shiftï¼‰**ï¼šå‚æ•°æ‰°åŠ¨ä¼šæ”¹å˜ä¸‹æ¸¸ MoE å±‚çš„è¾“å…¥ï¼Œä»è€Œå½±å“è·¯ç”±å™¨é€‰æ‹©çš„ä¸“å®¶é›†åˆï¼Œç ´åæ¨¡å‹åŸæœ‰çš„çŸ¥è¯†è·¯å¾„å’Œç¨³å®šæ€§ã€‚

è¿™äº›é—®é¢˜ä½¿å¾—åœ¨ MoE æ¨¡å‹ä¸­å®ç°**é«˜æ•ˆã€ç¨³å®šä¸”å±€éƒ¨åŒ–**çš„çŸ¥è¯†ç¼–è¾‘å˜å¾—æä¸ºå›°éš¾ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šMoEEdit
ä½œè€…æå‡ºäº† **MoEEdit**ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸º MoE æ¶æ„è®¾è®¡çš„ã€å…·æœ‰ **routing-stable** ç‰¹æ€§çš„å‚æ•°ä¿®æ”¹å‹çŸ¥è¯†ç¼–è¾‘æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰Per-Expert Null-Space Projectionï¼ˆæ¯ä¸“å®¶é›¶ç©ºé—´æŠ•å½±ï¼‰
- **æ€æƒ³æ¥æº**ï¼šå— AlphaEdit åœ¨ dense æ¨¡å‹ä¸­ä½¿ç”¨ null-space æŠ•å½±å¯å‘ã€‚
- **æ”¹è¿›ä¹‹å¤„**ï¼šåœ¨æ¯ä¸ªä¸“å®¶å†…éƒ¨ç‹¬ç«‹æ„é€ ä¸€ä¸ªæ­£äº¤æŠ•å½±çŸ©é˜µ $ P_n $ï¼Œç¡®ä¿å¯¹ä¸“å®¶å‚æ•°çš„æ›´æ–° $ \Delta_n $ åªä½œç”¨äºä¸å½±å“ä¿ç•™é›†ï¼ˆpreservation setï¼‰çš„æ–¹å‘ã€‚
- **æ•ˆæœ**ï¼šä¿è¯ç¼–è¾‘ä¸å¹²æ‰°æ— å…³çŸ¥è¯†ï¼Œå¹¶**æ˜¾å¼æŠ‘åˆ¶è·¯ç”±è¾“å…¥çš„å˜åŒ–**ï¼Œä»è€Œé˜²æ­¢ä¸‹æ¸¸è·¯ç”±åˆ†å¸ƒå‘ç”Ÿåç§»ã€‚

#### ï¼ˆ2ï¼‰Randomized Block Coordinate Descent (BCD) Solver
- å°†å…¨å±€ä¼˜åŒ–é—®é¢˜åˆ†è§£ä¸ºæŒ‰â€œä¸“å®¶â€åˆ’åˆ†çš„å—çŠ¶å­é—®é¢˜ã€‚
- ä½¿ç”¨éšæœºå—åæ ‡ä¸‹é™æ³•é€ä¸ªä¼˜åŒ–ç›¸å…³ä¸“å®¶ï¼Œé¿å…æ±‚è§£è§„æ¨¡è¾¾ $ O(Nd_k)^2 $ çš„å¤§å‹çŸ©é˜µé€†ã€‚
- **å¤æ‚åº¦ä» $ O(d_m (Nd_k)^2) $ é™è‡³ $ O(d_k^2) $**ï¼Œä»…ä¸ä¸“å®¶éšè—ç»´åº¦æœ‰å…³ï¼Œä¸æ€»ä¸“å®¶æ•°æ— å…³ï¼Œæå¤§æå‡å¯æ‰©å±•æ€§ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | MoEEdit | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ FT, UnKEï¼‰ |
|------|--------|--------------------------|
| **é€‚ç”¨æ¶æ„** | æ˜¾å¼æ”¯æŒ MoE | ä¸»è¦ä¸º dense æ¨¡å‹è®¾è®¡ |
| **è®¡ç®—æ•ˆç‡** | é«˜æ•ˆï¼Œä»…æ›´æ–°å…³é”®ä¸“å®¶ | æ›´æ–°å…¨éƒ¨ä¸“å®¶ï¼Œæˆæœ¬é«˜æ˜‚ |
| **è·¯ç”±ç¨³å®šæ€§** | å¼ºåˆ¶ä¿æŒè·¯ç”±ä¸å˜ï¼ˆvia æŠ•å½±ï¼‰ | å¿½è§†è·¯ç”±å˜åŒ–ï¼Œæ˜“é€ æˆ cascading drift |
| **ç¼–è¾‘ç‰¹å¼‚æ€§** | æ›´é«˜ï¼Œå¹²æ‰°æ›´å°‘ | å®¹æ˜“ç ´åæ— å…³è¡Œä¸º |
| **å¯æ‰©å±•æ€§** | æ”¯æŒä¸Šç™¾ä¸“å®¶çš„å¤§è§„æ¨¡ MoE | åœ¨å¤§è§„æ¨¡ MoE ä¸Šéš¾ä»¥è¿è¡Œ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **COUNTERFACT**ï¼šæ ‡å‡†åäº‹å®ç¼–è¾‘æ•°æ®é›†ï¼Œç”¨äºæµ‹è¯•å•è·³äº‹å®æ›¿æ¢ï¼ˆå¦‚ â€œBing Videos is from Microsoft â†’ Googleâ€ï¼‰ï¼Œå¼ºè°ƒç¼–è¾‘å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
- **zsRE (Zero-Shot Relation Extraction)**ï¼šé›¶æ ·æœ¬å…³ç³»æŠ½å–ä»»åŠ¡ï¼Œè¯„ä¼°æ¨¡å‹åœ¨æœªè§è¿‡å¥å¼ä¸‹çš„æ¨ç†è¿ç§»èƒ½åŠ›ã€‚

> ç¤ºä¾‹è§é™„å½•ï¼ˆAppendix Dï¼‰ï¼ŒåŒ…å«åŸå§‹é¢„æµ‹ã€ç›®æ ‡å¯¹è±¡ã€æ”¹å†™æç¤ºç­‰ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
| é¡¹ç›® | è®¾ç½® |
|------|------|
| **æ¨¡å‹** | `Qwen3-30B-A3B`ï¼ˆ128ä¸“å®¶ï¼Œtop-8æ¿€æ´»ï¼‰ã€`GPT-OSS-20B`ï¼ˆ32ä¸“å®¶ï¼Œtop-4æ¿€æ´»ï¼‰ |
| **ç¼–è¾‘æ–¹å¼** | æ‰¹é‡è¿ç»­ç¼–è¾‘ï¼ˆsequential batched editsï¼‰ï¼Œå…±è¿›è¡Œ 1,000 æ¬¡ç¼–è¾‘ï¼Œæ¯æ¬¡æ‰¹å¤§å°ä¸º 50 |
| **ç¡¬ä»¶å¹³å°** | å•å°é…å¤‡ NVIDIA H20 GPU çš„èŠ‚ç‚¹ï¼Œæƒé‡ä½¿ç”¨ BF16ï¼Œä¼˜åŒ–è¿‡ç¨‹ä½¿ç”¨ FP32 |
| **ç¼–è¾‘å±‚** | å¤šå±‚è”åˆç¼–è¾‘ï¼ˆå¦‚ Qwen ç¼–è¾‘ layer {3,4,5,6,7}ï¼‰ |

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
æ²¿ç”¨ KE é¢†åŸŸæ ‡å‡†ä¸‰å…ƒç»„æŒ‡æ ‡ï¼š
1. **Efficacyï¼ˆæœ‰æ•ˆæ€§ï¼‰**ï¼šåœ¨ç¼–è¾‘æç¤ºä¸‹æ­£ç¡®ç”Ÿæˆæ–°äº‹å®çš„æ¯”ä¾‹ã€‚
2. **Generalizationï¼ˆæ³›åŒ–æ€§ï¼‰**ï¼šåœ¨åŒä¹‰æ”¹å†™ï¼ˆparaphraseï¼‰ä¸Šä¸‹æ–‡ä¸­ä»èƒ½æ­£ç¡®è¾“å‡ºçš„èƒ½åŠ›ã€‚
3. **Specificityï¼ˆç‰¹å¼‚æ€§ï¼‰**ï¼šå¯¹æ— å…³æ§åˆ¶æ ·ä¾‹æ— å½±å“çš„ç¨‹åº¦ï¼ˆå³ localityï¼‰ã€‚
4. **Utilityï¼ˆç»¼åˆæ•ˆç”¨ï¼‰**ï¼šä¸Šè¿°ä¸‰é¡¹çš„å¹³å‡å€¼ï¼Œåæ˜ æ•´ä½“å¹³è¡¡è¡¨ç°ã€‚

æ­¤å¤–å¼•å…¥ï¼š
- **Routing Similarity (RS)**ï¼šå‰åç¼–è¾‘ Top-K è·¯ç”±ä¸“å®¶é›†åˆçš„ Jaccard ç›¸ä¼¼åº¦ï¼Œè¡¡é‡è·¯ç”±ç¨³å®šæ€§ã€‚
- **KL Divergence**ï¼šå‰åè·¯ç”±åˆ†å¸ƒä¹‹é—´çš„ KL æ•£åº¦ï¼Œé‡åŒ–åˆ†å¸ƒæ¼‚ç§»ç¨‹åº¦ã€‚

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æ˜¯å¦é€‚é… MoE |
|------|------|-------------|
| **Fine-Tuning (FT)** | å…¨å¾®è°ƒ | æ˜¯ï¼Œä½†ä½æ•ˆ |
| **FT-L** | åŠ èŒƒæ•°çº¦æŸçš„å¾®è°ƒ | æ˜¯ |
| **AdaLoRA** | å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ | æ˜¯ |
| **UnKE** | ç»“æ„åŒ– KE æ–¹æ³•ï¼ˆåŸä¸º dense è®¾è®¡ï¼‰ | æ˜¯ï¼Œç›´æ¥åº”ç”¨ |
| **MoEEdit (Ours)** | ä¸“ä¸º MoE è®¾è®¡çš„ KE æ–¹æ³• | âœ… æœ¬æ–‡æå‡º |

> æ‰€æœ‰ baseline å‡å°è¯•è¿ç§»åˆ° MoE æ¶æ„ä¸Šè¿è¡Œã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ï¼ˆTable 1ï¼‰

#### åœ¨ **Qwen3-30B-A3B + COUNTERFACT** ä¸Šï¼š
| Method | Eff. â†‘ | Gen. â†‘ | Spe. â†‘ | Utility â†‘ |
|--------|--------|--------|--------|-----------|
| UnKE | 89.30 | 82.85 | 48.15 | 73.43 |
| **MoEEdit** | **99.30** | **94.10** | **80.97** | **91.46** |

> MoEEdit åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå…¨é¢é¢†å…ˆï¼Œå°¤å…¶åœ¨ specificity ä¸Šè¿œè¶… UnKEï¼ˆ+32.82ï¼‰ï¼Œè¯´æ˜æ›´å¼ºçš„å±€éƒ¨æ€§ã€‚

#### åœ¨ **GPT-OSS-20B + ZsRE** ä¸Šï¼š
| Method | Eff. â†‘ | Gen. â†‘ | Spe. â†‘ | Utility â†‘ |
|--------|--------|--------|--------|-----------|
| AdaLoRA | 43.46 | 42.96 | 33.60 | 40.01 |
| **MoEEdit** | **81.68** | **68.44** | **32.55** | **60.89** |

> MoEEdit åœ¨ efficacy å’Œ generalization ä¸Šåˆ†åˆ«æå‡è¶…è¿‡ **+38** å’Œ **+25** ä¸ªç™¾åˆ†ç‚¹ï¼Œå°½ç®¡ specificity ç•¥ä½ï¼Œä½†æ•´ä½“ utility æ˜¾è‘—æ›´é«˜ã€‚

âœ… **ç»“è®º**ï¼šMoEEdit åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡å–å¾— SOTA è¡¨ç°ï¼Œåœ¨å‡†ç¡®æ€§ä¸æ³›åŒ–æ€§æ–¹é¢ä¼˜åŠ¿å·¨å¤§ã€‚

---

### ğŸ› ï¸ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ï¼ˆ1ï¼‰æ¶ˆå»æŠ•å½±æœºåˆ¶çš„å½±å“ï¼ˆTable 3ï¼‰
| è®¾ç½® | Editing Set RS â†“ | Preservation Set RS â†“ |
|------|------------------|------------------------|
| MoEEdit (å®Œæ•´ç‰ˆ) | 86.62â€“89.93 | 87.02â€“90.22 |
| MoEEdit (w/o Proj) | 73.64â€“73.75 | 73.59â€“73.50 |

> ç§»é™¤æŠ•å½±åï¼Œ**å¹³å‡ RS ä¸‹é™çº¦ 15 ä¸ªç™¾åˆ†ç‚¹**ï¼ŒKL æ•£åº¦ä» 0.02 å‡è‡³ 0.0834ï¼Œè¯æ˜ **null-space projection å¯¹ç»´æŒè·¯ç”±ç¨³å®šè‡³å…³é‡è¦**ã€‚

#### ï¼ˆ2ï¼‰BCD ä¸é—­å¼æ±‚è§£å™¨å¯¹æ¯”ï¼ˆFigure 3ï¼‰
- **é—­å¼æ±‚è§£å™¨ï¼ˆClosed-formï¼‰**ï¼šæ—¶é—´éšä¸“å®¶æ•°é‡ $ N $ å‘ˆè¿‘äºŒæ¬¡å¢é•¿ï¼Œåœ¨ $ N > 60 $ åä¸å¯è¡Œã€‚
- **BCD Solver**ï¼šè¿è¡Œæ—¶é—´å‡ ä¹æ’å®šï¼Œå³ä½¿åœ¨ $ N=128 $ æ—¶ä»é«˜æ•ˆã€‚
> âœ… éªŒè¯äº† BCD çš„**å¯æ‰©å±•æ€§ä¼˜åŠ¿**ã€‚

#### ï¼ˆ3ï¼‰BCD è¿­ä»£æ¬¡æ•°åˆ†æï¼ˆFigure 4ï¼‰
- **2â€“6 æ¬¡ pass**ï¼šæ€§èƒ½å¿«é€Ÿä¸Šå‡ï¼›
- **>10 æ¬¡ pass**ï¼šæ”¶ç›Šé€’å‡ã€‚
> æ¨èä½¿ç”¨ **6â€“10 æ¬¡ pass**ï¼Œå³å¯è¾¾åˆ°æ€§èƒ½ä¸æ•ˆç‡çš„æœ€ä½³å¹³è¡¡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **MoE æ¶æ„éœ€è¦ä¸“é—¨çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•**  
   ç›´æ¥è¿ç§» dense æ¨¡å‹çš„ KE æ–¹æ³•ä¼šå¯¼è‡´ä¸¥é‡çš„è·¯ç”±ä¸ç¨³å®šå’Œè®¡ç®—ç“¶é¢ˆã€‚

2. **Routing Stability æ˜¯ MoE ç¼–è¾‘çš„æ ¸å¿ƒæŒ‘æˆ˜**  
   å¾®å°å‚æ•°æ‰°åŠ¨å¯é€šè¿‡è·¯ç”±æœºåˆ¶ä¼ æ’­å¹¶æ”¾å¤§ï¼Œç ´åæ¨¡å‹å†…éƒ¨çš„çŸ¥è¯†ç»„ç»‡ç»“æ„ã€‚MoEEdit é¦–æ¬¡æ˜ç¡®æå‡ºå¹¶è§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚

3. **Expert-Aware Design è‡³å…³é‡è¦**  
   åˆ©ç”¨ MoE çš„æ¨¡å—åŒ–ç‰¹æ€§ï¼ˆæ¯ä¸ª expert æ˜¯ç‹¬ç«‹å—ï¼‰ï¼Œé€šè¿‡ per-expert null-space projection å’Œ block-wise BCD å®ç°é«˜æ•ˆç¨³å®šçš„ç¼–è¾‘ã€‚

4. **MoEEdit å®ç°äº† SOTA æ€§èƒ½ä¸å“è¶Šæ•ˆç‡çš„ç»Ÿä¸€**  
   ä¸ä»…åœ¨ Efficacyã€Generalization ä¸Šæ˜¾è‘—ä¼˜äº baselinesï¼Œè¿˜åœ¨ Specificity å’Œ Routing Stability ä¸Šè¡¨ç°å‡ºè‰²ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **ä¾èµ–å®šä½é˜¶æ®µï¼ˆlocate-then-editï¼‰**  
   ä»éœ€ä¾èµ– causal tracing ç­‰æ‰‹æ®µç¡®å®šç¼–è¾‘å±‚å’Œä¸“å®¶ï¼Œå°šæœªå®Œå…¨è‡ªåŠ¨åŒ–ã€‚
2. **ä¸»è¦é¢å‘ FFN å±‚ä¸­çš„ key-value å­˜å‚¨å‡è®¾**  
   å‡è®¾çŸ¥è¯†å­˜å‚¨åœ¨ FFN çš„ down-projection ä¸­ï¼Œå¯èƒ½ä¸é€‚ç”¨äºæ‰€æœ‰ç±»å‹çš„äº‹å®ã€‚
3. **æœªå¤„ç†å¤šè·³æˆ–å¤šä¸“å®¶ååŒç¼–è¾‘åœºæ™¯**  
   å½“å‰æ–¹æ³•èšç„¦äºå•è·³äº‹å®ï¼Œå¤æ‚çŸ¥è¯†ç»“æ„çš„ç¼–è¾‘å°šå¾…ç ”ç©¶ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **è‡ªåŠ¨è¯†åˆ«å…³é”®ä¸“å®¶å­é›†**  
   å¼€å‘æ— éœ€äººå·¥æŒ‡å®šç¼–è¾‘å±‚/ä¸“å®¶çš„é€‰æ‹©æœºåˆ¶ã€‚
2. **æ”¯æŒå¤šè·³çŸ¥è¯†ç¼–è¾‘ä¸å› æœé“¾æ›´æ–°**  
   åº”å¯¹æ¶‰åŠå¤šä¸ªå®ä½“å’Œå…³ç³»çš„å¤åˆäº‹å®ã€‚
3. **ç»“åˆ parameter-preserving æ–¹æ³•æ„å»º hybrid ç¼–è¾‘ç³»ç»Ÿ**  
   å¦‚èåˆ SERAC æˆ– LEMoE çš„å¤–éƒ¨è®°å¿†æœºåˆ¶ï¼Œå¢å¼ºçµæ´»æ€§ä¸å¯é€†æ€§ã€‚
4. **æ¢ç´¢ MoEEdit åœ¨æ¨¡å‹é—å¿˜ï¼ˆmachine unlearningï¼‰ä¸­çš„åº”ç”¨**  
   åˆ©ç”¨å…¶é«˜ specificity å®ç°ç²¾å‡†åˆ é™¤ç‰¹å®šçŸ¥è¯†ã€‚

---

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **MoEEdit æ˜¯é¦–ä¸ªä¸“ä¸º Mixture-of-Experts æ¶æ„è®¾è®¡çš„ routing-stable çŸ¥è¯†ç¼–è¾‘æ–¹æ³•ï¼Œé€šè¿‡ per-expert null-space projection æŠ‘åˆ¶è·¯ç”±æ¼‚ç§»ï¼Œé…åˆé«˜æ•ˆçš„ randomized BCD æ±‚è§£å™¨ï¼Œåœ¨ä¿æŒé«˜ specificity çš„åŒæ—¶å®ç°äº† SOTA çš„ç¼–è¾‘æ€§èƒ½ä¸å“è¶Šçš„å¯æ‰©å±•æ€§ï¼Œä¸º MoE-based LLMs çš„æŒç»­å¯æ§æ¼”åŒ–å¥ å®šäº†åšå®åŸºç¡€ã€‚**

ğŸ”— **ä»£ç å¼€æºåœ°å€**ï¼š[https://github.com/Terence-Gu/MoEEdit](https://github.com/Terence-Gu/MoEEdit)

</details>

---

### 2. [Learning to Evict from Key-Value Cache](https://arxiv.org/abs/2602.10238)

**Authors**: Luca Moschella, Laura Manduchi, Ozan Sener  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.10238v1  

#### Abstract
The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve o...

---

### 3. [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)

**Authors**: Yansong Qu, Zihao Sheng, Zilin Huang, Jiancong Chen, Yuhao Luo, Tianyi Wang, Yiheng Feng, Samuel Labi, Sikai Chen  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.10458v1  

#### Abstract
Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offeri...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# ã€ŠFound-RL: foundation model-enhanced reinforcement learning for autonomous drivingã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

- **æ ·æœ¬æ•ˆç‡ä½**ï¼šä¼ ç»Ÿ Reinforcement Learning (RL) åœ¨è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸­ä¾èµ–å¤§é‡è¯•é”™ï¼Œè®­ç»ƒè¿‡ç¨‹ç¼“æ…¢ä¸”ä½æ•ˆã€‚
- **è¯­ä¹‰å¯è§£é‡Šæ€§å·®**ï¼šRL ç­–ç•¥ç¼ºä¹å¯¹å¤æ‚äº¤é€šåœºæ™¯çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œéš¾ä»¥å¤„ç†å®‰å…¨æ•æ„Ÿçš„é©¾é©¶å†³ç­–ã€‚
- **è®¡ç®—å»¶è¿Ÿç“¶é¢ˆ**ï¼šå°†å¤§å‹ Foundation Modelsï¼ˆå¦‚ VLMsï¼‰ç›´æ¥é›†æˆåˆ°é«˜é¢‘ RL è®­ç»ƒå¾ªç¯ä¸­ä¼šå› æ¨ç†å»¶è¿Ÿå¯¼è‡´è®­ç»ƒåœæ»ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æå‡º **Found-RL** â€”â€” ä¸€ä¸ªä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„ **foundation model-enhanced RL å¹³å°**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰**å¼‚æ­¥æ‰¹é‡æ¨ç†æ¡†æ¶ï¼ˆAsynchronous Batch Inference Frameworkï¼‰**
- å°†é«˜å¼€é”€çš„ VLM æ¨ç†ä»å®æ—¶ä»¿çœŸå¾ªç¯ä¸­è§£è€¦ã€‚
- é€šè¿‡å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ï¼Œå°†å¤šä¸ªç¯å¢ƒçš„è§‚æµ‹æ‰“åŒ…æˆå¾®æ‰¹æ¬¡è¿›è¡Œå¹¶è¡Œæ¨ç†ï¼Œæ˜¾è‘—é™ä½å»¶è¿Ÿå½±å“ã€‚
- æ”¯æŒ VLM åé¦ˆçš„â€œéé˜»å¡â€è·å–ï¼Œå®ç°è¿‘å®æ—¶çš„ RL è®­ç»ƒé—­ç¯ã€‚

#### ï¼ˆ2ï¼‰**VLM åŠ¨ä½œå¼•å¯¼æœºåˆ¶ï¼ˆVLM Action Guidanceï¼‰**
- **Value-Margin Regularization (VMR)**ï¼šåœ¨ critic å­¦ä¹ ä¸­å¼•å…¥æ­£åˆ™é¡¹ï¼Œå¼ºåˆ¶ VLM å»ºè®®åŠ¨ä½œçš„ä»·å€¼é«˜äºå½“å‰ç­–ç•¥åŠ¨ä½œä¸€ä¸ªå›ºå®š marginï¼Œå¼•å¯¼ç­–ç•¥å‘ä¸“å®¶è¡Œä¸ºé æ‹¢ã€‚
- **Advantage-Weighted Action Guidance (AWAG)**ï¼šä»…å½“ VLM å»ºè®®åŠ¨ä½œè¢« critic åˆ¤æ–­ä¼˜äºå½“å‰ç­–ç•¥æ—¶ï¼Œæ‰é€šè¿‡åŠ æƒæ–¹å¼æ¨¡ä»¿è¯¥åŠ¨ä½œï¼Œå®ç°å®‰å…¨çš„æ¢ç´¢çº¦æŸã€‚

#### ï¼ˆ3ï¼‰**åŸºäº CLIP çš„å¥–åŠ±å¡‘å½¢ï¼ˆCLIP-based Reward Shapingï¼‰**
- æå‡º **Conditional Contrastive Action Alignment** æ–¹æ³•ï¼š
  - å°†è½¦é€Ÿå’Œå¯¼èˆªæŒ‡ä»¤ç¦»æ•£åŒ–ï¼Œæ„å»ºä¸Šä¸‹æ–‡æ¡ä»¶æç¤ºï¼ˆcontext-conditioned promptsï¼‰ã€‚
  - å®šä¹‰å°è§„æ¨¡çš„åŠ¨ä½œé”šç‚¹é›†ï¼ˆaction anchorsï¼‰ï¼Œé¿å…å…¨å±€åˆ†ç±»çš„æ¦‚ç‡ç¨€é‡Šã€‚
  - è®¡ç®— VLM å¯¹å½“å‰åŠ¨ä½œä¸æœ€éš¾è´Ÿæ ·æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ marginï¼Œå¹¶æ ‡å‡†åŒ–åä½œä¸ºç¨ å¯†å¥–åŠ±ä¿¡å·ã€‚
- æœ‰æ•ˆç¼“è§£äº† CLIP æ¨¡å‹å¯¹åŠ¨æ€çŠ¶æ€ä¸æ•æ„Ÿçš„é—®é¢˜ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | Found-RL çš„ä¼˜åŠ¿ |
|------|----------------|
| **æ•ˆç‡** | å¼‚æ­¥æ‰¹å¤„ç†ä½¿ VLM åé¦ˆååé‡æ¥è¿‘ 100%ï¼Œæ”¯æŒå®æ—¶è®­ç»ƒï¼ˆ~500 FPSï¼‰ |
| **æ€§èƒ½** | è½»é‡çº§ RL æ¨¡å‹ï¼ˆ3.82M å‚æ•°ï¼‰è¾¾åˆ°ç”šè‡³è¶…è¶Šæ•°åäº¿å‚æ•° VLM çš„é©¾é©¶è¡¨ç° |
| **å®‰å…¨æ€§** | æ˜¾è‘—é™ä½ç¢°æ’ç‡å’Œçº¢ç¯è¿è§„æ¬¡æ•°ï¼Œç­–ç•¥æ›´ç¬¦åˆäº¤é€šè§„åˆ™ |
| **é€šç”¨æ€§** | æ¨¡å—åŒ–è®¾è®¡æ”¯æŒå¤šç§ VLM å’Œ RL ç®—æ³•ç»„åˆï¼Œæ˜“äºæ‰©å±• |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†ä¸ä»¿çœŸå¹³å°**

- **ä»¿çœŸç¯å¢ƒ**ï¼šåŸºäº **CARLA** æ„å»ºï¼Œæ”¯æŒå¤šåŸé•‡ï¼ˆTown01â€“Town06ï¼‰ã€å¤šæ ·åŒ–äº¤é€šæµå’Œå¤©æ°”æ¡ä»¶ã€‚
- **åŸºå‡†æµ‹è¯•**ï¼š
  - **Leaderboard Benchmark**ï¼šè®­ç»ƒäº Town01/03/04/06ï¼Œæµ‹è¯•äºæ‰€æœ‰å…­é•‡ã€‚
  - **NoCrash Benchmark**ï¼šè®­ç»ƒäº Town01ï¼Œæµ‹è¯•äº Town01ï¼ˆseenï¼‰å’Œ Town02ï¼ˆunseenï¼‰ã€‚
- **VLM å¾®è°ƒæ•°æ®**ï¼šæ”¶é›†çº¦ **137ä¸‡æ¡** state-action è½¨è¿¹ï¼Œæ¥è‡ª Roach PPO å’Œ Autopilot ä¸“å®¶ç­–ç•¥ã€‚

### **å®éªŒè®¾ç½®**

- **è§‚å¯Ÿç©ºé—´**ï¼š
  - VLM-based agentsï¼šBEV å›¾åƒ (192Ã—192Ã—3) + æ–‡æœ¬æç¤ºã€‚
  - RL-based agentsï¼šBEV Masks (96Ã—96Ã—15) + ç»“æ„åŒ–çŠ¶æ€ï¼ˆé€Ÿåº¦ã€å‘½ä»¤ç­‰ï¼‰ã€‚
- **åŠ¨ä½œç©ºé—´**ï¼šè¿ç»­æ§åˆ¶ï¼ˆthrottle/brake, steerï¼‰ã€‚
- **è®­ç»ƒæ¨¡å¼**ï¼šOff-policy actor-critic æ¡†æ¶ï¼ˆSAC, DrQv2, TD3ï¼‰ã€‚

### **è¯„ä¼°æŒ‡æ ‡**

åˆ†ä¸ºå››å¤§ç±»ï¼š

| ç±»åˆ« | æŒ‡æ ‡ |
|------|------|
| **Comprehensive** | Return, Driving Score, Infraction Penalty |
| **Route** | Success Rate, Route Completion, Speed |
| **Energy** | Icellï¼ˆç”µæ± ç”µæµä¼°è®¡ï¼‰, Fuel Rate |
| **Safety** | Collisions (Ped./Veh.), Red Light Violations |

> æ³¨ï¼šè¯„ä¼°é‡‡ç”¨æ›´ä¸¥æ ¼çš„ç»ˆæ­¢é€»è¾‘ï¼ˆå¦‚è½»å¾®åç¦»å³ç»ˆæ­¢ï¼‰ï¼Œç¡®ä¿ç­–ç•¥é²æ£’æ€§ã€‚

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

#### ï¼ˆ1ï¼‰**Online RL Baselines**
- TD3, SAC, DrQv2ï¼ˆæ ‡å‡† RL ç®—æ³•ï¼‰

#### ï¼ˆ2ï¼‰**VLM Baselines**
- InternVL3-1B/2B, Qwen2.5-vl-3B/7B, Visual RWKV-0.1Bï¼ˆå‡ä¸º 192Ã—192 è¾“å…¥ï¼‰

#### ï¼ˆ3ï¼‰**å…¶ä»– AD æ–¹æ³•**
- NEAT, LAV, TransFuser ç­‰ï¼ˆåŸºäº RGB/LiDARï¼‰
- BC, AWAC, CQL, IQLï¼ˆIL / offline RLï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆLeaderboard Benchmarkï¼‰**

| æ–¹æ³• | Driving Score â†‘ | Success Rate â†‘ | Red Light â†“ |
|------|------------------|----------------|-------------|
| **DrQv2-CLIP (Ours)** | **0.77** | **0.57** | **0.01** |
| DrQv2 | 0.56 | 0.38 | 0.02 |
| SAC | 0.21 | 0.03 | 0.05 |
| Qwen2.5-vl-7B | 0.76 | 0.65 | 0.05 |
| TransFuser | 0.61 | 0.87 | 0.71 |

> âœ… **Found-RL åœ¨é©¾é©¶å¾—åˆ†ä¸Šè¾¾åˆ° SOTAï¼ŒåŒæ—¶çº¢ç¯è¿è§„è¿œä½äºå¤§å¤šæ•°åŸºçº¿ã€‚**

### **NoCrash Benchmark æ€§èƒ½ï¼ˆTown02, Unseenï¼‰**

| æ–¹æ³• | Driving Score â†‘ | Success Rate â†‘ | Vehicle Collisions â†“ |
|------|------------------|----------------|------------------------|
| **DrQv2-CLIP (Ours)** | **0.76** | **0.71** | **0.73/km** |
| DrQv2 | 0.73 | 0.77 | 1.27/km |
| Qwen2.5-vl-7B | 0.89 | 0.83 | 0.07/km |
| SAC | 0.24 | 0.07 | 2.44/km |

> âœ… **SAC-VMR å°†æˆåŠŸç‡ä» 7% æå‡è‡³ 47%ï¼›DrQv2-CLIP åœ¨æœªçŸ¥åŸé•‡ä¿æŒé«˜æ³›åŒ–èƒ½åŠ›ã€‚**

### **ä¸ VLM çš„å¯¹æ¯”ï¼ˆæ¨¡å‹å¤§å°ä¸æ•ˆç‡ï¼‰**

| æ–¹æ³• | Model Size | Input Size | FPS | Driving Score |
|------|------------|------------|-----|---------------|
| **Found-RL (DrQv2-CLIP)** | **3.82M** | 96Ã—96 | **500** | 0.77 |
| Qwen2.5-vl-7B | 7B | 192Ã—192 | 0.89 | 0.76 |
| InternVL3-1B | 1B | 192Ã—192 | 1.26 | 0.63 |

> âœ… **æ¨¡å‹ç¼©å° 260â€“1800 å€ï¼Œè¾“å…¥åˆ†è¾¨ç‡é™ä½ 75%ï¼Œæ¨ç†é€Ÿåº¦æå‡ 500 å€ä»¥ä¸Šï¼Œæ€§èƒ½æŒå¹³ç”šè‡³åè¶…ã€‚**

### **æ¶ˆèå®éªŒç»“æœ**

- **VMR/AWAG æœ‰æ•ˆæ€§**ï¼š
  - åŠ å…¥ VMR åï¼ŒSAC çš„ Driving Score ä» 0.21 â†’ 0.53ï¼ˆLeaderboardï¼‰ã€‚
  - AWAG æ˜¾è‘—æå‡æ—©æœŸå­¦ä¹ æ•ˆç‡ï¼Œå°¤å…¶åœ¨å¼±åŸºçº¿ä¸Šæ•ˆæœæ˜æ˜¾ã€‚
- **CLIP å¥–åŠ±å¡‘å½¢ä½œç”¨**ï¼š
  - DrQv2-CLIP ç›¸æ¯” DrQv2ï¼ŒSuccess Rate æå‡ 19%ï¼ŒVehicle Collisions ä¸‹é™ 26%ã€‚
  - æ¶ˆèæ˜¾ç¤ºç§»é™¤ CLIP å¥–åŠ±ä¼šå¯¼è‡´æ”¶æ•›å˜æ…¢ã€å®‰å…¨æ€§ä¸‹é™ã€‚
- **å¼‚æ­¥æ¨ç†æ•ˆç‡**ï¼š
  - å¥–åŠ±å¯ç”¨æ€§ç»´æŒåœ¨ **>99.8%**ï¼ŒéªŒè¯äº†é«˜ååæ‰¹å¤„ç†çš„æœ‰æ•ˆæ€§ï¼ˆè§ Fig. 10ï¼‰ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. âœ… **è¯­ä¹‰å…ˆéªŒå¯é«˜æ•ˆè’¸é¦è¿›è½»é‡ RL æ¨¡å‹**  
   å³ä½¿æ˜¯ç™¾ä¸‡çº§å‚æ•°çš„ RL ç­–ç•¥ï¼Œä¹Ÿèƒ½é€šè¿‡ VLM çš„ç›‘ç£è¾¾åˆ°åäº¿çº§ VLM çš„é©¾é©¶æ°´å¹³ã€‚

2. âœ… **å¼‚æ­¥æ‰¹å¤„ç†æ˜¯å®ç”¨åŒ–å…³é”®**  
   è§£è€¦ VLM æ¨ç†ä¸ä»¿çœŸå¾ªç¯ï¼Œä½¿å¾—é«˜å»¶è¿Ÿæ¨¡å‹ä»å¯ç”¨äºå®æ—¶è®­ç»ƒï¼Œçªç ´å·¥ç¨‹ç“¶é¢ˆã€‚

3. âœ… **ç¨ å¯†è¯­ä¹‰å¥–åŠ±æ˜¾è‘—æå‡æ ·æœ¬æ•ˆç‡ä¸å®‰å…¨æ€§**  
   CLIP-based reward shaping æä¾›ç»†ç²’åº¦åé¦ˆï¼ŒåŠ é€Ÿç­–ç•¥æ”¶æ•›å¹¶å¢å¼ºåˆè§„æ€§ã€‚

4. âœ… **è½»é‡æ¨¡å‹åœ¨è§„åˆ™éµå®ˆæ–¹é¢æ›´å…·ä¼˜åŠ¿**  
   å°½ç®¡å¤§ VLM åœ¨ä¸Šé™æ€§èƒ½ä¸Šæ›´å¼ºï¼Œä½† Found-RL åœ¨çº¢ç¯è¿è§„ç­‰é™æ€è§„åˆ™ä¸Šè¡¨ç°æ›´ä¼˜ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

- **è§†è§‰è¾“å…¥å‹ç¼©å¯èƒ½æŸå¤±ç»†èŠ‚**ï¼šä½¿ç”¨ 96Ã—96 BEV Mask å¯èƒ½åœ¨æç«¯å¤æ‚åŠ¨æ€åœºæ™¯ä¸‹ä¸å¦‚åŸå§‹å›¾åƒé²æ£’ã€‚
- **VLM åé¦ˆå¯èƒ½å­˜åœ¨åå·®**ï¼šè‹¥å¾®è°ƒæ•°æ®å­˜åœ¨ç³»ç»Ÿæ€§é”™è¯¯ï¼Œå¯èƒ½è¯¯å¯¼ RL ç­–ç•¥ã€‚
- **å½“å‰ä¾èµ–é¢„å®šä¹‰åŠ¨ä½œç©ºé—´**ï¼šCLIP å¥–åŠ±éœ€ç¦»æ•£åŒ–åŠ¨ä½œï¼Œé™åˆ¶äº†å®Œå…¨ç«¯åˆ°ç«¯çš„è¿ç»­æ§åˆ¶æ½œåŠ›ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **è¿›ä¸€æ­¥ä¼˜åŒ–æ¨ç†å»¶è¿Ÿ**ï¼šæ¢ç´¢ VLM é‡åŒ–ã€è’¸é¦ã€ç¡¬ä»¶åŠ é€Ÿä»¥æ”¯æŒæ›´å¤§æ¨¡å‹ã€‚
2. **æŒ‘æˆ˜æ›´é«˜éš¾åº¦åŸºå‡†**ï¼šåœ¨ CARLA Leaderboard 2.0 æˆ–çœŸå®ä¸–ç•Œæ•°æ®ä¸ŠéªŒè¯æ³›åŒ–æ€§ã€‚
3. **å¤š VLM ååŒæŒ‡å¯¼**ï¼šæ„å»ºå¤šä¸“å®¶ç³»ç»Ÿï¼Œä¸åŒ VLM åˆ†åˆ«æä¾›å®‰å…¨æ€§ã€èˆ’é€‚æ€§ã€æ•ˆç‡ç­‰æ–¹é¢çš„åé¦ˆã€‚
4. **è½»é‡ VLM ä½œä¸ºç­–ç•¥ä¸»å¹²**ï¼šå°è¯•å°†å°å‹ VLM ç›´æ¥ç”¨äº end-to-end è§„åˆ’ï¼Œè€Œéä»…ä½œè¾…åŠ©ç›‘ç£ã€‚

---

> ğŸ”— **ä»£ç ã€æ•°æ®ä¸æ¨¡å‹å·²å¼€æº**ï¼šhttps://github.com/ys-qu/found-rl

</details>

---

### 4. [How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning](https://arxiv.org/abs/2602.10622)

**Authors**: Jiahao Yuan, Yike Xu, Jinyong Wen, Baokun Wang, Yang Chen, Xiaotong Lin, Wuliang Huang, Ziyi Gao, Xing Fu, Yu Cheng, Weiqiang Wang  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.10622v1  

#### Abstract
Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention ma...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç°æœ‰çš„ **decoder-only LLMs** è™½ç„¶åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä½œä¸º **user representation learning** çš„ç¼–ç å™¨æ—¶ï¼Œå…¶æ ‡å‡†çš„ **causal attention masking** é™åˆ¶äº†æ¨¡å‹å¯¹ç”¨æˆ·è¡Œä¸ºåºåˆ—ä¸­é•¿è·ç¦»ä¾èµ–å’Œæœªæ¥ä¸Šä¸‹æ–‡çš„å»ºæ¨¡èƒ½åŠ›ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶å°è¯•å¼•å…¥ **bidirectional attention** æˆ– **hybrid masking**ï¼Œä½†ç¼ºä¹ç³»ç»Ÿæ€§çš„æ¯”è¾ƒï¼Œä¸”ä»å› æœæ³¨æ„åŠ›å‘åŒå‘æ³¨æ„åŠ›è¿‡æ¸¡çš„è®­ç»ƒåŠ¨æ€æœªè¢«å……åˆ†æ¢ç´¢ã€‚

æœ¬æ–‡æ—¨åœ¨è§£å†³ä»¥ä¸‹æ ¸å¿ƒé—®é¢˜ï¼š
- ä¸åŒ attention masking ç­–ç•¥ï¼ˆcausalã€hybridã€bidirectionalï¼‰å¦‚ä½•å½±å“ decoder-only LLM åœ¨ç”¨æˆ·è¡¨å¾å­¦ä¹ ä¸­çš„è¡¨ç°ï¼Ÿ
- å¦‚ä½•å¹³ç¨³åœ°å®ç°ä» causal åˆ° bidirectional attention çš„è®­ç»ƒè¿‡æ¸¡ï¼Œä»¥æå‡ä¼˜åŒ–ç¨³å®šæ€§å’Œæœ€ç»ˆè¡¨å¾è´¨é‡ï¼Ÿ

### æå‡ºçš„æ–°æ–¹æ³•ï¼šGradient-Guided Soft Masking (GG-SM)

ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæœºåˆ¶â€”â€”**Gradient-Guided Soft Masking (GG-SM)**ï¼Œç”¨äºæ”¹å–„ä» causal åˆ° bidirectional attention çš„è¿‡æ¸¡è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

1. **Gradient-Guided Warmup Phase**  
   åœ¨è®­ç»ƒåˆæœŸï¼ˆå‰ $ T_{\text{warm}} $ æ­¥ï¼‰ï¼Œé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹éšè—çŠ¶æ€çš„æ¢¯åº¦èŒƒæ•° $\| \nabla_{h_i} \mathcal{L} \|$ æ¥åŠ¨æ€å†³å®šæœªæ¥ token çš„å¯è§ç¨‹åº¦ã€‚æ¢¯åº¦è¶Šå¤§ï¼Œè¯´æ˜è¯¥ token å¯¹å½“å‰è¡¨ç¤ºè¶Šé‡è¦ï¼Œå› æ­¤èµ‹äºˆæ›´é«˜çš„â€œè½¯æ©ç æƒé‡â€ã€‚è¿™ä¸€é˜¶æ®µæ— éœ€é¢å¤–å‚æ•°ï¼Œå®Œå…¨åŸºäºæ•°æ®é©±åŠ¨ã€‚

2. **Linear Scheduler Phase**  
   åœ¨ warmup ç»“æŸåï¼Œå°†å¾—åˆ°çš„ soft mask å›ºå®šï¼Œå¹¶çº¿æ€§æ’å€¼åˆ°å…¨åŒå‘ attentionï¼ˆå³é€æ¸æ‰“å¼€æ‰€æœ‰ future token çš„è®¿é—®æƒé™ï¼‰ï¼Œç›´åˆ°è®­ç»ƒç»“æŸã€‚

æœ€ç»ˆæ¨ç†æ—¶ä½¿ç”¨ **fully bidirectional attention**ï¼Œæœ€å¤§åŒ–ä¸Šä¸‹æ–‡æ•´åˆèƒ½åŠ›ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | ç¼ºé™· | GG-SM çš„æ”¹è¿› |
|------|------|---------------|
| **Causal Masking** | ä»…èƒ½åˆ©ç”¨å†å²ä¿¡æ¯ï¼Œæ— æ³•æ•æ‰æœªæ¥è¡Œä¸ºçº¿ç´¢ | å…è®¸æ¨¡å‹é€æ­¥å­¦ä¹ å…¨å±€ä¾èµ– |
| **Hybrid Masking** | æ¶æ„å¤æ‚ï¼Œéœ€å¼•å…¥é¢å¤–æ¨¡å—ï¼ˆå¦‚ MLPã€global queryï¼‰ | æ— é¢å¤–å‚æ•°ï¼Œçº¯åŸºäºæ¢¯åº¦ä¿¡å· |
| **Scheduler-only** | çº¿æ€§è°ƒåº¦æ˜¯å›ºå®šçš„ï¼Œä¸è€ƒè™‘æ ·æœ¬æˆ– token çº§åˆ«çš„è¯­ä¹‰é‡è¦æ€§ | å¼•å…¥æ•°æ®è‡ªé€‚åº”çš„ warmupï¼Œæ›´å¹³æ»‘ç¨³å®š |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼šGG-SM åœ¨ä¿æŒä¸ decoder é¢„è®­ç»ƒå…¼å®¹çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº† bidirectional è¡¨å¾çš„è´¨é‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†

å®éªŒåŸºäºå¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®ï¼Œæ¥è‡ª **Alipay ç”¨æˆ·è¡Œä¸ºæ—¥å¿—**ï¼Œæ„å»ºä¸¤ç±»å¯¹é½è®­ç»ƒæ•°æ®ï¼š

1. **Rule-based Behavioral Trajectories Dataset ($ D_{\text{behavior}} $)**  
   - è¾“å…¥ï¼šè¿‡å» 90 å¤©å¤šæ¨¡æ€ç”¨æˆ·è¡Œä¸ºåºåˆ—ï¼ˆPayBillã€Mini Programã€SPMã€Appã€Searchã€Tabularï¼‰
   - è¾“å‡ºï¼šæœªæ¥ä¸€ä¸ªæœˆå†…çš„èšåˆè¡Œä¸ºä½œä¸ºé¢„æµ‹ç›®æ ‡
   - æ„é€ æ–¹å¼ï¼šè§„åˆ™å¯¹é½ + éšæœºé‡‡æ ·å­é›†ä½œä¸ºæ­£ä¾‹

2. **LLM-Synthesized Query-Answer Alignments Dataset ($ D_{\text{qa}} $)**  
   - ä½¿ç”¨ **Qwen-Max** è‡ªåŠ¨ç”Ÿæˆç”¨æˆ·ç†è§£ç›¸å…³çš„ QA å¯¹
   - æµç¨‹ï¼š
     1. ä»ç§å­æ± ä¸­æ£€ç´¢ä¸ç”¨æˆ·å†å²ç›¸å…³çš„æƒ…æ™¯
     2. ç”Ÿæˆåˆå§‹ QA å¯¹å¹¶æ„å»ºæ ¡å‡†é›† $ D_c $
     3. ä½¿ç”¨å¼º embedding æ¨¡å‹è¯„ä¼° alignment difficultyï¼ˆ$ S_d = 1 - \text{Sim}(u \oplus q, a) $ï¼‰
     4. ä¿ç•™é«˜éš¾åº¦æ ·æœ¬ï¼ˆ$ S_d \geq T_{\text{filter}} $ï¼‰
     5. åº”ç”¨ inductive feature completion æå–å…±æ€§è§„åˆ™ä¼˜åŒ– prompt
     6. æ‰©å±•ç”Ÿæˆé«˜è´¨é‡ hard positive æ ·æœ¬

> ğŸ’¡ è¿™ç§åˆæˆç­–ç•¥é¿å…äº†å®æ—¶è´Ÿæ ·æœ¬æŒ–æ˜çš„æˆæœ¬ï¼Œæå‡äº†è®­ç»ƒæ•°æ®çš„ä¿¡æ¯å¯†åº¦ã€‚

### å®éªŒè®¾ç½®

- **æ¨¡å‹æ¶æ„**ï¼š
  - ä¸»å¹² LLMï¼š`Qwen2.5-0.5B-Instruct`ï¼ˆdecoder-onlyï¼‰
  - æ¨¡æ€ç¼–ç å™¨ï¼š`gte-base-zh` åˆ†åˆ«å¤„ç†å„è¡Œä¸ºæ¨¡æ€
  - æŠ•å½±é€‚é…å™¨ï¼šLoRAï¼ˆrank=64, Î±=64ï¼‰
- **è®­ç»ƒç›®æ ‡**ï¼šInfoNCE contrastive loss
  $$
  \mathcal{L}_{\text{cl}} = -\log \frac{\exp(s(u_i, a_i)/\tau)}{\sum_j \exp(s(u_i, a_j)/\tau)}
  $$
  å¹¶å¼•å…¥ mask factor $ m_{ij} $ é˜²æ­¢ false negativesã€‚
- **ç¡¬ä»¶é…ç½®**ï¼š64 Ã— A100-80GB GPUsï¼ˆè®­ç»ƒï¼‰ï¼Œå•å¡æ¨ç†
- **è¶…å‚ç»Ÿä¸€**ï¼šbatch size=2048ï¼ŒAdamWï¼Œlr=2e-4ï¼Œcosine decayï¼Œ7w steps

### è¯„ä¼°æŒ‡æ ‡

- **ä¸‹æ¸¸ä»»åŠ¡**ï¼š9 ä¸ªå·¥ä¸šçº§ binary classification benchmarkï¼Œæ¶µç›–ä¸‰å¤§ç±»ï¼š
  1. **User Prediction**ï¼ˆç™»å½•ã€æµå¤±ã€æ¼”å”±ä¼šç‚¹å‡»ç­‰ï¼‰
  2. **Behavior Preference**ï¼ˆå…¬å…±äº¤é€šåå¥½ã€æ¶ˆè´¹èƒ½åŠ›ã€é¥®é£Ÿå…´è¶£ã€ç”µå½±åå¥½ï¼‰
  3. **Marketing Sensitivity**ï¼ˆæˆå°±å¯¼å‘ã€èº«ä½“ç®¡ç†å€¾å‘ï¼‰
- **è¯„ä»·æŒ‡æ ‡**ï¼šAUCï¼ˆArea Under ROC Curveï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **é€šç”¨ Embedding æ¨¡å‹** | Qwen3-Embedding-0.6B, Llama-embed-nemotron-8B, KaLM-Embedding-Gemma3-12B |
| **ä¼ ç»Ÿç”¨æˆ·å»ºæ¨¡** | MSDP, One4all, CPC |
| **LLM-based ç”¨æˆ·è¡¨å¾** | FOUND, InstructUE |
| **ä¸åŒ masking ç­–ç•¥** | Causal, Hybrid (mask/gq/mlp), Bidirectional (direct/scheduler/GG-SM) |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆå¹³å‡ AUCï¼‰

| æ–¹æ³• | Avg AUC |
|------|---------|
| **Qwen3-Embedding-0.6B** | 0.6720 |
| **Llama-embed-nemotron-8B** | 0.7357 |
| **KaLM-Embedding** | 0.7156 |
| **FOUND (Dou et al.)** | 0.7690 |
| **InstructUE (Gao et al.)** | 0.7728 |
| **Ours (w/ GG-SM)** | **0.7745** âœ… |

> ğŸ“Œ å°½ç®¡ `Qwen2.5-0.5B` å‚æ•°é‡è¿œå°äºå…¶ä»– 8B+ æ¨¡å‹ï¼Œä½†é€šè¿‡ GG-SM å®ç°äº†æœ€ä¼˜æ€§èƒ½ã€‚

### è¯¦ç»†ä»»åŠ¡æ€§èƒ½å¯¹æ¯”ï¼ˆéƒ¨åˆ†å…³é”®é¡¹ï¼‰

| æ–¹æ³• | Concert | Power | Movie | Achiev. | Physical |
|------|--------|-------|-------|--------|----------|
| Causal (Oracle) | 0.5173 | 0.7638 | 0.6435 | 0.5415 | 0.5592 |
| w/Causal | 0.5716 | 0.9678 | 0.7922 | 0.6054 | 0.6589 |
| w/Bidirectional | 0.5707 | 0.9671 | 0.7906 | 0.6043 | 0.6607 |
| w/Scheduler | 0.5742 | 0.9688 | 0.7908 | 0.6056 | 0.6605 |
| **w/GG-SM (Ours)** | **0.5767** | **0.9689** | **0.7913** | **0.6078** | **0.6615** |

> âœ… GG-SM åœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡ä¸Šå‡å–å¾—æœ€ä½³è¡¨ç°ï¼Œå°¤å…¶åœ¨ç¨€ç–è¡Œä¸ºé¢„æµ‹ï¼ˆConcertï¼‰å’Œæ•æ„Ÿæ„å›¾è¯†åˆ«ï¼ˆPhysicalï¼‰ä¸Šæœ‰æ˜æ˜¾å¢ç›Šã€‚

### æ¶ˆèå®éªŒç»“æœ

#### (1) ä¸åŒ masking ç­–ç•¥å¯¹æ¯”ï¼ˆåŒä¸€ backbone ä¸‹ï¼‰

| æ–¹æ³• | Avg AUC |
|------|---------|
| Causal | 0.7709 |
| Hybrid-mask | 0.7710 |
| Hybrid-gq | 0.7706 |
| Hybrid-mlp | 0.7718 |
| Bidirectional (direct) | 0.7721 |
| Bidirectional + Scheduler | 0.7733 |
| **Bidirectional + GG-SM (Ours)** | **0.7745** |

> ğŸ” å‘ç°ï¼š
> - å•çº¯åˆ‡æ¢ä¸º bidirectional å¯å¸¦æ¥å¢ç›Šï¼›
> - åŠ å…¥ scheduler åè¿›ä¸€æ­¥æå‡ï¼›
> - **GG-SM åœ¨æ­¤åŸºç¡€ä¸Šä»èƒ½æŒç»­å¢ç›Š 0.12% AUC**ï¼Œè¯æ˜ warmup é˜¶æ®µçš„é‡è¦æ€§ã€‚

#### (2) è®­ç»ƒç¨³å®šæ€§åˆ†æï¼ˆFig. 3ï¼‰

- **GG-SM æ”¶æ•›æ›´å¿«ã€æ³¢åŠ¨æ›´å°**
- Scheduler æ–¹æ³•åœ¨ transition é˜¶æ®µå‡ºç°æ˜æ˜¾ loss spike
- GG-SM åˆ©ç”¨æ¢¯åº¦å¼•å¯¼ï¼Œä½¿æ¨¡å‹â€œæœ‰å‡†å¤‡â€åœ°æ¥å—æœªæ¥ä¿¡æ¯ï¼Œé¿å… abrupt change

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. **Attention masking è®¾è®¡è‡³å…³é‡è¦**  
   å¯¹äº decoder-only LLM ç”¨äº user representation learningï¼Œä¸èƒ½ç®€å•æ²¿ç”¨ causal maskingï¼›**bidirectional attention æ›´é€‚åˆ sentence-level è¡¨å¾æå–**ã€‚

2. **è¿‡æ¸¡è·¯å¾„æ¯”æœ€ç»ˆ mask æ›´å…³é”®**  
   ä» causal åˆ° bidirectional çš„è®­ç»ƒåŠ¨æ€ç›´æ¥å½±å“ä¼˜åŒ–ç¨³å®šæ€§å’Œæœ€ç»ˆæ€§èƒ½ã€‚**ç›´æ¥è·³è½¬ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œè€Œæ¸è¿›å¼å¼€æ”¾éœ€é…åˆæ•°æ®æ„ŸçŸ¥æœºåˆ¶**ã€‚

3. **GG-SM æ˜¾è‘—æå‡è¡¨å¾è´¨é‡å’Œè®­ç»ƒç¨³å®šæ€§**  
   é€šè¿‡æ¢¯åº¦æŒ‡å¯¼çš„ soft masking warmupï¼Œå®ç°äº†æ›´å¹³æ»‘çš„æ³¨æ„åŠ›æ¼”åŒ–ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä¼˜å…ˆå…³æ³¨å¯¹ä»»åŠ¡æ›´é‡è¦çš„ future tokensã€‚

4. **å°æ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šå¤§æ¨¡å‹**  
   å°½ç®¡ä»…ä½¿ç”¨ 0.5B å‚æ•°çš„ Qwen2.5ï¼Œä½†é€šè¿‡é¢†åŸŸé€‚é…çš„è®­ç»ƒç­–ç•¥ï¼ˆGG-SM + åˆæˆ hard positiveï¼‰ï¼Œæ€§èƒ½è¶…è¿‡å¤šä¸ª 8B+ çš„é€šç”¨ embedding æ¨¡å‹ï¼Œè¡¨æ˜ **parameter scale ä¸ç­‰äºæ•ˆæœä¸Šé™ï¼Œdomain alignment æ›´å…³é”®**ã€‚

5. **Hybrid masking æ˜¯æœ‰æ½œåŠ›çš„æ–¹å‘**  
   å°½ç®¡æœ¬æ–‡ focus on GG-SMï¼Œä½†ä¹ŸéªŒè¯äº† hybrid maskingï¼ˆå¦‚ global queryï¼‰å…·æœ‰ä¸€å®šç«äº‰åŠ›ï¼Œå€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢ã€‚

### æ–¹æ³•çš„å±€é™æ€§

- **ä¾èµ–é«˜è´¨é‡åˆæˆæ•°æ®**ï¼šè™½ç„¶æå‡ºäº†å…è®­ç»ƒçš„æ•°æ®å¢å¼ºæµç¨‹ï¼Œä½†ä»éœ€ä¸€ä¸ªå¼ºå¤§çš„ base LLMï¼ˆå¦‚ Qwen-Maxï¼‰æ¥ç”Ÿæˆåˆå§‹ QA å¯¹ã€‚
- **è®¡ç®—å¼€é”€å¢åŠ **ï¼šgradient norm è®¡ç®—å¢åŠ äº†æ¯æ­¥è®­ç»ƒæˆæœ¬ï¼ˆçº¦ +15%ï¼‰ï¼Œä¸é€‚åˆæä½å»¶è¿Ÿåœºæ™¯ã€‚
- **ä»…é€‚ç”¨äº contrastive æ¡†æ¶**ï¼šç›®å‰è®¾è®¡é’ˆå¯¹ dual-tower alignmentï¼Œæ˜¯å¦å¯æ¨å¹¿è‡³ generation-based ç”¨æˆ·å»ºæ¨¡æœ‰å¾…éªŒè¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘

- æ¢ç´¢ **æ›´é«˜æ•ˆçš„ gradient approximation** æ–¹æ¡ˆä»¥é™ä½è®­ç»ƒæˆæœ¬
- å°† GG-SM æ€æƒ³æ‰©å±•è‡³ **multi-modal sequence modeling**ï¼ˆå¦‚è§†é¢‘ã€è¯­éŸ³ï¼‰
- ç ”ç©¶ **online adaptation ç‰ˆæœ¬**ï¼Œæ”¯æŒ streaming user behavior æ›´æ–°
- æ¢ç´¢ **hybrid masking + GG-SM çš„ç»„åˆç­–ç•¥**ï¼Œå…¼é¡¾ç”Ÿæˆèƒ½åŠ›å’Œè¡¨å¾å®Œæ•´æ€§

---

> ğŸ”— **ä»£ç å¼€æºåœ°å€**ï¼š[https://github.com/JhCircle/Deepfind-GGSM](https://github.com/JhCircle/Deepfind-GGSM)

</details>

---

### 5. [BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.10729)

**Authors**: Youhe Jiang, Fangcheng Fu, Eiko Yoneki  
**Category**: cs.DC  
**Published**: 2026-02-12  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.10729v1  

#### Abstract
The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**BOUTE: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡ç³»ç»Ÿé¢ä¸´é«˜æ˜‚çš„éƒ¨ç½²æˆæœ¬ï¼Œå°¤å…¶æ˜¯åœ¨æ»¡è¶³ä½å»¶è¿Ÿå’Œé«˜è´¨é‡è¾“å‡ºè¦æ±‚æ—¶ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»ä¸¤ä¸ªç‹¬ç«‹è§’åº¦ä¼˜åŒ–ï¼š
- **ç®—æ³•å±‚é¢**ï¼šé€šè¿‡ **heterogeneous model routing** å°†ç®€å•æŸ¥è¯¢è·¯ç”±åˆ°å°æ¨¡å‹ï¼Œå¤æ‚æŸ¥è¯¢äº¤ç»™å¤§æ¨¡å‹ã€‚
- **ç³»ç»Ÿå±‚é¢**ï¼šé€šè¿‡ **heterogeneous model deployment** åˆ©ç”¨ä¸åŒç±»å‹çš„GPUï¼ˆå¦‚H100 vs RTX 5090ï¼‰æ„å»ºæ›´å…·æ€§ä»·æ¯”çš„æœåŠ¡æ¶æ„ã€‚

ç„¶è€Œï¼Œè¿™ä¸¤ä¸ªå†³ç­–ä¹‹é—´å­˜åœ¨å¼ºè€¦åˆå…³ç³»ï¼š
- è·¯ç”±ç­–ç•¥å½±å“å„æ¨¡å‹çš„è´Ÿè½½åˆ†å¸ƒï¼Œä»è€Œå½±å“æœ€ä¼˜éƒ¨ç½²ï¼›
- éƒ¨ç½²é…ç½®å†³å®šæ¯ä¸ªæ¨¡å‹çš„å“åº”å»¶è¿Ÿï¼Œè¿›è€Œå½±å“è·¯ç”±é€‰æ‹©ã€‚

å› æ­¤ï¼Œ**å­¤ç«‹ä¼˜åŒ–ä¼šå¯¼è‡´æ¬¡ä¼˜è§£**ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€**ç®—æ³•-ç³»ç»ŸååŒè®¾è®¡ï¼ˆco-designï¼‰éš¾é¢˜**ï¼Œå®ç°ç«¯åˆ°ç«¯çš„æˆæœ¬é«˜æ•ˆLLMæœåŠ¡ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

ä½œè€…æå‡º **BOUTE** â€”â€” ä¸€ä¸ªåŸºäº **Multi-Objective Bayesian Optimization (MOBO)** çš„è´¨é‡æ„ŸçŸ¥è°ƒåº¦ç³»ç»Ÿï¼Œè”åˆä¼˜åŒ–ä»¥ä¸‹ä¸¤ä¸ªç»´åº¦ï¼š

1. **Routing Strategy**  
   - ä½¿ç”¨é˜ˆå€¼æœºåˆ¶å°†æŸ¥è¯¢åŠ¨æ€åˆ†é…ç»™ä¸åŒè§„æ¨¡çš„LLMï¼ˆå¦‚ Llama3.1-8B å’Œ Llama3.1-70Bï¼‰ã€‚
   - å†³ç­–å˜é‡ä¸ºè·¯ç”±é˜ˆå€¼ `T`ï¼Œæ§åˆ¶è´Ÿè½½åœ¨æ¨¡å‹é—´çš„åˆ†å¸ƒã€‚

2. **Model Deployment Strategy**  
   - åœ¨å¼‚æ„GPUé›†ç¾¤ä¸Šè¿›è¡Œèµ„æºåˆ†é…ï¼ˆå¦‚ H100ã€RTX 5090ã€RTX 4090 ç­‰ï¼‰ï¼ŒåŒ…æ‹¬ï¼š
     - GPUåˆ†é…çŸ©é˜µ `A`
     - å¹¶è¡Œç­–ç•¥ï¼ˆData Parallelism, Tensor Parallelism, Pipeline Parallelismï¼‰
   - æ”¯æŒæ··åˆéƒ¨ç½²ï¼ˆhomogeneous + heterogeneous GPU typesï¼‰

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
| åˆ›æ–° | æè¿° |
|------|------|
| **è”åˆä¼˜åŒ–æ¡†æ¶** | é¦–æ¬¡å°† routing ä¸ deployment å»ºæ¨¡ä¸ºç»Ÿä¸€çš„å¤šç›®æ ‡çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œç›®æ ‡æ˜¯ **æœ€å°åŒ–P95å»¶è¿Ÿ** åŒæ—¶ **æœ€å¤§åŒ–å“åº”è´¨é‡ï¼ˆaccuracy/scoreï¼‰**ã€‚ |
| **MOBO æ¡†æ¶è®¾è®¡** | å¼•å…¥ **Multi-Objective Bayesian Optimization (MOBO)** æ¥é«˜æ•ˆæœç´¢é«˜ç»´ã€éå‡¸çš„å†³ç­–ç©ºé—´ï¼Œé¿å…ç©·ä¸¾ã€‚ |
| **ç»“æ„åŒ–å…ˆéªŒæ³¨å…¥ï¼ˆStructural Informationï¼‰** | åœ¨GPæ¨¡å‹ä¸­å¼•å…¥å¤šç§ç»“æ„ä¿¡æ¯æå‡æ”¶æ•›é€Ÿåº¦ä¸æ•ˆæœï¼š<br>â€¢ Load-fraction encodingï¼ˆè´Ÿè½½æ¯”ä¾‹ç¼–ç ï¼‰<br>â€¢ Budget/GPU availability constraints<br>â€¢ Model-GPUåå¥½ç¼–ç ï¼ˆPreference-weighted kernelï¼‰<br>â€¢ Objective normalizationï¼ˆlog/logitå˜æ¢ï¼‰ |
| **ä¸¤é˜¶æ®µæ¨¡æ‹Ÿå™¨ï¼ˆOffline Preparationï¼‰** | æ„å»ºæ€§èƒ½æ•°æ®åº“ï¼Œé¢„ä¼°ä»»æ„ `(deployment, load)` ç»„åˆä¸‹çš„å»¶è¿Ÿï¼Œæ˜¾è‘—é™ä½åœ¨çº¿ä¼˜åŒ–å¼€é”€ã€‚ |

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| å¯¹æ¯”ç»´åº¦ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ RouteLLMï¼‰ | BOUTE |
|--------|--------------------------|-------|
| ä¼˜åŒ–ç²’åº¦ | ä»…ä¼˜åŒ– routing æˆ– deployment å•ç‹¬ä¸€æ–¹ | è”åˆä¼˜åŒ– routing + deployment |
| èµ„æºåˆ©ç”¨ | å‡åŒ€èµ„æºåˆ†é…ï¼ˆuniform allocationï¼‰ | è‡ªé€‚åº”èµ„æºåˆ†é…ï¼ŒåŒ¹é…æ¨¡å‹éœ€æ±‚ |
| GPUå¼‚æ„æ€§ | å¿½ç•¥ä¸åŒGPUå¯¹æ¨¡å‹çš„é€‚é…å·®å¼‚ | æ˜¾å¼å»ºæ¨¡ Model-GPU suitabilityï¼ˆå¦‚ RTX æ›´é€‚åˆå°æ¨¡å‹ï¼‰ |
| æœç´¢æ•ˆç‡ | è§„åˆ™å¯å‘å¼æˆ–æ‰‹åŠ¨è°ƒå‚ | MOBO + ç»“æ„ä¿¡æ¯ â†’ å¿«é€Ÿæ”¶æ•›è‡³ Pareto æœ€ä¼˜è§£ |
| æˆæœ¬æ•ˆç›Š | å±€éƒ¨æœ€ä¼˜ | å…¨å±€æ›´ä¼˜ï¼Œå¹³å‡èŠ‚çœæˆæœ¬ **38%** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†ä¸å·¥ä½œè´Ÿè½½
- **GSM8K**ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼° **accuracy**ï¼ˆèšåˆå‡†ç¡®ç‡ï¼‰
- **MTBench**ï¼šå¤šè½®å¯¹è¯ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼° **score**ï¼ˆäººå·¥è¯„åˆ†èšåˆå¾—åˆ†ï¼‰
- æŸ¥è¯¢ trace é‡‡æ ·è‡ªçœŸå®æ•°æ®åˆ†å¸ƒï¼Œå¹¶æŒ‰å¤æ‚åº¦åŠ æƒç”Ÿæˆå­æ ·æœ¬ï¼ˆsubsampled traceï¼‰

### âš™ï¸ å®éªŒè®¾ç½®
- **ç¡¬ä»¶ç¯å¢ƒ**ï¼š
  - H100-80G ($2.64â€“3.07/h)
  - RTX PRO 6000-96G ($1.84â€“2.09/h)
  - RTX 5090-32G ($0.89/h)
  - RTX 4090-24G ($0.59/h)
- **é¢„ç®—é™åˆ¶**ï¼šå›ºå®šæ¯å°æ—¶ $30 æˆæœ¬ä¸Šé™
- **æ¨¡å‹ç»„åˆ**ï¼šLlama3.1-8Bï¼ˆè½»é‡ï¼‰ã€Llama3.1-70Bï¼ˆé‡é‡ï¼‰
- **ç³»ç»Ÿè´Ÿè½½**ï¼šæ€»è¯·æ±‚é€Ÿç‡ 100 req/s

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **P95 Latency** | 95% è¯·æ±‚å®Œæˆæ—¶é—´çš„æœ€å¤§å€¼ |
| **Throughput** | ç³»ç»Ÿæ•´ä½“ååé‡ï¼ˆqueries/secï¼‰ |
| **Response Quality** | GSM8K ä¸Šçš„ accuracy / MTBench ä¸Šçš„ score |
| **Cost Efficiency** | è¾¾æˆç›¸åŒæ€§èƒ½ç›®æ ‡æ‰€éœ€çš„æœ€ä½æˆæœ¬ |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿ | æè¿° |
|-----|------|
| **Stand-Alone-Homo** | ä½¿ç”¨ vLLM éƒ¨ç½²å•ä¸€å¤§æ¨¡å‹ï¼ˆ70Bï¼‰äºåŒæ„H100é›†ç¾¤ |
| **Stand-Alone-Hetero** | åŒä¸Šï¼Œä½†åœ¨å¼‚æ„GPUä¸Šéƒ¨ç½² |
| **RouteLLM** | å½“å‰SOTAè·¯ç”±ç³»ç»Ÿï¼Œé‡‡ç”¨åå¥½æ•°æ®é©±åŠ¨è·¯ç”±ï¼Œä½†èµ„æºå‡åŒ€åˆ†é… |
| **BOUTE-Homo** | BOUTE çš„å˜ä½“ï¼Œä»…ä½¿ç”¨åŒæ„GPUï¼ˆæ¶ˆèå®éªŒå¯¹ç…§ç»„ï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ æ€§èƒ½æå‡ï¼ˆvs. åŸºçº¿ï¼‰

| åœºæ™¯ | æŒ‡æ ‡ | æå‡å¹…åº¦ |
|------|------|---------|
| **GSM8K å·¥ä½œè´Ÿè½½** | P95 Latency â†“ | æœ€å¤š **91%**ï¼ˆå¹³å‡ 57%ï¼‰ |
| | Throughput â†‘ | æœ€å¤š **90%**ï¼ˆå¹³å‡ 55%ï¼‰ |
| **MTBench å·¥ä½œè´Ÿè½½** | Performance Improvement â†‘ | æœ€å¤š **157%**ï¼ˆå¹³å‡ 115% vs Stand-Aloneï¼‰<br>æœ€å¤š **80%**ï¼ˆå¹³å‡ 42% vs RouteLLMï¼‰ |
| **vs BOUTE-Homo** | P95 Latency â†“ | æœ€å¤š **15%**ï¼ˆå¹³å‡ 14%ï¼‰<br>Throughput â†‘ æœ€å¤š **16%**ï¼ˆå¹³å‡ 14%ï¼‰ |

> ğŸ’¡ è¡¨æ˜ï¼š**å¼•å…¥å¼‚æ„GPUæœ¬èº«å°±èƒ½å¸¦æ¥æ˜¾è‘—å¢ç›Š**ï¼Œè€Œ BOUTE è¿›ä¸€æ­¥é€šè¿‡ååŒä¼˜åŒ–æ”¾å¤§è¯¥ä¼˜åŠ¿ã€‚

---

### ğŸ’° æˆæœ¬æ•ˆç‡åˆ†æï¼ˆTable 3ï¼‰

| è´¨é‡+å»¶è¿Ÿè¦æ±‚ | BOUTE æˆæœ¬ | RouteLLM æˆæœ¬ | èŠ‚çœæ¯”ä¾‹ |
|---------------|------------|----------------|----------|
| GSM8K (87, 8s) | $32.25/h | $53.55/h | **39.8%** |
| GSM8K (91, 12s) | $32.18/h | $52.38/h | **38.6%** |
| MTBench (8.1, 10s) | $30.98/h | $55.90/h | **44.6%** |
| MTBench (8.5, 15s) | $31.23/h | $44.00/h | **29.0%** |

âœ… **å¹³å‡èŠ‚çœæˆæœ¬è¾¾ 38%**ï¼Œæœ€é«˜è¾¾ **61%**ï¼ŒåŒæ—¶ä¿æŒç›¸åŒæœåŠ¡è´¨é‡ã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

| å˜ä½“ | æè¿° | å½±å“ |
|------|------|------|
| **BOUTE (w/o structural info)** | ç§»é™¤ load-fraction ç¼–ç ã€output transformã€GPUåå¥½å»ºæ¨¡ç­‰ç»“æ„ä¿¡æ¯ | æ”¶æ•›é€Ÿåº¦ä¸‹é™çº¦ **8å€**ï¼Œéœ€æ›´å¤š BO iterationï¼›æœ€ç»ˆæ€§èƒ½åŠ£äºå®Œæ•´ç‰ˆï¼Œç”šè‡³ä¸å¦‚ BOUTE-Homo |
| **BOUTE (w/o offline prep)** | ä¸ä½¿ç”¨ç¦»çº¿æ€§èƒ½æ•°æ®åº“ï¼Œæ¯æ¬¡è¯„ä¼°éƒ½é‡æ–°è·‘æ¨¡æ‹Ÿ | è°ƒåº¦æ—¶é—´ä» <1min ä¸Šå‡è‡³ **>30åˆ†é’Ÿ**ï¼Œæ— æ³•æ»¡è¶³å®æ—¶éƒ¨ç½²éœ€æ±‚ |
| **BOUTE-Homo** | ä»…ä½¿ç”¨åŒæ„GPUï¼ˆå¦‚å…¨H100ï¼‰ | æ€§èƒ½æ˜æ˜¾ä½äºå®Œæ•´BOUTEï¼Œè¯´æ˜ **heterogeneous GPU æ˜¯å…³é”®å¢ç›Šæ¥æºä¹‹ä¸€** |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **å¼‚æ„æ¨¡å‹è·¯ç”± + å¼‚æ„GPUéƒ¨ç½² å¤©ç„¶äº’è¡¥**  
   - å°æ¨¡å‹æ›´é€‚åˆç»æµå‹GPUï¼ˆå¦‚RTXç³»åˆ—ï¼‰ï¼Œå¤§æ¨¡å‹ä¾èµ–é«˜ç«¯GPUï¼ˆå¦‚H100ï¼‰ã€‚
   - è”åˆä¼˜åŒ–å¯é‡Šæ”¾åŒé‡çº¢åˆ©ï¼š**ä½æˆæœ¬æœåŠ¡å°æ¨¡å‹ + æ‰©å®¹å¤§æ¨¡å‹å®¹é‡**ã€‚

2. **è·¯ç”±ä¸éƒ¨ç½²å¿…é¡»ååŒä¼˜åŒ–**  
   - å­¤ç«‹ä¼˜åŒ–ä»»ä¸€ç¯èŠ‚éƒ½ä¼šå¯¼è‡´ç³»ç»Ÿç“¶é¢ˆï¼ˆå¦‚å¤§æ¨¡å‹è¿‡è½½ï¼‰ã€‚
   - BOUTE é€šè¿‡ MOBO æ‰¾åˆ° **Pareto-optimal è§£é›†**ï¼Œæ”¯æŒçµæ´»æƒè¡¡ latency ä¸ qualityã€‚

3. **ç»“æ„åŒ–å…ˆéªŒæå¤§æå‡ä¼˜åŒ–æ•ˆç‡**  
   - Load-fraction ç¼–ç ç¨³å®šä¼˜åŒ–è¿‡ç¨‹ï¼›
   - Preference-weighted kernel å¼•å¯¼æœç´¢æœé«˜æ€§èƒ½åŒºåŸŸå‰è¿›ï¼›
   - ç¦»çº¿æ¨¡æ‹Ÿå™¨ä½¿åœ¨çº¿è°ƒåº¦å¯åœ¨ **30ç§’å†…å®Œæˆ**ï¼Œå…·å¤‡å®ç”¨ä»·å€¼ã€‚

4. **æˆæœ¬èŠ‚çº¦æ½œåŠ›å·¨å¤§**  
   - åœ¨ç›¸åŒæ€§èƒ½ç›®æ ‡ä¸‹ï¼ŒBOUTE **å¹³å‡å‡å°‘38%æˆæœ¬**ï¼Œæœ€é«˜è¾¾61%ï¼Œæ˜¾è‘—æ¨åŠ¨LLMæœåŠ¡å¹³æ°‘åŒ–ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

| å±€é™ | è¯´æ˜ |
|------|------|
| **å†·å¯åŠ¨å¼€é”€** | ç¦»çº¿å‡†å¤‡é˜¶æ®µéœ€è¦æ•°ååˆ†ç§è¿›è¡Œ profiling å’Œæ¨¡æ‹Ÿï¼Œé€‚ç”¨äºé™æ€åœºæ™¯ï¼Œé¢‘ç¹å˜æ›´æ¨¡å‹æˆ–ç¡¬ä»¶æ—¶éœ€é‡åšã€‚ |
| **æ‰©å±•æ€§è¾¹ç•Œ** | å½“GPUç±»å‹ >6 æˆ–æ¨¡å‹ç§ç±» >3 æ—¶ï¼Œæœç´¢ç©ºé—´æŒ‡æ•°å¢é•¿ï¼Œè™½ä»å¯æ‰©å±•ï¼Œä½†éœ€æ›´å¼ºç®—åŠ›æ”¯æŒã€‚ |
| **ä¾èµ–æ¨¡æ‹Ÿç²¾åº¦** | æ€§èƒ½é¢„æµ‹ä¾èµ–æ¨¡æ‹Ÿå™¨å‡†ç¡®æ€§ï¼ˆè¯¯å·® ~2â€“7%ï¼‰ï¼Œæç«¯æƒ…å†µå¯èƒ½åç¦»çœŸå®è¡¨ç°ã€‚ |
| **è·¯ç”±æ¨¡å‹å‡è®¾ç†æƒ³** | ä½¿ç”¨å·²æœ‰ routerï¼ˆå¦‚ RouteLLMï¼‰ï¼Œæœªè€ƒè™‘å¯¹æŠ—æ”»å‡»æˆ–åˆ†å¸ƒåç§»å¯¹è·¯ç”±ç¨³å®šæ€§çš„å½±å“ã€‚ |

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **åŠ¨æ€è‡ªé€‚åº”è°ƒåº¦**  
   æ”¯æŒè¿è¡Œæ—¶è´Ÿè½½æ¼‚ç§»ã€æ¨¡å‹æ›´æ–°ã€GPUæ•…éšœç­‰åŠ¨æ€äº‹ä»¶ä¸‹çš„åœ¨çº¿é‡ä¼˜åŒ–ã€‚

2. **è·¨äº‘/è¾¹ç¼˜ååŒéƒ¨ç½²**  
   å°† BOUTE æ‹“å±•è‡³ multi-cloud æˆ– edge-cloud ååŒåœºæ™¯ï¼Œè¿›ä¸€æ­¥é™ä½æˆæœ¬ã€‚

3. **å®‰å…¨ä¸é²æ£’æ€§å¢å¼º**  
   æŠµå¾¡ adversarial queries å¯¹ routing çš„è¯¯å¯¼ï¼Œä¿éšœç³»ç»Ÿå¯é æ€§ã€‚

4. **è‡ªåŠ¨åŒ–æ¨¡å‹é€‰å‹**  
   ä¸ä»…ä¼˜åŒ–å·²æœ‰æ¨¡å‹çš„è·¯ç”±ä¸éƒ¨ç½²ï¼Œè¿˜æ”¯æŒè‡ªåŠ¨æ¨èæœ€ä½³æ¨¡å‹ç»„åˆï¼ˆe.g., cascade designï¼‰ã€‚

5. **ç»¿è‰²è®¡ç®—é›†æˆ**  
   åŠ å…¥èƒ½è€—ä½œä¸ºç¬¬ä¸‰ä¼˜åŒ–ç›®æ ‡ï¼Œæ‰“é€  **cost-, latency-, energy-** ä¸‰ç›®æ ‡è”åˆä¼˜åŒ–æ¡†æ¶ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **BOUTE æ˜¯é¦–ä¸ªå°† heterogeneous model routing ä¸ deployment è”åˆå»ºæ¨¡å¹¶é€šè¿‡ MOBO å®ç°é«˜æ•ˆååŒä¼˜åŒ–çš„ç³»ç»Ÿï¼Œåœ¨ä¿è¯è´¨é‡çš„å‰æä¸‹ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ¡ˆæœ€å¤šæå‡ 157% æ€§èƒ½æˆ–èŠ‚çœ 61% æˆæœ¬ï¼Œä¸ºå¤§è§„æ¨¡ LLM æœåŠ¡æä¾›äº†å…¨æ–°çš„ cost-efficient èŒƒå¼ã€‚**

</details>

---

### 6. [PRISM: Parallel Residual Iterative Sequence Model](https://arxiv.org/abs/2602.10796)

**Authors**: Jie Jiang, Ke Cheng, Xin Xu, Mengyang Pang, Tianhao Lu, Jiaheng Li, Yue Liu, Yuan Wang, Jun Zhang, Huan Yu, Zhouchen Lin  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.10796v1  

#### Abstract
Generative sequence modeling faces a fundamental tension between the expressivity of Transformers and the efficiency of linear sequence models. Existing efficient architectures are theoretically bounded by shallow, single-step linear updates, while powerful iterative methods like Test-Time Training ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šPRISM: Parallel Residual Iterative Sequence Model**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
ç”Ÿæˆå¼åºåˆ—å»ºæ¨¡ï¼ˆGenerative sequence modelingï¼‰é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§çš„çŸ›ç›¾ï¼š
- **Transformers** è™½ç„¶è¡¨è¾¾èƒ½åŠ›å¼ºï¼Œä½†å…¶ $O(N^2)$ çš„è®¡ç®—å¤æ‚åº¦åœ¨å¤„ç†è¶…é•¿åºåˆ—æ—¶æ•ˆç‡æä½ã€‚
- **çº¿æ€§åºåˆ—æ¨¡å‹**ï¼ˆå¦‚ Linear Attentionã€SSMsï¼‰è™½ç„¶é«˜æ•ˆï¼ˆ$O(N)$ï¼‰ï¼Œä½†å—é™äºå•æ­¥çº¿æ€§æ›´æ–°æœºåˆ¶ï¼Œå­˜åœ¨â€œ**çº¿æ€§ç“¶é¢ˆ**â€ï¼ˆLinearity Bottleneckï¼‰ï¼Œå³åªèƒ½è¿›è¡Œ Rank-1 æ›´æ–°ï¼Œéš¾ä»¥æ•æ‰é«˜é˜¶ç‰¹å¾ç›¸å…³æ€§å’Œè¿­ä»£ä¼˜åŒ–èƒ½åŠ›ã€‚
- **æ˜¾å¼ä¼˜åŒ–æ–¹æ³•**ï¼ˆå¦‚ Test-Time Training, TTTï¼‰è™½èƒ½å®ç°å¤šæ­¥éçº¿æ€§ä¼˜åŒ–ï¼Œæå‡è¡¨è¾¾åŠ›ï¼Œä½†ç”±äºæ¢¯åº¦ä¾èµ–äºå½“å‰çŠ¶æ€ï¼ˆstate-dependent gradientsï¼‰ï¼Œå¯¼è‡´ä¸²è¡Œä¾èµ–ï¼Œç ´åäº†ç¡¬ä»¶å¹¶è¡Œæ€§ã€‚

PRISM æ­£æ˜¯ä¸ºäº†**è§£å†³è¡¨è¾¾åŠ›ä¸æ•ˆç‡ä¹‹é—´çš„æ ¹æœ¬å¼ åŠ›**è€Œæå‡ºçš„ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

PRISMï¼ˆ**Parallel Residual Iterative Sequence Model**ï¼‰æå‡ºäº†ä¸€ç§**æ‘Šé”€æ®‹å·®ä¼˜åŒ–æ¡†æ¶**ï¼ˆAmortized Residual Optimization Frameworkï¼‰ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

#### **Write-Forget Decouplingï¼ˆå†™å…¥-é—å¿˜è§£è€¦ï¼‰**
- å°†çŠ¶æ€æ›´æ–°åˆ†è§£ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š
  - **é—å¿˜ï¼ˆForgetï¼‰**ï¼šé‡‡ç”¨å›ºå®šçš„ã€é«˜æ•ˆçš„çº¿æ€§è¡°å‡ï¼ˆRank-1ï¼‰ï¼Œä¿è¯ç¨³å®šæ€§ä¸å¹¶è¡Œæ€§ã€‚
  - **å†™å…¥ï¼ˆWriteï¼‰**ï¼šé€šè¿‡ä¸€ä¸ª**é«˜ç§©ï¼ˆRank-Lï¼‰ã€éçº¿æ€§æ³¨å…¥ç®—å­**æ¥æ¨¡æ‹Ÿå¤šæ­¥è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œçªç ´ Rank-1 ç“¶é¢ˆã€‚

#### **Input-Anchored Loop Unrollingï¼ˆè¾“å…¥é”šå®šå¾ªç¯å±•å¼€ï¼‰**
- ä¸ºäº†ç»•è¿‡æ˜¾å¼æ±‚è§£å™¨çš„ä¸²è¡Œä¾èµ–ï¼ŒPRISM å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µä»£ç†æ¶æ„ï¼š
  1. **çŸ­å·ç§¯é”šå®š**ï¼ˆShortConvï¼‰ï¼šåˆ©ç”¨å±€éƒ¨å†å²èƒ½é‡ä¼°è®¡åˆå§‹æ®‹å·®ï¼ˆproxy for $S_{t-1}k_t$ï¼‰ã€‚
  2. **å­¦ä¹ å‹é¢„æµ‹å™¨**ï¼šç›´æ¥ä»è¾“å…¥ä¸­ä¼°è®¡å¤šæ­¥ç»†åŒ–æ›´æ–°ï¼Œå°†éçº¿æ€§è¿­ä»£è¿‡ç¨‹â€œåç¼©â€ä¸ºä¸€ä¸ªå¯å¹¶è¡Œçš„å‰é¦ˆç®—å­ã€‚

è¯¥è®¾è®¡å°†è¿­ä»£ä¼˜åŒ–çš„ç»“æ„å…ˆéªŒï¼ˆstructural inductive biasï¼‰ç¼–ç è¿›æ¨¡å‹ä¸­ï¼Œè€Œéå®æ—¶æ‰§è¡Œã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | PRISM | TTT / TITANS | Linear Models (e.g., DeltaNet) |
|------|-------|---------------|-------------------------------|
| **è¡¨è¾¾åŠ›** | âœ… é«˜ï¼ˆRank-L, éçº¿æ€§ï¼‰ | âœ… æé«˜ï¼ˆæ˜¾å¼å¤šæ­¥ä¼˜åŒ–ï¼‰ | âŒ ä½ï¼ˆRank-1, çº¿æ€§ï¼‰ |
| **å¹¶è¡Œæ€§** | âœ… å®Œå…¨å¹¶è¡Œï¼ˆ$O(N)$ è®­ç»ƒï¼‰ | âŒ ä¸²è¡Œï¼ˆ$O(N)$ æ¨ç†å»¶è¿Ÿï¼‰ | âœ… å®Œå…¨å¹¶è¡Œ |
| **ååé‡** | â¬†ï¸ é«˜è¾¾ 174Ã— TTT | â¬‡ï¸ æä½ | â¬†ï¸ é«˜ |
| **ç†è®ºè¡¨è¾¾æ€§** | âœ… è¯æ˜å…·æœ‰ Rank Accumulation | âœ… æ˜¾å¼ä¼˜åŒ– | âŒ Rank-1 é™åˆ¶ |

> **æ ¸å¿ƒä¼˜åŠ¿**ï¼šåœ¨ä¿æŒ $O(N)$ å¹¶è¡Œè®­ç»ƒæ•ˆç‡çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†ä¸ TTT ç±»æ–¹æ³•ç›¸å½“ç”šè‡³æ›´ä¼˜çš„å»ºæ¨¡ç²¾åº¦ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
åœ¨å››ä¸ªæ¨èç³»ç»ŸåŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä½œä¸ºå¯¹**é«˜ç§©ç¨€ç–æ€§**ï¼ˆHigh-Rank Sparsityï¼‰çš„ä¸¥æ ¼æµ‹è¯•ï¼š
- **Amazon Books**
- **Amazon Movies**
- **Amazon Elecs**
- **Yelp**

è¿™äº›åœºæ™¯ä¸‹ç”¨æˆ·å…´è¶£æ¼”åŒ–é«˜åº¦å¤šæ¨¡æ€ï¼Œè¿œè¶… NLP ä¸­å¸¸è§çš„ä½ç§©æ³¨æ„åŠ›æ¨¡å¼ã€‚

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### **è¯„ä¼°æŒ‡æ ‡**
- **æ¨èæ€§èƒ½**ï¼š`NDCG@K`, `Hit@K`ï¼ˆK=200, 500ï¼‰
- **AUC**ï¼ˆArea Under Curveï¼‰
- **æ•ˆç‡æŒ‡æ ‡**ï¼š**Training Throughput**ï¼ˆåƒ token/ç§’ï¼‰ï¼Œåœ¨ NVIDIA H20 GPU ä¸Šæµ‹é‡

#### **æ¨¡å‹è§„æ¨¡**
- ä¸»è¦å¯¹æ¯” 0.13B å‚æ•°æ¨¡å‹
- åºåˆ—é•¿åº¦æ‰©å±•è‡³ 16Kï¼Œæµ‹è¯•é•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

åˆ†ä¸ºä¸‰ç±»ï¼š

| ç±»åˆ« | ä»£è¡¨æ¨¡å‹ |
|------|--------|
| **Upper Bound (éçº¿æ€§)** | SASRec, HSTU (Transformer-based) |
| **Linear Recurrences** | SLA, GLA, MoM, GSA, Mamba-2 |
| **Optimization-Based** | Gated DeltaNet, TTT-Linear, ATLAS, TITANS |

PRISM ä¸è¿™äº›æ–¹æ³•åœ¨ç›¸åŒè®¾ç½®ä¸‹å…¬å¹³æ¯”è¾ƒã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **è¡¨ï¼šAmazon Books ä¸Š NDCG@200 å’Œ AUC è¡¨ç°**
| Model | NDCG@200 | AUC |
|-------|----------|-----|
| SASRec (Transformer) | 0.0215 | **0.8910** |
| HSTU (Transformer) | 0.0233 | 0.8835 |
| **PRISM (Ours)** | **0.0238** | **0.8888** |
| TITANS | 0.0243 | 0.8869 |

> PRISM åœ¨çº¿æ€§æ¨¡å‹ä¸­å…¨é¢é¢†å…ˆï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ¥è¿‘ç”šè‡³è¶…è¶Š Transformerã€‚

#### **å¹³å‡æ’åï¼ˆMean Rankï¼‰**
- PRISM åœ¨æ‰€æœ‰çº¿æ€§æ¨¡å‹ä¸­ **Mean Rank æœ€ä½ï¼ˆ2.62ï¼‰**ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ˆå¦‚ TITANS ä¸º 3.62ï¼‰ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

1. **æ€§èƒ½æ–¹é¢**ï¼š
   - PRISM æ€§èƒ½ä¸ TTTã€TITANS ç­‰æ˜¾å¼ä¼˜åŒ–æ–¹æ³•**ç›¸å½“ç”šè‡³æ›´ä¼˜**ã€‚
   - åœ¨ Amazon Movies ä¸Šï¼ŒPRISM çš„ AUC æ˜¯æ‰€æœ‰çº¿æ€§æ¨¡å‹ä¸­æœ€é«˜çš„ã€‚
   - ä¸ Transformer çš„å·®è·æå°ï¼ˆå¦‚ Amazon Books ä¸Šå‡ ä¹æŒå¹³ï¼‰ã€‚

2. **æ•ˆç‡æ–¹é¢**ï¼š
   - **è®­ç»ƒååé‡é«˜è¾¾ TTT çš„ 174Ã—**ã€‚
   - åœ¨ä¸åŒåºåˆ—é•¿åº¦ï¼ˆ2Kâ€“16Kï¼‰å’Œæ‰¹é‡å¤§å°ä¸‹ï¼ŒPRISM ååç¨³å®šåœ¨ ~61K tokens/sã€‚
   - ç›¸æ¯”ä¹‹ä¸‹ï¼ŒTTT ä»… 0.34K tokens/sï¼ŒTransformer++ åœ¨é•¿åºåˆ—ä¸‹ä¸¥é‡é€€åŒ–ã€‚

> **ç»“è®º**ï¼šPRISM å®ç°äº†â€œ**æ˜¾å¼ä¼˜åŒ–çš„è´¨é‡ + çº¿æ€§æ¨¡å‹çš„é€Ÿåº¦**â€ã€‚

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰**

åœ¨ Amazon Elecs ä¸ŠéªŒè¯å„ç»„ä»¶ä½œç”¨ï¼š

| å˜ä½“ | Hit@200 | AUC | åˆ†æ |
|------|--------|-----|------|
| **PRISM (Full)** | **0.1409** | **0.7134** | åŸºçº¿ |
| w/o Iterative Refinement (L=1) | 0.1155 | 0.6805 | â¬‡ï¸ æ˜¾è‘—ä¸‹é™ï¼Œè¯æ˜**è¿­ä»£æ·±åº¦è‡³å…³é‡è¦** |
| w/o Non-Linearity | 0.1316 | 0.7047 | â¬‡ï¸ è¯´æ˜**éçº¿æ€§æ¿€æ´»å¿…è¦** |
| w/o ShortConv Anchor | 0.1406 | 0.7076 | â¬‡ï¸ é”šå®šæä¾›å…³é”®ä¸Šä¸‹æ–‡ |
| w/o Gain Predictor | 0.1383 | 0.7098 | â¬‡ï¸ è‡ªé€‚åº”å¢ç›Šæå‡ç¨³å®šæ€§ |

> æ‰€æœ‰ç»„ä»¶å‡å¯¹æœ€ç»ˆæ€§èƒ½æœ‰æ­£å‘è´¡çŒ®ï¼Œå°¤å…¶æ˜¯**è¿­ä»£ç»†åŒ–**å’Œ**éçº¿æ€§**ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**

1. **â€œä¸€æ¬¡æ€§ç“¶é¢ˆâ€æ˜¯çº¿æ€§æ¨¡å‹çš„æ ¹æœ¬é™åˆ¶**ï¼šæ ‡å‡†çº¿æ€§æ›´æ–°æ— æ³•è¿›è¡Œå¤šæ­¥è¯¯å·®ä¿®æ­£ï¼Œé™åˆ¶äº†å»ºæ¨¡å¤æ‚è¯­ä¹‰ä¾èµ–çš„èƒ½åŠ›ã€‚
2. **æ‘Šé”€ä¼˜åŒ–æ˜¯å¯è¡Œè·¯å¾„**ï¼šæ— éœ€åœ¨æ¨ç†æ—¶æ‰§è¡Œæ˜¾å¼æ¢¯åº¦ä¸‹é™ï¼Œå³å¯é€šè¿‡ç»“æ„å…ˆéªŒå­¦ä¹ åˆ°ä¼˜åŒ–è½¨è¿¹ã€‚
3. **Rank Accumulation å¯è¢«ç†è®ºè¯æ˜**ï¼šPRISM çš„è®¾è®¡å®ç°äº†ä» Rank-1 åˆ° Rank-L çš„ç»“æ„æ‰©å±•ï¼Œçªç ´äº†ä¼ ç»Ÿçº¿æ€§æ¨¡å‹çš„å‡è®¾ç©ºé—´ã€‚
4. **æ•ˆç‡ä¸è´¨é‡å¯ä»¥å…¼å¾—**ï¼šPRISM åœ¨ä¿æŒå®Œå…¨å¹¶è¡Œæ€§çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†ä¸ä¸²è¡Œä¼˜åŒ–æ–¹æ³•ç›¸å½“çš„å»ºæ¨¡ä¿çœŸåº¦ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

1. **å±€éƒ¨è¿‘ä¼¼ vs. å…¨å±€ååº”æ€§**ï¼š
   - PRISM ä¾èµ– ShortConv ä»£ç†å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œ**æ— æ³•å“åº”é¥è¿œå†å²ä¸­çš„è®°å¿†å†²çª**ï¼ˆå¦‚â€œè¿™ä¸ªäº‹å®æ˜¯å¦å·²å­˜å‚¨ï¼Ÿâ€ï¼‰ã€‚
   - å¯¹â€œé•¿å°¾æ¼‚ç§»â€ï¼ˆlong-tail driftï¼‰ç¼ºä¹åŠ¨æ€çº æ­£èƒ½åŠ›ã€‚

2. **å®¹é‡æœªæ‰©å±•**ï¼š
   - PRISM æå‡çš„æ˜¯**å†™å…¥è´¨é‡**ï¼ˆFidelityï¼‰ï¼Œè€Œé**å†…å­˜å®¹é‡**ï¼ˆCapacityï¼‰ã€‚
   - å›ºå®šç»´åº¦çŠ¶æ€ä»ä¼šé­é‡è¦†ç›–é—®é¢˜ï¼ˆOverwritingï¼‰ã€‚

3. **ç†è®ºå‡è®¾ä¾èµ–**ï¼š
   - å†™å…¥-é—å¿˜è§£è€¦çš„æœ‰æ•ˆæ€§åŸºäºè°±æ‰°åŠ¨åˆ†æï¼Œå®é™…æ•ˆæœå¯èƒ½å—ä»»åŠ¡ç‰¹æ€§å½±å“ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **æ˜¾å¼æ‰©å±•å®¹é‡**ï¼š
   - ç»“åˆ **Mixture-of-Memories (MoM)** æˆ– **Gated Slot Attention (GSA)**ï¼Œå¢åŠ çŠ¶æ€å®½åº¦ã€‚
   - PRISM å¯ä½œä¸ºé«˜è´¨é‡å†™å…¥ç®—å­ï¼Œä¸å®¹é‡æ‰©å±•æ–¹æ³•ååŒã€‚

2. **æ··åˆæ¶æ„ï¼ˆHybrid Architecturesï¼‰**ï¼š
   - å¦‚ Jambaã€Kimi Linear æ‰€ç¤ºï¼Œå‘¨æœŸæ€§æ’å…¥ Transformer å±‚å¯çº æ­£å±€éƒ¨çº¿æ€§è¿‘ä¼¼çš„é•¿æœŸæ¼‚ç§»ã€‚
   - PRISM å¯ä½œä¸ºä¸»å¹²é«˜æ•ˆæ¨¡å—ï¼Œè¾…ä»¥å°‘é‡å…¨å±€æ³¨æ„åŠ›å±‚ã€‚

3. **æ›´é²æ£’çš„é”šå®šæœºåˆ¶**ï¼š
   - æ¢ç´¢æ›´é•¿æ„Ÿå—é‡çš„è½»é‡çº§é”šå®šç½‘ç»œï¼ˆå¦‚ dilated convolutionsï¼‰ä»¥å¢å¼ºä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€‚

4. **ç†è®ºæ·±åŒ–**ï¼š
   - è¿›ä¸€æ­¥åˆ†ææ‘Šé”€ä¼˜åŒ–ä¸çœŸå®ä¼˜åŒ–è·¯å¾„çš„é€¼è¿‘ç¨‹åº¦ã€‚
   - æ¢ç´¢ PRISM åœ¨ä¸åŒ ODE åŠ¨åŠ›å­¦ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

---

> **æ€»ç»“**ï¼šPRISM æˆåŠŸåœ°å°†è¿­ä»£ä¼˜åŒ–çš„æ€æƒ³â€œè’¸é¦â€è¿›ä¸€ä¸ªå¯å¹¶è¡Œçš„åºåˆ—æ¨¡å‹ä¸­ï¼Œä¸ºæ„å»ºé«˜æ•ˆä¸”å¯Œæœ‰è¡¨è¾¾åŠ›çš„ä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹æä¾›äº†åšå®çš„æ–°èŒƒå¼ã€‚

</details>

---

### 7. [QTALE: Quantization-Robust Token-Adaptive Layer Execution for LLMs](https://arxiv.org/abs/2602.10431)

**Authors**: Kanghyun Noh, Jinheon Choi, Yulwha Kim  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.10431v1  

#### Abstract
Large language models (LLMs) demand substantial computational and memory resources, posing challenges for efficient deployment. Two complementary approaches have emerged to address these issues: token-adaptive layer execution, which reduces floating-point operations (FLOPs) by selectively bypassing ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠQTALE: Quantization-Robust Token-Adaptive Layer Execution for LLMsã€‹æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éƒ¨ç½²æ—¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š
- **é«˜è®¡ç®—æˆæœ¬**ï¼šå¤§é‡ FLOPs å¯¼è‡´æ¨ç†å»¶è¿Ÿé«˜ã€‚
- **é«˜å†…å­˜å ç”¨**ï¼šå…¨ç²¾åº¦æƒé‡æ¶ˆè€—å¤§é‡æ˜¾å­˜ã€‚

å·²æœ‰ä¸¤ç§ä¸»æµä¼˜åŒ–æŠ€æœ¯ï¼š
- **Token-adaptive layer execution**ï¼ˆå¦‚ D-LLMï¼‰ï¼šé€šè¿‡åŠ¨æ€è·³è¿‡éƒ¨åˆ† Transformer å±‚å‡å°‘ FLOPsã€‚
- **Quantization**ï¼šé™ä½æƒé‡ç²¾åº¦ä»¥å‡å°æ¨¡å‹ä½“ç§¯å’Œå†…å­˜å¸¦å®½éœ€æ±‚ã€‚

ç„¶è€Œï¼Œ**ç›´æ¥å°†ä¸¤è€…ç»“åˆä¼šå¯¼è‡´æ˜¾è‘—çš„å‡†ç¡®ç‡ä¸‹é™**ã€‚å…¶æ ¹æœ¬åŸå› æ˜¯ token-adaptive æ¨¡å‹æœ¬èº«å·²å‡å°‘äº†å†—ä½™ï¼ˆredundancyï¼‰ï¼Œè€Œé‡åŒ–è¿›ä¸€æ­¥å¼•å…¥æ‰°åŠ¨ï¼Œä½¿å¾—æ¨¡å‹å¯¹é”™è¯¯æ›´æ•æ„Ÿã€‚

> ğŸ” **æ ¸å¿ƒé—®é¢˜**ï¼šå¦‚ä½•åœ¨ä¸ç‰ºç‰²å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œæ— ç¼é›†æˆ token-adaptive æ‰§è¡Œä¸é‡åŒ–ï¼Ÿ

---

### ğŸš€ **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡º **QTALE**ï¼ˆQuantization-Robust Token-Adaptive Layer Executionï¼‰ï¼Œä¸€ä¸ªæ”¯æŒé«˜æ•ˆä¸”é²æ£’é‡åŒ–çš„ token-adaptive æ¨ç†æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š

#### ï¼ˆ1ï¼‰**Quantization-Robust Training Strategyï¼ˆé‡åŒ–é²æ£’è®­ç»ƒç­–ç•¥ï¼‰**
- å¼•å…¥ **ç†µæœ€å¤§åŒ–ç›®æ ‡å‡½æ•° $ \mathcal{L}_{\text{entropy}} $**ï¼Œé¼“åŠ±è·¯ç”±å™¨ï¼ˆrouterï¼‰è¾“å‡ºå…·æœ‰æ›´é«˜çš„ä¸ç¡®å®šæ€§ã€‚
- ç›®æ ‡æ˜¯è®©â€œæ‰§è¡Œâ€ä¸â€œè·³è¿‡â€çš„ logits å·®è·å˜å°ï¼Œä»è€Œå¢å¼ºè·¯å¾„å¤šæ ·æ€§ï¼ˆpath diversityï¼‰ã€‚
- åœ¨è®­ç»ƒä¸­å¼•å…¥ Gumbel-Softmax æŠ€å·§ï¼Œä¿ƒè¿›ä¸åŒæ‰§è¡Œè·¯å¾„è¢«æ¢ç´¢ï¼Œæå‡è®­ç»ƒè·¯å¾„å†—ä½™ï¼ˆtraining-path redundancyï¼‰ã€‚

> ğŸ’¡ ç±»æ¯” Dropout å’Œ Stochastic Depth â€”â€” éšæœºæ€§æå‡äº†æ³›åŒ–èƒ½åŠ›å’ŒæŠ—å¹²æ‰°èƒ½åŠ›ã€‚

#### ï¼ˆ2ï¼‰**Post-Training Execution Ratio Adjustmentï¼ˆæ¨ç†é˜¶æ®µæ‰§è¡Œæ¯”ä¾‹è°ƒèŠ‚æœºåˆ¶ï¼‰**
- åœ¨æ¨ç†æ—¶ï¼Œå¯é€šè¿‡è°ƒæ•´é˜ˆå€¼ $\theta$ åŠ¨æ€æ§åˆ¶ layer bypass çš„æ¯”ä¾‹ã€‚
- å½“æ£€æµ‹åˆ°é‡åŒ–å¸¦æ¥è¾ƒå¤§è¯¯å·®æ—¶ï¼Œå¯ä¸´æ—¶æé«˜æ‰§è¡Œæ¯”ä¾‹ï¼ˆå³å°‘è·³è¿‡å±‚ï¼‰ï¼Œé‡æ–°å¼•å…¥å‚æ•°å†—ä½™ï¼ˆparameter redundancyï¼‰ï¼Œæå‡é²æ£’æ€§ã€‚
- ä½¿ç”¨ç®€å•çš„ä¸¤é˜¶æ®µç½‘æ ¼æœç´¢ï¼ˆcoarse + fine grid searchï¼‰åœ¨æ ¡å‡†é›†ä¸Šè‡ªåŠ¨ç¡®å®šæœ€ä¼˜ $\theta$ã€‚

---

### â­ **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | D-LLMï¼ˆBaselineï¼‰ | QTALEï¼ˆæœ¬æ–‡æ–¹æ³•ï¼‰ |
|------|-------------------|--------------------|
| è·¯å¾„å¤šæ ·æ€§ | ä½ï¼ˆæ”¶æ•›ä¸ºå›ºå®šæ¨¡å¼ï¼Œç±»ä¼¼å‰ªæï¼‰ | é«˜ï¼ˆè®­ç»ƒä¸­ä¸»åŠ¨æ¢ç´¢å¤šç§è·¯å¾„ï¼‰ |
| å¯¹é‡åŒ–çš„é²æ£’æ€§ | å·®ï¼ˆå¾®å°æ‰°åŠ¨å¯¼è‡´è·¯å¾„æ¼‚ç§»ï¼‰ | å¼ºï¼ˆå¹³æ»‘è·¯ç”±å†³ç­–ï¼ŒæŠµæŠ—é‡åŒ–å™ªå£°ï¼‰ |
| å†—ä½™ç®¡ç† | å›ºå®šã€é™æ€ | å¯è°ƒã€åŠ¨æ€ |
| ä¸é‡åŒ–å…¼å®¹æ€§ | å·®ï¼Œç²¾åº¦æŸå¤±å¤§ | å¥½ï¼Œå‡ ä¹æ— æŸ |

> âœ… QTALE å®ç°äº† **FLOPs ä¸ memory çš„åŒé‡å‹ç¼©**ï¼ŒåŒæ—¶ä¿æŒæ¥è¿‘ full-model çš„ accuracyã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š **ä½¿ç”¨çš„æ•°æ®é›†**

#### ä¸»è¦è¯„ä¼°åŸºå‡†ï¼š
- **CommonsenseQA (CSQA)** å¥—ä»¶ï¼š
  - PIQA, SIQA, ARCe, ARCc, BoolQ, Winogrande, OBQA
- **MMLU**ï¼šå¤šä»»åŠ¡çŸ¥è¯†ç†è§£
- **Stanford-Alpaca** å’Œ **SAMSum**ï¼šç”¨äº PPLï¼ˆPerplexityï¼‰è¯„ä¼°

#### å¾®è°ƒä¸æ ¡å‡†ï¼š
- ä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡æ•°æ®è¿›è¡Œ fine-tuning
- å°è§„æ¨¡ calibration set ç”¨äºæœç´¢æœ€ä¼˜æ‰§è¡Œé˜ˆå€¼ $\theta$

---

### âš™ï¸ **å®éªŒè®¾ç½®**

| é¡¹ç›® | è®¾ç½® |
|------|------|
| **æ¨¡å‹** | LLaMA2-7B, LLaMA3.1-8B, LLaMA3.2-3B |
| **é‡åŒ–æ–¹å¼** | AWQï¼ˆ4-bit å’Œ 3-bit æ•´æ•°é‡åŒ–ï¼‰ï¼Œgroup size=128 |
| **Token-adaptive æ–¹æ³•** | åŸºäº D-LLM æ¶æ„ï¼Œæ¯å±‚åŠ  router MLP |
| **è®­ç»ƒé…ç½®** | å­¦ä¹ ç‡ã€epoch æ•°ç­‰å¤ç°åŸ D-LLM è®¾ç½® |
| **è¯„ä¼°æŒ‡æ ‡** | Zero-shot Accuracy (%) / PPL â†“, FLOPs â†“, Model Size (Memory) â†“, Latency (Speedup) â†‘ |

---

### ğŸ” **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| æ–¹æ³• | æè¿° |
|------|------|
| **Full-layer Execution (FP16/INT4)** | ä¸è·³å±‚çš„æ ‡å‡†æ¨¡å‹ï¼Œä½œä¸º upper bound |
| **AWQ** | å•çº¯é‡åŒ–ï¼Œæ—  token-adaptive |
| **D-LLM** | åŸå§‹ token-adaptive æ–¹æ³• |
| **D-LLM + AWQ** | D-LLM ä¸é‡åŒ–çš„ç®€å•ç»„åˆï¼ˆnaive integrationï¼‰ |
| **QTALE (Ours)** | æœ¬æ–‡æ–¹æ³•ï¼šå¸¦ç†µæ­£åˆ™è®­ç»ƒ + æ¨ç†æ—¶æ‰§è¡Œæ¯”è°ƒèŠ‚ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… åœ¨ **LLaMA2-7B + 3-bit é‡åŒ– + CSQA** ä¸Šçš„ç»“æœï¼ˆTable 1ï¼‰ï¼š

| æ–¹æ³• | Accuracy (%) | ç›¸è¾ƒ Full æ¨¡å‹å·®è· |
|------|-------------|------------------|
| Full-layer (3-bit) | 72.22 | â€” |
| D-LLM (3-bit) | 70.57 | â–¼ 1.65% |
| **QTALE (3-bit)** | **72.79** | â–² +0.57% |

> âœ… QTALE ä¸ä»…å¼¥è¡¥äº†ç²¾åº¦æŸå¤±ï¼Œç”šè‡³ç•¥å¾®åè¶…ï¼

#### âœ… åœ¨ **LLaMA3.2-3B + 4-bit é‡åŒ– + CSQA** ä¸Šçš„ç»“æœï¼š

| æ–¹æ³• | Accuracy (%) |
|------|--------------|
| Full-layer (4-bit) | 77.55 |
| D-LLM (4-bit) | 73.96 (**â–¼ 3.59%**) |
| **QTALE (4-bit)** | **78.41** (**â–² +0.86%**) |

> â— D-LLM å‡ºç°ä¸¥é‡é€€åŒ–ï¼Œè€Œ QTALE è¡¨ç°ç¨³å¥ã€‚

---

### ğŸ” **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Study, Table 4ï¼‰**

éªŒè¯ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶çš„ä½œç”¨ï¼š

| æ–¹æ³• | CSQA Accuracy (%) | æ˜¯å¦æ¢å¤ç²¾åº¦ï¼Ÿ |
|------|------------------|----------------|
| D-LLMï¼ˆæ— ä»»ä½•æ”¹è¿›ï¼‰ | 73.96 | å¦ |
| + Only $ \mathcal{L}_{\text{entropy}} $ | 75.82 | éƒ¨åˆ†æ¢å¤ |
| + Only $\theta$ è°ƒæ•´ | 75.10 | éƒ¨åˆ†æ¢å¤ |
| **+ Both (QTALE)** | **78.41** | âœ… å®Œå…¨æ¢å¤å¹¶è¶…è¶Š |

> âœ… ä¸¤é¡¹è®¾è®¡ç¼ºä¸€ä¸å¯ï¼ŒååŒä½œç”¨æ˜¾è‘—ã€‚

---

### ğŸ“ˆ **æ•ˆç‡è¯„ä¼°ï¼ˆFigure 6ï¼‰**

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **FLOPs Reduction** | å¹³å‡é™ä½çº¦ **46%**ï¼ˆå½’ä¸€åŒ– FLOPs â‰ˆ 0.54ï¼‰ |
| **Memory Footprint** | ä» 13.5GBï¼ˆFP16ï¼‰é™è‡³ **<4.5GB**ï¼ˆ4-bitï¼‰ |
| **ç»¼åˆä¼˜åŠ¿** | åŒæ—¶å®ç° **FLOPsâ†“ + Memoryâ†“**ï¼Œé€‚åˆè¾¹ç¼˜è®¾å¤‡éƒ¨ç½² |

> ğŸ’¬ è™½ç„¶ QTALE å›  slightly higher execution ratio å¯¼è‡´é€Ÿåº¦ç•¥æ…¢äº D-LLMï¼Œä½†æ¢æ¥çš„æ˜¯**ç²¾åº¦ç¨³å®šæ€§**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… **ä¸»è¦å‘ç°**

1. **Token-adaptive ä¸ Quantization çš„å†²çªæ ¹æºåœ¨äºâ€œå†—ä½™ç¼ºå¤±â€**ï¼š
   - D-LLM è®­ç»ƒè·¯å¾„å•ä¸€ + å‚æ•°åˆ©ç”¨ç‡ä½ â†’ å¯¹é‡åŒ–æ‰°åŠ¨æåº¦æ•æ„Ÿã€‚
   
2. **QTALE æˆåŠŸé‡å»ºäº†ä¸¤ç§å†—ä½™**ï¼š
   - **Training-path redundancy**ï¼šé€šè¿‡ç†µæ­£åˆ™ç¡®ä¿å¤šæ ·è·¯å¾„è¢«æ¢ç´¢ã€‚
   - **Parameter redundancy**ï¼šé€šè¿‡æ¨ç†æ—¶åŠ¨æ€ä¸Šè°ƒ execution ratio æ¥è¡¥å¿æŸå¤±ã€‚

3. **QTALE å®ç°è¿‘ä¹æ— æŸçš„è”åˆå‹ç¼©**ï¼š
   - åœ¨ CommonsenseQA ä¸Šï¼Œä¸çº¯é‡åŒ–æ¨¡å‹çš„å·®è· **< 0.5%**ã€‚
   - æ˜¾è‘—ä¼˜äº naive combination æ–¹æ¡ˆï¼ˆæœ€å¤§å·®è·è¾¾ 3.6%ï¼‰ã€‚

4. **å…·å¤‡è‰¯å¥½é€šç”¨æ€§å’Œæ‰©å±•æ€§**ï¼š
   - å…¼å®¹å…¶ä»– PTQ æ–¹æ³•ï¼ˆå¦‚ MagR+GPTQï¼‰â†’ è¡¨ç°ç¨³å®šã€‚
   - å¯ä¸å…¶ä»–å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚ Wanda å‰ªæï¼‰ç»“åˆï¼Œåœ¨ 50% sparsity ä¸‹ä»ä¿æŒæ€§èƒ½ã€‚

---

### âš ï¸ **å±€é™æ€§**

1. **ä¾èµ– fine-tuning**ï¼šéœ€è¦é¢å¤–çš„å¾®è°ƒè¿‡ç¨‹æ¥å®æ–½ç†µæ­£åˆ™è®­ç»ƒï¼Œå¢åŠ è®­ç»ƒå¼€é”€ã€‚
2. **router æ¨¡å—å¸¦æ¥è½»å¾® overhead**ï¼šè™½ç„¶è½»é‡ï¼Œä½†ä»éœ€é¢å¤–å‚æ•°å’Œè®¡ç®—ã€‚
3. **ç›®å‰èšç„¦äº post-training quantization (PTQ)**ï¼šæœªæ¶‰åŠé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

---

### ğŸ”® **æœªæ¥å·¥ä½œæ–¹å‘**

1. å°† QTALE æ‰©å±•è‡³ **QATï¼ˆQuantization-Aware Trainingï¼‰** æ¡†æ¶ä¸­ï¼Œè¿›ä¸€æ­¥æå‡æé™å‹ç¼©ä¸‹çš„ç¨³å®šæ€§ã€‚
2. æ¢ç´¢ **æ›´é«˜æ•ˆçš„ router è®¾è®¡**ï¼ˆå¦‚å…±äº«å‚æ•°ã€ç¨€ç–æ¿€æ´»ï¼‰ä»¥é™ä½å¼€é”€ã€‚
3. åº”ç”¨äº **vision-language models æˆ– MoE æ¶æ„** ä¸­ï¼Œç ”ç©¶è·¨æ¨¡æ€æˆ–ä¸“å®¶é€‰æ‹©ä¸­çš„é²æ£’è‡ªé€‚åº”æ‰§è¡Œã€‚
4. å¼€å‘ **è‡ªåŠ¨åŒ– threshold selection pipeline**ï¼Œé€‚é…ä¸åŒç¡¬ä»¶å¹³å°ä¸è´Ÿè½½æ¡ä»¶ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯

> **QTALE é€šè¿‡â€œè®­ç»ƒæ—¶å¢å¼ºè·¯å¾„å¤šæ ·æ€§ + æ¨ç†æ—¶çµæ´»è°ƒèŠ‚æ‰§è¡Œå¼ºåº¦â€ï¼Œé¦–æ¬¡å®ç°äº† token-adaptive layer execution ä¸ low-bit quantization çš„é«˜æ•ˆã€æ— æŸèåˆï¼Œä¸º LLM çš„ç«¯ä¾§é«˜æ•ˆéƒ¨ç½²æä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚**

</details>

---

### 8. [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)

**Authors**: Jie Jiang, Yangru Huang, Zeyu Wang, Changping Wang, Yuling Xiong, Jun Zhang, Huan Yu  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.10699v1  

#### Abstract
Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated dec...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSpend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜

æœ¬æ–‡é’ˆå¯¹**ç”Ÿæˆå¼æ¨èï¼ˆGenerative Recommendation, GRï¼‰** ä¸­å­˜åœ¨çš„ **æ¦‚ç‡-å¥–åŠ±ä¸åŒ¹é…ï¼ˆprobability-reward misalignmentï¼‰** é—®é¢˜å±•å¼€ç ”ç©¶ã€‚è¯¥é—®é¢˜åœ¨åŸºäºè‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚Transformerï¼‰è¿›è¡Œç«¯åˆ°ç«¯æ¨èæ—¶å°¤ä¸ºä¸¥é‡ï¼Œå…·ä½“è¡¨ç°ä¸ºä¸¤ä¸ªç»“æ„æ€§ç¼ºé™·ï¼š

- **æ¢ç´¢ä¸è¶³ï¼ˆInsufficient Explorationï¼‰**ï¼šä¼ ç»ŸåŸºäºæ¦‚ç‡çš„è§£ç ç­–ç•¥ï¼ˆå¦‚ beam searchï¼‰å€¾å‘äºä¿ç•™é«˜æ¦‚ç‡å‰ç¼€ï¼Œå¯¼è‡´ä½æ¦‚ç‡ä½†é«˜å¥–åŠ±çš„é•¿å°¾ç‰©å“ï¼ˆlong-tail itemsï¼‰åœ¨æ—©æœŸå°±è¢«å‰ªæï¼Œæ— æ³•è¢«é‡‡æ ·ã€‚
- **ä¼˜åŠ¿å‹ç¼©ï¼ˆAdvantage Compressionï¼‰**ï¼šç”±äºå€™é€‰é›†ä¸­çš„é¡¹ç›®å¾€å¾€å…±äº«é«˜æ¦‚ç‡å‰ç¼€ï¼Œå½¢æˆâ€œå…„å¼ŸèŠ‚ç‚¹â€ï¼ˆsiblingsï¼‰ï¼Œå…¶å¥–åŠ±é«˜åº¦ç›¸å…³ï¼Œå¯¼è‡´ç»„å†…æ–¹å·®å°ï¼ŒGRPOç­‰RLç®—æ³•çš„å­¦ä¹ ä¿¡å·å¼±åŒ–ã€‚

è¿™ä¸¤ä¸ªé—®é¢˜å…±åŒå‰Šå¼±äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ‰æ•ˆæ€§ï¼Œé™åˆ¶äº†æ¨¡å‹å¯¹é«˜ä»·å€¼é¡¹ç›®çš„å‘ç°èƒ½åŠ›ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•æˆ–æ–°æ€è·¯

ä½œè€…æå‡º **V-STAR**ï¼ˆ**Value-guided Sampling and Tree-structured Advantage Reinforcement**ï¼‰ï¼Œä¸€ä¸ªè‡ªæ¼”åŒ–çš„é‡‡æ ·-å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…å«ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼š

#### ï¼ˆ1ï¼‰Value-Guided Efficient Decoding (VED)

- **ç›®æ ‡**ï¼šåœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œæå‡å€™é€‰é›†çš„è´¨é‡ä¸å¤šæ ·æ€§ã€‚
- **æœºåˆ¶**ï¼š
  - åˆ©ç”¨ä¸€ä¸ªè½»é‡çº§çš„ **value model** é¢„æµ‹æ¯ä¸ªå‰ç¼€ï¼ˆprefixï¼‰çš„æœŸæœ›å›æŠ¥ã€‚
  - ç»“åˆå‰ç¼€çš„ **value** å’Œ **policy entropy**ï¼ˆä¸ç¡®å®šæ€§ï¼‰æ„å»ºä¼˜å…ˆçº§è¯„åˆ† $ G(s) = V(s) + \lambda H(s) $ã€‚
  - åœ¨åˆå§‹åŒ–çš„æµ…å±‚æ ‘ï¼ˆç”±beam searchç”Ÿæˆï¼‰åŸºç¡€ä¸Šï¼Œé€‰æ‹©é«˜ä»·å€¼ä¸”é«˜ä¸ç¡®å®šæ€§çš„â€œå†³å®šæ€§èŠ‚ç‚¹â€ï¼ˆdecisive nodesï¼‰è¿›è¡Œå±€éƒ¨æ‰©å±•ï¼Œé¿å…ç©·ä¸¾æœç´¢ã€‚
- **åˆ›æ–°ç‚¹**ï¼šå°†è§£ç è§†ä¸º**æœ‰é¢„ç®—çš„èµ„æºåˆ†é…é—®é¢˜**ï¼Œå®ç°â€œèŠ±åœ¨åˆ€åˆƒä¸Šâ€çš„æ™ºèƒ½æ¢ç´¢ã€‚

#### ï¼ˆ2ï¼‰Sibling-GRPO

- **ç›®æ ‡**ï¼šç¼“è§£å…„å¼ŸèŠ‚ç‚¹é—´çš„ä¼˜åŠ¿å‹ç¼©é—®é¢˜ã€‚
- **æœºåˆ¶**ï¼š
  - åˆ©ç”¨ç”Ÿæˆè¿‡ç¨‹ä¸­è‡ªç„¶å½¢æˆçš„**å‰ç¼€æ ‘ç»“æ„**ï¼Œå°†å…±äº«çˆ¶å‰ç¼€çš„å€™é€‰åˆ’åˆ†ä¸ºâ€œå…„å¼Ÿç»„â€ï¼ˆsibling groupsï¼‰ã€‚
  - åœ¨æ¯ç»„å†…éƒ¨è®¡ç®—**ç›¸å¯¹ä¼˜åŠ¿ï¼ˆrelative advantageï¼‰**ï¼Œå³ $ A_{\text{node}}(x;h,v) = \frac{R(x;h,v) - \mu_h}{\sigma_h + \epsilon} $ã€‚
  - åœ¨è®­ç»ƒä¸­å¯¹è¿™äº›å…„å¼ŸèŠ‚ç‚¹åº”ç”¨GRPOæ›´æ–°ï¼Œä½¿æ¢¯åº¦é›†ä¸­åœ¨**å…³é”®åˆ†æ”¯å†³ç­–**ä¸Šã€‚
- **åˆ›æ–°ç‚¹**ï¼šé¦–æ¬¡å°†**æ ‘å½¢ç»“æ„ä¿¡ç”¨åˆ†é…**å¼•å…¥ç”Ÿæˆå¼æ¨èçš„RLä¼˜åŒ–ï¼Œå¢å¼ºäº†å­¦ä¹ ä¿¡å·çš„åˆ¤åˆ«åŠ›ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ Beam Search + GRPOï¼‰ | V-STAR |
|------|-------------------------------|--------|
| æ¢ç´¢æ•ˆç‡ | ä½ï¼Œæ˜“å‰ªæé«˜ä»·å€¼ä½æ¦‚ç‡è·¯å¾„ | é«˜ï¼Œé€šè¿‡valueå¼•å¯¼èšç„¦é«˜æ½œåŠ›åˆ†æ”¯ |
| å­¦ä¹ ä¿¡å· | å¼±ï¼Œå…¨å±€å½’ä¸€åŒ–å¯¼è‡´ä¼˜åŠ¿å‹ç¼© | å¼ºï¼Œå…„å¼Ÿç»„å†…å½’ä¸€åŒ–å¢å¼ºå¯¹æ¯” |
| è®¡ç®—å¼€é”€ | å›ºå®šï¼Œéš¾ä»¥åŠ¨æ€è°ƒæ•´ | å¯æ§ï¼Œä»…åœ¨å…³é”®èŠ‚ç‚¹æ‰©å±• |
| å€™é€‰å¤šæ ·æ€§ | ä½ï¼Œå…„å¼Ÿé¡¹å†—ä½™ä¸¥é‡ | é«˜ï¼Œè¦†ç›–æ›´å¹¿çš„è¯­ä¹‰åˆ†æ”¯ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†

- **ç¦»çº¿æ•°æ®é›†**ï¼š
  - **Amazon Review Dataset** çš„ä¸¤ä¸ªå­é›†ï¼š
    - **Industrial Products**
    - **Office Products**
- **åœ¨çº¿æ•°æ®é›†**ï¼š
  - å¾®ä¿¡è§†é¢‘å·ï¼ˆWeChat Channelsï¼‰çš„çœŸå®çº¿ä¸Šæµé‡ï¼Œç”¨äºA/Bæµ‹è¯•ã€‚

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹æ¶æ„
- ä¸»å¹²æ¨¡å‹ï¼š**Qwen2.5-1.5B**
- è®­ç»ƒæµç¨‹ï¼šä¸¤é˜¶æ®µ
  1. **Supervised Fine-Tuning (SFT)**
  2. **RL Fine-tuning with GRPO**

#### è§£ç è®¾ç½®
- VED æ„é€  16 ä¸ªå€™é€‰ï¼ŒSIDé•¿åº¦ $ L=3 $
- Beam width åˆå§‹åŒ–ä¸º 8
- Discount factor $ \gamma = 0.99 $ï¼Œentropy weight $ \lambda = 0.1 $

#### è¯„ä¼°æŒ‡æ ‡

| ç±»å‹ | æŒ‡æ ‡ |
|------|------|
| **ç¦»çº¿** | HR@K, NDCG@K ï¼ˆK=3,5,10ï¼‰ |
| **åœ¨çº¿** | GMVï¼ˆGross Merchandise Volumeï¼‰, GMV-Normalï¼ˆç‚¹å‡»/è½¬åŒ–ä¼˜åŒ–å¹¿å‘Šçš„GMVï¼‰ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| ç±»å‹ | åŸºçº¿æ–¹æ³• |
|------|----------|
| ä¼ ç»Ÿåºåˆ—æ¨è | GRU4Rec, Caser, SASRec |
| ç”Ÿæˆå¼éSID | HSTU, BIGRec |
| SIDç”Ÿæˆå¼ï¼ˆæ— RLï¼‰ | TIGER, LCRec, D3 |
| SIDç”Ÿæˆå¼ + RL/DPO | S-DPO, MiniOneRec |
| è§£ç ç­–ç•¥å¯¹æ¯” | Beam Search, Top-K Sampling |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

| æ–¹æ³• | Industrial HR@3 | Industrial NDCG@10 | Office HR@3 | Office NDCG@10 |
|------|------------------|---------------------|-------------|---------------|
| MiniOneRec (æœ€å¼ºåŸºçº¿) | 0.1143 | 0.1167 | 0.1217 | 0.1242 |
| **Ours (V-STAR)** | **0.1189** (+4.0%) | **0.1217** (+4.3%) | **0.1344** (+10.4%) | **0.1340** (+7.9%) |

> âœ… **æ‰€æœ‰æŒ‡æ ‡å‡æ˜¾è‘—é¢†å…ˆäºç°æœ‰SOTAæ–¹æ³•**

---

### åœ¨çº¿A/Bæµ‹è¯•ç»“æœï¼ˆ5å¤©ï¼Œ5%æµé‡ï¼‰

| æŒ‡æ ‡ | æå‡å¹…åº¦ |
|------|---------|
| **GMV** | **+1.23%** |
| **GMV-Normal** | **+1.87%** |

> âœ… æ˜¾è‘—æå‡å•†ä¸šæ”¶ç›Šï¼ŒéªŒè¯äº†æ–¹æ³•çš„å®é™…ä»·å€¼ã€‚

---

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰è§£ç ç­–ç•¥å¯¹æ¯”ï¼ˆTable 2ï¼‰

| è§£ç ç­–ç•¥ | Industrial NDCG@10 | Office HR@10 |
|----------|--------------------|--------------|
| Beam Search | 0.1194 | 0.1684 |
| Top-K | 0.1090 | 0.1644 |
| **Ours (VED)** | **0.1217** | **0.1746** |

> âœ… VEDåœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¸Šå‡ä¼˜äºä¼ ç»Ÿç­–ç•¥ã€‚

#### ï¼ˆ2ï¼‰VEDæ‰©å±•è§„åˆ™æ¶ˆèï¼ˆTable 3ï¼‰

| æ‰©å±•è§„åˆ™ | Industrial HR@10 |
|----------|------------------|
| Value-only $ V(s) $ | 0.1627 |
| Entropy-only $ H(s) $ | 0.1604 |
| **Joint Score $ G(s) $** | **0.1641** |

> âœ… **ä»·å€¼ä¸ä¸ç¡®å®šæ€§çš„è”åˆè¯„åˆ†æ•ˆæœæœ€ä½³**ï¼Œè¯æ˜ä¸¤è€…ååŒä½œç”¨ã€‚

#### ï¼ˆ3ï¼‰è®­ç»ƒç›®æ ‡æ¶ˆèï¼ˆTable 4ï¼‰

| è®­ç»ƒç›®æ ‡ | Industrial NDCG@10 |
|----------|--------------------|
| GRPO (Global) | 0.1189 |
| Sibling-GRPO | 0.1204 |
| **GRPO + Sibling-GRPO (Joint)** | **0.1217** |

> âœ… è”åˆä¼˜åŒ–æ•ˆæœæœ€å¥½ï¼Œè¯´æ˜å…¨å±€å¯¹é½ä¸ç»“æ„æ„ŸçŸ¥æ›´æ–°ç›¸è¾…ç›¸æˆã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. âœ… **æ¦‚ç‡é©±åŠ¨çš„è§£ç å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·**ï¼šbeam searchç­‰ç­–ç•¥ä¼šå¯¼è‡´é«˜å¥–åŠ±é•¿å°¾é¡¹è¢«æå‰å‰ªæï¼Œå¹¶å¼•å‘ä¼˜åŠ¿å‹ç¼©ï¼Œå‰Šå¼±RLå­¦ä¹ æ•ˆç‡ã€‚
2. âœ… **ä»·å€¼å¼•å¯¼çš„ç¨€ç–æœç´¢æ›´é«˜æ•ˆ**ï¼šVEDèƒ½ä»¥æå°é¢å¤–è®¡ç®—æˆæœ¬ï¼Œåœ¨å…³é”®èŠ‚ç‚¹è¿›è¡Œæ¢ç´¢ï¼Œæ˜¾è‘—æå‡é«˜ä»·å€¼å€™é€‰çš„å¯è¾¾æ€§ã€‚
3. âœ… **å…„å¼Ÿç»„å†…å½’ä¸€åŒ–å¢å¼ºå­¦ä¹ ä¿¡å·**ï¼šSibling-GRPOåˆ©ç”¨ç”Ÿæˆæ ‘ç»“æ„ï¼Œæ¢å¤äº†è¢«å‹ç¼©çš„æ¢¯åº¦ä¿¡å·ï¼Œä½¿æ¨¡å‹æ›´å…³æ³¨å†³å®šæ€§åˆ†æ”¯ã€‚
4. âœ… **V-STARå½¢æˆè‡ªæ¼”åŒ–é—­ç¯**ï¼šæ›´å¥½çš„é‡‡æ · â†’ æ›´å¼ºçš„å­¦ä¹ ä¿¡å· â†’ æ›´ä¼˜çš„ç­–ç•¥ â†’ æ›´å¥½çš„ä»·å€¼ä¼°è®¡ â†’ æ›´ç²¾å‡†çš„é‡‡æ ·ï¼ŒæŒç»­è¿­ä»£æå‡ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§

- **ä¾èµ–é«˜è´¨é‡çš„value model**ï¼šè‹¥valueä¼°è®¡ä¸å‡†ï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯çš„æ‰©å±•å†³ç­–ã€‚
- **ä»å—é™äºé¢„å®šä¹‰çš„SIDç»“æ„**ï¼šæ–¹æ³•å»ºç«‹åœ¨Semantic IDçš„åŸºç¡€ä¸Šï¼Œå¯¹å…¶ä»–ç”ŸæˆèŒƒå¼é€‚é…æ€§å¾…éªŒè¯ã€‚
- **åœ¨çº¿æ¨ç†æœªå®Œå…¨é‡Šæ”¾æ½œåŠ›**ï¼šæµ‹è¯•æ—¶é»˜è®¤ä½¿ç”¨beam searchä»¥ä¿è¯å»¶è¿Ÿï¼ŒVEDçš„å¢ç›Šä¸»è¦ä½“ç°åœ¨è®­ç»ƒé˜¶æ®µã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. **å°†VEDåº”ç”¨äºæ¨ç†é˜¶æ®µ**ï¼šåœ¨å…è®¸æ›´é«˜å»¶è¿Ÿçš„åœºæ™¯ä¸­éƒ¨ç½²VEDï¼Œè¿›ä¸€æ­¥æå‡çº¿ä¸Šæ•ˆæœã€‚
2. **åŠ¨æ€è°ƒæ•´è®¡ç®—é¢„ç®—**ï¼šæ ¹æ®queryéš¾åº¦è‡ªé€‚åº”åˆ†é…æœç´¢èµ„æºã€‚
3. **ç»“åˆæ›´å¤šå¤–éƒ¨ä¿¡å·**ï¼šå¦‚ç”¨æˆ·å®æ—¶åé¦ˆã€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–value modelã€‚
4. **æ‰©å±•è‡³å¤šæ¨¡æ€ç”Ÿæˆæ¨è**ï¼šå°†æ¡†æ¶æ¨å¹¿åˆ°å›¾æ–‡ã€è§†é¢‘ç­‰å¤æ‚itemçš„ç”Ÿæˆä»»åŠ¡ä¸­ã€‚

---

> **æ€»ç»“**ï¼šV-STARé€šè¿‡**ä»·å€¼å¼•å¯¼çš„ç»“æ„åŒ–é‡‡æ ·**ä¸**å…„å¼Ÿæ„ŸçŸ¥çš„ä¼˜åŠ¿ä¼˜åŒ–**ï¼Œæœ‰æ•ˆè§£å†³äº†ç”Ÿæˆå¼æ¨èä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œåœ¨ä¸¥æ ¼å»¶è¿Ÿçº¦æŸä¸‹å®ç°äº†æ€§èƒ½ä¸å¤šæ ·æ€§çš„åŒé‡çªç ´ï¼Œä¸ºå·¥ä¸šçº§ç”Ÿæˆæ¨èç³»ç»Ÿæä¾›äº†æ–°çš„ä¼˜åŒ–èŒƒå¼ã€‚

</details>

---

### 9. [How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge](https://arxiv.org/abs/2602.10210)

**Authors**: Junhong Lin, Bing Zhang, Song Wang, Ziyan Liu, Dan Gutfreund, Julian Shun, Yada Zhu  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.10210v1  

#### Abstract
Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly contin...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šHow Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs?

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ç”¨äºè¯„ä¼° **Retrieval-Augmented Generation (RAG)** å’Œ **KG-RAG** æ¨¡å‹çš„åŸºå‡†ï¼ˆå¦‚ HotpotQAã€WebQuestionsï¼‰å­˜åœ¨ä¸¥é‡çš„ **pretraining contamination** é—®é¢˜â€”â€”å³è¿™äº›åŸºå‡†ä¸­çš„çŸ¥è¯†æ—©å·²è¢«åŒ…å«åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é¢„è®­ç»ƒè¯­æ–™ä¸­ã€‚è¿™å¯¼è‡´æ¨¡å‹å¯èƒ½é€šè¿‡å‚æ•°åŒ–è®°å¿†ï¼ˆparametric recallï¼‰è€ŒéçœŸæ­£çš„æ£€ç´¢ä¸æ¨ç†æ¥å›ç­”é—®é¢˜ï¼Œä»è€Œé«˜ä¼°äº†å…¶èƒ½åŠ›ã€‚

æ­¤å¤–ï¼Œç°æœ‰åŸºå‡†éš¾ä»¥æœ‰æ•ˆåŒºåˆ†ä»¥ä¸‹å‡ ç§èƒ½åŠ›ï¼š
- æ¨¡å‹æ˜¯å¦çœŸæ­£è¿›è¡Œäº†å¤šè·³æ¨ç†ï¼ˆmulti-hop reasoningï¼‰
- æ˜¯å¦ä¾èµ–å¤–éƒ¨æ£€ç´¢
- æ˜¯å¦æœ‰æ•ˆèåˆäº†ç»“æ„åŒ–ï¼ˆKGï¼‰ä¸éç»“æ„åŒ–ï¼ˆtextï¼‰çŸ¥è¯†

### æå‡ºçš„æ–°æ–¹æ³•ï¼šHYBRIDRAG-BENCH
ä½œè€…æå‡ºäº† **HYBRIDRAG-BENCH**ï¼Œä¸€ä¸ªå…¨è‡ªåŠ¨çš„åŸºå‡†æ„å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
- **æ—¶æ•ˆæ€§ä¸æŠ—æ±¡æŸ“æ€§ï¼ˆContamination-Awareï¼‰**  
  æ‰€æœ‰çŸ¥è¯†å‡æ¥è‡ªè¿‘æœŸï¼ˆå¦‚ 2023â€“2024 å¹´ï¼‰çš„ arXiv ç§‘å­¦æ–‡çŒ®ï¼Œç¡®ä¿å…¶æ™šäºä¸»æµ LLMs çš„é¢„è®­ç»ƒæˆªæ­¢æ—¶é—´ï¼ˆä¾‹å¦‚ Qwen2.5 æˆªæ­¢äº 2023 å¹´ 10 æœˆï¼‰ï¼Œä»è€Œé¿å…å‚æ•°åŒ–è®°å¿†ã€‚
  
- **æ··åˆçŸ¥è¯†è¡¨ç¤ºï¼ˆHybrid Knowledgeï¼‰**  
  åŒæ—¶æ„å»º **éç»“æ„åŒ–æ–‡æœ¬ç‰‡æ®µ**ï¼ˆtext chunksï¼‰å’Œ **ç»“æ„åŒ–çŸ¥è¯†å›¾è°±**ï¼ˆKGï¼‰ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­ä¿¡æ¯çš„åŒé‡å½¢æ€ã€‚

- **æ˜¾å¼æ¨ç†è·¯å¾„ç”Ÿæˆï¼ˆExplicit Reasoning Pathsï¼‰**  
  é—®é¢˜-ç­”æ¡ˆå¯¹åŸºäºæ˜ç¡®çš„ KG è·¯å¾„ç”Ÿæˆï¼Œå¹¶ç»“åˆæ”¯æŒæ–‡æœ¬ï¼Œç¡®ä¿æ¯ä¸ªé—®é¢˜éƒ½éœ€è¦å¤šæ­¥æ¨ç†ï¼Œä¸”ç­”æ¡ˆå¯è¿½æº¯ã€‚

- **è‡ªåŠ¨åŒ–ä¸å¯æ‰©å±•æ€§**  
  æ•´ä¸ªæµç¨‹ï¼ˆä»æ•°æ®æ”¶é›†ã€KG æ„å»ºåˆ° QA ç”Ÿæˆï¼‰å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œæ”¯æŒæŒ‰é¢†åŸŸã€æ—¶é—´èŒƒå›´çµæ´»å®šåˆ¶ï¼Œä¾¿äºæŒç»­æ›´æ–°ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»ŸåŸºå‡†ï¼ˆå¦‚ HotpotQAï¼‰ | HYBRIDRAG-BENCH |
|------|------------------------|------------------|
| æ•°æ®æ–°é²œåº¦ | é™æ€ã€è¿‡æ—¶ | åŠ¨æ€ã€è¿‘æœŸ arXiv æ–‡çŒ® |
| æŠ—æ±¡æŸ“èƒ½åŠ› | å¼±ï¼ˆæ˜“ä¸é¢„è®­ç»ƒæ•°æ®é‡å ï¼‰ | å¼ºï¼ˆæ—¶é—´ä¸Šéš”ç¦»ï¼‰ |
| æ¨ç†å¯æ§æ€§ | æœ‰é™ | æ˜¾å¼æ§åˆ¶æ¨ç†è·¯å¾„é•¿åº¦ä¸ç±»å‹ |
| çŸ¥è¯†å½¢å¼ | å•ä¸€ï¼ˆé€šå¸¸ä¸ºæ–‡æœ¬æˆ– KGï¼‰ | æ··åˆï¼ˆtext + KGï¼‰ |
| å¯æ‰©å±•æ€§ | ä¾èµ–äººå·¥æ ‡æ³¨ | å…¨è‡ªåŠ¨æµæ°´çº¿ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†æ„å»º
- **æ¥æº**ï¼šä» arXiv æ”¶é›†ä¸‰ä¸ªé¢†åŸŸçš„è¿‘æœŸè®ºæ–‡ï¼š
  - **AI**ï¼ˆäººå·¥æ™ºèƒ½ï¼Œå¼ºåŒ–å­¦ä¹ æ–¹å‘ï¼‰
  - **CY**ï¼ˆGovernance and Policyï¼Œæ²»ç†ä¸æ”¿ç­–ï¼‰
  - **BIO**ï¼ˆBioinformaticsï¼Œç”Ÿç‰©ä¿¡æ¯å­¦ï¼‰
- **æ—¶é—´çª—å£**ï¼šé€‰æ‹©æ™šäºä¸»æµ LLM é¢„è®­ç»ƒæˆªæ­¢æ—¶é—´çš„æ–‡çŒ®ï¼ˆå¦‚ 2023 å¹´åï¼‰ï¼Œç¡®ä¿çŸ¥è¯†ä¸å¯è¢«â€œè®°ä½â€ã€‚
- **æœ€ç»ˆæ•°æ®**ï¼šæ¯ä¸ªé¢†åŸŸçº¦ 1000 ä¸ª QA å¯¹ï¼Œå…± 2869 æ¡ï¼Œè¦†ç›–å¤šç§æ¨ç†ç±»å‹ï¼ˆè§ä¸‹è¡¨ï¼‰ã€‚

#### è¡¨ï¼šé—®é¢˜ç±»å‹åˆ†å¸ƒï¼ˆTable 2ï¼‰
| é—®é¢˜ç±»å‹ | AI (%) | CY (%) | BIO (%) |
|--------|-------|-------|--------|
| Single-hop | 29% | 25% | 25% |
| Single-hop w. Condition | 16% | 13% | 19% |
| Multi-hop (Regular) | 19% | 15% | 19% |
| Multi-hop (Difficult) | 17% | 17% | 13% |
| Counterfactual | 13% | 18% | 6% |
| Open-ended | 5% | 12% | 18% |

### å®éªŒè®¾ç½®
- **æ¨¡å‹**ï¼šä½¿ç”¨å¤šä¸ªå‰æ²¿ LLMs è¿›è¡Œæµ‹è¯•ï¼š
  - `DeepSeek-V3.2` (685B)
  - `Qwen2.5` (72B)
  - `LLaMA3.3` (70B)
  - `LLaMA3.1` (8B)

- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼ŒæŠ¥å‘Š 5 æ¬¡è¿è¡Œçš„å¹³å‡å€¼ä¸æ ‡å‡†å·®ã€‚

- **åŸºçº¿æ–¹æ³•å¯¹æ¯”**ï¼š
  | ç±»åˆ« | æ–¹æ³• |
  |------|------|
  | **LLM-only** | IOï¼ˆç›´æ¥æé—®ï¼‰ã€Chain-of-Thought (CoT)ã€Self-Consistency (SC) |
  | **Text-based RAG** | Dense retrieval + generation |
  | **Naive KG Augmentation** | 1-hop KGï¼ˆæ³¨å…¥é‚»è¿‘ä¸‰å…ƒç»„ï¼‰ã€RAG + 1-hop KG |
  | **Advanced KG-RAG** | CoK, RoG, ToG, ToG2.0, PoG, R2-KG, HippoRAG2.0, EvoReasoner |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 3ï¼‰
åœ¨æ‰€æœ‰ä¸‰ä¸ªé¢†åŸŸä¸­ï¼Œ**LLM-only æ–¹æ³•è¡¨ç°æå·®**ï¼Œè¡¨æ˜é—®é¢˜ç¡®å®æ— æ³•é€šè¿‡è®°å¿†è§£å†³ï¼š

| æ–¹æ³• | AI (avg) | CY (avg) | BIO (avg) |
|------|---------|---------|----------|
| **IO Prompting** | ~30% | ~35% | ~30% |
| **RAG** | ~45% | ~42% | ~48% |
| **Best KG-RAG (EvoReasoner)** | **75.7%** | **69.2%** | **75.9%** |

> âœ… **æœ€å¤§æå‡**ï¼šç›¸æ¯” LLM-onlyï¼Œå¼•å…¥å¤–éƒ¨æ£€ç´¢ä½¿å‡†ç¡®ç‡æå‡ **7â€“29 ä¸ªç™¾åˆ†ç‚¹**ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **RAG vs LLM-only**ï¼šRAG æ˜¾è‘—ä¼˜äºçº¯æç¤ºæ–¹æ³•ï¼Œè¯æ˜å¤–éƒ¨æ£€ç´¢æ˜¯å¿…è¦çš„ã€‚
- **KG-RAG vs RAG**ï¼šæ··åˆæ–¹æ³•ï¼ˆå¦‚ **ToG2.0**, **EvoReasoner**ï¼‰æ˜¾è‘—ä¼˜äºçº¯æ–‡æœ¬ RAGï¼Œè¯´æ˜ç»“æ„åŒ–çŸ¥è¯†æä¾›äº†é¢å¤–ä»·å€¼ã€‚
- **1-hop KG çš„å±€é™æ€§**ï¼šç®€å•æ³¨å…¥ KG é‚»å±…åè€Œå¯èƒ½é™ä½æ€§èƒ½ï¼ˆå™ªå£°å¹²æ‰°ï¼‰ï¼Œå¼ºè°ƒéœ€è¦æ™ºèƒ½æ£€ç´¢æœºåˆ¶ã€‚

### æŒ‰é—®é¢˜ç±»å‹çš„ç»†ç²’åº¦åˆ†æï¼ˆTable 4ï¼‰

| é—®é¢˜ç±»å‹ | æœ€ä½³æ–¹æ³• | å…³é”®å‘ç° |
|--------|--------|--------|
| **Multi-hop** | ToG2.0, EvoReasoner | æ˜¾å¼å›¾éå†ï¼ˆgraph traversalï¼‰å¯¹å¤šè·³æ¨ç†è‡³å…³é‡è¦ |
| **Counterfactual** | CoT, ToG2.0, EvoReasoner | æˆåŠŸä¾èµ–äºå¼ºæ¨ç†èƒ½åŠ›ï¼Œè€Œéæ£€ç´¢ï¼›1-hop KG å‡ ä¹å¤±æ•ˆ |
| **Open-ended** | RAG, EvoReasoner | æ›´ä¾èµ–æ–‡æœ¬æè¿°æ•´åˆï¼Œç»“æ„åŒ–è·¯å¾„ä½œç”¨è¾ƒå° |
| **Conditional Single-hop** | ToG2.0, RAG+1-hop KG | éœ€è¦åŒæ—¶ç†è§£æ–‡æœ¬æ¡ä»¶ä¸ç»“æ„å…³ç³» |

> ğŸ” **é‡è¦å‘ç°**ï¼šå³ä½¿åœ¨ç›¸åŒæ¨¡å‹è§„æ¨¡ä¸‹ï¼Œä¸åŒ KG-RAG æ–¹æ³•ä¹‹é—´æ€§èƒ½å·®è·å·¨å¤§ï¼ˆå¦‚ EvoReasoner > CoK è¾¾ 30%+ï¼‰ï¼Œè¯´æ˜è¯¥åŸºå‡†èƒ½æœ‰æ•ˆåŒºåˆ†æ¨ç†ç­–ç•¥ä¼˜åŠ£ã€‚

### æ¶ˆèå®éªŒä¸æœ‰æ•ˆæ€§éªŒè¯
- **KG æ„å»ºè´¨é‡**ï¼ˆTable 5ï¼‰ï¼šä½¿ç”¨ **EvoKG** æå–çš„ KG åœ¨ MINE åŸºå‡†ä¸Šè¾¾åˆ° **71.36%** çš„äº‹å®æ•è·ç‡ï¼Œä¼˜äº OpenIEã€GraphRAG ç­‰æ–¹æ³•ã€‚
- **è®¡ç®—æˆæœ¬åˆ†æ**ï¼šKG æ„å»ºçš„ token æ¶ˆè€—ä¸æ–‡æ¡£é•¿åº¦å‘ˆè¿‘ä¼¼çº¿æ€§å¢é•¿ï¼Œå…·å¤‡è‰¯å¥½å¯æ‰©å±•æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦ç»“è®º
1. âœ… **HYBRIDRAG-BENCH æˆåŠŸå®ç°äº†æŠ—æ±¡æŸ“è¯„ä¼°**ï¼š  
   LLM-only æ–¹æ³•è¡¨ç°ä¸ä½³ï¼Œè€Œå¼•å…¥å¤–éƒ¨æ£€ç´¢å¸¦æ¥æ˜¾è‘—æå‡ï¼Œè¯´æ˜æ€§èƒ½å¢ç›Šæ¥è‡ªçœŸå®æ£€ç´¢ä¸æ¨ç†ï¼Œè€Œéè®°å¿†ã€‚

2. âœ… **ç»“æ„åŒ–çŸ¥è¯†ï¼ˆKGï¼‰æä¾›äº’è¡¥ä»·å€¼**ï¼š  
   ç»“åˆ KG ä¸æ–‡æœ¬çš„æ··åˆæ–¹æ³•ï¼ˆKG-RAGï¼‰ç³»ç»Ÿæ€§åœ°ä¼˜äºä»…ç”¨æ–‡æœ¬æˆ–ä»…ç”¨ KG çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤šè·³ã€åäº‹å®ç­‰å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šã€‚

3. âœ… **èƒ½æœ‰æ•ˆåŒºåˆ†ä¸åŒæ¨ç†æœºåˆ¶**ï¼š  
   ä¸åŒ KG-RAG æ–¹æ³•åœ¨ä¸åŒç±»å‹é—®é¢˜ä¸Šçš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œæ­ç¤ºäº†å„è‡ªä¼˜åŠ¿ï¼ˆå¦‚ ToG2.0 æ“…é•¿å›¾æ¨ç†ï¼ŒRAG æ“…é•¿å¼€æ”¾ç”Ÿæˆï¼‰ã€‚

4. âœ… **æŒ‘æˆ˜å½“å‰ SOTA æ¨¡å‹**ï¼š  
   å³ä½¿æ˜¯æœ€å¤§è§„æ¨¡çš„ LLMï¼ˆå¦‚ DeepSeek-V3.2ï¼‰ï¼Œä¹Ÿæ— æ³•åœ¨ä¸å€ŸåŠ©æ£€ç´¢çš„æƒ…å†µä¸‹å–å¾—é«˜åˆ†ï¼Œè¯´æ˜è¯¥åŸºå‡†å…·æœ‰è¶³å¤Ÿéš¾åº¦ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ– LLM è¿›è¡Œ KG æ„å»ºä¸ QA ç”Ÿæˆ**ï¼šè™½ç„¶è‡ªåŠ¨åŒ–ï¼Œä½†ä»å¯èƒ½å¼•å…¥ç”Ÿæˆåå·®æˆ–é”™è¯¯ä¼ æ’­ã€‚
- **é¢†åŸŸé™åˆ¶**ï¼šç›®å‰ä»…è¦†ç›–ç§‘ç ”ç±»é¢†åŸŸï¼ˆarXivï¼‰ï¼Œå¯¹å¸¸è¯†ã€æ—¥å¸¸çŸ¥è¯†ç­‰åœºæ™¯é€‚ç”¨æ€§å¾…éªŒè¯ã€‚
- **KG è¦†ç›–åº¦**ï¼šå°½ç®¡ä½¿ç”¨ EvoKG æå–ï¼Œä»å¯èƒ½å­˜åœ¨é—æ¼æˆ–è¯¯è¿å…³ç³»ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³æ›´å¤šé¢†åŸŸï¼ˆå¦‚æ–°é—»ã€ç¤¾äº¤åª’ä½“ï¼‰å’Œè¯­è¨€ã€‚
- å¼•å…¥åŠ¨æ€æ›´æ–°æœºåˆ¶ï¼Œå®ç°æŒç»­è¯„ä¼°ï¼ˆcontinual evaluationï¼‰ã€‚
- æ¢ç´¢æ›´å¤æ‚çš„æ¨ç†æ¨¡å¼ï¼ˆå¦‚å› æœã€æ•°å€¼æ¨ç†ï¼‰ã€‚
- å°†æ¡†æ¶åº”ç”¨äºæ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–æŒ‡å¯¼ï¼Œè€Œä¸ä»…æ˜¯è¯„ä¼°ã€‚

---

## æ€»ç»“
**HYBRIDRAG-BENCH** æ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„ã€é¢å‘æœªæ¥çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒé€šè¿‡**åŸºäºæœ€æ–°ç§‘å­¦æ–‡çŒ®çš„æ··åˆçŸ¥è¯†æ„é€ **å’Œ**æ˜¾å¼æ¨ç†è·¯å¾„å¼•å¯¼çš„ QA ç”Ÿæˆ**ï¼Œè§£å†³äº†ä¼ ç»ŸåŸºå‡†å›  **pretraining contamination** å¯¼è‡´çš„è¯„ä¼°å¤±çœŸé—®é¢˜ã€‚å…¶å®éªŒç»“æœæœ‰åŠ›è¯æ˜ï¼š
> ğŸ¯ **çœŸæ­£çš„å¤šè·³æ¨ç†èƒ½åŠ›å¿…é¡»ä¾èµ–æœ‰æ•ˆçš„å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ä¸ç»“æ„åŒ–æ¨ç†æœºåˆ¶ï¼Œè€Œéå•çº¯çš„æ¨¡å‹è§„æ¨¡æ‰©å¤§æˆ–å‚æ•°è®°å¿†ã€‚**

è¯¥å·¥ä½œä¸ºæœªæ¥ RAG ä¸ KG-RAG ç³»ç»Ÿçš„ç ”ç©¶æä¾›äº†åšå®ã€å¯é ä¸”å¯æ‰©å±•çš„æµ‹è¯•å¹³å°ã€‚

</details>

---

### 10. [Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training](https://arxiv.org/abs/2602.10314)

**Authors**: Jaeyeon Kim, Jonathan Geuter, David Alvarez-Melis, Sham Kakade, Sitan Chen  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.10314v1  

#### Abstract
Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces. By generating sequences in any order and allowing for parallel decoding, they enable fast inference and strong performance on non-causal tasks. However, this flexibility comes with a train...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šStop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
Masked Diffusion Models (MDMs) è™½ç„¶åœ¨æ¨ç†é˜¶æ®µæ”¯æŒä»»æ„é¡ºåºç”Ÿæˆï¼ˆany-order generationï¼‰å¹¶èƒ½å®ç°å¹¶è¡Œè§£ç ï¼Œä½†åœ¨è®­ç»ƒæ—¶é¢ä¸´ä¸¥é‡çš„**è®¡ç®—æ•ˆç‡ç“¶é¢ˆ**ã€‚å…¶æ ¹æœ¬åŸå› åœ¨äºï¼š

- **è®­ç»ƒ-æµ‹è¯•ä¸åŒ¹é…ï¼ˆtrain-test mismatchï¼‰**ï¼š  
  MDM åœ¨è®­ç»ƒæ—¶é‡‡ç”¨éšæœºæ©ç ï¼ˆrandom maskingï¼‰ï¼Œéœ€è¦†ç›–æŒ‡æ•°çº§æ•°é‡çš„æ©ç æ¨¡å¼ï¼›è€Œæ¨ç†æ—¶ä½¿ç”¨çš„ unmasking policyï¼ˆå¦‚åŸºäºç½®ä¿¡åº¦çš„ Top-Kï¼‰åªé›†ä¸­åœ¨ä¸€å°éƒ¨åˆ†é«˜åº¦ç»“æ„åŒ–çš„æ©ç è·¯å¾„ä¸Šã€‚
- è¿™å¯¼è‡´å¤§é‡è®¡ç®—èµ„æºè¢«æµªè´¹åœ¨å¯¹æœ€ç»ˆæ¨ç†æ— ç”¨çš„æ©ç æ¨¡å¼ä¸Šï¼Œå½¢æˆâ€œä¸ºæœ€åæƒ…å†µè®­ç»ƒâ€ï¼ˆTrain for the Worstï¼‰çš„é—®é¢˜ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡º **Progressive UnMAsking (PUMA)** â€”â€”ä¸€ç§é€šè¿‡é‡æ„å‰å‘æ©ç è¿‡ç¨‹æ¥å¯¹é½è®­ç»ƒä¸æ¨ç†åˆ†å¸ƒçš„æ–°èŒƒå¼ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- å¼•å…¥ **teacher-forced chain**ï¼šä»çœŸå®æ ·æœ¬ $x_0 \sim p_{\text{data}}$ å‡ºå‘ï¼ŒæŒ‰ç…§å½“å‰æ¨¡å‹å®šä¹‰çš„ unmasking policy $\mathcal{g}_\theta$ é€æ­¥æ­ç¤º tokenï¼ˆä½†ä½¿ç”¨çœŸå®æ ‡ç­¾è€Œéé‡‡æ ·ï¼‰ï¼Œä»è€Œç”Ÿæˆä¸€ç³»åˆ—ä¸­é—´æ©ç çŠ¶æ€ä½œä¸ºè®­ç»ƒæ ·æœ¬ã€‚
- è¿™ä½¿å¾—è®­ç»ƒä¸­å‡ºç°çš„æ©ç æ¨¡å¼**åŠ¨æ€åœ°é€¼è¿‘æ¨ç†æ—¶å®é™…è®¿é—®çš„è·¯å¾„**ã€‚

#### åˆ›æ–°ä¹‹å¤„ï¼š
- **é¦–æ¬¡ç›´æ¥ä¿®æ”¹ forward masking process** æ¥æå‡è®­ç»ƒæ•ˆç‡ï¼Œè€Œéä¾èµ–æ¶æ„æ”¹è¿›æˆ–æŸå¤±é‡åŠ æƒç­‰é—´æ¥æ‰‹æ®µã€‚
- ç†è®ºä¿è¯ä¸¤ä¸ªå…³é”®æ€§è´¨ï¼š
  1. **Marginal Agreement**ï¼šPUMA ç”Ÿæˆçš„æ©ç åºåˆ—è¾¹ç¼˜åˆ†å¸ƒä¸ç†æƒ³åŒ– MDM æ¨ç†ä¸€è‡´ã€‚
  2. **Minimizer Preservation**ï¼šå°½ç®¡æ”¹å˜äº†æ•°æ®åˆ†å¸ƒï¼Œä½†äº¤å‰ç†µæŸå¤±çš„æœ€ä¼˜è§£ä»ä¿æŒä¸ºçœŸå®çš„ unmasking posteriorã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | PUMA | å…¶ä»–å¸¸è§æ–¹æ³•ï¼ˆå¦‚ ARM init, Block Diffusionï¼‰ |
|------|------|---------------------------------------------|
| **ä½œç”¨å±‚çº§** | Forward process å±‚çº§ | Architecture / Recipe å±‚çº§ |
| **å…¼å®¹æ€§** | å®Œå…¨æ­£äº¤ï¼Œå¯å åŠ ä½¿ç”¨ | å¤šæ•°ä¸å¯ç»„åˆæˆ–å­˜åœ¨å†²çª |
| **é¢å¤–å¼€é”€** | æ—  forward-pass å¼€é”€ | ARM init éœ€é¢„è®­ç»ƒï¼›Block Diffusion æ”¾å¼ƒéƒ¨åˆ†çµæ´»æ€§ |
| **åŠ é€Ÿæœºåˆ¶** | å‡å°‘æ— æ•ˆæ©ç æ¨¡å¼çš„å­¦ä¹  | åˆ©ç”¨å…ˆéªŒçŸ¥è¯†åˆå§‹åŒ–æˆ–é™åˆ¶è§£ç ç©ºé—´ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
1. **Sudoku Puzzle Dataset**  
   - æ¥æºï¼šRadcliffe (2020)ï¼Œç» Shah et al. (2024) å®ç°
   - è§„æ¨¡ï¼š180ä¸‡è®­ç»ƒæ ·æœ¬ï¼Œ10ä¸‡æµ‹è¯•æ ·æœ¬
   - ç‰¹ç‚¹ï¼šç»“æ„å¼ºã€é€»è¾‘ä¾èµ–æ˜ç¡®ï¼Œé€‚åˆéªŒè¯ unmasking policy å¯¹é½æ•ˆæœ

2. **TinyGSM**  
   - æ¥æºï¼šLiu et al. (2023)
   - è§„æ¨¡ï¼š1180ä¸‡æ ·æœ¬
   - å†…å®¹ï¼šå°† GSM8K æ•°å­¦é¢˜è½¬åŒ–ä¸ºç®€çŸ­ Python ç¨‹åº
   - ç›®æ ‡ä»»åŠ¡ï¼šzero-shot ä¸‹æ¸¸ GSM8K æµ‹è¯•é›†å‡†ç¡®ç‡

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
| è®¾ç½®é¡¹ | æè¿° |
|-------|------|
| **æ¨¡å‹è§„æ¨¡** | 125M å‚æ•° MDMï¼ˆä¸»å®éªŒï¼‰ï¼›6.8M Transformerï¼ˆSudokuï¼‰ |
| **æ¶æ„** | Qwen2-style attentionï¼ŒåŒå‘æ³¨æ„åŠ› |
| **è®­ç»ƒæ­¥æ•°** | æœ€å¤š 900k iterations |
| **è¯„ä¼°é¢‘ç‡** | æ¯ 5k æ­¥åœ¨ GSM8K test ä¸Šè¯„ä¼°ä¸€æ¬¡ |
| **è¯„ä¼°æ–¹å¼** | ä½¿ç”¨ EMA checkpoints æå‡ç¨³å®šæ€§ |
| **æ¨ç†ç­–ç•¥** | Top-K, Top-K margin, entropy-based ç­‰å¤šç§ unmasking policy |
| **ä¸»è¦æŒ‡æ ‡** | **GSM8K Accuracy vs. Training Iterations**ï¼ˆè¡¡é‡æ”¶æ•›é€Ÿåº¦ï¼‰ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Baseline**: æ ‡å‡† MDMï¼ˆrandom maskingï¼‰
- **ARM init**: ä»é¢„è®­ç»ƒ AR æ¨¡å‹åˆå§‹åŒ– MDM
- **Block diffusion**: ç»“åˆå—å†…è‡ªå›å½’ä¸ MDM æ¨ç†
- **PUMA variants**: ä¸åŒ K-scheduling å’Œ confidence threshold è®¾ç½®

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
| æ–¹æ³• | åŠ é€Ÿå€æ•°ï¼ˆvs. Baselineï¼‰ | è¾¾åˆ°ç›¸åŒç²¾åº¦æ‰€éœ€è¿­ä»£æ¬¡æ•°å‡å°‘ |
|------|--------------------------|-------------------------------|
| **PUMA (125M, TinyGSM)** | **~2.5Ã—** | ä» ~750k â†’ ~300k iterations |
| **PUMA (Sudoku)** | **1.4Ã—** | æ˜æ˜¾æ›´å¿«è¾¾åˆ°é«˜å‡†ç¡®ç‡ |
| **PUMA + ARM init** | **~2.3Ã— relative to ARM-only** | å¤åˆåŠ é€Ÿæ˜¾è‘— |
| **PUMA + Block diffusion** | **ä»æœ‰é¢å¤–åŠ é€Ÿ** | è¡¨æ˜äº’è¡¥æ€§ |

> ğŸ“ˆ å›¾1æ˜¾ç¤ºï¼Œåœ¨ç›¸åŒè¿­ä»£æ¬¡æ•°ä¸‹ï¼ŒPUMA æ›²çº¿è¿œè¶… baselineï¼Œä¸”æ–œç‡æ›´é™¡ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **ç»å¯¹æ€§èƒ½ä¼˜åŠ¿**ï¼šPUMA åœ¨æ‰€æœ‰è¯„ä¼°ç‚¹å‡ä¼˜äºæ ‡å‡† MDMã€‚
- **é²æ£’æ€§å¼º**ï¼šå³ä½¿è®­ç»ƒæ—¶ä½¿ç”¨æŸä¸€ policyï¼ˆå¦‚ Top-Kï¼‰ï¼Œåœ¨å…¶ä»– inference policyï¼ˆå¦‚ entropy-basedï¼‰ä¸‹ä¾ç„¶è¡¨ç°ä¼˜å¼‚ï¼ˆè§å›¾4ï¼‰ã€‚
- **å…¼å®¹æ€§éªŒè¯**ï¼š
  - ä¸ **autoregressive initialization** ç»“åˆåè¿›ä¸€æ­¥æé€Ÿï¼›
  - åœ¨ **block diffusion** æ¡†æ¶ä¸‹ä»æœ‰æ•ˆï¼ˆå›¾6å³ä¸‹è§’ï¼‰ï¼Œè¯´æ˜å…¶å¢ç›Šç‹¬ç«‹äºç»“æ„çº¦æŸã€‚

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰K-scheduling çš„å½±å“
- å›ºå®š $K=42$ï¼ˆè¿‡æ—©ä½¿ç”¨ç²¾ç»† policyï¼‰ä¼šå¯¼è‡´æ—©æœŸè®­ç»ƒä¸ç¨³å®šã€æ•ˆç‡ä¸‹é™ã€‚
- åŠ¨æ€ K-schedulingï¼ˆä»12å¢è‡³42ï¼‰æ˜¯å…³é”®è®¾è®¡ï¼Œä½¿æ¨¡å‹åˆæœŸä¿æŒå¤šæ ·æ€§ï¼ŒåæœŸèšç„¦é«˜è´¨é‡è·¯å¾„ã€‚

#### ï¼ˆ2ï¼‰confidence thresholding çš„å½±å“
- é˜ˆå€¼è¿‡ä½ï¼ˆå¦‚ 0.4 æˆ– 0.65ï¼‰ä¼šæå‰æš´éœ²ä½ç½®ä¿¡ä½ç½®ï¼Œç ´å teacher-forced chain çš„è¯­ä¹‰è¿è´¯æ€§ï¼Œæ€§èƒ½ä¸‹é™ã€‚
- é˜ˆå€¼ 0.9 æ•ˆæœæœ€ä½³ï¼Œå…è®¸å¿«é€Ÿè·³è¿‡é«˜ç½®ä¿¡åŒºåŸŸè€Œä¸ç‰ºç‰²å­¦ä¹ ä¿¡å·ã€‚

#### ï¼ˆ3ï¼‰policy ç¨³å®šæ€§åˆ†æï¼ˆSudokuï¼‰
- å›¾7æ˜¾ç¤ºï¼šPUMA åœ¨è®­ç»ƒæ—©æœŸï¼ˆçº¦1ä¸‡æ­¥å†…ï¼‰å°±èƒ½æ‰¾åˆ°æ¥è¿‘æœ€ç»ˆæ¨¡å‹çš„ unmasking è·¯å¾„ã€‚
- è¯´æ˜ **position ranking æ¯” posterior calibration æ›´æ—©ç¨³å®š**ï¼Œæ”¯æŒäº†â€œç”¨å½“å‰æ¨¡å‹æŒ‡å¯¼è®­ç»ƒâ€çš„åˆç†æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **è®­ç»ƒ-æ¨ç†å¯¹é½å¯å¤§å¹…æå‡æ•ˆç‡**ï¼šPUMA é€šè¿‡ align training-time masking patterns with inference-time pathsï¼Œå®ç°äº†é«˜è¾¾ **2.5Ã— çš„è®­ç»ƒåŠ é€Ÿ**ã€‚
2. âœ… **æ— éœ€é¢å¤–è®¡ç®—å¼€é”€**ï¼šåˆ©ç”¨å·²æœ‰ logits è¿›è¡Œ scoringï¼Œæ— é¢å¤– forward-pass æˆæœ¬ã€‚
3. âœ… **ä¸ä¸»æµæŠ€æœ¯å®Œå…¨å…¼å®¹**ï¼šPUMA å¯å åŠ äº ARM initã€block diffusion ç­‰æ–¹æ³•ä¹‹ä¸Šï¼Œå¸¦æ¥**å¤åˆåŠ é€Ÿæ•ˆæœ**ã€‚
4. âœ… **ç†è®ºä¿éšœå¯é **ï¼šè¯æ˜äº† marginal agreement ä¸ minimizer preservationï¼Œç¡®ä¿ä¼˜åŒ–ç›®æ ‡ä¸å˜ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰å®éªŒä¸»è¦åŸºäº **åˆæˆ/ç»“æ„åŒ–æ•°æ®é›†**ï¼ˆTinyGSM, Sudokuï¼‰ï¼Œåœ¨çœŸå®å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰å¾…éªŒè¯ã€‚
- å¯¹ **unmasking policy çš„è´¨é‡æ•æ„Ÿ**ï¼šè‹¥å½“å‰æ¨¡å‹ policy æå·®ï¼ˆå¦‚è®­ç»ƒåˆæœŸï¼‰ï¼Œå¯èƒ½å¼•å…¥åå·®ã€‚
- **æœªæ¢ç´¢æç«¯é•¿ç¨‹ä¾èµ–ä»»åŠ¡**ï¼šåœ¨å…·æœ‰å¤æ‚ä¸Šä¸‹æ–‡ä¾èµ–çš„çœŸå®è¯­è¨€å»ºæ¨¡ä¸­ï¼Œalignment æ•ˆç›Šå¯èƒ½å—é™ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å°† PUMA æ‰©å±•è‡³ç™¾äº¿å‚æ•°çº§åˆ«çš„çœŸå®é¢„è®­ç»ƒåœºæ™¯ï¼ˆå¦‚ LLM on The Pileï¼‰ã€‚
- æ¢ç´¢ **learned unmasking policy** æ›¿ä»£ heuristic rulesï¼Œå®ç°ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ã€‚
- ç ”ç©¶å¦‚ä½•å°† PUMA æ€æƒ³è¿ç§»åˆ° **continuous diffusion models** ä¸­ï¼ˆä¾‹å¦‚é€šè¿‡ latent space maskingï¼‰ã€‚
- åˆ†æä¸åŒä»»åŠ¡ç±»å‹ä¸‹ optimal unmasking trajectory çš„ç»“æ„ç‰¹æ€§ï¼ŒæŒ‡å¯¼ forward process è®¾è®¡ã€‚

---

> ğŸ”— **å¼€æºä¿¡æ¯**ï¼šä½œè€…å·²å…¬å¼€ä»£ç åº“ï¼ˆæ–‡ä¸­æä¾›é“¾æ¥ï¼‰ï¼Œä¾¿äºå¤ç°ä¸ç¤¾åŒºæ‰©å±•ã€‚

</details>

---

### 11. [Kalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking](https://arxiv.org/abs/2602.10743)

**Authors**: Vaisakh Shaj, Cameron Barker, Aidan Scannell, Andras Szecsenyi, Elliot J. Crowley, Amos Storkey  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.10743v1  

#### Abstract
State-space language models such as Mamba and gated linear attention (GLA) offer efficient alternatives to transformers due to their linear complexity and parallel training, but often lack the expressivity and robust state-tracking needed for complex reasoning. We address these limitations by refram...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šKalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking

---

## 1. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰ä¸»æµçš„ **State-Space Models (SSMs)** å¦‚ Mamba å’Œ **Gated Linear Attention (GLA)** è™½ç„¶å®ç°äº†çº¿æ€§å¤æ‚åº¦å’Œå¹¶è¡Œè®­ç»ƒï¼Œä½†åœ¨ä»¥ä¸‹æ–¹é¢å­˜åœ¨å±€é™ï¼š
- éšè—çŠ¶æ€æ›´æ–°æ˜¯**çº¿æ€§æˆ–ä»¿å°„**çš„ï¼Œè¡¨è¾¾èƒ½åŠ›å—é™ï¼›
- ç¼ºä¹å¯¹**çŠ¶æ€ä¸ç¡®å®šæ€§**çš„æ˜¾å¼å»ºæ¨¡ï¼›
- åœ¨éœ€è¦å¤æ‚æ¨ç†ã€é•¿ç¨‹ä¾èµ–å’Œé²æ£’çŠ¶æ€è¿½è¸ªçš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ã€‚

æœ¬æ–‡æå‡ºä»**æ¦‚ç‡è§†è§’é‡æ„åºåˆ—å»ºæ¨¡**ï¼Œå°†è¯­è¨€å»ºæ¨¡è§†ä¸ºåœ¨å™ªå£°è§‚æµ‹ä¸‹çš„**è´å¶æ–¯çŠ¶æ€ä¼°è®¡é—®é¢˜**ï¼Œä»è€Œå¼•å…¥æ›´ä¸°å¯Œçš„éçº¿æ€§åŠ¨æ€å’Œå¯è§£é‡Šçš„é—¨æ§æœºåˆ¶ã€‚

---

### ğŸ†• æå‡ºçš„æ–°æ–¹æ³•ï¼šKalman Linear Attention (KLA)

KLA æ˜¯ä¸€ç§æ–°å‹çš„ **sequence mixer å±‚**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
> å°†æ¯ä¸ªè¾“å…¥ token è§†ä¸ºå¯¹æ½œåœ¨è¯­ä¹‰çŠ¶æ€çš„**å¸¦å™ªè§‚æµ‹**ï¼Œé€šè¿‡ **Kalman Filter** è¿›è¡Œåœ¨çº¿åéªŒæ¨æ–­æ¥æ›´æ–°ä¿¡å¿µçŠ¶æ€ã€‚

#### åˆ›æ–°ç‚¹ï¼š
- **C1. å¡å°”æ›¼æ»¤æ³¢çš„ä¿¡æ¯å½¢å¼é‡å‚æ•°åŒ–ï¼ˆAssociative Reparameterisationï¼‰**  
  å°† Kalman Filter è½¬æ¢ä¸º**ä¿¡æ¯å½¢å¼ï¼ˆinformation formï¼‰**ï¼Œè¯æ˜å…¶ç²¾åº¦ï¼ˆprecisionï¼‰æ›´æ–°æ˜¯ä¸€ä¸ª **Mobius (fractional-linear) å˜æ¢**ï¼Œè¯¥å˜æ¢å…·æœ‰**ç»“åˆå¾‹**ï¼Œå› æ­¤å¯é€šè¿‡ **associative scan** å®ç°å¹¶è¡Œè®¡ç®—ã€‚
  - ç»“æœï¼šå®ç° $O(\log T)$ å¹¶è¡Œæ·±åº¦ï¼Œä¸ Mamba åŒçº§æ•ˆç‡ã€‚

- **C2. ä¸ç¡®å®šæ€§é©±åŠ¨çš„éçº¿æ€§é—¨æ§ï¼ˆNonlinear Gating from Uncertaintyï¼‰**  
  é—¨æ§ç”±**å†å²ç²¾åº¦ä¸å½“å‰è§‚æµ‹ç²¾åº¦çš„æ¯”ä¾‹**å†³å®šï¼Œå½¢æˆ**å†å²ä¾èµ–ä¸”éçº¿æ€§**çš„æ›´æ–°è§„åˆ™ï¼Œè¶…è¶Šäº† GLA çš„çº¿æ€§/ä»¿å°„é—¨æ§ã€‚

- **C3. æ˜¾å¼çš„ä¿¡å¿µçŠ¶æ€ä¸ç¡®å®šæ€§å»ºæ¨¡**  
  è¾“å‡ºä¸ä»…åŒ…å«çŠ¶æ€å‡å€¼ $\mu_t$ï¼Œè¿˜è¾“å‡º**åæ–¹å·®/ç²¾åº¦ $\Lambda_t$**ï¼Œå¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚

- **C4. Drop-in æ›¿ä»£æ¨¡å—è®¾è®¡**  
  KLA å¯ä½œä¸º Transformer æˆ– SSM ä¸­çš„ **drop-in replacement**ï¼Œæ— ç¼é›†æˆåˆ°ç°ä»£è¯­è¨€æ¨¡å‹æ¶æ„ä¸­ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç‰¹æ€§ | Softmax Attention | SSMs / GLA | KLA (æœ¬æ–‡) |
|------|-------------------|-----------|------------|
| è¡¨è¾¾åŠ› (Expressivity) | éçº¿æ€§ (softmax) | çº¿æ€§/ä»¿å°„ | **åˆ†æ•°çº¿æ€§ (Mobius)** |
| è®­ç»ƒæ•ˆç‡ | $O(T^2)$ | $O(T)$ | $O(T)$ |
| æ¨ç†æ•ˆç‡ | $O(T)$ | $O(1)$ | $O(1)$ |
| æ˜¾å¼çŠ¶æ€ä¸ç¡®å®šæ€§ | âŒ | âŒ | âœ… |
| å¹¶è¡Œè®­ç»ƒæ”¯æŒ | âŒ | âœ… | âœ… |
| çŠ¶æ€è¿½è¸ªèƒ½åŠ› | ä¸­ç­‰ | å¼±ï¼ˆçº¿æ€§é™åˆ¶ï¼‰ | **å¼ºï¼ˆéçº¿æ€§æ›´æ–°ï¼‰** |

> âœ… **KLA åœ¨ä¿æŒ SSM æ•ˆç‡çš„åŒæ—¶ï¼Œè·å¾—äº†æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥èƒ½åŠ›ã€‚**

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†ä¸ä»»åŠ¡

#### ï¼ˆ1ï¼‰MAD-Lab Synthetic Tasksï¼ˆ6é¡¹åˆæˆè¯­è¨€ä»»åŠ¡ï¼‰
ç”¨äºæµ‹è¯•ä¸åŒè¯­è¨€æŠ€èƒ½ï¼ŒåŒ…æ‹¬ï¼š
- **Compression**ï¼šä¿¡æ¯å‹ç¼©èƒ½åŠ›
- **Memorization**ï¼šå›ºå®šæ˜ å°„è®°å¿†
- **Context Recall (CR)**ï¼šä¸Šä¸‹æ–‡é”®å€¼æ£€ç´¢
- **Noisy Recall (NR)**ï¼šå™ªå£°ç¯å¢ƒä¸‹çš„é€‰æ‹©æ€§æ³¨æ„åŠ›
- **Fuzzy Recall (FR)**ï¼šå¤š token é”®å€¼åŒ¹é…
- **Selective Copy (SC)**ï¼šæœ‰åºå¤åˆ¶ä¸å¹²æ‰°æŠ‘åˆ¶

> æ‰€æœ‰ä»»åŠ¡å‡ä¸ºå¯æ§çš„ç¦»æ•£ token æ“ä½œä»»åŠ¡ï¼Œä¾¿äºåˆ†ææ¨¡å‹è¡Œä¸ºã€‚

#### ï¼ˆ2ï¼‰Long-Context Multi-Query Associative Recall (MQAR)
- åºåˆ—é•¿åº¦ï¼š**T = 2048**
- è¯è¡¨å¤§å°ï¼š**V = 256**
- å¤šæŸ¥è¯¢ç»‘å®šä¸è¿œè·ç¦»æ£€ç´¢æŒ‘æˆ˜
- æµ‹è¯•æ¨¡å‹åœ¨é«˜è´Ÿè½½ä¸‹çš„å­˜å‚¨ä¸æ£€ç´¢èƒ½åŠ›

#### ï¼ˆ3ï¼‰A5 Permutation Composition Task
- åŸºäºäº¤æ›¿ç¾¤ $A_5$ çš„æ’åˆ—ç»„åˆä»»åŠ¡
- æœ¬è´¨æ˜¯**ä¸å¯å¹¶è¡ŒåŒ–çš„é¡ºåºçŠ¶æ€è¿½è¸ªé—®é¢˜**
- ç”¨äºæ£€éªŒæ¨¡å‹æ˜¯å¦å…·å¤‡çœŸæ­£çš„â€œçŠ¶æ€æœºâ€èƒ½åŠ›

---

### âš™ï¸ å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

| è®¾ç½®é¡¹ | æè¿° |
|-------|------|
| **æ¨¡å‹ç»“æ„** | å•å±‚æˆ–åŒå±‚ blockï¼Œç»Ÿä¸€ä½¿ç”¨ Mamba-style fused-MLP æ¶æ„ |
| **ç»´åº¦é…ç½®** | ç»Ÿä¸€æ§åˆ¶æœ‰æ•ˆçŠ¶æ€å¤§å°ï¼ˆå¦‚ 2048ï¼‰ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ |
| **è®­ç»ƒåè®®** | AdamW ä¼˜åŒ–å™¨ï¼Œå›ºå®šå­¦ä¹ ç‡ $1\times10^{-3}$ï¼Œæœ€å¤§ 750 è½® |
| **è¯„ä¼°æŒ‡æ ‡** | å‡†ç¡®ç‡ï¼ˆAccuracy %ï¼‰ï¼Œè¾¾åˆ° â‰¥90% è§†ä¸ºè§£å†³ä»»åŠ¡ |
| **éšæœºç§å­** | æŠ¥å‘Š 5 ä¸ª seed çš„å¹³å‡ç»“æœï¼Œæå‡ç»Ÿè®¡å¯é æ€§ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿ | ç±»å‹ | ç®€ä»‹ |
|------|------|------|
| **Mamba** | SSM | å½“å‰æœ€æµè¡Œçš„ selective SSMï¼Œè¾“å…¥ä¾èµ–åŠ¨æ€ |
| **GLA (Gated Linear Attention)** | Linear Attention | ç»Ÿä¸€ Mamba ç­‰æ¨¡å‹çš„æ¡†æ¶ï¼Œçº¿æ€§å¤æ‚åº¦ |
| **GDN (Gated Delta Net)** | SSM | åŸºäº delta rule çš„è®°å¿†æœºåˆ¶ï¼Œæ“…é•¿å…³è”å›å¿† |
| **mLSTM** | RNN | æ”¹è¿›ç‰ˆ LSTMï¼Œæ”¯æŒå¹¶è¡Œè®­ç»ƒ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»ï¼ˆMAD-Lab ä»»åŠ¡ï¼ŒTest Accuracy %ï¼‰

| Algorithm | Comp | Mem | CR | NR | FR | SC |
|----------|------|-----|----|----|----|----|
| GDN      | 65.53 | 99.93 | 99.94 | 99.95 | 37.04 | 90.30 |
| GLA      | 49.45 | 99.99 | 73.60 | 85.24 | 18.22 | 82.41 |
| Mamba    | 78.35 | 99.96 | 99.92 | 99.93 | 30.83 | 80.60 |
| mLSTM    | 57.17 | 99.98 | 99.98 | 99.99 | 25.43 | 36.61 |
| **KLA (Ours)** | **85.03** | **98.87** | **99.95** | **99.93** | **45.70** | **90.67** |
| **KLA+ (Probabilistic Decoding)** | **88.87** | 99.94 | 99.94 | 99.95 | 43.32 | 91.45 |

> âœ… KLA åœ¨å¤šæ•°ä»»åŠ¡ä¸Š**åŒ¹é…æˆ–è¶…è¶Šæ‰€æœ‰åŸºçº¿**ï¼Œå°¤å…¶åœ¨ **Selective Copy** å’Œ **Compression** ä¸Šé¢†å…ˆæ˜¾è‘—ã€‚

---

### ğŸ”¬ Long-Context MQARï¼ˆT=2048, V=256ï¼‰

| Model Dim | KLA | GDN | Mamba | GLA |
|----------|-----|-----|--------|------|
| d=64     | ~80% | âœ… æœ€ä½³ | âœ… | âŒ æœªå­¦ä¼š |
| d=128    | ~90% | âœ… | âœ… | âŒ |
| **d=256** | **>95% âœ…** | ~90% | ~85% | âŒ |

> âœ… KLA åœ¨æé™è®¾ç½®ä¸‹ä»æ¥è¿‘å®Œç¾å‡†ç¡®ç‡ï¼Œ**é¦–æ¬¡åœ¨æ­¤éš¾åº¦ä¸‹çªç ´ 95%**ï¼Œå»ºç«‹æ–° SOTAã€‚

---

### ğŸ§ª A5 State Trackingï¼ˆPermutation Compositionï¼‰

- **ä»»åŠ¡è¦æ±‚**ï¼šéšç€åºåˆ—å¢é•¿ï¼Œæ‰€éœ€å±‚æ•°åº”ä¸å¢åŠ ï¼ˆç†æƒ³æƒ…å†µ 1â€“2 å±‚å³å¯è§£å†³ï¼‰
- **ç»“æœï¼ˆè§ Figure 1ï¼‰**ï¼š
  - **KLA**ï¼šä»…éœ€ **1â€“2 å±‚** å³å¯è§£å†³ä»»æ„é•¿åº¦ä»»åŠ¡
  - **Mamba / GLA / Transformer**ï¼šæ‰€éœ€å±‚æ•°éšé•¿åº¦çº¿æ€§å¢é•¿ â†’ è¡¨æ˜å…¶æ— æ³•çœŸæ­£â€œè¿½è¸ªçŠ¶æ€â€

> âœ… KLA çš„ **fractional-linear æ›´æ–°ç»“æ„**ä½¿å…¶èƒ½æ‰§è¡Œä¼ ç»Ÿçº¿æ€§ SSM æ— æ³•å®Œæˆçš„**ç¡¬æ€§çŠ¶æ€è¿½è¸ªä»»åŠ¡**ã€‚

---

### ğŸ” æ¶ˆèå®éªŒï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰OU Prior Discretisation Ablationï¼ˆå›¾4ï¼‰
- ç§»é™¤ OU åŠ¨åŠ›å­¦å»ºæ¨¡ â†’ å‡†ç¡®ç‡ä¸‹é™ï¼Œè®­ç»ƒä¸ç¨³å®š
- å°¤å…¶åœ¨æ·±å±‚æ¨¡å‹ä¸­å½±å“æ˜æ˜¾
- âœ… è¯æ˜è¿ç»­æ—¶é—´å…ˆéªŒ + æ­£ç¡®ç¦»æ•£åŒ–å¯¹æ€§èƒ½è‡³å…³é‡è¦

#### ï¼ˆ2ï¼‰Process Noise Ablationï¼ˆå›¾10ï¼‰
- å›ºå®šè¿‡ç¨‹å™ªå£° $p_t = 0$ï¼ˆé€€åŒ–ä¸ºç¡®å®šæ€§ç³»ç»Ÿï¼‰â†’ æ€§èƒ½å¤§å¹…ä¸‹é™
- å¹³å‡å‡†ç¡®ç‡ä¸‹é™ **49.8ä¸ªç™¾åˆ†ç‚¹**
- åœ¨ Memorization å’Œ Recall ä»»åŠ¡ä¸­ä¸‹é™è¶… **90%**

> âœ… **è¿‡ç¨‹å™ªå£° $p_t$ æ˜¯ç»´æŒæ¨¡å‹è¡¨è¾¾åŠ›çš„å…³é”®ç»„ä»¶**ï¼Œä¸èƒ½ç®€å•è®¾ä¸ºé›¶ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **è´å¶æ–¯è¿‡æ»¤è§†è§’å¯è¡Œä¸”é«˜æ•ˆ**  
   å°†è¯­è¨€å»ºæ¨¡è§†ä¸º Kalman Filter é—®é¢˜ï¼Œä¸ä»…èƒ½æä¾›**å¯è§£é‡Šçš„ä¸ç¡®å®šæ€§ä¿¡å·**ï¼Œè¿˜èƒ½é€šè¿‡ä¿¡æ¯å½¢å¼å®ç°**å®Œå…¨å¹¶è¡Œè®­ç»ƒ**ã€‚

2. **Mobius ç»“æ„è§£é”æ›´å¼ºè¡¨è¾¾åŠ›**  
   KLA çš„ precision update æ˜¯ **fractional-linear (Mobius) map**ï¼Œæ¯”çº¿æ€§ SSM æ›´å…·è¡¨è¾¾åŠ›ï¼Œè¶³ä»¥è§£å†³ $A_5$ è¿™ç±»**çœŸÂ·é¡ºåºä»»åŠ¡**ã€‚

3. **ä¸ç¡®å®šæ€§å³é—¨æ§æœºåˆ¶**  
   ç²¾åº¦æ¯”è‡ªåŠ¨è°ƒèŠ‚æ–°æ—§ä¿¡æ¯æƒé‡ï¼Œå½¢æˆ**è‡ªé€‚åº”ã€å†å²ä¾èµ–çš„éçº¿æ€§é—¨æ§**ï¼Œæ— éœ€é¢å¤–è®¾è®¡ gating å‡½æ•°ã€‚

4. **ä¼˜äºç°æœ‰ SSMï¼Œåœ¨æç«¯ä»»åŠ¡ä¸­é¢†å…ˆæ˜æ˜¾**  
   åœ¨é•¿ä¸Šä¸‹æ–‡ã€å™ªå£°å¹²æ‰°ã€çŠ¶æ€è¿½è¸ªç­‰ä»»åŠ¡ä¸Šå…¨é¢èƒœå‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨ **MQAR (T=2048)** å’Œ **A5** ä¸Šè¡¨ç°çªå‡ºã€‚

---

### âš ï¸ å±€é™æ€§ï¼ˆLimitationsï¼‰

1. **å°šæœªåº”ç”¨äºå¤§è§„æ¨¡é¢„è®­ç»ƒ**  
   å½“å‰å®éªŒé›†ä¸­åœ¨æµ…å±‚ã€å°è§„æ¨¡è®¾ç½®ï¼ŒæœªéªŒè¯å…¶åœ¨ç™¾äº¿å‚æ•°ä»¥ä¸Š LLM ä¸­çš„è¡¨ç°ã€‚

2. **æœªå……åˆ†åˆ©ç”¨ posterior covariance**  
   è™½ç„¶è¾“å‡ºäº†ä¸ç¡®å®šæ€§ï¼Œä½†å½“å‰ä»»åŠ¡æœªç”¨äº **hallucination detectionã€OOD detectionã€calibration** ç­‰å®é™…åœºæ™¯ã€‚

3. **ç¡¬ä»¶ä¼˜åŒ–ä»æœ‰ç©ºé—´**  
   å½“å‰ Triton kernel å·²æé€Ÿï¼Œä½†ä»æœªè¾¾åˆ° Mamba çº§åˆ«çš„ fused kernel ä¼˜åŒ–æ°´å¹³ï¼Œå­˜åœ¨è¿›ä¸€æ­¥åŠ é€Ÿæ½œåŠ›ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **æ„å»º Probabilistic LLM**  
   å°† KLA æ‰©å±•è‡³æ·±å±‚ã€å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œæ¢ç´¢å…¶åœ¨å¼€æ”¾ç”Ÿæˆã€æ¨ç†é“¾ã€æ ¡å‡†é¢„æµ‹ç­‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚

2. **åˆ©ç”¨ posterior uncertainty è¿›è¡Œå®‰å…¨ AI åº”ç”¨**  
   å¦‚ï¼šæ£€æµ‹å¹»è§‰ã€è¯†åˆ« OOD è¾“å…¥ã€ä¸»åŠ¨å­¦ä¹ æç¤ºé€‰æ‹©ç­‰ã€‚

3. **ç»“åˆ variational inference æˆ– particle filtering**  
   æ¢ç´¢æ›´å¤æ‚çš„éçº¿æ€§/éé«˜æ–¯æ‰©å±•ç‰ˆæœ¬ï¼ˆå¦‚ Particle KLAï¼‰ã€‚

4. **ç«¯åˆ°ç«¯ä¼˜åŒ– custom kernels**  
   å®ç°ç±»ä¼¼ Mamba çš„ fused kernelï¼Œæœ€å¤§åŒ–ååé‡ä¸èƒ½æ•ˆã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯

> **KLA æˆåŠŸåœ°å°† Kalman Filter çš„åŸåˆ™æ€§ä¸ç¡®å®šæ€§å»ºæ¨¡ä¸ SSM çš„é«˜æ•ˆå¹¶è¡Œè®­ç»ƒç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§å…¼å…·è¡¨è¾¾åŠ›ã€æ•ˆç‡ä¸å¯è§£é‡Šæ€§çš„æ–°ä¸€ä»£ sequence mixerï¼Œåœ¨å¤šé¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šå±•ç°å‡ºè¶…è¶Šç°æœ‰ SSM çš„å¼ºå¤§çŠ¶æ€è¿½è¸ªä¸é²æ£’å»ºæ¨¡èƒ½åŠ›ã€‚**

</details>

---

### 12. [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)

**Authors**: Xuecheng Zou, Yu Tang, Bingbing Wang  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.10845v1  

#### Abstract
Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile di...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠSynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergyã€‹æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰çš„ **Knowledge Graph Completion (KGC)** æ–¹æ³•åœ¨èåˆé¢„è®­ç»ƒè¯­ä¹‰ï¼ˆå¦‚ PLMsï¼‰ä¸å›¾ç»“æ„ä¿¡æ¯ï¼ˆå¦‚ GNNsï¼‰æ—¶é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§ç“¶é¢ˆï¼š**Passive Structural-Semantic Alignment**ã€‚è¿™ç§è¢«åŠ¨å¯¹é½å¯¼è‡´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š
- **Representation Collapse**ï¼šåœ¨ç¨€ç–å›¾ä¸­ï¼Œå®ä½“ç¼ºä¹è¶³å¤Ÿçš„ç»“æ„æ”¯æ’‘ï¼Œå¯¼è‡´è¡¨ç¤ºå´©æºƒï¼›
- **Identity Redundancy**ï¼šåœ¨å¯†é›†å›¾ä¸­ï¼Œæ˜¾å¼çš„è‡ªç¯ç»“æ„å¼•å…¥ä¸å¯å¿½ç•¥çš„å™ªå£°ï¼Œé€ æˆè¿‡å¹³æ»‘ã€‚

æ­¤å¤–ï¼Œè®¸å¤šæ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå­˜åœ¨ **Structural Decoupling**ï¼ˆæ¨ç†æ—¶å…³é—­ç»“æ„èšåˆï¼‰ï¼Œå¼•å‘ **Inference-time Distribution Shift**ï¼ŒæŸå®³éƒ¨ç½²æ€§èƒ½ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡º **SynergyKGC**ï¼Œä¸€ç§ä¸»åŠ¨ã€æŒ‡ä»¤é©±åŠ¨çš„åŒæ¨¡æ€ååŒæ¡†æ¶ï¼Œæ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰**Cross-Modal Synergy Expert**
- å¼•å…¥åŸºäº **relation-aware cross-attention** çš„ååŒä¸“å®¶æ¨¡å—ï¼Œå°†è¯­ä¹‰å‘é‡ä½œä¸ºâ€œæŸ¥è¯¢æŒ‡ä»¤â€æ¥ä¸»åŠ¨æ£€ç´¢å¹¶è¿‡æ»¤æ‹“æ‰‘ä¸Šä¸‹æ–‡ï¼›
- ä¸åŒäºä¼ ç»Ÿ GNN çš„è¢«åŠ¨é‚»å±…èšåˆï¼Œå®ç° **active structural retrieval**ï¼Œæå‡ä¿¡æ¯æ•´åˆæ•ˆç‡ã€‚

#### ï¼ˆ2ï¼‰**Dual-Axis Consistency Principle**
- **Architecture Axis**ï¼šå¼ºåˆ¶ Query Tower ä¸ Entity Tower åœ¨è¡¨ç¤ºå­¦ä¹ ä¸Šä¿æŒå¯¹ç§°ååŒï¼›
- **Lifecycle Axis**ï¼šç¡®ä¿è®­ç»ƒä¸æ¨ç†é˜¶æ®µå‡æ¿€æ´» Synergy Expertï¼Œæ¶ˆé™¤åˆ†å¸ƒåç§»ï¼ˆdistribution shiftï¼‰ã€‚

#### ï¼ˆ3ï¼‰**Density-Aware Identity Anchoring (IA) Strategy**
- æå‡ºâ€œç»“æ„å³èº«ä»½â€ï¼ˆStructure ~ Identityï¼‰ç°è±¡ç†è®ºï¼šåœ¨ç¨€ç–å›¾ä¸­ï¼Œè‡ªèº«ä»½ä¿¡å·æ˜¯å¿…è¦çš„å®šä½æ”¯æ¶ï¼›åœ¨ç¨ å¯†å›¾ä¸­åˆ™åº”æŠ‘åˆ¶ä»¥é¿å…å†—ä½™ã€‚
- åŠ¨æ€åˆ‡æ¢æ˜¯å¦ä¿ç•™ `self-loop`ï¼šå¯¹å¯†é›†å›¾è®¾é˜ˆå€¼ $ \phi = 1 $ï¼Œç¨€ç–å›¾ä¸è®¾é™ã€‚

#### ï¼ˆ4ï¼‰**Instruction-Driven Paradigm ä¸ â€œCatch-up Effectâ€**
- åˆ©ç”¨è¯­ä¹‰æ„å›¾ä½œä¸ºæ¿€æ´»ä¿¡å·ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µè§¦å‘å³æ—¶åŒæ­¥ï¼›
- è§‚å¯Ÿåˆ° **â€œcatch-up effectâ€**ï¼šSynergy Expert æ¿€æ´»åï¼Œè½åçš„åå‘æµè¿…é€Ÿè¿½ä¸Šï¼Œæ˜¾è‘—åŠ é€Ÿæ”¶æ•›ï¼ˆæ— éœ€ >30 epochs çš„ warm-upï¼‰ã€‚

---

### âš–ï¸ ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ•ˆç‡** | è·³è¿‡å†—é•¿ warm-up é˜¶æ®µï¼Œå¿«é€Ÿæ”¶æ•›ï¼ˆEpoch 20 å³è¾¾æœ€ä¼˜ï¼‰ |
| **ç²¾åº¦** | æ˜¾è‘—æå‡ Hits@1ï¼Œå°¤å…¶åœ¨ç¨€ç–å›¾ WN18RR ä¸Šè¡¨ç°çªå‡º |
| **é²æ£’æ€§** | è‡ªé€‚åº”å¤„ç†ä¸åŒå¯†åº¦å›¾çš„æ‹“æ‰‘å¼‚è´¨æ€§ |
| **ä¸€è‡´æ€§** | æ¶ˆé™¤è®­ç»ƒ-æ¨ç†é—´çš„è¡¨ç¤ºæ¼‚ç§»ï¼Œå¢å¼ºéƒ¨ç½²ç¨³å®šæ€§ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†
ä½¿ç”¨ä¸¤ä¸ªæ ‡å‡† benchmarkï¼Œå…·æœ‰æåŒ–ç»“æ„ç‰¹æ€§ï¼š

| æ•°æ®é›† | ç‰¹å¾ |
|-------|------|
| **FB15k-237** | å¯†é›†å…³ç³»å›¾ï¼ˆP50=22ï¼‰ï¼Œæ–‡æœ¬è¾ƒé•¿ï¼ˆavg. 114.6 tokensï¼‰ |
| **WN18RR** | ç¨€ç–å±‚æ¬¡å›¾ï¼ˆP50=3ï¼‰ï¼Œè¯­ä¹‰ç®€æ´ï¼ˆavg. 17.1 tokensï¼‰ |

> è¡¨æ ¼ 1 æä¾›äº†è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œæ¶µç›–å®ä½“æ•°ã€å…³ç³»æ•°ã€è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ ·æœ¬é‡åŠå¯†åº¦åˆ†ä½æ•°ç­‰ã€‚

---

### ğŸ§ª å®éªŒè®¾ç½®
- **æ¨¡å‹æ¶æ„**ï¼š
  - ä½¿ç”¨ `bert-base-uncased` ä½œä¸ºè¯­ä¹‰ç¼–ç å™¨ï¼›
  - Synergy Expert é‡‡ç”¨ 4-head cross-attention ç»“æ„ï¼ˆd=768ï¼‰ï¼›
- **ä¼˜åŒ–å™¨**ï¼šAdamWï¼Œbatch size=768ï¼Œå•å¡ A100ï¼›
- **å­¦ä¹ ç‡**ï¼šä» {1e-5, 5e-5, 5e-4} ä¸­é€‰æ‹©ï¼›
- **ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥**ï¼š
  - Phase Iï¼šä»…ä¼˜åŒ–è¯­ä¹‰ç©ºé—´ï¼ˆcontrastive lossï¼‰ï¼›
  - Phase IIï¼šæ¿€æ´» Synergy Expertï¼Œè”åˆä¼˜åŒ– $ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{NCE}} + \lambda (\mathcal{L}_{\text{align}} + \mathcal{L}_{\text{gate}}) $ï¼Œå…¶ä¸­ $ \lambda = 0.1 $ï¼›
  - å¯åŠ¨æ—¶æœºï¼šFB15k-237 åœ¨ Epoch 5ï¼ŒWN18RR åœ¨ Epoch 20ã€‚

---

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
é‡‡ç”¨æ ‡å‡† **filtered setting** ä¸‹çš„æ’åºæŒ‡æ ‡ï¼š
- **MRR**ï¼ˆMean Reciprocal Rankï¼‰
- **Hits@k**ï¼ˆk âˆˆ {1, 3, 10}ï¼‰
- **MR**ï¼ˆMean Rankï¼‰

é‡ç‚¹å…³æ³¨ **Hits@1** å’Œ **MRR**ï¼Œåæ˜ é«˜ç²¾åº¦é¢„æµ‹èƒ½åŠ›ã€‚

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ¶µç›–ä¸‰ç±»ä¸»æµæ–¹æ³•ï¼š
| ç±»å‹ | ä»£è¡¨æ¨¡å‹ |
|------|--------|
| **Embedding-based** | TransE, RotatE, DistMult, ComplEx, TuckER |
| **PLM-based / Hybrid** | KG-BERT, StAR, LP-BERT, KG-S2S, SimKGC, ProgKGC |

ç‰¹åˆ«ä»¥ **ProgKGC (2025)** ä¸ºæœ€å¼ºæ··åˆåŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 2ï¼‰

| Model | FB15k-237 (MRR / H@1) | WN18RR (MRR / H@1) |
|-------|------------------------|--------------------|
| **RotatE** | 47.6 / 42.8 | 33.8 / 24.1 |
| **SimKGC** | 33.6 / 24.9 | 66.6 / 58.7 |
| **ProgKGC** | 34.4 / 25.5 | 68.2 / 59.7 |
| **SynergyKGC (Ours)** | **39.9 / 30.2** | **74.2 / 67.7** |
| **Improvement** | +5.5 / +4.7 | **+6.0 / +8.0** |

> ğŸ’¥ åœ¨ WN18RR ä¸Šå–å¾— **+8.0% Hits@1 çš„ç»å¯¹å¢ç›Š**ï¼Œåˆ·æ–° SOTAã€‚

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆTable 4 & Figure 4ï¼‰

#### ï¼ˆ1ï¼‰ç»„ä»¶æ¶ˆèåˆ†æï¼ˆTable 4ï¼‰
ç§»é™¤å…³é”®æ¨¡å—åæ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼š
- ç§»é™¤ **Cross-Attention** åœ¨ WN18RR ä¸Šå¯¼è‡´ MRR â†“21.3%ï¼ŒHits@1 â†“25.7%ï¼Œè¯´æ˜å…¶ä¸ºç¨€ç–ç¯å¢ƒä¸­çš„â€œä½ç½®æ”¯æ¶â€ï¼›
- ç§»é™¤ **Adaptive Gate** åœ¨ FB15k-237 ä¸Š MRR â†“3.1%ï¼Œè¡¨æ˜å…¶æœ‰æ•ˆæŠ‘åˆ¶å¯†é›†å›¾ä¸­çš„ç»“æ„å™ªå£°ï¼›
- ç§»é™¤ **MSE Alignment Loss** å¯¼è‡´åŒå¡”å¤±é…ï¼ŒéªŒè¯äº†è¯­ä¹‰ä¸€è‡´æ€§çš„å¿…è¦æ€§ã€‚

#### ï¼ˆ2ï¼‰Dual-Tower Consistency åˆ†æï¼ˆFigure 4ï¼‰
- **Architectural Consistency**ï¼šQuery ä¸ Entity Tower å¿…é¡»åŒæ—¶å¯ç”¨ Synergyï¼Œå¦åˆ™æ€§èƒ½å¤§å¹…ä¸‹æ»‘ï¼›
- **Lifecycle Consistency**ï¼šè®­ç»ƒç”¨ç»“æ„ã€æ¨ç†ä¸ç”¨ â†’ æ€§èƒ½éª¤é™ï¼Œè¯æ˜è·¨é˜¶æ®µä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚

#### ï¼ˆ3ï¼‰Topological Scalingï¼ˆTable 3ï¼‰
- **FB15k-237ï¼ˆå¯†é›†ï¼‰**ï¼šæœ€ä½³æ€§èƒ½å‡ºç°åœ¨ **2-hop** é‚»åŸŸï¼Œæ›´æ·±æ— ç›Šï¼›
- **WN18RRï¼ˆç¨€ç–ï¼‰**ï¼šè™½ç„¶ Hits@k åœ¨ Hop1 è¾¾é¥±å’Œï¼Œä½† **5-hop å¯æ˜¾è‘—é™ä½ Mean Rank**ï¼Œä½“ç°æ·±å±‚æ‹“æ‰‘å¯¹å…¨å±€æ’åºçš„ä¼˜åŒ–ä½œç”¨ã€‚

#### ï¼ˆ4ï¼‰Identity Anchoring æ•æ„Ÿæ€§åˆ†æï¼ˆTable 5ï¼‰
- å¯¹ **WN18RR**ï¼Œæœ€ä¼˜é˜ˆå€¼æ¥è¿‘æœ€å¤§åº¦ï¼ˆ482ï¼‰ï¼Œè¯´æ˜éœ€ä¿ç•™æ‰€æœ‰è¿æ¥ï¼›
- å¯¹ **FB15k-237**ï¼Œæœ€å°é˜ˆå€¼ $ \phi=1 $ æœ€ä¼˜ï¼Œè¿‡å¤§åè€Œå¼•å…¥å™ªå£°ï¼›
- å›¾ 5 è¿›ä¸€æ­¥è¯æ˜ï¼šå³ä½¿å‹ç¼©è‡³å•ä¸€å…³ç³»ï¼Œé”šå®šæ•ˆæœä»ç”±æ‹“æ‰‘å¯†åº¦å†³å®šï¼Œè€Œéå…³ç³»å¤æ‚åº¦ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **æ‹“æ‰‘å¼‚è´¨æ€§æ˜¯ KGC çš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼šå›¾å¯†åº¦å·®å¼‚å¯¼è‡´ç»“æ„ä¿¡å·çš„ä½œç”¨æˆªç„¶ä¸åŒï¼›
2. **SynergyKGC æˆåŠŸå®ç°äº†åŠ¨æ€è°ƒåˆ¶**ï¼šé€šè¿‡ IA ç­–ç•¥å’Œ adaptive gateï¼Œåœ¨ç¨€ç–å›¾ä¸­å¼ºåŒ–èº«ä»½é”šå®šï¼Œåœ¨å¯†é›†å›¾ä¸­æŠ‘åˆ¶å†—ä½™ï¼›
3. **Dual-Axis Consistency æ˜¯ç¨³å®šæ€§çš„åŸºçŸ³**ï¼šæ¶æ„ä¸ç”Ÿå‘½å‘¨æœŸçš„ä¸€è‡´æ€§è®¾è®¡æœ‰æ•ˆç¼“è§£ manifold driftï¼›
4. **â€œCatch-up Effectâ€ åŠ é€Ÿæ”¶æ•›**ï¼šSynergy Expert æ¿€æ´»åç«‹å³å®ç°åŒæµåŒæ­¥ï¼Œè·³è¿‡ä¼ ç»Ÿæ–¹æ³•æ‰€éœ€çš„é•¿ warm-upï¼›
5. **WN18RR ä¸Š +8.0% Hits@1 éªŒè¯äº†æ³›åŒ–èƒ½åŠ›**ï¼šå°¤å…¶æ“…é•¿å¤„ç†ä½å¯†åº¦ã€é•¿å°¾åˆ†å¸ƒåœºæ™¯ã€‚

---

### âš ï¸ å±€é™æ€§
- å½“å‰æ–¹æ³•ä¾èµ– BERT ç¼–ç å™¨ï¼Œå¯èƒ½å—é™äº PLM çš„è¯­ä¹‰ç²’åº¦ï¼›
- è™½ç„¶æ”¯æŒå¤šè·³é‚»åŸŸï¼Œä½†æœªæ¢ç´¢æ›´å¤æ‚çš„å­å›¾ç»“æ„ï¼ˆå¦‚è·¯å¾„ã€è§„åˆ™ï¼‰ï¼›
- Identity Anchoring çš„é˜ˆå€¼ç›®å‰ä¸ºç»éªŒè®¾å®šï¼Œå°šæœªå®Œå…¨è‡ªåŠ¨åŒ–ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- å°† IA ç­–ç•¥æ‰©å±•ä¸ºå¯å­¦ä¹ çš„åŠ¨æ€é—¨æ§æœºåˆ¶ï¼›
- æ¢ç´¢ç»“åˆ symbolic reasoning ä¸ç¥ç»ååŒçš„ hybrid æ¶æ„ï¼›
- åº”ç”¨äºæ›´å¤§è§„æ¨¡å·¥ä¸šçŸ¥è¯†å›¾è°±ï¼ˆå¦‚ç”µå•†ã€åŒ»ç–— KGï¼‰ï¼›
- ç ”ç©¶å¦‚ä½•å°†è¯¥èŒƒå¼è¿ç§»è‡³å…¶ä»–éå‡åŒ€ç»“æ„æ•°æ®ä»»åŠ¡ï¼ˆå¦‚ social networks, citation graphsï¼‰ã€‚

---

## ğŸ”— å¼€æºä¿¡æ¯
ä»£ç å·²å…¬å¼€ï¼š
ğŸ‘‰ [https://github.com/XuechengZou-2001/SynergyKGC-main](https://github.com/XuechengZou-2001/SynergyKGC-main)

åŒ…å«å®Œæ•´æ¨¡å‹å®ç°ã€è®­ç»ƒè„šæœ¬ä¸é¢„å¤„ç†æ•°æ®é›†ï¼Œä¾¿äºå¤ç°ä¸æ‹“å±•ç ”ç©¶ã€‚

</details>

---

### 13. [Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models](https://arxiv.org/abs/2602.10520)

**Authors**: Williams Jonathan, Tureci Esin  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.10520v1  

#### Abstract
Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives su...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆ**Reinforcement Learning with Verifiable Rewards, RLVR**ï¼‰æ–¹æ³•ï¼ˆå¦‚ **GRPO**ï¼‰åœ¨åº”ç”¨äº **Looped Language Models (LoopLMs)** æ—¶æ•ˆæœä¸ä½³ã€‚å…¶æ ¹æœ¬åŸå› åœ¨äºï¼Œè¿™äº›æ–¹æ³•åªå°†å¥–åŠ±åˆ†é…ç»™æœ€ç»ˆçš„éšçŠ¶æ€ï¼ˆlatent stateï¼‰ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹å†…éƒ¨å¤šæ­¥è¿­ä»£æ¨ç†çš„å®Œæ•´è¿‡ç¨‹ã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªâ€œä¿¡ç”¨åˆ†é…ç“¶é¢ˆâ€ï¼ˆcredit assignment bottleneckï¼‰ï¼Œå³å­¦ä¹ ä¿¡å·å¿…é¡»é€šè¿‡å¤šä¸ªå†…éƒ¨å¾ªç¯åå‘ä¼ æ’­ï¼Œä»è€Œå‰Šå¼±äº†è®­ç»ƒæ•ˆç‡ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **RLTT (Reward Latent Thought Trajectories)**ï¼Œä¸€ç§å…¨æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**å¦‚æœä¸€ä¸ª LoopLM åœ¨å¤šä¸ªéšçŠ¶æ€è¿­ä»£ä¸­è¿›è¡Œæ¨ç†ï¼Œé‚£ä¹ˆå­¦ä¹ ä¿¡å·å°±åº”è¯¥å¥–åŠ±æ•´ä¸ªæ¨ç†è½¨è¿¹ï¼Œè€Œä¸ä»…ä»…æ˜¯ç»ˆç‚¹**ã€‚

- **æ ¸å¿ƒæœºåˆ¶**ï¼šRLTT å°†å¥–åŠ±æ²¿ç€æ•´ä¸ªéšçŠ¶æ€è½¨è¿¹è¿›è¡Œåˆ†é…ã€‚å®ƒèšåˆäº†æ¯ä¸ªå†…éƒ¨å¾ªç¯ï¼ˆloopï¼‰äº§ç”Ÿçš„ next-token åˆ†å¸ƒï¼Œå¹¶åˆ©ç”¨è¿™ä¸ªèšåˆåˆ†å¸ƒæ¥å¡‘é€ ç­–ç•¥æ¢¯åº¦ï¼ˆpolicy gradientï¼‰ï¼Œä»è€Œå®ç°äº†æ›´å¯†é›†çš„ä¿¡ç”¨åˆ†é…ã€‚
- **æ— éœ€å¤–éƒ¨éªŒè¯å™¨**ï¼šè¯¥æ–¹æ³•ç›´æ¥åˆ©ç”¨æ¨¡å‹å†…éƒ¨çš„è®¡ç®—ï¼Œä¸ä¾èµ–äºå¤–éƒ¨çš„è¯„åˆ†æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰æ¥è¯„ä¼°ä¸­é—´æ­¥éª¤ï¼Œé¿å…äº†é«˜æ˜‚çš„è®¡ç®—å¼€é”€ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **å¯¹é½æ¨¡å‹æ¶æ„**ï¼šRLTT ä¸ LoopLM çš„å¤šæ­¥å†…éƒ¨è®¡ç®—æœ¬è´¨å®Œç¾å¯¹é½ï¼Œè§£å†³äº† GRPO ç­‰æ–¹æ³•ä¸æ¨¡å‹å†…åœ¨è®¡ç®—é€»è¾‘çš„æ ¹æœ¬æ€§é”™é…ã€‚
- **é«˜æ•ˆä¸”è½»é‡**ï¼šRLTT å¯ä»¥ç›´æ¥æ›¿ä»£ GRPOï¼Œè®¡ç®—å¼€é”€æå°ï¼ˆä¸»è¦æ˜¯å†…å­˜å ç”¨å¢åŠ ï¼‰ï¼Œæ˜“äºé›†æˆã€‚
- **æå‡æ¨ç†æ•ˆç‡**ï¼šé¼“åŠ±æ¨¡å‹åœ¨æ›´æ—©çš„éšçŠ¶æ€å°±æ”¶æ•›åˆ°æ­£ç¡®çš„ç­”æ¡ˆï¼Œå‡å°‘äº†ä¸å¿…è¦çš„â€œè¿‡åº¦æ€è€ƒâ€ï¼ˆoverthinkingï¼‰ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
- **æ•°å­¦æ¨ç†åŸºå‡†**ï¼ˆMathematical Reasoning Benchmarksï¼‰ï¼š
  - `MATH-500`, `AIME24`, `BeyondAIME`, `GSM8k`
- **éæ•°å­¦æ¨ç†åŸºå‡†**ï¼ˆNon-mathematical Reasoning Benchmarksï¼‰ï¼š
  - `ARC-C` (å¸¸è¯†æ¨ç†), `MMLU-ST` (å¤šé¢†åŸŸé—®ç­”), `GPQA` (ç ”ç©¶ç”Ÿçº§åˆ«é—®ç­”), `MBPP` (ä»£ç ç”Ÿæˆ)

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **åŸºç¡€æ¨¡å‹**ï¼šæ‰€æœ‰å®éªŒå‡åŸºäºå¼€æºçš„ **Ouro-2.6B-Thinking** æ¨¡å‹ã€‚
- **è®­ç»ƒæ¡ä»¶**ï¼šä¸¥æ ¼åŒ¹é…è®¡ç®—èµ„æºï¼Œç¡®ä¿ä¸åŸºçº¿æ–¹æ³•ï¼ˆGRPOï¼‰åœ¨ç›¸åŒçš„ `rollout` é¢„ç®—ã€ä¼˜åŒ–è®¾ç½®ã€å¥–åŠ±å‡½æ•°å’Œä¼˜åŠ¿å½’ä¸€åŒ–ä¸‹è¿›è¡Œæ¯”è¾ƒã€‚
- **è¯„ä¼°åè®®**ï¼š
  - **é›¶æ ·æœ¬è¯„ä¼°**ï¼ˆZero-shot evaluationï¼‰
  - **ç¡®å®šæ€§è§£ç **ï¼ˆDeterministic decodingï¼‰
  - **ç²¾ç¡®åŒ¹é…**ï¼ˆExact-match answer parsingï¼‰
  - **å›ºå®šæ¨ç†é¢„ç®—**ï¼ˆFixed inference budgetsï¼‰ï¼Œä¾‹å¦‚ MATH-500 ä½¿ç”¨ 2048 ä¸ª tokenã€‚
- **æ¶ˆèå®éªŒ**ï¼šæµ‹è¯•äº†ä¸åŒçš„ `loop weighting` ç­–ç•¥ï¼ˆUniform, Progressive, Exit-probabilityï¼‰ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- ä¸»è¦å¯¹æ¯”æ–¹æ³•ä¸º **GRPO**ï¼ˆGroup Relative Policy Optimizationï¼‰ï¼Œè¿™æ˜¯å½“å‰ç”¨äº LLM å¼ºåŒ–å­¦ä¹ çš„æ ‡å‡†æ–¹æ³•ã€‚
- åŒæ—¶ä¹ŸæŠ¥å‘Šäº† **SFT**ï¼ˆç›‘ç£å¾®è°ƒï¼‰å’Œå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚ Qwen3, DeepSeekR1ï¼‰çš„ç»“æœä½œä¸ºå‚è€ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
åœ¨ **Ouro-2.6B-Thinking** æ¨¡å‹ä¸Šï¼ŒRLTT ç›¸æ¯” GRPO å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼š

| Benchmark     | GRPO Accuracy | RLTT Accuracy | Improvement |
|---------------|---------------|---------------|-------------|
| **MATH-500**  | 71.6%         | **86.0%**     | **+14.4%**  |
| **AIME24**    | 16.7%         | **33.3%**     | **+16.6%**  |
| **BeyondAIME**| 6.0%          | **16.0%**     | **+10.0%**  |
| **GSM8k**     | 59.7%         | **94.0%**     | **+34.3%**  |

- **å¹³å‡æå‡**ï¼šåœ¨æ•°å­¦åŸºå‡†ä¸Šå¹³å‡æå‡ **+18.8%**ï¼Œåœ¨éæ•°å­¦åŸºå‡†ä¸Šå¹³å‡æå‡ **+6.6%**ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **å…¨é¢è¶…è¶Š**ï¼šRLTT åœ¨æ‰€æœ‰æ•°å­¦å’Œéæ•°å­¦åŸºå‡†ä¸Šå‡æ˜¾è‘—ä¼˜äº GRPOã€‚
- **ç»Ÿè®¡æ˜¾è‘—æ€§**ï¼šé€šè¿‡é…å¯¹ t æ£€éªŒï¼ˆpaired t-testsï¼‰è¯å®ï¼Œæ‰€æœ‰æå‡å‡å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼ˆp < 0.05ï¼‰ã€‚
- **é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›**ï¼šå°½ç®¡ä»…åœ¨æ•°å­¦æ•°æ®ä¸Šè®­ç»ƒï¼ŒRLTT åœ¨éæ•°å­¦ä»»åŠ¡ï¼ˆå¦‚ GPQAï¼‰ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶æ¨ç†èƒ½åŠ›çš„æ³›åŒ–æ€§ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **å¯¹æƒé‡ç­–ç•¥ä¸æ•æ„Ÿ**ï¼šæ— è®ºé‡‡ç”¨ Uniformã€Progressive è¿˜æ˜¯ Exit-probability æƒé‡ï¼ŒRLTT çš„æ€§èƒ½éƒ½ä¿æŒç¨³å®šä¸”è¿œè¶… GRPOã€‚è¿™è¡¨æ˜å…¶æˆåŠŸçš„å…³é”®åœ¨äºâ€œå¥–åŠ±æ•´ä¸ªè½¨è¿¹â€çš„æ€æƒ³æœ¬èº«ï¼Œè€Œéå…·ä½“çš„ä¿¡ç”¨åˆ†é…æ–¹æ¡ˆã€‚
- **è§£ç é•¿åº¦é²æ£’æ€§**ï¼šå³ä½¿åœ¨è¿œè¶…è®­ç»ƒé•¿åº¦ï¼ˆå¦‚ 4096 tokensï¼‰çš„æ¨ç†é¢„ç®—ä¸‹ï¼ŒRLTT ä¾ç„¶ä¿æŒä¼˜åŠ¿ï¼Œè¯æ˜å…¶æœªè¿‡æ‹Ÿåˆç‰¹å®šé•¿åº¦ã€‚
- **æ—©æœŸæ¨ç†æ›´å¼º**ï¼šåœ¨å—é™çš„å¾ªç¯æ¬¡æ•°ï¼ˆå¦‚ 1-2 loopsï¼‰ä¸‹ï¼ŒRLTT çš„æ€§èƒ½è¿œé«˜äº GRPOï¼Œè¯´æ˜å…¶ç¡®å®æå‡äº†æ—©æœŸéšçŠ¶æ€çš„æ¨ç†è´¨é‡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **è¿‡ç¨‹ä¼˜å…ˆè‡³å…³é‡è¦**ï¼šå¯¹äº LoopLMsï¼Œå¼ºåŒ–å­¦ä¹ çš„æˆåŠŸå…³é”®åœ¨äºå¥–åŠ±**æ¨ç†è¿‡ç¨‹**ï¼ˆlatent thought trajectoryï¼‰ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆç»“æœã€‚
2. **æå‡æ¨ç†æ•ˆç‡**ï¼šRLTT ä¿ƒä½¿æ¨¡å‹æ›´å¿«åœ°æ”¶æ•›åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œç”Ÿæˆçš„å“åº”æ›´çŸ­ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘çº¦ 10%ï¼Œä½“ç°äº†æ›´é«˜çš„**token æ•ˆç‡**ã€‚
3. **æä¾›æ›´ä¸°å¯Œçš„æ¢¯åº¦ä¿¡å·**ï¼šåœ¨æœ€å›°éš¾çš„ä»»åŠ¡ï¼ˆå¦‚ AIME24ï¼‰ä¸Šï¼ŒRLTT æ˜¾è‘—æé«˜äº†æ¢¯åº¦ä¿¡å™ªæ¯”ï¼ˆGSNRï¼‰ï¼Œä¸ºæ¨¡å‹æä¾›äº†æ›´ä¸°å¯Œã€æ›´ä¸€è‡´çš„å­¦ä¹ ä¿¡å·ã€‚
4. **å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›**ï¼šRLTT å­¦ä¹ åˆ°çš„æ˜¯ä¸€ç§æ›´é€šç”¨çš„ã€é«˜æ•ˆçš„æ¨ç†æ¨¡å¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¿ç§»åˆ°æœªè§è¿‡çš„éæ•°å­¦ä»»åŠ¡ä¸­ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å†…å­˜å¼€é”€**ï¼šéœ€è¦å­˜å‚¨æ¯ä¸ªå¾ªç¯çš„å¯¹æ•°æ¦‚ç‡ï¼ˆlog-probabilitiesï¼‰ï¼Œå¢åŠ äº†å†…å­˜å ç”¨ï¼Œåœ¨å›ºå®š GPU å†…å­˜ä¸‹ä¼šé™åˆ¶æ¯æ¬¡ä¼˜åŒ–æ­¥æ‰€èƒ½å¤„ç†çš„ token æ•°é‡ã€‚
- **æ¶æ„ç‰¹å¼‚æ€§**ï¼šç›®å‰ä»…é€‚ç”¨äºå…·æœ‰æ˜¾å¼å¤šæ­¥éšçŠ¶æ€è¿­ä»£çš„ **Looped Architectures**ï¼Œæ— æ³•ç›´æ¥åº”ç”¨äºæ ‡å‡†çš„éå¾ªç¯ LLMã€‚
- **å›ºå®šå¾ªç¯æ·±åº¦**ï¼šå®éªŒä¸­ä½¿ç”¨äº†å›ºå®šçš„å¾ªç¯æ·±åº¦ï¼Œç‰ºç‰²äº†æ¨¡å‹åŸç”Ÿçš„è‡ªé€‚åº”æå‰é€€å‡ºï¼ˆadaptive early-exitï¼‰èƒ½åŠ›ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼€å‘**å†…å­˜æ›´é«˜æ•ˆçš„å®ç°**ï¼Œä»¥ç¼“è§£å†…å­˜ç“¶é¢ˆã€‚
- æ¢ç´¢å°† RLTT ä¸**è‡ªé€‚åº”ç»ˆæ­¢æœºåˆ¶**ç›¸ç»“åˆï¼Œæ¢å¤æŒ‰éœ€åˆ†é…è®¡ç®—èµ„æºçš„èƒ½åŠ›ã€‚
- å°†è¯¥æ¡†æ¶æ‰©å±•åˆ°å…¶ä»–ç±»å‹çš„**æ½œå˜é‡æ¨ç†æ¨¡å‹**ï¼ˆlatent reasoning modelsï¼‰ã€‚

</details>

---

### 14. [Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models](https://arxiv.org/abs/2602.11057)

**Authors**: Xinyu Yuan, Yan Qiao, Zonghui Wang, Wenzhi Chen  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.11057v1  

#### Abstract
The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in ba...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDivide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹ **Multi-Commodity Flow (MCF)** é—®é¢˜ï¼Œå³åœ¨å¤æ‚ç½‘ç»œæ‹“æ‰‘ä¸­ä¸ºå¤šä¸ªå•†å“æµï¼ˆå¦‚ç½‘ç»œæµé‡ã€ç‰©æµéœ€æ±‚ï¼‰åˆ†é…è·¯å¾„ä»¥ä¼˜åŒ–å…¨å±€ç›®æ ‡ï¼ˆå¦‚æœ€å°åŒ–æœ€å¤§é“¾è·¯åˆ©ç”¨ç‡ MLUã€æœ€å¤§åŒ–æ€»æµé‡ç­‰ï¼‰ã€‚è¯¥é—®é¢˜æ˜¯ç»„åˆä¼˜åŒ–ä¸­çš„ç»å…¸éš¾é¢˜ï¼Œå¹¿æ³›åº”ç”¨äºäº¤é€šã€é€šä¿¡ã€äº‘è®¡ç®—ç­‰é¢†åŸŸã€‚

ä¼ ç»Ÿæ–¹æ³•ä¾èµ– **Linear Programming (LP)** æ±‚è§£å™¨ï¼Œåœ¨å¤§è§„æ¨¡ç³»ç»Ÿä¸­é¢ä¸´ä¸¥é‡å¯æ‰©å±•æ€§ç“¶é¢ˆï¼›è€Œç°æœ‰çš„ **Machine Learning (ML)** æ–¹æ³•è™½ç„¶é€Ÿåº¦å¿«ï¼Œä½†åœ¨æ³›åŒ–èƒ½åŠ›ã€å‡†ç¡®æ€§åŠå¯¹æœªè§ç¯å¢ƒçš„é²æ£’æ€§æ–¹é¢è¡¨ç°ä¸ä½³ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ï¼šPRAM
ä½œè€…æå‡º **PRAM (Partitioned Resource Allocation with Multimodal Language Models)** â€”â€”é¦–ä¸ªåŸºäº **Multimodal Language Models (MLMs)** çš„ç«¯åˆ°ç«¯ MCF æ±‚è§£æ¡†æ¶ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼šâ€œåˆ†è€Œæ²»ä¹‹ï¼Œå†åè°ƒâ€
- **Divide (åˆ’åˆ†)**ï¼šå°†åŸé—®é¢˜æŒ‰æºèŠ‚ç‚¹åˆ’åˆ†ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼ˆæ¯ä¸ªå­ä»»åŠ¡å¤„ç†ä»åŒä¸€æºå‡ºå‘çš„æ‰€æœ‰æµï¼‰ï¼Œä»è€Œé™ä½å•ä¸ªå­é—®é¢˜è§„æ¨¡ã€‚
- **Harmonize (åè°ƒ)**ï¼šé€šè¿‡è½»é‡çº§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æœºåˆ¶å®ç°å„å­ä»»åŠ¡é—´çš„ååŒä¼˜åŒ–ï¼Œç¡®ä¿å…¨å±€ä¸€è‡´æ€§ã€‚
- **Conquer (æ±‚è§£)**ï¼šåˆ©ç”¨é¢„è®­ç»ƒ MLM ä½œä¸ºå…±äº«â€œä»£ç†â€æ¨¡å‹ï¼Œç›´æ¥æ¨ç†ç”Ÿæˆé«˜è´¨é‡è·¯å¾„æƒé‡é…ç½®ã€‚

---

### åˆ›æ–°ç‚¹ä¸ä¼˜åŠ¿

| åˆ›æ–°ç‚¹ | æè¿° | ä¼˜åŠ¿ |
|--------|------|------|
| âœ… **é¦–æ¬¡å¼•å…¥ MLM è§£å†³ MCF** | å°† MCF å»ºæ¨¡ä¸ºå¤šæ¨¡æ€è¾“å…¥ä¸‹çš„å†³ç­–ä»»åŠ¡ï¼Œå›¾åƒè¡¨ç¤ºå­æ‹“æ‰‘ï¼Œæ–‡æœ¬æè¿°éœ€æ±‚ç»Ÿè®¡ä¿¡æ¯ã€‚ | é¿å…æ‰‹åŠ¨è®¾è®¡ä¸“ç”¨ DNN æ¶æ„ï¼ˆå¦‚ GNNï¼‰ï¼Œæå‡é€šç”¨æ€§å’Œè¿ç§»èƒ½åŠ›ã€‚ |
| âœ… **è½»é‡çº§è‡ªé€‚åº”æ¡†æ¶** | ç»“åˆ LoRA å’Œå‰ç¼€ä¸Šä¸‹æ–‡åµŒå…¥ï¼ˆlearned global contextï¼‰ï¼Œå®ç°å‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚ | æ˜¾è‘—å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œæ”¯æŒå¿«é€Ÿéƒ¨ç½²ä¸æ›´æ–°ã€‚ |
| âœ… **å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åè°ƒæœºåˆ¶** | ä½¿ç”¨ counterfactual policy gradients ä¼°è®¡ä¸ªä½“è´¡çŒ®ï¼Œé©±åŠ¨å…¨å±€æ”¶æ•›ã€‚ | ç†è®ºä¸Šè¯æ˜èƒ½æ¨¡æ‹Ÿæ¢¯åº¦ä¸‹é™è¿‡ç¨‹ï¼Œé€¼è¿‘æœ€ä¼˜è§£ã€‚ |
| âœ… **ç†è®ºä¿éšœ** | åœ¨åˆç†å‡è®¾ä¸‹ï¼Œè¯æ˜ PRAM æ”¶æ•›è‡³å±€éƒ¨æœ€ä¼˜ï¼Œå¹¶èƒ½å†…éƒ¨æ¨¡æ‹Ÿ GD æ›´æ–°æ­¥éª¤ã€‚ | æä¾›ä¼˜äºçº¯é»‘ç®± ML æ–¹æ³•çš„æ€§èƒ½ä¿è¯ã€‚ |

---

ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿ï¼š
- **é€Ÿåº¦è¿œè¶… LP**ï¼šè¿è¡Œæ—¶é—´å¿« 10â€“100 å€ï¼›
- **ç²¾åº¦æ¥è¿‘æœ€ä¼˜**ï¼šå¹³å‡æ€§èƒ½å·®è· <8%ï¼›
- **å¼ºé²æ£’æ€§**ï¼šé¢å¯¹é“¾è·¯æ•…éšœæˆ–æµé‡çªå¢æ—¶æ€§èƒ½ä¸‹é™ <10%ï¼›
- **æ— éœ€é‡æ–°è®­ç»ƒå³å¯æ³›åŒ–**ï¼šé€‚ç”¨äºä¸åŒæ‹“æ‰‘ä¸éœ€æ±‚åˆ†å¸ƒã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†

#### å®é™…ä¸–ç•Œæ•°æ®é›†ï¼ˆSmall-scaleï¼‰
| åç§° | èŠ‚ç‚¹æ•° | é“¾è·¯æ•° | åº”ç”¨åœºæ™¯ |
|------|--------|--------|----------|
| MetaDB / MetaWEB | 4 / 8 | 12 / 56 | ç¤¾äº¤ç½‘ç»œé›†ç¾¤ |
| Abilene | 12 | 30 | ç¾å›½å­¦æœ¯éª¨å¹²ç½‘ |
| CERNET | 14 | 32 | ä¸­å›½æ•™è‚²ç§‘ç ”ç½‘ |
| GEANT | 23 | 74 | æ¬§æ´²ç ”ç©¶ç½‘ç»œ |

#### å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼ˆLarge-scaleï¼‰
| åç§° | èŠ‚ç‚¹æ•° | é“¾è·¯æ•° | æ¥æº |
|------|--------|--------|-------|
| GtsCe | 149 | 386 | Internet Topology Zoo |
| Colt | 153 | 354 | åŒä¸Š |
| UsCarrier | 158 | 378 | åŒä¸Š |
| Cogentco | 197 | 486 | åŒä¸Š |
| Kdl | 754 | 1,790 | åŒä¸Š |

> æ‰€æœ‰åˆæˆæ•°æ®é‡‡ç”¨ Gravityã€Poisson å’Œ Bimodal æ¨¡å‹ç”Ÿæˆï¼Œæ¨¡æ‹ŸçœŸå®æµé‡ç‰¹å¾ã€‚

---

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### ä¸‰å¤§ä¼˜åŒ–ç›®æ ‡
1. **Maximum Link Utilization (MLU)**ï¼šæœ€å°åŒ–æœ€é«˜é“¾è·¯è´Ÿè½½ï¼Œå¢å¼ºå®¹é”™èƒ½åŠ›ã€‚
2. **Maximum Total Flow (MTF)**ï¼šæœ€å¤§åŒ–æˆåŠŸä¼ è¾“æ€»é‡ã€‚
3. **Maximum Concurrent Flow (MCF)**ï¼šæœ€å¤§åŒ–å…¬å¹³å¹¶å‘ååç‡ã€‚

> æ‰€æœ‰ç»“æœå½’ä¸€åŒ–ä¸ºç›¸å¯¹äº LP æœ€ä¼˜è§£çš„æ¯”ä¾‹ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰ã€‚

#### è¾“å…¥è¡¨ç¤ºæ–¹å¼
- **è§†è§‰æ¨¡æ€**ï¼šä½¿ç”¨ Matplotlib ç»˜åˆ¶ä»æŸæºèŠ‚ç‚¹å‡ºå‘çš„æ‰€æœ‰å€™é€‰è·¯å¾„æ„æˆçš„å­å›¾ï¼ˆPNG å›¾åƒï¼‰ï¼›
- **æ–‡æœ¬æ¨¡æ€**ï¼šæä¾›è¿‘æœŸæµé‡ã€å†å²å‡å€¼ç­‰ç»Ÿè®¡æ•°æ®ï¼›
- **æç¤ºå·¥ç¨‹ï¼ˆPromptingï¼‰**ï¼šæ„é€ ç»“æ„åŒ– prompt å¼•å¯¼ MLM è¾“å‡º `[path_weight_1, ..., path_weight_k]` å½¢å¼çš„è·¯å¾„æƒé‡å‘é‡ã€‚

#### å¾®è°ƒç­–ç•¥
- ä½¿ç”¨ **multi-agent RL** è¿›è¡Œ fine-tuningï¼›
- é‡‡ç”¨ **counterfactual baseline** è®¡ç®—æ¯ä¸ª agent çš„ advantageï¼›
- å¯è®­ç»ƒæ¨¡å—ä»…é™äº LoRA é€‚é…å™¨ä¸ context embeddingsï¼Œä¿æŒä¸»å¹²å†»ç»“ã€‚

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | æ˜¯å¦é¢„æµ‹ |
|------|------|---------|
| **LP (Gurobi)** | æ•°å­¦è§„åˆ’ | âœ…ï¼ˆç†æƒ³æœªæ¥å·²çŸ¥ï¼‰ |
| **LP (pred)** | æ•°å­¦è§„åˆ’ + ç§»åŠ¨å¹³å‡é¢„æµ‹ | âŒ |
| **POP** | åˆ†è§£ + å¹¶è¡Œ LP | âœ… |
| **POP (pred)** | åŒä¸Š + é¢„æµ‹ | âŒ |
| **LP-top** | å¯å‘å¼ï¼ˆTop 10% æµç”¨ LPï¼‰ | âœ… |
| **LP-top (pred)** | åŒä¸Š + é¢„æµ‹ | âŒ |
| **DRL** | æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆPPOï¼‰ | âŒ |
| **HARP** | GNN-based ML | âŒ |
| **Aether** | Graph Transformer + MARL | âŒ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Figure 6 & 7ï¼‰

| æŒ‡æ ‡ | PRAM è¡¨ç° | å¯¹æ¯”è¯´æ˜ |
|------|-----------|----------|
| **MLU (å½’ä¸€åŒ–)** | å¹³å‡ 0.92â€“0.98 | æ¥è¿‘ LP æœ€ä¼˜ï¼ˆ1.0ï¼‰ï¼Œä¼˜äºæ‰€æœ‰ ML æ–¹æ³•ï¼ˆDRL: ~0.75, HARP: ~0.88ï¼‰ |
| **MTF (å½’ä¸€åŒ–)** | >90% LP æ€§èƒ½ | æ˜¾è‘—ä¼˜äº HARP (+16.6%) å’Œ Aether (+7.3%) |
| **MCF (å½’ä¸€åŒ–)** | >90% LP æ€§èƒ½ | è¶…å‡º HARP (+24.8%) å’Œ Aether (+13.5%) |
| **è¿è¡Œæ—¶é—´** | **å¿« 10â€“100Ã—** | åœ¨ Kdl æ‹“æ‰‘ä¸Šï¼š<25 ç§’ vs. LP >2500 ç§’ |
| **é²æ£’æ€§ï¼ˆé“¾è·¯å¤±æ•ˆï¼‰** | MLU ä¸Šå‡ <15%ï¼ˆÏƒ=2ï¼‰ | æ˜æ˜¾ä¼˜äºå…¶ä»– ML æ–¹æ³•ï¼ˆä¸Šå‡ 20â€“30%ï¼‰ |

> ç‰¹åˆ«åœ°ï¼Œåœ¨ CERNET å’Œ GEANT ä¸Šï¼ŒPRAM çš„ MLU ç”šè‡³ **ä½äº LP**ï¼ˆåˆ†åˆ«ä½ 21% å’Œ 45%ï¼‰ï¼Œè¡¨æ˜å…¶åœ¨ç‰¹å®šå‡¸æ€§æ¡ä»¶ä¸‹æ›´å…·ç¨³å®šæ€§ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ï¼ˆ1ï¼‰æ˜¯å¦è¿›è¡Œé—®é¢˜åˆ’åˆ†ï¼Ÿ
- **æ— åˆ’åˆ†ç‰ˆæœ¬ï¼ˆPRAM w/o par.ï¼‰**ï¼šå‚æ•°é‡éšç½‘ç»œè§„æ¨¡çˆ†ç‚¸å¢é•¿ï¼ˆKdl è¾¾ 31,600 MBï¼‰ï¼Œæ— æ³•å®é™…è¿è¡Œï¼›
- **PRAM**ï¼šå‚æ•°é‡ç¨³å®šåœ¨æ•°ç™¾ MB å†…ï¼Œå…·å¤‡è‰¯å¥½å¯æ‰©å±•æ€§ã€‚

> âœ… **ç»“è®º**ï¼šåˆ’åˆ†æ˜¯åº”å¯¹å¤§è§„æ¨¡ MCF çš„å…³é”®ã€‚

#### ï¼ˆ2ï¼‰å„ç»„ä»¶ä½œç”¨åˆ†æï¼ˆFigure 9bï¼‰
| å˜ä½“ | MLU ä¸‹é™å¹…åº¦ï¼ˆvs. å®Œæ•´ PRAMï¼‰ |
|------|-------------------------------|
| w/o MLM | â†“ ~15% |
| w/o context | â†“ ~10% |
| w/o LoRA | â†“ ~8% |
| w/o MARL | â†“ ~12% |
| w/o partition | ä¸å¯è¡Œï¼ˆè¿‡å¤§ï¼‰ |

> âœ… æ‰€æœ‰ç»„ä»¶å‡æœ‰æ˜¾è‘—è´¡çŒ®ï¼Œå°¤å…¶æ˜¯ **MLM æ¨ç†èƒ½åŠ›** ä¸ **MARL ååŒæœºåˆ¶**ã€‚

#### ï¼ˆ3ï¼‰å¯è§†åŒ–åˆ†æï¼ˆFigure 11ï¼‰
- Cross-attention map æ˜¾ç¤º context embeddings èƒ½æœ‰æ•ˆå…³æ³¨ä¸ MCF ç›¸å…³çš„å…³é”®è¯ï¼ˆå¦‚ "Flow", "Capacity"ï¼‰ï¼›
- å­¦ä¹ åˆ°çš„ prototypes æˆåŠŸå¯¹é½ä»»åŠ¡è¯­ä¹‰ç©ºé—´ã€‚

> âœ… è¡¨æ˜ context æ¨¡å—ç¡®å®æ•æ‰åˆ°äº†å…¨å±€åè°ƒä¿¡å·ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. âœ… **MLM å…·å¤‡è§£å†³å¤æ‚èµ„æºåˆ†é…é—®é¢˜çš„æ½œåŠ›**  
   é¢„è®­ç»ƒ MLMs å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†ä¸æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å°‘é‡å¾®è°ƒåèƒœä»» MCF å†³ç­–ä»»åŠ¡ã€‚

2. âœ… **â€œåˆ’åˆ† + åè°ƒâ€èŒƒå¼æœ‰æ•ˆå¹³è¡¡æ•ˆç‡ä¸è´¨é‡**  
   é€šè¿‡å°†å¤§é—®é¢˜åˆ†è§£å¹¶ç”±å…±äº«ä»£ç†å¹¶è¡Œæ±‚è§£ï¼Œå†é€šè¿‡ MARL å®ç°å…¨å±€ä¸€è‡´ï¼Œå®ç°äº† **scalability ä¸ optimality çš„åŒèµ¢**ã€‚

3. âœ… **PRAM åœ¨å®è·µä¸­å…¼å…·é«˜é€Ÿã€é«˜è´¨ã€é«˜é²æ£’æ€§**  
   - é€Ÿåº¦ï¼šæ¯” LP å¿« 1â€“2 ä¸ªæ•°é‡çº§ï¼›
   - ç²¾åº¦ï¼šæ¥è¿‘æœ€ä¼˜è§£ï¼ˆ<8% gapï¼‰ï¼›
   - æ³›åŒ–ï¼šå¯¹æ‹“æ‰‘å˜åŒ–ã€æµé‡æ³¢åŠ¨ã€é“¾è·¯å¤±æ•ˆå‡è¡¨ç°å‡ºå¼ºé²æ£’æ€§ã€‚

4. âœ… **ç†è®ºä¸Šæœ‰ä¿éšœ**  
   åœ¨ MCF ç›®æ ‡å‡½æ•°æ»¡è¶³å‡¸/æ‹Ÿå‡¹æ€§è´¨çš„å‰æä¸‹ï¼ŒPRAM å¯è¯æ˜æ”¶æ•›ï¼Œå¹¶èƒ½å†…éƒ¨æ¨¡æ‹Ÿ GD æ›´æ–°è¿‡ç¨‹ï¼ˆTheorem 1 & 2ï¼‰ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§

| å±€é™ | è¯´æ˜ |
|------|------|
| ğŸ”´ **å¾®è°ƒä»è¾ƒè€—èµ„æº** | å°½ç®¡ä½¿ç”¨ LoRA å’Œ truncationï¼Œå®Œæ•´ fine-tuning ä»éœ€ GPU æ—¶é—´ï¼Œä¸é€‚åˆå®æ—¶åŠ¨æ€é‡è®­ã€‚ |
| ğŸ”´ **è§†è§‰ç¼–ç å¯èƒ½å¼•å…¥åå·®** | å­å›¾ç»˜åˆ¶æ–¹å¼ï¼ˆå¦‚é‡å è¾¹ã€å¸ƒå±€ç®—æ³•ï¼‰å¯èƒ½å½±å“ MLM è§†è§‰ç†è§£æ•ˆæœã€‚ |
| ğŸ”´ **éè‡ªå›å½’ç”Ÿæˆé™åˆ¶è¡¨è¾¾åŠ›** | å½“å‰é‡‡ç”¨éè‡ªå›å½’æ–¹å¼è¾“å‡ºè·¯å¾„æƒé‡ï¼Œéš¾ä»¥å»ºæ¨¡å¤æ‚ä¾èµ–å…³ç³»ã€‚ |
| ğŸ”´ **ä¾èµ–é¢„è®­ç»ƒ MLM è´¨é‡** | è‹¥ MLM ç¼ºä¹æ•°å€¼æ¨ç†æˆ–å›¾ç†è§£èƒ½åŠ›ï¼Œåˆ™æ€§èƒ½å—é™ã€‚ |

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. **æ¢ç´¢æ›´é«˜æ•ˆçš„é€‚é…æŠ€æœ¯**  
   å¦‚ç»“åˆ **Adapter**, **BitFit**, æˆ– **Prompt Tuning** è¿›ä¸€æ­¥å‹ç¼©å‚æ•°é‡ã€‚

2. **å¼•å…¥è‡ªå›å½’å»ºæ¨¡æœºåˆ¶**  
   å€Ÿé‰´ Chain-of-Thought æˆ– Plan-and-Execute æ¡†æ¶ï¼Œè®© MLM è‡ªä¸»è¿­ä»£ä¼˜åŒ–æ–¹æ¡ˆã€‚

3. **æ„å»º MCF ä¸“ç”¨é¢„è®­ç»ƒä»»åŠ¡**  
   åœ¨å¤§è§„æ¨¡ç½‘ç»œæ•°æ®ä¸Šè¿›è¡Œ in-context pretrainingï¼Œæå‡é¢†åŸŸé€‚åº”æ€§ã€‚

4. **æ‹“å±•è‡³åŠ¨æ€ä¸æ—¶åº MCF åœºæ™¯**  
   æ”¯æŒè¿ç»­æ—¶é—´çª—å£ä¸‹çš„è”åˆè°ƒåº¦ä¸é¢„æµ‹ã€‚

5. **é›†æˆåå¤„ç† refinement æ¨¡å—**  
   å¦‚ç»“åˆ LP å±€éƒ¨ç²¾è°ƒï¼ˆpost-solution refinementï¼‰ï¼Œè¿›ä¸€æ­¥é€¼è¿‘æœ€ä¼˜ã€‚

---

> ğŸ“Œ **ä»£ç å¼€æºåœ°å€**ï¼š[https://github.com/Y-debug-sys/PRAM](https://github.com/Y-debug-sys/PRAM)  
> ğŸ“Œ **ä¼¦ç†å£°æ˜**ï¼šæœªä½¿ç”¨ä»»ä½•æ•æ„Ÿç”¨æˆ·æ•°æ®ï¼Œä»…åŸºäºèšåˆæµé‡çŸ©é˜µè¿›è¡Œç ”ç©¶ã€‚

</details>

---

### 15. [TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139)

**Authors**: Jingang Qu, David Holzm\"uller, Ga\"el Varoquaux, Marine Le Morvan  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.11139v1  

#### Abstract
Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classificatio...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# TabICLv2: A better, faster, scalable, and open tabular foundation model è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿçš„æ¢¯åº¦æå‡æ ‘ï¼ˆå¦‚ XGBoostã€LightGBMï¼‰é•¿æœŸåœ¨è¡¨æ ¼æ•°æ®é¢„æµ‹ä»»åŠ¡ä¸Šå æ®ä¸»å¯¼åœ°ä½ã€‚å°½ç®¡è¿‘æœŸå‡ºç°çš„ **Tabular Foundation Models (TFMs)** å¦‚ TabPFN å’Œ TabICL å±•ç°äº†æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- **å¯æ‰©å±•æ€§å·®**ï¼šéš¾ä»¥æœ‰æ•ˆå¤„ç†ç™¾ä¸‡çº§æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚
- **æ¨ç†é€Ÿåº¦æ…¢**ï¼šå°¤å…¶æ˜¯åœ¨é•¿åºåˆ—ï¼ˆå¤§é‡è®­ç»ƒæ ·æœ¬ï¼‰åœºæ™¯ä¸‹ï¼Œæ³¨æ„åŠ›æœºåˆ¶å­˜åœ¨â€œ**attention fading**â€é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
- **ä¾èµ–è°ƒå‚ä¸é›†æˆ**ï¼šæœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆå¦‚ RealTabPFN-2.5ï¼‰éœ€è¦å¤æ‚çš„è¶…å‚æ•°è°ƒä¼˜ï¼ˆtuningï¼‰ã€åŠ æƒé›†æˆï¼ˆensemblingï¼‰ç”šè‡³åœ¨çœŸå®æ•°æ®ä¸Šå¾®è°ƒï¼ˆfine-tuningï¼‰ï¼Œæˆæœ¬é«˜æ˜‚ä¸”ä¸å¼€æ”¾ã€‚
- **é¢„è®­ç»ƒæ•°æ®å¤šæ ·æ€§ä¸è¶³**ï¼šåˆæˆæ•°æ®ç”Ÿæˆå™¨çš„è®¾è®¡é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°
æœ¬æ–‡æå‡ºäº† **TabICLv2**ï¼Œä¸€ä¸ªå…¨æ–°çš„ã€å¼€æºçš„è¡¨æ ¼åŸºç¡€æ¨¡å‹ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°å»ºç«‹åœ¨ä¸‰å¤§æ”¯æŸ±ä¹‹ä¸Šï¼š

#### ï¼ˆ1ï¼‰æ–°å‹åˆæˆæ•°æ®ç”Ÿæˆå¼•æ“ (Novel Synthetic Data Generation Engine)
- è®¾è®¡äº†ä¸€ä¸ªé«˜åº¦å¤šæ ·åŒ–ã€å¯æ§çš„åˆæˆæ•°æ®å…ˆéªŒï¼ˆpriorï¼‰ï¼Œæ—¨åœ¨æœ€å¤§åŒ–é¢„è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚
- å¼•å…¥äº†æ–°çš„éšæœºå›¾ç”Ÿæˆæœºåˆ¶ï¼ˆâ€œrandom Cauchy graphâ€ï¼‰ï¼Œæ”¯æŒæ›´å¤æ‚çš„éæ ‘çŠ¶ä¾èµ–å…³ç³»ã€‚
- ä¸°å¯Œäº†éšæœºå‡½æ•°ç±»å‹ï¼ˆå…±8ç§ï¼‰ï¼ŒåŒ…æ‹¬ MLPã€æ ‘é›†æˆã€é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ã€åˆ†æ®µå‡½æ•°ç­‰ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒå¹³æ»‘æ€§å’Œå½’çº³åç½®çš„æ•°æ®æ¨¡å¼ã€‚
- é‡‡ç”¨ç›¸å…³é‡‡æ ·ï¼ˆcorrelated samplingï¼‰æœºåˆ¶ï¼Œä½¿ç”Ÿæˆçš„æ•°æ®å±æ€§ï¼ˆå¦‚ç±»åˆ«åŸºæ•°ï¼‰ä¹‹é—´å…·æœ‰ç›¸å…³æ€§ï¼Œæ›´è´´è¿‘çœŸå®ä¸–ç•Œæ•°æ®ã€‚

#### ï¼ˆ2ï¼‰æ¶æ„åˆ›æ–° (Architectural Innovations)
- **é‡å¤ç‰¹å¾åˆ†ç»„ (Repeated Feature Grouping)**ï¼šé€šè¿‡å¾ªç¯ç§»ä½å°†æ¯ä¸ªç‰¹å¾åˆ†é…åˆ°å¤šä¸ªç»„ä¸­ï¼Œæ‰“ç ´äº†ç‰¹å¾å¯¹ç§°æ€§ï¼Œç¼“è§£äº†è¡¨ç¤ºåç¼©ï¼ˆrepresentation collapseï¼‰é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™äº†æ‰€æœ‰ç‰¹å¾çš„ç²¾ç»†ä¿¡æ¯ã€‚
- **ç›®æ ‡æ„ŸçŸ¥åµŒå…¥ (Target-aware Embedding)**ï¼šåœ¨æ—©æœŸé˜¶æ®µå°±å°†æ ‡ç­¾/ç›®æ ‡å€¼åµŒå…¥åˆ°æ‰€æœ‰ç‰¹å¾è¡¨ç¤ºä¸­ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ç‰¹å¾ä¸ç›®æ ‡å…³è”æ€§çš„å­¦ä¹ ã€‚
- **æŸ¥è¯¢æ„ŸçŸ¥å¯æ‰©å±• Softmax (Query-aware Scalable Softmax, QASSMax)**ï¼š
  - è¿™æ˜¯è§£å†³ **long-context generalization** çš„æ ¸å¿ƒåˆ›æ–°ã€‚
  - åœ¨æ ‡å‡† Softmax åŸºç¡€ä¸Šå¼•å…¥äº†åŸºäº `log n`ï¼ˆnä¸ºä¸Šä¸‹æ–‡é•¿åº¦ï¼‰çš„åŠ¨æ€ç¼©æ”¾ï¼Œå¹¶ç»“åˆæŸ¥è¯¢æ„ŸçŸ¥çš„é—¨æ§æœºåˆ¶ã€‚
  - å…¬å¼ï¼š`q_hi = q_hi Â· MLP_base(log n)_hi : (1 + tanh(MLP_gate(q_h)))`
  - æœ‰æ•ˆç¼“è§£äº†â€œattention fadingâ€ï¼Œä½¿æ¨¡å‹èƒ½ä»æ•°ä¸‡ç”šè‡³æ•°åä¸‡çš„è®­ç»ƒæ ·æœ¬ä¸­æœ‰æ•ˆå­¦ä¹ ã€‚

#### ï¼ˆ3ï¼‰ä¼˜åŒ–çš„é¢„è®­ç»ƒåè®® (Optimized Pretraining Protocols)
- **ä¸‰é˜¶æ®µæ¸è¿›å¼é¢„è®­ç»ƒ**ï¼šé€æ­¥å¢åŠ é¢„è®­ç»ƒæ•°æ®é›†çš„å¤§å°ï¼ˆä»1Kåˆ°60Kæ ·æœ¬ï¼‰ï¼Œä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°é€‚åº”å¤§è§„æ¨¡æ•°æ®ã€‚
- **é‡‡ç”¨ Muon ä¼˜åŒ–å™¨**ï¼šæ›¿ä»£å¸¸ç”¨çš„ AdamWï¼Œç»“åˆè°¨æ…æƒé‡è¡°å‡ï¼ˆcautious weight decayï¼‰ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚
- **é«˜æ•ˆçš„æ¨ç†ä¼˜åŒ–**ï¼šå®ç°äº†ç£ç›˜å¸è½½ï¼ˆdisk offloadingï¼‰ï¼Œä½¿å¾—åœ¨ä»…éœ€ 50GB GPU å†…å­˜çš„æƒ…å†µä¸‹å³å¯å¤„ç†ç™¾ä¸‡çº§æ ·æœ¬çš„è¡¨æ ¼ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ€§èƒ½æ›´å¼º**ï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„ RealTabPFN-2.5ï¼ˆè¯¥æ¨¡å‹ç»è¿‡è°ƒå‚ã€é›†æˆå’Œå¾®è°ƒï¼‰ã€‚
- **é€Ÿåº¦å¿«å¾—å¤š**ï¼šåœ¨ H100 GPU ä¸Šï¼Œå¤„ç† 50K æ ·æœ¬æ—¶æ¯” TabPFN-2.5 å¿« 10.6 å€ï¼›åœ¨ CPU ä¸Šå¿« 11.8 å€ã€‚
- **å¯æ‰©å±•æ€§å¥½**ï¼šèƒ½åŸç”Ÿå¤„ç†ç™¾ä¸‡çº§æ ·æœ¬çš„æ•°æ®é›†ï¼Œè€Œæ— éœ€æ£€ç´¢ï¼ˆretrievalï¼‰æˆ–è’¸é¦ï¼ˆdistillationï¼‰ç­‰å¤æ‚ç­–ç•¥ã€‚
- **å®Œå…¨å¼€æº**ï¼šæ‰¿è¯ºå…¬å¼€æ¨ç†ä»£ç ã€æ¨¡å‹æƒé‡ã€åˆæˆæ•°æ®å¼•æ“å’Œé¢„è®­ç»ƒä»£ç ï¼Œæ¨åŠ¨ç¤¾åŒºå‘å±•ã€‚
- **å¼€ç®±å³ç”¨**ï¼šæ— éœ€ä»»ä½•è°ƒå‚æˆ–å¾®è°ƒï¼Œç›´æ¥ä½¿ç”¨é»˜è®¤é…ç½®å³å¯è·å¾—é¡¶å°–æ€§èƒ½ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **TabArena (Erickson et al., 2025)**ï¼šåŒ…å« 51 ä¸ªæ•°æ®é›†ï¼ˆ38 åˆ†ç±»ï¼Œ13 å›å½’ï¼‰ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å°åˆ°ä¸­ç­‰è§„æ¨¡æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
- **TALENT (Ye et al., 2024)**ï¼šåŒ…å« 300 ä¸ªæ•°æ®é›†ï¼ˆ120 äºŒåˆ†ç±»ï¼Œ80 å¤šåˆ†ç±»ï¼Œ100 å›å½’ï¼‰ï¼Œæ•°æ®é‡æ›´å¤§ï¼Œç”¨äºå…¨é¢è¯„ä¼°ã€‚
- **åˆæˆæ•°æ®**ï¼šæ‰€æœ‰é¢„è®­ç»ƒæ•°æ®å‡ä¸ºåˆæˆç”Ÿæˆï¼Œæœªä½¿ç”¨ä»»ä½•çœŸå®æ•°æ®è¿›è¡Œé¢„è®­ç»ƒã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Improvability**ï¼šè¡¡é‡æ–¹æ³•ç›¸å¯¹äºæœ€ä½³æ–¹æ³•çš„ç›¸å¯¹è¯¯å·®å·®è·ï¼Œæ•°å€¼è¶Šä½è¶Šå¥½ã€‚è¿™æ˜¯æœ¬æ–‡çš„ä¸»è¦æŠ¥å‘ŠæŒ‡æ ‡ã€‚
  - **Elo Rating**ï¼šåŸºäºæˆå¯¹æ¯”è¾ƒçš„è¯„åˆ†ç³»ç»Ÿï¼Œåˆ†æ•°è¶Šé«˜è¶Šå¥½ã€‚
  - **Average Rank**ï¼šåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å¹³å‡æ’åï¼Œæ•°å€¼è¶Šä½è¶Šå¥½ã€‚
  - **Runtime**ï¼šè®­ç»ƒå’Œæ¨ç†æ—¶é—´ï¼Œè¡¡é‡æ•ˆç‡ã€‚
- **å®éªŒè®¾ç½®**ï¼š
  - å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨ 8 æˆ– 32 ä¸ªä¼°è®¡å™¨ï¼ˆensembleï¼‰è¿›è¡Œé¢„æµ‹ã€‚
  - æ‰€æœ‰ç»“æœå‡é€šè¿‡äº¤å‰éªŒè¯æˆ–ç»™å®šçš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†è¿›è¡Œè¯„ä¼°ã€‚
  - TabICLv2 çš„æ¨ç†åœ¨ H100 GPU ä¸Šè¿›è¡Œï¼Œå…¶ä»–æ–¹æ³•çš„æ—¶é—´æ•°æ®æ¥è‡ªåŸºå‡†æµ‹è¯•æŠ¥å‘Šã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **ä¼ ç»Ÿæ¨¡å‹**ï¼šXGBoost, LightGBM, CatBoost, RandomForest, ExtraTrees ç­‰ã€‚
- **æ·±åº¦å­¦ä¹ æ¨¡å‹**ï¼šMLP, FT-Transformer, ResNet, ModernNCA ç­‰ã€‚
- **è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹  (AutoML)**ï¼šAutoGluonã€‚
- **å…¶ä»–è¡¨æ ¼åŸºç¡€æ¨¡å‹ (TFMs)**ï¼š
  - **TabPFNv2**, **TabPFN-2.5**
  - **RealTabPFN-2.5**ï¼šåœ¨çœŸå®æ•°æ®ä¸Šç»§ç»­é¢„è®­ç»ƒçš„ TabPFN-2.5ï¼Œæ˜¯å½“æ—¶çš„ SOTAã€‚
  - **TabICL**, **LimiX**, **Mitra**, **TabDPT** ç­‰ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ
- **åœ¨ TabArena åŸºå‡†ä¸Š**ï¼š
  - TabICLv2 **æ— éœ€ä»»ä½•è°ƒå‚**ï¼Œå…¶æ€§èƒ½å°±è¶…è¿‡äº†ç»è¿‡ **è°ƒå‚+é›†æˆ+å¾®è°ƒ** çš„ RealTabPFN-2.5ã€‚
  - åœ¨ **Improvability vs. Train Time** å’Œ **Improvability vs. Inference Time** çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼ˆPareto Frontï¼‰ä¸Šï¼ŒTabICLv2 æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ã€‚
  - åœ¨å›å½’ä»»åŠ¡ä¸Šï¼ŒTabICLv2 ä¸ RealTabPFN-2.5 å¹¶åˆ—ç¬¬ä¸€ï¼ˆå¹³å‡æ’å 4.54ï¼‰ã€‚

- **åœ¨ TALENT åŸºå‡†ä¸Š**ï¼š
  - TabICLv2 å–å¾—äº† **4.66** çš„å¹³å‡æ’åï¼Œä¼˜äº RealTabPFN-2.5 (5.11) å’Œ TabPFN-2.5 (5.45)ã€‚
  - æˆå¯¹èƒœç‡ï¼šTabICLv2 å¯¹ RealTabPFN-2.5 çš„èƒœç‡ä¸º **62%**ï¼Œå¯¹ TabPFN-2.5 çš„èƒœç‡ä¸º **65%**ã€‚
  - **å¤šåˆ†ç±»ä»»åŠ¡ (>10 ç±»)**ï¼šTabICLv2 è¡¨ç°å‡ºè‰²ï¼Œå¤§å¹…é¢†å…ˆäºæ‰€æœ‰åŸºçº¿ã€‚

- **è¿è¡Œæ•ˆç‡**ï¼š
  - **é€Ÿåº¦**ï¼šåœ¨ H100 GPU ä¸Šï¼Œå¤„ç† 50K æ ·æœ¬æ—¶ï¼ŒTabICLv2 æ¯” TabPFN-2.5 å¿« **10.6 å€**ã€‚
  - **å†…å­˜**ï¼šé€šè¿‡ç£ç›˜å¸è½½ï¼Œå¯åœ¨ **50GB GPU å†…å­˜**ä¸‹å¤„ç†ç™¾ä¸‡çº§æ ·æœ¬çš„è¡¨æ ¼ã€‚

### æ¶ˆèå®éªŒç»“æœ (Ablation Study)
æ¶ˆèå®éªŒé‡åŒ–äº†å„ç»„ä»¶çš„è´¡çŒ®ï¼ˆåœ¨éªŒè¯é›†ä¸Šï¼‰ï¼š
- **æœ€å¤§çš„å½±å“æ¥è‡ªåˆæˆæ•°æ®å…ˆéªŒ (Prior)**ï¼šä½¿ç”¨ TabICL çš„æ—§å…ˆéªŒè®­ç»ƒ TabICLv2ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ï¼ŒéªŒè¯äº†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„åˆæˆæ•°æ®å¯¹æ¨¡å‹æˆåŠŸè‡³å…³é‡è¦ã€‚
- **å…³é”®ç»„ä»¶è´¡çŒ®ç›¸å½“**ï¼ˆçº¦ 100 Elo å¢ç›Šï¼‰ï¼š
  - é‡‡ç”¨ **Muon ä¼˜åŒ–å™¨** æ›¿ä»£ AdamWã€‚
  - å¼•å…¥ **QASSMax** æ³¨æ„åŠ›æœºåˆ¶ã€‚
  - æ—©æœŸåŠ å…¥ **ç›®æ ‡ä¿¡æ¯ (early target inclusion)**ã€‚
- **å…¶ä»–æ”¹è¿›å¸¦æ¥è¾ƒå°å¢ç›Š**ï¼š
  - é‡å¤ç‰¹å¾åˆ†ç»„ (Repeated feature grouping)ã€‚
  - æ•°æ®è¿‡æ»¤ (Prior filtering)ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **é«˜è´¨é‡åˆæˆæ•°æ®æ˜¯åŸºçŸ³**ï¼šä¸€ä¸ªè®¾è®¡ç²¾è‰¯ã€é«˜åº¦å¤šæ ·åŒ–çš„åˆæˆæ•°æ®ç”Ÿæˆå™¨å¯¹äºè®­ç»ƒå¼ºå¤§çš„ TFM è‡³å…³é‡è¦ï¼Œå…¶ä½œç”¨å¯èƒ½è¶…è¿‡æ¶æ„æœ¬èº«çš„æ”¹è¿›ã€‚
2. **ç®€å•æœ‰æ•ˆçš„æ¶æ„åˆ›æ–°èƒ½å¸¦æ¥å·¨å¤§æ”¶ç›Š**ï¼šQASSMax ä½œä¸ºä¸€ç§è½»é‡çº§çš„æ³¨æ„åŠ›ä¿®æ”¹ï¼Œæœ‰æ•ˆè§£å†³äº†é•¿ä¸Šä¸‹æ–‡æ³›åŒ–é—®é¢˜ï¼Œæ˜¯ TabICLv2 èƒ½å¤„ç†å¤§è§„æ¨¡æ•°æ®çš„å…³é”®ã€‚
3. **ä¼˜åŒ–å™¨é€‰æ‹©è‡³å…³é‡è¦**ï¼šMuon ä¼˜åŒ–å™¨åœ¨ TFM çš„é¢„è®­ç»ƒä¸­è¡¨ç°å‡ºä¼˜äº AdamW çš„æ€§èƒ½ï¼Œæ˜¯ä¸€ä¸ªè¢«ä½ä¼°ä½†é‡è¦çš„å·¥ç¨‹é€‰æ‹©ã€‚
4. **å¯ä»¥åšåˆ°â€œæ›´å¥½ã€æ›´å¿«ã€æ›´å¼€æ”¾â€**ï¼šTabICLv2 è¯æ˜äº†æ— éœ€ä¾èµ–çœŸå®æ•°æ®å¾®è°ƒã€å¤æ‚é›†æˆå’Œé—­æºæŠ€æœ¯ï¼Œä¹Ÿèƒ½æ„å»ºå‡ºè¶…è¶Šå½“å‰ SOTA çš„è¡¨æ ¼æ¨¡å‹ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æ— æ³•åˆ©ç”¨è¯­ä¹‰ä¿¡æ¯**ï¼šä¸ LLM-based æ¨¡å‹ä¸åŒï¼Œå®ƒä¸èƒ½åˆ©ç”¨åˆ—åæˆ–æ–‡æœ¬ç‰¹å¾ä¸­çš„è¯­ä¹‰ä¿¡æ¯ã€‚
- **è¶…å¤§è§„æ¨¡æ•°æ®ä»å…·æŒ‘æˆ˜**ï¼šè™½ç„¶èƒ½å¤„ç†ç™¾ä¸‡çº§æ ·æœ¬ï¼Œä½†å¯¹äºæ•°åƒä¸‡æˆ–ä¸Šäº¿æ ·æœ¬çš„æ•°æ®é›†ï¼Œä¾ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚
- **ç¼ºå¤±å€¼å¤„ç†ç®€å•**ï¼šç›®å‰å¯¹ç¼ºå¤±å€¼çš„å¤„ç†æ–¹å¼ï¼ˆå‡å€¼å¡«å……ï¼‰è¾ƒä¸ºåŸºç¡€ï¼Œæœªåœ¨é¢„è®­ç»ƒä¸­ä¸“é—¨å»ºæ¨¡ã€‚
- **åˆ†å¸ƒå¤–æ³›åŒ–æœªçŸ¥**ï¼šåœ¨ä¸é¢„è®­ç»ƒå…ˆéªŒå·®å¼‚æå¤§çš„çœŸå®æ•°æ®ä¸Šçš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ³›åŒ–èƒ½åŠ›æœ‰å¾…è¿›ä¸€æ­¥ç ”ç©¶ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢å¦‚ä½•ç»“åˆåˆ—åå’Œæ–‡æœ¬ç‰¹å¾çš„è¯­ä¹‰ä¿¡æ¯ã€‚
- å¼€å‘æ›´é«˜æ•ˆçš„æ–¹æ³•æ¥å¤„ç†è¶…å¤§è§„æ¨¡ï¼ˆåäº¿çº§ï¼‰è¡¨æ ¼æ•°æ®ã€‚
- æ”¹è¿›å¯¹ç¼ºå¤±å€¼ã€å¤šè¾“å‡ºå›å½’å’Œåˆ†å¸ƒæ¼‚ç§»ï¼ˆdistribution shiftsï¼‰çš„å¤„ç†ã€‚
- ç ”ç©¶åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šçš„å¾®è°ƒï¼ˆfine-tuningï¼‰æˆ–ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆcontext optimizationï¼‰æ˜¯å¦èƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚
- æ¨åŠ¨æ›´å…¬å¹³ã€æ›´é€æ˜çš„è¡¨æ ¼å­¦ä¹ åŸºå‡†æµ‹è¯•ã€‚

</details>

---

### 16. [Neuro-Symbolic Synergy for Interactive World Modeling](https://arxiv.org/abs/2602.10480)

**Authors**: Hongyu Zhao, Siyu Zhou, Haolin Yang, Zengyi Qin, Tianyi Zhou  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.10480v1  

#### Abstract
Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical cons...

---

### 17. [dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning](https://arxiv.org/abs/2602.10603)

**Authors**: Arnav Shah, Junzhe Li, Parsa Idehpour, Adibvafa Fallahpour, Brandon Wang, Sukjun Hwang, Bo Wang, Patrick D. Hsu, Hani Goodarzi, Albert Gu  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.10603v1  

#### Abstract
Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coh...

---

### 18. [MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation](https://arxiv.org/abs/2602.11062)

**Authors**: Jialin Liu, Zhaorui Zhang, Ray C. C. Cheung  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.11062v1  

#### Abstract
Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content...

---

### 19. [MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092)

**Authors**: Cassandre Notton, Benjamin Stott, Philippe Schoeb, Anthony Walsh, Gr\'egoire Leboucher, Vincent Espitalier, Vassilis Apostolou, Louis-F\'elix Vigneux, Alexia Salavrakos, Jean Senellart  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.11092v1  

#### Abstract
Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework...

---

### 20. [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)

**Authors**: Shuai Han, Mehdi Dastani, Shihan Wang  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2602.10598v1  

#### Abstract
Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In ...

---

### 21. [Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens](https://arxiv.org/abs/2602.10229)

**Authors**: Weihao Liu, Dehai Min, Lu Cheng  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2602.10229v1  

#### Abstract
While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has e...

---

### 22. [Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs](https://arxiv.org/abs/2602.10740)

**Authors**: Yuming Yan, Shuo Yang, Kai Tang, Sihong Chen, Yang Zhang, Ke Xu, Dan Hu, Qun Yu, Pengfei Hu, Edith C. H. Ngai  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2602.10740v1  

#### Abstract
Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forget...

---

### 23. [R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting](https://arxiv.org/abs/2602.10312)

**Authors**: Lipai Huang, Kai Yin, Chia-Fu Liu, Ali Mostafavi  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2602.10312v1  

#### Abstract
R2RAG-Flood is a reasoning-reinforced, training-free retrieval-augmented generation framework for post-storm property damage nowcasting. Building on an existing supervised tabular predictor, the framework constructs a reasoning-centric knowledge base composed of labeled tabular records, where each s...

---

### 24. [Roughness-Informed Federated Learning](https://arxiv.org/abs/2602.10595)

**Authors**: Mohammad Partohaghighi, Roummel Marcia, Bruce J. West, YangQuan Chen  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2602.10595v1  

#### Abstract
Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet faces challenges in non-independent and identically distributed (non-IID) settings due to client drift, which impairs convergence. We propose RI-FedAvg, a novel FL algorithm tha...

---

### 25. [Learning Mixture Density via Natural Gradient Expectation Maximization](https://arxiv.org/abs/2602.10602)

**Authors**: Yutao Chen, Jasmine Bayrooti, Steven Morad  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2602.10602v1  

#### Abstract
Mixture density networks are neural networks that produce Gaussian mixtures to represent continuous multimodal conditional densities. Standard training procedures involve maximum likelihood estimation using the negative log-likelihood (NLL) objective, which suffers from slow convergence and mode col...

---

### 26. [Sample Efficient Generative Molecular Optimization with Joint Self-Improvement](https://arxiv.org/abs/2602.10984)

**Authors**: Serra Korkmaz, Adam Izdebski, Jonathan Pirnay, Rasmus M{\o}ller-Larsen, Michal Kmicikiewicz, Pankhil Gawade, Dominik G. Grimm, Ewa Szczurek  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2602.10984v1  

#### Abstract
Generative molecular optimization aims to design molecules with properties surpassing those of existing compounds. However, such candidates are rare and expensive to evaluate, yielding sample efficiency essential. Additionally, surrogate models introduced to predict molecule evaluations, suffer from...

---

### 27. [TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents](https://arxiv.org/abs/2602.10986)

**Authors**: Abhishek Vijaya Kumar, Bhaskar Kataria, Byungsoo Oh, Emaad Manzoor, Rachee Singh  
**Category**: cs.LG  
**Published**: 2026-02-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2602.10986v1  

#### Abstract
In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reus...

---

### 28. [LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation](https://arxiv.org/abs/2602.10367)

**Authors**: Zhiling Yan, Dingjie Song, Zhe Fang, Yisheng Ji, Xiang Li, Quanzheng Li, Lichao Sun  
**Category**: cs.AI  
**Published**: 2026-02-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2602.10367v1  

#### Abstract
The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, lea...

---

### 29. [LATA: A Tool for LLM-Assisted Translation Annotation](https://arxiv.org/abs/2602.10454)

**Authors**: Baorong Huang, Ali Asiri  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2602.10454v1  

#### Abstract
The construction of high-quality parallel corpora for translation research has increasingly evolved from simple sentence alignment to complex, multi-layered annotation tasks. This methodological shift presents significant challenges for structurally divergent language pairs, such as Arabic--English,...

---

### 30. [C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution](https://arxiv.org/abs/2602.10874)

**Authors**: Binwei Yan, Yifei Fu, Mingjian Zhu, Hanting Chen, Mingxuan Yuan, Yunhe Wang, Hailin Hu  
**Category**: cs.CL  
**Published**: 2026-02-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2602.10874v1  

#### Abstract
Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based Momentum Optimized Prompting), a framework that stabili...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
