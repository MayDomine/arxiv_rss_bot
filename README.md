# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-08-25 12:54:00 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)

**Authors**: Zeguan Xiao, Yun Chen, Guanhua Chen, Ke Tang  
**Category**: cs.CL  
**Published**: 2025-08-25  
**Score**: 6.0

arXiv:2506.09457v2 Announce Type: replace 
Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO), have emerged as efficient alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms for aligning large ...

---

### 2. [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)

**Authors**: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 5.5

arXiv:2505.20776v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a...

---

### 3. [Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS](https://arxiv.org/abs/2508.14313)

**Authors**: Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 5.5

arXiv:2508.14313v2 Announce Type: replace-cross 
Abstract: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample ...

---

### 4. [Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data](https://arxiv.org/abs/2508.15793)

**Authors**: Jiacheng Liu, Mayi Xu, Qiankun Pi, Wenli Li, Ming Zhong, Yuanyuan Zhu, Mengchi Liu, Tieyun Qian  
**Category**: cs.CL  
**Published**: 2025-08-25  
**Score**: 5.5

arXiv:2508.15793v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly employed in applications that require processing information from heterogeneous formats, including text, tables, infoboxes, and knowledge graphs. However, systematic biases toward particular formats may un...

---

### 5. [DAIQ: Auditing Demographic Attribute Inference from Question in LLMs](https://arxiv.org/abs/2508.15830)

**Authors**: Srikant Panda, Hitesh Laxmichand Patel, Shahad Al-Khalifa, Amit Agarwal, Hend Al-Khalifa, Sharefah Al-Ghamdi  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 5.0

arXiv:2508.15830v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are known to reflect social biases when demographic attributes, such as gender or race, are explicitly present in the input. But even in their absence, these models still infer user identities based solely on question ph...

---

### 6. [SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning](https://arxiv.org/abs/2508.16201)

**Authors**: Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 5.0

arXiv:2508.16201v1 Announce Type: cross 
Abstract: Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decodi...

---

### 7. [Hydra: A 1.6B-Parameter State-Space Language Model with Sparse Attention, Mixture-of-Experts, and Memory](https://arxiv.org/abs/2508.15099)

**Authors**: Siddharth Chaudhary, Bennett Browning  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 5.0

arXiv:2508.15099v2 Announce Type: replace-cross 
Abstract: We present Hydra as an architectural proposal for hybrid long-context language models that combine conditional computation, long-context memory mechanisms, and sparse mixture-of-experts within an approximately 1.6B parameter design envelope....

---

### 8. [SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression](https://arxiv.org/abs/2508.15806)

**Authors**: Mengjie Li, William J. Song  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 4.5

arXiv:2508.15806v1 Announce Type: cross 
Abstract: The increasing input sequence length in Large Language Models (LLMs) puts significant pressure on key-value (KV) cache storage, making efficient inference challenging. Explicitly distinguishing attention behavior into our self-defined surface memori...

---

### 9. [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)

**Authors**: David Chanin, Adri\`a Garriga-Alonso  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 4.5

arXiv:2508.16560v1 Announce Type: cross 
Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to single concepts. A core SAE training hyperparameter is L0: how many features should fire per token on average. Existing work compares SAE algorithms us...

---

### 10. [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)

**Authors**: Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siyang Gao, Siwei Liu, Nan Yin  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 4.5

arXiv:2508.00719v2 Announce Type: replace-cross 
Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA meth...

---

### 11. [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)

**Authors**: Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, Meng Wang  
**Category**: cs.CL  
**Published**: 2025-08-25  
**Score**: 4.5

arXiv:2508.10848v2 Announce Type: replace 
Abstract: Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augment...

---

### 12. [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881)

**Authors**: Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 4.0

arXiv:2508.15881v1 Announce Type: cross 
Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple dev...

---

### 13. [Time Series Based Network Intrusion Detection using MTF-Aided Transformer](https://arxiv.org/abs/2508.16035)

**Authors**: Poorvi Joshi (National University of Singapore), Mohan Gurusamy (National University of Singapore)  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 4.0

arXiv:2508.16035v1 Announce Type: cross 
Abstract: This paper introduces a novel approach to time series classification using a Markov Transition Field (MTF)-aided Transformer model, specifically designed for Software-Defined Networks (SDNs). The proposed model integrates the temporal dependency mod...

---

### 14. [Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers](https://arxiv.org/abs/2508.16311)

**Authors**: Lucas Maisonnave, Karim Haroun, Tom Pegeot  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 4.0

arXiv:2508.16311v1 Announce Type: cross 
Abstract: Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where each attention head contributes to the final representation. However, their computational complexity and high memory demands due to MHSA hinders their deployment at the ed...

---

### 15. [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)

**Authors**: Jongyeop Hyun, Bumsoo Kim  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 4.0

arXiv:2508.16313v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging cor...

---

### 16. [RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs](https://arxiv.org/abs/2508.16546)

**Authors**: Hangzhan Jin, Sicheng Lv, Sifan Wu, Mohammad Hamdaqa  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 4.0

arXiv:2508.16546v1 Announce Type: cross 
Abstract: Training large language models (LLMs) from scratch is increasingly impractical, making post-training methods such as supervised fine-tuning (SFT) and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern practice. Using an out-of-d...

---

### 17. [MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications](https://arxiv.org/abs/2508.10991)

**Authors**: Wenpeng Xing, Zhonghao Qi, Yupeng Qin, Yilin Li, Caini Chang, Jiahui Yu, Changting Lin, Zhenzhen Xie, Meng Han  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 4.0

arXiv:2508.10991v2 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) with external tools via protocols such as the Model Context Protocol (MCP) introduces critical security vulnerabilities, including prompt injection, data exfiltration, and other threats. To cou...

---

### 18. [A Probabilistic Inference Scaling Theory for LLM Self-Correction](https://arxiv.org/abs/2508.16456)

**Authors**: Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui  
**Category**: cs.CL  
**Published**: 2025-08-25  
**Score**: 4.0

arXiv:2508.16456v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated the capability to refine their generated answers through self-correction, enabling continuous performance improvement over multiple rounds. However, the mechanisms underlying how and why accuracy evolves ...

---

### 19. [Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration](https://arxiv.org/abs/2508.15928)

**Authors**: Jihua Huang, Yi Yao, Ajay Divakaran  
**Category**: cs.LG  
**Published**: 2025-08-25  
**Score**: 4.0

arXiv:2508.15928v1 Announce Type: new 
Abstract: We introduce a novel framework for temporal causal discovery and inference that addresses two key challenges: complex nonlinear dependencies and spurious correlations. Our approach employs a multi-layer Transformer-based time-series forecaster to capt...

---

### 20. [Decentralized Low-Rank Fine-Tuning of Large Language Models](https://arxiv.org/abs/2501.15361)

**Authors**: Sajjad Ghiasvand, Mahnoosh Alizadeh, Ramtin Pedarsani  
**Category**: cs.LG  
**Published**: 2025-08-25  
**Score**: 4.0

arXiv:2501.15361v5 Announce Type: replace 
Abstract: While parameter-efficient fine-tuning (PEFT) techniques like Low-Rank Adaptation (LoRA) offer computationally efficient adaptations of Large Language Models (LLMs), their practical deployment often assumes centralized data and training environment...

---

### 21. [LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions](https://arxiv.org/abs/2508.15801)

**Authors**: Seyedali Mohammadi, Manas Paldhe, Amit Chhabra  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2508.15801v1 Announce Type: cross 
Abstract: Phone call transcript labeling is prohibitively expensive (approximately 2 USD per minute) due to privacy regulations, consent requirements, and manual annotation costs requiring 3 hours of expert time per hour of audio. Existing extraction methods ...

---

### 22. [User-Assistant Bias in LLMs](https://arxiv.org/abs/2508.15815)

**Authors**: Xu Pan, Jingxuan Fan, Zidi Xiong, Ely Hahami, Jorin Overwiening, Ziqian Xie  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2508.15815v1 Announce Type: cross 
Abstract: Large language models (LLMs) can bias towards relying on their own or the user's information in chat history, leading to overly stubborn or agreeable behaviors in multi-turn conversations. In this paper, we formalize this model characteristic as use...

---

### 23. [Who's Asking? Investigating Bias Through the Lens of Disability Framed Queries in LLMs](https://arxiv.org/abs/2508.15831)

**Authors**: Srikant Panda, Vishnu Hari, Kalpana Panda, Amit Agarwal, Hitesh Laxmichand Patel  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2508.15831v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) routinely infer users demographic traits from phrasing alone, which can result in biased responses, even when no explicit demographic information is provided. The role of disability cues in shaping these inferences remai...

---

### 24. [Coarse-to-Fine Personalized LLM Impressions for Streamlined Radiology Reports](https://arxiv.org/abs/2508.15845)

**Authors**: Chengbo Sun, Hui Yi Leong, Lei Li  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2508.15845v1 Announce Type: cross 
Abstract: The manual creation of the "Impression" section in radiology reports is a primary driver of radiologist burnout. To address this challenge, we propose a coarse-to-fine framework that leverages open-source large language models (LLMs) to automaticall...

---

### 25. [CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning](https://arxiv.org/abs/2508.15868)

**Authors**: Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2508.15868v1 Announce Type: cross 
Abstract: Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed...

---

### 26. [Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding](https://arxiv.org/abs/2502.08363)

**Authors**: Konstantin Berestizshevsky, Renzo Andri, Lukas Cavigelli  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2502.08363v2 Announce Type: replace-cross 
Abstract: We present Top-Theta (Top-$\theta$) Attention, a training-free method for sparsifying transformer attention during inference. Our key insight is that static, per-head thresholds can be calibrated to retain the desired constant number of sign...

---

### 27. [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)

**Authors**: Licheng Liu, Zihan Wang, Linjie Li, Chenwei Xu, Yiping Lu, Han Liu, Avirup Sil, Manling Li  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2507.14295v2 Announce Type: replace-cross 
Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn parad...

---

### 28. [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)

**Authors**: Haoyang Li, Jiaqing Li, Jialun Cao, Zongyuan Yang, Yongping Xiong  
**Category**: cs.AI  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2508.09220v2 Announce Type: replace-cross 
Abstract: Large foundation models have achieved significant performance gains through scalable training on massive datasets. However, the field of \textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression \textbf{R}ecognition (HMER) has been impe...

---

### 29. [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)

**Authors**: Samiul Basir Bhuiyan, Md. Sazzad Hossain Adib, Mohammed Aman Bhuiyan, Muhammad Rafsan Kabir, Moshiur Farazi, Shafin Rahman, Nabeel Mohammed  
**Category**: cs.CL  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2508.15828v1 Announce Type: cross 
Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving remarkable performance across a wide range of natural language processing tasks. However, this progress has come at the cost of increasingly large model sizes, which pose ...

---

### 30. [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)

**Authors**: Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, Jun Wang  
**Category**: cs.CL  
**Published**: 2025-08-25  
**Score**: 3.5

arXiv:2508.16153v1 Announce Type: cross 
Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection ...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative Decoding

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
