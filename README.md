# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-09-11 03:23:04 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training](https://arxiv.org/abs/2507.13833)

**Authors**: Zhixin Wang, Tianyi Zhou, Liming Liu, Ao Li, Jiarui Hu, Dian Yang, Yinhui Lu, Jinlong Hou, Siyuan Feng, Yuan Cheng, Yuan Qi  
**Category**: cs.DC  
**Published**: 2025-09-10  
**Score**: 12.5

arXiv:2507.13833v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) has become the pivotal post-training technique for large language model (LLM). Effectively scaling reinforcement learning is now the key to unlocking advanced reasoning capabilities and ensuring safe, goal-aligned behav...

---

### 2. [Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference](https://arxiv.org/abs/2412.18934)

**Authors**: Libo Zhang, Zhaoning Zhang, Baizhou Xu, Rui Li, Zhiliang Tian, Songzhu Mei, Dongsheng Li  
**Category**: cs.CL  
**Published**: 2025-09-10  
**Score**: 11.0

arXiv:2412.18934v2 Announce Type: replace 
Abstract: With the continuous advancement in the performance of large language models (LLMs), their demand for computational resources and memory has significantly increased, which poses major challenges for efficient inference on consumer-grade devices and...

---

### 3. [DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference](https://arxiv.org/abs/2509.07379)

**Authors**: Yuning Zhang, Grant Pinkert, Nan Yang, Yanli Li, Dong Yuan  
**Category**: cs.DC  
**Published**: 2025-09-10  
**Score**: 11.0

arXiv:2509.07379v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance across a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances their capabilities by increasing model width through sparsely activated expert branches, which ...

---

### 4. [RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use](https://arxiv.org/abs/2509.06980)

**Authors**: Jiajun Chai, Guojun Yin, Zekun Xu, Chuhuai Yue, Yi Jia, Siyu Xia, Xiaohan Wang, Jiwen Jiang, Xiaoguang Li, Chengqi Dong, Hang He, Wei Lin  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 9.0

arXiv:2509.06980v1 Announce Type: cross 
Abstract: Large language models excel at basic reasoning but struggle with tasks that require interaction with external tools. We present RLFactory, a plug-and-play reinforcement learning post-training framework for multi-round tool use. RLFactory tackles (i)...

---

### 5. [Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?](https://arxiv.org/abs/2509.07593)

**Authors**: Gavin Tao, Yinuo Wang, Jinzhao Zhou  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 9.0

arXiv:2509.07593v1 Announce Type: cross 
Abstract: End-to-end reinforcement learning for motion control promises unified perception-action policies that scale across embodiments and tasks, yet most deployed controllers are either blind (proprioception-only) or rely on fusion backbones with unfavorab...

---

### 6. [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)

**Authors**: Ke Wang, Yiming Qin, Nikolaos Dimitriadis, Alessandro Favero, Pascal Frossard  
**Category**: cs.CL  
**Published**: 2025-09-10  
**Score**: 8.5

arXiv:2506.07899v2 Announce Type: replace 
Abstract: Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably-without retraining or forgetting previous information-remains a major ch...

---

### 7. [End-to-End Efficiency in Keyword Spotting: A System-Level Approach for Embedded Microcontrollers](https://arxiv.org/abs/2509.07051)

**Authors**: Pietro Bartoli, Tommaso Bondini, Christian Veronesi, Andrea Giudici, Niccol\`o Antonello, Franco Zappa  
**Category**: cs.LG  
**Published**: 2025-09-10  
**Score**: 8.5

arXiv:2509.07051v1 Announce Type: cross 
Abstract: Keyword spotting (KWS) is a key enabling technology for hands-free interaction in embedded and IoT devices, where stringent memory and energy constraints challenge the deployment of AI-enabeld devices. In this work, we systematically evaluate and co...

---

### 8. [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)

**Authors**: Andrew Kiruluta, Priscilla Burity  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 8.0

arXiv:2509.07017v1 Announce Type: new 
Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning framework that embeds logical rules as spectral templates and performs inference directly in the graph spectral domain. By leveraging graph signal processing (GSP) and frequency-sele...

---

### 9. [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)

**Authors**: Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, Alex Aiken  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 8.0

arXiv:2509.07506v1 Announce Type: cross 
Abstract: GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high perf...

---

### 10. [FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning](https://arxiv.org/abs/2509.07342)

**Authors**: Yuxuan Bai, Yuxuan Sun, Tan Chen, Wei Chen, Sheng Zhou, Zhisheng Niu  
**Category**: cs.DC  
**Published**: 2025-09-10  
**Score**: 8.0

arXiv:2509.07342v1 Announce Type: cross 
Abstract: Federated edge learning (FEEL) enables collaborative model training across distributed clients over wireless networks without exposing raw data. While most existing studies assume static datasets, in real-world scenarios clients may continuously col...

---

### 11. [Scaled Block Vecchia Approximation for High-Dimensional Gaussian Process Emulation on GPUs](https://arxiv.org/abs/2504.12004)

**Authors**: Qilong Pan, Sameh Abdulah, Mustafa Abduljabbar, Hatem Ltaief, Andreas Herten, Mathis Bode, Matthew Pratola, Arindam Fadikar, Marc G. Genton, David E. Keyes, Ying Sun  
**Category**: cs.DC  
**Published**: 2025-09-10  
**Score**: 8.0

arXiv:2504.12004v2 Announce Type: replace 
Abstract: Emulating computationally intensive scientific simulations is crucial for enabling uncertainty quantification, optimization, and informed decision-making at scale. Gaussian Processes (GPs) offer a flexible and data-efficient foundation for statist...

---

### 12. [Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s](https://arxiv.org/abs/2509.07928)

**Authors**: Mahmudul Islam Masum, Miad Islam, Arif I. Sarwat  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 7.5

arXiv:2509.07928v1 Announce Type: cross 
Abstract: As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically...

---

### 13. [TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection](https://arxiv.org/abs/2411.02886)

**Authors**: Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 7.5

arXiv:2411.02886v3 Announce Type: replace-cross 
Abstract: Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-di...

---

### 14. [veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD](https://arxiv.org/abs/2509.07003)

**Authors**: Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, Li-Wen Chang  
**Category**: cs.DC  
**Published**: 2025-09-10  
**Score**: 7.5

arXiv:2509.07003v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have scaled rapidly in size and complexity, requiring increasingly intricate parallelism for distributed training, such as 3D parallelism. This sophistication motivates a shift toward simpler, more debuggable programming...

---

### 15. [K2-Think: A Parameter-Efficient Reasoning System](https://arxiv.org/abs/2509.07604)

**Authors**: Zhoujun Cheng, Richard Fan, Shibo Hao, Taylor W. Killian, Haonan Li, Suqi Sun, Hector Ren, Alexander Moreno, Daqian Zhang, Tianjun Zhong, Yuxin Xiong, Yuanzhe Hu, Yutao Xie, Xudong Han, Yuqi Wang, Varad Pimpalkhute, Yonghao Zhuang, Aaryamonvikram Singh, Xuezhi Liang, Anze Xie, Jianshu She, Desai Fan, Chengqian Gao, Liqun Ma, Mikhail Yurochkin, John Maggs, Xuezhe Ma, Guowei He, Zhiting Hu, Zhengzhong Liu, Eric P. Xing  
**Category**: cs.LG  
**Published**: 2025-09-10  
**Score**: 7.5

arXiv:2509.07604v1 Announce Type: new 
Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance with a 32B parameter model, matching or surpassing much larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system shows that smaller models ...

---

### 16. [M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models](https://arxiv.org/abs/2504.10449)

**Authors**: Junxiong Wang, Wen-Ding Li, Daniele Paliotta, Daniel Ritter, Alexander M. Rush, Tri Dao  
**Category**: cs.LG  
**Published**: 2025-09-10  
**Score**: 7.5

arXiv:2504.10449v3 Announce Type: replace 
Abstract: Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are i...

---

### 17. [VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving](https://arxiv.org/abs/2509.04827)

**Authors**: Jiahuan Yu (University of Illinois Urbana-Champaign), Aryan Taneja (University of Illinois Urbana-Champaign), Junfeng Lin (Tsinghua University), Minjia Zhang (University of Illinois Urbana-Champaign)  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 7.0

arXiv:2509.04827v1 Announce Type: cross 
Abstract: Modern Large Language Model (LLM) serving systems increasingly support interactive applications, like real-time chat assistants, code generation tools, and agentic workflows. However, the soaring energy cost of LLM inference presents a growing chall...

---

### 18. [Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data](https://arxiv.org/abs/2509.07526)

**Authors**: Gokul Karthik Kumar, Rishabh Saraf, Ludovick Lepauloux, Abdul Muneer, Billel Mokeddem, Hakim Hacid  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 7.0

arXiv:2509.07526v1 Announce Type: cross 
Abstract: Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored -- despite audio's centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction...

---

### 19. [Towards Generalized Routing: Model and Agent Orchestration for Adaptive and Efficient Inference](https://arxiv.org/abs/2509.07571)

**Authors**: Xiyu Guo, Shan Wang, Chunfang Ji, Xuefeng Zhao, Wenhao Xi, Yaoyao Liu, Qinglan Li, Chao Deng, Junlan Feng  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 7.0

arXiv:2509.07571v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) and domain-specific AI agents has greatly expanded the ecosystem of AI-powered services. User queries, however, are highly diverse and often span multiple domains and task types, resulting in a c...

---

### 20. [Learning to Upsample and Upmix Audio in the Latent Domain](https://arxiv.org/abs/2506.00681)

**Authors**: Dimitrios Bralios, Paris Smaragdis, Jonah Casebeer  
**Category**: cs.LG  
**Published**: 2025-09-10  
**Score**: 7.0

arXiv:2506.00681v2 Announce Type: replace-cross 
Abstract: Neural audio autoencoders create compact latent representations that preserve perceptually important information, serving as the foundation for both modern audio compression systems and generation approaches like next-token prediction and la...

---

### 21. [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](https://arxiv.org/abs/2509.06982)

**Authors**: Xiaomeng Hu, Fei Huang, Chenhan Yuan, Junyang Lin, Tsung-Yi Ho  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 6.5

arXiv:2509.06982v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring the safety of their outputs during decoding has become a critical challenge. However, existing decoding-time interventions, such as Contrastive Decoding, ...

---

### 22. [1 bit is all we need: binary normalized neural networks](https://arxiv.org/abs/2509.07025)

**Authors**: Eduardo Lobo Lustoda Cabral, Paulo Pirozelli, Larissa Driemeier  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 6.5

arXiv:2509.07025v1 Announce Type: cross 
Abstract: The increasing size of large neural network models, specifically language models and foundational image models, poses deployment challenges, prompting efforts to reduce memory requirements and enhance computational efficiency. These efforts are crit...

---

### 23. [IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation](https://arxiv.org/abs/2509.07245)

**Authors**: Shalev Manor, Mohammad Kohandel  
**Category**: cs.LG  
**Published**: 2025-09-10  
**Score**: 6.5

arXiv:2509.07245v1 Announce Type: new 
Abstract: Solving inverse problems with Physics-Informed Neural Networks (PINNs) is computationally expensive for multi-query scenarios, as each new set of observed data requires a new, expensive training procedure. We present Inverse-Parameter Basis PINNs (IP-...

---

### 24. [Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost](https://arxiv.org/abs/2509.07829)

**Authors**: Mihai Nadas, Laura Diosan, Andreea Tomescu, Andrei Piscoran  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 6.0

arXiv:2509.07829v1 Announce Type: cross 
Abstract: Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We contribute to this ongoing research by introducing TINYFABUL...

---

### 25. [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)

**Authors**: Lang Mei, Zhihan Yang, Chong Chen  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 6.0

arXiv:2508.20368v3 Announce Type: replace 
Abstract: Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs' internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradig...

---

### 26. [Research on Conversational Recommender System Considering Consumer Types](https://arxiv.org/abs/2508.13209)

**Authors**: Yaying Luo, Hui Fang, Zhu Sun  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 6.0

arXiv:2508.13209v2 Announce Type: replace-cross 
Abstract: Conversational Recommender Systems (CRS) provide personalized services through multi-turn interactions, yet most existing methods overlook users' heterogeneous decision-making styles and knowledge levels, which constrains both accuracy and e...

---

### 27. [zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs](https://arxiv.org/abs/2508.21393)

**Authors**: Guofu Liao, Taotao Wang, Shengli Zhang, Jiqun Zhang, Shi Long, Dacheng Tao  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 6.0

arXiv:2508.21393v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) is crucial for adapting them to specific tasks, yet it remains computationally demanding and raises concerns about correctness and privacy, particularly in untrusted environments. Although parameter-e...

---

### 28. [What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?](https://arxiv.org/abs/2509.03790)

**Authors**: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma  
**Category**: cs.AI  
**Published**: 2025-09-10  
**Score**: 6.0

arXiv:2509.03790v2 Announce Type: replace-cross 
Abstract: Sparse-reward reinforcement learning (RL) remains fundamentally hard: without structure, any agent needs $\Omega(|\mathcal{S}||\mathcal{A}|/p)$ samples to recover rewards. We introduce Policy-Aware Matrix Completion (PAMC) as a first concret...

---

### 29. [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)

**Authors**: Haoming Huang, Yibo Yan, Jiahao Huo, Xin Zou, Xinfeng Li, Kun Wang, Xuming Hu  
**Category**: cs.CL  
**Published**: 2025-09-10  
**Score**: 6.0

arXiv:2505.14406v4 Announce Type: replace 
Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are hampered by hallucinations. A particularly challenging variant, knowledge overshadowing, occurs when one piece of activated knowledge inadvertently masks another relevant pie...

---

### 30. [MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?](https://arxiv.org/abs/2509.07727)

**Authors**: Songkai Ma, Zhaorui Zhang, Sheng Di, Benben Liu, Xiaodong Yu, Xiaoyi Lu, Dan Wang  
**Category**: cs.DC  
**Published**: 2025-09-10  
**Score**: 6.0

arXiv:2509.07727v1 Announce Type: cross 
Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models in the field of LLM learning, efficiently serving MoE models under limited GPU memory constraints has emerged as a significant challenge. Offloading the non-activated exper...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
