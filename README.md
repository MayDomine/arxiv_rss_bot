# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2025-12-25 06:01:12 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)

**Authors**: Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.20276v1  

#### Abstract
Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth rob...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡ã€ŠActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edgeã€‹æ ¸å¿ƒæ€»ç»“**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
Vision-Language-Action (VLA) æ¨¡å‹åœ¨æœºå™¨äººæ„ŸçŸ¥ä¸æ§åˆ¶ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œé•¿è§†é‡ä»»åŠ¡æ‰§è¡Œæ½œåŠ›ï¼Œä½†ç”±äºå…¶è‡ªå›å½’è§£ç ï¼ˆautoregressive decodingï¼‰è¿‡ç¨‹å…·æœ‰**å†…å­˜å—é™ï¼ˆmemory-boundï¼‰ç‰¹æ€§**ï¼Œå¯¼è‡´åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ¨ç†å»¶è¿Ÿæé«˜ï¼ˆé€šå¸¸ä»… 3â€“5 FPSï¼‰ï¼Œè¿œä½äºæœºå™¨äººåŠ¨æ€äº¤äº’æ‰€éœ€çš„ 20â€“30 Hz æ§åˆ¶é¢‘ç‡ã€‚è¿™ä¸€ç“¶é¢ˆä¸¥é‡é™åˆ¶äº† VLA åœ¨çœŸå®åŠ¨æ€ç¯å¢ƒä¸­çš„å®æ—¶éƒ¨ç½²ã€‚

ç°æœ‰ä¼˜åŒ–æ–¹æ³•å­˜åœ¨ä»¥ä¸‹ä¸è¶³ï¼š
- **ç³»ç»Ÿçº§ä¼˜åŒ–**ï¼ˆå¦‚ Continuous Batchingï¼‰é¢å‘å¤šç”¨æˆ·æœåŠ¡å™¨åœºæ™¯ï¼Œä¸é€‚ç”¨äºå•ç”¨æˆ·ã€ä½å»¶è¿Ÿçš„æœºå™¨äººæ§åˆ¶ï¼›
- **è¾¹ç¼˜ä¼˜åŒ–æŠ€æœ¯**ï¼ˆå¦‚é‡åŒ–ã€è’¸é¦ï¼‰è™½èƒ½å‡å°æ¨¡å‹è§„æ¨¡ï¼Œä½†æ— æ³•æ”¹å˜è§£ç é˜¶æ®µä¸²è¡Œã€å†…å­˜å¯†é›†çš„æœ¬è´¨ï¼›
- **ç®—æ³•çº§æ”¹è¿›**ï¼ˆå¦‚å¹¶è¡Œè§£ç ã€éè‡ªå›å½’ç”Ÿæˆï¼‰å¸¸éœ€é‡æ–°è®­ç»ƒï¼Œå¯èƒ½ç‰ºç‰²ç²¾åº¦æˆ–å¼•å…¥åŠ¨ä½œä¸è¿ç»­æ€§ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯**
æœ¬æ–‡æå‡ºäº† **ActionFlow** â€”â€”ä¸€ç§ä¸“ä¸ºèµ„æºå—é™è¾¹ç¼˜å¹³å°è®¾è®¡çš„**çº¯ç³»ç»Ÿçº§æ¨ç†åŠ é€Ÿæ¡†æ¶**ï¼Œæ— éœ€é‡è®­ç»ƒå³å¯æ˜¾è‘—æå‡ VLA æ¨ç†é€Ÿåº¦ã€‚

#### **æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š**

1. **Cross-Request Pipelining ç­–ç•¥**
   - å°†å•ä¸ª VLA è¯·æ±‚å†…éƒ¨çš„ `Prefill` å’Œå¤šä¸ª `Decode` é˜¶æ®µè§†ä¸ºä¸€ä¸ªâ€œå¾®è¯·æ±‚æµæ°´çº¿â€ï¼ˆmicro-request pipelineï¼‰ï¼›
   - åœ¨æ—¶é—´ç»´åº¦ä¸Šè·¨è¿ç»­è¯·æ±‚è¿›è¡Œè°ƒåº¦ï¼Œå°†å½“å‰å¸§çš„ `Prefill` ä¸å‰è‹¥å¹²å¸§çš„ `Decode` å¾®è¯·æ±‚**æ‰¹é‡åˆå¹¶æ‰§è¡Œ**ï¼Œå½¢æˆå®æµæ°´çº¿ï¼ˆmacro-pipelineï¼‰ï¼›
   - å®ç°è®¡ç®—å¯†é›†å‹ï¼ˆcompute-boundï¼‰ä¸å†…å­˜å¯†é›†å‹ï¼ˆmemory-boundï¼‰æ“ä½œçš„é«˜æ•ˆé‡å ï¼Œæœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚

2. **Cross-Request State Packed Forward ç®—å­**
   - è®¾è®¡äº†ä¸€ä¸ªèåˆç®—å­ï¼Œå°†å¤šä¸ªåˆ†æ•£çš„ Decode é˜¶æ®µä¸­çš„å°è§„æ¨¡çŸ©é˜µ-å‘é‡ä¹˜æ³•ï¼ˆGEMVï¼‰**èåˆä¸ºä¸€æ¬¡å¤§è§„æ¨¡çŸ©é˜µ-çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰**ï¼›
   - æ˜¾è‘—æé«˜ç®—æœ¯å¼ºåº¦ï¼ˆarithmetic intensityï¼‰ï¼Œä½¿å·¥ä½œè´Ÿè½½ä» memory-bound è½¬å‘ compute-boundï¼Œé‡Šæ”¾ GPU å³°å€¼ç®—åŠ›ã€‚

3. **Unified KV Ring Buffer**
   - å¼•å…¥ç»Ÿä¸€çš„ç¯å½¢ KV ç¼“å­˜ç»“æ„ï¼Œç‰©ç†ä¸Šè¿ç»­å­˜å‚¨å¤šä¸ªå¹¶å‘è¯·æ±‚çš„ KV çŠ¶æ€ï¼›
   - æ”¯æŒ Variable-Length Attentionï¼ˆVarlen-Attentionï¼‰é«˜æ•ˆè®¿é—®ä¸åŒé•¿åº¦çš„å†å²ä¸Šä¸‹æ–‡ï¼›
   - é…åˆ `INPLACEKVSHIFT` æ“ä½œå®ç°é›¶æ‹·è´æ»‘åŠ¨çª—å£æ›´æ–°ï¼Œé¿å… CPU-GPU åŒæ­¥å¼€é”€ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ActionFlow | ç°æœ‰æ–¹æ³• |
|------|-----------|----------|
| æ˜¯å¦éœ€è¦é‡è®­ç»ƒ | âŒ ä¸éœ€è¦ | âœ… å¤šæ•°éœ€è¦ï¼ˆå¦‚ parallel decodingï¼‰ |
| æ˜¯å¦ä¾èµ–å¤–éƒ¨æ‰¹å¤„ç† | âŒ å•ç”¨æˆ·å†…æ„é€ æ‰¹å¤„ç† | âœ… ä¾èµ–å¤šç”¨æˆ·è¯·æ±‚ï¼ˆå¦‚ Continuous Batchingï¼‰ |
| å¯¹ç¡¬ä»¶åˆ©ç”¨æ•ˆç‡ | â¬†ï¸ æ˜¾è‘—æå‡ï¼ˆè½¬å‘ compute-boundï¼‰ | â¬‡ï¸ Decode é˜¶æ®µä»ä¸º memory-bound |
| å¯ç»„åˆæ€§ | âœ… æ­£äº¤äºé‡åŒ–ã€Speculative Decoding ç­‰æŠ€æœ¯ | â€”â€” |
| åŠŸèƒ½ä¸€è‡´æ€§ | âœ… å®Œå…¨ç­‰ä»·è¾“å‡ºï¼ˆlosslessï¼‰ | âš ï¸ å¯èƒ½å½±å“ç”Ÿæˆè´¨é‡ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ¨¡å‹ä¸å¹³å°**
- **æ¨¡å‹**ï¼šOpenVLA-7Bï¼ˆä¸»æµå¼€æº VLA æ¨¡å‹ï¼‰
- **ç¡¬ä»¶å¹³å°**ï¼š
  - **NVIDIA Jetson AGX Orin**ï¼ˆ64GBï¼‰ï¼šå…¸å‹åµŒå…¥å¼è¾¹ç¼˜è®¾å¤‡ï¼Œä»£è¡¨èµ„æºå—é™åœºæ™¯ï¼›
  - **NVIDIA RTX 5090**ï¼šé«˜æ€§èƒ½è¾¹ç¼˜å·¥ä½œç«™ï¼ŒéªŒè¯è·¨æ¶æ„æœ‰æ•ˆæ€§ã€‚

### **è½¯ä»¶æ ˆ**
- PyTorch == 2.6.0
- Transformers == 4.49.0
- CUDA 12.6

### **è¯„ä¼°æŒ‡æ ‡**
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **FPS**ï¼ˆFrames Per Secondï¼‰ | ä¸»è¦æ€§èƒ½æŒ‡æ ‡ï¼Œè¡¡é‡ç³»ç»Ÿååç‡ |
| **Speedup** | ç›¸å¯¹äº Baseline çš„åŠ é€Ÿæ¯” |
| **Success Rate** | åœ¨ LIBERO åŸºå‡†ä¸Šçš„ä»»åŠ¡æˆåŠŸç‡ï¼ŒéªŒè¯åŠŸèƒ½æ­£ç¡®æ€§ |

### **å¯¹æ¯”åŸºçº¿æ–¹æ³•**
| æ–¹æ³• | æè¿° |
|------|------|
| **Baseline** | OpenVLA-7B æ ‡å‡†è‡ªå›å½’æ¨ç†ï¼Œæ— ä»»ä½•ç³»ç»Ÿä¼˜åŒ– |
| **Naive Pipe** | å®ç°è·¨è¯·æ±‚æµæ°´çº¿ï¼Œä½†ä½¿ç”¨åŠ¨æ€ KV ç¼“å­˜æ‹¼æ¥ + CPU åè°ƒæ‰¹å¤„ç†ï¼ˆæ— ç®—å­èåˆï¼‰ |
| **ActionFlow** | å®Œæ•´æ–¹æ¡ˆï¼šåŒ…å« Cross-Request Pipelining + Cross-Request State Packed Forward + Unified KV Ring Buffer |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **ç«¯åˆ°ç«¯æ€§èƒ½å¯¹æ¯”ï¼ˆTable 1ï¼‰**

| Method       | Platform      | FPS     | Speedup |
|--------------|---------------|---------|---------|
| Baseline     | AGX Orin      | 1.25    | 1.00x   |
| Naive Pipe   | AGX Orin      | 2.70    | 2.16x   |
| **ActionFlow** | **AGX Orin**  | **3.20**| **2.56x** |
| Baseline     | RTX 5090      | 7.62    | 1.00x   |
| Naive Pipe   | RTX 5090      | 15.60   | 2.04x   |
| **ActionFlow** | **RTX 5090**  | **19.45**| **2.55x** |

> âœ… **ç»“è®º**ï¼šActionFlow åœ¨ä¸¤ç§å¹³å°ä¸Šå‡å®ç°çº¦ **2.55Ã— åŠ é€Ÿ**ï¼Œä¸”æ€§èƒ½å¢ç›Šè·¨æ¶æ„ä¸€è‡´ã€‚

---

### **æ¶ˆèå®éªŒåˆ†æ**
- **ActionFlow vs. Naive Pipe**ï¼š
  - åœ¨ AGX Orin ä¸Šé¢å¤–å¸¦æ¥ **18.5%** æ€§èƒ½æå‡ï¼ˆ3.20 vs 2.70 FPSï¼‰ï¼›
  - åœ¨ RTX 5090 ä¸Šé¢å¤–å¸¦æ¥ **24.7%** æå‡ï¼ˆ19.45 vs 15.60 FPSï¼‰ï¼›
  - è¯æ˜ **Kernel Fusion**ï¼ˆå°¤å…¶æ˜¯ `FUSEDROPEANDWRITEKV` å’Œ `INPLACEKVSHIFT`ï¼‰å¯¹æ¶ˆé™¤åŒæ­¥æ°”æ³¡å’Œå†…å­˜å¼€é”€è‡³å…³é‡è¦ã€‚

- **æ•æ„Ÿæ€§åˆ†æï¼ˆFigure 5ï¼‰**ï¼š
  - éšç€ Decode é•¿åº¦ $K$ å¢åŠ ï¼ŒBaseline æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼ˆRTX 5090 ä¸Šä» 8.64 â†’ 2.45 FPSï¼Œé™å¹… 72%ï¼‰ï¼›
  - ActionFlow ä»…ä¸‹é™ 25%ï¼ˆ17.00 â†’ 12.72 FPSï¼‰ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼›
  - åœ¨æœ€é‡è´Ÿè½½ä¸‹ï¼ˆK=32, é•¿ Prefillï¼‰ï¼Œè¾¾åˆ° **4.06Ã— åŠ é€Ÿ**ï¼ˆRTX 5090ï¼‰å’Œ **4.36Ã— åŠ é€Ÿ**ï¼ˆAGX Orinï¼‰ã€‚

---

### **åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯ï¼ˆTable 2ï¼‰**

| Method        | Platform     | Spatial | Object | Goal  | Long  |
|---------------|--------------|--------|--------|-------|-------|
| OpenVLA       | RTX 5090     | 84.4%  | 73.8%  | 74.4% | 51.4% |
| ActionFlow    | RTX 5090     | 84.3%  | 71.2%  | 78.6% | 53.3% |
| ActionFlow    | AGX Orin     | 83.4%  | 68.8%  | 75.6% | 49.0% |

> âœ… **ç»“è®º**ï¼šActionFlow åœ¨æ‰€æœ‰ä»»åŠ¡ç±»åˆ«ä¸­ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„æˆåŠŸç‡ï¼Œæ³¢åŠ¨åœ¨ç»Ÿè®¡è¯¯å·®èŒƒå›´å†…ï¼Œ**åŠŸèƒ½å®Œå…¨ç­‰ä»·ï¼ˆfunctionally losslessï¼‰**ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **VLA æ¨ç†ç“¶é¢ˆæœ¬è´¨æ˜¯ Decode é˜¶æ®µçš„ä½æ•ˆæ€§**ï¼š
   - åˆ†æè¡¨æ˜ï¼ŒDecode é˜¶æ®µå› ç®—æœ¯å¼ºåº¦æä½ï¼ˆ~1.4 FLOPs/Byteï¼‰è€Œé™·å…¥ memory-boundï¼Œæ— æ³•å‘æŒ¥ GPU ç®—åŠ›ï¼›
   - å°æ ¸é¢‘ç¹å¯åŠ¨å¯¼è‡´å¤§é‡è°ƒåº¦å¼€é”€å’Œ GPU ç©ºè½¬ã€‚

2. **å•ç”¨æˆ·åœºæ™¯ä¹Ÿå¯æ„é€ æœ‰æ•ˆæ‰¹å¤„ç†æœºä¼š**ï¼š
   - åˆ©ç”¨ VLA è¿ç»­æ§åˆ¶çš„è‡ªç„¶æ—¶åºè¿ç»­æ€§ï¼Œå¯åœ¨å•ä¸ªè¯·æ±‚æµå†…éƒ¨æ„é€ è·¨æ—¶é—´æ­¥çš„æ‰¹å¤„ç†ï¼›
   - æå‡ºçš„ Cross-Request Pipelining æ˜¯é¦–ä¸ªåœ¨å•ç”¨æˆ·è¾¹ç¼˜åœºæ™¯ä¸‹å®ç°é«˜æ•ˆæ‰¹å¤„ç†çš„ç³»ç»Ÿæ–¹æ¡ˆã€‚

3. **ç³»ç»Ÿçº§ä¼˜åŒ–å¯ç‹¬ç«‹äºç®—æ³•å–å¾—æ˜¾è‘—æ”¶ç›Š**ï¼š
   - ActionFlow æ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„æˆ–é‡æ–°è®­ç»ƒï¼Œå³å¯å®ç° 2.55Ã— åŠ é€Ÿï¼›
   - æ‰€ææ–¹æ³•ä¸é‡åŒ–ã€æ¨æµ‹è§£ç ç­‰æ­£äº¤ï¼Œå…·å¤‡è‰¯å¥½å¯ç»„åˆæ€§ã€‚

---

### **å±€é™æ€§**
1. **ä¾èµ–å›ºå®šåŠ¨ä½œåºåˆ—é•¿åº¦ $K$**ï¼š
   - å½“å‰æ¡†æ¶å‡è®¾æ¯ä¸ªè¯·æ±‚ç”Ÿæˆçš„åŠ¨ä½œ token æ•°å›ºå®šï¼Œéš¾ä»¥ç›´æ¥æ”¯æŒå˜é•¿è¾“å‡ºï¼›
   - è‹¥ $K$ è¿‡å¤§ï¼Œä¼šå¢åŠ å†…å­˜å ç”¨å’Œåˆå§‹åŒ–å»¶è¿Ÿã€‚

2. **å¯¹çŸ­åºåˆ—ä»»åŠ¡å¢ç›Šæœ‰é™**ï¼š
   - å¯¹äº $K < 7$ çš„çŸ­åŠ¨ä½œåºåˆ—ï¼Œæµæ°´çº¿å¹¶è¡Œåº¦ä¸è¶³ï¼ŒåŠ é€Ÿæ•ˆæœå‡å¼±ã€‚

3. **Ring Buffer å†…å­˜å¼€é”€**ï¼š
   - ç»Ÿä¸€ KV Ring Buffer éœ€é¢„ç•™æœ€å¤§å®¹é‡ï¼Œå¯èƒ½å¯¼è‡´éƒ¨åˆ†å†…å­˜æµªè´¹ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **æ”¯æŒåŠ¨æ€åºåˆ—é•¿åº¦**ï¼šç»“åˆ early-exit æˆ– adaptive decodingï¼Œå®ç°å˜é•¿åŠ¨ä½œç”Ÿæˆä¸‹çš„é«˜æ•ˆæµæ°´çº¿ï¼›
2. **ä¸ç®—æ³•ä¼˜åŒ–ååŒè®¾è®¡**ï¼šæ¢ç´¢ä¸ Speculative Decodingã€Action Chunking çš„è”åˆä¼˜åŒ–ï¼›
3. **æ‰©å±•è‡³å¤šæ¨¡æ€ä¼ æ„Ÿå™¨è¾“å…¥**ï¼šæ”¯æŒè§¦è§‰ã€è¯­éŸ³ç­‰æ›´å¤šæ„ŸçŸ¥æ¨¡æ€çš„èåˆæ¨ç†ï¼›
4. **éƒ¨ç½²åˆ°çœŸå®æœºå™¨äººç³»ç»Ÿ**ï¼šåœ¨å®¶åº­æœåŠ¡ã€å·¥ä¸šåˆ†æ‹£ç­‰å®é™…åœºæ™¯ä¸­éªŒè¯ç«¯åˆ°ç«¯æ§åˆ¶èƒ½åŠ›ã€‚

---

> ğŸ”— **é¡¹ç›®åœ°å€**ï¼šhttps://anonymous.4open.science/r/ActionFlow-1D47  
> ğŸ“„ **è®ºæ–‡é“¾æ¥**ï¼šhttps://arxiv.org/abs/2512.20276

</details>

---

### 2. [Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications](https://arxiv.org/abs/2512.20953)

**Authors**: Yuxiao Wang, Yuedong Xu, Qingyang Duan, Yuxuan Liu, Lei Jiao, Yinghao Yu, Jun Wu  
**Category**: cs.DC  
**Published**: 2025-12-25  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.20953v1  

#### Abstract
The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D par...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDiving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰ä¸»æµçš„æ·±åº¦å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼ˆå¦‚ Megatron-LMã€Whaleï¼‰åœ¨**å¼‚æ„ GPU é›†ç¾¤**ä¸Šå­˜åœ¨ä»¥ä¸‹ä¸‰å¤§æŒ‘æˆ˜ï¼š
1. **å¹¶è¡Œç»“æ„å—é™äºå¯¹ç§°æ€§å‡è®¾**ï¼šä¼ ç»Ÿ 3D å¹¶è¡Œï¼ˆData Parallelism, Tensor Parallelism, Pipeline Parallelismï¼‰é€šå¸¸è¦æ±‚å„å¹¶è¡Œç»„å†… GPU æ•°é‡ä¸€è‡´ï¼Œéš¾ä»¥é€‚åº”ä¸åŒå‹å·ã€æ•°é‡ä¸å‡çš„å¼‚æ„ç¯å¢ƒã€‚
2. **è´Ÿè½½å‡è¡¡å›°éš¾**ï¼šå¼‚æ„ GPU çš„è®¡ç®—èƒ½åŠ›å’Œå†…å­˜å®¹é‡å·®å¼‚å¤§ï¼Œè‹¥ç®€å•å¹³å‡åˆ†é…æ¨¡å‹å±‚ä¼šå¯¼è‡´â€œæœ¨æ¡¶æ•ˆåº”â€ï¼Œä½æ€§èƒ½ GPU æˆä¸ºç“¶é¢ˆã€‚
3. **Spot Instance è¢«æŠ¢å åçš„æ¢å¤æ•ˆç‡ä½ä¸‹**ï¼šç°æœ‰ç³»ç»Ÿä¾èµ–äº‘å­˜å‚¨è¿›è¡Œ Checkpoint æ¢å¤ï¼Œé€šä¿¡å¸¦å®½æˆä¸ºæ¢å¤é€Ÿåº¦ç“¶é¢ˆã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šAutoHet
ä½œè€…æå‡º **AutoHet** â€”â€” ä¸€ä¸ªé¢å‘å¼‚æ„ Spot Instance GPU é›†ç¾¤çš„è‡ªåŠ¨åŒ– 3D å¹¶è¡Œè®­ç»ƒç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰æ”¯æŒéå¯¹ç§° 3D å¹¶è¡Œç»“æ„
- **ç†è®ºè§‚å¯Ÿ**ï¼šTPï¼ˆTensor Parallelismï¼‰å¿…é¡»å¯¹ç§°ï¼ˆå›  AllReduce å‰éœ€çŸ©é˜µè½¬ç½®ï¼Œå¼€é”€é«˜ï¼‰ï¼Œè€Œ PPï¼ˆPipeline Parallelismï¼‰å¯åœ¨ä¸åŒ DP ç»„é—´éå¯¹ç§°ã€‚
- åŸºäºæ­¤ï¼ŒAutoHet æ”¯æŒçµæ´»çš„è®¾å¤‡åˆ†ç»„ç­–ç•¥ï¼Œçªç ´ä¼ ç»Ÿå¯¹ç§°é™åˆ¶ã€‚

#### ï¼ˆ2ï¼‰ä¸¤é˜¶æ®µä¼˜åŒ–ç®—æ³•å®ç°æœ€ä¼˜å¹¶è¡Œè®¡åˆ’
- **ç¬¬ä¸€é˜¶æ®µï¼šæœ€å¤§åŒ–æœ‰æ•ˆè®¡ç®—èƒ½åŠ›ï¼ˆEffective Computing Powerï¼‰**
  - å°†è®¾å¤‡åˆ†ç»„å»ºæ¨¡ä¸º **éçº¿æ€§æ··åˆæ•´æ•°è§„åˆ’é—®é¢˜ï¼ˆNonlinear Mixed-Integer Programmingï¼‰**
  - ç›®æ ‡æ˜¯å¹³è¡¡å„ DP ç»„çš„æœ‰æ•ˆç®—åŠ›ï¼Œæœ€å°åŒ–æ¯è½®è¿­ä»£æ—¶é—´ã€‚
- **ç¬¬äºŒé˜¶æ®µï¼šGPU æ˜ å°„ä¸æ¨¡å‹åˆ’åˆ†**
  - è€ƒè™‘ NVLink å’Œ RDMA å¸¦å®½ä¼˜å…ˆçº§ï¼Œå°† TP é€šä¿¡ç»‘å®šåœ¨ NVLink ä¸Šã€‚
  - è®¾è®¡å¯å‘å¼ç®—æ³•å°†ä½ç®—åŠ› GPU åˆ†é…åˆ°æ—©æœŸ PP é˜¶æ®µï¼ˆå› å…¶å†…å­˜éœ€æ±‚é«˜ä½†è®¡ç®—è½»ï¼‰ï¼Œæå‡æ•´ä½“åˆ©ç”¨ç‡ã€‚

#### ï¼ˆ3ï¼‰é«˜æ•ˆçš„å¼¹æ€§æ¢å¤æœºåˆ¶ï¼ˆElastic Recoveryï¼‰
- **åˆ†å±‚ Checkpoint å­˜å‚¨**ï¼šä»¥ **layer ç²’åº¦**ä¿å­˜ `state_dict` å’Œ `optimizer states`ï¼Œä¾¿äºç»†ç²’åº¦æ¢å¤ã€‚
- **è‡ªé€‚åº”åŠ è½½ç­–ç•¥**ï¼š
  - è‹¥ TP ç»´åº¦ä¸å˜ â†’ ç›´æ¥æŒ‰å±‚åŠ è½½ï¼›
  - è‹¥ TP ç»´åº¦å¢åŠ  â†’ å¯¹å‚æ•°åˆ‡åˆ†é‡ç»„ï¼›
  - è‹¥ TP ç»´åº¦å‡å°‘ â†’ åˆå¹¶å¤šä¸ª checkpoint æ–‡ä»¶ã€‚
- **åŠ é€Ÿæ¢å¤ç­–ç•¥**ï¼š
  - ä¼˜å…ˆä»æœ¬åœ°èŠ‚ç‚¹è¯»å– checkpointï¼›
  - ç¼ºå¤±éƒ¨åˆ†æ‰ä»äº‘ç«¯ä¸‹è½½ï¼›
  - åˆ©ç”¨ RDMA åœ¨è®­ç»ƒèŠ‚ç‚¹é—´ç›´æ¥ä¼ è¾“çŠ¶æ€ï¼Œé¿å…é€šè¿‡äº‘å­˜å‚¨ä¸­è½¬ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | Megatron-LM / Whale | AutoHet |
|------|---------------------|--------|
| å¼‚æ„æ”¯æŒ | æœ‰é™ï¼ˆä»… DP æˆ– DP+PPï¼‰ | å…¨é¢æ”¯æŒéå¯¹ç§° 3D å¹¶è¡Œ |
| è´Ÿè½½å‡è¡¡ | å›ºå®šåˆ†å±‚ | åŠ¨æ€æŒ‰ç®—åŠ›åˆ†é… |
| æ¢å¤æœºåˆ¶ | æ•´ä½“æ–‡ä»¶æ¢å¤ï¼Œä¾èµ–äº‘å­˜å‚¨ | å±‚çº§æ¢å¤ + æœ¬åœ°ä¼˜å…ˆ + RDMA åŠ é€Ÿ |
| è§„åˆ’æ•ˆç‡ | æ‰‹åŠ¨é…ç½®æˆ–å—é™æœç´¢ | è‡ªåŠ¨åŒ–è§„åˆ’ï¼Œå¼€é”€ä½ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ§ª å®éªŒå¹³å°é…ç½®
- **GPU ç±»å‹**ï¼šNVIDIA A100ï¼ˆ80GBï¼‰ã€H800ï¼ˆ80GBï¼‰ã€H20ï¼ˆ100GBï¼‰
- **é›†ç¾¤è§„æ¨¡**ï¼šå…± 24 å¼  GPUï¼Œåˆ†å¸ƒåœ¨ 4 ä¸ªèŠ‚ç‚¹ï¼ˆNode 0/3: A100ï¼›Node 1: H800ï¼›Node 2: H20ï¼‰
- **é€šä¿¡æ–¹å¼**ï¼š
  - èŠ‚ç‚¹å†…ï¼šNVLinkï¼ˆ600 GB/sï¼‰
  - èŠ‚ç‚¹é—´ï¼šRoCEv2ï¼ˆ400 Gbpsï¼‰

### ğŸ“š æ¨¡å‹ä¸ä»»åŠ¡
- **æ¨¡å‹æ¶æ„**ï¼š
  - BERT-Largeï¼ˆ340M å‚æ•°ï¼‰
  - GPT-3ï¼ˆ6.7B å‚æ•°ï¼‰
  - LLaMAï¼ˆ6.7B å‚æ•°ï¼‰
- **è®­ç»ƒæ¨¡å¼**ï¼šPre-trainingï¼ˆé¢„è®­ç»ƒï¼‰

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
- **ååé‡ï¼ˆThroughputï¼‰**ï¼štokens/s
- **æ¢å¤æ—¶é—´ï¼ˆRecovery Timeï¼‰**ï¼šä» Spot Instance è¢«æŠ¢å åé‡æ–°å¯åŠ¨æ‰€éœ€æ—¶é—´
- **è§„åˆ’å¼€é”€ï¼ˆPlanning Overheadï¼‰**ï¼šç”Ÿæˆæœ€ä¼˜å¹¶è¡Œç­–ç•¥çš„æ—¶é—´
- **Profile æ—¶é—´**ï¼šæ€§èƒ½é‡‡æ ·è€—æ—¶

### âš–ï¸ åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Megatron-LM**ï¼šä¸»æµæ¨¡å‹å¹¶è¡Œæ¡†æ¶ï¼Œå‡è®¾åŒæ„ç¯å¢ƒ
- **Whale**ï¼šç¡¬ä»¶æ„ŸçŸ¥è´Ÿè½½å‡è¡¡ç³»ç»Ÿï¼Œæ”¯æŒå¼‚æ„ DP+PP
- **Varuna**ï¼šä½æˆæœ¬å¼¹æ€§è®­ç»ƒç³»ç»Ÿï¼Œç”¨äºæ¢å¤æ€§èƒ½å¯¹æ¯”

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ ç«¯åˆ°ç«¯è®­ç»ƒæ€§èƒ½ï¼ˆThroughputï¼‰
| æ¨¡å‹ | åœºæ™¯ | AutoHet vs. Megatron-LM | AutoHet vs. Whale |
|------|------|--------------------------|--------------------|
| BERT-Large | å‡åŒ€åˆ†å¸ƒ | **1.38Ã— æ›´å¿«** | â€” |
| GPT-3 | å‡åŒ€åˆ†å¸ƒ | **1.53Ã— æ›´å¿«** | **1.27Ã— æ›´å¿«** |
| LLaMA | éå‡åŒ€åˆ†å¸ƒï¼ˆH800+A100ï¼‰ | **1.79Ã— æ›´å¿«** | **1.51Ã— æ›´å¿«** |
| LLaMA | éå‡åŒ€åˆ†å¸ƒï¼ˆA100+H20ï¼‰ | **1.44Ã— æ›´å¿«** | **1.16Ã— æ›´å¿«** |

> ğŸ’¡ åœ¨éå‡åŒ€ GPU åˆ†å¸ƒä¸‹ä¼˜åŠ¿æ›´æ˜æ˜¾ï¼Œè¯´æ˜ AutoHet èƒ½æ›´å¥½åˆ©ç”¨ç¢ç‰‡åŒ–èµ„æºã€‚

### ğŸ” æ€§èƒ½åˆ†è§£å®éªŒï¼ˆGPT-3 6.7Bï¼‰
é€æ­¥æ·»åŠ æ¨¡å—çš„æ•ˆæœï¼ˆç›¸å¯¹äºåŸºç¡€ Pipeline Parallelismï¼‰ï¼š
1. **Device Grouping** â†’ +11% åå
2. **Node & Stage Mapping** â†’ +16%
3. **Workload Balancing** â†’ **+79%**

> è¡¨æ˜è´Ÿè½½å‡è¡¡æ˜¯æœ€å…³é”®çš„ä¼˜åŒ–ç¯èŠ‚ã€‚

### â±ï¸ è§„åˆ’ä¸ Profile å¼€é”€
| GPU æ•°é‡ | è§„åˆ’æ—¶é—´ï¼ˆç§’ï¼‰ | Profile æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ |
|---------|---------------|-----------------------|
| 16      | 1.23          | ~12                   |
| 24      | 5.72          | ~13                   |
| 32      | 16.96         | ~14                   |
| 64      | 159.12        | ~15.4                 |

> ç›¸æ¯” Alpaï¼ˆéœ€ 240 åˆ†é’Ÿæœç´¢ç­–ç•¥ï¼‰ï¼ŒAutoHet çš„æ•°å­¦å»ºæ¨¡å¤§å¹…å‡å°‘äº†æ— æ•ˆæœç´¢ç©ºé—´ï¼Œ**é€Ÿåº¦å¿«è¿‘ 10 å€**ã€‚

### ğŸ”„ å¼¹æ€§æ¢å¤æ€§èƒ½ï¼ˆvs. Varunaï¼‰
| åœºæ™¯ | æ¢å¤æ–¹å¼ | AutoHet åŠ é€Ÿæ¯” |
|------|--------|----------------|
| A | å®Œå…¨æœ¬åœ°å¯æ¢å¤ | **4.38Ã— æ›´å¿«** |
| B | éƒ¨åˆ†ç¼ºå¤±ï¼Œéœ€ä»äº‘è¡¥å…¨ | **1.49Ã— æ›´å¿«** |
| C | æ–°å¢èŠ‚ç‚¹ï¼ŒRDMA å†…éƒ¨åŒæ­¥ | **3.59Ã— æ›´å¿«** |

> åˆ©ç”¨æœ¬åœ°å­˜å‚¨å’Œ RDMA æ˜¾è‘—é™ä½æ¢å¤å»¶è¿Ÿï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡æ‰©å±•æ—¶ä¼˜åŠ¿æ˜¾è‘—ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **TP å¿…é¡»å¯¹ç§°ï¼ŒPP å¯éå¯¹ç§°**ï¼šè¿™æ˜¯æ„å»ºé«˜æ•ˆå¼‚æ„å¹¶è¡Œçš„åŸºç¡€å‰æã€‚
2. **éå¯¹ç§°å¹¶è¡Œç»“æ„èƒ½æ˜¾è‘—æå‡èµ„æºåˆ©ç”¨ç‡**ï¼šå°¤å…¶åœ¨ GPU æ•°é‡ä¸å‡ã€å‹å·æ··æ‚çš„çœŸå®åœºæ™¯ä¸­ã€‚
3. **è´Ÿè½½å‡è¡¡åº”ç»“åˆç®—åŠ›ä¸å†…å­˜ç‰¹æ€§**ï¼šä½æ€§èƒ½ GPU æ›´é€‚åˆæ‰¿æ‹…å†…å­˜å¯†é›†å‹çš„å‰åº PP é˜¶æ®µã€‚
4. **Checkpoint åº”ä»¥ layer ç²’åº¦ç®¡ç†**ï¼šæ”¯æŒåŠ¨æ€å¹¶è¡Œç»“æ„è°ƒæ•´ä¸‹çš„çµæ´»æ¢å¤ã€‚
5. **æœ¬åœ°ä¼˜å…ˆ + RDMA æ˜¯å¿«é€Ÿæ¢å¤çš„å…³é”®è·¯å¾„**ï¼šè¿œä¼˜äºçº¯äº‘å­˜å‚¨æ–¹æ¡ˆã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **æœªè€ƒè™‘ CPU æˆ–å†…å­˜ç“¶é¢ˆ**ï¼šç›®å‰èšç„¦ GPU å¼‚æ„ï¼Œæœªæ¶‰åŠä¸»æœºä¾§èµ„æºçº¦æŸã€‚
- **ä¾èµ–å‡†ç¡®çš„ Profiling æ•°æ®**ï¼šè™½ç„¶æå‡ºäº†åŠ é€Ÿç­–ç•¥ï¼Œä½†ä»éœ€ä¸€å®šé¢„çƒ­æ—¶é—´ã€‚
- **æš‚æœªæ”¯æŒè·¨æ•°æ®ä¸­å¿ƒéƒ¨ç½²**ï¼šRDMA ä¾èµ–ä½å»¶è¿Ÿç½‘ç»œï¼Œåœ¨å¹¿åŸŸç½‘ä¸­å¯èƒ½å—é™ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³ **CPU-GPU ååŒå¼‚æ„è®­ç»ƒ**
- æ”¯æŒ **åŠ¨æ€è°ƒæ•´å¹¶è¡Œç­–ç•¥**ï¼ˆduring trainingï¼‰
- æ¢ç´¢ **æ›´æ™ºèƒ½çš„åœ¨çº¿ Profiling æœºåˆ¶**
- ç»“åˆ **Spot Instance é¢„æµ‹æ¨¡å‹** å®ç°ä¸»åŠ¨å®¹é”™è°ƒåº¦

---

## æ€»ç»“
AutoHet æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§è§£å†³ **å¼‚æ„ Spot Instance GPU ä¸Š 3D å¹¶è¡Œè®­ç»ƒä¸å¼¹æ€§æ¢å¤** çš„è‡ªåŠ¨åŒ–æ¡†æ¶ã€‚å®ƒé€šè¿‡ç†è®ºåˆ†ææŒ‡å¯¼ç³»ç»Ÿè®¾è®¡ï¼Œå®ç°äº†ï¼š
- **æ›´é«˜åå**ï¼ˆæœ€é«˜ 1.79Ã— æå‡ï¼‰
- **æ›´å¿«æ¢å¤**ï¼ˆæœ€é«˜ 4.38Ã— åŠ é€Ÿï¼‰
- **æ›´ä½è§„åˆ’å¼€é”€**ï¼ˆæ¯” Alpa å¿« 10 å€ï¼‰

è¯¥å·¥ä½œå¯¹å®é™…ç”Ÿäº§ç¯å¢ƒä¸­å¤§è§„æ¨¡ LLM è®­ç»ƒå…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æˆæœ¬æ•æ„Ÿã€èµ„æºæ³¢åŠ¨å¤§çš„äº‘å¹³å°ä¸Šã€‚

</details>

---

### 3. [SHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression](https://arxiv.org/abs/2512.20635)

**Authors**: Zeli Su, Ziyin Zhang, Wenzheng Zhang, Zhou Liu, Guixian Xu, Wentao Zhang  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.20635v1  

#### Abstract
Transformer encoders are widely deployed in large-scale web services for natural language understanding tasks such as text classification, semantic retrieval, and content ranking. However, their high inference latency and memory consumption pose significant challenges for real-time serving and scala...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
Transformer ç¼–ç å™¨åœ¨å¤§è§„æ¨¡ Web æœåŠ¡ä¸­çš„è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€è¯­ä¹‰æ£€ç´¢ã€å†…å®¹æ’åºï¼‰ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶é«˜æ¨ç†å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—ä¸¥é‡åˆ¶çº¦äº†å®æ—¶æ€§å’Œå¯æ‰©å±•æ€§ã€‚è¿™ç§ä½æ•ˆä¸»è¦æºäº**æ¶æ„å†—ä½™**ï¼Œå°¤å…¶æ˜¯ **Multi-Head Attention** ä¸­å­˜åœ¨å¤§é‡æœªè¢«å……åˆ†åˆ©ç”¨çš„ attention heads å’Œè®¡ç®—å¯†é›†çš„ Feed-Forward Network (FFN)ã€‚

ç°æœ‰å‹ç¼©æ–¹æ³•å­˜åœ¨ä»¥ä¸‹ä¸è¶³ï¼š
- **ç²—ç²’åº¦å‰ªæ**ï¼ˆå¦‚å±‚ä¸¢å¼ƒï¼‰å¯¼è‡´ç²¾åº¦æ€¥å‰§ä¸‹é™ï¼›
- **ç»†ç²’åº¦å‰ªæ**ä¾èµ–å¯å‘å¼è¯„åˆ†æˆ–æ©ç æœºåˆ¶ï¼Œéš¾ä»¥éƒ¨ç½²ï¼›
- å¤šæ•°æ–¹æ³•ä»…å…³æ³¨ attention headsï¼Œå¿½ç•¥å ä¸»å¯¼åœ°ä½çš„ FFN å†—ä½™ï¼›
- å¼•å…¥åŠ¨æ€è·¯ç”±ä¼šå¸¦æ¥æ¨ç†å¼€é”€ï¼Œä¸åˆ©äºç”Ÿäº§ç¯å¢ƒã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šSHRPï¼ˆSpecialized Head Routing and Pruningï¼‰
SHRP æ˜¯ä¸€ç§**ç»“æ„åŒ–å‰ªææ¡†æ¶**ï¼Œé€šè¿‡å°†æ¯ä¸ª attention head è§†ä¸ºç‹¬ç«‹â€œä¸“å®¶â€ï¼Œå¹¶ç»“åˆå…±äº«è½»é‡çº§ Expander-FFNï¼Œå®ç°é«˜æ•ˆç¼–ç å™¨å‹ç¼©ã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
1. **Expert Attention è®¾è®¡**
   - å°†æ ‡å‡† Multi-Head Attention æ”¹é€ ä¸ºæ¨¡å—åŒ–çš„ **Expert Attention**ï¼Œæ¯ä¸ª head ä½œä¸ºç‹¬ç«‹ä¸“å®¶åˆ†æ”¯ï¼›
   - æ‰€æœ‰ head è¾“å‡ºä¸å†æ‹¼æ¥ï¼Œè€Œæ˜¯åˆ†åˆ«é€šè¿‡ä¸€ä¸ªå…±äº«çš„è½»é‡ **Expander FFN** è¿›è¡Œç»´åº¦æ¢å¤ä¸éçº¿æ€§å˜æ¢ï¼›
   - å®ç° attention ä¸ FFN å­å±‚çš„è”åˆå‰ªæã€‚

2. **ä¸¤é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥**
   - **ç¬¬ä¸€é˜¶æ®µï¼šBalanced Explorationï¼ˆè´Ÿè½½å‡è¡¡æ¢ç´¢ï¼‰**
     - å¼•å…¥åŸºäº KL æ•£åº¦çš„ load balancing lossï¼Œé˜²æ­¢æ—©æœŸä¸“å®¶å´©æºƒï¼Œç¡®ä¿æ‰€æœ‰ head éƒ½èƒ½è·å¾—æ¢¯åº¦ä¿¡å·ï¼›
   - **ç¬¬äºŒé˜¶æ®µï¼šExpert Specializationï¼ˆä¸“å®¶ä¸“ä¸šåŒ–ï¼‰**
     - åœç”¨å¹³è¡¡æŸå¤±ï¼Œå…è®¸é—¨æ§æœºåˆ¶è‡ªç„¶é€‰æ‹©æœ€ä¼˜ headï¼Œä¿ƒè¿› head åŠŸèƒ½ç‰¹åŒ–ï¼›
   - åˆ†å±‚é€æ­¥è½¬æ¢ï¼ˆlayer-wise conversionï¼‰ï¼Œé¿å…è®­ç»ƒä¸ç¨³å®šã€‚

3. **Top-1 è·¯ç”± + åå¤„ç†å‰ªææœºåˆ¶**
   - ä½¿ç”¨ **Top-1 gating**ï¼šæ¯å±‚ä»…æ¿€æ´»ä¸€ä¸ªå¾—åˆ†æœ€é«˜çš„ headï¼›
   - è®­ç»ƒå®Œæˆåç»Ÿè®¡å„ head çš„ä½¿ç”¨é¢‘ç‡ï¼Œç§»é™¤ä½é¢‘ä¸“å®¶ï¼›
   - ç§»é™¤è·¯ç”±æ¨¡å—ï¼Œå¾—åˆ°**é™æ€ã€ç¡®å®šæ€§æ¨¡å‹**ï¼Œæ— ä»»ä½•æ¨ç†æ—¶åŠ¨æ€è¡Œä¸ºã€‚

4. **ç«¯åˆ°ç«¯éƒ¨ç½²å‹å¥½è®¾è®¡**
   - å‰ªæåæ¨¡å‹æ— éœ€å¾®è°ƒå³å¯ç›´æ¥éƒ¨ç½²ï¼›
   - æ¶ˆé™¤è·¯ç”±å¼€é”€ï¼Œæ˜¾è‘—æå‡ååé‡ï¼›
   - ç»“æ„æ¸…æ™°ï¼Œä¾¿äºåˆ†æä¸ä¼˜åŒ–ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ Michel et al., Voita et al., DSPï¼‰ | SHRP |
|------|---------------------------------------------|------|
| å‰ªæå¯¹è±¡ | ä»… attention heads | **joint pruning** of attention + FFN |
| å‰ªæä¾æ® | å¯å‘å¼è¯„åˆ†ã€æ¢¯åº¦æ•æ„Ÿæ€§ã€è¾…åŠ©æŸå¤± | **usage-driven**ï¼ŒåŸºäºå®é™…è¿è¡Œç»Ÿè®¡ |
| æ¨ç†æ¨¡å¼ | é™æ€ç»“æ„æˆ–ä¿ç•™å†—ä½™ | **å®Œå…¨å»é™¤è·¯ç”±æ¨¡å—**ï¼Œçº¯é™æ€æ‰§è¡Œ |
| å‹ç¼©å¼ºåº¦ | æ¸©å’Œå‰ªæå°šå¯ï¼Œæ¿€è¿›å‰ªææ€§èƒ½å´©å¡Œ | æç«¯å‰ªæä¸‹ä»ä¿æŒå¯ç”¨æ€§èƒ½ï¼ˆå¦‚ 11/12 å±‚å‰ªæï¼‰ |
| éƒ¨ç½²å…¼å®¹æ€§ | å¯èƒ½éœ€å®šåˆ¶æ¨ç†å¼•æ“ | **æ ‡å‡† Transformer æ¨ç†æµç¨‹**ï¼Œæ— ç¼é›†æˆ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
åœ¨ **GLUE benchmark** ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–ä¸ƒé¡¹å…¸å‹ NLU ä»»åŠ¡ï¼š
- MNLIï¼ˆè‡ªç„¶è¯­è¨€æ¨æ–­ï¼‰
- QQPï¼ˆå¥å­å¯¹ç›¸ä¼¼åº¦ï¼‰
- QNLIï¼ˆé—®ç­”è•´å«ï¼‰
- SST-2ï¼ˆæƒ…æ„Ÿåˆ†ç±»ï¼‰
- MRPCï¼ˆé‡Šä¹‰æ£€æµ‹ï¼‰
- RTEï¼ˆæ–‡æœ¬è•´å«ï¼‰
- CoLAï¼ˆè¯­æ³•å¯æ¥å—æ€§ï¼‰

ä½¿ç”¨ **BERT-base-uncased**ï¼ˆ12 å±‚ï¼Œæ¯å±‚ 12 ä¸ª headï¼‰ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
- **ç¡¬ä»¶å¹³å°**ï¼šå•å¼  NVIDIA A800 GPUï¼ˆ48GBï¼‰
- **æ‰¹å¤§å°**ï¼š64ï¼ˆéµå¾ª GLUE æ ‡å‡†ï¼‰
- **ä¼˜åŒ–å™¨**ï¼šAdamWï¼Œå­¦ä¹ ç‡ $2 \times 10^{-5}$ï¼Œweight decay=0.01
- **å­¦ä¹ ç‡è°ƒåº¦**ï¼šcosine annealing with 10% warmup
- **æ¢¯åº¦è£å‰ª**ï¼šmax-norm=1.0

#### å‰ªæåè®®ï¼š
- é€æ­¥æ›¿æ¢å‰ $Z$ å±‚ä¸º Expert Attention æ¨¡å—ï¼ˆ$Z = 2, 4, ..., 11$ï¼‰ï¼›
- é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆBalanced â†’ Specializationï¼‰ï¼›
- è®­ç»ƒç»“æŸåè®°å½•éªŒè¯é›†ä¸Šçš„ head ä½¿ç”¨é¢‘ç‡ï¼›
- ä¿ç•™æ¯å±‚ä½¿ç”¨æœ€é¢‘ç¹çš„ top-$m$ ä¸ª headï¼Œå…¶ä½™åˆ é™¤ï¼›
- æœ€ç»ˆç§»é™¤ routerï¼Œç”Ÿæˆç´§å‡‘æ¨¡å‹ã€‚

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| ç±»åˆ« | æŒ‡æ ‡ |
|------|------|
| **å‡†ç¡®æ€§** | å„ä»»åŠ¡ accuracyï¼Œå¹³å‡ performance retention |
| **å‹ç¼©ç¨‹åº¦** | å‚æ•°å‡å°‘æ¯”ä¾‹ï¼ˆ%ï¼‰ï¼ŒFLOPs å‰©ä½™æ¯”ä¾‹ï¼ˆ%ï¼‰ |
| **æ¨ç†æ•ˆç‡** | ååé‡ï¼ˆsamples/secï¼Œbatch=64ï¼‰ï¼Œå»¶è¿Ÿï¼ˆms/sampleï¼‰ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
ä¸ä»¥ä¸‹ä¸»æµ attention head pruning æ–¹æ³•æ¯”è¾ƒï¼š
- **Michel et al.**ï¼šåŸºäºæ¢¯åº¦é‡è¦æ€§çš„å‰ªæ
- **Voita et al.**ï¼šå¼•å…¥è¾…åŠ©æŸå¤±å¼•å¯¼äºŒå€¼åŒ– head é€‰æ‹©
- **Pipelined DSP / Joint DSP**ï¼šè¿­ä»£æˆ–ç«¯åˆ°ç«¯ä¼˜åŒ– head ç§»é™¤
- **STE**ï¼šç›´é€šä¼°è®¡å™¨ç”¨äºå¯å¾®å‰ªæ

æ‰€æœ‰æ–¹æ³•æ§åˆ¶ç›¸åŒä¿ç•™ head æ•°é‡ä»¥å…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1 & Table 4ï¼‰

| å‰ªææ¯”ä¾‹ï¼ˆå±‚æ•°ï¼‰ | å‚æ•°å‡å°‘ | FLOPs å‰©ä½™ | ååå¢ç›Š | å‡†ç¡®ç‡ä¿ç•™ |
|------------------|----------|------------|-----------|-------------|
| 6/12             | 48.2%    | 51.7%      | 1.81Ã—     | 93.1%       |
| 8/12             | 64.3%    | 35.7%      | 2.46Ã—     | 89.2%       |
| 10/12            | 80.4%    | 19.6%      | 3.82Ã—     | 89.6%       |
| **11/12**        | **88.5%**| **11.5%**  | **4.24Ã—** | **84.4%**   |

> ğŸ’¡ åœ¨æç«¯å‹ç¼©åœºæ™¯ï¼ˆä»…ä¿ç•™ 1 å±‚ï¼‰ï¼ŒSHRP ä»ç»´æŒè¶…è¿‡ **84% çš„åŸå§‹å‡†ç¡®ç‡**ï¼ŒåŒæ—¶å®ç° **4.2Ã— ååæå‡**ï¼ŒFLOPs é™è‡³åŸæ¨¡å‹çš„ **11.5%**ã€‚

---

### ğŸ“Š ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ï¼ˆä»¥ MNLI ä¸ºä¾‹ï¼Œè§ Table 2ï¼‰

| ä¿ç•™ heads | Michel et al. | Voita et al. | STE | Joint DSP | **SHRP (Ours)** |
|------------|---------------|--------------|-----|-----------|------------------|
| 12         | 0.4059        | 0.7691       | 0.7379 | 0.7974     | **0.6853**       |
| å‚æ•°å‡å°‘   | 24.0%         | â€”            | â€”   | 24.0%     | **88.5%**        |

> â— å°½ç®¡ç»å¯¹ accuracy ç•¥ä½äºéƒ¨åˆ†åŸºçº¿ï¼ˆå¦‚ Joint DSPï¼‰ï¼Œä½† SHRP å®ç°äº†**è¿œè¶…çš„å‚æ•°å‹ç¼©ç‡**ï¼ˆ88.5% vs 24%ï¼‰ï¼Œä¸”æ¨¡å‹æ›´å°ã€æ›´å¿«ã€æ›´é€‚åˆéƒ¨ç½²ã€‚

æ­¤å¤–ï¼Œåœ¨ moderate pruning ä¸‹ï¼ˆå¦‚ 96 headsï¼‰ï¼ŒSHRP è¡¨ç°ä¸æœ€ä½³åŸºçº¿ç›¸å½“ç”šè‡³ç•¥ä¼˜ï¼ˆ0.8409 vs 0.8441ï¼‰ï¼ŒåŒæ—¶å‹ç¼©æ›´å¼ºã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆTop-k è·¯ç”±å½±å“ï¼Œè§ Table 3 & 5ï¼‰

ç ”ç©¶ä¸åŒ Top-k è·¯ç”±ç­–ç•¥å¯¹å‰ªæåæ€§èƒ½çš„å½±å“ï¼ˆåœ¨ QQP ä¸Šæµ‹è¯•ï¼‰ï¼š

| Top-k | å¹³å‡ accuracy å·®å¼‚ï¼ˆpost-prune âˆ’ pre-pruneï¼‰ |
|-------|---------------------------------------------|
| **Top-1** | **+0.0021**ï¼ˆè½»å¾®æå‡ï¼‰                    |
| Top-3 | -0.2490                                     |
| Top-5 | -0.2775                                     |
| Top-7 | -0.2817                                     |
| Top-9 | -0.2769                                     |
| Top-11| -0.3039                                     |

#### å‘ç°ï¼š
- **Top-1 æ˜¯å”¯ä¸€ç¨³å®šé…ç½®**ï¼Œå‰ªæåæ€§èƒ½åŸºæœ¬ä¸å˜ç”šè‡³ç•¥æœ‰æå‡ï¼›
- æ›´å¤§ $k$ å¯¼è‡´ä¸¥é‡æ€§èƒ½é€€åŒ–ï¼Œå› å¤šä¸“å®¶å…±å­˜å¼•å‘å‚æ•°å¹²æ‰°ï¼›
- Top-1 æä¾›æ¸…æ™°çš„ usage ç»Ÿè®¡ï¼Œä¾¿äºè¯†åˆ«å†—ä½™ headï¼›
- higher $k$ è·¯ç”±æ¨¡å¼å™ªå£°å¤§ï¼Œå‰ªææ•ˆæœå·®ã€‚

> âœ… éªŒè¯äº† Top-1 è·¯ç”±æ˜¯å®ç°é«˜æ•ˆã€å¯è§£é‡Šå‰ªæçš„å…³é”®è®¾è®¡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **attention heads å¯è§†ä¸ºç‹¬ç«‹ä¸“å®¶**ï¼Œé€šè¿‡ modular design å¯æœ‰æ•ˆæ”¯æŒç»“æ„åŒ–å‰ªæï¼›
2. **Top-1 usage-driven routing + pruning** æ˜¯å®ç°é«˜å‹ç¼©æ¯”ä¸é«˜æ€§èƒ½å¹³è¡¡çš„æœ€ä½³è·¯å¾„ï¼›
3. **è”åˆå‰ªæ attention ä¸ FFN** æ˜¾è‘—ä¼˜äºä»…å‰ª attention çš„ä¼ ç»Ÿæ–¹æ³•ï¼›
4. **æ¸è¿›å¼è®­ç»ƒ + ä¸¤é˜¶æ®µç›®æ ‡**ï¼ˆå…ˆå‡è¡¡æ¢ç´¢ï¼Œåä¸“å®¶ç‰¹åŒ–ï¼‰èƒ½æœ‰æ•ˆé¿å…è®­ç»ƒå´©æºƒå¹¶è¯±å¯¼å¯å‰ªæç»“æ„ï¼›
5. å‰ªæåçš„æ¨¡å‹ä¸ä»…æ›´å°æ›´å¿«ï¼Œè€Œä¸”åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œè¯´æ˜å†—ä½™ head å¯èƒ½å¼•å…¥å™ªå£°ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **å½“å‰èšç„¦äº encoder-only æ¶æ„**ï¼ˆå¦‚ BERTï¼‰ï¼Œå°šæœªæ‰©å±•è‡³ decoder-onlyï¼ˆå¦‚ LLaMAï¼‰æˆ– encoder-decoder æ¨¡å‹ï¼›
2. **Expert Attention æ”¹å˜äº†åŸå§‹ MHA çš„ä¿¡æ¯èåˆæ–¹å¼**ï¼Œå¯èƒ½å½±å“æŸäº›éœ€è¦å¤šå¤´ååŒçš„ä»»åŠ¡ï¼›
3. è™½ç„¶ Top-1 è¡¨ç°æœ€å¥½ï¼Œä½†ä¹Ÿæ„å‘³ç€**æ¯å±‚åªåˆ©ç”¨å•ä¸€ head çš„ä¿¡æ¯**ï¼Œæ½œåœ¨è¡¨è¾¾èƒ½åŠ›å—é™ï¼›
4. å½“å‰å‰ªæç­–ç•¥åŸºäºå…¨å±€ usage ç»Ÿè®¡ï¼Œæœªè€ƒè™‘è¾“å…¥æ¡ä»¶ä¸‹çš„åŠ¨æ€ç¨€ç–æ€§ä¿ç•™ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. å°† SHRP æ‰©å±•è‡³ **decoder-based models** å’Œ **large language models**ï¼›
2. æ¢ç´¢ **conditional expert selection**ï¼ˆè¾“å…¥æ„ŸçŸ¥è·¯ç”±ï¼‰ä»¥è¿›ä¸€æ­¥æå‡æ•ˆç‡ï¼›
3. ç»“åˆ **quantization** æˆ– **low-rank decomposition** å®ç°æ··åˆå‹ç¼©ï¼›
4. åœ¨çœŸå®å·¥ä¸šç³»ç»Ÿä¸­éƒ¨ç½² SHRP æ¨¡å‹ï¼Œæµ‹é‡ç«¯åˆ°ç«¯å»¶è¿Ÿä¸èµ„æºèŠ‚çœï¼›
5. ç ”ç©¶å¦‚ä½•ä¿ç•™æ›´å¤šè¯­ä¹‰å¤šæ ·æ€§ä¸“å®¶ï¼Œé¿å…è¿‡åº¦ç®€åŒ–ã€‚

---

## æ€»ç»“
SHRP æå‡ºäº†ä¸€ç§æ–°é¢–çš„ã€é¢å‘éƒ¨ç½²çš„ Transformer ç¼–ç å™¨å‹ç¼©èŒƒå¼ã€‚å®ƒé€šè¿‡ **Expert Attention + Shared Expander-FFN + Top-1 Routing + Usage-Based Pruning** çš„ç»„åˆï¼Œå®ç°äº†ï¼š
- é«˜è¾¾ **88.5% å‚æ•°å‡å°‘**ï¼Œ
- **4.2Ã— ååæå‡**ï¼Œ
- ä»…æŸå¤±çº¦ **15% å‡†ç¡®ç‡**ï¼ˆæç«¯å‹ç¼©ä¸‹ä»ä¿ç•™ 84%+ï¼‰ï¼Œ
- å¹¶äº§å‡º**é™æ€ã€ç¡®å®šæ€§ã€æ— éœ€ç‰¹æ®Šæ¨ç†æ”¯æŒ**çš„è½»é‡æ¨¡å‹ã€‚

è¯¥å·¥ä½œæ¨åŠ¨äº† model compression ä»â€œå¯å‘å¼å‰ªæâ€å‘â€œå¯è§£é‡Šã€ç»“æ„åŒ–ã€éƒ¨ç½²å°±ç»ªâ€çš„ç³»ç»Ÿè®¾è®¡è½¬å˜ï¼Œå…·æœ‰å¾ˆå¼ºçš„å®é™…åº”ç”¨ä»·å€¼ã€‚

</details>

---

### 4. [Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality](https://arxiv.org/abs/2512.20968)

**Authors**: Sirui Chen, Jingji Chen, Siqi Zhu, Ziheng Jiang, Yanghua Peng, Xuehai Qian  
**Category**: cs.DC  
**Published**: 2025-12-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.20968v1  

#### Abstract
Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-At...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ä¸»æµçš„åˆ†å¸ƒå¼æ³¨æ„åŠ›æœºåˆ¶ **Ring-Attention** åœ¨æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼ˆcontext windowï¼‰æ—¶é¢ä¸´ä¸¥é‡çš„é€šä¿¡ç“¶é¢ˆã€‚å…¶é€šä¿¡é‡éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼Œå¯¼è‡´åœ¨å¤§è§„æ¨¡ GPU é›†ç¾¤ä¸Šæ‰©å±•æ€§å·®ï¼Œé€šä¿¡å¼€é”€å¯å å‰å‘ä¼ æ’­æ—¶é—´çš„ **91.5%**ï¼ˆåœ¨ 128 GPUs ä¸Šå¤„ç† 1M tokens æ—¶ï¼‰ï¼Œä¸¥é‡é™åˆ¶äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„åˆ†å¸ƒå¼æ³¨æ„åŠ›ç®—æ³•â€”â€”**Mesh-Attention**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **åŸºäºçŸ©é˜µçš„å»ºæ¨¡è§†è§’ï¼ˆAssignment Matrix, AMï¼‰**  
  å°†åˆ†å¸ƒå¼æ³¨æ„åŠ›ä»»åŠ¡å½¢å¼åŒ–ä¸ºä¸€ä¸ª $n \times n$ çš„åˆ†é…çŸ©é˜µï¼ˆAMï¼‰ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´  $AM[i][j]$ è¡¨ç¤ºè´Ÿè´£è®¡ç®—ç¬¬ $i$ ä¸ª Q chunk å’Œç¬¬ $j$ ä¸ª KV chunk ä¹‹é—´æ³¨æ„åŠ›çš„ GPU IDã€‚è¿™ä¸€æŠ½è±¡æ¨¡å‹æ­ç¤ºäº†é€šä¿¡ä¸è®¡ç®—ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸ºä¼˜åŒ–æä¾›äº†ç†è®ºåŸºç¡€ã€‚

- **äºŒç»´ Tile åˆ†é…ç­–ç•¥**  
  ä¸åŒäº Ring-Attention çš„ä¸€ç»´è¡Œ/åˆ—åˆ’åˆ†ï¼ˆå³æ¯ä¸ª GPU è´Ÿè´£æ•´è¡Œæˆ–æ•´åˆ—ï¼‰ï¼ŒMesh-Attention å°† AM åˆ’åˆ†ä¸ºäºŒç»´çš„ $(a \times b)$ tileï¼Œæ¯ä¸ª GPU è¢«åˆ†é…ä¸€ä¸ª tileã€‚è¿™ç§è®¾è®¡å…è®¸é€šè¿‡è°ƒæ•´ tile å½¢çŠ¶ï¼ˆå¦‚ $a=\sqrt{n}, b=n/a$ï¼‰æ¥è°ƒèŠ‚ **Communication-Computation Ratio (CommCom)**ï¼Œä»è€Œå®ç°æ›´ä¼˜çš„æ•°æ®å±€éƒ¨æ€§ã€‚

- **Local Q-KV Property**  
  é€šè¿‡é‡æ˜ å°„ KV chunk çš„ç´¢å¼•ï¼Œç¡®ä¿æ¯ä¸ª GPU éƒ½èƒ½æ‰§è¡Œå…¶æœ¬åœ° Q ä¸å…¶æœ¬åœ° KV ä¹‹é—´çš„è®¡ç®—ï¼Œé¿å…ä¸å¿…è¦çš„è¿œç¨‹è®¿é—®ï¼Œè¿›ä¸€æ­¥æå‡æ•°æ®å±€éƒ¨æ€§ã€‚

- **é«˜æ•ˆçš„è°ƒåº¦ç”Ÿæˆç®—æ³•ï¼ˆGreedy Schedulerï¼‰**  
  æå‡ºä¸€ç§å¸¦çº¦æŸçš„è´ªå¿ƒç®—æ³•ï¼Œåœ¨æ»¡è¶³é€šä¿¡ä¾èµ–çš„å‰æä¸‹ï¼Œæœ€å¤§åŒ–è®¡ç®—ä¸é€šä¿¡çš„é‡å ã€‚è¯¥ç®—æ³•åŸºäºâ€œæ”¶ç›Šâ€ï¼ˆprofitï¼‰æ¦‚å¿µï¼šé€‰æ‹©èƒ½è§£é”æœ€å¤šå¾…æ‰§è¡Œè®¡ç®—å—çš„é€šä¿¡æ“ä½œï¼ŒåŒæ—¶å»¶è¿Ÿéå…³é”®è·¯å¾„ä¸Šçš„è®¡ç®—ä»¥ä¼˜åŒ–é‡å ã€‚

- **ç†è®ºé€šä¿¡å¤æ‚åº¦é™ä½**  
  ç†è®ºåˆ†æè¡¨æ˜ï¼ŒMesh-Attention çš„é€šä¿¡å¤æ‚åº¦ä» Ring-Attention çš„ $\mathcal{O}(Nd)$ é™è‡³ $\mathcal{O}(\sqrt{n} Nd)$ï¼Œå®ç°äº† $\sqrt{n}$ å€çš„æ¸è¿‘æ”¹è¿›ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | ä¼˜åŠ¿ |
|------|------|
| **vs Ring-Attention** | æ˜¾è‘—å‡å°‘é€šä¿¡é‡ï¼ˆå¹³å‡ 79%ï¼‰ã€æå‡é€Ÿåº¦ï¼ˆå¹³å‡ 2.9Ã—ï¼‰ã€æ›´å¼ºçš„å¯æ‰©å±•æ€§ |
| **vs DS-Ulysses** | ä¸å— attention head æ•°é‡é™åˆ¶ï¼Œæ”¯æŒä»»æ„è§„æ¨¡å¹¶è¡Œ |
| **vs StarTrail** | æ¶ˆé™¤å†—ä½™é€šä¿¡ï¼Œé€šä¿¡ç»“æ„æ›´é«˜æ•ˆï¼Œé‡å èƒ½åŠ›æ›´å¼º |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### å®éªŒå¹³å°
- **ç¡¬ä»¶ç¯å¢ƒ**ï¼š256 GPUs é›†ç¾¤ï¼ˆé™¤éç‰¹åˆ«è¯´æ˜ï¼‰
- **æ¨¡å‹é…ç½®**ï¼š
  - Attention heads: 32
  - Head dimension: 128
  - Hidden size: 4096
  - åºåˆ—é•¿åº¦ï¼šæœ€é«˜è¾¾ 4M tokensï¼ˆç”¨äº GQA æµ‹è¯•ï¼‰

### æ•°æ®é›†ä¸ä»»åŠ¡
- å¹¶æœªä½¿ç”¨ä¼ ç»Ÿ NLP æ•°æ®é›†è¿›è¡Œç«¯åˆ°ç«¯ä»»åŠ¡è¯„ä¼°ã€‚
- å®éªŒèšç„¦äº **åˆ†å¸ƒå¼æ³¨æ„åŠ›ç®—å­æœ¬èº«çš„æ€§èƒ½åŸºå‡†æµ‹è¯•**ï¼Œæ¨¡æ‹Ÿä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„å‰å‘/åå‘ä¼ æ’­è¡Œä¸ºã€‚
- åŒ…å«å¯¹ **å› æœæ©ç ï¼ˆcausal maskï¼‰** åœºæ™¯çš„æ”¯æŒéªŒè¯ï¼ˆé€šè¿‡ Striped Attention é›†æˆï¼‰ã€‚

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Throughput** | è¿­ä»£é€Ÿåº¦ï¼ˆunit: $10^{-2}$ iter/sï¼‰ |
| **MFU (Model FLOPs Utilization)** | æ¨¡å‹æµ®ç‚¹åˆ©ç”¨ç‡ï¼Œè¡¡é‡ç¡¬ä»¶åˆ©ç”¨æ•ˆç‡ |
| **End-to-end Runtime** | æ€»æ‰§è¡Œæ—¶é—´ï¼ˆåŒºåˆ†å¼º/å¼±å¯æ‰©å±•æ€§ï¼‰ |
| **Communication Volume** | æ¯ GPU é€šä¿¡æ•°æ®é‡ï¼ˆå•ä½ï¼šelementsï¼‰ |
| **Memory Consumption** | å³°å€¼æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰ |
| **Speedup** | ç›¸å¯¹äº Ring-Attention çš„åŠ é€Ÿæ¯” |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Ring-Attention**ï¼šä¸»è¦å¯¹æ¯”åŸºçº¿
- **DS-Ulysses**ï¼šä½œä¸ºå¦ä¸€ç§ sequence parallelism æ–¹æ³•è¿›è¡Œç†è®ºæ¯”è¾ƒ
- **StarTrail**ï¼šè¿‘æœŸæå‡ºçš„ 3D å¹¶è¡Œæ–¹æ¡ˆï¼Œç”¨äºé€šä¿¡å¤æ‚åº¦åˆ†æ
- æ‰€æœ‰æ–¹æ³•å‡åœ¨åŒä¸€ç¡¬ä»¶å’Œæ¨¡å‹é…ç½®ä¸‹è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆ256 GPUs, Sequence Length = 1Mï¼‰

| æŒ‡æ ‡ | Mesh-Attention | Ring-Attention | æå‡å¹…åº¦ |
|------|----------------|----------------|----------|
| **æ€»è¿è¡Œæ—¶é—´ï¼ˆForward + Backwardï¼‰** | 11.9 ç§’ | 37.5 ç§’ | **3.15Ã— æ›´å¿«** |
| **æœ€å¤§åŠ é€Ÿæ¯”** | â€” | â€” | **3.4Ã—** |
| **å¹³å‡åŠ é€Ÿæ¯”** | â€” | â€” | **2.9Ã—** |
| **é€šä¿¡é‡å‡å°‘** | â€” | â€” | **å¹³å‡ 79.0%ï¼Œæœ€é«˜ 85.4%** |
| **MFU æå‡** | æœ€é«˜ 51.0% | æœ€é«˜ 29.9% | **å¹³å‡ 2.5Ã—ï¼Œæœ€é«˜ 3.4Ã—** |

### å¼ºå¯æ‰©å±•æ€§ï¼ˆStrong Scalabilityï¼‰
- å›ºå®šåºåˆ—é•¿åº¦ä¸º 1M tokensï¼Œå¢åŠ  GPU æ•°é‡ã€‚
- **Ring-Attention**ï¼šæ€§èƒ½åœ¨è¶…è¿‡ 64 GPUs åæ˜¾è‘—ä¸‹é™ã€‚
- **Mesh-Attention**ï¼šå¯æŒç»­æ‰©å±•è‡³ 128 GPUs ä»¥ä¸Šï¼Œè¡¨ç°å‡ºè‰¯å¥½çº¿æ€§åŠ é€Ÿè¶‹åŠ¿ã€‚

### å¼±å¯æ‰©å±•æ€§ï¼ˆWeak Scalabilityï¼‰
- å½“ GPU æ•°é‡ç¿»å€æ—¶ï¼Œåºåˆ—é•¿åº¦æŒ‰ $\sqrt{2}$ å€å¢é•¿ï¼Œä¿æŒæ¯ GPU è®¡ç®—è´Ÿè½½æ’å®šã€‚
- Ring-Attention åœ¨ 256 GPUs ä¸‹æ¯” 32 GPUs æ…¢ **3.74Ã—**
- Mesh-Attention ä»…æ…¢ **2.83Ã—**ï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„æ¨ªå‘æ‰©å±•èƒ½åŠ›ã€‚

### é€šä¿¡å¼€é”€åˆ†æ
- **é€šä¿¡ç­‰å¾…æ—¶é—´å‡å°‘**ï¼šMesh-Attention çš„é€šä¿¡ç­‰å¾…æ—¶é—´æ¯” Ring-Attention å‡å°‘äº† **å¹³å‡ 74.0%ï¼Œæœ€é«˜ 74.9%**ã€‚
- å°½ç®¡å¦‚æ­¤ï¼Œåœ¨ 256 GPUs ä¸‹ï¼Œé€šä¿¡ä»å æ€»æ—¶é—´çš„ **86.6%**ï¼Œè¡¨æ˜ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚

### å†…å­˜æ¶ˆè€—åˆ†æ
- Mesh-Attention çš„å³°å€¼å†…å­˜é«˜äº Ring-Attentionï¼ˆä¾‹å¦‚åœ¨ 1M åºåˆ—ä¸‹çº¦é«˜å‡º 2â€“3 å€ï¼‰ï¼Œå› ä¸ºå®ƒéœ€è¦ç¼“å­˜å¤šä¸ª Q/KV chunksã€‚
- ä½†ä½œè€…æŒ‡å‡ºè¿™æ˜¯**ç¬æ€å†…å­˜**ï¼Œåœ¨æ¯å±‚ attention å®Œæˆåå³å¯é‡Šæ”¾ï¼Œä¸ä¼šå½±å“æ•´ä½“æ¿€æ´»å­˜å‚¨ã€‚

### GQA åœºæ™¯ä¸‹çš„è¡¨ç°
- åœ¨ Grouped-Query Attentionï¼ˆGQAï¼‰åœºæ™¯ä¸­ï¼ˆKV heads å‡å°‘ $g$ å€ï¼‰ï¼ŒRing-Attention çš„é€šä¿¡å‹åŠ›å‡è½»ã€‚
- å³ä¾¿å¦‚æ­¤ï¼ŒMesh-Attention ä¾ç„¶åœ¨æ‰€æœ‰ $g=1,2,4,8$ è®¾ç½®ä¸‹ä¿æŒé¢†å…ˆï¼Œå°¤å…¶åœ¨ $g=1$ï¼ˆæ ‡å‡† MHAï¼‰æ—¶ä¼˜åŠ¿æœ€æ˜æ˜¾ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Ring-Attention çš„é€šä¿¡ç“¶é¢ˆæ˜¯ç³»ç»Ÿçº§é™åˆ¶**ï¼šå…¶è¡Œå¼åˆ†é…å¯¼è‡´æ‰€æœ‰ GPU å¿…é¡»æ¥æ”¶å…¨éƒ¨ KV chunksï¼Œé€ æˆé€šä¿¡é‡çˆ†ç‚¸ã€‚
2. **äºŒç»´ tile åˆ†é…æ˜¾è‘—æ”¹å–„ CommCom ratio**ï¼šé€šè¿‡å°†ä»»åŠ¡åˆ’åˆ†ä¸ºäºŒç»´ tileï¼Œå¯åœ¨ Q å’Œ KV ä¸¤ä¸ªç»´åº¦ä¸Šå¹³è¡¡é€šä¿¡ä¸è®¡ç®—ï¼Œæ‰“ç ´â€œæç«¯å±€éƒ¨æ€§â€å›°å¢ƒã€‚
3. **Mesh-Attention å…·å¤‡ç†è®ºæœ€ä¼˜æ€§**ï¼šå½“ tile å¤§å°è®¾ä¸º $a \approx \sqrt{n}$ æ—¶ï¼Œé€šä¿¡å¤æ‚åº¦è¾¾åˆ°æœ€å° $\mathcal{O}(Nd/\sqrt{n})$ã€‚
4. **è‡ªåŠ¨è°ƒåº¦ç®—æ³•æœ‰æ•ˆæå‡é‡å ç‡**ï¼šæå‡ºçš„è´ªå¿ƒè°ƒåº¦å™¨èƒ½åœ¨å¤æ‚é€šä¿¡æ¨¡å¼ä¸‹è‡ªåŠ¨ç”Ÿæˆé«˜æ•ˆæ‰§è¡Œè®¡åˆ’ï¼Œæœ€å¤§åŒ–æµæ°´çº¿æ•ˆç‡ã€‚
5. **å¯æ‰©å±•æ€§è¿œè¶…ç°æœ‰æ–¹æ³•**ï¼šæ— è®ºæ˜¯å¼ºè¿˜æ˜¯å¼±æ‰©å±•åœºæ™¯ï¼ŒMesh-Attention éƒ½å±•ç°å‡ºå“è¶Šçš„æ¨ªå‘æ‰©å±•èƒ½åŠ›ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æ›´é«˜çš„å³°å€¼å†…å­˜éœ€æ±‚**ï¼šç”±äºéœ€ç¼“å­˜å¤šä¸ª Q/KV chunksï¼ŒMesh-Attention å¯¹å•å¡æ˜¾å­˜è¦æ±‚æ›´é«˜ï¼Œå¯èƒ½é™åˆ¶åœ¨ä½æ˜¾å­˜è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚
- **è°ƒåº¦ä¾èµ–é™æ€ profiling**ï¼šè°ƒåº¦å‚æ•°ï¼ˆå¦‚ $c_Q, c_{KV}, c_O$ï¼‰éœ€é¢„å…ˆæµ‹é‡ï¼Œå¯èƒ½å› ç¡¬ä»¶å˜åŒ–è€Œå¤±æ•ˆã€‚
- **å½“å‰å®ç°å‡è®¾åŒæ­¥æ‰§è¡Œ**ï¼šæ‰€æœ‰ GPU åœ¨ç»„å†…åŒæ­¥æ‰§è¡Œ P2P æ“ä½œï¼Œå¯èƒ½åœ¨å¼‚æ„ç½‘ç»œç¯å¢ƒä¸­å—é™ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **memory-efficient scheduling**ï¼šæ›´æ—©é‡Šæ”¾ä¸å†ä½¿ç”¨çš„ Q/KV chunksï¼Œé™ä½å³°å€¼å†…å­˜ã€‚
- æ”¯æŒ **å¼‚æ­¥é€šä¿¡ä¸åŠ¨æ€è°ƒåº¦**ï¼šé€‚åº”å¼‚æ„ç½‘ç»œç¯å¢ƒï¼Œæå‡é²æ£’æ€§ã€‚
- ç»“åˆ **å…¶ä»–å¹¶è¡ŒèŒƒå¼**ï¼ˆå¦‚ tensor parallelismï¼‰æ„å»ºç»Ÿä¸€çš„å¤§è§„æ¨¡è®­ç»ƒæ¡†æ¶ã€‚
- å°† Mesh-Attention æ‰©å±•è‡³ **decoder-only æ¨¡å‹çš„æ¨ç†é˜¶æ®µ**ï¼Œæ”¯æŒè¶…é•¿æ–‡æœ¬ç”Ÿæˆã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> Mesh-Attention é€šè¿‡å¼•å…¥äºŒç»´ tile åˆ†é…å’ŒåŸºäº Assignment Matrix çš„å»ºæ¨¡ï¼Œä»æ ¹æœ¬ä¸Šé‡æ„äº†åˆ†å¸ƒå¼æ³¨æ„åŠ›çš„é€šä¿¡æ¨¡å¼ï¼Œåœ¨ç†è®ºä¸Šå’Œå®è·µä¸­éƒ½å®ç°äº†ç›¸æ¯” Ring-Attention çš„æ•°é‡çº§æ€§èƒ½æå‡ï¼Œæ˜¯è¿ˆå‘ç™¾ä¸‡ä¹ƒè‡³åƒä¸‡ token ä¸Šä¸‹æ–‡çª—å£çš„å…³é”®åŸºç¡€è®¾æ–½åˆ›æ–°ã€‚

</details>

---

### 5. [Data-Free Pruning of Self-Attention Layers in LLMs](https://arxiv.org/abs/2512.20636)

**Authors**: Dhananjay Saikumar, Blesson Varghese  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.20636v1  

#### Abstract
Many self-attention sublayers in large language models (LLMs) can be removed with little to no loss. We attribute this to the Attention Suppression Hypothesis: during pre-training, some deep attention layers learn to mute their own contribution, leaving the residual stream and the MLP to carry the r...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡ã€ŠData-Free Pruning of Self-Attention Layers in LLMsã€‹æ ¸å¿ƒæ€»ç»“**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´é«˜å»¶è¿Ÿã€é«˜èƒ½è€—å’Œé«˜éƒ¨ç½²æˆæœ¬çš„é—®é¢˜ã€‚å°½ç®¡å·²æœ‰å¤§é‡ç ”ç©¶é€šè¿‡**é‡åŒ–ï¼ˆquantizationï¼‰** å’Œ**æƒé‡å‰ªæï¼ˆweight pruningï¼‰** æ¥å‹ç¼©æ¨¡å‹ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–ä¸“ç”¨ç¡¬ä»¶æ”¯æŒï¼ˆå¦‚ç¨€ç–çŸ©é˜µæ ¸ï¼‰æˆ–éœ€è¦å¾®è°ƒä»¥æ¢å¤ç²¾åº¦ã€‚è€Œ**æ·±åº¦ç»´åº¦ï¼ˆdepthï¼‰ä¸Šçš„ç»“æ„åŒ–å‰ªæ**â€”â€”å³ç§»é™¤å†—ä½™çš„Transformerå±‚â€”â€”ä»ç¼ºä¹é«˜æ•ˆã€å®ç”¨çš„æ–¹æ³•ã€‚

ç‰¹åˆ«æ˜¯ï¼Œç°æœ‰çš„ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼ˆå¦‚ShortGPTï¼‰ä¾èµ–äº**æ ¡å‡†æ•°æ®é›†**å’Œå¤šæ¬¡å‰å‘ä¼ æ’­æ¥è®¡ç®—é‡è¦æ€§åˆ†æ•°ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ã€éš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®æ—¶åº”ç”¨ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**

#### **ï¼ˆ1ï¼‰Attention Suppression Hypothesisï¼ˆæ³¨æ„åŠ›æŠ‘åˆ¶å‡è¯´ï¼‰**
ä½œè€…æå‡ºä¸€ä¸ªç†è®ºè§£é‡Šï¼šåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ·±å±‚çš„self-attentionå­å±‚ä¼šä¸»åŠ¨â€œæŠ‘åˆ¶â€è‡ªèº«çš„è¾“å‡ºæ›´æ–°ï¼Œä½¿å¾—å…¶å¯¹æ®‹å·®æµï¼ˆresidual streamï¼‰çš„è´¡çŒ®è¶‹è¿‘äºé›¶ã€‚å› æ­¤ï¼Œè¡¨ç¤ºä¿¡æ¯ä¸»è¦ç”±æ®‹å·®è·¯å¾„å’Œåç»­çš„MLPå±‚ä¼ é€’ï¼Œè€Œéæ³¨æ„åŠ›æœºåˆ¶æœ¬èº«ã€‚

è¿™ä¸€ç°è±¡å¯é€šè¿‡ä¸¤ä¸ªå®è¯è§‚å¯ŸéªŒè¯ï¼š
- æ·±å±‚attentionå±‚è¾“å…¥ä¸è¾“å‡ºä¹‹é—´çš„**cosine similarity æ¥è¿‘1**
- **attention-to-input norm ratio** éšå±‚æ•°åŠ æ·±æ˜¾è‘—ä¸‹é™ï¼ˆåæœŸå±‚ä¸­<0.1ï¼‰

#### **ï¼ˆ2ï¼‰Gate-Normï¼šé¦–ä¸ªå®Œå…¨æ— æ•°æ®çš„æ³¨æ„åŠ›å±‚é‡è¦æ€§è¯„åˆ†æ–¹æ³•**
åŸºäºä¸Šè¿°å‡è®¾ï¼Œä½œè€…æå‡ºäº† **Gate-Norm**ï¼Œä¸€ç§ä»…ä¾èµ–æ¨¡å‹æƒé‡ã€æ— éœ€ä»»ä½•æ•°æ®ã€å‰å‘ä¼ æ’­æˆ–å¾®è°ƒçš„**one-shotå‰ªæç®—æ³•**ã€‚

- **æ ¸å¿ƒæ€æƒ³**ï¼šå¦‚æœæŸå±‚queryå’Œkeyæƒé‡çŸ©é˜µä¹˜ç§¯çš„FrobeniusèŒƒæ•°å¾ˆå°ï¼Œåˆ™è¯¥å±‚æ— æ³•æœ‰æ•ˆè¿›è¡Œtoken mixingï¼Œå¯è¢«å®‰å…¨ç§»é™¤ã€‚
- **è®¡ç®—æ–¹å¼**ï¼š
  $$
  M_l = W_{q,l} W_{k,l}^T,\quad m_l = \|M_l\|_F
  $$
  å¯¹æ‰€æœ‰å±‚æŒ‰ $m_l$ å‡åºæ’åˆ—ï¼Œç§»é™¤æœ€å°çš„Nä¸ªå±‚ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | Gate-Norm | æ•°æ®é©±åŠ¨æ–¹æ³•ï¼ˆå¦‚He et al., ShortGPTï¼‰ |
|------|----------|-------------------------------|
| æ˜¯å¦éœ€è¦æ•°æ® | âŒ å®Œå…¨ä¸éœ€è¦ | âœ… éœ€è¦æ•°åƒtokenæ ¡å‡†æ•°æ® |
| æ˜¯å¦éœ€è¦å‰å‘ä¼ æ’­ | âŒ ä¸éœ€è¦ | âœ… å¤šæ¬¡forward pass |
| æ˜¯å¦éœ€è¦GPU | âŒ å¯åœ¨CPUè¿è¡Œ | âœ… é€šå¸¸éœ€GPUæ˜¾å­˜ |
| å‰ªæé€Ÿåº¦ | âš¡ï¸ **~300msï¼ˆGPUï¼‰ï¼Œ<30sï¼ˆCPUï¼‰** | ğŸ¢ æ•°åˆ†é’Ÿè‡³å°æ—¶çº§ |
| ç²¾åº¦ä¿æŒèƒ½åŠ› | âœ… åŒ¹é…ç”šè‡³ç•¥ä¼˜äºæ•°æ®é©±åŠ¨æ–¹æ³• | âœ… è¾ƒå¥½ï¼Œä½†ä¾èµ–æ•°æ®è´¨é‡ |
| å®ç”¨æ€§ | âœ… æ”¯æŒon-deviceã€éšç§æ•æ„Ÿåœºæ™¯ | âŒ éš¾ä»¥åŠ¨æ€éƒ¨ç½² |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼š**è¶…è¿‡1000å€çš„é€Ÿåº¦æå‡**ï¼ŒåŒæ—¶ä¿æŒä¸æ•°æ®é©±åŠ¨æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ¨¡å‹**
- ä¸»è¦å®éªŒï¼š`LLaMA-13B v1` å’Œ `v2`ï¼ˆå‡ä¸º40å±‚ï¼‰
- æ‰©å±•å®éªŒï¼š`Vicuna-7B`, `Vicuna-13B`, `LLaMA-3.1-8B`

### **æ•°æ®é›†**
| ä»»åŠ¡ | æ•°æ®é›† |
|------|--------|
| **Perplexityè¯„ä¼°** | WikiText-2 |
| **Zero-shotå‡†ç¡®ç‡** | BoolQ, RTE, HellaSwag, WinoGrande, ARC-Easy, ARC-Challenge, OpenBookQA |
| **Fine-tuningé€‚é…** | WikiText-2ï¼ˆç”¨äºLoRAå¾®è°ƒï¼‰ |

---

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**

#### **å‰ªæç­–ç•¥å¯¹æ¯”**
| æ–¹æ³• | ç±»å‹ | æ˜¯å¦éœ€è¦æ•°æ® |
|------|------|-------------|
| **Gate-Norm-Attn (Ours)** | ç§»é™¤attention sublayer | âŒ |
| **Data-driven-Attn** | åŸºäºcosine similarityçš„attentionå‰ªæ | âœ… |
| **ShortGPT-Block** | ç§»é™¤æ•´ä¸ªTransformer block | âœ… |
| **Random-Attn / Random-Block** | éšæœºç§»é™¤attentionæˆ–block | âŒ |

#### **è¯„ä¼°æŒ‡æ ‡**
1. **Perplexity on WikiText-2**ï¼šè¡¡é‡è¯­è¨€å»ºæ¨¡èƒ½åŠ›ä¿ç•™ç¨‹åº¦
2. **Zero-shot Accuracy**ï¼šå¹³å‡7é¡¹NLPä»»åŠ¡çš„å‡†ç¡®ç‡
3. **Speed-Up**ï¼šç«¯åˆ°ç«¯æ¨ç†ååé‡æå‡
4. **Pruning Latency**ï¼šå‰ªæå†³ç­–è€—æ—¶
5. **Post-pruning fine-tuning with LoRA**ï¼šéªŒè¯å¯æ¢å¤æ€§

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **ï¼ˆ1ï¼‰Perplexity ç»“æœï¼ˆWikiText-2ï¼‰**
- åœ¨ `LLaMA-13B` ä¸Šç§»é™¤ **8â€“16ä¸ªattentionå±‚**ï¼š
  - Gate-Norm çš„å›°æƒ‘åº¦å¢é•¿æå°ï¼Œ**æ¥è¿‘data-drivenæ–¹æ³•**
  - ç§»é™¤16å±‚åï¼ŒGate-Norm perplexity â‰ˆ 6.93ï¼ˆv1ï¼‰ï¼Œvs baseline 5.10
  - æ˜¾è‘—ä¼˜äºéšæœºå‰ªæï¼ˆrandom attention removal å¯¼è‡´perplexityé£™å‡è‡³æ•°ç™¾ï¼‰
- åœ¨ `Vicuna` å’Œ `LLaMA-3.1` ç³»åˆ—ä¸Šä¹Ÿè¡¨ç°å‡ºä¸€è‡´ä¼˜è¶Šæ€§ï¼ˆè§Table 1ï¼‰

> ğŸ” **ç»“è®º**ï¼šGate-Norm èƒ½ç²¾å‡†è¯†åˆ«å†—ä½™å±‚ï¼Œç»´æŒè¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚

---

#### **ï¼ˆ2ï¼‰Zero-shot å‡†ç¡®ç‡ï¼ˆ7é¡¹ä»»åŠ¡å¹³å‡ï¼‰**
| æ–¹æ³• | ç§»é™¤å±‚æ•° | å¹³å‡å‡†ç¡®ç‡ | ç›¸å¯¹äºBaselineä¸‹é™ | åååŠ é€Ÿ |
|------|---------|------------|---------------------|-----------|
| Baseline | 0 | 65.66% (v1), 65.59% (v2) | â€” | 1.00x |
| Gate-Norm-Attn | 4 | 64.99% | ~0.7% â†“ | 1.06x |
| Gate-Norm-Attn | 8 | 64.10% | ~1.6% â†“ | 1.12x |
| Gate-Norm-Attn | 16 | 63.82% | ~1.8% â†“ | **1.30x** |
| Data-driven-Attn | 16 | 64.02% (v1), 63.34% (v2) | ~2.3% â†“ | 1.30x |

> âœ… **å…³é”®å‘ç°**ï¼š
> - Gate-Norm åœ¨ **zero-shotå‡†ç¡®ç‡ä¸Šå‡ ä¹å®Œå…¨åŒ¹é…ç”šè‡³ç•¥å¾®è¶…è¶Š** data-drivenæ–¹æ³•
> - å³ä½¿å‰ªæ‰ **40%çš„attentionå±‚ï¼ˆ16/40ï¼‰**ï¼Œå¹³å‡å‡†ç¡®ç‡ä»ä¿æŒåœ¨ **baselineçš„97%ä»¥ä¸Š**
> - å®ç°æœ€é«˜è¾¾ **1.30Ã— æ¨ç†ååæå‡**

---

#### **ï¼ˆ3ï¼‰å‰ªææ•ˆç‡ï¼ˆPruning Latencyï¼‰**
| æ–¹æ³• | æ—¶é—´æ¶ˆè€— | ç¡¬ä»¶ä¾èµ– |
|------|--------|---------|
| **Gate-Norm** | **~300msï¼ˆGPUï¼‰**, **<30sï¼ˆCPUï¼‰** | ä»…éœ€ç³»ç»ŸRAM |
| **Data-driven methods** | æ•°åˆ†é’Ÿåˆ°æ•°å°æ—¶ | éœ€GPU + å¤§é‡VRAMå­˜å‚¨æ¿€æ´»å€¼ |

> âš¡ï¸ **é€Ÿåº¦ä¼˜åŠ¿**ï¼š**>1000Ã— åŠ é€Ÿ**ï¼Œé€‚åˆåŠ¨æ€ã€åœ¨çº¿å‰ªæã€‚

---

#### **ï¼ˆ4ï¼‰æ¶ˆèå®éªŒï¼šLoRAå¾®è°ƒæ¢å¤èƒ½åŠ›ï¼ˆTable 3ï¼‰**
- ç§»é™¤20ä¸ªattentionå±‚åï¼Œperplexityä¸¥é‡æ¶åŒ–ï¼ˆå¦‚Vicuna-13Bä»5.95å‡è‡³14.13ï¼‰
- ç»è¿‡ä»… **2000æ­¥LoRAå¾®è°ƒ**ï¼ŒperplexityåŸºæœ¬æ¢å¤ï¼š
  - Vicuna-13B: 14.13 â†’ **6.57**ï¼ˆGate-Normï¼‰
  - æ•°æ®é©±åŠ¨æ–¹æ³•ï¼š25.88 â†’ 6.65
- **Gate-Normå‰ªæåçš„æ¨¡å‹æ›´æ˜“æ¢å¤**

> âœ… è¡¨æ˜ Gate-Norm ä¸ç ´åå…³é”®å‚æ•°ç»“æ„ï¼Œå…¼å®¹è½»é‡çº§é€‚é…ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **Attention Suppression æ˜¯æ™®éç°è±¡**ï¼š
   - æ·±å±‚attentionå±‚è¾“å‡ºæ›´æ–°è¶‹è¿‘äºé›¶ï¼Œä¿¡æ¯ä¸»è¦é€šè¿‡æ®‹å·®è·¯å¾„æµåŠ¨ã€‚
   - è¿™ä¸º**æ— æ•°æ®å‰ªææä¾›äº†ç†è®ºåŸºç¡€**ã€‚

2. âœ… **Gate-Norm æ˜¯æœ‰æ•ˆçš„ä»£ç†æŒ‡æ ‡**ï¼š
   - ä»…ç”¨ $W_q W_k^T$ çš„FrobeniusèŒƒæ•°å³å¯é¢„æµ‹å±‚çš„é‡è¦æ€§ã€‚
   - ä¸çœŸå®cosine similarityé«˜åº¦ç›¸å…³ã€‚

3. âœ… **æ— éœ€æ•°æ®ä¹Ÿèƒ½å®ç°é«˜æ€§èƒ½å‰ªæ**ï¼š
   - åœ¨å¤šä¸ªLLMä¸Šï¼ŒGate-Norm **åŒ¹é…ç”šè‡³ç•¥ä¼˜äºæ•°æ®é©±åŠ¨æ–¹æ³•**ã€‚
   - æ”¯æŒ**é›¶æ ·æœ¬ã€é›¶æ ¡å‡†ã€é›¶å¾®è°ƒ**çš„å³æ—¶å‹ç¼©ã€‚

4. âœ… **æé«˜å®ç”¨æ€§**ï¼š
   - å¯åœ¨CPUä¸Šå®Œæˆï¼Œé€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡ã€éšç§ä¿æŠ¤åœºæ™¯ã€‚
   - æ”¯æŒå¤§è§„æ¨¡éƒ¨ç½²ä¸­çš„åŠ¨æ€æ¨¡å‹ç˜¦èº«ã€‚

---

### **å±€é™æ€§**
1. ğŸš« å½“å‰æ–¹æ³•ä¸“æ³¨äº **dense decoder-only Transformers**ï¼Œæœªæ‰©å±•åˆ°MoEæ¶æ„ï¼ˆå¦‚Mixtralï¼‰ã€‚
2. ğŸš« æç«¯å‰ªæï¼ˆ>20å±‚ï¼‰ä¼šå¯¼è‡´æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œé€‚ç”¨èŒƒå›´é™äºä¸­ç­‰å¼ºåº¦å‹ç¼©ã€‚
3. ğŸš« æœªæ¢ç´¢ä¸å…¶ä»–å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚é‡åŒ–ï¼‰çš„è”åˆä¼˜åŒ–ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
- å°† Gate-Norm æ€è·¯æ¨å¹¿è‡³ **vision Transformers** å’Œ **multimodal models**
- è®¾è®¡ **hybrid pruning schemes**ï¼šç»“åˆGate-Normä¸åŠ¨æ€è·¯ç”±ï¼ˆå¦‚SkipGPTï¼‰
- æ¢ç´¢ **layer-wise architecture design**ï¼šè‡ªåŠ¨å‡å°‘æ·±å±‚attentionå¤´æ•°
- å¼€å‘ **training-time regularization**ï¼Œé¼“åŠ±æ—©æœŸå½¢æˆattention suppressionï¼Œä¾¿äºåæœŸå‰ªæ

---

## âœ… **æ€»ç»“ä¸€å¥è¯**
> æœ¬æ–‡æå‡º **Gate-Norm**ï¼Œé¦–æ¬¡å®ç°äº†**å®Œå…¨æ— æ•°æ®ã€æ¯«ç§’çº§ã€one-shotçš„self-attentionå±‚å‰ªæ**ï¼Œåœ¨ä¿æŒzero-shotæ€§èƒ½å‡ ä¹ä¸å˜çš„å‰æä¸‹ï¼Œå°†LLMæ¨ç†ååæå‡é«˜è¾¾ **1.3Ã—**ï¼Œä¸”é€Ÿåº¦æ¯”ä¼ ç»Ÿæ–¹æ³•å¿«**ä¸Šåƒå€**ï¼Œä¸ºé«˜æ•ˆã€ç»¿è‰²ã€è¾¹ç¼˜å‹å¥½çš„LLMéƒ¨ç½²æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 6. [RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks](https://arxiv.org/abs/2512.20920)

**Authors**: Ningyuan Liu, Jing Yang, Kaitong Cai, Keze Wang  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.20920v1  

#### Abstract
Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale L...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šRevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿›è¡Œ **Full Fine-Tuning** æ—¶é¢ä¸´ä¸¥é‡çš„å†…å­˜ç“¶é¢ˆï¼Œä¸»è¦åŸå› æ˜¯åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­éœ€è¦ç¼“å­˜å¤§é‡ä¸­é—´æ¿€æ´»å€¼ï¼ˆactivationsï¼‰ã€‚å¯¹äºå‚æ•°é‡è¾¾æ•°åäº¿ç”šè‡³ä¸Šç™¾äº¿çš„ç°ä»£ LLMsï¼Œè¿™ç§å†…å­˜å¼€é”€ä½¿å¾—å•å¡è®­ç»ƒå‡ ä¹ä¸å¯è¡Œã€‚

å°½ç®¡å·²æœ‰å¦‚ **DeepSpeed ZeRO** å’Œ **FSDP** ç­‰åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶é€šè¿‡å¤šGPUå¹¶è¡Œæˆ–CPUå¸è½½ç¼“è§£è¯¥é—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•ä¾èµ–æ˜‚è´µç¡¬ä»¶ä¸”é™ä½è®­ç»ƒé€Ÿåº¦ï¼›è€Œ **Parameter-Efficient Fine-Tuning (PEFT)** æ–¹æ³•ï¼ˆå¦‚ LoRAï¼‰è™½èŠ‚çœå†…å­˜ï¼Œå´åªæ›´æ–°å°‘é‡å‚æ•°ï¼Œç‰ºç‰²äº†æ¨¡å‹æ€§èƒ½æ½œåŠ›ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šRevFFN
ä½œè€…æå‡º **RevFFN** â€”â€”ä¸€ç§åŸºäºå¯é€†ç½‘ç»œï¼ˆreversible networksï¼‰è®¾è®¡çš„é«˜æ•ˆå…¨å‚æ•°å¾®è°ƒèŒƒå¼ï¼Œä¸“ä¸º **Mixture-of-Experts (MoE)** æ¶æ„çš„å¤§æ¨¡å‹ä¼˜åŒ–ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- å°†æ ‡å‡† Transformer å±‚çš„éšè—çŠ¶æ€æ‹†åˆ†ä¸ºä¸¤ä¸ªæµï¼ˆ$X_1$, $X_2$ï¼‰ï¼Œé‡‡ç”¨è€¦åˆæ›´æ–°æœºåˆ¶æ„å»º **å¯é€†å—ï¼ˆreversible blockï¼‰**ã€‚
- åœ¨å‰å‘ä¼ æ’­ä¸­è®¡ç®—è¾“å‡º $Y_1, Y_2$ï¼Œè€Œåœ¨åå‘ä¼ æ’­æ—¶å¯ä»¥ä»è¾“å‡ºç²¾ç¡®é‡æ„è¾“å…¥ï¼Œä»è€Œæ— éœ€å­˜å‚¨å¤§å¤šæ•°ä¸­é—´æ¿€æ´»ã€‚
- å¼•å…¥è½»é‡çº§ **Projection Adapters** æ¥é€‚é…é¢„è®­ç»ƒæƒé‡ä¸åŠç»´è¾“å…¥ä¹‹é—´çš„ç»´åº¦ä¸åŒ¹é…é—®é¢˜ï¼Œä¿ç•™åŸå§‹ MoE ç»“æ„å®Œæ•´æ€§ã€‚

#### åˆ›æ–°äº®ç‚¹ï¼š
1. **å†…å­˜æ•ˆç‡é«˜**ï¼šé¿å…ç¼“å­˜å¤§éƒ¨åˆ† activationï¼Œæ˜¾è‘—é™ä½å³°å€¼æ˜¾å­˜å ç”¨ã€‚
2. **ä¿æŒå…¨å‚æ•°æ›´æ–°èƒ½åŠ›**ï¼šä¸åŒäº PEFTï¼ŒRevFFN æ”¯æŒå¯¹æ‰€æœ‰å‚æ•°è¿›è¡Œ fine-tuningï¼Œæœ€å¤§åŒ–æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚
3. **å…¼å®¹ MoE æ¶æ„**ï¼šå®Œæ•´ä¿ç•™ MoE çš„è·¯ç”±æœºåˆ¶å’Œä¸“å®¶ç»“æ„ï¼Œé€‚ç”¨äºå½“å‰ä¸»æµç¨€ç–å¤§æ¨¡å‹ã€‚
4. **ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆTwo-Stage Training Scheduleï¼‰**ï¼š
   - Stage 1ï¼šå†»ç»“ä¸»å¹²ï¼Œä»…è®­ç»ƒ projection adaptersï¼ˆadapter warm-upï¼‰
   - Stage 2ï¼šè§£å†»ä¸»å¹²ï¼Œè”åˆå¾®è°ƒï¼ˆä»…æ›´æ–°ä¸“å®¶æƒé‡å’Œ adapterï¼‰ï¼Œæå‡ç¨³å®šæ€§ä¸æ”¶æ•›æ€§ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³•ç±»åˆ« | å†…å­˜æ•ˆç‡ | æ€§èƒ½ä¸Šé™ | æ˜¯å¦æ”¯æŒå…¨å‚æ•°æ›´æ–° | å•å¡å¯è¡Œæ€§ |
|--------|---------|----------|---------------------|------------|
| PEFT (LoRA, DoRA) | â­â­â­â­â˜† | â­â­â˜†â˜†â˜† | âŒ | âœ… |
| SFT + Checkpointing | â­â­â˜†â˜†â˜† | â­â­â­â­â˜† | âœ… | âŒï¼ˆéœ€å¤šå¡ï¼‰ |
| GaLore / LoMo | â­â­â­â˜†â˜† | â­â­â­â˜†â˜† | âœ… | âœ…ï¼ˆæœ‰é™ï¼‰ |
| **RevFFN** | â­â­â­â­â˜† | â­â­â­â­â˜† | âœ… | âœ… |

> âœ… RevFFN å®ç°äº† **PEFTçº§åˆ«çš„å†…å­˜æ•ˆç‡ + Full Fine-Tuning çš„æ€§èƒ½ä¼˜åŠ¿**ï¼Œå¯åœ¨å•å¼ æ¶ˆè´¹çº§æˆ–æœåŠ¡å™¨çº§ GPU ä¸Šå®Œæˆå¤§è§„æ¨¡ MoE æ¨¡å‹çš„å…¨å‚æ•°å¾®è°ƒã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- ä¸»è¦å¾®è°ƒæ•°æ®é›†ï¼š**databricks-dolly-15k**  
  > ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¼€æºæŒ‡ä»¤è·Ÿéšï¼ˆinstruction-followingï¼‰æ•°æ®é›†ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡é€‚åº”ã€‚

### ğŸ’» å®éªŒå¹³å°
- **ç¡¬ä»¶**ï¼šå•å¼  **NVIDIA H800 GPUï¼ˆ80GB VRAMï¼‰**
- **è½¯ä»¶æ¡†æ¶**ï¼šPyTorch + HuggingFace Transformers
- **åŸºç¡€æ¨¡å‹**ï¼š`Qwen/Qwen1.5-MoE-A2.7B`ï¼ˆæ¿€æ´»å‚æ•°çº¦ 2.7B çš„ MoE æ¨¡å‹ï¼‰

---

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Peak VRAM Usage (GB)** â†“ | è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€å¤§æ˜¾å­˜å ç”¨ï¼Œè¡¡é‡å†…å­˜æ•ˆç‡ |
| **Training Throughput (samples/sec)** â†‘ | æ¯ç§’å¤„ç†æ ·æœ¬æ•°ï¼Œåæ˜ è®­ç»ƒé€Ÿåº¦ |
| **Downstream Performance** â†‘ | å¤šé¡¹åŸºå‡†æµ‹è¯•å¾—åˆ†ï¼ŒåŒ…æ‹¬ï¼š<br>â€¢ **MMLU**ï¼ˆå¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼‰<br>â€¢ **GSM8K**ï¼ˆæ•°å­¦æ¨ç†ï¼‰<br>â€¢ **Multilingual Benchmark**ï¼ˆè·¨è¯­è¨€èƒ½åŠ›ï¼‰<br>â€¢ **MT-Bench**ï¼ˆå¯¹è¯è´¨é‡è¯„åˆ†ï¼‰ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

#### Parameter-Efficient Fine-Tuning (PEFT)
- **LoRA** [10]
- **DoRA** [19]
- **(IA)^3** [20]

#### Full-Parameter Fine-Tuning with Memory Optimization
- **SFT + Activation Checkpointing** [21]
- **LoMO** [22]
- **GaLore** [23]

> æ‰€æœ‰æ–¹æ³•å‡åœ¨ç›¸åŒæ¡ä»¶ä¸‹å°½å¯èƒ½å¢å¤§ batch size ä»¥å……åˆ†åˆ©ç”¨ 80GB æ˜¾å­˜é™åˆ¶ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š è¡¨æ ¼ 1ï¼šæ˜¾å­˜ä¸ååå¯¹æ¯”ï¼ˆSingle H800ï¼‰

| Method | Peak VRAM (GB) â†“ | Throughput (samples/s) â†‘ |
|--------|------------------|----------------------------|
| LoRA | 18.2 | 75.4 |
| DoRA | 19.5 | 71.8 |
| (IA)Â³ | 17.9 | 74.1 |
| SFT + Checkpointing | 65.4 | 19.7 |
| LoMO | 42.2 | 17.3 |
| GaLore | 45.1 | 35.2 |
| **RevFFN** | **39.5** | **24.6** |

> âœ… **RevFFN æ˜¾å­˜ä»…ä¸º SFT çš„ 60.4%ï¼ˆä¸‹é™ 49%ï¼‰**ï¼Œä¼˜äº GaLore å’Œ LoMOï¼Œæ¥è¿‘ PEFT æ°´å¹³ï¼ŒåŒæ—¶ä»æ”¯æŒå…¨å‚æ•°æ›´æ–°ã€‚

---

### ğŸ“ˆ è¡¨æ ¼ 2ï¼šä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å¯¹æ¯”

| Method | MMLU (%) â†‘ | GSM8K (%) â†‘ | Multilingual (%) â†‘ | MT-Bench Score â†‘ |
|--------|-------------|--------------|----------------------|--------------------|
| Base Model | 62.4 | 61.2 | 40.4 | 6.25 |
| LoRA | 65.2 | 71.5 | 38.5 | 7.18 |
| DoRA | 65.7 | 70.8 | 38.9 | 7.25 |
| (IA)Â³ | 65.0 | 70.2 | 38.2 | 7.15 |
| SFT + Checkpointing | 66.1 | 74.8 | 39.5 | 7.52 |
| LoMO | 66.2 | 74.6 | 39.3 | 7.50 |
| GaLore | 66.3 | 74.2 | 39.2 | 7.46 |
| **RevFFN** | **66.7** | **75.1** | **38.8** | **7.65** |

> âœ… **RevFFN åœ¨æ‰€æœ‰å››é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€ä¼˜è¡¨ç°**ï¼Œå°¤å…¶åœ¨ MMLU å’Œ GSM8K ä¸Šè¶…è¶Šä¼ ç»Ÿ SFT æ–¹æ³•ï¼ŒéªŒè¯å…¶æ›´å¼ºçš„æ³›åŒ–ä¸æ¨ç†èƒ½åŠ›ã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰

| é…ç½® | MMLU (%) |
|------|----------|
| RevFFNï¼ˆå®Œæ•´æ–¹æ³•ï¼‰ | **66.7** |
| w/o Stage 1ï¼ˆè·³è¿‡ adapter warm-upï¼‰ | 57.1 |
| w/o Stage 2ï¼ˆä»…è®­ç»ƒ projectionï¼‰ | 54.5 |

> ğŸ” å‘ç°ï¼š
- è·³è¿‡ Stage 1 å¯¼è‡´ä¸¥é‡æ€§èƒ½ä¸‹é™ â†’ è¡¨æ˜ **adapter åˆå§‹åŒ–é˜¶æ®µå¯¹è®­ç»ƒç¨³å®šè‡³å…³é‡è¦**
- è‹¥ä¸è¿›å…¥ Stage 2 å…¨å‚æ•°å¾®è°ƒï¼Œåˆ™æ€§èƒ½å¤§å¹…å—é™ â†’ è¯´æ˜ **å¿…é¡»è§£å†»ä¸»å¹²æ‰èƒ½é‡Šæ”¾å…¨éƒ¨æ½œåŠ›**

> âœ… ä¸¤é˜¶æ®µç­–ç•¥æ˜¯ RevFFN æˆåŠŸçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **RevFFN å®ç°äº†å†…å­˜æ•ˆç‡ä¸æ¨¡å‹æ€§èƒ½çš„å¹³è¡¡**ï¼š
   - æ˜¾å­˜æ¶ˆè€—æ¯”ä¼ ç»Ÿ SFT é™ä½ **49%**ï¼Œå¯åœ¨å•å¡å®Œæˆ MoE æ¨¡å‹çš„å…¨å‚æ•°å¾®è°ƒã€‚
   - æ€§èƒ½å…¨é¢è¶…è¿‡å„ç±» PEFT æ–¹æ³•ï¼Œå¹¶ç•¥å¾®ä¼˜äºæ ‡å‡† SFTï¼Œè¡¨æ˜å¯é€†ç»“æ„æœªæŸå®³æ¨¡å‹è¡¨è¾¾åŠ›ã€‚

2. **å¯é€†è®¾è®¡ + æŠ•å½±é€‚é…å™¨ æ˜¯æœ‰æ•ˆç»„åˆ**ï¼š
   - å¯é€†å—æ¶ˆé™¤ activation å­˜å‚¨éœ€æ±‚ï¼›
   - è½»é‡ projection adapter è§£å†³ç»´åº¦é”™é…ï¼Œä¸å½±å“åŸæ¨¡å‹å®¹é‡ã€‚

3. **ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ˜¾è‘—æå‡ç¨³å®šæ€§ä¸æœ€ç»ˆæ€§èƒ½**ï¼š
   - å…ˆå›ºå®šä¸»å¹²è®­ç»ƒ adapterï¼Œå†è”åˆå¾®è°ƒï¼Œé¿å…ç¾éš¾æ€§é—å¿˜å’Œè®­ç»ƒéœ‡è¡ã€‚

---

### âš ï¸ å±€é™æ€§
1. **è®¡ç®—å¼€é”€å¢åŠ **ï¼šç”±äºåå‘ä¼ æ’­éœ€åŠ¨æ€é‡å»º activationï¼Œå¸¦æ¥ä¸€å®šè®¡ç®—å†—ä½™ï¼Œå¯¼è‡´ååä½äº PEFT æ–¹æ³•ï¼ˆ~24.6 vs ~75 samples/sï¼‰ã€‚
2. **ç›®å‰ä»…éªŒè¯äº Qwen1.5-MoE-A2.7B**ï¼šå°šæœªæ‰©å±•åˆ°æ›´å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ 10B+ï¼‰æˆ–å¤šæ¨¡æ€åœºæ™¯ã€‚
3. **MoE è·¯ç”±å™¨å†»ç»“**ï¼šä¸ºä¿è¯è·¯ç”±ç¨³å®šæ€§ï¼Œå®éªŒä¸­å†»ç»“äº† router å‚æ•°ï¼Œå¯èƒ½é™åˆ¶æŸäº›ä»»åŠ¡ä¸‹çš„çµæ´»æ€§ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³æ›´å¤§è§„æ¨¡ LLMsï¼ˆå¦‚ç™¾äº¿çº§ä»¥ä¸Š MoE æ¨¡å‹ï¼‰
- æ¢ç´¢ä¸å…¶ä»–å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚ Quantization, Knowledge Distillationï¼‰ç»“åˆ
- åº”ç”¨äºå¤šæ¨¡æ€æ¨¡å‹ä¸é•¿åºåˆ—å»ºæ¨¡ä»»åŠ¡
- åŠ¨æ€è°ƒæ•´å¯é€†å—æ•°é‡ä»¥å®ç°ç²¾åº¦-æ•ˆç‡æƒè¡¡

---

## âœ… æ€»ç»“
**RevFFN æä¾›äº†ä¸€æ¡å®ç”¨è·¯å¾„ï¼Œä½¿ Full-Parameter Fine-Tuning ä¸å†å±€é™äºå¤šGPUé›†ç¾¤ç¯å¢ƒ**ã€‚å®ƒé€šè¿‡å¯é€†ç½‘ç»œè®¾è®¡å¤§å¹…å‡å°‘æ˜¾å­˜å ç”¨ï¼Œåœ¨ä¿æŒ MoE æ¶æ„å®Œæ•´æ€§å’Œå…¨å‚æ•°æ›´æ–°èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†æ¥è¿‘ PEFT çš„å†…å­˜æ•ˆç‡å’Œè¶…è¶Š PEFT çš„æ€§èƒ½è¡¨ç°ã€‚è¿™ä¸€æˆæœæ¨åŠ¨äº†å¤§æ¨¡å‹å¾®è°ƒæŠ€æœ¯å‘æ›´é«˜æ•ˆã€æ›´æ˜“éƒ¨ç½²çš„æ–¹å‘å‘å±•ã€‚

</details>

---

### 7. [NVIDIA Nemotron 3: Efficient and Open Intelligence](https://arxiv.org/abs/2512.20856)

**Authors**: NVIDIA,  :, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frank Sun, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt, Gargi Prasad, George Armstrong, Gerald Shen, Gorkem Batmaz, Grigor Nalbandyan, Haifeng Qian, Harsh Sharma, Hayley Ross, Helen Ngo, Herbert Hum, Herman Sahota, Hexin Wang, Himanshu Soni, Hiren Upadhyay, Huizi Mao, Huy C Nguyen, Huy Q Nguyen, Iain Cunningham, Ido Galil, Ido Shahaf, Igor Gitman, Ilya Loshchilov, Itamar Schen, Itay Levy, Ivan Moshkov, Izik Golan, Izzy Putterman, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jatin Mitra, Jeffrey Glick, Jenny Chen, Jesse Oliver, Jian Zhang, Jiaqi Zeng, Jie Lou, Jimmy Zhang, Jinhang Choi, Jining Huang, Joey Conway, Joey Guman, John Kamalu, Johnny Greco, Jonathan Cohen, Joseph Jennings, Joyjit Daw, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kai Xu, Kan Zhu, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kevin Shih, Kezhi Kong, Khushi Bhardwaj, Kirthi Shankar, Krishna C. Puvvada, Krzysztof Pawelec, Kumar Anik, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Li Ding, Lizzie Wei, Lucas Liebenwein, Luis Vega, Maanu Grover, Maarten Van Segbroeck, Maer Rodrigues de Melo, Mahdi Nazemi, Makesh Narsimhan Sreedhar, Manoj Kilaru, Maor Ashkenazi, Marc Romeijn, Marcin Chochowski, Mark Cai, Markus Kliegl, Maryam Moosaei, Matt Kulka, Matvei Novikov, Mehrzad Samadi, Melissa Corpuz, Mengru Wang, Meredith Price, Michael Andersch, Michael Boone, Michael Evans, Miguel Martinez, Mikail Khona, Mike Chrzanowski, Minseok Lee, Mohammad Dabbah, Mohammad Shoeybi, Mostofa Patwary, Nabin Mulepati, Najeeb Nabwani, Natalie Hereth, Nave Assaf, Negar Habibi, Neta Zmora, Netanel Haber, Nicola Sessions, Nidhi Bhatia, Nikhil Jukar, Nikki Pope, Nikolai Ludwig, Nima Tajbakhsh, Nir Ailon, Nirmal Juluru, Nishant Sharma, Oleksii Hrinchuk, Oleksii Kuchaiev, Olivier Delalleau, Oluwatobi Olabiyi, Omer Ullman Argov, Omri Puny, Oren Tropp, Ouye Xie, Parth Chadha, Pasha Shamis, Paul Gibbons, Pavlo Molchanov, Pawel Morkisz, Peter Dykas, Peter Jin, Pinky Xu, Piotr Januszewski, Pranav Prashant Thombre, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Qing Miao, Qiyu Wan, Rabeeh Karimi Mahabadi, Rachit Garg, Ran El-Yaniv, Ran Zilberstein, Rasoul Shafipour, Rich Harang, Rick Izzo, Rima Shahbazyan, Rishabh Garg, Ritika Borkar, Ritu Gala, Riyad Islam, Robert Hesse, Roger Waleffe, Rohit Watve, Roi Koren, Ruoxi Zhang, Russell Hewett, Russell J. Hewett, Ryan Prenger, Ryan Timbrook, Sadegh Mahdavi, Sahil Modi, Samuel Kriman, Sangkug Lim, Sanjay Kariyappa, Sanjeev Satheesh, Saori Kaji, Satish Pasumarthi, Saurav Muralidharan, Sean Narentharen, Sean Narenthiran, Seonmyeong Bak, Sergey Kashirsky, Seth Poulos, Shahar Mor, Shanmugam Ramasamy, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shiqing Fan, Shreya Gopal, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Simeng Sun, Smita Ithape, Somshubra Majumdar, Soumye Singhal, Stas Sergienko, Stefania Alborghetti, Stephen Ge, Sugam Dipak Devare, Sumeet Kumar Barua, Suseella Panguluri, Suyog Gupta, Sweta Priyadarshi, Syeda Nahida Akter, Tan Bui, Teodor-Dumitru Ene, Terry Kong, Thanh Do, Tijmen Blankevoort, Tim Moon, Tom Balough, Tomer Asida, Tomer Bar Natan, Tomer Ronen, Tugrul Konuk, Twinkle Vashishth, Udi Karpas, Ushnish De, Vahid Noorozi, Vahid Noroozi, Venkat Srinivasan, Venmugil Elango, Victor Cui, Vijay Korthikanti, Vinay Rao, Vitaly Kurin, Vitaly Lavrukhin, Vladimir Anisimov, Wanli Jiang, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenfei Zhou, Will Jennings, William Zhang, Wojciech Prazuch, Xiaowei Ren, Yashaswi Karnati, Yejin Choi, Yev Meyer, Yi-Fu Wu, Yian Zhang, Yigong Qin, Ying Lin, Yonatan Geifman, Yonggan Fu, Yoshi Subara, Yoshi Suhara, Yubo Gao, Zach Moshe, Zhen Dong, Zhongbo Zhu, Zihan Liu, Zijia Chen, Zijie Yan  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.20856v1  

#### Abstract
We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# NVIDIA Nemotron 3: Efficient and Open Intelligence è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
NVIDIA Nemotron 3 æ—¨åœ¨è§£å†³å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨**æ¨ç†æ•ˆç‡ã€é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€å¤šä»»åŠ¡æ³›åŒ–èƒ½åŠ›å’Œéƒ¨ç½²çµæ´»æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜**ã€‚å…·ä½“åŒ…æ‹¬ï¼š
- å¦‚ä½•åœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æå‡æ¨ç†ååé‡ï¼›
- å¦‚ä½•æ”¯æŒç™¾ä¸‡çº§ Token ä¸Šä¸‹æ–‡é•¿åº¦è€Œä¸ç‰ºç‰²æ€§èƒ½ï¼›
- å¦‚ä½•å®ç°é«˜æ•ˆçš„å¤šæ­¥æ¨ç†ä¸å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼›
- å¦‚ä½•åœ¨ä¸åŒç¡¬ä»¶åœºæ™¯ä¸‹å…¼é¡¾å»¶è¿Ÿæ•æ„Ÿå‹å’Œååä¼˜å…ˆå‹éƒ¨ç½²éœ€æ±‚ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
Nemotron 3 å®¶æ—æ¨¡å‹ï¼ˆNanoã€Superã€Ultraï¼‰å¼•å…¥äº†ä¸€ç³»åˆ—å…³é”®æŠ€æœ¯ç»„åˆï¼Œå½¢æˆäº†ä¸€å¥—é«˜æ•ˆä¸”å¼€æ”¾çš„æ™ºèƒ½æ¶æ„ä½“ç³»ï¼š

#### ï¼ˆ1ï¼‰**Hybrid Mamba-Transformer MoE æ¶æ„**
- ç»“åˆ **Mamba-2** çš„åºåˆ—å»ºæ¨¡ä¼˜åŠ¿ä¸ **Transformer MoE** çš„è¡¨è¾¾èƒ½åŠ›ã€‚
- å¤§éƒ¨åˆ†å±‚é‡‡ç”¨ Mamba-2 æ›¿ä»£ä¼ ç»Ÿ Self-Attention å±‚ï¼Œæ˜¾è‘—é™ä½ KV Cache å­˜å‚¨å¼€é”€ï¼Œæå‡ç”Ÿæˆé˜¶æ®µçš„æ¨ç†é€Ÿåº¦ã€‚
- ä»…ä¿ç•™å°‘é‡ Self-Attention å±‚ç”¨äºå…³é”®è¯­ä¹‰äº¤äº’ã€‚

#### ï¼ˆ2ï¼‰**LatentMoE æ¶æ„ï¼ˆSuper & Ultraï¼‰**
- åˆ›æ–°æ€§åœ°å°†ä¸“å®¶è·¯ç”±è¿‡ç¨‹ä»åŸå§‹éšè—ç»´åº¦ $d$ æŠ•å½±åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ $l < d$ è¿›è¡Œè®¡ç®—ã€‚
- å‡å°‘ All-to-All é€šä¿¡é‡å’Œå‚æ•°åŠ è½½é‡çº¦ $d/l \approx 4\times$ã€‚
- å°†èŠ‚çœä¸‹æ¥çš„èµ„æºç”¨äºå¢åŠ æ€»ä¸“å®¶æ•° $N'$ å’Œæ¿€æ´»ä¸“å®¶æ•° $K'$ï¼Œä»è€Œæå‡æ¨¡å‹éçº¿æ€§å®¹é‡å’Œå¤šæ ·æ€§ï¼Œåœ¨ä¸å¢åŠ æ¨ç†æˆæœ¬çš„å‰æä¸‹æé«˜å‡†ç¡®ç‡ã€‚

#### ï¼ˆ3ï¼‰**Multi-Token Prediction (MTP)**
- åœ¨è®­ç»ƒæ—¶é¢„æµ‹å¤šä¸ªåç»­ Tokenï¼Œæä¾›æ›´å¯†é›†çš„ç›‘ç£ä¿¡å·ã€‚
- æ”¯æŒ Speculative Decodingï¼ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œå°¤å…¶é€‚ç”¨äº batch-size-1 æˆ–é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ã€‚
- MTP é¢„æµ‹ç»“æœå¯ä½œä¸ºâ€œè‰ç¨¿ Tokenâ€ï¼Œæ¥å—ç‡é«˜è¾¾ ~97%ã€‚

#### ï¼ˆ4ï¼‰**NVFP4 è®­ç»ƒæŠ€æœ¯ï¼ˆSuper & Ultraï¼‰**
- ä½¿ç”¨åŸç”Ÿ **NVFP4** æ•°å€¼æ ¼å¼è¿›è¡Œé¢„è®­ç»ƒï¼ˆæƒé‡ã€æ¢¯åº¦ã€æ¿€æ´»å‡é‡åŒ–ä¸º NVFP4ï¼‰ï¼Œåˆ©ç”¨ GB300 GPU ä¸Šçš„ cuBLAS åŠ é€Ÿã€‚
- ç›¸æ¯” FP8ï¼Œå³°å€¼ååæå‡ 3Ã—ã€‚
- é€šè¿‡å¾®å—ç¼©æ”¾ï¼ˆmicro-block scalingï¼‰ã€éšæœºå“ˆè¾¾ç›å˜æ¢ï¼ˆRHTï¼‰ã€æ¢¯åº¦éšæœºèˆå…¥ç­‰ç­–ç•¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ã€‚
- æ•æ„Ÿå±‚ï¼ˆå¦‚ QKVã€Mamba è¾“å‡ºï¼‰ä¿ç•™åœ¨ BF16/MXFP8 ä»¥ç»´æŒç²¾åº¦ã€‚

#### ï¼ˆ5ï¼‰**Long Context Support up to 1M Tokens**
- ä¸ä¾èµ– RoPEï¼Œé¿å…å¤–æ¨æ—¶çš„ä½ç½®ç¼–ç å¤±é…é—®é¢˜ã€‚
- é€šè¿‡ Continued Pretraining (CPT) åœ¨ 512k åºåˆ—ä¸Šè®­ç»ƒï¼ŒSFT åœ¨ 256k ä¸Šå®Œæˆï¼Œå¹¶åœ¨ RL é˜¶æ®µåŠ å…¥æœ€é•¿ 32k çš„é•¿ä¸Šä¸‹æ–‡ç¯å¢ƒã€‚
- æ”¯æŒ RAGã€ä»£ç ç†è§£ã€å¤šè·³é—®ç­”ç­‰éœ€è¦è¶…é•¿ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ã€‚

#### ï¼ˆ6ï¼‰**Multi-environment Reinforcement Learning Post-training**
- ç»Ÿä¸€åœ¨å¤šä¸ªå¼‚æ„ RL ç¯å¢ƒä¸­è”åˆè®­ç»ƒï¼Œæ¶µç›–æ•°å­¦æ¨ç†ã€ç¼–ç¨‹ç«èµ›ã€æŒ‡ä»¤éµå¾ªã€è½¯ä»¶å·¥ç¨‹ã€æœç´¢ã€èŠå¤©ã€å·¥å…·ä½¿ç”¨ç­‰ã€‚
- é¿å…åˆ†é˜¶æ®µè®­ç»ƒå¯¼è‡´çš„èƒ½åŠ›é€€åŒ–æˆ– Reward Hackingã€‚
- ä½¿ç”¨ GRPO + Masked Importance Sampling æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚

#### ï¼ˆ7ï¼‰**Granular Reasoning Budget Control**
- ç”¨æˆ·å¯åœ¨æ¨ç†æ—¶æŒ‡å®šæœ€å¤§æ€è€ƒ Token æ•°ï¼ˆthinking trace budgetï¼‰ã€‚
- è¾¾åˆ°é¢„ç®—åè‡ªåŠ¨åˆ‡æ¢è‡³æœ€ç»ˆå“åº”ç”Ÿæˆï¼Œå®ç°ç²¾åº¦ä¸å»¶è¿Ÿä¹‹é—´çš„ç»†ç²’åº¦æ§åˆ¶ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | Nemotron 3 | å…¸å‹ MoE / Dense LLM |
|------|-----------|------------------------|
| æ¨ç†åå | æ˜¾è‘—æ›´é«˜ï¼ˆå¦‚ Nano æ¯” Qwen3 å¿« 3.3Ã—ï¼‰ | å—é™äº Attention KV Cache |
| ä¸Šä¸‹æ–‡æ‰©å±•èƒ½åŠ› | æ”¯æŒ 1M tokensï¼Œæ—  RoPE å¤–æ¨é—®é¢˜ | RoPE å¤–æ¨æ€§èƒ½éª¤é™ |
| æ¨ç†æ•ˆç‡ä¼˜åŒ– | MTP + LatentMoE + Mamba ååŒå¢æ•ˆ | é€šå¸¸åªç”¨å…¶ä¸­ä¸€ç§ |
| å¼€æ”¾ç¨‹åº¦ | å…¬å¼€æ¨¡å‹æƒé‡ã€è®­ç»ƒæ•°æ®ã€é…æ–¹ã€ä»£ç  | å¤šæ•°é—­æºæˆ–éƒ¨åˆ†å¼€æº |
| éƒ¨ç½²çµæ´»æ€§ | æ”¯æŒæ¨ç†æ—¶åŠ¨æ€æ§åˆ¶æ¨ç†æ·±åº¦ | å›ºå®šæ¨ç†è·¯å¾„ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **é¢„è®­ç»ƒæ•°æ®**ï¼šè¶…è¿‡ **10 ä¸‡äº¿ tokens** çš„é«˜è´¨é‡å¤šé¢†åŸŸæ–‡æœ¬ï¼Œæ¶µç›–ç½‘é¡µã€ä¹¦ç±ã€ä»£ç ã€ç§‘å­¦æ–‡çŒ®ç­‰ã€‚
- **å¼ºåŒ–å­¦ä¹ ç¯å¢ƒé›†åˆï¼ˆNeMo-Gymï¼‰**ï¼š
  - æ•°å­¦ï¼šAIME25ã€GSM8Kã€MATH
  - ç¼–ç¨‹ï¼šHumanEvalã€MBPPã€LiveCodeBenchã€SciCode
  - æŒ‡ä»¤è·Ÿéšï¼šIFBench
  - å¸¸è¯†æ¨ç†ï¼šARC-Challengeã€HellaSwagã€Winogrande
  - é•¿ä¸Šä¸‹æ–‡æ£€ç´¢ï¼šRULER @ 1M
  - å·¥å…·ä½¿ç”¨ï¼šT2-Benchã€SWE-Bench
  - å¯¹è¯è´¨é‡ï¼šArena-Hard-v2

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
| ç±»åˆ« | è®¾ç½®è¯´æ˜ |
|------|---------|
| æ¨¡å‹è§„æ¨¡ | Nano: 30B å‚æ•°ï¼ˆA3B æ´»è·ƒä¸“å®¶ï¼‰ï¼›Super/Ultra æ›´å¤§ï¼ˆæœªå…¬å¼€ï¼‰ |
| ä¸Šä¸‹æ–‡é•¿åº¦ | æœ€é•¿è¾¾ 1M tokensï¼›CPT åœ¨ 512kï¼ŒSFT åœ¨ 256k |
| è®­ç»ƒæ–¹å¼ | é¢„è®­ç»ƒ â†’ å¤šç¯å¢ƒ RL å¾®è°ƒ |
| æ¨ç†æ§åˆ¶ | æ”¯æŒ `</think>` æ ‡è®°å®ç°é¢„ç®—æ§åˆ¶ |
| è¯„ä¼°æŒ‡æ ‡ | Accuracy (%)ã€Pass@1ã€CoT EMã€RULER Scoreã€Throughput (tokens/sec)ã€NLL |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Qwen3-30B-A3B-Thinking**
- **GPT-OSS-20B-A4B**
- **Standard MoE æ¶æ„ï¼ˆå¯¹ç…§ç»„ï¼‰**
- **BF16 è®­ç»ƒæ¨¡å‹ï¼ˆç”¨äº NVFP4 å¯¹æ¯”ï¼‰**

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… æ¨ç†å‡†ç¡®æ€§ï¼ˆå›¾2 & è¡¨1ï¼‰
| æ¨¡å‹ | MMLU-Pro | MMLU | Code | Math | Commonsense |
|------|----------|------|------|------|-------------|
| **Nemotron-3-Nano-30B-A3B** | **52.87** | **72.11** | **55.14** | **80.19** | **82.10** |
| Standard MoE (baseline) | 48.30 | 70.10 | 51.95 | 78.32 | 81.73 |

> LatentMoE åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå…¨é¢è¶…è¶Šæ ‡å‡† MoEï¼Œå¹³å‡æå‡çº¦ 3â€“4 ä¸ªç™¾åˆ†ç‚¹ã€‚

#### âœ… æ¨ç†ååå¯¹æ¯”ï¼ˆå›¾2ï¼‰
- Nemotron-3-Nano åœ¨ 8k è¾“å…¥ / 16k è¾“å‡ºæ¡ä»¶ä¸‹ï¼Œç›¸æ¯” Qwen3-30B-A3Bï¼š
  - **ååæå‡ 3.3Ã—**
  - æ›´é•¿åºåˆ—ä¸‹ä¼˜åŠ¿è¿›ä¸€æ­¥æ‰©å¤§

#### âœ… MTP æ¶ˆèå®éªŒï¼ˆè¡¨2ï¼‰
åœ¨ 8B active MoE åŸºç¡€æ¨¡å‹ä¸ŠåŠ å…¥ MTP åå¹³å‡æå‡çº¦ **2.4%**ï¼š
| ä»»åŠ¡ | Baseline | +MTP | Î” |
|------|----------|-------|----|
| MMLU | 70.06 | 71.26 | +1.20 |
| MMLU-Pro (CoT) | 45.05 | 47.84 | +2.79 |
| GSM8K | 82.49 | 84.46 | +1.97 |
| RACE | 84.02 | 85.36 | +1.34 |

è¡¨æ˜ MTP èƒ½æœ‰æ•ˆå¢å¼ºå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚

#### âœ… NVFP4 è®­ç»ƒæ•ˆæœï¼ˆå›¾4 & å›¾5ï¼‰
- **è®­ç»ƒæŸå¤±å·®è· <1%**ï¼ˆvs BF16ï¼‰ï¼ŒéªŒè¯æŸå¤±å·®è·è¿›ä¸€æ­¥ç¼©å°è‡³ **<0.6%**ï¼ˆæ›´å¤§æ¨¡å‹ A8Bï¼‰
- ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°å‡ ä¹ä¸€è‡´ï¼ˆè§å›¾5ï¼‰ï¼Œè¯æ˜ NVFP4 ä¸æŸå®³æœ€ç»ˆæ€§èƒ½
- å®ç° **3Ã— FP4 GEMM ååæå‡**

#### âœ… é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼ˆè¡¨3 & å›¾6ï¼‰
| æ¨¡å‹ | 128k | 256k | 512k | **1M** |
|------|------|------|------|--------|
| Nemotron-Nano-12B-v2 (Dense Hybrid) | 85.13 | 79.85 | 75.12 | **23.43** |
| **Nemotron-3-Nano-30B-A3B (MoE Hybrid)** | 74.48 | 71.67 | 66.02 | **54.19** |

> MoE Hybrid æ¶æ„åœ¨ 1M ä¸Šä¸‹æ–‡ä¸‹è¡¨ç°æ›´ç¨³å¥ï¼Œæ— æ˜æ˜¾æ–­å´–å¼ä¸‹é™ã€‚

- **å›¾6ï¼šä»£ç åºåˆ—è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰éšä½ç½®é€’å‡**
  - è¡¨æ˜æ¨¡å‹èƒ½æŒç»­åˆ©ç”¨é•¿è·ç¦»ä¸Šä¸‹æ–‡è¿›è¡Œé¢„æµ‹
  - å¹‚å¾‹æ‹Ÿåˆè‰¯å¥½ï¼ˆRÂ²=0.883ï¼‰ï¼Œä½“ç°é•¿æœŸè®°å¿†èƒ½åŠ›

#### âœ… æ¨ç†é¢„ç®—æ§åˆ¶ï¼ˆå›¾8ï¼‰
- æä¾›å¹³æ»‘çš„ **accuracy-efficiency trade-off curve**
- ç”¨æˆ·å¯æ ¹æ®åº”ç”¨éœ€æ±‚è°ƒèŠ‚æ€è€ƒ Token æ•°é‡ï¼Œçµæ´»å¹³è¡¡å“åº”é€Ÿåº¦ä¸ç­”æ¡ˆè´¨é‡

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Hybrid Mamba-Transformer MoE æ˜¯é«˜æ•ˆæ¨ç†çš„ç†æƒ³æ¶æ„**  
   åœ¨ä¿æŒç”šè‡³è¶…è¶Šçº¯ Transformer MoE ç²¾åº¦çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½æ¨ç†å†…å­˜å ç”¨å’Œå»¶è¿Ÿã€‚

2. **LatentMoE å®ç°äº†â€œè´¨é‡-æ•ˆç‡å¸•ç´¯æ‰˜å‰æ²¿â€çš„çªç ´**  
   é€šè¿‡é™ç»´è·¯ç”± + æ‰©å±•ä¸“å®¶æ•°é‡ï¼Œåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹è·å¾—æ›´é«˜çš„æ¨¡å‹è¡¨è¾¾åŠ›ã€‚

3. **NVFP4 å¯ç¨³å®šç”¨äºå¤§è§„æ¨¡é¢„è®­ç»ƒ**  
   ç»“åˆæ•æ„Ÿå±‚é«˜ç²¾åº¦ä¿æŠ¤æœºåˆ¶ï¼Œå¯åœ¨ä¸æŸå¤±æ€§èƒ½çš„å‰æä¸‹å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡ã€‚

4. **MTP ä¸ä»…æå‡è®­ç»ƒä¿¡å·å¯†åº¦ï¼Œè¿˜å¤©ç„¶æ”¯æŒå¿«é€Ÿæ¨ç†**  
   ä¸ Speculative Decoding å®Œç¾ç»“åˆï¼Œæ˜¯æœªæ¥é«˜æ•ˆ LLM çš„æ ‡é…æŠ€æœ¯ä¹‹ä¸€ã€‚

5. **ç»Ÿä¸€å¤šç¯å¢ƒ RL è®­ç»ƒä¼˜äºåˆ†é˜¶æ®µè®­ç»ƒ**  
   æ›´ç¨³å®šã€ä¸æ˜“è¿‡æ‹Ÿåˆç‰¹å®šä»»åŠ¡ï¼Œå…¨é¢æå‡ agentic èƒ½åŠ›ã€‚

6. **å¼€æ”¾é€æ˜æ˜¯æ¨åŠ¨ç”Ÿæ€å‘å±•çš„å…³é”®**  
   NVIDIA æ‰¿è¯ºå…¬å¼€æ¨¡å‹æƒé‡ã€è®­ç»ƒæ•°æ®ã€è½¯ä»¶æ ˆå’Œé…æ–¹ï¼Œæå¤§ä¿ƒè¿›ç¤¾åŒºå¤ç°ä¸åˆ›æ–°ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **LatentMoE è®¾è®¡å¤æ‚åº¦è¾ƒé«˜**ï¼Œéœ€ç²¾ç»†è°ƒæ•´æŠ•å½±ç»´åº¦ä¸ä¸“å®¶åˆ†å¸ƒã€‚
- **Mamba å±‚å¯¹æŸäº›ç»“æ„åŒ–æ¨ç†ä»»åŠ¡çš„æ”¯æŒå°šå¾…éªŒè¯**ï¼ˆå¦‚å½¢å¼é€»è¾‘ã€ç¬¦å·æ“ä½œï¼‰ã€‚
- **ç›®å‰ä»… Nano å…¬å¼€ï¼ŒSuper/Ultra å°šæœªå‘å¸ƒ**ï¼Œå®é™…è½åœ°ä»éœ€ç­‰å¾…ã€‚
- **é•¿ä¸Šä¸‹æ–‡çš„å®é™…åº”ç”¨åœºæ™¯ä»æœ‰é™**ï¼ŒçœŸå®ä¸–ç•Œä¸­ 1M tokens çš„æœ‰æ•ˆåˆ©ç”¨ç‡æœ‰å¾…è§‚å¯Ÿã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- è¿›ä¸€æ­¥ä¼˜åŒ– LatentMoE ä¸ MTP çš„ååŒæ•ˆåº”
- æ¢ç´¢æ›´å¤šç¨€ç–åŒ–ä¸å‹ç¼©æŠ€æœ¯ï¼ˆå·²åœ¨ Compression å›¢é˜Ÿåå•ä¸­ä½“ç°ï¼‰
- æ„å»ºæ›´å¤§è§„æ¨¡çš„ agentic simulation environments
- æ¨åŠ¨ NVFP4 åœ¨å…¶ä»–æ¨¡æ€ï¼ˆè§†è§‰ã€è¯­éŸ³ï¼‰ä¸­çš„åº”ç”¨
- å‘å¸ƒ Super å’Œ Ultra æ¨¡å‹åŠå…¶å®Œæ•´è®­ç»ƒè½¨è¿¹

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **Nemotron 3 é€šè¿‡ Hybrid Mamba-MoE + LatentMoE + MTP + NVFP4 + Multi-Env RL çš„ç³»ç»Ÿçº§åˆ›æ–°ï¼Œå®ç°äº†å½“å‰æœ€å…ˆè¿›æ°´å¹³çš„â€œé«˜ç²¾åº¦ + é«˜æ•ˆç‡ + é•¿ä¸Šä¸‹æ–‡ + å¯æ§æ¨ç†â€å¹³è¡¡ï¼Œå¹¶ä»¥å®Œå…¨å¼€æ”¾çš„å§¿æ€å¼•é¢†ä¸‹ä¸€ä»£å¼€æº agentic AI å‘å±•ã€‚**

</details>

---

### 8. [Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits](https://arxiv.org/abs/2512.20755)

**Authors**: Yizhak Yisrael Elboher, Avraham Raviv, Amihay Elboher, Zhouxing Shi, Omri Azencot, Hillel Kugler, Guy Katz  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.20755v1  

#### Abstract
Ensuring the safety and efficiency of AI systems is a central goal of modern research. Formal verification provides guarantees of neural network robustness, while early exits improve inference efficiency by enabling intermediate predictions. Yet verifying networks with early exits introduces new cha...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡èšç„¦äº**æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å®‰å…¨æ€§å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡**ã€‚ä¸€æ–¹é¢ï¼Œ**å½¢å¼åŒ–éªŒè¯**ï¼ˆformal verificationï¼‰å¯ä¸ºDNNæä¾›é²æ£’æ€§ä¿è¯ï¼Œç¡®ä¿å…¶å¯¹è¾“å…¥æ‰°åŠ¨çš„ç¨³å®šæ€§ï¼›å¦ä¸€æ–¹é¢ï¼Œ**æ—©é€€æœºåˆ¶**ï¼ˆEarly Exits, EEsï¼‰é€šè¿‡å…è®¸ç½‘ç»œåœ¨ä¸­é—´å±‚æå‰ç»ˆæ­¢æ¨ç†æ¥æå‡æ¨ç†æ•ˆç‡ã€‚ç„¶è€Œï¼Œå°†EEå¼•å…¥DNNåï¼Œå…¶**æ¡ä»¶æ‰§è¡Œè·¯å¾„**ï¼ˆconditional execution pathsï¼‰ä½¿å¾—ä¼ ç»Ÿå½¢å¼åŒ–éªŒè¯æ–¹æ³•éš¾ä»¥ç›´æ¥åº”ç”¨ã€‚

å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³ä»¥ä¸‹æ ¸å¿ƒé—®é¢˜ï¼š
- å¦‚ä½•ä¸ºå…·æœ‰æ—©é€€æœºåˆ¶çš„DNNå®šä¹‰åˆé€‚çš„**é²æ£’æ€§æ€§è´¨**ï¼Ÿ
- å¦‚ä½•è®¾è®¡ä¸€ä¸ª**é«˜æ•ˆä¸”å®Œå¤‡**çš„å½¢å¼åŒ–éªŒè¯ç®—æ³•æ¥å¤„ç†è¿™ç§åŠ¨æ€æ¶æ„ï¼Ÿ

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
ä½œè€…æå‡ºäº†é¦–ä¸ªé’ˆå¯¹**å¸¦æœ‰æ—©é€€æœºåˆ¶çš„DNNè¿›è¡Œå½¢å¼åŒ–éªŒè¯**çš„ç³»ç»Ÿæ¡†æ¶ï¼Œä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š

1. **å®šä¹‰äº†é€‚ç”¨äºæ—©é€€ç½‘ç»œçš„å±€éƒ¨é²æ£’æ€§æ€§è´¨**ï¼ˆrevised local robustness propertyï¼‰  
   - æ˜ç¡®è€ƒè™‘äº†å¤šä¸ªè¾“å‡ºå±‚çš„å­˜åœ¨ä»¥åŠæ—©é€€é€»è¾‘ä¸­çš„æ¡ä»¶åˆ†æ”¯ã€‚
   - å½¢å¼åŒ–åœ°æè¿°äº†â€œåä¾‹â€å¿…é¡»æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š(i) æŸä¸ªéä¸»å¯¼ç±»åœ¨æŸä¸€å±‚èƒœå‡ºï¼›(ii) ä¸»å¯¼ç±»æœªåœ¨ä¹‹å‰ä»»ä½•ä¸€å±‚èƒœå‡ºã€‚

2. **æå‡ºäº†ä¸€ç§é€šç”¨çš„éªŒè¯ç®—æ³•ï¼ˆAlg. 1ï¼‰åŠå…¶ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆAlg. 2ï¼‰**
   - **Alg. 1** æ˜¯åŸºç¡€ç®—æ³•ï¼Œéå†æ‰€æœ‰å¯èƒ½çš„exitå’Œrunner-upç±»åˆ«ï¼Œè°ƒç”¨åº•å±‚éªŒè¯å™¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¿åé²æ£’æ€§çš„è¾“å…¥ã€‚
   - **Alg. 2** å¼•å…¥ä¸¤é¡¹å…³é”®ä¼˜åŒ–ï¼š
     - **Break Optimization**ï¼ˆæ—©åœç­–ç•¥ï¼‰ï¼šè‹¥å½“å‰exitä¸­ä¸»å¯¼ç±»å¾—åˆ†è¶³å¤Ÿé«˜ï¼ˆ>1âˆ’Tï¼‰ï¼Œåˆ™æ— éœ€æ£€æŸ¥å…¶ä»–ç±»åˆ«å³å¯è·³è¿‡å†…å±‚å¾ªç¯ã€‚
     - **Continue Optimization**ï¼ˆæå‰è¿”å›ï¼‰ï¼šè‹¥èƒ½è¯æ˜æ‰€æœ‰è¾“å…¥éƒ½ä¼šé€šè¿‡å½“å‰exitä¸”ä¸»å¯¼ç±»å§‹ç»ˆé¢†å…ˆï¼Œåˆ™å¯æå‰è¿”å›`SAFE`ï¼Œæ— éœ€ç»§ç»­åç»­exitã€‚

3. **ç†è®ºåˆ†ææ­ç¤ºäº†å¤æ‚åº¦ä¼˜åŠ¿**
   - åœ¨**è½¨è¿¹ç¨³å®šæ€§å‡è®¾**ï¼ˆtrace stabilityï¼‰ä¸‹ï¼ˆå³é‚»åŸŸå†…æ‰€æœ‰è¾“å…¥èµ°ç›¸åŒè·¯å¾„ï¼‰ï¼ŒéªŒè¯å¤æ‚åº¦ä»ä¾èµ–æ•´ä¸ªç½‘ç»œè§„æ¨¡å˜ä¸ºä»…ä¾èµ–å®é™…æ‰§è¡Œè·¯å¾„é•¿åº¦ã€‚
   - è¯æ˜äº†è¯¥é—®é¢˜å±äº**å›ºå®šå‚æ•°å¯è¿½è¸ª**ï¼ˆFPTï¼‰é—®é¢˜ï¼Œå‚æ•°ä¸º `kÂ·|T(x)|`ï¼ˆå±‚å®½ Ã— è·¯å¾„é•¿åº¦ï¼‰ã€‚

4. **å°†EEä½œä¸ºæå‡æ ‡å‡†æ¨¡å‹å¯éªŒè¯æ€§çš„æ‰‹æ®µ**
   - ä¸ä»…éªŒè¯EEç½‘ç»œæœ¬èº«ï¼Œè¿˜å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å‘æ ‡å‡†ç½‘ç»œæ·»åŠ EEæ¥**åŠ é€Ÿå…¶å½¢å¼åŒ–éªŒè¯è¿‡ç¨‹**ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **å¡«è¡¥ç©ºç™½**ï¼šé¦–æ¬¡ç³»ç»Ÿç ”ç©¶EEç½‘ç»œçš„å½¢å¼åŒ–éªŒè¯é—®é¢˜ã€‚
- **ä¿æŒsoundnessä¸completeness**ï¼šæå‡ºçš„ä¼˜åŒ–ä¸ç‰ºç‰²æ­£ç¡®æ€§ã€‚
- **æ˜¾è‘—æå‡æ•ˆç‡**ï¼šå°¤å…¶åœ¨`SAFE`æ¡ˆä¾‹ä¸Šï¼ŒéªŒè¯æ—¶é—´å¤§å¹…ç¼©çŸ­ï¼ˆæœ€é«˜è¾¾10å€ä»¥ä¸Šï¼‰ã€‚
- **å¢å¼ºå¯éªŒè¯æ€§**ï¼šEEä¸ä»…åŠ é€Ÿæ¨ç†ï¼Œä¹Ÿä½¿æ›´å¤æ‚çš„æŸ¥è¯¢èƒ½åœ¨æœ‰é™æ—¶é—´å†…è¢«éªŒè¯ä¸º`SAFE`ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **MNIST**ï¼šæ‰‹å†™æ•°å­—åˆ†ç±»ï¼Œ10ç±»ï¼Œç°åº¦å›¾åƒï¼ˆ1Ã—28Ã—28ï¼‰
- **CIFAR-10**ï¼šå½©è‰²å›¾åƒåˆ†ç±»ï¼Œ10ç±»ï¼ŒRGBå›¾åƒï¼ˆ3Ã—32Ã—32ï¼‰
- **CIFAR-100**ï¼šæ›´å¤æ‚çš„å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œ100ç±»

### æ¨¡å‹æ¶æ„
- **Fully Connected (FC-6)**ï¼šç”¨äºMNIST
- **LeNet-5 (CNN)**ï¼šç”¨äºCIFAR-10
- **ResNet-18**ï¼ˆä¿®æ”¹ç‰ˆï¼Œç”¨AveragePoolæ›¿ä»£MaxPoolï¼‰
- **VGG-16**

æ‰€æœ‰æ¨¡å‹å‡åœ¨å…¶ç‰¹å®šå±‚åæ·»åŠ äº†early exitsï¼ˆå…¨è¿æ¥å±‚+SoftMaxï¼‰ï¼Œå¹¶é€šè¿‡é€å±‚å¾®è°ƒè®­ç»ƒã€‚

### å®éªŒè®¾ç½®
- **éªŒè¯å·¥å…·**ï¼šåŸºäºAlpha-Beta CROWNï¼ˆstate-of-the-art incomplete verifierï¼‰ï¼Œæ”¯æŒReLUå’ŒSoftMaxã€‚
- **æ—©é€€é˜ˆå€¼**ï¼šé»˜è®¤ $ T = 0.9 $ï¼Œéƒ¨åˆ†å®éªŒæµ‹è¯•ä¸åŒé˜ˆå€¼ï¼ˆ0.6â€“0.95ï¼‰ã€‚
- **æ‰°åŠ¨èŒƒå›´**ï¼š$ \epsilon \in \{0.001, 0.005, 0.01, 0.05, 0.1\} $
- **æ¯ç»„å®éªŒæ ·æœ¬æ•°**ï¼š100ä¸ªæµ‹è¯•æ ·æœ¬
- **è¶…æ—¶é™åˆ¶**ï¼šæ¯ä¸ªéªŒè¯æŸ¥è¯¢æœ€å¤š30åˆ†é’Ÿ

### è¯„ä¼°æŒ‡æ ‡
- **éªŒè¯è¿è¡Œæ—¶é—´**ï¼ˆmean/std/medianï¼‰
- **ç»“æœåˆ†å¸ƒ**ï¼š`SAFE` / `UNSAFE` / `UNKNOWN`ï¼ˆè¶…æ—¶æˆ–å¤±è´¥ï¼‰
- **é²æ£’æ€§æ¯”ä¾‹**ï¼š`#SAFE / (#SAFE + #UNSAFE)`
- **æ¨ç†å»¶è¿Ÿ**ï¼ˆinference latencyï¼‰
- **çƒ­åŠ›å›¾åˆ†æ**ï¼šæ¯”è¾ƒæ¨ç†exitä¸éªŒè¯exitçš„ç›¸å…³æ€§

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Alg. 1 vs Alg. 2**ï¼šéªŒè¯ä¼˜åŒ–ç­–ç•¥çš„æœ‰æ•ˆæ€§
- **å¸¦EEçš„æ¨¡å‹ vs åŸå§‹æ¨¡å‹ï¼ˆvanillaï¼‰**ï¼šè¯„ä¼°EEå¯¹éªŒè¯æ•ˆç‡çš„å½±å“
- **ä¸åŒexitä½ç½®ä¸é˜ˆå€¼è®¾ç½®**ï¼šåˆ†æè®¾è®¡é€‰æ‹©å¯¹æ€§èƒ½çš„å½±å“

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### è¡¨1ï¼šéªŒè¯è¿è¡Œæ—¶é—´å¯¹æ¯”ï¼ˆå•ä½ï¼šç§’ï¼‰

| Benchmark           | Alg.1 SAFE Mean | Alg.2 SAFE Mean | åŠ é€Ÿæ¯” |
|---------------------|------------------|------------------|--------|
| MNIST, FC6          | 13.037           | 1.143            | ~11.4Ã— |
| CIFAR10, LeNet      | 56.927           | 7.255            | ~7.8Ã—  |
| CIFAR10, ResNet-18  | 409.395          | 42.646           | ~9.6Ã—  |
| CIFAR10, VGG16      | â€”                | 1532.418         | æˆåŠŸéªŒè¯ |

> æ³¨ï¼šVGG16ä½¿ç”¨Alg.1æœªèƒ½å®Œæˆä»»ä½•`SAFE`éªŒè¯ï¼ˆå…¨éƒ¨è¶…æ—¶ï¼‰ï¼Œè€ŒAlg.2æˆåŠŸã€‚

#### è¡¨2ï¼šUNKNOWNï¼ˆè¶…æ—¶/å¤±è´¥ï¼‰æ•°é‡å¯¹æ¯”

| Benchmark           | Alg.1 | Alg.2 | ä¸‹é™å¹…åº¦ |
|---------------------|-------|-------|----------|
| MNIST, FC6          | 672   | 652   | ~3%      |
| CIFAR10, LeNet      | 253   | 256   | +1.2%    |
| CIFAR10, ResNet-18  | 159   | 162   | +1.9%    |
| CIFAR10, VGG16      | 68    | 53    | **â†“22%** |

> åœ¨æœ€å¤§æ¨¡å‹VGG16ä¸Šï¼ŒUNKNOWNæ•°å‡å°‘è¶…è¿‡20%ï¼Œè¯´æ˜ä¼˜åŒ–æå‡äº†æ±‚è§£èƒ½åŠ›ã€‚

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

- **å›¾2æ˜¾ç¤º**ï¼šå¯¹äºResNet-18ç­‰å¤æ‚æ¨¡å‹ï¼Œ**å¸¦EEçš„æ¨¡å‹ä½¿ç”¨Alg.2éªŒè¯é€Ÿåº¦è¿œå¿«äºåŸå§‹æ¨¡å‹**ã€‚
  - ç‰¹åˆ«æ˜¯åœ¨`SAFE`æ¡ˆä¾‹ä¸­ï¼ŒåŠ é€Ÿå¯è¾¾**4â€“5å€ä»¥ä¸Š**ã€‚
  - åœ¨CIFAR-100ä¸Šçš„ResNet-18å®éªŒä¸­ï¼ŒåŸå§‹æ¨¡å‹åœ¨2å°æ—¶å†…æ— æ³•å®Œæˆ`SAFE`éªŒè¯ï¼Œè€Œå¸¦EEçš„æ¨¡å‹çº¦1å°æ—¶å®Œæˆã€‚

- **å›¾3çƒ­åŠ›å›¾æ˜¾ç¤º**ï¼š
  - å¯¹äº`SAFE`æ ·æœ¬ï¼Œ**éªŒè¯exitä¸æ¨ç†exité«˜åº¦ä¸€è‡´**ï¼Œæ”¯æŒâ€œè½¨è¿¹ç¨³å®šæ€§â€å‡è®¾ã€‚
  - å¯¹äº`UNSAFE`æ ·æœ¬ï¼Œcounterexampleé€šå¸¸åœ¨ç¬¬ä¸€ä¸ªexitå°±è¢«å‘ç°ï¼Œç‹¬ç«‹äºåŸå§‹æ¨ç†è·¯å¾„ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

- **å›¾9å¯¹æ¯”äº†å››ç§å˜ä½“**ï¼š
  - **Alg.3**ï¼ˆä»…breakä¼˜åŒ–ï¼‰ä¼˜äºAlg.1
  - **Alg.4**ï¼ˆä»…continueä¼˜åŒ–ï¼‰ä¼˜äºAlg.3
  - **Alg.2**ï¼ˆä¸¤è€…ç»“åˆï¼‰è¡¨ç°æœ€ä½³ï¼Œå°¤å…¶åœ¨`SAFE`æƒ…å†µä¸‹
- ç»“è®ºï¼šä¸¤ç§ä¼˜åŒ–äº’è¡¥ï¼Œè”åˆä½¿ç”¨æ•ˆæœæœ€ä¼˜ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **æ—©é€€æœºåˆ¶ä¸ä»…èƒ½åŠ é€Ÿæ¨ç†ï¼Œè¿˜èƒ½æ˜¾è‘—æå‡å¯éªŒè¯æ€§**ï¼š
   - æ›´å¤š`SAFE`æŸ¥è¯¢å¯åœ¨åˆç†æ—¶é—´å†…è¢«éªŒè¯ã€‚
   - éªŒè¯è·¯å¾„æ›´çŸ­ï¼Œç¬¦åˆFPTå¤æ‚åº¦ç†è®ºé¢„æµ‹ã€‚

2. âœ… **æå‡ºçš„Alg.2åœ¨ä¿æŒsoundnessä¸completenessçš„åŒæ—¶ï¼Œå¤§å¹…æå‡æ•ˆç‡**ï¼š
   - æœ€é«˜å®ç°**10å€ä»¥ä¸Šçš„éªŒè¯åŠ é€Ÿ**ã€‚
   - åœ¨å¤§æ¨¡å‹ï¼ˆå¦‚VGG16ï¼‰ä¸Šï¼Œç”šè‡³å®ç°äº†ä»â€œæ— æ³•éªŒè¯â€åˆ°â€œæˆåŠŸéªŒè¯â€çš„è·¨è¶Šã€‚

3. âœ… **exitä½ç½®å½±å“é²æ£’æ€§ä¸æ•ˆç‡æƒè¡¡**ï¼š
   - è¾ƒæ—©çš„exitï¼ˆå¦‚ResNet-18 block1åï¼‰å¸¦æ¥æ›´é«˜é²æ£’æ€§ï¼ˆæ›´å¤š`SAFE`ï¼‰å’Œæ›´å¿«éªŒè¯ã€‚
   - è¾ƒæ·±çš„exitè™½ç²¾åº¦ç•¥é«˜ï¼Œä½†é™ä½å¯éªŒè¯æ€§ã€‚

4. âœ… **é˜ˆå€¼é€‰æ‹©å­˜åœ¨å¤šé‡æƒè¡¡**ï¼š
   - é«˜é˜ˆå€¼ â†’ é«˜ç²¾åº¦ã€é«˜é²æ£’æ€§ï¼Œä½†æ¨ç†å’ŒéªŒè¯æ—¶é—´æ›´é•¿ã€‚
   - ç”¨æˆ·éœ€ç»¼åˆè€ƒè™‘ accuracy, latency, robustness, verifiability å››é¡¹æŒ‡æ ‡ã€‚

5. âœ… **æ–¹æ³•ç¡¬ä»¶æ— å…³**ï¼š
   - åœ¨Apple M3å’ŒNVIDIA A100ä¸Šé‡å¤å®éªŒï¼Œå‡è§‚å¯Ÿåˆ°ä¸€è‡´çš„æ€§èƒ½å¢ç›Šè¶‹åŠ¿ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ¡†æ¶ä¸»è¦é’ˆå¯¹**å±€éƒ¨é²æ£’æ€§**ï¼Œæ‰©å±•è‡³å…¶ä»–æ€§è´¨ï¼ˆå¦‚safety, fairnessï¼‰éœ€è¦è°ƒæ•´â€œcontinueâ€ä¼˜åŒ–ã€‚
- ä¾èµ–åº•å±‚éªŒè¯å™¨å¯¹SoftMaxçš„æ”¯æŒï¼ˆå¦‚Alpha-Beta CROWNï¼‰ï¼ŒæŸäº›å·¥å…·å¯èƒ½ä¸å…¼å®¹ã€‚
- æ—©é€€è®­ç»ƒå¯èƒ½å¯¼è‡´è½»å¾®ç²¾åº¦ä¸‹é™ï¼ˆtrade-off between accuracy and efficiencyï¼‰ã€‚
- â€œè½¨è¿¹ç¨³å®šæ€§â€æ˜¯ç†è®ºåŠ é€Ÿçš„å‰æï¼Œåœ¨æç«¯æ‰°åŠ¨ä¸‹å¯èƒ½ä¸æˆç«‹ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•éªŒè¯æ€§è´¨**ï¼šæ”¯æŒsafetyã€fairnessç­‰æ›´å¹¿æ³›çš„å±æ€§ã€‚
2. **å¹¶è¡ŒåŒ–éªŒè¯**ï¼šåˆ©ç”¨åˆ†å¸ƒå¼è®¡ç®—åŠ é€Ÿå¤šexitæˆ–å¤šç±»åˆ«çš„æŸ¥è¯¢ã€‚
3. **æ¢ç´¢æ›´å¤æ‚çš„exitæ¡ä»¶å‡½æ•°**ï¼šè¶…è¶Šç®€å•çš„SoftMaxé˜ˆå€¼ã€‚
4. **ç»“åˆcertified training**ï¼šå°†EEä¸è®¤è¯è®­ç»ƒç»“åˆï¼Œè¿›ä¸€æ­¥æå‡é²æ£’æ€§ã€‚
5. **åº”ç”¨äºæ›´å¤§æ¨¡å‹**ï¼šå¦‚Transformer-based architectures with early exiting.

---

> **æ€»ç»“**ï¼šæœ¬æ–‡å¼€åˆ›æ€§åœ°å°†**æ—©é€€æœºåˆ¶**ä¸**å½¢å¼åŒ–éªŒè¯**ç›¸ç»“åˆï¼Œä¸ä»…è§£å†³äº†EEç½‘ç»œçš„éªŒè¯éš¾é¢˜ï¼Œè¿˜åè¿‡æ¥åˆ©ç”¨EEæ¥**æå‡æ ‡å‡†æ¨¡å‹çš„å¯éªŒè¯æ€§**ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¸ç‰ºç‰²æ­£ç¡®æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜äº†éªŒè¯æ•ˆç‡ï¼Œä¸ºæ„å»º**é«˜æ•ˆä¸”å¯ä¿¡çš„AIç³»ç»Ÿ**æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 9. [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)

**Authors**: Alexandros Christoforos, Chadbourne Davis  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.20724v1  

#### Abstract
Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By se...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç°æœ‰çš„ **diffusion-based æ–‡æœ¬ç”Ÿæˆæ¨¡å‹**ï¼ˆå¦‚ DiffuSeqï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´ä¸¥é‡çš„**è®¡ç®—æˆæœ¬é«˜ã€å†…å­˜å¼€é”€å¤§ã€æ”¶æ•›é€Ÿåº¦æ…¢**ç­‰é—®é¢˜ã€‚åŒæ—¶ï¼Œä¼ ç»Ÿ Transformer æ¶æ„ä¸­çš„å…¨æ³¨æ„åŠ›æœºåˆ¶å…·æœ‰ $O(n^2)$ çš„å¤æ‚åº¦ï¼Œéš¾ä»¥æ‰©å±•åˆ°æ•°åƒç”šè‡³ä¸Šä¸‡ token çš„é•¿åºåˆ—ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå½“å‰å¤§å¤šæ•° LLM åœ¨è¶…è¿‡è®­ç»ƒé•¿åº¦ï¼ˆé€šå¸¸ä¸º 8K tokensï¼‰åæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡º **SA-DiffuSeq**ï¼Œä¸€ç§ç»“åˆ **Sparse Attention (SA)** å’Œ **Mixture of Experts (MoE)** çš„æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œä¸“ä¸ºé«˜æ•ˆä¸”é«˜è´¨é‡çš„é•¿æ–‡æœ¬ç”Ÿæˆè®¾è®¡ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶é›†æˆ**ï¼šé‡‡ç”¨ Longformer é£æ ¼çš„æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆsliding window attentionï¼‰ã€ç©ºæ´æ»‘çª—ï¼ˆdilated sliding windowï¼‰ä»¥åŠå¯¹å…³é”® tokenï¼ˆå¦‚ [CLS]ï¼‰çš„å…¨å±€æ³¨æ„åŠ›ï¼Œå°†æ³¨æ„åŠ›è®¡ç®—ä» $O(n^2)$ é™ä½è‡³ $O(n \times w)$ï¼Œæ˜¾è‘—æå‡å¯æ‰©å±•æ€§ã€‚
  
- **æ‰©æ•£è¿‡ç¨‹ä¼˜åŒ–**ï¼š
  - å¼•å…¥ **soft absorbing state** ä»¥ç¨³å®šæ‰©æ•£è½¨è¿¹å¹¶åŠ é€Ÿå»å™ªï¼›
  - ä½¿ç”¨ **DPM-solver++** åŠ é€Ÿé‡‡æ ·ï¼Œå‡å°‘æ‰€éœ€ diffusion steps æ•°é‡è€Œä¸ç‰ºç‰²è´¨é‡ã€‚

- **åŠ¨æ€èµ„æºåˆ†é…**ï¼šå¼•å…¥ MoE æ¶æ„ï¼Œåœ¨æ¯ä¸ª Transformer å±‚ä¸­é€šè¿‡é—¨æ§æœºåˆ¶ï¼ˆgating mechanismï¼‰åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶ç½‘ç»œï¼Œå®ç°æŒ‰éœ€è®¡ç®—ï¼Œæå‡æ•ˆç‡ã€‚

- **è”åˆå»å™ªä¸æŸå¤±å‡½æ•°è®¾è®¡**ï¼šè®¾è®¡å…¼é¡¾è¿ç»­å™ªå£°é¢„æµ‹å’Œç¦»æ•£è¯­ä¹‰é‡å»ºçš„è”åˆæŸå¤±å‡½æ•°ï¼Œå¢å¼ºç”Ÿæˆæ–‡æœ¬çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- æ˜¾è‘—ä¼˜äº DiffuSeq å’Œ Longformer åœ¨å¤šä¸ªé•¿æ–‡æœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼›
- åœ¨è®­ç»ƒæ—¶é—´å’Œæ¨ç†é€Ÿåº¦æ–¹é¢å‡æœ‰æå‡ï¼ˆè®­ç»ƒæ—¶é—´å‡å°‘çº¦ 15%ï¼Œæ¨ç†æ›´å¿«ï¼‰ï¼›
- èƒ½ç¨³å®šå¤„ç†è¶…è¿‡ 8,000 tokens çš„è¶…é•¿åºåˆ—ï¼Œå…‹æœäº†ä¸»æµ LLM çš„ä¸Šä¸‹æ–‡é•¿åº¦ç“¶é¢ˆï¼›
- ä¿æŒé«˜ BLEUã€ROUGE å’Œ BERTScore åˆ†æ•°çš„åŒæ—¶ï¼Œæå‡äº†ç”Ÿæˆå¤šæ ·æ€§ï¼ˆnoveltyï¼‰ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
å®éªŒåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œï¼š

| æ•°æ®é›† | ä»»åŠ¡ç±»å‹ | ç‰¹ç‚¹ |
|--------|---------|------|
| **Arxiv Abstract Dataset** | ç§‘å­¦æ–‡çŒ®æ‘˜è¦ç”Ÿæˆ | é•¿æ–‡æ¡£ã€æŠ€æœ¯æ€§å¼ºã€éœ€è¦å…¨å±€è¿è´¯æ€§ |
| **HotpotQA** | å¤šè·³é—®ç­”ï¼ˆmulti-hop QAï¼‰ | éœ€è¦è·¨æ®µè½æ¨ç†ä¸æ”¯æŒäº‹å®è¿½è¸ª |
| **Commonsense Conversation Dataset** | å¯¹è¯ç”Ÿæˆ | ä¸Šä¸‹æ–‡ä¾èµ–å¼ºã€éœ€å¸¸è¯†ç†è§£ |
| **Quora Question Pairs (QQP)** | é—®å¥å¤è¿°è¯†åˆ« | æµ‹è¯•è¯­ä¹‰ç­‰ä»·åˆ¤æ–­èƒ½åŠ› |

### å®éªŒè®¾ç½®
- **æ¨¡å‹é…ç½®**ï¼š
  - 12 å±‚ S12Transformerï¼Œæ¯å±‚ 12 ä¸ª attention headsï¼›
  - ä½¿ç”¨ Longformer çš„ sparse attention ç»“æ„ï¼›
  - æ¯å±‚é›†æˆ MoE æ¨¡å—ï¼ŒåŠ¨æ€è·¯ç”±è¾“å…¥ token è‡³ä¸åŒ expertï¼›
  - 2048 diffusion stepsï¼Œsquare-root noise scheduleï¼›
  - å‰å‘æ‰©æ•£å…¬å¼ï¼š$ z_t = \sqrt{\alpha_t} z_0 + \sqrt{1 - \alpha_t} e $ï¼Œå…¶ä¸­ $e$ ä¸º Gaussian noiseã€‚

- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA A100 GPUsã€‚

### è¯„ä¼°æŒ‡æ ‡
ç»¼åˆä½¿ç”¨ä»¥ä¸‹è‡ªåŠ¨è¯„ä»·æŒ‡æ ‡è¡¡é‡ç”Ÿæˆè´¨é‡ï¼š

| æŒ‡æ ‡ | æè¿° |
|------|------|
| **BLEU** | è¡¡é‡ n-gram åŒ¹é…ç¨‹åº¦ï¼Œåæ˜ è¡¨é¢ç›¸ä¼¼æ€§ |
| **ROUGE (R1/R2/RL)** | è¡¡é‡å¬å›ç‡ï¼Œå°¤å…¶é€‚ç”¨äºæ‘˜è¦ä»»åŠ¡ |
| **BERTScore** | åŸºäº BERT çš„è¯­ä¹‰ç›¸ä¼¼åº¦è¯„åˆ†ï¼Œæ›´è´´è¿‘äººç±»åˆ¤æ–­ |
| **Accuracy** | ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚ QQPï¼‰ |
| **Inference Time** | æ¨ç†å»¶è¿Ÿï¼Œè¯„ä¼°æ•ˆç‡ |
| **2-gram Novelty** | è¡¡é‡ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼ˆè¶Šé«˜æ–°é¢–æ€§è¶Šé«˜ï¼‰ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **DiffuSeq**ï¼šåŸºç¡€ diffusion-based åºåˆ—ç”Ÿæˆæ¨¡å‹ï¼›
- **Longformer**ï¼šåŸºäºç¨€ç–æ³¨æ„åŠ›çš„ç»å…¸é•¿æ–‡æœ¬ Transformer æ¨¡å‹ï¼›
- **GPT-4**ï¼šä½œä¸ºå¼ºå¤§é€šç”¨è¯­è¨€æ¨¡å‹å‚è€ƒï¼ˆéƒ¨åˆ†ä»»åŠ¡æåŠï¼‰ï¼›

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### Arxiv Abstract Datasetï¼ˆç§‘å­¦æ‘˜è¦ï¼‰
| Model | R1 | R2 | RL |
|-------|----|----|-----|
| Longformer | 41.44 | 17.52 | 38.70 |
| DiffuSeq | 39.12 | 16.43 | 37.88 |
| **SA-DiffuSeq** | **44.41** | **18.73** | **39.89** |

> âœ… SA-DiffuSeq åœ¨æ‰€æœ‰ ROUGE æŒ‡æ ‡ä¸Šå…¨é¢é¢†å…ˆï¼Œå°¤å…¶ R1 æå‡æ˜¾è‘—ã€‚

#### ä¸åŒé•¿åº¦ä¸‹çš„ Arxiv æ€§èƒ½å¯¹æ¯”ï¼ˆè¡¨2ï¼‰
| Sequence Length | Model | R1 | R2 | RL |
|------------------|--------|-----|-----|------|
| 8K | SA-DiffuSeq | **46.85** | **19.72** | **41.35** |
| 12K | SA-DiffuSeq | **45.40** | **18.94** | **40.50** |
| 16K | SA-DiffuSeq | **43.60** | **18.11** | **39.75** |

> ğŸ” SA-DiffuSeq åœ¨é•¿è¾¾ 16K tokens çš„è¾“å…¥ä¸‹ä»ä¿æŒä¼˜å¼‚æ€§èƒ½ï¼Œè€Œå…¶ä»–æ¨¡å‹æ˜æ˜¾é€€åŒ–ã€‚

#### HotpotQAï¼ˆå¤šè·³é—®ç­”ï¼‰
| Model | Answer EM/F1 | Support EM/F1 |
|--------|---------------|----------------|
| Longformer | 71.21 / 82.42 | 65.11 / 89.50 |
| DiffuSeq | 70.91 / 81.43 | 64.60 / 88.51 |
| **SA-DiffuSeq** | **72.88 / 85.42** | **66.69 / 90.40** |

> âœ… æ˜¾è‘—æå‡ç­”æ¡ˆå‡†ç¡®æ€§å’Œè¯æ®æ”¯æŒè¯†åˆ«èƒ½åŠ›ï¼Œè¯´æ˜å…¶æ›´å¼ºçš„é•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›ã€‚

#### Commonsense Conversation Datasetï¼ˆå¯¹è¯ç”Ÿæˆï¼‰
| Model | BLEU | ROUGE-L | BERTScore |
|--------|--------|----------|------------|
| Longformer | 0.030 | 0.139 | 0.602 |
| DiffuSeq | 0.022 | 0.119 | 0.501 |
| **SA-DiffuSeq** | **0.049** | **0.233** | **0.628** |

> âœ… åœ¨å¯¹è¯æµç•…æ€§ã€ç›¸å…³æ€§å’Œè¯­ä¹‰å‡†ç¡®æ€§æ–¹é¢å‡å¤§å¹…è¶…è¶ŠåŸºçº¿ã€‚

#### QQPï¼ˆå¤è¿°è¯†åˆ«ï¼‰
| Model | Accuracy |
|--------|-----------|
| Longformer | 92.3 |
| DiffuSeq | 91.7 |
| **SA-DiffuSeq** | **95.3** |

> âœ… å‡†ç¡®ç‡æœ€é«˜ï¼Œè¡¨æ˜å…¶åœ¨è¯­ä¹‰ä¿ç•™ä¸è¡¨è¾¾å˜æ¢ä¹‹é—´å–å¾—äº†æ›´å¥½å¹³è¡¡ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

| é…ç½®ä¿®æ”¹ | R1 / R2 / RL |
|----------|-------------|
| å®Œæ•´ SA-DiffuSeqï¼ˆbaselineï¼‰ | 44.41 / 18.73 / 39.89 |
| ç§»é™¤ Sparse Attention | â†“ 42.52 / 17.99 / 38.41 |
| å‡å°‘ diffusion steps åˆ° 1024 | ç•¥é™ |
| å¢åŠ  diffusion steps åˆ° 4096 | å¾®å‡è‡³ 44.71 / 18.55 / 40.20ï¼ˆè¾¹é™…æ”¶ç›Šé€’å‡ï¼‰ |
| æ³¨æ„åŠ›çª—å£ w=256 â†’ 512 â†’ 1024 | å°å¹…æå‡ï¼Œå¤§çª—å£æ›´æœ‰åˆ© |

> ğŸ” ç»“è®ºï¼š
> - **Sparse Attention æ˜¯æœ€å…³é”®ç»„ä»¶**ï¼Œç§»é™¤åæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼›
> - æ›´å¤š diffusion steps å¯ç•¥å¾®æ”¹å–„è´¨é‡ï¼Œä½†å­˜åœ¨æ”¶ç›Šé¥±å’Œï¼›
> - è¾ƒå¤§çš„ attention window æœ‰åŠ©äºæ•æ‰æ›´å¹¿ä¸Šä¸‹æ–‡ï¼Œä½†æ•ˆæœä¸å¦‚ç¨€ç–ç»“æ„æœ¬èº«é‡è¦ã€‚

### æ•ˆç‡ä¸æ–°é¢–æ€§å¯¹æ¯”ï¼ˆTable 7ï¼‰

| Model | Inference Time (ç›¸å¯¹) | 2-gram Novelty |
|--------|------------------------|----------------|
| SA-DiffuSeq | 0.80 | **0.90** |
| DiffuSeq | 1.00 | 0.75 |
| Longformer | 1.40 | 0.60 |

> âš¡ SA-DiffuSeq å®ç°äº†**æ¨ç†é€Ÿåº¦å¿« + ç”Ÿæˆå¤šæ ·æ€§é«˜**çš„æœ€ä½³æƒè¡¡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ç»“æ„åŒ–ç¨€ç–æ€§æ˜¯è§£å†³é•¿æ–‡æœ¬ç”Ÿæˆç“¶é¢ˆçš„æœ‰æ•ˆè·¯å¾„**ï¼šå°† Sparse Attention æœ‰æ•ˆèå…¥ diffusion æ¡†æ¶ï¼Œå¯åœ¨ä¸ç‰ºç‰²è¯­ä¹‰ä¸€è‡´æ€§çš„å‰æä¸‹å¤§å¹…æå‡å¯æ‰©å±•æ€§ã€‚
2. **MoE æ¶æ„å¢å¼ºäº†æ¨¡å‹å®¹é‡ä¸æ•ˆç‡çš„å¹³è¡¡**ï¼šé€šè¿‡åŠ¨æ€æ¿€æ´»ä¸“å®¶ç½‘ç»œï¼Œå®ç°äº†â€œæŒ‰éœ€è®¡ç®—â€ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†å¼‚è´¨æ€§å¼ºçš„é•¿æ–‡æ¡£ã€‚
3. **soft absorbing state å’Œ DPM-solver++ æå‡äº†æ‰©æ•£ç¨³å®šæ€§ä¸é‡‡æ ·æ•ˆç‡**ï¼šä½¿æ¨¡å‹èƒ½åœ¨è¾ƒå°‘æ­¥éª¤å†…å®Œæˆé«˜è´¨é‡é‡å»ºã€‚
4. **SA-DiffuSeq åœ¨å¤šç§é•¿æ–‡æœ¬ä»»åŠ¡ä¸­ consistently è¶…è¶Š SOTA**ï¼Œå°¤å…¶æ˜¯åœ¨è¶…è¿‡ 8K tokens çš„åœºæ™¯ä¸‹ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰å®ç°ä¾èµ–äºé¢„å®šä¹‰çš„ attention patternï¼ˆå¦‚å›ºå®šçª—å£å¤§å°ï¼‰ï¼Œç¼ºä¹å®Œå…¨è‡ªé€‚åº”çš„æ³¨æ„åŠ›ç¨€ç–ç­–ç•¥ï¼›
- MoE å¸¦æ¥çš„é¢å¤–å‚æ•°ç®¡ç†ä¸è´Ÿè½½å‡è¡¡é—®é¢˜å¯èƒ½å½±å“éƒ¨ç½²æ•ˆç‡ï¼›
- æ‰©æ•£æ¨¡å‹å›ºæœ‰çš„è¿­ä»£ç”Ÿæˆæ–¹å¼ä»æ¯” autoregressive æ¨¡å‹æ…¢ï¼Œå°½ç®¡å·²é€šè¿‡ DPM-solver++ åŠ é€Ÿï¼›
- å®éªŒæœªæ¶µç›–æ›´å¤šçœŸå®ä¸–ç•Œåº”ç”¨åœºæ™¯ï¼ˆå¦‚ä¹¦ç±ç”Ÿæˆã€æ³•å¾‹æ–‡ä¹¦æ’°å†™ï¼‰ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **learnable sparse attention patterns**ï¼Œè€Œéæ‰‹å·¥è®¾è®¡ï¼›
- ç»“åˆ **retrieval-augmented generation (RAG)** è¿›ä¸€æ­¥å¢å¼ºé•¿ä¸Šä¸‹æ–‡çš„ä¿¡æ¯åˆ©ç”¨ï¼›
- å°† SA-DiffuSeq æ‰©å±•è‡³ **å¤šæ¨¡æ€é•¿åºåˆ—ç”Ÿæˆ**ï¼ˆå¦‚å›¾æ–‡æŠ¥å‘Šã€è§†é¢‘æè¿°ï¼‰ï¼›
- ç ”ç©¶æ›´é«˜æ•ˆçš„ MoE è·¯ç”±æœºåˆ¶ä¸åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼Œæ¨åŠ¨å·¥ä¸šçº§è½åœ°ã€‚

---

> ğŸ“Œ **æ€»ä½“è¯„ä»·**ï¼š  
> SA-DiffuSeq æˆåŠŸåœ°å°† **structured sparsity**ã€**MoE åŠ¨æ€è®¡ç®—** ä¸ **diffusion modeling** æœ‰æœºç»“åˆï¼Œä¸ºé•¿æ–‡æœ¬ç”Ÿæˆæä¾›äº†ä¸€ä¸ªå…¼å…· **scalabilityã€efficiency ä¸ expressiveness** çš„æ–°èŒƒå¼ï¼Œä»£è¡¨äº† diffusion-based NLP æ¨¡å‹çš„é‡è¦è¿›å±•ã€‚

</details>

---

### 10. [SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation](https://arxiv.org/abs/2512.21204)

**Authors**: Mahi Luthra, Jiayi Shen, Maxime Poli, Angelo Ortiz, Yosuke Higuchi, Youssef Benchekroun, Martin Gleize, Charles-Eric Saint-James, Dongyan Lin, Phillip Rust, Angel Villar, Surya Parimi, Vanessa Stark, Rashel Moritz, Juan Pino, Yann LeCun, Emmanuel Dupoux  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.21204v1  

#### Abstract
Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰çš„è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹ï¼ˆå¦‚ HuBERTã€WavLMï¼‰åœ¨å­¦ä¹ è¯­è¨€è¡¨ç¤ºæ—¶éœ€è¦æ•°åƒå°æ—¶çš„è®­ç»ƒæ•°æ®ï¼Œä¸”å¯¹å£°å­¦å’Œä¸Šä¸‹æ–‡å˜åŒ–æ•æ„Ÿï¼Œå¯¼è‡´å…¶åœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„é€‚åº”èƒ½åŠ›å·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å©´å„¿ä»…é€šè¿‡å‡ ç™¾å°æ—¶çš„è¯­éŸ³è¾“å…¥å³å¯å¿«é€ŸæŒæ¡æ–°è¯­è¨€çš„åŸºæœ¬å•ä½ã€‚æœ¬æ–‡æ—¨åœ¨ç¼©å°è¿™ä¸€æ•ˆç‡å·®è·ï¼Œå®ç°**æä½èµ„æºä¸‹çš„å¿«é€Ÿè¯­éŸ³è¡¨ç¤ºå­¦ä¹ **ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡ºäº† **SpidR-Adapt**ï¼Œä¸€ä¸ªç”¨äºå°‘æ ·æœ¬è¯­éŸ³è¡¨ç¤ºå­¦ä¹ çš„é€šç”¨æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯å°†ä½èµ„æºè¯­éŸ³è¡¨ç¤ºå­¦ä¹ å»ºæ¨¡ä¸º**å…ƒå­¦ä¹ ï¼ˆmeta-learningï¼‰é—®é¢˜**ï¼Œå¹¶å¼•å…¥ä»¥ä¸‹ä¸‰å¤§ç»„ä»¶ï¼š

1. **å¤šä»»åŠ¡è‡ªé€‚åº”é¢„è®­ç»ƒï¼ˆMAdaPTï¼‰**  
   å°†æ¨¡å‹è®­ç»ƒè§†ä¸ºåŒå±‚ä¼˜åŒ–é—®é¢˜ï¼šå†…å¾ªç¯æ¨¡æ‹Ÿåœ¨ç›®æ ‡è¯­è¨€ä¸Šçš„â€œä¸€ç”Ÿâ€ä½èµ„æºå­¦ä¹ è¿‡ç¨‹ï¼›å¤–å¾ªç¯é€šè¿‡å¤šä¸ªæºè¯­è¨€ä»»åŠ¡æ›´æ–°å…ƒå‚æ•°ï¼Œå½¢æˆå¼ºå½’çº³åç½®ï¼Œæå‡è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶å¼•å…¥**ä¸»åŠ¨é—å¿˜ï¼ˆactive forgettingï¼‰**æœºåˆ¶ï¼Œåœ¨æ¯ä¸ªepisodeå¼€å§‹æ—¶é‡ç½®é¢„æµ‹å¤´å’Œç æœ¬ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆå†å²è¯­è¨€çŸ¥è¯†ï¼Œä¿ƒè¿›å¯è¿ç§»è¡¨ç¤ºçš„å­¦ä¹ ã€‚

2. **ä¸€é˜¶åŒå±‚ä¼˜åŒ–ï¼ˆFOBLOï¼‰**  
   é’ˆå¯¹MAdaPTä¸­çš„åŒå±‚ä¼˜åŒ–è®¡ç®—å¼€é”€å¤§é—®é¢˜ï¼Œæå‡ºä¸€ç§é«˜æ•ˆçš„ä¸€é˜¶è¿‘ä¼¼ç®—æ³• FOBLOã€‚è¯¥æ–¹æ³•é¿å…äº†äºŒé˜¶æ¢¯åº¦è®¡ç®—ï¼Œåˆ©ç”¨å†…å¾ªç¯ç»“æŸä¸å¤–å¾ªç¯ç»“æŸä¹‹é—´çš„å‚æ•°å·®å¼‚æ¥ä¼°è®¡å…ƒæ¢¯åº¦ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å…ƒè®­ç»ƒã€‚

3. **äº¤é”™ç›‘ç£ï¼ˆinterleaved supervisionï¼‰**  
   åœ¨å…ƒè®­ç»ƒå‰è¿›è¡Œé¢„è®­ç»ƒé˜¶æ®µï¼Œäº¤æ›¿ä½¿ç”¨è‡ªç›‘ç£æŸå¤±ï¼ˆSSLï¼‰å’Œå°‘é‡æ ‡æ³¨æ•°æ®çš„ç›‘ç£æŸå¤±ï¼ˆphoneme-levelï¼‰ï¼Œä»¥è·å¾—æ›´é²æ£’çš„åˆå§‹åŒ–ã€‚è¿™ç§ç­–ç•¥ç»“åˆäº†æ— æ ‡ç­¾æ•°æ®çš„å¤§è§„æ¨¡ä¼˜åŠ¿ä¸æœ‰æ ‡ç­¾ä¿¡å·çš„è¯­ä¹‰å¼•å¯¼ï¼Œæå‡äº†æ¨¡å‹å¯¹å£°å­¦å’Œä¸Šä¸‹æ–‡å˜å¼‚çš„é²æ£’æ€§ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ•°æ®æ•ˆç‡æé«˜**ï¼šåœ¨ç›®æ ‡è¯­è¨€ä¸Šä»…éœ€ **1å°æ—¶æœªæ ‡æ³¨éŸ³é¢‘** å³å¯è¾¾åˆ°ä¼ ç»Ÿæ–¹æ³•åœ¨6000å°æ—¶æ•°æ®ä¸Šè®­ç»ƒçš„æ€§èƒ½ï¼Œ**æ•°æ®æ•ˆç‡æå‡è¶…è¿‡100å€**ã€‚
- **æ¶æ„æ— å…³æ€§**ï¼šè™½ç„¶åŸºäº SpidR æ„å»ºï¼Œä½† MAdaPT-FOBLO æ¡†æ¶å¯æ¨å¹¿è‡³å…¶ä»–è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹ã€‚
- **æ›´å¼ºçš„OODæ³›åŒ–èƒ½åŠ›**ï¼šç›¸æ¯”æ ‡å‡†å¤šä»»åŠ¡é¢„è®­ç»ƒï¼ˆMulti-Task-PTï¼‰ï¼ŒSpidR-Adapt åœ¨æœªè§è¯­è¨€ä¸Šçš„é€‚åº”é€Ÿåº¦æ›´å¿«ã€æ€§èƒ½æ›´é«˜ã€‚
- **ç¨³å®šé«˜æ•ˆçš„è®­ç»ƒæœºåˆ¶**ï¼šé€šè¿‡äº¤é”™ç›‘ç£åˆå§‹åŒ–å’Œä¸»åŠ¨é—å¿˜æœºåˆ¶ï¼Œè§£å†³äº†å…ƒè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **æºè¯­è¨€ï¼ˆSource Languagesï¼‰**ï¼š19ç§è¯­è¨€ï¼Œæ¥è‡ª **VoxPopuli** å’Œ **VoxCommunis Corpus**ã€‚
  - æ¯ç§è¯­è¨€æä¾›çº¦300å°æ—¶æœªæ ‡æ³¨è¯­éŸ³ï¼ˆç”¨äºå†…å¾ªç¯ï¼‰ï¼›
  - æœ€å¤š50å°æ—¶å¸¦éŸ³ç´ å¯¹é½çš„æ ‡æ³¨è¯­éŸ³ï¼ˆç”¨äºå¤–å¾ªç¯ç›‘ç£ï¼‰ã€‚
- **ç›®æ ‡è¯­è¨€ï¼ˆTarget Languagesï¼‰**ï¼š
  - å¼€å‘é›†ï¼š5ç§è¯­è¨€ï¼ˆSwahili, Tamil, Thai, Turkish, Ukrainianï¼‰ï¼Œæ¥è‡ª **CommonVoice**ï¼›
  - æµ‹è¯•é›†ï¼š3ç§è¯­è¨€ï¼ˆEnglish, French, Germanï¼‰ï¼Œä» VoxPopuli ä¸­é‡‡æ ·ä¸åŒè§„æ¨¡å­é›†ï¼ˆ10åˆ†é’Ÿã€1å°æ—¶ã€10å°æ—¶ã€100å°æ—¶ï¼‰ç”¨äºfew-shot adaptationã€‚
- **å¯¹æ¯”è®¾ç½®**ï¼š
  - **In-Domain Mono-Task-PT**ï¼šåœ¨ç›®æ ‡è¯­è¨€ä¸Šç”¨6000å°æ—¶æ•°æ®è®­ç»ƒçš„â€œoracleâ€æ¨¡å‹ï¼Œä½œä¸ºæ€§èƒ½ä¸Šé™å‚è€ƒã€‚

### å®éªŒè®¾ç½®
- **éª¨å¹²æ¨¡å‹**ï¼šSpidRï¼ˆstate-of-the-art è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹ï¼‰
- **å…ƒè®­ç»ƒé…ç½®**ï¼š
  - 800ä¸ª episodesï¼Œæ¯ä¸ªepisodeåŒ…å«1800æ­¥å†…å¾ªç¯ï¼ˆè‡ªç›‘ç£ï¼‰+ 200æ­¥å¤–å¾ªç¯ï¼ˆç›‘ç£ï¼‰ï¼›
  - åˆ†å¸ƒå¼è®­ç»ƒäº16å—GPUï¼›
  - å†…å¾ªç¯ä½¿ç”¨éšæœºé€‰å–çš„10å°æ—¶æºè¯­è¨€ç‰‡æ®µï¼›
  - å¤–å¾ªç¯ä½¿ç”¨æ ‡æ³¨æ•°æ®è¿›è¡Œç›‘ç£æ›´æ–°ã€‚
- **å…ƒæµ‹è¯•**ï¼šåœ¨ç›®æ ‡è¯­è¨€ä¸Šè¿›è¡Œå¿«é€Ÿå¾®è°ƒï¼ˆfast adaptationï¼‰ï¼Œè¯„ä¼°ä¸åŒæ•°æ®é‡ä¸‹çš„è¡¨ç°ã€‚

### è¯„ä¼°æŒ‡æ ‡
1. **ABX discriminability**ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ï¼š
   - è¡¡é‡åµŒå…¥ç©ºé—´ä¸­éŸ³ç´ åŒºåˆ†èƒ½åŠ›ï¼›
   - åŒ…æ‹¬ within-speaker å’Œ across-speaker æ¡ä»¶ã€‚
2. **ä¸‹æ¸¸å£è¯­è¯­è¨€å»ºæ¨¡ï¼ˆSLMï¼‰ä»»åŠ¡**ï¼š
   - **sWUGGY**ï¼šè¯æ³•åˆ¤æ–­ï¼ˆçœŸè¯ vs. éè¯ï¼‰ï¼›
   - **sBLIMP**ï¼šå¥æ³•åˆ¤æ–­ï¼ˆè¯­æ³•æ­£ç¡® vs. é”™è¯¯ï¼‰ï¼›
   - **tSCï¼ˆspoken Topic StoryClozeï¼‰**ï¼šç¯‡ç« è¿è´¯æ€§åˆ¤æ–­ã€‚
3. **Phoneme Discovery Benchmark**ï¼š
   - **PNMI**ï¼ˆPhone-normalized Mutual Informationï¼‰ï¼šè¶Šé«˜è¶Šå¥½ï¼›
   - **PER**ï¼ˆPhoneme Error Rateï¼‰ï¼šè¶Šä½è¶Šå¥½ï¼›
   - ABX scoresã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | æè¿° |
|------|------|
| **In-Domain Mono-Task-PT** | åœ¨ç›®æ ‡è¯­è¨€ä¸Šç”¨6kå°æ—¶è®­ç»ƒçš„æ ‡å‡†æ¨¡å‹ï¼ˆoracleï¼‰ |
| **Multi-Task-PT[SSL]** | å¤šä»»åŠ¡é¢„è®­ç»ƒï¼Œä»…ä½¿ç”¨è‡ªç›‘ç£ç›®æ ‡ |
| **Multi-Task-PT[SSL/SL]** | å¤šä»»åŠ¡é¢„è®­ç»ƒï¼Œé‡‡ç”¨äº¤é”™ç›‘ç£ï¼ˆinterleaved supervisionï¼‰ |
| **MAdaPT-Reptile** | ä½¿ç”¨ Reptile å¯å‘å¼çš„çº¯è‡ªç›‘ç£å…ƒå­¦ä¹ å˜ä½“ |
| **MAdaPT-FOBLO** | æœ¬æ–‡æå‡ºçš„å®Œæ•´æ–¹æ³• |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰ABX Discriminabilityï¼ˆå›¾2 & è¡¨5ï¼‰
- åœ¨ä»…ä½¿ç”¨ **1å°æ—¶ç›®æ ‡è¯­è¨€éŸ³é¢‘** çš„æƒ…å†µä¸‹ï¼Œ**MAdaPT-FOBLO** çš„ ABX å¾—åˆ†å³è¿½å¹³ In-Domain PT æ¨¡å‹ã€‚
- æ•°æ®æ•ˆç‡æå‡è¾¾ **100Ã—ä»¥ä¸Š**ã€‚
- åœ¨ **10åˆ†é’Ÿåˆ°100å°æ—¶èŒƒå›´å†…**ï¼ŒMAdaPT-FOBLO å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œå°¤å…¶åœ¨å°æ•°æ®åœºæ™¯ä¸‹å¢ç›Šæ˜¾è‘—ã€‚

#### ï¼ˆ2ï¼‰å£è¯­è¯­è¨€å»ºæ¨¡ï¼ˆTable 2ï¼‰
| æ–¹æ³• | Avg. SLM Score (%) |
|------|---------------------|
| In-Domain Mono-Task-PT | 61.85 |
| Multi-Task-PT[SSL] | 61.07 |
| + MAdaPT-Reptile | 61.36 |
| **+ MAdaPT-FOBLO** | **62.58** |
| Multi-Task-PT[SSL/SL] | 62.49 |
| **+ MAdaPT-FOBLO** | **62.89** âœ…

â†’ MAdaPT-FOBLO ä¸ä»…è¶…è¶Š oracleï¼Œè¿˜åœ¨é›¶æ ·æœ¬ï¼ˆ0hï¼‰å’Œå°‘æ ·æœ¬æ¡ä»¶ä¸‹å®ç°å¿«é€Ÿå¢ç›Šã€‚

#### ï¼ˆ3ï¼‰Phoneme Discovery Benchmarkï¼ˆTable 3ï¼‰
| æ–¹æ³• | PNMI â†‘ | PER â†“ | ABX (within) â†“ | ABX (across) â†“ |
|------|--------|-------|----------------|----------------|
| Multi-Task-PT[SSL] + HuBERT | 0.58 | 76.01 | 6.62 | 7.77 |
| Multi-Task-PT[SSL] + SpidR | 0.66 | 60.17 | 4.83 | 5.72 |
| MAdaPT-Reptile | 0.69 | 38.27 | 4.12 | 4.57 |
| **MAdaPT-FOBLO** | **0.71** | **37.70** | **4.09** | **4.55** |

â†’ SpidR-Adapt æ˜¾è‘—ä¼˜äº HuBERT å’Œ Reptile å˜ä½“ï¼Œè¯æ˜å…¶æ›´å¼ºçš„éŸ³ç´ å‘ç°èƒ½åŠ›ã€‚

---

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰ä¸»åŠ¨é—å¿˜ï¼ˆActive Forgettingï¼‰çš„å½±å“ï¼ˆè¡¨10â€“11ï¼‰
- ç§»é™¤ä¸»åŠ¨é—å¿˜åï¼ŒABX æ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼ˆå¦‚ development set ä¸Šä» 6.40 â†’ 7.00ï¼‰ï¼›
- è¯´æ˜ä¸»åŠ¨é—å¿˜æœ‰æ•ˆç¼“è§£äº†å¯¹æºè¯­è¨€çš„è¿‡æ‹Ÿåˆï¼Œå¢å¼ºäº†æ¨¡å‹å¯å¡‘æ€§ã€‚

#### ï¼ˆ2ï¼‰å…ƒåˆå§‹åŒ–ï¼ˆMeta-Initializationï¼‰çš„å½±å“ï¼ˆè¡¨12â€“13ï¼‰
- ä½¿ç”¨ **random initialization** å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œæ€§èƒ½æå·®ï¼ˆABX >30%ï¼‰ï¼›
- **Multi-Task-PT[SSL/SL]** åˆå§‹åŒ–ä¼˜äºçº¯è‡ªç›‘ç£åˆå§‹åŒ–ï¼›
- ç»“è®ºï¼šé«˜è´¨é‡çš„åˆå§‹åŒ–å¯¹å…ƒè®­ç»ƒè‡³å…³é‡è¦ã€‚

#### ï¼ˆ3ï¼‰å…ƒå­¦ä¹ ç‡ Î² çš„å½±å“ï¼ˆè¡¨14ï¼‰
- Î² = 0.01 æ—¶æ€§èƒ½æœ€ä½³ï¼›
- è¿‡é«˜ï¼ˆå¦‚ Î²=1ï¼‰ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼›
- è¶…å‚é€‰æ‹©åŸºäºå¼€å‘é›†è°ƒä¼˜ï¼Œå¹¶åº”ç”¨äºæµ‹è¯•é›†ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å…ƒå­¦ä¹ æ¡†æ¶å¯æå¤§æå‡è¯­éŸ³æ¨¡å‹çš„æ•°æ®æ•ˆç‡**ï¼šSpidR-Adapt åœ¨ä»…1å°æ—¶ç›®æ ‡è¯­è¨€æ•°æ®ä¸‹å³å¯åŒ¹æ•Œ6000å°æ—¶è®­ç»ƒçš„ä¼ ç»Ÿæ¨¡å‹ã€‚
2. **MAdaPT + FOBLO æ˜¯æœ‰æ•ˆçš„å…ƒå­¦ä¹ èŒƒå¼**ï¼šåŒå±‚ä¼˜åŒ–ç»“æ„ + ä¸€é˜¶è¿‘ä¼¼æ–¹æ¡ˆå®ç°äº†é«˜æ•ˆç¨³å®šçš„å…ƒè®­ç»ƒã€‚
3. **äº¤é”™ç›‘ç£ + ä¸»åŠ¨é—å¿˜ æå‡ç¨³å®šæ€§ä¸æ³›åŒ–æ€§**ï¼šå‰è€…æä¾›é²æ£’åˆå§‹åŒ–ï¼Œåè€…é˜²æ­¢è¯­è¨€ç‰¹å¼‚æ€§çŸ¥è¯†å›ºåŒ–ã€‚
4. **åˆå§‹åŒ–è´¨é‡å†³å®šå…ƒè®­ç»ƒæˆè´¥**ï¼šéšæœºåˆå§‹åŒ–æ— æ³•æ”¶æ•›ï¼Œå¿…é¡»ä¾èµ–è‰¯å¥½çš„é¢„è®­ç»ƒèµ·ç‚¹ã€‚

### å±€é™æ€§
1. **ä¾èµ–æºè¯­è¨€çš„æ ‡æ³¨æ•°æ®**ï¼šå¤–å¾ªç¯ä»éœ€å°‘é‡æ ‡æ³¨è¯­éŸ³ï¼Œé™åˆ¶äº†å®Œå…¨æ— ç›‘ç£æ‰©å±•ï¼›
2. **åˆå§‹åŒ–æ•æ„Ÿ**ï¼šæ€§èƒ½å—å…ƒåˆå§‹åŒ–æ–¹å¼å½±å“è¾ƒå¤§ï¼›
3. **æœªå°†SLMè®­ç»ƒçº³å…¥å…ƒå­¦ä¹ **ï¼šå½“å‰ä»…ä¼˜åŒ–è¯­éŸ³ç¼–ç å™¨ï¼Œä¸‹æ¸¸SLMè®­ç»ƒä»æ˜¯ç‹¬ç«‹è¿‡ç¨‹ï¼Œæœªèƒ½å®ç°ç«¯åˆ°ç«¯æ•°æ®é«˜æ•ˆï¼›
4. **è®¡ç®—èµ„æºéœ€æ±‚è¾ƒé«˜**ï¼šå°½ç®¡FOBLOé™ä½äº†å¤æ‚åº¦ï¼Œä½†å…ƒè®­ç»ƒä»éœ€å¤§é‡episodeå’ŒGPUæ”¯æŒã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ— éœ€ç›‘ç£ä¿¡å·çš„çº¯è‡ªç›‘ç£å…ƒå­¦ä¹ æ–¹æ¡ˆï¼›
- å°†æ•´ä¸ª SLM è®­ç»ƒæµç¨‹çº³å…¥å…ƒå­¦ä¹ æ¡†æ¶ï¼Œå®ç°çœŸæ­£çš„â€œå©´å„¿å¼â€è¯­è¨€ä¹ å¾—æ¨¡æ‹Ÿï¼›
- æ‰©å±•è‡³æ›´å¤šæ¨¡æ€ï¼ˆå¦‚è§†è§‰-å¬è§‰è”åˆå­¦ä¹ ï¼‰ï¼›
- ç ”ç©¶æ›´é«˜æ•ˆçš„å…ƒåˆå§‹åŒ–ç­–ç•¥ï¼Œå‡å°‘å¯¹é¢„è®­ç»ƒçš„ä¾èµ–ã€‚

---

> ğŸ”— **å¼€æºä¿¡æ¯**ï¼šä½œè€…å·²å…¬å¼€è®­ç»ƒä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼š[https://github.com/facebookresearch/spidr-adapt](https://github.com/facebookresearch/spidr-adapt)

</details>

---

### 11. [ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting](https://arxiv.org/abs/2512.21009)

**Authors**: S. M. Shovan, Arindam Khanda, Sanjukta Bhowmick, Sajal K. Das  
**Category**: cs.DC  
**Published**: 2025-12-25  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.21009v1  

#### Abstract
Higher-order interactions beyond pairwise relationships in large complex networks are often modeled as hypergraphs. Analyzing hypergraph properties such as triad counts is essential, as hypergraphs can reveal intricate group interaction patterns that conventional graphs fail to capture. In real-worl...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

- **åŠ¨æ€è¶…å›¾åˆ†æç¼ºä¹é«˜æ•ˆæ•°æ®ç»“æ„**ï¼šä¼ ç»Ÿå›¾æ¨¡å‹ä»…èƒ½è¡¨ç¤ºæˆå¯¹äº¤äº’ï¼ˆdyadic interactionsï¼‰ï¼Œè€Œç°å®ä¸–ç•Œä¸­çš„å¤æ‚ç³»ç»Ÿï¼ˆå¦‚å¤šä½œè€…åˆä½œã€åŒ–å­¦ååº”ï¼‰å¾€å¾€æ¶‰åŠé«˜é˜¶ç¾¤ä½“äº¤äº’ï¼ˆpolyadic interactionsï¼‰ï¼Œéœ€ç”¨ **hypergraph** å»ºæ¨¡ã€‚
- ç°æœ‰è¶…å›¾æ•°æ®ç»“æ„å¤§å¤šä¸ºé™æ€è®¾è®¡ï¼Œéš¾ä»¥æ”¯æŒé«˜æ•ˆçš„ **åŠ¨æ€æ“ä½œ**ï¼ˆå¦‚ hyperedge æ’å…¥/åˆ é™¤ã€é¡¶ç‚¹ä¿®æ”¹ï¼‰ï¼Œä¸”ç¼ºä¹å¯¹ GPU å¹¶è¡Œè®¡ç®—çš„æ”¯æŒã€‚
- è¶…å›¾ä¸Šçš„ **triad counting**ï¼ˆç±»æ¯”äºå›¾ä¸­çš„ä¸‰è§’å½¢è®¡æ•°ï¼‰æ˜¯ç¤¾åŒºæ£€æµ‹ã€æ¨èç³»ç»Ÿç­‰ä»»åŠ¡çš„åŸºç¡€ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯ä¸‹æ•ˆç‡ä½ä¸‹ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æå‡º **ESCHER**ï¼ˆEfficient and Scalable Hypergraph Evolution Representationï¼‰â€”â€”é¦–ä¸ªé¢å‘å¤§è§„æ¨¡åŠ¨æ€è¶…å›¾çš„ **GPU-centric å¹¶è¡Œæ•°æ®ç»“æ„**ï¼Œå¹¶åŸºäºå…¶æ„å»ºäº†é«˜æ•ˆçš„ triad count æ›´æ–°æ¡†æ¶ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š

- âœ… **ç»Ÿä¸€çš„ä¸‰å‘æ˜ å°„æ”¯æŒ**ï¼š
  - æ”¯æŒ `h2v`ï¼ˆhyperedge â†’ incident verticesï¼‰
  - æ”¯æŒ `v2h`ï¼ˆvertex â†’ incident hyperedgesï¼‰
  - æ”¯æŒ `h2h`ï¼ˆhyperedge â†’ neighboring hyperedgesï¼‰
  - å¯çµæ´»æ”¯æŒ bipartite graphã€line graphã€clique graph ç­‰å¤šç§è¡¨ç¤ºå½¢å¼ã€‚

- âœ… **é«˜æ•ˆçš„ GPU å†…å­˜ç®¡ç†æœºåˆ¶**ï¼š
  - ä½¿ç”¨ **é¢„åˆ†é… + åˆ†å—å†…å­˜å—**ï¼ˆmemory blocks of size multiple of 32ï¼‰é¿å…é¢‘ç¹åŠ¨æ€åˆ†é…ã€‚
  - å¼•å…¥ **balanced complete binary tree-based block manager** æ¥è¿½è¸ªç©ºé—²å—ï¼Œå®ç° $O(\log |E|)$ æ—¶é—´å†…æŸ¥æ‰¾å¯ç”¨å†…å­˜ã€‚
  - æ”¯æŒ **å†…å­˜å¤ç”¨** å’Œ **æº¢å‡ºé“¾è¡¨æ‰©å±•**ï¼ˆoverflow chaining via pointersï¼‰ï¼Œæå‡ç©ºé—´åˆ©ç”¨ç‡ã€‚

- âœ… **æ”¯æŒåŒå‘åŠ¨æ€æ›´æ–°**ï¼š
  - **Vertical operations**ï¼šhyperedge æ’å…¥/åˆ é™¤ï¼ˆå½±å“â€œåˆ—â€ï¼‰
  - **Horizontal operations**ï¼šincident vertex æ’å…¥/åˆ é™¤ï¼ˆå½±å“â€œè¡Œâ€ï¼‰
  - æ‰€æœ‰æ“ä½œå‡æ”¯æŒå¹¶è¡ŒåŒ–å¤„ç†ã€‚

- âœ… **é€šç”¨ triad count æ›´æ–°æ¡†æ¶**ï¼š
  - åŸºäºå±€éƒ¨å—å½±å“å­å›¾é‡è®¡ç®— triad æ•°é‡ï¼Œé¿å…å…¨å±€é‡æ„ã€‚
  - æ¡†æ¶å¯é€‚é…å¤šç§ triad ç±»å‹ï¼ˆhyperedge-based, incident-vertex-based, temporalï¼‰ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ€§èƒ½** | åœ¨ triad counting ä¸Šæ¯” MoCHy å¿«è¾¾ **104.5Ã—**ï¼Œæ¯” THyMe+ å¿«è¾¾ **112.5Ã—** |
| **å¯æ‰©å±•æ€§** | æ”¯æŒåƒä¸‡çº§è¶…è¾¹çš„å¤§è§„æ¨¡åŠ¨æ€è¶…å›¾ |
| **é€šç”¨æ€§** | æ”¯æŒå¤šç§ triad ç±»å‹åŠå›¾æ¨¡å‹ç‰¹ä¾‹ï¼ˆå¦‚ triangle countingï¼‰ |
| **ç¡¬ä»¶åˆ©ç”¨** | å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œèƒ½åŠ›ï¼Œå‡å°‘ host-device æ•°æ®ä¼ è¾“ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

| Dataset | Hyperedges | Vertices | Max Cardinality | æ¥æº |
|--------|------------|----------|------------------|------|
| **Coauth** | 2.6M | 1.9M | 280 | å­¦æœ¯åˆè‘—ç½‘ç»œ |
| **Tags** | 5.7M | 50K | 4 | Stack Overflow æ ‡ç­¾ |
| **Orkut** | 6.3M | 3.1M | 27K | ç¤¾äº¤ç½‘ç»œ |
| **Threads** | 9.7M | 2.7M | 67 | åœ¨çº¿è®ºå›è®¨è®º |
| **Random** | 15M | 5M | 10K | åˆæˆéšæœºè¶…å›¾ |

> æ³¨ï¼šæ‰€æœ‰å®éªŒè¿è¡Œäº **NVIDIA A100 GPU (80GB)** + AMD EPYC CPU ä¸»æœºç¯å¢ƒã€‚

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

- **ä»»åŠ¡ç±»å‹**ï¼š
  - åŠ¨æ€ hyperedge ä¿®æ”¹ï¼ˆæ’å…¥/åˆ é™¤ï¼‰
  - åŠ¨æ€ incident vertex ä¿®æ”¹
  - Temporal triad countingï¼ˆæ—¶é—´çª—å£çº¦æŸï¼‰
- **æ‰¹å¤„ç†å¤§å°**ï¼š50Kã€100Kã€200K å˜åŒ–è¶…è¾¹
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - æ‰§è¡Œæ—¶é—´ï¼ˆexecution timeï¼‰
  - åŠ é€Ÿæ¯”ï¼ˆspeedupï¼‰vs åŸºçº¿
  - å†…å­˜ä½¿ç”¨æƒ…å†µ
  - å¯æ‰©å±•æ€§åˆ†æï¼ˆéšæ•°æ®è§„æ¨¡å¢é•¿çš„è¡¨ç°ï¼‰

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| æ–¹æ³• | ç±»å‹ | æ”¯æŒåŠŸèƒ½ |
|------|------|---------|
| **MoCHy [5]** | é™æ€è¶…å›¾ triad countingï¼ˆå…±äº«å†…å­˜ï¼‰ | ä»…æ”¯æŒ hyperedge-based triads |
| **MoCHy (GPU)** | æœ¬æ–‡å®ç°çš„ GPU ç‰ˆæœ¬ MoCHy | ç”¨äºå…¬å¹³æ¯”è¾ƒ |
| **THyMe+ [14]** | æ—¶åºè¶…å›¾ motif è®¡æ•°ï¼ˆå…±äº«å†…å­˜ï¼‰ | æ”¯æŒ temporal triads |
| **THyMe+ (GPU)** | æœ¬æ–‡å®ç°çš„ GPU ç‰ˆ THyMe+ | æ›´å…¬å¹³å¯¹æ¯” |
| **StatHyper [7]** | incident-vertex-based triad ç»Ÿè®¡æ¨æ–­ | æ”¯æŒ vertex-level triads |
| **Hornet [12]** | åŠ¨æ€å›¾ç»“æ„ï¼ˆdyadic graphsï¼‰ | ç”¨äºä¸ä¼ ç»Ÿå›¾æ–¹æ³•å¯¹æ¯” |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table IVï¼‰**

| æ–¹æ³• | å¹³å‡åŠ é€Ÿæ¯”ï¼ˆAvg Speedupï¼‰ | æœ€å¤§åŠ é€Ÿæ¯”ï¼ˆMax Speedupï¼‰ |
|------|----------------------------|----------------------------|
| **MoCHy (shared mem.)** | **37.8Ã—** | **104.5Ã—** |
| **MoCHy (GPU)** | 19.5Ã— | 57.5Ã— |
| **THyMe+ (shared mem.)** | **36.3Ã—** | **112.5Ã—** |
| **THyMe+ (GPU)** | 25Ã— | 57Ã— |
| **StatHyper (GPU)** | 243.2Ã— | **473.7Ã—** |

> âš ï¸ æ³¨æ„ï¼šStatHyper æ˜¯å”¯ä¸€æ”¯æŒ incident-vertex triads çš„åŸºçº¿ï¼Œå› æ­¤å¯¹æ¯”å°¤ä¸ºæ˜¾è‘—ã€‚

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

#### ğŸ”¹ Hyperedge-based Triad Counting
- åœ¨å°æ‰¹é‡æ›´æ–°æ—¶ï¼ŒESCHER æ˜¾è‘—ä¼˜äº MoCHyï¼ˆæœ€é«˜ **104.5Ã—**ï¼‰ã€‚
- å³ä½¿ä¸è‡ªå®ç°çš„ GPU ç‰ˆ MoCHy å¯¹æ¯”ï¼Œä»å–å¾—å¹³å‡ **19.5Ã—** åŠ é€Ÿã€‚
- éšç€å˜æ›´æ•°é‡å¢åŠ ï¼ŒMoCHy å› éœ€å…¨é‡é‡å»ºè€Œè€—æ—¶å‰§å¢ï¼›ESCHER ä»…æ›´æ–°å—å½±å“åŒºåŸŸï¼Œä¿æŒé«˜æ•ˆã€‚

#### ğŸ”¹ Incident-Vertex-based Triad Counting
- é¦–æ¬¡å®ç°è¯¥ç±» triad çš„ **å¹¶è¡ŒåŠ¨æ€æ›´æ–°**ã€‚
- ç›¸æ¯” StatHyper çš„é™æ€é‡ç®—ï¼Œ**å¹³å‡åŠ é€Ÿ 243.2Ã—ï¼Œæœ€é«˜è¾¾ 473.7Ã—**ã€‚
- Type 3 triadsï¼ˆä¸‰ä¸ªé¡¶ç‚¹åˆ†åˆ«å±äºä¸åŒè¶…è¾¹ï¼‰æ”¶ç›Šæœ€å¤§ï¼Œå› å…¶é™æ€æ‰«æä»£ä»·æé«˜ã€‚

#### ğŸ”¹ Temporal Triad Counting
- æ”¯æŒè¿ç»­æ—¶é—´æˆ³å†…çš„ triad æ„å»ºï¼ˆé™å®šæ—¶é—´çª—å£ï¼‰ã€‚
- ç›¸æ¯” THyMe+ï¼Œ**å¹³å‡å¿« 36.3Ã—ï¼Œæœ€é«˜ 112.5Ã—**ã€‚
- å³ä¾¿å¯¹æ¯” GPU ç‰ˆ THyMe+ï¼Œä»å¯è¾¾ **25Ã— å¹³å‡åŠ é€Ÿ**ã€‚

#### ğŸ”¹ Triangle Countingï¼ˆå›¾æ¨¡å‹ç‰¹ä¾‹ï¼‰
- å°† ESCHER åº”ç”¨äºæ™®é€šåŠ¨æ€å›¾ï¼ˆå°†æ¯æ¡è¾¹è§†ä¸ºä¸€ä¸ª 2-node hyperedgeï¼‰ã€‚
- ä¸ Hornetï¼ˆä¸“ä¸ºåŠ¨æ€å›¾è®¾è®¡çš„æ•°æ®ç»“æ„ï¼‰å¯¹æ¯”ï¼š
  - å½“è¾¹é•¿åº¦æ–¹å·®è¾ƒå°æ—¶ï¼ŒHornet æ›´ä¼˜ï¼›
  - å½“è¾¹é•¿åº¦å˜åŒ–å¤§ï¼ˆhigh STDï¼‰ï¼ŒESCHER å› æ›´ä¼˜å†…å­˜ç®¡ç†ç­–ç•¥åè¶…ã€‚

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰**

- **å†…å­˜å—å¤§å°å½±å“**ï¼šé€‰æ‹© 32 çš„å€æ•°èƒ½æ›´å¥½åŒ¹é… GPU warp å¤§å°ï¼Œä¼˜åŒ–è®¿å­˜æ•ˆç‡ã€‚
- **æ ‘å¹³è¡¡ç»´æŠ¤æˆæœ¬ä½**ï¼šç”±äºé‡‡ç”¨å®Œå…¨äºŒå‰æ ‘æ•°ç»„å­˜å‚¨ï¼Œæ— éœ€æ—‹è½¬å³å¯å¿«é€Ÿé‡å»ºã€‚
- **batch processing æ•ˆç›Šæ˜æ˜¾**ï¼šæ‰¹é‡å¤„ç†æ˜¾è‘—é™ä½å•ä½æ“ä½œå¼€é”€ï¼Œæå‡ååç‡ã€‚
- **cardinality å½±å“æ‰§è¡Œæ—¶é—´**ï¼šé«˜åŸºæ•° hyperedge æ›´å®¹æ˜“è§¦å‘ overflow block åˆ†é…ï¼Œç•¥å¾®å¢åŠ å»¶è¿Ÿã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. âœ… **ESCHER æ˜¯é¦–ä¸ªæ”¯æŒå¤§è§„æ¨¡åŠ¨æ€è¶…å›¾åˆ†æçš„ GPU æ•°æ®ç»“æ„**ï¼Œå¡«è¡¥äº†é¢†åŸŸç©ºç™½ã€‚
2. âœ… æå‡ºçš„ **å±€éƒ¨æ›´æ–°ç­–ç•¥** æå¤§å‡å°‘äº†å†—ä½™è®¡ç®—ï¼Œåœ¨ triad counting ä¸Šå®ç°æ•°é‡çº§åŠ é€Ÿã€‚
3. âœ… æ”¯æŒå¤šç§ triad ç±»å‹ï¼ˆhyperedge-, vertex-, temporal-basedï¼‰ï¼Œå±•ç°é«˜åº¦é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚
4. âœ… åœ¨çœŸå®ä¸åˆæˆæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½å¯æ‰©å±•æ€§ï¼Œé€‚ç”¨äºäº¿çº§è¶…è¾¹åœºæ™¯ã€‚
5. âœ… å³ä½¿åº”ç”¨äºä¼ ç»Ÿå›¾ä»»åŠ¡ï¼ˆtriangle countingï¼‰ï¼Œä¹Ÿèƒ½åœ¨ç‰¹å®šæ¡ä»¶ä¸‹åª²ç¾ä¸“ç”¨ç»“æ„ï¼ˆå¦‚ Hornetï¼‰ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**

- âŒ **å†…å­˜å ç”¨è¾ƒé«˜**ï¼šä¸ºæ”¯æŒåŠ¨æ€æ€§å’Œæº¢å‡ºé“¾ï¼Œé¢„åˆ†é…è¾ƒå¤šå†…å­˜ï¼Œç©ºé—´æ•ˆç‡ä½äºç´§å‡‘é™æ€ç»“æ„ã€‚
- âŒ **å½“å‰æœªæ”¯æŒ hyperedge å±æ€§æˆ–æƒé‡**ï¼šä»…å…³æ³¨æ‹“æ‰‘ç»“æ„æ¼”åŒ–ï¼Œæœªè€ƒè™‘å±æ€§åŠ¨æ€å˜åŒ–ã€‚
- âŒ **ä¾èµ– CUDA å¹³å°**ï¼šç›®å‰å®ç°ç»‘å®š NVIDIA GPUï¼Œè·¨å¹³å°ç§»æ¤æ€§å—é™ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**

- ğŸ”® è¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ¢ç´¢æ›´ç´§å‡‘çš„å­˜å‚¨æ–¹æ¡ˆï¼ˆå¦‚å‹ç¼©æŒ‡é’ˆã€ç¨€ç–ç¼–ç ï¼‰ã€‚
- ğŸ”® æ‰©å±•è‡³ **weighted/temporal hyperedges with attributes**ï¼Œæ”¯æŒæ›´å¤æ‚çš„åŠ¨æ€å»ºæ¨¡ã€‚
- ğŸ”® æ¢ç´¢ **åˆ†å¸ƒå¼ç‰ˆæœ¬ ESCHER**ï¼Œä»¥å¤„ç†è¶…å‡ºå•å¡æ˜¾å­˜çš„è¶…å¤§è§„æ¨¡è¶…å›¾ã€‚
- ğŸ”® å°†æ¡†æ¶æ¨å¹¿è‡³å…¶ä»–é«˜é˜¶ motif å‘ç°ã€community detectionã€link prediction ç­‰åº”ç”¨ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **ESCHER é€šè¿‡åˆ›æ–°çš„ GPU å‹å¥½å‹åŠ¨æ€æ•°æ®ç»“æ„ï¼Œå®ç°äº†å¯¹è¶…å›¾ triad counting çš„é«˜æ•ˆã€å¯æ‰©å±•ã€å¤šç±»å‹æ”¯æŒï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸ºé«˜é˜¶ç½‘ç»œåˆ†ææä¾›äº†å¼ºæœ‰åŠ›çš„åŸºç¡€è®¾æ–½æ”¯æ’‘ã€‚**

</details>

---

### 12. [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135)

**Authors**: Zhuo Yang, Yeyun Chen, Jiaqing Xie, Ben Gao, Shuaike Shen, Wanhao Liu, Liujia Yang, Beilun Wang, Tianfan Fu, Yuqiang Li  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.20135v2  

#### Abstract
Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
åˆ†å­ç¼–è¾‘ï¼ˆMolecular Editingï¼‰å’Œåˆ†å­ä¼˜åŒ–ï¼ˆMolecular Optimizationï¼‰æ˜¯è¯ç‰©è®¾è®¡ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸é‡‡ç”¨å•æ­¥ç”Ÿæˆæˆ–é™æ€æŒ‡ä»¤å¾®è°ƒæ–¹å¼å¤„ç†ï¼Œéš¾ä»¥å»ºæ¨¡å¤šæ­¥å†³ç­–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ç¼ºä¹å¯¹åŒ–å­¦æœ‰æ•ˆæ€§ã€ç»“æ„ç›¸ä¼¼æ€§å’Œå¤–éƒ¨åé¦ˆæœºåˆ¶çš„æ˜¾å¼å»ºæ¨¡ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœä¸å¯é æˆ–ä¸å…·å¯è§£é‡Šæ€§ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
æœ¬æ–‡æå‡º **MolAct** â€”â€”ä¸€ç§åŸºäº **Agentic Reinforcement Learningï¼ˆä»£ç†å¼å¼ºåŒ–å­¦ä¹ ï¼‰** çš„æ¡†æ¶ï¼Œå°†åˆ†å­è®¾è®¡å½¢å¼åŒ–ä¸ºä¸€ä¸ª**å¤šæ­¥ã€å·¥å…·å¢å¼ºçš„å†³ç­–è¿‡ç¨‹**ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼**ï¼š
  - **Stage 1ï¼ˆMolEditAgentï¼‰**ï¼šåœ¨åˆ†å­ç¼–è¾‘ä»»åŠ¡ä¸Šé¢„è®­ç»ƒï¼Œå­¦ä¹ åŸºæœ¬æ“ä½œï¼ˆadd/delete/substituteï¼‰ï¼ŒæŒæ¡åŒ–å­¦æœ‰æ•ˆæ€§å’Œç»“æ„ç›¸ä¼¼æ€§æ§åˆ¶ã€‚
  - **Stage 2ï¼ˆMolOptAgentï¼‰**ï¼šåœ¨Stage 1åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒï¼Œä¸“æ³¨äºå±æ€§ä¼˜åŒ–ï¼ˆå¦‚LogPã€solubilityç­‰ï¼‰ï¼Œå¤ç”¨å·²å­¦ç¼–è¾‘è¡Œä¸ºã€‚
- **å·¥å…·è°ƒç”¨æœºåˆ¶ï¼ˆTool-Augmented Decision Makingï¼‰**ï¼š
  - æ¨¡å‹ä½œä¸ºâ€œæ™ºèƒ½ä½“â€å¯ä¸»åŠ¨è°ƒç”¨å¤–éƒ¨åŒ–å­¦å·¥å…·è¿›è¡ŒéªŒè¯ï¼ˆvalidityï¼‰ã€è¯„ä¼°æ€§è´¨ï¼ˆproperty oracleï¼‰å’Œè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆTanimotoï¼‰ï¼Œå¹¶æ ¹æ®åé¦ˆè°ƒæ•´åç»­åŠ¨ä½œã€‚
- **å¤šè½®äº¤äº’å¼æ¨ç†æµç¨‹**ï¼š
  - å®ç° `think â†’ tool call â†’ observation â†’ revise` çš„å¾ªç¯æœºåˆ¶ï¼Œæ¨¡æ‹ŸçœŸå®è¯ç‰©åŒ–å­¦å®¶çš„å·¥ä½œæµã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»ŸLLMæ–¹æ³• | MolAct |
|------|-------------|--------|
| å†³ç­–æ¨¡å¼ | å•æ­¥ç”Ÿæˆï¼ˆone-shotï¼‰ | å¤šæ­¥ã€åºåˆ—åŒ–å†³ç­– |
| åé¦ˆæœºåˆ¶ | æ— æ˜¾å¼åé¦ˆ | æ˜¾å¼å·¥å…·åé¦ˆé—­ç¯ |
| åŒ–å­¦æœ‰æ•ˆæ€§ | ä¾èµ–æ¨¡å‹å†…éšçŸ¥è¯† | å¤–éƒ¨å·¥å…·å¼ºåˆ¶æ ¡éªŒ |
| å¯è§£é‡Šæ€§ | é»‘ç®±è¾“å‡º | ä¸­é—´æ­¥éª¤å¯è§‚æµ‹ |
| æ³›åŒ–èƒ½åŠ› | å›ºå®šç›®æ ‡ä¼˜åŒ– | æ”¯æŒå¤šæ ·åŒ–ä»»åŠ¡ç»„åˆ |

> âœ… **é¦–æ¬¡å°†åˆ†å­è®¾è®¡é—®é¢˜å½¢å¼åŒ–ä¸º Agentic RL é—®é¢˜**ï¼Œå®ç°äº†æ¨ç†ã€å·¥å…·ä½¿ç”¨ä¸ä¼˜åŒ–çš„ç«¯åˆ°ç«¯è”åˆå­¦ä¹ ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- ä½¿ç”¨ **ChemCoTDatasets (Li et al., 2025)** æ„å»ºè®­ç»ƒæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¸¦é“¾å¼æ€è€ƒæ ‡æ³¨çš„å¤šæ­¥åˆ†å­ä¿®æ”¹æ ·æœ¬ã€‚
- ç§»é™¤åŸå§‹CoTæ¨ç†æ–‡æœ¬ï¼Œä»…ä¿ç•™ï¼š
  - è¾“å…¥ï¼šæºåˆ†å­ï¼ˆSMILESï¼‰ + ç¼–è¾‘æŒ‡ä»¤ / ä¼˜åŒ–ç›®æ ‡
  - è¾“å‡ºï¼šç›®æ ‡åˆ†å­ï¼ˆSMILESï¼‰
- æ‰€æœ‰åˆ†å­ç»è¿‡æ ‡å‡†åŒ–ï¼ˆcanonicalizationï¼‰å’Œæœ‰æ•ˆæ€§æ£€æŸ¥ã€‚

### å®éªŒè®¾ç½®
#### æ¨¡å‹æ¶æ„
- åŸºäº **Qwen-2.5ç³»åˆ—LLM** ä½œä¸ºbackboneï¼š
  - MolEditAgent-3B / -7Bï¼šåˆ†åˆ«åŸºäº Qwen-2.5-3B å’Œ Qwen-2.5-7B
  - MolOptAgent-3B / -7Bï¼šç”±å¯¹åº”MolEditAgentåˆå§‹åŒ–åç»§ç»­è®­ç»ƒ
- é‡‡ç”¨ **Group-Relative Policy Optimization (GRPO)** è¿›è¡ŒRLè®­ç»ƒï¼Œä»…å¯¹agentç”Ÿæˆtokenæ›´æ–°æ¢¯åº¦ã€‚

#### å·¥å…·æ¥å£
æ”¯æŒä»¥ä¸‹å·¥å…·è°ƒç”¨ï¼š
- **ç¼–è¾‘æ“ä½œ**ï¼šadd / delete / substitute åŠŸèƒ½å›¢ï¼ˆé™„å¸¦ä½ç½®éªŒè¯ï¼‰
- **è¯„ä¼°å·¥å…·**ï¼š
  - åŒ–å­¦æœ‰æ•ˆæ€§ï¼ˆValidity Checkerï¼‰
  - Murcko Scaffold Tanimotoï¼ˆç»“æ„ä¸€è‡´æ€§ï¼‰
  - å±æ€§é¢„æµ‹å™¨ï¼šLogP, Solubility, QED, DRD2, JNK3, GSK3Î²

#### äº¤äº’æµç¨‹
æ¯æ¡promptå¤åˆ¶æˆKä¸ªå¹¶è¡Œrollouté“¾ï¼Œæ‰§è¡Œå¤šè½®äº¤äº’ç›´è‡³ç»ˆæ­¢æˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼ˆmax_turns=16ï¼‰ï¼š
```
[User Prompt]
â†’ <think> ... </think>
â†’ <tool_call> ... </tool_call>
â†’ <response> ... </response>
â†’ ... ï¼ˆå¾ªç¯ï¼‰
â†’ terminate â†’ è¿”å›æœ€ç»ˆSMILES
```

### è¯„ä¼°æŒ‡æ ‡
| ä»»åŠ¡ç±»å‹ | æŒ‡æ ‡ | å®šä¹‰ |
|--------|------|------|
| åˆ†å­ç¼–è¾‘ | **Pass@1 Accuracy** | æ˜¯å¦æ­£ç¡®å®ŒæˆæŒ‡å®šç¼–è¾‘æ“ä½œ |
|          | **Validity (%)** | ç”Ÿæˆåˆ†å­æ˜¯å¦åŒ–å­¦æœ‰æ•ˆ |
| åˆ†å­ä¼˜åŒ– | **Î” (Mean Improvement)** | å±æ€§æå‡å‡å€¼ï¼ˆè´Ÿå€¼è¡¨ç¤ºé€€åŒ–ï¼‰ |
|          | **SR% (Success Rate)** | æˆåŠŸæå‡å±æ€§çš„æ¯”ä¾‹ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ¶µç›–ä¸¤ç±»ä¸»æµLLMåŸºçº¿ï¼š
- **With Thinkingï¼ˆå…·å¤‡æ¨ç†èƒ½åŠ›ï¼‰**ï¼š
  - Gemini-2.5-pro-think, Claude3.7-sonnet-think, DeepSeek-R1 ç­‰
- **Without Thinkingï¼ˆç›´æ¥ç”Ÿæˆï¼‰**ï¼š
  - GPT-4o, Qwenç³»åˆ—, Llamaç³»åˆ—, BioMedGPTç­‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### åˆ†å­ç¼–è¾‘ä»»åŠ¡è¡¨ç°ï¼ˆTable 1 & 2ï¼‰

| æ¨¡å‹ | Add Acc. | Delete Acc. | Sub Acc. | Add Valid. | Delete Valid. | Sub Valid. |
|------|----------|-------------|----------|------------|----------------|-------------|
| **MolEditAgent-7B** | **90.0%** | **80.0%** | **78.3%** | **100.0%** | **95.0%** | **98.0%** |
| Gemini-2.5-pro-think | 100.0% | 85.0% | 81.7% | â€“ | â€“ | â€“ |
| GPT-4o | 80.0% | 80.0% | 65.0% | ~70% | ~70% | ~65% |
| **MolEditAgent-3B** | 80.0% | 70.0% | 16.7% | 95.0% | 80.0% | 71.7% |

> ğŸ”¹ MolEditAgent-7B åœ¨å¤šæ•°ä»»åŠ¡ä¸­ä»…æ¬¡äºæœ€å¼ºthinkingæ¨¡å‹ï¼Œåœ¨Delete/Subä»»åŠ¡ä¸Šä¼˜äºå¤šæ•°é—­æºæ¨¡å‹  
> ğŸ”¹ **æ‰€æœ‰MolActæ¨¡å‹ä¿æŒæé«˜æœ‰æ•ˆæ€§ï¼ˆ95â€“100%ï¼‰**ï¼Œæ˜¾è‘—ä¼˜äºåŸºç¡€LLMï¼ˆ~65%ï¼‰

### åˆ†å­ä¼˜åŒ–ä»»åŠ¡è¡¨ç°ï¼ˆTable 3ï¼‰

| æ¨¡å‹ | LogP Î” (SR%) | Solubility Î” (SR%) | QED Î” (SR%) | DRD2 Î” (SR%) | GSK3Î² Î” (SR%) |
|------|---------------|--------------------|-------------|--------------|----------------|
| **MolOptAgent-7B** | **0.89 (92%)** | **1.42 (84%)** | 0.04 (35%) | 0.02 (38%) | 0.04 (36%) |
| Gemini-2.5-pro-think | -0.28 (81%) | 1.91 (92%) | 0.21 (84%) | **0.35 (74%)** | 0.04 (68%) |
| Claude3.7-sonnet-think | 0.41 (81%) | 0.59 (77%) | 0.09 (73%) | 0.18 (66%) | 0.01 (57%) |
| DeepSeek-R1 | 0.36 (74%) | 1.48 (97%) | 0.05 (72%) | 0.10 (62%) | -0.02 (41%) |
| **MolOptAgent-3B** | -0.24 (12%) | -0.021 (8%) | -0.017 (5%) | -0.009 (7%) | -0.0026 (10%) |

> âœ… **MolOptAgent-7Båœ¨LogPä¼˜åŒ–ä¸Šå¤§å¹…é¢†å…ˆæ‰€æœ‰åŸºçº¿ï¼ˆÎ”=0.89, SR%=92%ï¼‰**  
> âœ… åœ¨Solubilityä¸Šä»…æ¬¡äºGeminiå’ŒDeepSeek-R1ï¼Œä»å…·ç«äº‰åŠ›  
> âš ï¸ åœ¨QEDã€JNK3ç­‰å¤æ‚ç”Ÿç‰©æ´»æ€§ä»»åŠ¡ä¸Šä»æœ‰æå‡ç©ºé—´

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ä¸¤é˜¶æ®µè®­ç»ƒ vs å•é˜¶æ®µè®­ç»ƒï¼ˆTable 4ï¼‰
| æ¨¡å‹ | LogP SR% | Solubility SR% | QED SR% | DRD2 SR% | GSK3Î² SR% |
|------|---------|----------------|---------|----------|-----------|
| Qwen-2.5-7B-Instructï¼ˆone-stageï¼‰ | 0% | 0% | 12% | 0% | 0% |
| Qwen-2.5-3B-Instructï¼ˆone-stageï¼‰ | 0% | 0% | 0% | 0% | 0% |
| **MolOptAgent-7Bï¼ˆtwo-stageï¼‰** | **92%** | **84%** | **35%** | **38%** | **36%** |
| **MolOptAgent-3Bï¼ˆtwo-stageï¼‰** | **12%** | **8%** | **5%** | **7%** | **10%** |

> â— ç»“è®ºï¼š**æ²¡æœ‰ç¼–è¾‘é¢„è®­ç»ƒçš„one-stageæ–¹æ³•å‡ ä¹å®Œå…¨å¤±è´¥**ï¼Œè¯æ˜ä¸¤é˜¶æ®µè®¾è®¡è‡³å…³é‡è¦ã€‚

#### æ¨¡å‹å®¹é‡å½±å“åˆ†æï¼ˆFigure 4â€“6ï¼‰
- **7Bæ¨¡å‹**ï¼šå“åº”é•¿åº¦ç¨³å®šï¼Œå·¥å…·è°ƒç”¨é«˜æ•ˆï¼ŒæˆåŠŸæ‰§è¡Œç­–ç•¥
- **3Bæ¨¡å‹**ï¼šå‡ºç°æç«¯é•¿å“åº”ï¼ˆæœ€é«˜è¾¾5000 tokensï¼‰ï¼Œè¡¨æ˜æ¨ç†å†—ä½™ã€å·¥å…·è°ƒåº¦æ··ä¹±
- å°½ç®¡rewardç›¸è¿‘ï¼Œä½†**å°æ¨¡å‹æ— æ³•åœ¨æœ‰é™turn budgetå†…å®Œæˆæœ‰æ•ˆä¼˜åŒ–**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **å¤šæ­¥ã€å·¥å…·å¢å¼ºçš„å†³ç­–æœºåˆ¶æ˜¾è‘—æå‡åˆ†å­ç¼–è¾‘ä¸ä¼˜åŒ–çš„å¯é æ€§ä¸æœ‰æ•ˆæ€§**
2. âœ… **ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆå…ˆç¼–è¾‘åä¼˜åŒ–ï¼‰æ˜¯æˆåŠŸçš„å…³é”®**ï¼šç¼–è¾‘é¢„è®­ç»ƒæ•™ä¼šæ¨¡å‹å¦‚ä½•æ­£ç¡®ä½¿ç”¨å·¥å…·å’Œç»ˆæ­¢æ¡ä»¶
3. âœ… **å¤§æ¨¡å‹ï¼ˆ7BåŠä»¥ä¸Šï¼‰æ›´èƒ½å°†å­¦åˆ°çš„rewardè½¬åŒ–ä¸ºå¯æ‰§è¡Œç­–ç•¥**ï¼Œå°æ¨¡å‹å­˜åœ¨â€œå¥–åŠ±å¹»è§‰â€
4. âœ… **é«˜åŒ–å­¦æœ‰æ•ˆæ€§ï¼ˆ95â€“100%ï¼‰å¾—ç›Šäºå¤–éƒ¨å·¥å…·åé¦ˆé—­ç¯**ï¼Œé¿å…æ— æ•ˆç»“æ„ç”Ÿæˆ
5. âœ… MolActåœ¨LogPç­‰ç‰©ç†åŒ–å­¦å±æ€§ä¼˜åŒ–ä¸Šè¶…è¶Šå½“å‰æœ€å¼ºé—­æºthinkingæ¨¡å‹ï¼ˆå¦‚Claude 3.7ï¼‰

### æ–¹æ³•å±€é™æ€§
1. âŒ **å¯¹æŸäº›ç‰¹å®šé¶ç‚¹ï¼ˆå¦‚JNK3ï¼‰ä¼˜åŒ–æ•ˆæœä¸€èˆ¬**ï¼šå¯èƒ½å› backboneç¼ºä¹é¢†åŸŸä¸“ä¸šçŸ¥è¯†
2. âŒ **æœªè€ƒè™‘åˆæˆå¯è¡Œæ€§ï¼ˆsynthetic feasibilityï¼‰**ï¼šç”Ÿæˆåˆ†å­è™½æœ‰æ•ˆä½†æœªå¿…å¯åˆæˆ
3. âŒ **å°æ¨¡å‹ï¼ˆ3Bï¼‰æ‰§è¡ŒåŠ›å·®**ï¼šå³ä½¿rewardé«˜ä¹Ÿéš¾ä»¥åœ¨é¢„ç®—å†…å®Œæˆä»»åŠ¡
4. âŒ **ä¾èµ–é«˜è´¨é‡å·¥å…·API**ï¼šè‹¥å·¥å…·ä¸å‡†æˆ–å»¶è¿Ÿé«˜ï¼Œä¼šå½±å“æ•´ä½“æ€§èƒ½
5. âŒ å½“å‰æ¡†æ¶å‡è®¾ç¼–è¾‘æŠ€èƒ½å¯è¿ç§»åˆ°ä¼˜åŒ–ä»»åŠ¡ï¼Œä½†å¹¶éæ‰€æœ‰ä»»åŠ¡éƒ½é€‚ç”¨æ­¤è¿ç§»

### æœªæ¥å·¥ä½œæ–¹å‘
1. ğŸ”„ **å¼•å…¥ååº”è·¯å¾„ä¸å¯åˆæˆæ€§çº¦æŸ**ï¼šç»“åˆReaction Prediction Toolsï¼Œç¡®ä¿ç”Ÿæˆåˆ†å­å¯å®é™…åˆæˆ
2. ğŸ§  **å¢å¼ºé”™è¯¯æ¢å¤æœºåˆ¶**ï¼šå½“å·¥å…·è¿”å›å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨å›æº¯æˆ–ä¿®æ­£ç­–ç•¥
3. ğŸ“š **æ¢ç´¢æ›´å¤æ‚çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥**ï¼šè¶…è¶Šä¸¤é˜¶æ®µï¼Œæ„å»ºæ¸è¿›å¼ä»»åŠ¡éš¾åº¦æ›²çº¿
4. ğŸ¯ **å¼€å‘é¢†åŸŸå®šåˆ¶åŒ–å·¥å…·åŒ…ä¸çŸ¥è¯†æ³¨å…¥æœºåˆ¶**ï¼šæå‡å¯¹ç‰¹å®šè›‹ç™½é¶ç‚¹çš„ä¼˜åŒ–èƒ½åŠ›
5. ğŸ“Š **å»ºç«‹â€œç­–ç•¥å¯æ‰§è¡Œæ€§â€è¯„ä¼°æ ‡å‡†**ï¼šä¸ä»…çœ‹rewardï¼Œè¿˜è¦è¡¡é‡æ˜¯å¦èƒ½åœ¨budgetå†…è¾¾æˆç›®æ ‡
6. ğŸ” **é›†æˆæ›´å¤šRLç®—æ³•ä¸ç¨€ç–å¥–åŠ±è®¾è®¡**ï¼šæé«˜æ ·æœ¬æ•ˆç‡ä¸è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›

---

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> MolAct é¦–æ¬¡å°†åˆ†å­è®¾è®¡æ„å»ºæˆ **Agentic RL æ¡†æ¶**ï¼Œé€šè¿‡ **ä¸¤é˜¶æ®µè®­ç»ƒ + å·¥å…·å¢å¼ºäº¤äº’**ï¼Œå®ç°äº†é«˜æœ‰æ•ˆæ€§ã€å¯è§£é‡Šä¸”æ€§èƒ½é¢†å…ˆçš„åˆ†å­ç¼–è¾‘ä¸ä¼˜åŒ–ç³»ç»Ÿï¼Œä¸ºAIé©±åŠ¨è¯ç‰©å‘ç°æä¾›äº†æ–°çš„èŒƒå¼ã€‚

</details>

---

### 13. [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)

**Authors**: Hamed Firooz, Rui Liu, Yuchen Lu, Zhenyu Hou, Fangzhou Xiong, Xiaoyang Zhang, Changshu Jian, Zhicheng Zhu, Jiayuan Ma, Jacob Tao, Chaitali Gupta, Xiaochang Peng, Shike Mei, Hang Cui, Yang Qin, Shuo Tang, Jason Gaedtke, Arpit Mittal  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.20061v1  

#### Abstract
Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential f...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Scaling Reinforcement Learning for Content Moderation with Large Language Models*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

æœ¬æ–‡èšç„¦äºå¤§è§„æ¨¡ **content moderationï¼ˆå†…å®¹å®¡æ ¸ï¼‰** ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼š  
- **æ ‡ç­¾ç¨€ç¼ºæ€§ï¼ˆlabel sparsityï¼‰**ï¼šé«˜è´¨é‡ä¸“å®¶æ ‡æ³¨æˆæœ¬é«˜ã€è·å–æ…¢ï¼›  
- **æ”¿ç­–å¤æ‚æ€§**ï¼šçœŸå®ä¸–ç•Œçš„å†…å®¹æ”¿ç­–å…·æœ‰å±‚æ¬¡åŒ–ã€æ¡ä»¶ä¾èµ–ã€è¯­å¢ƒæ•æ„Ÿç­‰ç‰¹ç‚¹ï¼Œéš¾ä»¥é€šè¿‡ç®€å•çš„ç›‘ç£å­¦ä¹ ï¼ˆSFTï¼‰å®Œå…¨æ•æ‰ï¼›  
- **æ¨ç†æ·±åº¦éœ€æ±‚**ï¼šéœ€è¦æ¨¡å‹è¿›è¡Œå¤šæ­¥ã€ç»†ç²’åº¦ã€åŸºäºæ”¿ç­–çš„ **nuanced reasoningï¼ˆç»†è‡´æ¨ç†ï¼‰**ï¼Œè€Œéæµ…å±‚æ¨¡å¼åŒ¹é…ã€‚

å°½ç®¡ LLM åœ¨å†…å®¹å®¡æ ¸ä¸­å·²æœ‰åº”ç”¨ï¼ˆå¦‚ prompting æˆ– SFTï¼‰ï¼Œä½†åœ¨ **expert-level å‡†ç¡®ç‡** å’Œ **å·¥ä¸šçº§å¯æ‰©å±•æ€§** ä¸Šä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è€Œ **Reinforcement Learningï¼ˆRLï¼‰** è™½åœ¨ LLM å¯¹é½é¢†åŸŸæˆåŠŸï¼Œå´å°šæœªç³»ç»Ÿåº”ç”¨äºå¤§è§„æ¨¡å†…å®¹å®¡æ ¸åœºæ™¯ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡ºäº†ä¸€å¥—å®Œæ•´çš„ **scaling RL for content moderation çš„å®è¯æ¡†æ¶**ï¼Œæ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

1. **æ··åˆå¥–åŠ±å¡‘é€ ç­–ç•¥ï¼ˆHybrid Reward Shapingï¼‰**ï¼š
   - ç»“åˆ **verifiable rewardsï¼ˆå¯éªŒè¯å¥–åŠ±ï¼‰**ã€**rubric-based evaluatorsï¼ˆåŸºäºè¯„åˆ†æ ‡å‡†çš„è¯„ä¼°å™¨ï¼‰** å’Œ **LLM-as-judge æ¡†æ¶**ï¼Œæ„å»ºæ›´ä¸°å¯Œã€æŠ— reward hacking çš„åé¦ˆä¿¡å·ã€‚
   - æå‡º **Rubric-Based Reasoning Reward (Rrub)**ï¼Œå¯¹æ•´ä¸ª reasoning chain è¿›è¡Œè´¨é‡è¯„ä¼°ï¼Œè¶…è¶Šä»…å…³æ³¨æœ€ç»ˆæ ‡ç­¾çš„ç›‘ç£æ–¹å¼ã€‚

2. **ç¼“è§£ RL å¤±è´¥æ¨¡å¼çš„æœ‰æ•ˆç­–ç•¥**ï¼š
   - é’ˆå¯¹ **reward hacking / length collapseï¼ˆé•¿åº¦åç¼©ï¼‰**ï¼šå¼•å…¥ **Targeted Reasoning Length Reward (Rlen)** å¼•å¯¼æ¨¡å‹ç”Ÿæˆå……åˆ†æ¨ç†ã€‚
   - é’ˆå¯¹ **bimodal probability distributionï¼ˆåŒå³°æ¦‚ç‡åˆ†å¸ƒï¼‰**ï¼šé‡‡ç”¨ **Monte-Carlo é‡‡æ ·** å’Œ **Reflection-Aided Prompting** æ”¹å–„ç½®ä¿¡åº¦æ ¡å‡†ã€‚
   - é’ˆå¯¹ **factuality vs. faithfulness trade-offï¼ˆäº‹å®æ€§ä¸å¿ å®æ€§æƒè¡¡ï¼‰**ï¼šé€šè¿‡ rubric å¥–åŠ±æå‡æ¨ç†çš„äº‹å®åŸºç¡€ã€‚

3. **Disagreement Filtering æ•°æ®ç­›é€‰æœºåˆ¶**ï¼š
   - åˆ©ç”¨æ¨¡å‹è‡ªæ´½æ€§ï¼ˆself-consistencyï¼‰è¯†åˆ«â€œæœ‰å­¦ä¹ ä»·å€¼â€çš„æ ·æœ¬ï¼ˆå³é¢„æµ‹ä¸ä¸€è‡´çš„æ ·æœ¬ï¼‰ï¼Œæ˜¾è‘—æå‡ RL çš„æ•°æ®æ•ˆç‡ã€‚

4. **å·¥ä¸šçº§ RL è®­ç»ƒé…æ–¹ï¼ˆPractical Training Recipeï¼‰**ï¼š
   - æ¨èä½¿ç”¨ **Group Relative Policy Optimization (GRPO)** æ›¿ä»£ PPOï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ä¸è®¡ç®—æ•ˆç‡ï¼›
   - å¼ºè°ƒ **sequence-level reward normalization** å’Œè¶³å¤Ÿå¤§çš„ **effective batch sizeï¼ˆâ‰¥1024ï¼‰**ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ•°æ®æ•ˆç‡** | RL è¾¾åˆ°ä¸ SFT ç›¸å½“ç”šè‡³æ›´ä¼˜æ€§èƒ½æ‰€éœ€æ•°æ®é‡å°‘ **10â€“100Ã—**ï¼Œå°¤å…¶é€‚åˆæ ‡æ³¨ç¨€ç¼ºåœºæ™¯ |
| **æ¨ç†èƒ½åŠ›** | æ˜¾è‘—æå‡éœ€å¤æ‚æ”¿ç­–æ¨ç†çš„ä»»åŠ¡è¡¨ç°ï¼Œä¼˜äºä»…é  SFT çš„æ¨¡å‹ |
| **è®­ç»ƒç¨³å®šæ€§** | é€šè¿‡ reward shaping å’Œ MC é‡‡æ ·ç­‰æ‰‹æ®µæœ‰æ•ˆæŠ‘åˆ¶ reward hacking å’Œè¾“å‡ºé€€åŒ– |
| **å¯æ‰©å±•æ€§** | æ­ç¤º RL çš„ **sigmoid-like scaling behavior**ï¼Œä¸ºå·¥ä¸šéƒ¨ç½²æä¾›èµ„æºåˆ†é…æŒ‡å¯¼ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- æ¥æºäº **Meta Platforms, Inc.** çš„ä¸‰ä¸ªçœŸå®ç”Ÿäº§ç¯å¢ƒä¸­çš„ **policy-violation classification ä»»åŠ¡**ï¼ˆTask1, Task2, Task3ï¼‰ï¼š
  - å‡ä¸ºäºŒåˆ†ç±»ä»»åŠ¡ï¼ˆæ˜¯å¦è¿åæŸé¡¹å†…å®¹æ”¿ç­–ï¼‰ï¼›
  - è¾“å…¥åŒ…å«æ–‡æœ¬ã€è´¦æˆ·ä¿¡æ¯ã€ä¸Šä¸‹æ–‡ç­‰å¤šæ¨¡æ€å†…å®¹ï¼ˆ[CONTENT_MULTIMODAL]ï¼‰ï¼›
  - æ•°æ®å…·æœ‰å…¸å‹å·¥ä¸šç‰¹å¾ï¼šæ ‡ç­¾ç¨€ç–ã€æ”¿ç­–å®šä¹‰å¤æ‚ã€éœ€é“¾å¼æ¨ç†ã€‚

> æ³¨ï¼šå…·ä½“å…¬å¼€æ•°æ®é›†æœªå‘½åï¼Œä½†éƒ¨åˆ†å®éªŒä¹Ÿåœ¨å¤–éƒ¨åŸºå‡† **GSM8K** ä¸ŠéªŒè¯è®­ç»ƒæ•ˆç‡ã€‚

### **å®éªŒè®¾ç½®**

- **æ¨¡å‹æ¶æ„**ï¼šä¸»è¦ä½¿ç”¨ **Qwen2.5-VL-7B** å’Œ **Qwen-3-8B** ç­‰å¤§è¯­è¨€æ¨¡å‹ï¼›
- **RL ç®—æ³•**ï¼šé‡‡ç”¨ **Group Relative Policy Optimization (GRPO)**ï¼Œæ— éœ€æ˜¾å¼ value networkï¼Œè®¡ç®—æ›´é«˜æ•ˆï¼›
- **KL æ§åˆ¶**ï¼šè®¾ Î² = 0ï¼Œå…è®¸æ›´å¼ºæ¢ç´¢ï¼›
- **å¥–åŠ±å½’ä¸€åŒ–**ï¼šä½¿ç”¨ **sequence-level normalization** æå‡ç¨³å®šæ€§ï¼›
- **è®­ç»ƒæ¡†æ¶å¯¹æ¯”**ï¼šæ¯”è¾ƒ **HuggingFace TRL** ä¸ **VeRL** çš„ååé‡ã€‚

### **è¯„ä¼°æŒ‡æ ‡**

| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **R@P90** | Recall at Precision â‰¥ 90%ï¼Œè¡¡é‡é«˜ç²¾åº¦ä¸‹çš„å¬å›èƒ½åŠ›ï¼Œè´´è¿‘å®é™…ä¸šåŠ¡éœ€æ±‚ |
| **PRAUC** | Precision-Recall Area Under Curveï¼Œç»¼åˆè¯„ä¼°åˆ†ç±»æ€§èƒ½ |
| **F1 Score** | å¹³è¡¡ precision ä¸ recall çš„å¸¸ç”¨æŒ‡æ ‡ |
| **Faithfulness / Factuality** | ä½¿ç”¨ **Gemini-2.5-Pro** å’Œ **HHEM** ä½œä¸º LLM Judge è¯„ä¼°æ¨ç†å¿ å®æ€§ä¸äº‹å®æ€§ |
| **Throughput (tokens/s/GPU)** | è¡¡é‡è®­ç»ƒæ•ˆç‡ |

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| åŸºçº¿ | æè¿° |
|------|------|
| **SFT-COT** | åŸºäº Chain-of-Thought çš„ç›‘ç£å¾®è°ƒ |
| **Zero-shot** | æ— å¾®è°ƒç›´æ¥æ¨ç† |
| **RL-Only** | ç›´æ¥åœ¨ base model ä¸Šåº”ç”¨ RL |
| **SFT â†’ RL** | å…ˆ SFT å† RL çš„ä¸¤é˜¶æ®µè®­ç»ƒ |
| **maj@N / pass@N** | å¤š rollout ä¸‹çš„å¤šæ•°æ­£ç¡®ç‡ä¸è‡³å°‘ä¸€ä¸ªæ­£ç¡®çš„æ¦‚ç‡ï¼Œç”¨äºåˆ†æ RL æ˜¯å¦æå‡å“åº”é€‰æ‹©èƒ½åŠ› |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… æ•°æ®æ•ˆç‡ä¼˜åŠ¿ï¼ˆFigure 6ï¼‰
- **RL-Only** åœ¨ä»…ä½¿ç”¨ **æ•°ç™¾ä¸ªæ ·æœ¬** è®­ç»ƒæ—¶ï¼Œå³å¯åŒ¹æ•Œç”šè‡³è¶…è¿‡ **SFT æ¨¡å‹åœ¨æ•°ä¸‡æ ·æœ¬ä¸Šè®­ç»ƒçš„ç»“æœ**ã€‚
- åœ¨ Task3 ä¸­ï¼ŒRL ä»…ç”¨ **200 ä¸ªæ ·æœ¬** å³è¾¾åˆ° SFT åœ¨ 60K æ ·æœ¬ä¸Šçš„æ€§èƒ½æ°´å¹³ â†’ **çº¦ 100Ã— æ•°æ®æ•ˆç‡æå‡**ã€‚

#### âœ… RL ç¼©æ”¾è¡Œä¸ºï¼ˆFigures 7 & 8ï¼‰
- RL è¡¨ç°éµå¾ª **sigmoid-like scaling curve**ï¼š
  - æ€§èƒ½éš **è®­ç»ƒ token æ•°é‡**ã€**rollout æ•°é‡**ã€**ä¼˜åŒ–æ­¥æ•°** å¹³æ»‘ä¸Šå‡ï¼Œéšåè¶‹äºé¥±å’Œï¼›
  - å­˜åœ¨æ˜æ˜¾æ”¶ç›Šé€’å‡åŒºï¼Œä¸ºèµ„æºåˆ†é…æä¾›ä¾æ®ã€‚

#### âœ… æœ‰æ•ˆæ‰¹å¤§å°å½±å“ï¼ˆTable 7ï¼‰
| Effective Batch Size | Task1 R@P90 |
|-----------------------|-------------|
| 128                   | 0.18        |
| 1024                  | 0.81        |
| 2048+                 | ~0.85       |
> æ¨èæœ€å° effective batch size ä¸º **1024** ä»¥ä¿è¯ç¨³å®šè®­ç»ƒã€‚

#### âœ… å¥–åŠ±å¡‘é€ æ¶ˆèå®éªŒï¼ˆTable 6ï¼‰
åœ¨ Task1 ä¸Šä½¿ç”¨ Qwen-38B æ¨¡å‹ï¼š

| Reward Setup | F1 |
|--------------|-----|
| Racc + Rfmt (Baseline) | 0.49 |
| + Rlen | 0.49 â†’ æ— æ˜¾è‘—æå‡ |
| + Racc + Rfmt + Rlen + Rrub | **0.61** (+12%) |

> **Rubric-based reward æ˜¯æœ€å¤§å¢ç›Šæ¥æº**ï¼Œè¯æ˜å…¶å¯¹æå‡æ¨ç†è´¨é‡è‡³å…³é‡è¦ã€‚

#### âœ… åæ€æç¤ºï¼ˆReflection-Aided Promptingï¼‰æ•ˆæœï¼ˆTable 4ï¼‰
| Scoring Method | PRAUC | R@P90 |
|----------------|--------|--------|
| Without Reflection | 0.77 | 0.05 |
| With Reflection | **0.89** | **0.59** |

> åæ€æœºåˆ¶æå¤§æ”¹å–„ç½®ä¿¡åº¦åˆ†å¸ƒï¼Œé¿å…æç«¯æåŒ–ã€‚

#### âœ… Disagreement Filtering æ•ˆæœï¼ˆTable 8ï¼‰
| æ•°æ®å­é›† | æ•°æ®é‡ | F1 | PRAUC |
|----------|--------|-----|--------|
| All | 677 | 0.87 | 0.85 |
| Disagreement Only | **61** | **0.88** | **0.90** |

> ä»…ç”¨ **61 ä¸ª disagreement æ ·æœ¬** è®­ç»ƒ RLï¼Œæ€§èƒ½åè¶…å…¨é‡æ•°æ® â†’ **æè‡´æ•°æ®æ•ˆç‡ + æ›´å¼ºæ³›åŒ–**ã€‚

#### âœ… è®­ç»ƒæ¡†æ¶æ•ˆç‡å¯¹æ¯”ï¼ˆTable 1ï¼‰
| Dataset | Model | VeRL (tokens/s/GPU) | TRL | Speedup |
|--------|--------|----------------------|------|---------|
| Task2 | Qwen2.5-VL-7B | **4600** | 1854 | **2.5Ã—** |
| GSM8K | Qwen2.5-VL-7B | **1500** | 730 | **2.0Ã—** |

> **VeRL æ˜¾è‘—ä¼˜äº TRL**ï¼Œæ›´é€‚åˆå¤§è§„æ¨¡ RL è®­ç»ƒã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. âœ… **RL åœ¨å†…å®¹å®¡æ ¸ä¸­å…·å¤‡æé«˜æ•°æ®æ•ˆç‡**ï¼šç›¸æ¯” SFT å¯å®ç° **10â€“100Ã— çš„æ•°æ®èŠ‚çœ**ï¼Œç‰¹åˆ«é€‚ç”¨äºä¸“å®¶æ ‡æ³¨æ˜‚è´µçš„åœºæ™¯ã€‚
2. âœ… **RL å±•ç°å‡ºå¯é¢„æµ‹çš„ç¼©æ”¾è§„å¾‹**ï¼šæ€§èƒ½éšæ•°æ®ã€rolloutã€compute å‘ˆ **sigmoid-like å¢é•¿**ï¼Œä¸ºå·¥ä¸šéƒ¨ç½²æä¾›è“å›¾ã€‚
3. âœ… **çº¯ accuracy reward ä¸è¶³**ï¼šæ˜“å¯¼è‡´ **reward hacking** å’Œ **reasoning collapse**ï¼Œå¿…é¡»ç»“åˆ **rubric-based reward** æ‰èƒ½å¼•å¯¼é«˜è´¨é‡æ¨ç†ã€‚
4. âœ… **RL æ›´åƒæ˜¯ response selector è€Œéèƒ½åŠ›å¢å¼ºå™¨**ï¼šå…¶æå‡ä¸»è¦æ¥è‡ªæé«˜ä»å¤šä¸ª rollout ä¸­é€‰å‡ºæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ï¼ˆpass@N æå‡ï¼‰ï¼Œè€Œéæ ¹æœ¬æ”¹å˜ base model çš„æ¨ç†èƒ½åŠ›ã€‚
5. âœ… **Disagreement Filtering æ˜¯é«˜æ•ˆæ•°æ®åˆ©ç”¨åˆ©å™¨**ï¼šè‡ªåŠ¨ç­›é€‰â€œéš¾ä¸”å¯å­¦â€æ ·æœ¬ï¼Œè¿›ä¸€æ­¥æ”¾å¤§ RL çš„æ•°æ®ä¼˜åŠ¿ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**

- **ä¾èµ–é«˜è´¨é‡å¥–åŠ±è®¾è®¡**ï¼šrubric æ„å»ºæœ¬èº«éœ€è¦é¢†åŸŸä¸“å®¶å‚ä¸ï¼Œå¯èƒ½æˆä¸ºç“¶é¢ˆï¼›
- **LLM-as-Judge å¼•å…¥é¢å¤–ä¸ç¡®å®šæ€§**ï¼šjudge æ¨¡å‹è‡ªèº«å¯èƒ½å­˜åœ¨åè§æˆ–é”™è¯¯ï¼›
- **compute æˆæœ¬è¾ƒé«˜**ï¼šå°½ç®¡ VeRL æå‡æ•ˆç‡ï¼Œä½† RL ä»è¿œé«˜äº SFT çš„è®¡ç®—å¼€é”€ï¼›
- **æ³›åŒ–æ€§å¾…éªŒè¯**ï¼šå½“å‰å®éªŒé›†ä¸­äº Meta å†…éƒ¨ä»»åŠ¡ï¼Œè·¨å¹³å°/è·¨æ”¿ç­–è¿ç§»èƒ½åŠ›æœªçŸ¥ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**

- æ¢ç´¢ **automated rubric generation** æˆ– **self-improving reward models**ï¼›
- ç ”ç©¶ **multi-task RL** æ¡†æ¶ä»¥å…±äº« policy knowledgeï¼›
- å°†è¯¥èŒƒå¼æ‰©å±•è‡³ **multimodal content moderation**ï¼ˆå›¾åƒ+æ–‡æœ¬è”åˆå®¡æ ¸ï¼‰ï¼›
- å¼€å‘æ›´è½»é‡åŒ–çš„ RL ç®—æ³•ï¼Œé™ä½éƒ¨ç½²é—¨æ§›ï¼›
- æ¢ç´¢ **human-in-the-loop RL**ï¼Œå®ç°æŒç»­è¿­ä»£ä¸æ”¿ç­–é€‚åº”ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡ç³»ç»Ÿè®ºè¯äº† **Reinforcement Learning** æ˜¯é€šå¾€ **é«˜æ•°æ®æ•ˆç‡ã€ä¸“å®¶çº§ content moderation ç³»ç»Ÿ** çš„å¯è¡Œè·¯å¾„ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ **reward shaping** å’Œ **training recipe**ï¼Œå¯åœ¨çœŸå®å·¥ä¸šåœºæ™¯ä¸­æ˜¾è‘—è¶…è¶Šä¼ ç»Ÿ SFT æ–¹æ³•ã€‚

</details>

---

### 14. [Neural Probe-Based Hallucination Detection for Large Language Models](https://arxiv.org/abs/2512.20949)

**Authors**: Shize Liang, Hongzhi Wang  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.20949v1  

#### Abstract
Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Neural Probe-Based Hallucination Detection for Large Language Models è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆå’ŒçŸ¥è¯†é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“äº§ç”Ÿ**å¹»è§‰ï¼ˆhallucinationï¼‰**â€”â€”å³ç”Ÿæˆäº‹å®é”™è¯¯æˆ–ç¼ºä¹è¯æ®æ”¯æŒçš„å†…å®¹ã€‚è¿™åœ¨åŒ»ç–—ã€æ³•å¾‹ç­‰é«˜é£é™©é¢†åŸŸå°¤ä¸ºå±é™©ã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- **åŸºäºä¸ç¡®å®šæ€§çš„æ–¹æ³•**ï¼ˆå¦‚ perplexityã€semantic entropyï¼‰ï¼šå¸¸å‡ºç°â€œé«˜ç½®ä¿¡åº¦é”™è¯¯â€ã€‚
- **åŸºäºæ£€ç´¢çš„æ–¹æ³•**ï¼ˆretrieval-basedï¼‰ï¼šä¾èµ–å¤–éƒ¨çŸ¥è¯†åº“è¦†ç›–åº¦å’Œæ£€ç´¢æ•ˆç‡ï¼Œå®æ—¶æ€§å·®ã€‚
- **ä¼ ç»Ÿçº¿æ€§æ¢é’ˆï¼ˆlinear probesï¼‰**ï¼šåªèƒ½æ•æ‰æµ…å±‚è¯­ä¹‰æ¨¡å¼ï¼Œéš¾ä»¥å»ºæ¨¡æ·±å±‚éšè—çŠ¶æ€ä¸­çš„éçº¿æ€§å¹»è§‰ç‰¹å¾ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºä¸€ç§**åŸºäºç¥ç»ç½‘ç»œæ¢é’ˆçš„ token-level å¹»è§‰æ£€æµ‹æ¡†æ¶**ï¼Œæ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

- **è½»é‡çº§ MLP æ¢é’ˆæ¶æ„**ï¼šå†»ç»“ LLM å‚æ•°ï¼Œåœ¨ä¸­é—´å±‚æ’å…¥å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä½œä¸ºéçº¿æ€§æ¢é’ˆï¼Œå¯¹é«˜ç»´éšè—çŠ¶æ€è¿›è¡Œåˆ†å±‚å»ºæ¨¡ï¼Œæœ‰æ•ˆæ•è· token-level çš„å¤æ‚éçº¿æ€§å¹»è§‰æ¨¡å¼ã€‚
- **å¤šç›®æ ‡è”åˆæŸå¤±å‡½æ•°ï¼ˆmulti-objective joint lossï¼‰**ï¼š
  - ç»“åˆ `token-level focal loss`ï¼ˆç¼“è§£ç±»åˆ«ä¸å¹³è¡¡ï¼‰
  - `soft span aggregation`ï¼ˆå¢å¼ºè¿ç»­å¹»è§‰å®ä½“çš„ä¸€è‡´æ€§ï¼‰
  - `sparsity regularization`ï¼ˆé˜²æ­¢è¾“å‡ºè¿‡åº¦æ¿€æ´»ï¼‰
  - `KL-divergence constraint`ï¼ˆä¿æŒåŸå§‹è¯­è¨€æ¨¡å‹åˆ†å¸ƒç¨³å®šæ€§ï¼‰
- **å±‚ä½ç½®-æ¢é’ˆæ€§èƒ½å“åº”æ¨¡å‹ + è´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨æœç´¢æœ€ä¼˜æ’å…¥å±‚**ï¼š
  - æ„å»ºæ•°å­¦æ¨¡å‹æè¿°ä¸åŒ Transformer å±‚çš„ç‰¹å¾å¯åˆ†æ€§ä¸æ¢é’ˆæ€§èƒ½çš„å…³ç³»ã€‚
  - ä½¿ç”¨ **Bayesian Optimization** è‡ªåŠ¨å¯»æ‰¾æœ€ä½³æ¢é’ˆæ’å…¥å±‚ï¼Œæ›¿ä»£äººå·¥ç»éªŒé€‰æ‹©ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **å‡†ç¡®æ€§** | æ˜¾è‘—ä¼˜äºä¸ç¡®å®šæ€§ä¼°è®¡å’Œçº¿æ€§æ¢é’ˆï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæå‡ AUC å’Œ Recall |
| **å®æ—¶æ€§** | æ¢é’ˆè½»é‡ä¸”æ— éœ€å¤–éƒ¨æ£€ç´¢ï¼Œé€‚åˆåœ¨çº¿æ¨ç†é˜¶æ®µå®æ—¶æ£€æµ‹ |
| **æ³›åŒ–èƒ½åŠ›** | åœ¨è·¨é¢†åŸŸï¼ˆåŒ»å­¦ã€å¸¸è¯†ã€æ³•å¾‹ï¼‰ä»»åŠ¡ä¸­è¡¨ç°ç¨³å®š |
| **å¯è§£é‡Šæ€§** | æ”¯æŒ token-level è¾“å‡ºæ¦‚ç‡å¯è§†åŒ–ï¼ˆè§å›¾1ï¼‰ï¼Œä¾¿äºå®šä½å¹»è§‰å®ä½“ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **LongFact-annotations**ï¼šåŸºäº LongFact å’Œ LongFact++ æ„é€ çš„é•¿æ–‡æœ¬è¯­æ–™ï¼Œä¿ç•™ token å¯¹é½ï¼Œæ ‡æ³¨å®ä½“çº§åˆ«å¹»è§‰ï¼ˆäººç‰©ã€ç»„ç»‡ã€åœ°ç‚¹ã€æ—¥æœŸã€å¼•ç”¨ç­‰ï¼‰ã€‚
- å››ä¸ªè¯„ä¼°æ•°æ®é›†ï¼š
  1. **LongFact**ï¼šç”± Qwen2.5-7B-Instruct ç”Ÿæˆ
  2. **LongFact-Augmented**ï¼šåŒä¸Šï¼Œå¢å¼ºç‰ˆæœ¬
  3. **HealthBench**ï¼šåŒ»å­¦çŸ¥è¯†é—®ç­”ï¼ŒMeta-Llama-3.1-8B-Instruct ç”Ÿæˆ
  4. **TriviaQA**ï¼šå¸¸è¯†é—®ç­”ï¼ŒMeta-Llama-3.1-8B-Instruct ç”Ÿæˆ

> æ³¨ï¼šæ‰€æœ‰æ•°æ®é›†å‡é€šè¿‡å…·å¤‡ç½‘é¡µæ£€ç´¢èƒ½åŠ›çš„ LLMï¼ˆClaude 4 Sonnetï¼‰è‡ªåŠ¨éªŒè¯å¹¶æ ‡æ³¨å®ä½“çœŸå®æ€§ã€‚

### å®éªŒè®¾ç½®
- **åŸºç¡€æ¨¡å‹**ï¼šQwen2.5-7B-Instruct å’Œ Meta-Llama-3.1-8B-Instruct
- **æ¢é’ˆè®­ç»ƒæ–¹å¼**ï¼š
  - å†»ç»“ä¸»æ¨¡å‹å‚æ•°
  - æ¢é’ˆé™„åŠ äºæŒ‡å®šä¸­é—´å±‚ï¼ˆé»˜è®¤ $ l = \lfloor 0.95 \times \text{num\_layers} \rfloor $ï¼‰
  - ä»…è®­ç»ƒæ¢é’ˆå‚æ•°
- **ä¼˜åŒ–ç­–ç•¥**ï¼šä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨æœç´¢æœ€ä¼˜æ¢é’ˆæ’å…¥å±‚

### è¯„ä¼°æŒ‡æ ‡
- **AUC**ï¼šè¡¡é‡æ•´ä½“åˆ†ç±»æ€§èƒ½
- **R@0.1 FPR**ï¼ˆRecall at 0.1 False Positive Rateï¼‰ï¼šä½è¯¯æŠ¥ç‡ä¸‹çš„å¬å›ç‡ï¼Œåæ˜ æ•æ„Ÿæ€§å’Œå®ç”¨æ€§
- **Accuracy / Precision / Recall**ï¼šæ ‡å‡†åˆ†ç±»æŒ‡æ ‡
- **Probe Loss & LM Loss**ï¼šåˆ†æè®­ç»ƒç¨³å®šæ€§å’Œå¯¹åŸæ¨¡å‹å½±å“

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æè¿° |
|------|------|------|
| **Perplexity** | ä¸ç¡®å®šæ€§æ–¹æ³• | åˆ©ç”¨ä¸‹ä¸€ä¸ª token åˆ†å¸ƒçš„å›°æƒ‘åº¦ |
| **Entropy** | ä¸ç¡®å®šæ€§æ–¹æ³• | ä½¿ç”¨è¯­ä¹‰ç†µè¡¡é‡ä¸ç¡®å®šæ€§ |
| **Linear Probe** | æ¢é’ˆæ–¹æ³• | ä¼ ç»Ÿçº¿æ€§åˆ†ç±»å™¨æ¢é’ˆï¼Œç”¨äºå¯¹æ¯”éçº¿æ€§ä¼˜åŠ¿ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

| Dataset | Method | AUC â†‘ | R@0.1 â†‘ | Accuracy | Precision | Recall |
|--------|--------|-------|---------|----------|-----------|--------|
| **LongFact** | Perplexity | 0.4623 | 0.0879 | â€” | â€” | â€” |
| | Entropy | 0.5124 | 0.1076 | â€” | â€” | â€” |
| | Linear Probe | 0.9022 | 0.6891 | 0.9022 | 0.8829 | 0.5280 |
| | **MLP Probe (Ours)** | **0.9528** | **0.7024** | **0.9528** | **0.8902** | **0.5715** |
| **TriviaQA** | Linear Probe | 0.8336 | 0.1730 | 0.8336 | 0.1178 | 0.3255 |
| | **MLP Probe (Ours)** | **0.9223** | **0.6891** | **0.9223** | **0.3886** | **0.4477** |
| > æå‡å¹…åº¦ | â€” | +8.87% AUC | +297% R@0.1 | â€” | **+229% Precision** | +37% Recall |

> âœ… åœ¨ TriviaQA ä¸Šï¼Œ**Precision æå‡è¶…è¿‡ 270%**ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½æ›´ç²¾å‡†è¯†åˆ«å¹»è§‰è€Œä¸è¿‡åº¦æŠ¥è­¦ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **MLP Probe æ˜¾è‘—ä¼˜äºä¸ç¡®å®šæ€§æ–¹æ³•**ï¼š
  - AUC æå‡çº¦ 40â€“60 ä¸ªç™¾åˆ†ç‚¹
  - R@0.1 æå‡ 5â€“7 å€
- **MLP Probe æ˜æ˜¾ä¼˜äº Linear Probe**ï¼š
  - åœ¨ LongFact ä¸Š AUC æå‡ 5.6%ï¼ŒRecall æå‡ 8.2%
  - åœ¨ HealthBench å’Œ TriviaQA ä¸Šå±•ç°å‡ºæ›´å¼ºçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›
- **LM Loss æ›´å¿«æ”¶æ•›**ï¼šMLP æ¢é’ˆåœ¨æ›´å°‘è®­ç»ƒè½®æ¬¡å†…è¾¾åˆ°ç¨³å®šï¼Œè¯´æ˜å…¶è¡¨è¾¾èƒ½åŠ›æ›´å¼ºï¼ˆè§ Figure 5bï¼‰

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 2ï¼‰
ç§»é™¤å„æŸå¤±é¡¹åçš„æ€§èƒ½ä¸‹é™ï¼ˆâ–³AUC å’Œ â–³R@0.1ï¼‰ï¼š

| ç»„ä»¶ | â–³AUC | â–³R@0.1 |
|------|------|--------|
| `Lfocal`ï¼ˆfocal lossï¼‰ | -0.3720 | -0.4414 |
| `Lsparse`ï¼ˆç¨€ç–æ­£åˆ™ï¼‰ | -0.0567 | -0.1393 |
| `CKL`ï¼ˆKL çº¦æŸï¼‰ | -0.0234 | -0.0552 |
| `Lspan`ï¼ˆspan èšåˆï¼‰ | -0.0132 | -0.1706 |

> ğŸ” å‘ç°ï¼š
> - `Lfocal` æ˜¯æœ€å…³é”®ç»„ä»¶ï¼Œæ˜¾è‘—æå‡å¯¹å°‘æ•°ç±»ï¼ˆå¹»è§‰ tokenï¼‰çš„æ•æ„Ÿæ€§
> - `Lspan` å¯¹ R@0.1 å½±å“æœ€å¤§ï¼Œè¯´æ˜å…¶å¢å¼ºäº†å¯¹è¿ç»­å¹»è§‰ç‰‡æ®µçš„æ£€æµ‹èƒ½åŠ›
> - `Lsparse` ä¸»è¦æå‡ AUCï¼ŒæŠ‘åˆ¶æ¢é’ˆè¿‡æ´»è·ƒï¼Œæé«˜å¯è§£é‡Šæ€§

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **éçº¿æ€§æ¢é’ˆæ˜¾è‘—ä¼˜äºçº¿æ€§æ¢é’ˆ**ï¼šMLP æ¢é’ˆèƒ½å¤Ÿæ›´å¥½åœ°å»ºæ¨¡æ·±å±‚è¯­ä¹‰ç©ºé—´ä¸­çš„éçº¿æ€§å¹»è§‰æ¨¡å¼ï¼Œåœ¨ token-level æ£€æµ‹ä¸­æ›´å…·ä¼˜åŠ¿ã€‚
2. **å¤šç›®æ ‡æŸå¤±è®¾è®¡æœ‰æ•ˆæå‡ç¨³å®šæ€§ä¸åˆ¤åˆ«åŠ›**ï¼šç»“åˆ focal lossã€span aggregationã€ç¨€ç–çº¦æŸå’Œ KL æ­£åˆ™ï¼Œå®ç°äº†é«˜ç²¾åº¦ã€ä½è¯¯æŠ¥ã€å¼ºé²æ£’æ€§çš„æ£€æµ‹æ•ˆæœã€‚
3. **æœ€ä¼˜æ¢é’ˆæ’å…¥å±‚å¯é€šè¿‡è´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨å®šä½**ï¼š
   - Qwen2.5-7B-Instruct æœ€ä¼˜å±‚ä¸º **ç¬¬ 29 å±‚**
   - Meta-Llama-3.1-8B-Instruct æœ€ä¼˜å±‚ä¸º **ç¬¬ 22 å±‚**
   - éªŒè¯äº†â€œä¸­é«˜å±‚è¯­ä¹‰æŠ½è±¡åŒºâ€å…·æœ‰æœ€ä½³ç‰¹å¾å¯åˆ†æ€§ï¼ˆè§ Figure 6ï¼‰
4. **ç†è®ºä¿éšœ**ï¼šè¯æ˜äº†è´å¶æ–¯ä¼˜åŒ–åœ¨è¯¥é»‘ç®±æœç´¢é—®é¢˜ä¸Šçš„æ¸è¿‘æœ€ä¼˜æ€§ï¼ˆTheorem 1ï¼‰ï¼Œç¡®ä¿é«˜æ•ˆæ”¶æ•›è‡³å…¨å±€æœ€ä¼˜å±‚ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰èšç„¦äº **entity-level hallucinations**ï¼ˆå¦‚è™šæ„äººåã€æœºæ„ã€æ—¶é—´ï¼‰ï¼Œå°šæœªæ‰©å±•åˆ° assertion-level æˆ– reasoning-level å¹»è§‰ã€‚
- æ¢é’ˆä»éœ€ç›‘ç£è®­ç»ƒæ•°æ®ï¼ˆå¸¦æ ‡æ³¨çš„å¹»è§‰ tokenï¼‰ï¼Œä¾èµ–é«˜è´¨é‡æ ‡æ³¨ pipelineã€‚
- è™½ç„¶è½»é‡ï¼Œä½†åœ¨å¤§è§„æ¨¡éƒ¨ç½²æ—¶ä»éœ€è€ƒè™‘é¢å¤–å»¶è¿Ÿï¼ˆå°½ç®¡è¿œä½äº retrieval-based æ–¹æ³•ï¼‰ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³ **cross-lingual å’Œ multimodal åœºæ™¯** ä¸‹çš„å¹»è§‰æ£€æµ‹
- æ¢ç´¢ **æ— ç›‘ç£æˆ–å¼±ç›‘ç£æ¢é’ˆè®­ç»ƒæ–¹æ³•**
- å°†æ¢é’ˆæœºåˆ¶ä¸ retrieval-augmented æ–¹æ³•ç»“åˆï¼Œå½¢æˆ hybrid detection system
- åº”ç”¨äº **real-time streaming generation** ä¸­çš„åŠ¨æ€å¹²é¢„ä¸ä¿®æ­£

--- 

âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäº **neural probe** çš„é«˜æ•ˆã€å‡†ç¡®ã€å¯è§£é‡Šçš„ token-level å¹»è§‰æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡ **MLP æ¢é’ˆ + å¤šç›®æ ‡æŸå¤± + è´å¶æ–¯ä¼˜åŒ–é€‰å±‚** ä¸‰é‡åˆ›æ–°ï¼Œåœ¨å¤šä¸ªæƒå¨æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨ä½è¯¯æŠ¥æ¡ä»¶ä¸‹å¤§å¹…æå‡å¬å›ç‡å’Œç²¾ç¡®ç‡ï¼Œä¸ºæ„å»ºå¯ä¿¡ LLM ç³»ç»Ÿæä¾›äº†å®ç”¨å·¥å…·ã€‚

</details>

---

### 15. [SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance](https://arxiv.org/abs/2512.21280)

**Authors**: Divij Dudeja, Mayukha Pal  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.21280v1  

#### Abstract
The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡é’ˆå¯¹**å·¥ç¨‹æ‰‹å†Œï¼ˆEngineering Manuals, EMï¼‰ç­‰æŠ€æœ¯æ–‡æ¡£é—®ç­”ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜**æå‡ºè§£å†³æ–¹æ¡ˆï¼Œä¸»è¦åŒ…æ‹¬ï¼š
- **ç»“æ„åŒ–å†…å®¹ä¸¢å¤±**ï¼šä¼ ç»Ÿ Transformer å°†æ–‡æ¡£è§†ä¸ºæ‰å¹³ token æµï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†è¡¨æ ¼ã€å‚æ•°åˆ—è¡¨ç­‰ç»“æ„åŒ–ä¿¡æ¯ã€‚
- **äº‹å®å‡†ç¡®æ€§å·®**ï¼šå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ˜“äº§ç”Ÿâ€œå¹»è§‰â€ï¼Œå°¤å…¶åœ¨æ•°å€¼å‹ç­”æ¡ˆä¸Šå®¹æ˜“å‡ºé”™ã€‚
- **è®°å¿†å®¹é‡æœ‰é™**ï¼šSLM å‚æ•°å°‘ï¼Œæ— æ³•æœ‰æ•ˆå­˜å‚¨å¤§é‡ç¦»æ•£äº‹å®ã€‚
- **å»¶è¿Ÿä¸å¯ç”¨æ€§çŸ›ç›¾**ï¼šå®Œæ•´åˆ†æé•¿æ–‡æ¡£è®¡ç®—å¼€é”€å¤§ï¼Œå½±å“å®æ—¶å“åº”ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
ä½œè€…æå‡ºäº† **SMARTï¼ˆStructured Memory and Reasoning Transformerï¼‰æ¶æ„**ï¼Œå…¶æ ¸å¿ƒæ˜¯å°†è¯­è¨€ç†è§£ä»»åŠ¡è§£è€¦ä¸ºä¸‰ä¸ªååŒæ¨¡å—ï¼š

1. **Grammarianï¼ˆè¯­æ³•å­¦å®¶ï¼‰â€”â€”åŸºäº Tree-LSTM çš„ Fact Extractor**
   - åˆ©ç”¨å¥æ³•è§£æå™¨ï¼ˆspaCy + beneparï¼‰è¯†åˆ«å¥å­ç»“æ„ã€‚
   - ä½¿ç”¨ **Tree-LSTM** æå– (Subject, Relation, Object) å½¢å¼çš„ç»“æ„åŒ–ä¸‰å…ƒç»„äº‹å®ã€‚
   - è¾“å‡ºè¯­ä¹‰æ¸…æ™°ã€å¯éªŒè¯çš„æ ‡å‡†åŒ–äº‹å®è¡¨ç¤ºã€‚

2. **Librarianï¼ˆå›¾ä¹¦ç®¡ç†å‘˜ï¼‰â€”â€”åŸºäº MANN çš„å¤–éƒ¨è®°å¿†ç³»ç»Ÿ**
   - å°†æå–çš„äº‹å®ç¼–ç ä¸º **384 ç»´å‘é‡**å¹¶å­˜å…¥ **Memory-Augmented Neural Network (MANN)**ã€‚
   - ç»“åˆ **FAISS** å®ç°å¿«é€Ÿè¿‘ä¼¼æœ€è¿‘é‚»æ£€ç´¢ï¼Œæ”¯æŒé«˜æ•ˆæŸ¥è¯¢ã€‚
   - æ¯ä¸ªäº‹å®é™„å¸¦æ¥æºä¿¡æ¯ï¼ˆprovenanceï¼‰ï¼Œæå‡å¯è¿½æº¯æ€§ã€‚

3. **Transformer Reasoning Engineï¼ˆæ¨ç†ç”Ÿæˆå¼•æ“ï¼‰**
   - é‡‡ç”¨ä¸€ä¸ªä»… **6 å±‚çš„å°å‹ Transformer**ï¼ˆå…± 45.51M å‚æ•°ï¼‰è¿›è¡Œè‡ªç„¶è¯­è¨€ç”Ÿæˆã€‚
   - å¼•å…¥ **gated memory fusion æœºåˆ¶**ï¼šé€šè¿‡é—¨æ§æœºåˆ¶èåˆä»è®°å¿†ä¸­æ£€ç´¢åˆ°çš„äº‹å®ä¸Šä¸‹æ–‡ï¼Œé¿å…ç›´æ¥ä¾èµ–æ¨¡å‹å†…éƒ¨è®°å¿†ã€‚

æ­¤å¤–ï¼Œè®¾è®¡äº†**åŒæ¨¡å¼æ¨ç†æœºåˆ¶**ï¼š
- **é¢„ç´¢å¼•å¿«é€Ÿè·¯å¾„ï¼ˆPre-indexed Fast Pathï¼‰**ï¼šå¯¹å·²çŸ¥æ–‡æ¡£æå‰æ„å»ºè®°å¿†çŸ©é˜µï¼Œå®ç°äºšç§’çº§å“åº”ã€‚
- **åŠ¨æ€è·¯å¾„ï¼ˆDynamic Pathï¼‰**ï¼šå¯¹æ–°æ–‡æ¡£ä½¿ç”¨ RAG å…ˆæ£€ç´¢ç›¸å…³æ®µè½ï¼ˆTop-20 FAISSï¼‰ï¼Œå†è¿è¡Œ FactExtractor æ„å»ºå±€éƒ¨è®°å¿†ï¼ˆé™åˆ¶æœ€å¤š 64 slotsï¼‰ï¼Œå…¼é¡¾çµæ´»æ€§ä¸ä½å»¶è¿Ÿã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | SMART çš„ä¼˜åŠ¿ |
|------|--------------|
| **å‚æ•°æ•ˆç‡** | ä»… 45.51M å‚æ•°ï¼Œæ¯” GPT-2ï¼ˆ124Mï¼‰å‡å°‘ 64%ï¼Œæ¯” BERTï¼ˆ133Mï¼‰å‡å°‘ 69% |
| **å‡†ç¡®ç‡** | åœ¨ç›¸åŒä»»åŠ¡ä¸‹æ¯” GPT-2 é«˜å‡º **21.3% å‡†ç¡®ç‡**ï¼Œæœ€ç»ˆæŸå¤±é™ä½ 16.0% |
| **äº‹å®å¯é æ€§** | å¤–éƒ¨è®°å¿†å­˜å‚¨æ˜¾å¼ç»“æ„åŒ–äº‹å®ï¼Œæ˜¾è‘—å‡å°‘å¹»è§‰ï¼Œæ”¯æŒç­”æ¡ˆæº¯æº |
| **éƒ¨ç½²å®ç”¨æ€§** | æ”¯æŒåŒæ¨¡å¼æ¨ç†ï¼Œåœ¨ç²¾åº¦ã€é€Ÿåº¦ã€çµæ´»æ€§ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ |
| **å¯ç»´æŠ¤æ€§** | é”™è¯¯äº‹å®å¯å•ç‹¬ç¼–è¾‘æˆ–åˆ é™¤ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- ä¸»è¦æ•°æ®æ¥è‡ªçœŸå®ä¸–ç•Œ **å·¥ç¨‹è®¾å¤‡æ‰‹å†Œï¼ˆEngineering Manualsï¼‰**ã€‚
- æ–‡æ¡£è¢«åˆ‡åˆ†ä¸ºçº¦ **380,438 ä¸ªç‰‡æ®µ**ï¼ˆæ¯æ®µçº¦ 100 tokensï¼‰ï¼Œç”¨äºç´¢å¼•å’Œæ£€ç´¢ã€‚
- è®­ç»ƒåˆæœŸå¼•å…¥ **TinyStories** æ•°æ®é›†è¾…åŠ©å­¦ä¹ åŸºç¡€è‹±æ–‡æµç•…æ€§ã€‚

### å®éªŒè®¾ç½®
- **æ¨¡å‹è§„æ¨¡**ï¼š
  - Hidden dimension: 384
  - Feed-forward inner dimension: 1536
  - Transformer layers: 6
  - Attention heads: 8
  - Total parameters: **45.51M**

- **æ£€ç´¢ç»„ä»¶**ï¼š
  - ä½¿ç”¨ `all-MiniLM-L6-v2` ç”Ÿæˆæ®µè½åµŒå…¥ã€‚
  - FAISS IndexFlatIP è¿›è¡Œå†…ç§¯ç›¸ä¼¼åº¦æœç´¢ï¼ˆç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ã€‚
  - æŸ¥è¯¢æ—¶è¿”å› Top-20 æœ€ç›¸å…³æ®µè½ã€‚

- **è®­ç»ƒç­–ç•¥åˆ†ä¸‰é˜¶æ®µ**ï¼š
  1. **è¯­è¨€é¢„è®­ç»ƒ**ï¼šnext-token predictionï¼Œä½¿ç”¨ TinyStories å’Œéƒ¨åˆ† EM æ•°æ®ã€‚
  2. **è®°å¿†çƒ­å¯åŠ¨ï¼ˆMemory Warmupï¼‰**ï¼šæœ€å°åŒ–æŸ¥è¯¢å‘é‡ä¸å¯¹åº”äº‹å®å‘é‡ä¹‹é—´çš„ MSE æŸå¤±ã€‚
  3. **è”åˆå¾®è°ƒï¼ˆJoint Fine-tuningï¼‰**ï¼š
     - InfoNCE å¯¹æ¯”æŸå¤±ï¼ˆæ‹‰è¿‘æ­£æ ·æœ¬ï¼Œæ¨å¼€è´Ÿæ ·æœ¬ï¼‰
     - å¯é€‰é‡å»ºæŸå¤±ï¼ˆdecoder é‡æ„åŸå§‹æ–‡æœ¬ä¸‰å…ƒç»„ï¼‰
     - æ€»æŸå¤±ï¼š`L = 1.0*L_MSE + 1.0*L_InfoNCE + 0.5*L_recon`

### è¯„ä¼°æŒ‡æ ‡
- **ä¸»æŒ‡æ ‡**ï¼šFinal Lossï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰
- **ç”Ÿæˆè´¨é‡æŒ‡æ ‡**ï¼š
  - BLEU-1/2/4
  - ROUGE-1/2/L
- **æ•ˆç‡æŒ‡æ ‡**ï¼š
  - å‚æ•°æ•°é‡ï¼ˆMï¼‰
  - å“åº”æ—¶é—´ï¼ˆResponse Timeï¼‰
  - å‚æ•°æ•ˆç‡å¾—åˆ†ï¼š`Efficiency = 1 / (Parameters Ã— Loss)`
- **å®šæ€§åˆ†æ**ï¼šäººå·¥æ£€æŸ¥ç”Ÿæˆå›ç­”çš„äº‹å®ä¸€è‡´æ€§ä¸å¯è§£é‡Šæ€§

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ¨¡å‹ | ç±»å‹ | å‚æ•°é‡ï¼ˆMï¼‰ |
|------|------|------------|
| DistilBERT | è’¸é¦æ¨¡å‹ | 89.8 |
| GPT-2 | è‡ªå›å½’æ¨¡å‹ | 124.4 |
| BERT | åŒå‘ç¼–ç å™¨ | 133.0 |
| Pure Transformer | è‡ªç ”çº¯ Transformer åŸºçº¿ | 52.0 |
| **SMART (Ours)** | â€”â€” | **45.51** |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### è¡¨ Iï¼šæ€§èƒ½å¯¹æ¯”ï¼ˆSMART vs åŸºçº¿æ¨¡å‹ï¼‰

| Model | Parameters (M) | Final Loss |
|-------|----------------|-----------|
| DistilBERT | 89.8 | 10.430 |
| GPT-2 | 124.4 | 2.787 |
| BERT | 133.0 | 10.460 |
| Pure Transformer | 52.0 | 3.456 |
| **SMART (Ours)** | **45.51** | **2.341** |
| **Improvement vs GPT-2** | **â†“63.4%** | **â†“16.0%** |
| **Improvement vs Pure Transformer** | **â†“12.5%** | **â†“32.3%** |

> âœ… SMART åœ¨æ›´å°å‚æ•°é‡ä¸‹å®ç°äº†æœ€ä½æŸå¤±ï¼Œè¡¨æ˜å…¶å¯¹ç›®æ ‡ä»»åŠ¡æ‹Ÿåˆæ›´å¥½ã€‚

#### è¡¨ IIï¼šç”Ÿæˆè´¨é‡å¯¹æ¯”ï¼ˆSMART vs Pure Transformerï¼‰

| Metric | SMART (SLM) | Pure Transformer |
|--------|-------------|------------------|
| BLEU-1 Score | **0.1445** | 0.0238 (+482%) |
| BLEU-2 Score | **0.0512** | 0.0080 (+540%) |
| BLEU-4 Score | **0.0148** | 0.0038 (+289%) |
| ROUGE-1 Score | **0.2032** | 0.0511 (+298%) |
| ROUGE-2 Score | **0.0394** | 0.0015 (+2438%) |
| ROUGE-L Score | **0.1734** | 0.0481 (+260%) |
| Response Time (s) | 0.4578 | **0.2696** |

> ğŸ”º SMART åœ¨æ‰€æœ‰ç”ŸæˆæŒ‡æ ‡ä¸Šå¤§å¹…é¢†å…ˆï¼Œä½†å“åº”æ—¶é—´å¢åŠ çº¦ 68%ï¼Œå±äºä¸ºé«˜è´¨é‡ä»˜å‡ºçš„åˆç†ä»£ä»·ã€‚

#### è¡¨ IIIï¼šå‚æ•°æ•ˆç‡åˆ†æ

| Model | Efficiency Score |
|-------|------------------|
| Pure Transformer | 5.56Ã—10â»Â² |
| **SMART (Ours)** | **9.12Ã—10â»Â²** |
| **Improvement** | **+117.3%** |

> ğŸ“ˆ SMART çš„å‚æ•°æ•ˆç‡è¶…è¿‡æœ€ä½³åŸºçº¿ 117%ï¼Œè¯´æ˜å…¶ä»¥æä½æˆæœ¬å®ç°é«˜ç²¾åº¦ã€‚

### æ¶ˆèå®éªŒï¼ˆAblation Studiesï¼‰
è™½ç„¶æ–‡ä¸­æœªåˆ—å‡ºè¯¦ç»†æ¶ˆèè¡¨ï¼Œä½†ä»è®¾è®¡é€»è¾‘å’Œè®¨è®ºä¸­å¯æ¨æ–­ä»¥ä¸‹å…³é”®ç»„ä»¶çš„ä½œç”¨ï¼š
- **Grammarianï¼ˆTree-LSTM æå–å™¨ï¼‰**ï¼šç¡®ä¿è¾“å…¥è®°å¿†çš„æ˜¯é«˜è´¨é‡ã€ç»“æ„åŒ–çš„ SRO ä¸‰å…ƒç»„ï¼Œè€Œéå™ªå£°æ–‡æœ¬ã€‚
- **Gated Memory Fusion**ï¼šæ§åˆ¶è®°å¿†ä¿¡æ¯æ³¨å…¥å¼ºåº¦ï¼Œé˜²æ­¢å¹²æ‰°è¯­è¨€æµç•…æ€§ã€‚
- **Provenance Tracking**ï¼šæ”¯æŒç­”æ¡ˆæº¯æºï¼Œå¢å¼ºå¯ä¿¡åº¦ã€‚
- **Dual Inference Modes**ï¼šå®æµ‹æ˜¾ç¤ºé¢„ç´¢å¼•æ¨¡å¼å¯åœ¨ <1 ç§’å†…è¿”å›ç­”æ¡ˆï¼›åŠ¨æ€æ¨¡å¼è™½æ…¢ä½†ä»å¯æ§ï¼ˆ~0.46sï¼‰ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **åˆ†ç¦»å¼æ¶æ„ä¼˜äºç«¯åˆ°ç«¯è®°å¿†**ï¼šå°†â€œäº‹å®è®°å¿†â€ä¸â€œè¯­è¨€ç”Ÿæˆâ€åˆ†ç¦»ï¼Œèƒ½è®© SLM åœ¨ä¿æŒå°ä½“ç§¯çš„åŒæ—¶è·å¾—é«˜äº‹å®å‡†ç¡®æ€§ã€‚
2. **ç»“æ„åŒ–æå–è‡³å…³é‡è¦**ï¼šTree-LSTM èƒ½æœ‰æ•ˆæ•æ‰æŠ€æœ¯æ–‡æœ¬ä¸­çš„å¤æ‚å¥æ³•å…³ç³»ï¼Œç”Ÿæˆè§„èŒƒåŒ–çš„ (S,R,O) äº‹å®ã€‚
3. **å¤–éƒ¨è®°å¿†æ˜¾è‘—é™ä½å¹»è§‰**ï¼šé€šè¿‡æŸ¥è¯¢å¤–éƒ¨ MANN è·å–è¯æ®ï¼Œæ¨¡å‹ä¸å†ä¾èµ–â€œèƒŒè¯µâ€èƒ½åŠ›ï¼Œå›ç­”æ›´å…·ä¾æ®ã€‚
4. **å‚æ•°æ•ˆç‡ä¸æ€§èƒ½å¯ä»¥å…¼å¾—**ï¼šSMART è¯æ˜äº†é€šè¿‡åˆç†æ¶æ„è®¾è®¡ï¼Œå°æ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šå¤§æ¨¡å‹çš„è¡¨ç°ã€‚
5. **åŒæ¨¡å¼æ¨ç†æå‡å®ç”¨æ€§**ï¼šé¢„ç´¢å¼• + RAG åŠ¨æ€ç¼–è¯‘çš„ç»„åˆæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚ï¼Œé€‚åˆå·¥ä¸šéƒ¨ç½²ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–å¥æ³•è§£æè´¨é‡**ï¼šOCR é”™è¯¯ã€æ ¼å¼æ··ä¹±çš„ PDF ä¼šå½±å“ Tree-LSTM çš„æå–æ•ˆæœã€‚
- **æ£€ç´¢å¤±è´¥å¯¼è‡´æ¼ç­”**ï¼šè‹¥ RAG æœªèƒ½å¬å›å…³é”®æ®µè½ï¼Œåˆ™åç»­æ— æ³•æå–æ­£ç¡®äº‹å®ï¼ˆå•ç‚¹æ•…éšœï¼‰ã€‚
- **ç¼ºä¹å†²çªè§£å†³æœºåˆ¶**ï¼šå½“å¤šä¸ªæ–‡æ¡£æä¾›çŸ›ç›¾ä¿¡æ¯æ—¶ï¼Œä»…é å¯å‘å¼è§„åˆ™ï¼ˆå¦‚æ—¶é—´ä¼˜å…ˆï¼‰å¤„ç†ä¸å¤Ÿç¨³å¥ã€‚
- **æ•°å€¼å•ä½æ­§ä¹‰ä»å­˜åœ¨é£é™©**ï¼šå°½ç®¡åšäº†å½’ä¸€åŒ–ï¼Œéšå«å•ä½æˆ–ä¸Šä¸‹æ–‡ä¾èµ–çš„å°ºåº¦å¯èƒ½å¼•å‘é”™è¯¯ã€‚
- **å¯å‘å¼è®¾è®¡è¾ƒå¤š**ï¼šå¦‚ slot æ•°é‡ä¸Šé™ã€å»é‡é˜ˆå€¼ç­‰ä¾èµ–ç»éªŒè®¾å®šï¼Œæ³›åŒ–æ€§æœ‰å¾…éªŒè¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **Table-aware Extraction**ï¼šå¼€å‘ä¸“ç”¨è¡¨æ ¼è§£æå™¨ï¼Œæå‡å¤šåˆ—å‚æ•°è¡¨çš„ä¿¡æ¯æŠ½å–èƒ½åŠ›ã€‚
- **æ”¹è¿›æ£€ç´¢æ¨¡å—**ï¼šåŠ å…¥è½»é‡çº§ cross-encoder reranker æˆ–é¢†åŸŸå¾®è°ƒ retrieverï¼Œæé«˜å¬å›ç‡ã€‚
- **ç»“æ„åŒ–è®°å¿†å›¾è°±**ï¼šå°†æ‰å¹³ memory æ‰©å±•ä¸º graph ç»“æ„ï¼Œæ”¯æŒå¤šè·³æ¨ç†ä¸çº¦æŸæ£€æŸ¥ã€‚
- **é›†æˆæ•°å€¼æ ¡éªŒå™¨**ï¼šå¼•å…¥ unit checker å’Œ range validatorï¼Œè‡ªåŠ¨æ£€æµ‹æ•°å€¼åˆç†æ€§ã€‚
- **ç”¨æˆ·åé¦ˆé—­ç¯**ï¼šå…è®¸ç”¨æˆ·æ ‡è®°é”™è¯¯äº‹å®ï¼Œå¹¶è‡ªåŠ¨æ›´æ–°è®°å¿†ç´¢å¼•ã€‚
- **æ‰©å±•è¯„æµ‹åŸºå‡†**ï¼šæ„å»ºæ¶µç›–å¤šæ–‡æ¡£ã€æ¨¡ç³ŠæŸ¥è¯¢ã€æ—¶æ•ˆæ€§æ›´æ–°çš„æ ‡å‡†æµ‹è¯•é›†ï¼Œå¹¶å…¬å¼€æ ‡æ³¨æ•°æ®ã€‚

---

## æ€»ç»“
**SMART SLM** æ˜¯ä¸€ç§é¢å‘æŠ€æœ¯æ–‡æ¡£é—®ç­”çš„æ–°å‹å°å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œé€šè¿‡ **Grammarian â†’ Librarian â†’ Transformer** çš„ä¸‰çº§åä½œæœºåˆ¶ï¼Œå®ç°äº†**é«˜å‡†ç¡®æ€§ã€å¼ºå¯è§£é‡Šæ€§ã€ä½å‚æ•°é‡å’Œå®ç”¨åŒ–éƒ¨ç½²èƒ½åŠ›**çš„ç»Ÿä¸€ã€‚å®ƒå±•ç¤ºäº†â€œæå–ä¼˜è´¨äº‹å® + æ˜¾å¼å­˜å‚¨ + å°æ¨¡å‹æŸ¥è¯¢â€çš„èŒƒå¼åœ¨ä¸“ä¸šé¢†åŸŸçš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæ„å»º**å¯ä¿¡ã€å¯å®¡è®¡ã€å¯ç»´æŠ¤çš„æ–‡æ¡£åŠ©æ‰‹ç³»ç»Ÿ**æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚

</details>

---

### 16. [Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models](https://arxiv.org/abs/2512.20633)

**Authors**: MunHwan Lee, Shaika Chowdhury, Xiaodi Li, Sivaraman Rajaganapathy, Eric W Klee, Ping Yang, Terence Sio, Liewei Wang, James Cerhan, Nansu NA Zong  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.20633v1  

#### Abstract
Accurate prediction of treatment outcomes in lung cancer remains challenging due to the sparsity, heterogeneity, and contextual overload of real-world electronic health data. Traditional models often fail to capture semantic information across multimodal streams, while large-scale fine-tuning approa...

---

### 17. [Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches](https://arxiv.org/abs/2512.20082)

**Authors**: Chaithra, Kamesh Kadimisetty, Biju R Mohan  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.20082v2  

#### Abstract
Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we pro...

---

### 18. [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)

**Authors**: Ze Gong, Pradeep Varakantham, Akshat Kumar  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.20173v1  

#### Abstract
Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previ...

---

### 19. [Parallel Token Prediction for Language Models](https://arxiv.org/abs/2512.21323)

**Authors**: Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.21323v1  

#### Abstract
We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregres...

---

### 20. [Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs](https://arxiv.org/abs/2512.20861)

**Authors**: Pierre Abillama, Changwoo Lee, Juechu Dong, David Blaauw, Dennis Sylvester, Hun-Seok Kim  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.20861v1  

#### Abstract
Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this...

---

### 21. [Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions](https://arxiv.org/abs/2512.20974)

**Authors**: Jingyang You, Hanna Kurniawati  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.20974v1  

#### Abstract
Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applic...

---

### 22. [C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling](https://arxiv.org/abs/2512.21332)

**Authors**: Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di, Rui Wang  
**Category**: cs.CL  
**Published**: 2025-12-25  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.21332v1  

#### Abstract
We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing...

---

### 23. [RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale](https://arxiv.org/abs/2512.20795)

**Authors**: Aymen Alsaadi, Mason Hooten, Mariya Goliyad, Andre Merzky, Andrew Shao, Mikhail Titov, Tianle Wang, Yian Chen, Maria Kalantzi, Kent Lee, Andrew Park, Indira Pimpalkhare, Nick Radcliffe, Colin Wahl, Pete Mendygral, Matteo Turilli, Shantenu Jha  
**Category**: cs.DC  
**Published**: 2025-12-25  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.20795v1  

#### Abstract
Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent ...

---

### 24. [HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model](https://arxiv.org/abs/2512.20674)

**Authors**: Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.20674v1  

#### Abstract
Vision Language Models (VLMs) have undergone significant advancements, particularly with the emergence of mobile-oriented VLMs, which offer a wide range of application scenarios. However, the substantial computational requirements for training these models present a significant obstacle to their pra...

---

### 25. [GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface](https://arxiv.org/abs/2512.20813)

**Authors**: Miguel Esparza, Vamshi Battal, Ali Mostafavi  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.20813v1  

#### Abstract
As wildfires increasingly evolve into urban conflagrations, traditional risk models that treat structures as isolated assets fail to capture the non-linear contagion dynamics characteristic of the wildland urban interface (WUI). This research bridges the gap between mechanistic physics and data driv...

---

### 26. [Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks](https://arxiv.org/abs/2512.20893)

**Authors**: Runqi Lin  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.20893v1  

#### Abstract
With deep neural networks (DNNs) increasingly embedded in modern society, ensuring their safety has become a critical and urgent issue. In response, substantial efforts have been dedicated to the red-blue adversarial framework, where the red team focuses on identifying vulnerabilities in DNNs and th...

---

### 27. [Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks](https://arxiv.org/abs/2512.21241)

**Authors**: Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.21241v1  

#### Abstract
In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding...

---

### 28. [Learning to Solve PDEs on Neural Shape Representations](https://arxiv.org/abs/2512.21311)

**Authors**: Lilian Welschinger, Yilin Liu, Zican Wang, Niloy Mitra  
**Category**: cs.LG  
**Published**: 2025-12-25  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.21311v1  

#### Abstract
Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface P...

---

### 29. [A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution](https://arxiv.org/abs/2512.19882)

**Authors**: Mahdi Mostajabdaveh, F. Sibel Salman, Walter J. Gutjahr  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.19882v1  

#### Abstract
The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating ...

---

### 30. [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)

**Authors**: Xingyou Yin, Ceyao Zhang, Min Hu, Kai Chen  
**Category**: cs.AI  
**Published**: 2025-12-25  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.20140v1  

#### Abstract
Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
