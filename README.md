# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-01-14 05:58:50 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion](https://arxiv.org/abs/2601.08482)

**Authors**: Chenxu Han, Sean Bin Yang, Jilin Hu  
**Category**: cs.LG  
**Published**: 2026-01-14  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.08482v1  

#### Abstract
Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue t...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šDiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
- **å™ªå£°æ•æ„Ÿæ€§**ï¼šä¼ ç»ŸåŸºäº HMM çš„æ–¹æ³•å¯¹ GPS è½¨è¿¹ä¸­çš„å™ªå£°éå¸¸æ•æ„Ÿï¼Œå°¤å…¶æ˜¯åœ¨åŸå¸‚å¤æ‚ç¯å¢ƒä¸­ï¼ŒGPS æ¼‚ç§»ä¸¥é‡ï¼Œå¯¼è‡´åŒ¹é…é”™è¯¯ã€‚
- **ç¨€ç–è½¨è¿¹æ€§èƒ½ä¸‹é™**ï¼šç”±äºå­˜å‚¨æˆæœ¬é™åˆ¶ï¼Œè®¸å¤šåœ°å›¾æœåŠ¡é‡‡ç”¨ä½é¢‘é‡‡æ ·ç­–ç•¥ï¼Œå¯¼è‡´è½¨è¿¹ç‚¹ç¨€ç–ï¼Œéš¾ä»¥æ•æ‰æ—¶ç©ºè¿ç»­æ€§ï¼Œä¸¥é‡å½±å“ map matching å‡†ç¡®ç‡ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯**
- **DiffMM æ¡†æ¶**ï¼šæå‡ºä¸€ç§åŸºäº **encoder-diffusion** çš„ map matching æ–°æ¡†æ¶ï¼Œé¦–æ¬¡å°† **diffusion model** å¼•å…¥ map matching ä»»åŠ¡ä¸­ã€‚
- **One-step Diffusion via Shortcut Model**ï¼šé‡‡ç”¨ **shortcut model** å®ç°å•æ­¥å»å™ªæ‰©æ•£è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ï¼Œé¿å…ä¼ ç»Ÿ diffusion æ¨¡å‹å¤šæ­¥é‡‡æ ·çš„é«˜å»¶è¿Ÿã€‚
- **Road Segment-Aware Trajectory Encoder**ï¼š
  - è”åˆç¼–ç è½¨è¿¹ç‚¹ä¸å€™é€‰é“è·¯æ®µï¼ˆcandidate road segmentsï¼‰ï¼›
  - åˆ©ç”¨ **attention mechanism** å»ºæ¨¡è½¨è¿¹ç‚¹ä¸é“è·¯æ®µä¹‹é—´çš„ç©ºé—´ä¸ä¸Šä¸‹æ–‡å…³ç³»ï¼›
  - å¼•å…¥æ–¹å‘ä¿¡æ¯ï¼ˆå¦‚å‰åå‘é‡å¤¹è§’ï¼‰å’ŒæŠ•å½±è·ç¦»ï¼Œå¢å¼ºé²æ£’æ€§ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | DiffMM ä¼˜åŠ¿ |
|------|-------------|
| **å‡†ç¡®æ€§** | åœ¨ç¨€ç–å’Œå™ªå£°åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äº HMMã€DeepMMã€GraphMM ç­‰åŸºçº¿æ–¹æ³• |
| **æ•ˆç‡** | æ¨ç†é€Ÿåº¦æ¯”ç¬¬äºŒå¿«çš„æ–¹æ³•ï¼ˆHMMï¼‰å¿«çº¦ **17å€**ï¼ˆ1.18s vs 20.57s / 1000æ¡è½¨è¿¹ï¼‰ |
| **é²æ£’æ€§** | å¯¹è®­ç»ƒæ•°æ®é‡å‡å°‘ä¸æ•æ„Ÿï¼Œåœ¨å°æ ·æœ¬ä¸‹ä»ä¿æŒé«˜æ€§èƒ½ |
| **å»ºæ¨¡èƒ½åŠ›** | å°† map matching è§†ä¸ºæ¡ä»¶åˆ†å¸ƒå­¦ä¹ é—®é¢˜ï¼Œè€Œéåºåˆ—é¢„æµ‹ï¼Œæ›´ç¬¦åˆæ¦‚ç‡ç”Ÿæˆè§†è§’ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **Porto (PT)**ï¼šè‘¡è„ç‰™æ³¢å°”å›¾å‡ºç§Ÿè½¦è½¨è¿¹ï¼ŒåŸå§‹é‡‡æ ·é—´éš” 15 ç§’
- **Beijing (BJ)**ï¼šåŒ—äº¬å‡ºç§Ÿè½¦è½¨è¿¹ï¼ŒåŸå§‹é‡‡æ ·é—´éš” 60 ç§’
- æ•°æ®æ¥æºï¼šOpenStreetMap æä¾›è·¯ç½‘æ‹“æ‰‘ï¼ˆG = (V, E)ï¼‰
- æ„é€ ç¨€ç–è½¨è¿¹ï¼šé€šè¿‡éšæœºé‡‡æ ·æ¨¡æ‹Ÿä¸åŒç¨€ç–ç¨‹åº¦ï¼ˆratio r âˆˆ {0.025~0.5}ï¼‰ï¼Œå¯¹åº”å¹³å‡æ—¶é—´é—´éš”è¾¾ 600 ç§’

| æ•°æ®é›† | è½¨è¿¹æ•° | åŒºåŸŸé¢ç§¯ (kmÂ²) | è·¯æ®µæ•° | äº¤å‰å£æ•° |
|--------|--------|----------------|--------|----------|
| Porto  | ~1M    | 11.7Ã—5.2       | 11,491 | 5,330     |
| Beijing| ~1.17M | 29.6Ã—30.0      | 65,276 | 28,738    |

### **å®éªŒè®¾ç½®**
- **æœç´¢åŠå¾„**ï¼š50 ç±³å†…æ‰€æœ‰è·¯æ®µä½œä¸ºå€™é€‰é›†ï¼ˆR-tree åŠ é€ŸæŸ¥è¯¢ï¼‰
- **æ¨¡å‹å‚æ•°**ï¼š
  - Embedding dim: 128 â†’ Condition dim: 256
  - Transformer Encoder: 2 å±‚ï¼Œ4 å¤´æ³¨æ„åŠ›
  - DiT Block: 2 å±‚ï¼Œhidden dim 512
  - è®­ç»ƒ step size d âˆˆ {1, 1/2}ï¼Œæ¨ç†ä½¿ç”¨ **one-step (M=1)**
- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA RTX 3090 GPUï¼ŒPyTorch 2.4.0

### **è¯„ä¼°æŒ‡æ ‡**
- **Accuracy**ï¼šé¢„æµ‹è·¯æ®µä¸çœŸå®åŒ¹é…è·¯æ®µä¸€è‡´çš„æ¯”ä¾‹
  $$
  \text{Accuracy}(R, \hat{R}) = \frac{1}{l} \sum_{i=1}^{l} \mathbb{I}[r_i = \hat{r}_i]
  $$

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **HMM** | éå­¦ä¹ æ–¹æ³• | ç»å…¸éšé©¬å°”å¯å¤«æ¨¡å‹ï¼Œä¾èµ–ç©ºé—´é‚»è¿‘æ€§å’Œè½¬ç§»æ¦‚ç‡ |
| **DeepMM** | å­¦ä¹ æ–¹æ³• | åŸºäº RNN çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå¸¦æ•°æ®å¢å¼º |
| **GraphMM** | å­¦ä¹ æ–¹æ³• | å›¾ç¥ç»ç½‘ç»œå»ºæ¨¡è½¨è¿¹é—´ç›¸å…³æ€§ |
| **RNTrajRec** | å­¦ä¹ æ–¹æ³• | ç»“åˆé“è·¯ç½‘ç»œç»“æ„è¿›è¡Œè½¨è¿¹æ¢å¤ |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 2ï¼‰**

#### **Porto æ•°æ®é›†ï¼ˆç¨€ç–åº¦è¶Šé«˜ï¼Œå·®è·è¶Šå¤§ï¼‰**
| r=0.2 (75s) | r=0.1 (150s) | r=0.05 (300s) | r=0.025 (600s) |
|------------|--------------|---------------|----------------|
| **93.43**  | **91.47**    | **89.08**     | **86.87**      |

> HMM åœ¨ r=0.025 æ—¶ä»… 40.04%ï¼Œä¸‹é™è¶… 40 ä¸ªç™¾åˆ†ç‚¹ï¼›DiffMM ä¸‹é™ä¸åˆ° 7 ä¸ªç™¾åˆ†ç‚¹ã€‚

#### **Beijing æ•°æ®é›†ï¼ˆå¤§è§„æ¨¡å¤æ‚è·¯ç½‘ï¼‰**
| r=0.5 (120s) | r=0.3 (200s) | r=0.2 (300s) | r=0.1 (600s) |
|-------------|--------------|--------------|--------------|
| **90.32**   | **88.45**    | **87.65**    | **85.39**    |

> å½“ r=0.1 æ—¶ï¼ŒDiffMM æ¯”ç¬¬äºŒå DeepMM (**68.25**) é«˜å‡º **15.28 ä¸ªç™¾åˆ†ç‚¹**ï¼

---

### **ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ç»“æœï¼ˆTable 3ï¼‰**

| æ–¹æ³• | æ¨ç†æ—¶é—´ (s/1000 traj) | è®­ç»ƒæ—¶é—´ (min/epoch) |
|------|------------------------|-----------------------|
| HMM | 20.57 | â€“ |
| DeepMM | 88.82 | 9.07 |
| RNTrajRec | 627.65 | 868.23 |
| **DiffMM (Ours)** | **1.18** | **10.66** |

- **æ¨ç†æ•ˆç‡**ï¼šæ¯” HMM å¿« **17.4 å€**ï¼Œæ¯” DeepMM å¿« **75 å€ä»¥ä¸Š**
- **è®­ç»ƒæ•ˆç‡**ï¼šä¸æœ€å¿«åŸºçº¿ç›¸å½“ï¼Œè¿œä¼˜äº RNTrajRec

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Study, Table 4ï¼‰**

åœ¨ Beijing æ•°æ®é›†ä¸ŠéªŒè¯å…³é”®æ¨¡å—ä½œç”¨ï¼š

| å˜ä½“ | r=0.5 | r=0.3 | r=0.2 | r=0.1 |
|------|------|------|------|------|
| w/o Trans (æ—  Transformer) | 90.06 | 88.33 | 87.12 | 84.89 |
| w/o Attn (æ—  Attention) | 88.79 | 87.25 | 85.70 | 82.71 |
| w/o Shortcut (ä¼ ç»Ÿ diffusion) | 89.67 | 87.92 | 86.84 | 83.53 |
| **DiffMM (å®Œæ•´)** | **90.32** | **88.45** | **87.65** | **85.39** |

- **Transformer ç¼–ç å™¨**ï¼šå¯¹ç¨€ç–è½¨è¿¹å°¤å…¶é‡è¦ï¼Œç¼ºå¤±åç²¾åº¦ä¸‹é™æ˜æ˜¾
- **Attention èåˆæœºåˆ¶**ï¼šæœ‰æ•ˆæŠ‘åˆ¶å™ªå£°å½±å“ï¼Œç®€å•å‡å€¼èåˆæ€§èƒ½å¤§å¹…ä¸‹é™
- **Shortcut Model**ï¼šç›¸æ¯”ä¼ ç»Ÿ diffusion æ›´é€‚åˆ one-step æ¨ç†ï¼Œæå‡æ•ˆç‡ä¸ç²¾åº¦

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **DiffMM æ˜¯é¦–ä¸ªå°† diffusion model æˆåŠŸåº”ç”¨äº map matching çš„å·¥ä½œ**ï¼Œå¼€åˆ›äº†æ–°çš„å»ºæ¨¡èŒƒå¼ã€‚
2. âœ… **one-step diffusion + shortcut model** æ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒé«˜ç²¾åº¦ï¼Œé€‚ç”¨äºå®æ—¶ç³»ç»Ÿã€‚
3. âœ… **joint embedding of trajectory and road segments** æœ‰æ•ˆèåˆå¤šæºä¿¡æ¯ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹å™ªå£°å’Œç¨€ç–æ€§çš„é²æ£’æ€§ã€‚
4. âœ… åœ¨é«˜åº¦ç¨€ç–ï¼ˆ600s é—´éš”ï¼‰å’Œå¤æ‚è·¯ç½‘æ¡ä»¶ä¸‹ï¼Œæ€§èƒ½è¿œè¶…ç°æœ‰ SOTA æ–¹æ³•ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–å€™é€‰è·¯æ®µæ£€ç´¢**ï¼šéœ€é¢„å®šä¹‰æœç´¢åŠå¾„ï¼ˆå¦‚ 50mï¼‰ï¼Œæç«¯æ¼‚ç§»å¯èƒ½æ¼æ£€æ­£ç¡®è·¯æ®µã€‚
- **æœªè€ƒè™‘åŠ¨æ€äº¤é€šå› ç´ **ï¼šå½“å‰æ¨¡å‹ä»…åŸºäºå‡ ä½•ä¸æ‹“æ‰‘ä¿¡æ¯ï¼Œæœªå¼•å…¥å®æ—¶äº¤é€šæµã€é™é€Ÿç­‰è¯­ä¹‰ç‰¹å¾ã€‚
- **DiT ç»“æ„è¾ƒé‡**ï¼šå°½ç®¡æ¨ç†é«˜æ•ˆï¼Œä½†åœ¨æ›´å¤§è§„æ¨¡åŸå¸‚éƒ¨ç½²æ—¶ä»éœ€ä¼˜åŒ–å†…å­˜å ç”¨ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- ğŸ”„ **æ‰©å±•è‡³ trajectory recovery**ï¼šåˆ©ç”¨ conditioned diffusion ç”Ÿæˆå¯†é›†è·¯å¾„åºåˆ—ï¼Œå®ç°ä»ç¨€ç–åˆ°ç¨ å¯†çš„è½¨è¿¹é‡å»ºã€‚
- ğŸŒ **å¼•å…¥åŠ¨æ€è·¯ç½‘çŠ¶æ€**ï¼šç»“åˆ traffic flowã€congestion ç­‰ä¿¡æ¯ä½œä¸ºé¢å¤–æ¡ä»¶è¾“å…¥ã€‚
- âš™ï¸ **è½»é‡åŒ–è®¾è®¡**ï¼šæ¢ç´¢æ›´é«˜æ•ˆçš„ backboneï¼ˆå¦‚ MobileNet-style DiTï¼‰ä»¥æ”¯æŒç§»åŠ¨ç«¯éƒ¨ç½²ã€‚
- ğŸ§© **å¤šæ¨¡æ€èåˆ**ï¼šé›†æˆæ‰‹æœºä¼ æ„Ÿå™¨ï¼ˆIMUï¼‰ã€è§†è§‰ä¿¡æ¯ç­‰è¾…åŠ©å®šä½ä¿¡å·ã€‚

---

> ğŸ”— **ä»£ç å¼€æºåœ°å€**ï¼š[https://github.com/decisionintelligence/DiffMM](https://github.com/decisionintelligence/DiffMM)

</details>

---

### 2. [MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm](https://arxiv.org/abs/2601.08800)

**Authors**: Bowen Zhou, Jinrui Jia, Wenhao He, Yong Zhang, Fang Dong  
**Category**: cs.DC  
**Published**: 2026-01-14  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.08800v1  

#### Abstract
The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node & multi-GPU based serving systems. Thus, communi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# MixServe è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

éšç€ Mixture of Experts (MoE) æ¨¡å‹è§„æ¨¡ä¸æ–­å¢å¤§ï¼ˆæ•°åäº¿è‡³ä¸‡äº¿å‚æ•°ï¼‰ï¼Œå…¶éƒ¨ç½²ä¾èµ–äºå¤šGPUç”šè‡³å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼ç³»ç»Ÿã€‚ç„¶è€Œï¼Œåœ¨æ­¤ç±»ç³»ç»Ÿä¸­ï¼Œ**é€šä¿¡å¼€é”€æˆä¸ºæ¨ç†æ€§èƒ½çš„å…³é”®ç“¶é¢ˆ**ï¼Œå°¤å…¶æ˜¯è·¨èŠ‚ç‚¹ï¼ˆinter-nodeï¼‰é€šä¿¡ã€‚

å½“å‰ä¸»æµçš„å¹¶è¡Œç­–ç•¥å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

- **Tensor Parallelism (TP)**ï¼šåŸºäº All-Reduce (AR) çš„ TP åœ¨å•èŠ‚ç‚¹å†…é«˜æ•ˆï¼Œä½†è·¨èŠ‚ç‚¹æ‰©å±•æ€§å·®ï¼Œå—é™äºä½å¸¦å®½ inter-node è¿æ¥ã€‚
- **Expert Parallelism (EP)**ï¼šåŸºäº All-to-All (A2A) çš„ EP èƒ½æ›´å¥½åœ°è·¨èŠ‚ç‚¹æ‰©å±•ï¼Œä½†å®¹æ˜“å¯¼è‡´è´Ÿè½½ä¸å‡è¡¡ï¼Œå°¤å…¶åœ¨é«˜å¹¶è¡Œåº¦ä¸‹æ•ˆç‡ä¸‹é™ã€‚
- ç¼ºä¹å¯¹æ¨¡å‹è¶…å‚æ•°ã€ç½‘ç»œæ‹“æ‰‘å’Œç¡¬ä»¶èµ„æºçš„ç³»ç»ŸåŒ–åˆ†æï¼Œç°æœ‰ç­–ç•¥å¤šä¾èµ–ç»éªŒè€Œéç†è®ºæŒ‡å¯¼ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **MixServe**ï¼Œä¸€ä¸ªé¢å‘ MoE æ¨¡å‹çš„è‡ªåŠ¨åˆ†å¸ƒå¼æ¨ç†æœåŠ¡ç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### âœ… è‡ªåŠ¨åŒ–å¹¶è¡Œç­–ç•¥é€‰æ‹©ç³»ç»Ÿ
- åœ¨ç¦»çº¿é˜¶æ®µç»¼åˆè€ƒè™‘æ¨¡å‹è¶…å‚æ•°ï¼ˆå¦‚å±‚æ•°ã€éšè—ç»´åº¦ã€ä¸“å®¶æ•°ï¼‰ã€ç¡¬ä»¶é…ç½®ï¼ˆè®¡ç®—èƒ½åŠ›ã€å†…å­˜ï¼‰å’Œç½‘ç»œæ‹“æ‰‘ï¼ˆintra-node / inter-node å¸¦å®½ï¼‰ï¼Œé€šè¿‡ç†è®ºå»ºæ¨¡ä¸å®æµ‹åˆ†æç›¸ç»“åˆçš„æ–¹å¼ï¼Œ**è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„æ··åˆå¹¶è¡Œç­–ç•¥**ï¼ˆTP/EP/DPç»„åˆï¼‰ã€‚

#### âœ… TP-EP æ··åˆå¹¶è¡Œè®¾è®¡ï¼ˆHybrid TP-EPï¼‰
- å°† MoE å—ä¸­çš„ TP å’Œ EP è§£è€¦ï¼š**MoE æƒé‡é‡‡ç”¨ intra-node TP + inter-node EP**ï¼ŒAttention å—åˆ™ä½¿ç”¨ intra-node TP + inter-node DPã€‚
- å¼•å…¥ç»†ç²’åº¦ä¼˜åŒ–ï¼šå°† AR æ“ä½œæ‹†åˆ†ä¸º Reduce Scatter (RS) å’Œ All Gather (AG)ï¼Œå¹¶ä¸ A2A é‡æ–°ç»„ç»‡ä¸º **RS-A2A-AG æˆ– AG-Dispatch æµç¨‹**ï¼Œå®ç°é€šä¿¡é‡å ã€‚

#### âœ… èåˆé€šä¿¡ç®—æ³•ï¼ˆFused AR-A2A Communication Algorithmï¼‰
- æå‡ºä¸¤ç§æ–°å‹èåˆé€šä¿¡ç®—æ³•ï¼š
  - **Fused RS-Combine**ï¼šé‡å  intra-node RS ä¸ inter-node A2Aï¼ˆCombine é˜¶æ®µï¼‰
  - **Fused AG-Dispatch**ï¼šé‡å  intra-node AG ä¸ inter-node A2Aï¼ˆDispatch é˜¶æ®µï¼‰
- åˆ©ç”¨å¼‚æ­¥é€šä¿¡æœºåˆ¶ï¼Œä½¿ intra-node å’Œ inter-node é€šä¿¡å¹¶è¡Œæ‰§è¡Œï¼Œæ˜¾è‘—é™ä½æ€»é€šä¿¡å»¶è¿Ÿã€‚

#### âœ… ç†è®ºé©±åŠ¨çš„æ€§èƒ½å»ºæ¨¡
- æ„å»ºäº†å®Œæ•´çš„é€šä¿¡ä¸è®¡ç®—å»¶è¿Ÿæ¨¡å‹ï¼Œæ¶µç›– TTFTã€ITLã€ååé‡ç­‰æŒ‡æ ‡ï¼Œå¹¶å¼•å…¥æ’é˜Ÿè®ºæ¨¡å‹ï¼ˆM/M/1ï¼‰ä¼°è®¡è¯·æ±‚é˜Ÿåˆ—å»¶è¿Ÿï¼Œæå‡é¢„æµ‹å‡†ç¡®æ€§ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **çµæ´»æ€§** | ä¸å†å›ºå®šä½¿ç”¨çº¯ EP æˆ–çº¯ TPï¼Œè€Œæ˜¯æ ¹æ®ç¡¬ä»¶åŠ¨æ€é€‰æ‹©æœ€ä¼˜ç­–ç•¥ |
| **æ•ˆç‡** | æ˜¾è‘—å‡å°‘è·¨èŠ‚ç‚¹é€šä¿¡é‡ï¼Œå……åˆ†åˆ©ç”¨å¸¦å®½å±‚çº§ç»“æ„ï¼ˆhigh intra-node vs low inter-nodeï¼‰ |
| **è‡ªåŠ¨åŒ–** | æ›¿ä»£äººå·¥è°ƒä¼˜ï¼Œé€‚åº”ä¸åŒæ¨¡å‹æ¶æ„ä¸é›†ç¾¤ç¯å¢ƒ |
| **æ€§èƒ½æå‡** | åœ¨ TTFTã€ITL å’Œååé‡ä¸Šå…¨é¢è¶…è¶Šä¸»æµåŸºçº¿ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- **ShareGPT-V3**ï¼šå¤§è§„æ¨¡äººç±»å¯¹è¯æ•°æ®é›†ï¼ŒåŒ…å«çº¦ 12 äº¿ tokensï¼Œç”¨äºç”ŸæˆçœŸå®è¯·æ±‚è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

### **å®éªŒè®¾ç½®**

#### ç¡¬ä»¶å¹³å°

| å¹³å° | èŠ‚ç‚¹æ•° | è®¾å¤‡ | intra-node å¸¦å®½ | inter-node å¸¦å®½ |
|------|--------|-------|------------------|------------------|
| H20 Cluster | 2 nodes | 8Ã—Nvidia H20 GPUs (96GB) | NVLink 4.0 (900 GB/s) | InfiniBand (400 Gbps) |
| Ascend 910B Cluster | 4 nodes | 8Ã—Ascend 910B NPUs (64GB) | HCCS (480 Gbps) | RoCE (200 Gbps) |

#### æ¨¡å‹

- **DeepSeek-R1**ï¼š671B å‚æ•° MoE æ¨¡å‹ï¼Œ256 ä¸ªè·¯ç”±ä¸“å®¶ + 1 å…±äº«ä¸“å®¶ï¼Œæ¯ token æ¿€æ´» 37B å‚æ•°
- **Qwen3-235B-A22B**ï¼š235B å‚æ•° MoE æ¨¡å‹ï¼Œ128 ä¸“å®¶ï¼Œæ¯ token æ¿€æ´» 22B å‚æ•°

#### è¯·æ±‚è´Ÿè½½
- è¯·æ±‚é€Ÿç‡ï¼š2, 4, 8 req/s
- æœ€å¤§æ‰¹å¤§å°ï¼š16
- æœ€å¤§åºåˆ—é•¿åº¦ï¼š4096 tokens

---

### **è¯„ä¼°æŒ‡æ ‡**

| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **TTFT (Time to First Token)** | æ”¶åˆ°è¯·æ±‚åç”Ÿæˆç¬¬ä¸€ä¸ª token æ‰€éœ€æ—¶é—´ï¼Œåæ˜  Prefill é˜¶æ®µæ€§èƒ½ |
| **ITL (Inter-Token Latency)** | ç›¸é‚»ä¸¤ä¸ªè¾“å‡º token ä¹‹é—´çš„å¹³å‡é—´éš”ï¼Œåæ˜  Decode é˜¶æ®µæ€§èƒ½ |
| **Throughput (token/s)** | å•ä½æ—¶é—´å†…ç”Ÿæˆçš„ token æ•°é‡ï¼Œè¡¡é‡ç³»ç»Ÿæ•´ä½“ååèƒ½åŠ› |
| **P99 Latency** | 99% çš„è¯·æ±‚æ»¡è¶³çš„æ—¶é—´ä¸Šé™ï¼Œä½“ç°ç¨³å®šæ€§ |

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| åŸºçº¿ | å¹¶è¡Œç­–ç•¥è¯´æ˜ |
|------|--------------|
| **vLLM** | æ”¯æŒ TP+PPï¼ˆé€šç”¨LLMï¼‰ã€DP+EPï¼ˆMoEï¼‰ï¼›æµ‹è¯•å¤šç§ TP åº¦ï¼ˆ4/8ï¼‰ |
| **Tutel** | æ”¯æŒ TP+EP æ··åˆå¹¶è¡Œï¼Œä½†åœ¨ Ascend ä¸Šä¸æ”¯æŒéƒ¨åˆ†é…ç½® |

å…·ä½“é…ç½®è§åŸæ–‡ Table IIIã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆvs. åŸºçº¿ï¼‰**

| æŒ‡æ ‡ | æå‡èŒƒå›´ | è¯¦ç»†è¡¨ç° |
|------|---------|----------|
| **TTFT åŠ é€Ÿæ¯”** | **1.08Ã— ~ 3.80Ã—** | 
| | | - åœ¨ Ascend 910B ä¸Šï¼ŒDeepSeek-R1 è¾¾åˆ° **2.67Ã—** åŠ é€Ÿï¼ˆvs vLLM TP+PPï¼‰<br>- Qwen3 è¾¾åˆ° **3.80Ã—** åŠ é€Ÿï¼ˆvs vLLM TP+PPï¼‰<br>- H20 ä¸Šä¹Ÿæœ‰ **1.08Ã—~1.23Ã—** æå‡ |
| **ITL åŠ é€Ÿæ¯”** | **1.03Ã— ~ 1.66Ã—** |
| | | - DeepSeek-R1ï¼šä» 227.33ms â†’ 160.06ms (**1.42Ã—**) <br>- Qwen3ï¼šä» 134.27ms â†’ 81.1ms (**1.66Ã—**) |
| **ååé‡æå‡** | **+5.2% ~ +50.3%** |
| | | - Ascend 910Bï¼š<br>ã€€â€¢ DeepSeek-R1: +22.0% (100.61 â†’ 122.72 token/s)<br>ã€€â€¢ Qwen3: +32.2% (113.52 â†’ 150.08 token/s)<br>- H20ï¼š<br>ã€€â€¢ DeepSeek-R1: **+50.3%** (362.78 â†’ 545.23 token/s)<br>ã€€â€¢ Qwen3: **+43.5%** (435.82 â†’ 625.45 token/s) |

> æ³¨ï¼šæ‰€æœ‰ç»“æœå‡ä¸ºå¤šæ¬¡è¿è¡Œå‡å€¼ï¼Œè¯¯å·®æ¡è¡¨ç¤ºæ ‡å‡†å·®ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

#### ï¼ˆ1ï¼‰DP ä¸ EP çš„æƒè¡¡åˆ†æï¼ˆdDP vs dEPï¼‰

| è®¾ç½® | æ€§èƒ½è¶‹åŠ¿ |
|------|--------|
| **dDP = dEP**ï¼ˆå¹³è¡¡ï¼‰ | åœ¨ Ascend 910B ä¸Šè¡¨ç°æœ€ä½³ï¼ˆæœ€ä½ TTFTï¼Œæœ€é«˜ååï¼‰ |
| **dDP < dEP** | åœ¨ H20ï¼ˆé«˜NVLinkå¸¦å®½ï¼‰ç¯å¢ƒä¸‹æ›´ä¼˜ï¼Œå› å¯å‡å°‘é€šä¿¡å†—ä½™ |
| **dDP > dEP** | å¯¼è‡´ä¸“å®¶æƒé‡å†—ä½™ï¼Œå¢åŠ å†…å­˜å‹åŠ›ï¼Œæ€§èƒ½ä¸‹é™ |

ğŸ‘‰ ç»“è®ºï¼šMixServe çš„è‡ªåŠ¨åˆ†æå™¨å¯æ ¹æ®ç¡¬ä»¶ç‰¹æ€§è‡ªé€‚åº”é€‰æ‹©æœ€ä¼˜ dDP/dEP ç»„åˆã€‚

#### ï¼ˆ2ï¼‰é€šä¿¡é‡å çš„å½±å“ï¼ˆSync vs Asyncï¼‰

| å¯¹æ¯”é¡¹ | åŒæ­¥é€šä¿¡ï¼ˆSyncï¼‰ | å¼‚æ­¥èåˆé€šä¿¡ï¼ˆAsyncï¼‰ |
|--------|------------------|------------------------|
| TTFT | è¾ƒé«˜ | æ˜¾è‘—é™ä½ï¼ˆâ‰ˆå‡å°‘ inter-node é€šä¿¡æ—¶é—´ï¼‰ |
| ITL | è¾ƒé«˜ | æ›´ä½ |
| åå | è¾ƒä½ | æå‡æ˜æ˜¾ |

- Gantt å›¾æ˜¾ç¤ºï¼šå¼‚æ­¥æ¨¡å¼æˆåŠŸå®ç°äº† intra-node AG/RS ä¸ inter-node A2A çš„**æ—¶é—´é‡å **ï¼Œæœ‰æ•ˆâ€œéšè—â€äº†éƒ¨åˆ†é€šä¿¡å»¶è¿Ÿã€‚
- æ—¶é—´å¤æ‚åº¦ä»ä¸º O(n_node)ï¼Œä½†å®é™…å»¶è¿Ÿæ¥è¿‘æœ€å°å¯èƒ½å€¼ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **é€šä¿¡æ˜¯ MoE æ¨ç†çš„æ ¸å¿ƒç“¶é¢ˆ**ï¼Œç‰¹åˆ«æ˜¯è·¨èŠ‚ç‚¹ A2A å’Œ AR æ“ä½œï¼›
2. **è§£è€¦ intra-node ä¸ inter-node é€šä¿¡** æ˜¯ä¼˜åŒ–çš„å…³é”®è·¯å¾„ï¼›
3. **æ··åˆ TP-EP æ¯”çº¯ EP æˆ–çº¯ TP æ›´å…·æ½œåŠ›**ï¼Œå°¤å…¶æ˜¯åœ¨å¼‚æ„å¸¦å®½ç¯å¢ƒä¸­ï¼›
4. **èåˆé€šä¿¡ç®—æ³•ï¼ˆFused AR-A2Aï¼‰èƒ½æœ‰æ•ˆé‡å é€šä¿¡æ“ä½œ**ï¼Œæ˜¾è‘—é™ä½ç«¯åˆ°ç«¯å»¶è¿Ÿï¼›
5. **è‡ªåŠ¨åŒ–çš„ç­–ç•¥é€‰æ‹©ä¼˜äºç»éªŒé…ç½®**ï¼Œèƒ½å¤Ÿé€‚é…ä¸åŒæ¨¡å‹ä¸ç¡¬ä»¶ç»„åˆï¼›
6. **MixServe åœ¨å¤šç§ç¡¬ä»¶å¹³å°ï¼ˆH20 / Ascendï¼‰å’Œæ¨¡å‹ï¼ˆDeepSeek-R1 / Qwen3ï¼‰ä¸Šå‡å–å¾—ä¸€è‡´ä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡**ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

1. **å‡è®¾ç¨³å®šçš„ç½‘ç»œç¯å¢ƒ**ï¼šæœªå……åˆ†è€ƒè™‘ç½‘ç»œæŠ–åŠ¨æˆ–æ‹¥å¡åœºæ™¯ä¸‹çš„é²æ£’æ€§ï¼›
2. **ä¾èµ–å‡†ç¡®çš„ Profiling æ•°æ®**ï¼šè‹¥åˆå§‹ profiling ä¸å‡†ï¼Œå¯èƒ½å¯¼è‡´ç­–ç•¥è¯¯åˆ¤ï¼›
3. **ç›®å‰èšç„¦äºæ¨ç†é˜¶æ®µ**ï¼šè®­ç»ƒåœºæ™¯å°šæœªè¦†ç›–ï¼›
4. **å¯¹ç‰¹å®šç¡¬ä»¶æ‹“æ‰‘æ•æ„Ÿ**ï¼šä¾‹å¦‚ RoCE/NVLink çš„å®é™…åˆ©ç”¨ç‡å—åº•å±‚é©±åŠ¨å½±å“ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **æ”¯æŒåŠ¨æ€è°ƒæ•´å¹¶è¡Œç­–ç•¥**ï¼šåœ¨è¿è¡Œæ—¶æ ¹æ®è´Ÿè½½å˜åŒ–å®æ—¶åˆ‡æ¢ TP/EP é…ç½®ï¼›
2. **æ‰©å±•è‡³æ›´å¤šå¹¶è¡Œç»´åº¦**ï¼šé›†æˆ Pipeline Parallelism (PP)ã€Context Parallelism (CP) å½¢æˆ 4D/5D å¹¶è¡Œï¼›
3. **ç»“åˆ Prefill/Decode åˆ†ç¦»è°ƒåº¦**ï¼ˆå¦‚ DistServeï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡ï¼›
4. **æ¢ç´¢è½»é‡åŒ–é€šä¿¡åŸè¯­**ï¼šå¦‚å‹ç¼© A2A æ•°æ®ã€ç¨€ç–åŒ–ä¼ è¾“ï¼›
5. **åº”ç”¨äºæ›´å¤§è§„æ¨¡ MoE æ¨¡å‹**ï¼ˆå¦‚ä¸‡äº¿çº§ï¼‰éªŒè¯å¯æ‰©å±•æ€§ã€‚

---

> ğŸ”š **æ€»ç»“**ï¼š  
> MixServe æ˜¯é¦–ä¸ªå°†**ç†è®ºå»ºæ¨¡ + è‡ªåŠ¨åŒ–å†³ç­– + èåˆé€šä¿¡ä¼˜åŒ–**ç»“åˆçš„ MoE æ¨ç†æœåŠ¡ç³»ç»Ÿï¼Œçªç ´äº†ä¼ ç»Ÿ EP/TP çš„å±€é™ï¼Œæ˜¾è‘—æå‡äº† MoE æ¨¡å‹åœ¨çœŸå®åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„æ¨ç†æ•ˆç‡ï¼Œä¸ºå¤§è§„æ¨¡ LLM çš„å®ç”¨åŒ–éƒ¨ç½²æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ã€‚

</details>

---

### 3. [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)

**Authors**: Abhijnan Nath, Alireza Bagheri Garakani, Tianchen Zhou, Fan Yang, Nikhil Krishnaswamy  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.08403v1  

#### Abstract
Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when mo...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨èç³»ç»Ÿç­‰ç”Ÿæˆå¼æœç´¢ä»»åŠ¡ä¸­æ—¥ç›Šé‡è¦ï¼Œä½†ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å¦‚ **Group Relative Policy Optimization (GRPO)** å­˜åœ¨ä»¥ä¸‹å…³é”®ç¼ºé™·ï¼š

- **ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼ˆCredit Assignment Gapï¼‰**ï¼šGRPO ä»…åŸºäºåºåˆ—çº§å¥–åŠ±ï¼ˆsequence-level rewardsï¼‰ï¼Œå°†å•ä¸€ä¼˜åŠ¿å€¼ï¼ˆadvantageï¼‰å‡åŒ€åˆ†é…ç»™æ•´ä¸ªå“åº”çš„æ‰€æœ‰ tokenï¼Œæ— æ³•è¯†åˆ«å“ªäº›å…·ä½“è¯ç»„æˆ–æ¨ç†ç‰‡æ®µçœŸæ­£æ¨åŠ¨äº†æˆåŠŸã€‚
- **ç¼ºä¹å¯è§£é‡Šæ€§ä¸æ•ˆç‡**ï¼šç”±äºæ— æ³•åŒºåˆ†é«˜ä»·å€¼ä¸ä½ä»·å€¼ tokenï¼Œè®­ç»ƒè¿‡ç¨‹å®¹æ˜“è¿‡æ‹Ÿåˆäºè¡¨é¢æ¨¡å¼ï¼Œä¸”éš¾ä»¥è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–ã€‚
- **æ˜“å—å¥–åŠ±æ¬ºéª—ï¼ˆReward Hackingï¼‰**ï¼šæ¨¡å‹å¯èƒ½å­¦ä¼šåˆ©ç”¨é»‘ç®±æ£€ç´¢å™¨çš„æµ…å±‚ä¿¡å·è€ŒéçœŸå®è¯­ä¹‰æ¥æå‡è¡¨ç°ã€‚

è¿™äº›é—®é¢˜åœ¨ç”¨æˆ·æ„å›¾æ¨¡ç³Šã€æ— æ˜ç¡®æ ‡ç­¾çš„åœºæ™¯ä¸‹å°¤ä¸ºä¸¥é‡ï¼Œä¾‹å¦‚ä¸ªæ€§åŒ–å•†å“æœç´¢ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **Owen-Shapley Policy Optimization (OSPO)**ï¼Œä¸€ç§åŸºäºåˆä½œåšå¼ˆè®ºçš„æ–°å‹ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- å°†ç”Ÿæˆæ–‡æœ¬ä¸­çš„**è¯­ä¹‰è¿è´¯å•å…ƒ**ï¼ˆå¦‚æè¿°äº§å“å±æ€§çš„çŸ­è¯­æˆ–è¡¨è¾¾åå¥½çš„å¥å­ï¼‰è§†ä¸ºâ€œç©å®¶â€ï¼ˆplayersï¼‰ï¼Œç»„æˆâ€œè”ç›Ÿâ€ï¼ˆcoalitionsï¼‰ã€‚
- åˆ©ç”¨ **Owen-Shapley values**ï¼ˆOwen å€¼ï¼‰è®¡ç®—æ¯ä¸ªè¯­ä¹‰æ®µè½åœ¨æœ€ç»ˆå¥–åŠ±ä¸­çš„**è¾¹é™…è´¡çŒ®**ï¼ˆmarginal contributionï¼‰ï¼Œä»è€Œå®ç°ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…ã€‚
- é€šè¿‡ **Potential-Based Reward Shaping (PBRS)** å°†è¿™äº›æ®µè½çº§å½’å› æ˜ å°„åˆ° token çº§ä¼˜åŠ¿å€¼ï¼ŒæŒ‡å¯¼æ¢¯åº¦æ›´æ–°ã€‚

#### **å…³é”®æŠ€æœ¯äº®ç‚¹**ï¼š
- **æ— éœ€é¢å¤–ä»·å€¼æ¨¡å‹**ï¼šç›´æ¥ä»ä»»åŠ¡åé¦ˆï¼ˆå¦‚æ£€ç´¢å™¨è¿”å›çš„ NDCGï¼‰ä¸­è·å–ä¼˜åŠ¿å€¼ï¼Œé¿å…å¼•å…¥å‚æ•°åŒ– critic æˆ– reward modelã€‚
- **è¿ç»­æ€§çº¦æŸï¼ˆContiguity Constraintï¼‰**ï¼šåªè€ƒè™‘è¿ç»­çš„è¯­ä¹‰æ®µä½œä¸ºè”ç›Ÿï¼Œä¿è¯æŸ¥è¯¢è¯­ä¹‰å®Œæ•´æ€§ï¼Œå¹¶å°†è®¡ç®—å¤æ‚åº¦ä» $O(2^N)$ é™è‡³ $O(N \cdot w_{\text{max}})$ï¼Œä½¿å…¶é€‚ç”¨äºåœ¨çº¿ RL è®­ç»ƒã€‚
- **é•¿åº¦ä¸å˜æ€§ä¼˜åŠ¿é‡åˆ†é…**ï¼šé€šè¿‡ä¹˜ä»¥åºåˆ—é•¿åº¦ $T$ æ¥æ¶ˆé™¤é•¿åºåˆ—æ¢¯åº¦ç¨€é‡Šé—®é¢˜ï¼Œç¡®ä¿å¹³å‡ token ä¼˜åŠ¿ç­‰äºåŸå§‹åºåˆ—çº§ä¼˜åŠ¿ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| æ–¹æ³• | æ˜¯å¦éœ€è¦ Value Model | ä¿¡ç”¨åˆ†é…ç²’åº¦ | å¯è§£é‡Šæ€§ | æŠ—å¥–åŠ±æ¬ºéª—èƒ½åŠ› |
|------|------------------------|---------------|-----------|------------------|
| GRPO / PPO | âŒ | Token-level (uniform) | ä½ | å¼± |
| DPO | âœ… (éšå¼) | Sequence-level | ä¸­ | ä¸­ |
| Process Reward Models | âœ… | Step-level | é«˜ | è¾ƒå¼º |
| SCAR (Shapley-based) | âœ… | Segment-level | é«˜ | è¾ƒå¼º |
| **OSPO (æœ¬æ–‡)** | âŒ | **Segment-level (Owen)** | **é«˜** | **å¼º** |

- **æ›´é«˜æ•ˆçš„å­¦ä¹ **ï¼šèšç„¦äºçœŸæ­£å½±å“ç»“æœçš„ tokenï¼Œå‡å°‘æ— æ•ˆæ›´æ–°ã€‚
- **æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›**ï¼šå¯¹æœªè§è¿‡çš„æ£€ç´¢å™¨å…·æœ‰é²æ£’æ€§ã€‚
- **æ›´é«˜çš„æ ·æœ¬æ•ˆç‡**ï¼šå®éªŒæ˜¾ç¤ºè¾¾åˆ°ç›®æ ‡æ€§èƒ½æ‰€éœ€è®­ç»ƒæ­¥æ•°å‡å°‘çº¦ 50%ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

| æ•°æ®é›† | ä»»åŠ¡ç±»å‹ | æè¿° |
|--------|----------|------|
| **ESCI Shopping Queries** (Reddy et al., 2022) | å•†å“æœç´¢æŸ¥è¯¢æ‰©å±• | åŒ…å«çœŸå®ç”¨æˆ·è´­ç‰©æŸ¥è¯¢åŠæ ‡æ³¨çš„ç›¸å…³æ€§æ ‡ç­¾ï¼Œç”¨äºä¼ ç»Ÿæœç´¢ä»»åŠ¡ã€‚ |
| **H&M Fashion Recommendations** (Ling et al., 2022) | ä¸Šä¸‹æ–‡åŒ–å•†å“æœç´¢ & ç”¨æˆ·ç”»åƒæ‘˜è¦ç”Ÿæˆ | åŒ…å«ä¸°å¯Œçš„äº¤æ˜“å†å²ï¼Œä½†æ— æ˜¾å¼æŸ¥è¯¢ï¼›éœ€ç”±ä¸“å®¶æ¨¡å‹ï¼ˆClaude Sonnet 3.0ï¼‰ç”ŸæˆåˆæˆæŸ¥è¯¢å’Œæ‘˜è¦ç”¨äºè®­ç»ƒã€‚ |

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### **ä»»åŠ¡è®¾è®¡**
- **Product Search Query Refinement**ï¼šLLM æ¥æ”¶åŸå§‹æ¨¡ç³ŠæŸ¥è¯¢ï¼Œè¾“å‡ºæ‰©å±•åçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œäº¤ç”± FAISS + åŸŸç‰¹å®šç¼–ç å™¨ï¼ˆALL-MPNET-BASE-V2 / SIMCSE-LARGEï¼‰è¿›è¡Œå¯†é›†æ£€ç´¢ã€‚
- **User Profile Summarization**ï¼šLLM åŸºäºè´­ä¹°å†å²ç”Ÿæˆ Chain-of-Thought é£æ ¼çš„æ‘˜è¦ï¼Œå†é¢„æµ‹ä¸‹ä¸€è´­ä¹°ç‰©å“é¡ºåºã€‚

#### **è¯„ä¼°æŒ‡æ ‡**
| ä»»åŠ¡ | ä¸»è¦æŒ‡æ ‡ |
|------|---------|
| å•†å“æœç´¢ | **NDCG**, **AP**, **MRR**, **Recall** |
| ç”¨æˆ·ç”»åƒæ‘˜è¦ | **Pairwise Win Rate (WR)** ä½¿ç”¨ Qwen-3-Nemotron-32B-Reward æ¨¡å‹ä½œä¸ºè£åˆ¤ |

#### **å®ç°ç»†èŠ‚**
- **åŸºç¡€æ¨¡å‹**ï¼šQwen2.5-Instruct 7B
- **æ¯æç¤ºé‡‡æ ·æ•°ï¼ˆGroup Size Gï¼‰**ï¼š8
- **æœ€å¤§è”ç›Ÿå®½åº¦ $w_{\text{max}}$**ï¼š8
- **æ¯åºåˆ—é‡‡æ ·è”ç›Ÿæ•° M**ï¼š64
- **è®­ç»ƒæ­¥æ•°**ï¼š2000
- **å¥–åŠ±æ¥æº**ï¼š
  - æœç´¢ä»»åŠ¡ï¼šFAISS æ£€ç´¢ç»“æœçš„ NDCG@1000
  - æ‘˜è¦ä»»åŠ¡ï¼šBradley-Terry æ¨¡å‹æ‰“åˆ†ï¼ˆ90%ï¼‰+ æ ¼å¼æ­£ç¡®æ€§å¥–åŠ±ï¼ˆ10%ï¼‰

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| åŸºçº¿æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|---------|------|------|
| **SFT** | ç›‘ç£å¾®è°ƒ | ä½¿ç”¨é«˜è´¨é‡ä¸“å®¶æ ·æœ¬è®­ç»ƒ |
| **DPO** | ç¦»çº¿åå¥½ä¼˜åŒ– | ä½¿ç”¨ä¸“å®¶å¯¹æ¯”å¯¹è¿›è¡Œåå¥½å­¦ä¹  |
| **GRPO** | åœ¨çº¿ RLï¼ˆæ— å€¼å‡½æ•°ï¼‰ | å½“å‰ä¸»æµçš„ value-model-free RL æ–¹æ³•ï¼Œä½œä¸ºä¸»è¦å¯¹æ¯”åŸºå‡† |
| **OSPO-Prop** | æœ¬æ–‡æ–¹æ³•ï¼ˆæ¯”ä¾‹é‡åˆ†é…ï¼‰ | ä½¿ç”¨ Owen å€¼æŒ‰æ¯”ä¾‹é‡åˆ†é…ä¼˜åŠ¿ |
| **OSPO-Rank** | æœ¬æ–‡æ–¹æ³•ï¼ˆæ’åé‡åˆ†é…ï¼‰ | å°† Owen å€¼è½¬ä¸ºæ’ååé‡åˆ†é…ï¼Œå¢å¼ºé²æ£’æ€§ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **è¡¨ 1ï¼šå•†å“æœç´¢ä»»åŠ¡æ€§èƒ½ï¼ˆNDCGï¼‰**

| æ–¹æ³• | ESCI NDCG | H&M NDCG |
|------|------------|----------|
| Qwen2.5-72B-Instruct | 0.543 | 0.357 |
| **OSPO-Prop (7B)** | **0.522** | **0.436** |
| GRPO (7B) | 0.418 | 0.379 |
| DPO (7B) | 0.431 | 0.396 |
| SFT (7B) | 0.398 | 0.373 |

> ğŸ’¡ **ç»“è®º**ï¼šOSPO-Propï¼ˆ7Bï¼‰æ¥è¿‘ç”šè‡³è¶…è¶Š 72B å¤§æ¨¡å‹çš„è¡¨ç°ï¼Œåœ¨ H&M ä¸Šæ˜¾è‘—ä¼˜äºæ‰€æœ‰å…¶ä»– 7B æ¨¡å‹ã€‚

---

#### **è¡¨ 2ï¼šç”¨æˆ·ç”»åƒæ‘˜è¦ç”Ÿæˆçš„æˆå¯¹èƒœç‡ï¼ˆWin Rateï¼‰**

| å¯¹æ¯” | OSPO-Prop WR | OSPO-Rank WR |
|------|--------------|-------------|
| vs DPO | 49.6% | 45.8% |
| vs GRPO | 49.1% | 47.6% |
| vs SFT | 53.3% | 54.0% |

> ğŸ’¡ **ç»“è®º**ï¼šOSPO åœ¨ä¸­é—´æ¨ç†è´¨é‡ä¸Šæ˜æ˜¾ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œå°¤å…¶åœ¨ä¸ SFT çš„æ¯”è¾ƒä¸­ä¼˜åŠ¿æ˜¾è‘—ï¼ˆ+16.8~24.4 ä¸ªç™¾åˆ†ç‚¹ï¼‰ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

- **ç›¸æ¯” GRPO**ï¼š
  - ESCI ä¸Š NDCG æå‡ **24.9%**ï¼ˆ0.522 vs 0.418ï¼‰
  - H&M ä¸Š NDCG æå‡ **15.0%**ï¼ˆ0.436 vs 0.379ï¼‰
  - è®­ç»ƒç¨³å®šæ€§æ›´é«˜ï¼Œæ— ç¾éš¾æ€§å´©æºƒç°è±¡
- **ç›¸æ¯” DPO/SFT**ï¼š
  - æ˜¾è‘—ä¼˜äºç¦»çº¿æ–¹æ³•ï¼Œè¯´æ˜**æ£€ç´¢é©±åŠ¨çš„åœ¨çº¿åé¦ˆ**å¯¹äºç”Ÿæˆä»»åŠ¡è‡³å…³é‡è¦
- **è·¨è§„æ¨¡å¯¹æ¯”**ï¼š
  - OSPO-Prop (7B) > Qwen-32B > æ¥è¿‘ Qwen-72B
  - è¡¨æ˜**ç²¾ç»†ä¿¡ç”¨åˆ†é…æ¯”å•çº¯æ‰©å¤§å‚æ•°è§„æ¨¡æ›´æœ‰æ•ˆ**

---

### **æ¶ˆèå®éªŒç»“æœ**

#### **è”ç›Ÿç»“æ„çš„å½±å“ï¼ˆå›¾ 2 & è¡¨ 5ï¼‰**
- **æœ€ä½³é…ç½®**ï¼šä¸­ç­‰å®½åº¦ï¼ˆ$w=4\sim8$ï¼‰ã€è¶³å¤Ÿé‡‡æ ·æ•°ï¼ˆ$p=48\sim96$ï¼‰
- **è¿‡çª„è”ç›Ÿ**ï¼ˆ$w=1,2$ï¼‰ï¼šå¿«é€Ÿæ”¶æ•›ä½†åæœŸæ€§èƒ½å´©å¡Œï¼Œå› å™ªå£°å¤§ã€ä¸Šä¸‹æ–‡ä¸è¶³
- **è¿‡å®½è”ç›Ÿ**ï¼ˆ$w=12,16$ï¼‰ï¼šæ— æ³•å……åˆ†åˆ©ç”¨ï¼ˆå¤šæ•°æŸ¥è¯¢åªæœ‰ 6â€“10 ä¸ªç‰‡æ®µï¼‰ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™
- **éè¿ç»­è”ç›Ÿï¼ˆAll Subsetsï¼‰**ï¼šæ€§èƒ½æ€¥å‰§ä¸‹é™è‡³ NDCGâ‰ˆ0.11ï¼ŒéªŒè¯äº†**è¿ç»­æ€§å‡è®¾çš„é‡è¦æ€§**

#### **æ¢ç´¢æ·±åº¦ï¼ˆGeneration æ•°é‡ï¼‰**
- ä» 8 æ¬¡ç”Ÿæˆé™åˆ° 2 æ¬¡ï¼Œå‡†ç¡®ç‡å¤§å¹…ä¸‹é™ï¼Œè¯´æ˜**å……åˆ†çš„ Monte Carlo é‡‡æ ·å¯¹ç¨³å®šä¼˜åŠ¿ä¼°è®¡è‡³å…³é‡è¦**

#### **ä¼˜åŠ¿é‡åˆ†é…æ–¹å¼**
- **OSPO-Prop**ï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¡¨ç°æœ€å¥½
- **OSPO-Rank**ï¼šåœ¨é•¿åºåˆ—æˆ–å™ªå£°è¾ƒå¤§æ—¶æ›´å…·é²æ£’æ€§ï¼Œä½œä¸ºæœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆ

#### **å¥–åŠ±æ“çºµæ£€æµ‹**
- æŸ¥è¯¢é•¿åº¦ä¸å¥–åŠ±çš„ç›¸å…³ç³»æ•°ä»…ä¸º **-0.165**ï¼Œè¡¨æ˜ OSPO **æ²¡æœ‰æ˜æ˜¾çš„å¥–åŠ±æ¬ºéª—è¡Œä¸º**ï¼ˆå¦åˆ™åº”ä¸ºæ­£ç›¸å…³ï¼‰

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. âœ… **ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…æ˜¾è‘—æå‡æ€§èƒ½**ï¼šé€šè¿‡ Owen-Shapley å½’å› æœºåˆ¶ï¼ŒOSPO èƒ½ç²¾å‡†è¯†åˆ«æ¨åŠ¨ä¸‹æ¸¸ä»»åŠ¡æˆåŠŸçš„è¯­ä¹‰å•å…ƒï¼Œå®ç°æ›´é«˜æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚
2. âœ… **æ— éœ€å€¼å‡½æ•°ä¹Ÿèƒ½å®ç°å¯†é›†å¥–åŠ±å¡‘é€ **ï¼šåˆ©ç”¨ PBRS å’Œè”ç›Ÿé‡‡æ ·ï¼Œå®ç°äº†ç±»ä¼¼ dense reward çš„æ•ˆæœï¼ŒåŒæ—¶ä¿æŒç®—æ³•ç®€æ´ã€‚
3. âœ… **å°æ¨¡å‹å¯åª²ç¾å¤§æ¨¡å‹è¡¨ç°**ï¼šOSPO-Prop (7B) åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ¥è¿‘ç”šè‡³è¶…è¿‡ 32B å’Œ 72B æ¨¡å‹ï¼Œè¯æ˜**å¯¹é½è´¨é‡æ¯”æ¨¡å‹å¤§å°æ›´é‡è¦**ã€‚
4. âœ… **å“è¶Šçš„æ³›åŒ–èƒ½åŠ›**ï¼š
   - åœ¨æµ‹è¯•æ—¶é¢å¯¹æœªè§è¿‡çš„æ£€ç´¢å™¨ï¼ˆcross-retriever evaluationï¼‰ï¼ŒOSPO æ€§èƒ½ä¸‹é™å¹…åº¦è¿œå°äº GRPOã€‚
   - è¡¨æ˜å…¶å­¦åˆ°çš„æ˜¯**å¯è¿ç§»çš„ã€æ£€ç´¢å™¨æ— å…³çš„å¯¹é½ä¿¡å·**ã€‚
5. âœ… **ç¨³å®šçš„è®­ç»ƒåŠ¨æ€**ï¼šç›¸æ¯” GRPO å‡ºç°çš„â€œç¾éš¾æ€§é—å¿˜â€ï¼ŒOSPO ç»´æŒè¾ƒé•¿çš„æ¨ç†é“¾å’Œè¾ƒé«˜çš„å¥–åŠ±æ–¹å·®ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

- **è®¡ç®—å¼€é”€å¢åŠ **ï¼šéœ€å¤šæ¬¡è°ƒç”¨æ£€ç´¢å™¨/å¥–åŠ±æ¨¡å‹ä»¥è¯„ä¼°ä¸åŒè”ç›Ÿï¼Œå¸¦æ¥é¢å¤–å»¶è¿Ÿï¼ˆå°¤å…¶åœ¨ä½¿ç”¨æ…¢é€Ÿ reward model æ—¶ï¼‰ã€‚
- **è¶…å‚æ•°æ•æ„Ÿ**ï¼šæ€§èƒ½ä¾èµ–äº $w_{\text{max}}$ å’Œ $M$ çš„é€‰æ‹©ï¼Œéœ€ä»”ç»†è°ƒå‚ã€‚
- **è¿‘ä¼¼è¯¯å·®**ï¼šMonte Carlo é‡‡æ ·å¯èƒ½å¯¼è‡´é•¿è·ç¦»ä¾èµ–è¢«ä½ä¼°ã€‚
- **å½“å‰ä»…é™å•è½®äº¤äº’**ï¼šå°šæœªæ‰©å±•åˆ°å¤šè½®å¯¹è¯æˆ–ä»£ç†ç¯å¢ƒã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

- æ‰©å±•è‡³ **multi-turn, agentic environments**ï¼ˆå¦‚å¤šè½®æœç´¢ã€ä»£ç ç”Ÿæˆã€å¯¹è¯è§„åˆ’ï¼‰
- æ¢ç´¢æ›´é«˜æ•ˆçš„è”ç›Ÿé‡‡æ ·ç­–ç•¥ï¼ˆå¦‚é‡è¦æ€§é‡‡æ ·ï¼‰
- åº”ç”¨äºæ›´å¤šæ¨ç†å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è§£é¢˜ã€æ³•å¾‹åˆ†æï¼‰
- ç»“åˆå¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾æ–‡è”åˆæ¨èï¼‰

---

> ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **OSPO é€šè¿‡å¼•å…¥ Owen-Shapley å½’å› æœºåˆ¶ï¼Œåœ¨ä¸ä¾èµ–å€¼å‡½æ•°çš„å‰æä¸‹å®ç°äº†ç»†ç²’åº¦ã€å¯è§£é‡Šã€é«˜æ•ˆç‡çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº† LLM åœ¨ç”Ÿæˆå¼æœç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸é²æ£’æ€§ï¼Œä¸ºâ€œå°æ¨¡å‹å¤§èƒ½åŠ›â€çš„å¯¹é½è·¯å¾„æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚**

</details>

---

### 4. [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)

**Authors**: Zenghua Liao, Jinzhi Liao, Xiang Zhao  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.08653v1  

#### Abstract
Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approach...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šPrism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰ Large Language Models (LLMs) åœ¨å¤„ç†ç”¨æˆ·æ„å›¾æ—¶ï¼Œé€šå¸¸å‡è®¾æ¾„æ¸…é—®é¢˜æ˜¯**ç‹¬ç«‹ä¸”å¹¶è¡Œçš„**ï¼ˆå¦‚ Mistral-Interact å’Œ ITIU æ‰€é‡‡ç”¨çš„æ–¹æ³•ï¼‰ï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­ï¼Œç”¨æˆ·çš„æ„å›¾å¾€å¾€æ˜¯**å¤æ‚ä¸”å…·æœ‰é€»è¾‘ä¾èµ–å…³ç³»çš„**ã€‚ä¾‹å¦‚ï¼Œåœ¨è§„åˆ’æ—…è¡Œæ—¶ï¼Œâ€œæ´»åŠ¨å»ºè®®â€å¿…é¡»åœ¨ç¡®å®šâ€œç›®çš„åœ°â€ä¹‹åæ‰èƒ½åˆç†æå‡ºã€‚

è¿™ç§å¿½ç•¥é€»è¾‘ä¾èµ–çš„åšæ³•å¯¼è‡´ï¼š
- äº§ç”Ÿä¸è¿è´¯ç”šè‡³è’è°¬çš„å»ºè®®ï¼ˆå¦‚â€œ12æœˆåœ¨å†²ç»³æ½œæ°´â€ï¼Œè€Œå½“åœ°å¤©æ°”å¯’å†·ï¼‰ï¼›
- å¢åŠ ç”¨æˆ·çš„**è®¤çŸ¥è´Ÿè·**ï¼ˆCognitive Loadï¼‰ï¼Œè¿«ä½¿ç”¨æˆ·è‡ªè¡Œç®¡ç†å¤šä¸ªæœªå†³å˜é‡ä¹‹é—´çš„ä¾èµ–ï¼›
- é™ä½ä»»åŠ¡å®Œæˆæ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚

å› æ­¤ï¼Œæœ¬æ–‡æŒ‡å‡ºï¼š**å»ºæ¨¡æ¾„æ¸…é—®é¢˜é—´çš„é€»è¾‘ä¾èµ–æ˜¯å®ç°é«˜æ•ˆå¤æ‚æ„å›¾ç†è§£çš„å…³é”®æŒ‘æˆ˜**ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šPrism æ¡†æ¶

å—**è®¤çŸ¥è´Ÿè·ç†è®º**ï¼ˆCognitive Load Theory, CLTï¼‰å¯å‘ï¼Œä½œè€…æå‡ºäº† **Prism** â€”â€” ä¸€ä¸ªé€šè¿‡**é€»è¾‘åŒ–æ„å›¾æ¾„æ¸…**æ¥é™ä½ç”¨æˆ·è®¤çŸ¥è´Ÿæ‹…çš„æ–°å‹æ¡†æ¶ã€‚

#### æ ¸å¿ƒæ€æƒ³ç±»æ¯”
å°†å¤æ‚çš„ç”¨æˆ·æ„å›¾è§†ä¸ºâ€œç™½å…‰â€ï¼ŒPrism åˆ©ç”¨â€œæ£±é•œâ€ï¼ˆå³é€»è¾‘é©±åŠ¨çš„ schemaï¼‰å°†å…¶åˆ†è§£ä¸ºæœ‰åºçš„â€œå½©è‰²å…‰è°±â€ï¼ˆåˆ†å±‚å…ƒç´ ï¼‰ï¼Œä»è€ŒæŒ‡å¯¼æ¾„æ¸…é¡ºåºã€‚

#### å››å¤§æ¨¡å—è®¾è®¡
| æ¨¡å— | åŠŸèƒ½ |
|------|------|
| **â‘  Complex Intent Decomposition** | æ„å»º CID æ•°æ®é›†ï¼Œæ”¯æŒè·¨é¢†åŸŸå¤æ‚æ„å›¾çš„ç»“æ„åŒ–è§£æ„ï¼Œå¹¶æ˜¾å¼æ ‡æ³¨å…ƒç´ é—´å…ˆå†³ä¾èµ–å…³ç³» |
| **â‘¡ Logical Clarification Generation** | æ ¹æ®ä¾èµ–å±‚çº§ç»„ç»‡æ¾„æ¸…æµç¨‹ï¼šç‹¬ç«‹é—®é¢˜å¹¶è¡Œæé—®ï¼ˆäº¤äº’è¡¨æ ¼ï¼‰ï¼Œä¾èµ–é—®é¢˜ä¸²è¡Œè¿›è¡Œ |
| **â‘¢ Intent-Aware Reward Module** | è®¾è®¡åŸºäº token-level çš„å¥–åŠ±å‡½æ•°ï¼Œç»“åˆæ„å›¾é‡è¦æ€§å’Œç”Ÿæˆç½®ä¿¡åº¦ï¼Œç”¨äºè¯„ä¼°æ¾„æ¸…è½¨è¿¹è´¨é‡ |
| **â‘£ Self-Evolved Intent Tuning** | åŸºäºé«˜è´¨é‡æ¨¡æ‹Ÿäº¤äº’æ•°æ®è¿­ä»£ä¼˜åŒ–æ¨¡å‹ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯æå‡é€»è¾‘æ¾„æ¸…èƒ½åŠ› |

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ä¼˜åŠ¿è¯´æ˜ |
|------|----------|
| **é€»è¾‘ä¸€è‡´æ€§æ›´å¼º** | æ˜¾å¼å»ºæ¨¡æ¾„æ¸…é—®é¢˜é—´çš„ä¾èµ–ï¼Œé¿å…ä¸åˆé€»è¾‘çš„æ¨è |
| **ç”¨æˆ·è®¤çŸ¥è´Ÿè·æ›´ä½** | å‡å°‘å†—ä½™æ¨ç†è´Ÿæ‹…ï¼Œæå‡äº¤äº’æµç•…æ€§ |
| **æ³›åŒ–èƒ½åŠ›å¼º** | æ”¯æŒ 27 ä¸ªé¢†åŸŸã€429 ç§æ„å›¾ï¼Œé€‚ç”¨äºå¤šæ ·åŒ–çš„å¤æ‚ä»»åŠ¡ |
| **å¯æ‰©å±•æ€§å¼º** | è‡ªæ¼”åŒ–è®­ç»ƒæœºåˆ¶ï¼ˆself-evolved tuningï¼‰å®ç°é«˜è´¨é‡æ•°æ®è‡ªåŠ¨ç”Ÿæˆ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

| æ•°æ®é›† | æè¿° |
|-------|------|
| **TIN** [14] | åŒ…å« 200 æ¡æµ‹è¯•æ ·æœ¬ï¼Œç”± GPT-4 è¾…åŠ©äººå·¥æ ‡æ³¨ï¼Œæ¶µç›–å¤šè½®æ¾„æ¸…è¿‡ç¨‹ |
| **IN3** [21] | å« 108 æ¡æµ‹è¯•æ ·æœ¬ï¼Œå¼ºè°ƒæ›´å¤šæ¾„æ¸…è½®æ¬¡ä¸ç»†èŠ‚æŒ–æ˜ |
| **ABP** [36] | åŒ…å« 319 æ¡æµ‹è¯•æ ·æœ¬ï¼Œè¦†ç›–å¹¿æ³›é¢†åŸŸå’Œå¤šæ ·åŒ–æ„å›¾ |

æ‰€æœ‰æµ‹è¯•é›†å‡æŒ‰â€œç®€å•æ„å›¾â€ä¸â€œå¤æ‚æ„å›¾â€åˆ†ç±»ï¼ˆå€ŸåŠ© LLM è¾…åŠ©åˆ¤æ–­ï¼‰ï¼Œä»¥éªŒè¯æ–¹æ³•åœ¨ä¸åŒéš¾åº¦ä¸‹çš„è¡¨ç°ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®

- **ä¸»å¹²æ¨¡å‹**ï¼š
  - `Mistral-7B-Instruct-v0.3`
  - `LLaMA-3.1-8B-Instruct`

- **Prism å˜ä½“**ï¼š
  - **Prism-SFT**ï¼šç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰
  - **Prism-DPO**ï¼šç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimizationï¼‰

- **è®­ç»ƒæ–¹å¼**ï¼š
  - ä½¿ç”¨ Monte Carlo Sampling + ç”¨æˆ·æ¨¡æ‹Ÿå™¨ç”Ÿæˆå¤§è§„æ¨¡æ¾„æ¸…å¯¹è¯ï¼›
  - åŸºäº Intent-Aware Reward è¿‡æ»¤é«˜è´¨é‡è½¨è¿¹ç”¨äºè®­ç»ƒï¼›
  - å¤šè½®è‡ªæ¼”åŒ–ï¼ˆ3 è½®ï¼‰ï¼Œæ¯è½®ä½¿ç”¨ä¸Šä¸€è½®ä¼˜åŒ–åçš„æ¨¡å‹ä½œä¸ºç­–ç•¥æ¨¡å‹ç”Ÿæˆæ›´ä¼˜æ•°æ®ã€‚

- **ç¡¬ä»¶èµ„æº**ï¼š8Ã—80GB A100 GPUï¼Œæ€»è®­ç»ƒè€—æ—¶çº¦ 14.5 å°æ—¶ã€‚

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡

#### ï¼ˆ1ï¼‰æ¾„æ¸…äº¤äº’è¯„ä¼°ï¼ˆClarification Interactionï¼‰
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **Intents Cover Rate (%)** | æˆåŠŸè¦†ç›–çœŸå®æ‰€éœ€æ¾„æ¸…é¡¹çš„æ¯”ä¾‹ |
| **Logical Conflict Rate (%)** | æ¾„æ¸…é—®é¢˜è¿åå‰ç½®ä¾èµ–çš„æ¯”ä¾‹ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ |
| **Average Interaction Turns** | å¹³å‡äº¤äº’è½®æ•°ï¼ˆè¶Šå°‘è¶Šå¥½ï¼‰ |
| **Average Questions Per Turn** | æ¯è½®å¹³å‡æé—®æ•°é‡ï¼ˆåæ˜ æ•ˆç‡ï¼‰ |
| **Options Presenting Rate (%)** | æä¾›å‚è€ƒé€‰é¡¹çš„æ¯”ä¾‹ |
| **Options Reasonable Rate (%)** | æä¾›é€‰é¡¹åˆç†çš„æ¯”ä¾‹ |
| **Average Options Per Question** | æ¯ä¸ªé—®é¢˜æä¾›çš„å¹³å‡é€‰é¡¹æ•° |

#### ï¼ˆ2ï¼‰æ„å›¾æ‰§è¡Œè¯„ä¼°ï¼ˆIntent Executionï¼‰
é›†æˆåˆ° XAgent æ¡†æ¶ä¸­ï¼Œè¡¡é‡ä¸‹æ¸¸ä»»åŠ¡æ‰§è¡Œæ•ˆæœï¼š
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **IBLEU** | è¾“å‡ºä¸å‚è€ƒç­”æ¡ˆçš„ BLEU åˆ†æ•° |
| **Faithful (%)** | æœ€ç»ˆè¾“å‡ºæ˜¯å¦å®Œæ•´æ»¡è¶³æ¾„æ¸…åæ„å›¾ï¼ˆæ— é—æ¼/å¹»è§‰ï¼‰ |
| **Unnecessary Sub-tasks (US)** | ä¸å¿…è¦çš„å­ä»»åŠ¡å æ¯” |
| **General Sub-tasks (GS)** | è¿‡äºå®½æ³›è€Œéå…·ä½“çš„å­ä»»åŠ¡å æ¯” |
| **Tool Invocations (TI)** | å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œåæ˜ æ‰§è¡Œæ•ˆç‡ |

#### ï¼ˆ3ï¼‰è®¤çŸ¥è´Ÿè·è¯„ä¼°ï¼ˆCognitive Loadï¼‰
ç»¼åˆä¸‰ç±»æµ‹é‡ï¼š
- **è¡Œä¸ºå±‚é¢**ï¼šä»»åŠ¡è€—æ—¶ã€token æ•°é‡
- **ä¸»è§‚å±‚é¢**ï¼šç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†ï¼ˆ1â€“10ï¼‰
- **ç”Ÿç†å±‚é¢**ï¼šEEG è„‘ç”µå›¾åˆ†æï¼Œè®¡ç®— Power Spectral Density (PSD)ï¼Œåæ˜ å¤§è„‘æ´»è·ƒç¨‹åº¦ï¼ˆè¶Šé«˜è¡¨ç¤ºè®¤çŸ¥è´Ÿè·è¶Šå¤§ï¼‰

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | æ˜¯å¦è€ƒè™‘é€»è¾‘ä¾èµ– |
|------|------|------------------|
| **Mistral-Interact** [21] | Q&A å¯¹è¯å¼æ¾„æ¸… | âŒ |
| **ITIU** [14] | è¡¨æ ¼å½¢å¼å¹¶è¡Œæé—® | âŒï¼ˆè™½é«˜æ•ˆä½†æ— è§†ä¾èµ–ï¼‰ |
| **CoLLABLLM** [32] | å¤šè½®å¥–åŠ±å»ºæ¨¡å¢å¼ºå‚ä¸æ„Ÿ | âŒ |
| **Prism (Ours)** | å±‚çº§ä¾èµ–é©±åŠ¨çš„é€»è¾‘æ¾„æ¸… | âœ… |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»ï¼ˆæ¥è‡ª Table 1 & 2ï¼‰

| æŒ‡æ ‡ | Prism-DPO è¡¨ç° | æå‡å¹…åº¦ï¼ˆvs. Baselineï¼‰ |
|------|----------------|------------------------|
| **Logical Conflict Rate â†“** | **11.5%**ï¼ˆTIN-Complexï¼‰ | ç›¸æ¯” Mistral-Interact (-28pp)ï¼ŒITIU (-41.5pp) |
| **User Satisfaction â†‘** | **8.35 / 10** | +14.4% vs. Mistral-Interact |
| **Task Completion Time â†“** | **å‡å°‘ 34.8%** | vs. Mistral-Interactï¼›-22.3% vs. CoLLABLLM |
| **Interaction Turns â†“** | **~1.6 è½®**ï¼ˆå¤æ‚åœºæ™¯ï¼‰ | æ˜¾è‘—ä½äº Mistral-Interact (~5.6 è½®) |
| **Intent Cover Rate â†‘** | **71.98%**ï¼ˆLLaMA + TINï¼‰ | é«˜äºæ‰€æœ‰åŸºçº¿ |
| **Faithful Intent Execution â†‘** | **53.7% â†’ 53.1%**ï¼ˆå¤æ‚åœºæ™¯ï¼‰ | æå‡é€»è¾‘è§„åˆ’å‡†ç¡®æ€§ |
| **Unnecessary Sub-tasks â†“** | **2.27 â†’ 2.33**ï¼ˆMistralï¼‰ | ä¸‹æ¸¸ agent æ›´ç²¾å‡†æ‹†è§£ä»»åŠ¡ |
| **Tool Invocations â†“** | **3.04 â†’ 4.37**ï¼ˆMistralï¼‰ | æ‰§è¡Œæ•ˆç‡æ˜¾è‘—æé«˜ |

> æ³¨ï¼šä»¥ä¸Šæ•°æ®åŸºäº `LLaMA-3.1-8B-Instruct` ä¸ºä¸»å¹²æ¨¡å‹çš„ç»“æœã€‚

---

### ğŸ” ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

#### ï¼ˆ1ï¼‰åœ¨å¤æ‚æ„å›¾åœºæ™¯ä¸‹ä¼˜åŠ¿æ˜æ˜¾
- å½“ä»ç®€å•æ„å›¾è½¬å‘å¤æ‚æ„å›¾æ—¶ï¼š
  - Mistral-Interact çš„ Intent Cover Rate ä¸‹é™ **30.1%**
  - CoLLABLLM ä¸‹é™ **32.1%**
  - Prism-DPO ä»…ä¸‹é™ **17.7%**

â†’ è¡¨æ˜ Prism å¯¹å¤æ‚æ„å›¾æ›´å…·é²æ£’æ€§ã€‚

#### ï¼ˆ2ï¼‰ç»´æŒé«˜æ•ˆç‡çš„åŒæ—¶ä¿è¯é€»è¾‘æ€§
- ITIU è™½ç„¶äº¤äº’è½®æ¬¡å°‘ï¼ˆ~1.8ï¼‰ï¼Œä½† **Logical Conflict Rate é«˜è¾¾ 51.5%**
- Prism åœ¨ä¿æŒç›¸è¿‘äº¤äº’è½®æ¬¡ï¼ˆ~1.6ï¼‰çš„å‰æä¸‹ï¼Œå°†å†²çªç‡é™è‡³ **11.5%**

â†’ å®ç°äº†**æ•ˆç‡ä¸é€»è¾‘çš„ä¸€è‡´æ€§å¹³è¡¡**ã€‚

#### ï¼ˆ3ï¼‰è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›å¼º
åœ¨ TINã€IN3ã€ABP ä¸Šå‡å–å¾— SOTA è¡¨ç°ï¼ˆè§ Table 7 & 8ï¼‰ï¼Œè¡¨æ˜å…¶å…·å¤‡è‰¯å¥½çš„**è·¨åŸŸè¿ç§»èƒ½åŠ›**ã€‚

---

### ğŸ”§ æ¶ˆèå®éªŒï¼ˆéšå«åˆ†æï¼‰

è™½ç„¶æ–‡ä¸­æœªå•ç‹¬åˆ—å‡ºæ¶ˆèè¡¨ï¼Œä½†ä»æ¨¡å—è®¾è®¡å’Œ ablation-style åˆ†æå¯è§ï¼š

- **å¼•å…¥ CID æ•°æ®é›†å’Œå±‚çº§åˆ†è§£** â†’ æ˜¾è‘—é™ä½ Logical Conflict Rate
- **Intent-Aware Reward + Monte Carlo Sampling** â†’ æå‡è®­ç»ƒæ•°æ®è´¨é‡å’Œæ¾„æ¸…è½¨è¿¹åˆç†æ€§
- **Self-Evolved Tuning** â†’ æ¯ä¸€è½®è¿­ä»£éƒ½èƒ½ç”Ÿæˆæ›´é«˜ IRï¼ˆintent-aware rewardï¼‰çš„æ•°æ®ï¼Œå½¢æˆæ­£å‘åé¦ˆå¾ªç¯

æ­¤å¤–ï¼ŒFigure 5 æ˜¾ç¤ºï¼š
> åœ¨å¤æ‚æ„å›¾åœºæ™¯ä¸­ï¼ŒMistral-Interact å’Œ ITIU çš„æ€§èƒ½å¢ç›Šåˆ†åˆ«ä» 18.8%/24.8% ä¸‹é™åˆ° 6.2%/12.4%ï¼Œè¯æ˜å®ƒä»¬éš¾ä»¥åº”å¯¹ä¾èµ–ç»“æ„ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **é€»è¾‘ä¾èµ–å»ºæ¨¡è‡³å…³é‡è¦**  
   å¿½è§†æ¾„æ¸…é—®é¢˜é—´çš„ä¾èµ–ä¼šå¯¼è‡´ä¸¥é‡çš„é€»è¾‘å†²çªå’Œç”¨æˆ·ä½“éªŒä¸‹é™ï¼Œè¿™æ˜¯ç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯å¤±æ•ˆçš„æ ¹æœ¬åŸå› ã€‚

2. **Prism æ˜¾è‘—é™ä½ç”¨æˆ·è®¤çŸ¥è´Ÿè·**  
   - ä»»åŠ¡æ—¶é—´å‡å°‘ **34.8%**
   - ç”¨æˆ·è¯„åˆ†æå‡è‡³ **8.35**
   - EEG æ•°æ®æ˜¾ç¤º PSD æœ€ä½ï¼Œè¡¨æ˜å¤§è„‘è´Ÿæ‹…æœ€å°

3. **å®ç°äº†â€œä½æ‘©æ“¦â€çš„äººæœºåä½œæ¨¡å¼**  
   é€šè¿‡ç»“æ„åŒ–åˆ†è§£ + ä¾èµ–æ„ŸçŸ¥æé—®ï¼ŒPrism å¼•å¯¼ç”¨æˆ·è‡ªç„¶åœ°å®Œæˆæ„å›¾è¡¨è¾¾ï¼Œè€Œä¸æ˜¯è®©ç”¨æˆ·è‡ªå·±ç†æ¸…æ€è·¯ã€‚

4. **è‡ªæ¼”åŒ–è®­ç»ƒæœ‰æ•ˆæ›¿ä»£äººå·¥æ ‡æ³¨**  
   åˆ©ç”¨ GPT-4 ç”Ÿæˆåˆå§‹é«˜è´¨é‡æ•°æ®ï¼Œåç»­ç”±æ¨¡å‹è‡ªæˆ‘è¿›åŒ–ï¼Œå¤§å¹…é™ä½å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

| å±€é™ | è¯´æ˜ |
|------|------|
| **ä¾èµ– CID æ„å»ºè´¨é‡** | è‹¥æ–°é¢†åŸŸç¼ºä¹è¶³å¤Ÿç›¸ä¼¼æ„å›¾ï¼Œfew-shot æ„é€ å¯èƒ½ä¸å‡† |
| **ç”¨æˆ·æ¨¡æ‹Ÿä»æœ‰åå·®** | å°½ç®¡ä½¿ç”¨äº† user simulatorï¼Œä½†ä»æ— æ³•å®Œå…¨å¤ç°çœŸå®äººç±»è¡Œä¸ºå¤šæ ·æ€§ |
| **å®æ—¶æ€§è¦æ±‚è¾ƒé«˜** | å¤šè½® Monte Carlo é‡‡æ ·å’Œå±‚çº§æ¨ç†å¯èƒ½å¢åŠ å“åº”å»¶è¿Ÿ |
| **æœªå¼€æ”¾å…¨éƒ¨è®­ç»ƒæ•°æ®** | è™½å£°æ˜ä»£ç å¼€æºï¼ˆGitHub: [liaozenghua/Prism](https://github.com/liaozenghua/Prism)ï¼‰ï¼Œä½†éƒ¨åˆ†è®­ç»ƒæ•°æ®æœªå…¬å¼€ |

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **åŠ¨æ€æ›´æ–° CID**  
   æ”¯æŒåœ¨çº¿å­¦ä¹ æ–°é¢†åŸŸçš„æ„å›¾ç»“æ„ï¼Œå¢å¼ºé€‚åº”æ€§ã€‚

2. **è½»é‡åŒ–æ¨ç†æœºåˆ¶**  
   æ¢ç´¢æ›´é«˜æ•ˆçš„é‡‡æ ·ç­–ç•¥ï¼ˆå¦‚ beam search æ›¿ä»£ full MC samplingï¼‰ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ã€‚

3. **å¤šæ¨¡æ€æ„å›¾ç†è§£æ‰©å±•**  
   å°† Prism æ€è·¯åº”ç”¨äºå›¾åƒã€è¯­éŸ³ç­‰å¤šæ¨¡æ€è¾“å…¥åœºæ™¯ã€‚

4. **ä¸ªæ€§åŒ–ä¾èµ–å»ºæ¨¡**  
   å…è®¸ä¸åŒç”¨æˆ·æœ‰ä¸åŒçš„åå¥½ä¾èµ–è·¯å¾„ï¼ˆå¦‚æœ‰äººå…ˆå®šé¢„ç®—å†é€‰åœ°ç‚¹ï¼‰ã€‚

---

## æ€»ç»“

ğŸ“Œ **Prism æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§è§£å†³ LLM ä¸­å¤æ‚æ„å›¾ç†è§£ä¸­â€œé€»è¾‘ä¾èµ–â€é—®é¢˜çš„å·¥ä½œ**ï¼Œå…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºï¼š

- æå‡º **CID æ•°æ®é›†** å’Œ **å››æ¨¡å—ååŒæ¡†æ¶**ï¼Œå®ç°ç»“æ„åŒ–ã€ä¾èµ–æ„ŸçŸ¥çš„æ¾„æ¸…æµç¨‹ï¼›
- åŸºäº **Cognitive Load Theory** è®¾è®¡äº¤äº’æœºåˆ¶ï¼ŒçœŸæ­£ä»ç”¨æˆ·ä½“éªŒå‡ºå‘ï¼›
- å®éªŒå…¨é¢ï¼Œæ¶µç›–äº¤äº’è´¨é‡ã€ä»»åŠ¡æ‰§è¡Œã€ç”Ÿç†ä¿¡å·ä¸‰ä¸ªç»´åº¦ï¼›
- åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¾¾åˆ° SOTAï¼Œæ˜¾è‘—é™ä½é€»è¾‘å†²çªï¼ˆâ†“è‡³ 11.5%ï¼‰ã€æå‡ç”¨æˆ·æ»¡æ„åº¦ï¼ˆâ†‘14.4%ï¼‰ã€ç¼©çŸ­ä»»åŠ¡æ—¶é—´ï¼ˆâ†“34.8%ï¼‰ã€‚

ğŸ¯ **æ„ä¹‰**ï¼šæ¨åŠ¨ LLM ä»â€œè¢«åŠ¨åº”ç­”è€…â€å‘â€œä¸»åŠ¨ã€æœ‰æ¡ç†çš„åä½œè€…â€æ¼”è¿›ï¼Œè¿ˆå‘çœŸæ­£çš„æ™ºèƒ½äººæœºå…±äº‹æ—¶ä»£ã€‚

</details>

---

### 5. [Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering](https://arxiv.org/abs/2601.08427)

**Authors**: Nonghai Zhang, Weitao Ma, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Jingwen Xu  
**Category**: cs.CL  
**Published**: 2026-01-14  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.08427v1  

#### Abstract
Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å½“å‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›å¢å¼ºæ–¹æ³•ï¼ˆå¦‚ GRPOï¼‰ä¸¥é‡ä¾èµ–æ˜‚è´µçš„å¤–éƒ¨éªŒè¯å™¨ï¼ˆexternal verifiersï¼‰ï¼Œä¾‹å¦‚äººå·¥è§„åˆ™æˆ–å¦ä¸€ä¸ª LLMï¼ˆå¦‚ GPT-4oï¼‰ä½œä¸ºâ€œJudgeâ€æ¥æä¾›å¥–åŠ±ä¿¡å·ã€‚è¿™ç§ä¾èµ–å¸¦æ¥äº†ä»¥ä¸‹é—®é¢˜ï¼š

- **é«˜è®¡ç®—æˆæœ¬ä¸è®­ç»ƒå»¶è¿Ÿ**ï¼šè°ƒç”¨å¤–éƒ¨ LLM éªŒè¯éœ€è¦å¤§é‡ API è¯·æ±‚ï¼Œå¼•å…¥æ˜¾è‘—çš„æ¨ç†å»¶è¿Ÿã€‚
- **ç¨€ç–ä¸”ç¦»æ•£çš„å¥–åŠ±ä¿¡å·**ï¼šå¤§å¤šæ•°éªŒè¯æ–¹å¼ä»…æä¾›äºŒå…ƒå¥–åŠ±ï¼ˆ0/1ï¼‰ï¼Œç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹è´¨é‡çš„ç»†ç²’åº¦åé¦ˆï¼Œå¯¼è‡´ä¼˜åŒ–æ•ˆç‡ä½ä¸‹ã€‚
- **éªŒè¯å™¨åå·®ä¸ä¸ä¸€è‡´æ€§**ï¼šå¤–éƒ¨ Judge å¯èƒ½å­˜åœ¨è¯„åˆ†åå·®æˆ–å™ªå£°ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§ï¼Œç”šè‡³å¼•å‘æ¨¡å‹å´©æºƒï¼ˆmodel collapseï¼‰ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **Latent-GRPO**ï¼Œä¸€ç§æ— éœ€å¤–éƒ¨éªŒè¯å™¨çš„è‡ªéªŒè¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- **ä»éšç©ºé—´å‡ ä½•ç»“æ„ä¸­æå–å†…åœ¨å¥–åŠ±ä¿¡å·**ï¼šé€šè¿‡åˆ†æ LLM åœ¨ç”Ÿæˆæ¨ç†è½¨è¿¹åæœ€åä¸€ä¸ª token çš„ `last hidden state`ï¼Œå‘ç°**æ­£ç¡®æ¨ç†è·¯å¾„çš„ç»ˆç«¯è¡¨ç¤ºåœ¨éšç©ºé—´ä¸­å½¢æˆå¯†é›†èšç±»ï¼ˆdense clustersï¼‰ï¼Œè€Œé”™è¯¯è·¯å¾„åˆ™è¡¨ç°ä¸ºç¦»ç¾¤ç‚¹ï¼ˆoutliersï¼‰**ã€‚
- **æå‡º IRCE ç®—æ³•**ï¼šè®¾è®¡ **Iterative Robust Centroid Estimation (IRCE)** ç®—æ³•ï¼Œé€šè¿‡çƒé¢æŠ•å½±æ¶ˆé™¤å‘é‡æ¨¡é•¿æ³¢åŠ¨ï¼Œå¹¶è¿­ä»£åŠ æƒèšåˆä¼°è®¡ä¸€ä¸ªé²æ£’çš„â€œçœŸç›¸è´¨å¿ƒâ€ï¼ˆtruth centroidï¼‰ï¼Œè¿›è€Œå°†æ¯ä¸ªè½¨è¿¹åˆ°è¯¥è´¨å¿ƒçš„è·ç¦»è½¬åŒ–ä¸º**è¿ç»­ã€ç¨ å¯†çš„å†…åœ¨å¥–åŠ±**ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ•ˆç‡** | æ¶ˆé™¤å¤–éƒ¨éªŒè¯ç“¶é¢ˆï¼Œè®­ç»ƒé€Ÿåº¦æå‡ **2å€ä»¥ä¸Š**ï¼ˆ2Ã— speedupï¼‰ã€‚ |
| **å¥–åŠ±è´¨é‡** | æä¾›**è¿ç»­ã€ç¨ å¯†çš„å¥–åŠ±ä¿¡å·**ï¼Œä¼˜äºä¼ ç»Ÿçš„äºŒå…ƒç¨€ç–å¥–åŠ±ï¼Œæ”¯æŒæ›´ç²¾ç»†çš„æ¢¯åº¦ä¼˜åŒ–ã€‚ |
| **ç¨³å®šæ€§** | å®Œå…¨æ‘†è„±å¯¹å¤–éƒ¨ Judge çš„ä¾èµ–ï¼Œé¿å…å›  Judge ä¸ä¸€è‡´æˆ–å™ªå£°å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šã€‚ |
| **é€šç”¨æ€§** | ä¸ä¾èµ–ä»»åŠ¡ç‰¹å®šè§„åˆ™ï¼Œé€‚ç”¨äºå¼€æ”¾åŸŸå¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ Open-Platypusï¼‰ï¼Œè€Œè§„åˆ™æ–¹æ³•éš¾ä»¥è¦†ç›–ã€‚ |
| **å¯æ‰©å±•æ€§** | å¥–åŠ±è®¡ç®—åŸºäºå·²æœ‰çš„å‰å‘ä¼ æ’­éšè—çŠ¶æ€ï¼Œè®¡ç®—å¼€é”€æå°ï¼ˆO(GTd)ï¼‰ï¼Œè¿œä½äºå¤–éƒ¨ LLM éªŒè¯ï¼ˆO(GL)ï¼‰ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

| æ•°æ®é›† | æè¿° |
|--------|------|
| **GSM8K** | å°å­¦æ•°å­¦åº”ç”¨é¢˜ï¼Œæµ‹è¯•åŸºç¡€å¤šæ­¥ç®—æœ¯æ¨ç†èƒ½åŠ›ã€‚ |
| **MATH** | é«˜ä¸­åŠç«èµ›çº§æ•°å­¦é—®é¢˜ï¼Œéš¾åº¦æ›´é«˜ï¼Œæµ‹è¯•å¤æ‚æ•°å­¦æ¨ç†ã€‚ |
| **Open-Platypus** | å¤šæºæ··åˆæŒ‡ä»¤æ•°æ®é›†ï¼Œæ¶µç›–ç‰©ç†ã€é€»è¾‘ã€æ•°å­¦ç­‰å¤šæ ·åŒ–æ¨ç†ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°æ³›åŒ–èƒ½åŠ›ã€‚ |

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

- **æ¨¡å‹è§„æ¨¡**ï¼šåœ¨ **Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B** ä¸‰ä¸ªå°ºåº¦ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå¹¶åœ¨ **Llama3.2-3B** ä¸Šæµ‹è¯•è·¨æ¨¡å‹æ³›åŒ–ã€‚
- **è®­ç»ƒæ¡†æ¶**ï¼šåŸºäº GRPO æ¶æ„ï¼Œæ¯ç»„ç”Ÿæˆ G=8 æ¡è½¨è¿¹ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Task Accuracy**ï¼šåœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼ˆAccï¼‰ã€‚
  - **Training Efficiency**ï¼šæ¯è½®è®­ç»ƒæ—¶é—´ï¼ˆTime per epochï¼‰ï¼Œè¡¡é‡è®­ç»ƒé€Ÿåº¦ã€‚
- **ç¡¬ä»¶é…ç½®**ï¼šå• GPUï¼Œä½¿ç”¨ bfloat16 æ··åˆç²¾åº¦è®­ç»ƒã€‚

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| æ–¹æ³• | æè¿° |
|------|------|
| **LLM-as-Judge** | ä½¿ç”¨ GPT-4o å¯¹è¾“å‡ºæ–‡æœ¬æ‰“åˆ†ï¼Œæä¾›äºŒå…ƒå¥–åŠ±ï¼ˆ0/1ï¼‰ã€‚ |
| **Rule-based Verification** | ä½¿ç”¨ SymPy ç¬¦å·è®¡ç®—æˆ–æ²™ç®±æ‰§è¡ŒéªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§ï¼Œæä¾›äºŒå…ƒå¥–åŠ±ã€‚ |
| **Latent-GRPO (Ours)** | æœ¬æ–‡æ–¹æ³•ï¼ŒåŸºäºéšç©ºé—´å‡ ä½•ç»“æ„ç”Ÿæˆè¿ç»­å†…åœ¨å¥–åŠ±ã€‚ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰**

#### **GSM8K ç»“æœï¼ˆQwen3-4Bï¼‰**
| æ–¹æ³• | å‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ï¼ˆåˆ†é’Ÿ/epochï¼‰ |
|------|--------|------------------------|
| LLM-as-Judge | 79.87% | 651.45 |
| **Latent-GRPO (Ours)** | **82.34%** | **658.21** |

> âœ… **å‡†ç¡®ç‡æå‡ +2.47%ï¼Œè®­ç»ƒæ—¶é—´ç›¸è¿‘ï¼Œä½†æ— å¤–éƒ¨ä¾èµ–**

#### **MATH ç»“æœï¼ˆQwen3-1.7Bï¼‰**
| æ–¹æ³• | å‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ï¼ˆåˆ†é’Ÿ/epochï¼‰ |
|------|--------|------------------------|
| LLM-as-Judge | 65.77% | 1608.34 |
| **Latent-GRPO (Ours)** | **78.51%** | **811.51** |

> âœ… **å‡†ç¡®ç‡å¤§å¹…æå‡ +12.74%ï¼Œè®­ç»ƒé€Ÿåº¦æå‡è¿‘ 2å€ï¼ˆ1.98Ã—ï¼‰**

#### **Open-Platypus ç»“æœï¼ˆQwen3-4Bï¼‰**
| æ–¹æ³• | å‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ï¼ˆåˆ†é’Ÿ/epochï¼‰ |
|------|--------|------------------------|
| LLM-as-Judge | 65.21% | 3522.18 |
| **Latent-GRPO (Ours)** | **78.06%** | **1632.52** |

> âœ… **å‡†ç¡®ç‡æå‡ +12.85%ï¼Œè®­ç»ƒé€Ÿåº¦æå‡ 2.16Ã—ï¼Œæ•ˆæœæœ€æ˜¾è‘—**

> ğŸ”º æ€»ä½“è¶‹åŠ¿ï¼š**Latent-GRPO åœ¨æ‰€æœ‰æ•°æ®é›†å’Œæ¨¡å‹è§„æ¨¡ä¸Šå‡å®ç° 2Ã— å·¦å³çš„è®­ç»ƒåŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒæˆ–æ˜¾è‘—è¶…è¶ŠåŸºçº¿å‡†ç¡®ç‡**ã€‚

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰**

#### **(1) éšçŠ¶æ€æå–æ–¹å¼å¯¹æ¯”ï¼ˆTable 2ï¼‰**
| æ–¹æ³• | Qwen3-0.6B Acc | Qwen3-1.7B Acc | Qwen3-4B Acc |
|------|----------------|----------------|--------------|
| Mean Pooling | 58.74% | 71.05% | 79.45% |
| Weighted Mean | 57.12% | 69.88% | 78.12% |
| **Last Token (Ours)** | **61.25%** | **73.88%** | **82.34%** |

> âœ… **ä½¿ç”¨æœ€åä¸€ä¸ª token çš„éšçŠ¶æ€æ•ˆæœæœ€ä½³**ï¼Œè¯´æ˜æœ€ç»ˆè¯­ä¹‰æ±‡æ€»æœ€å…·åˆ¤åˆ«åŠ›ã€‚

#### **(2) è´¨å¿ƒä¼°è®¡ç®—æ³•å¯¹æ¯”ï¼ˆTable 3ï¼‰**
| æ–¹æ³• | Qwen3-0.6B Acc | Qwen3-1.7B Acc | Qwen3-4B Acc | æ—¶é—´ |
|------|----------------|----------------|--------------|------|
| Mean Pool | 57.12% | 68.45% | 77.89% | å¿« |
| K-Means | 58.85% | 70.12% | 79.23% | è¾ƒæ…¢ |
| Eigen Centrality | 59.43% | 71.56% | 80.56% | æœ€æ…¢ |
| **IRCE (Ours)** | **61.25%** | **73.88%** | **82.34%** | **å¿«** |

> âœ… **IRCE åœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡ä¸Šå…¨é¢é¢†å…ˆ**ï¼Œè¯æ˜å…¶é²æ£’æ€§å’Œå®ç”¨æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **LLM éšç©ºé—´å¤©ç„¶å…·å¤‡è‡ªæˆ‘éªŒè¯èƒ½åŠ›**ï¼šæ­£ç¡®æ¨ç†è·¯å¾„çš„ç»ˆç«¯éšçŠ¶æ€åœ¨é«˜ç»´ç©ºé—´ä¸­è‡ªç„¶èšç±»ï¼Œå½¢æˆâ€œå…±è¯†æ ¸å¿ƒâ€ï¼Œè€Œé”™è¯¯è·¯å¾„ä¸ºç¦»ç¾¤ç‚¹ã€‚è¿™ä¸€å‡ ä½•ç‰¹æ€§å¯ä½œä¸º**å†…åœ¨å¥–åŠ±ä¿¡å·çš„å¯é æ¥æº**ã€‚
2. **ç¨ å¯†å¥–åŠ±ä¼˜äºç¨€ç–å¥–åŠ±**ï¼šè¿ç»­çš„å‡ ä½•è·ç¦»å¥–åŠ±æä¾›äº†æ›´ä¸°å¯Œçš„ä¼˜åŒ–æ¢¯åº¦ï¼Œæ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚
3. **æ— éœ€å¤–éƒ¨ç›‘ç£å³å¯å®ç°é«˜æ•ˆå¯¹é½**ï¼šLatent-GRPO æˆåŠŸå®ç°äº†â€œself-verifierâ€ï¼Œåœ¨å¤šä¸ªå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Š LLM-as-Judge å’Œè§„åˆ™æ–¹æ³•ã€‚
4. **æ–¹æ³•å…·æœ‰å¼ºæ³›åŒ–æ€§**ï¼š
   - åœ¨ä¸åŒæ¨¡å‹å®¶æ—ï¼ˆQwen å’Œ Llamaï¼‰ä¸Šå‡æœ‰æ•ˆï¼ˆè§ Table 5ï¼‰ã€‚
   - åœ¨æœªè§è¿‡çš„åŸºå‡†ï¼ˆå¦‚ AIME, MMLU, BBHï¼‰ä¸Šä¿æŒæˆ–æå‡èƒ½åŠ›ï¼Œè¡¨æ˜æœªå‘ç”Ÿä»»åŠ¡è¿‡æ‹Ÿåˆï¼ˆè§ Table 4ï¼‰ã€‚

### **å±€é™æ€§**

1. **æ¨¡å‹è§„æ¨¡é™åˆ¶**ï¼šç›®å‰éªŒè¯æœ€å¤§ä¸º 8B å‚æ•°æ¨¡å‹ï¼Œ**åœ¨è¶…å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ 70B+ï¼‰ä¸Šçš„è¡¨ç°å°šå¾…æ¢ç´¢**ã€‚
2. **ç†è®ºåŸºç¡€å°šæµ…**ï¼šè™½ç„¶å®è¯å……åˆ†ï¼Œä½†å…³äºâ€œä¸ºä½•æ­£ç¡®æ¨ç†ä¼šå½¢æˆå‡ ä½•èšç±»â€çš„**å½¢å¼åŒ–æ•°å­¦ç†è®ºä»å¤„äºåˆæ­¥é˜¶æ®µ**ã€‚
3. **å¼€æ”¾ç”Ÿæˆä»»åŠ¡é€‚ç”¨æ€§æœªçŸ¥**ï¼šå½“å‰æ–¹æ³•èšç„¦äºæœ‰æ˜ç¡®ç­”æ¡ˆçš„æ¨ç†ä»»åŠ¡ï¼Œæ˜¯å¦é€‚ç”¨äºåˆ›æ„å†™ä½œç­‰å¼€æ”¾ç”Ÿæˆä»»åŠ¡æœ‰å¾…ç ”ç©¶ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **æ‰©å±•è‡³æ›´å¤§æ¨¡å‹å’Œæ›´å¤šä»»åŠ¡ç±»å‹**ï¼ŒéªŒè¯å…¶å¯æ‰©å±•æ€§ã€‚
2. **å»ºç«‹éšç©ºé—´èšç±»çš„æ•°å­¦ç†è®ºæ¡†æ¶**ï¼Œæ·±åŒ–å¯¹ LLM å†…éƒ¨æœºåˆ¶çš„ç†è§£ã€‚
3. **æ¢ç´¢æ··åˆæ¡†æ¶**ï¼šå°† IRCE çš„å†…åœ¨å¥–åŠ±ä¸ DPO ç­‰ç¦»çº¿å¯¹é½æ–¹æ³•ç»“åˆï¼Œè¿›ä¸€æ­¥ç¨³å®šè‡ªç›‘ç£å¯¹é½è¿‡ç¨‹ã€‚
4. **åº”ç”¨äº Test-Time Compute æ‰©å±•**ï¼Œå¦‚ä¸ Tree-of-Thoughts æˆ– Best-of-N ç»“åˆï¼Œåœ¨æ¨ç†æ—¶åˆ©ç”¨éšç©ºé—´å‡ ä½•è¿›è¡Œé«˜æ•ˆè·¯å¾„ç­›é€‰ã€‚

---

> **æ€»ç»“**ï¼š  
> *Latent-GRPO* é€šè¿‡æŒ–æ˜ LLM è‡ªèº«éšç©ºé—´çš„å‡ ä½•ç»“æ„ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆã€ç¨³å®šã€æ— éœ€å¤–éƒ¨ä¾èµ–çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ã€‚å®ƒä¸ä»…è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„æˆæœ¬ä¸æ•ˆç‡ç“¶é¢ˆï¼Œè¿˜æ­ç¤ºäº† LLM å†…åœ¨çš„â€œè‡ªæˆ‘è¯„åˆ¤â€èƒ½åŠ›ï¼Œä¸ºæœªæ¥çš„ verifier-free åè®­ç»ƒèŒƒå¼æä¾›äº†é‡è¦èŒƒä¾‹ã€‚

</details>

---

### 6. [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)

**Authors**: Bo Wang, Junzhuo Li, Hong Chen, Yuanlin Chu, Yuxuan Fan, Xuming Hu  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.08383v1  

#### Abstract
Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures,...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models â€”â€” æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³å½“å‰å¯¹ **Mixture-of-Experts (MoE)** æ¶æ„åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•è·å–å’Œç»„ç»‡çŸ¥è¯†çš„ç†è§£ä¸è¶³é—®é¢˜ã€‚å°½ç®¡ MoE å› å…¶è®¡ç®—æ•ˆç‡è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†å…¶å†…éƒ¨çŸ¥è¯†åŠ¨æ€æ¼”åŒ–æœºåˆ¶å°šä¸æ˜ç¡®ï¼Œå°¤å…¶ç¼ºä¹ä¸ä¼ ç»Ÿ **dense æ¨¡å‹** åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç³»ç»Ÿæ€§ã€æ—¶é—´åˆ†è¾¨çš„æ¯”è¾ƒã€‚

ç°æœ‰ç ”ç©¶å¤šé›†ä¸­äºï¼š
- åå¤„ç†åˆ†æï¼ˆpost-hoc analysisï¼‰
- é’ˆå¯¹ dense æ¨¡å‹çš„ç¥ç»å…ƒå½’å› æ–¹æ³•

æœ¬æ–‡å¡«è¡¥äº† **â€œæ¶æ„å·®å¼‚å¦‚ä½•å½±å“çŸ¥è¯†è·å–åŠ¨æ€â€** è¿™ä¸€ç©ºç™½ã€‚

---

### ğŸ§© æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯

#### æ–°æ–¹æ³•ï¼šGated-LPIï¼ˆGated Log-Probability Increaseï¼‰
- åŸºäºå·²æœ‰çš„ **Log-Probability Increase (LPI)** æ–¹æ³•è¿›è¡Œæ‰©å±•ã€‚
- é¦–æ¬¡å°†ç¥ç»å…ƒçº§å½’å› åˆ†æå¼•å…¥ **MoE æ¶æ„**ï¼Œæ”¯æŒå¯¹ä»¥ä¸‹ç»„ä»¶çš„è´¡çŒ®åˆ†è§£ï¼š
  - **Feed-Forward Network (FFN)** ä¸­çš„ä¸“å®¶ç¥ç»å…ƒï¼ˆexpert neuronsï¼‰
  - **Attention Heads** ä¸­çš„æ³¨æ„åŠ›ç¥ç»å…ƒï¼ˆattention neuronsï¼‰
- åˆ›æ–°åœ°ç»“åˆäº† **é—¨æ§æƒé‡ï¼ˆgating weightsï¼‰** å’Œç¥ç»å…ƒè¾“å‡ºï¼Œå®ç°å¯¹ç¨€ç–æ¿€æ´»è·¯å¾„ä¸­å„ç¥ç»å…ƒé¢„æµ‹è´¡çŒ®çš„é‡åŒ–ã€‚

#### æ–°è§†è§’ï¼šæ—¶é—´åˆ†è¾¨çš„çŸ¥è¯†åŠ¨æ€è¿½è¸ªï¼ˆTime-Resolved Knowledge Dynamicsï¼‰
- è·Ÿè¸ªæ¨¡å‹ä»è®­ç»ƒåˆæœŸåˆ°åæœŸå¤šä¸ª checkpoint çš„ç¥ç»å…ƒé‡è¦æ€§å˜åŒ–ã€‚
- åˆ†æä¸‰ä¸ªå±‚æ¬¡çš„ç¨³å®šæ€§ä¸åŠŸèƒ½ç‰¹æ€§ï¼š
  1. **Neuron-level stability**ï¼ˆç¥ç»å…ƒçº§ï¼‰
  2. **Layer-level consolidation**ï¼ˆå±‚çº§åˆ«ï¼‰
  3. **Functional robustness**ï¼ˆåŠŸèƒ½é²æ£’æ€§ï¼‰

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **é€‚ç”¨æ€§** | é¦–ä¸ªæ”¯æŒ MoE æ¶æ„çš„ neuron-level attribution æ–¹æ³•ï¼Œçªç ´ dense-only å±€é™ |
| **åŠ¨æ€æ€§** | ä¸ä»…åˆ†ææœ€ç»ˆæ¨¡å‹ï¼Œè¿˜è¿½è¸ªæ•´ä¸ª pre-training è¿‡ç¨‹ä¸­çš„çŸ¥è¯†æ¼”åŒ–è½¨è¿¹ |
| **å¯è§£é‡Šæ€§æ·±åº¦** | æ­ç¤ºäº† MoE å¦‚ä½•é€šè¿‡ç¨€ç–æ€§å½¢æˆç¨³å®šä¸”åˆ†å¸ƒå¼çš„çŸ¥è¯†å­˜å‚¨ç»“æ„ |
| **å®ç”¨æ€§** | ä¸º MoE æ¨¡å‹è®¾è®¡ã€è·¯ç”±ä¼˜åŒ–ã€ä¸“å®¶å‰ªæç­‰æä¾›ç†è®ºä¾æ® |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

- **å…³ç³»äº‹å®æ•°æ®é›†ï¼ˆRelational Facts Datasetï¼‰**
  - æ¥æºäº Hernandez et al. (2023)ï¼Œç”¨äºæ¢æµ‹è¯­è¨€æ¨¡å‹ä¸­çš„çº¿æ€§å…³ç³»ç»“æ„ã€‚
  - åŒ…å« **906 ä¸ªä¸»è°“å¯¹ï¼ˆsubject-object pairsï¼‰**
  - æ¶µç›–å››å¤§ç±»å…± 12 ç§å…³ç³»ï¼š
    - **Linguistic**: `adjective_antonym`, `word_first_letter`, `word_last_letter`
    - **Commonsense**: `object_superclass`, `fruit_inside_color`, `worklocation`
    - **Factual**: `country_language`, `country_capital_city`
    - **Bias**: `name_religion`, `occupation_age`, `occupation_gender`, `name_birthplace`

> æ‰€æœ‰æ ·æœ¬å‡ä»¥ cloze-style prompt å½¢å¼è¾“å…¥ï¼ˆå¦‚ï¼šâ€œThe capital of {} is the city ofâ€ï¼‰ï¼Œç›®æ ‡æ˜¯é¢„æµ‹æ­£ç¡®å¯¹è±¡è¯ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®

| æ¨¡å‹ | ç±»å‹ | å‚æ•°é‡ | å±‚æ•° | ä¸“å®¶æ•° | Routing | è®­ç»ƒæ­¥æ•° | Token æ€»é‡ |
|------|------|--------|-------|---------|----------|------------|-------------|
| **OLMo-7B** | Dense | ~7B | 32 | â€“ | â€“ | 600K | ~2.5T |
| **OLMoE-1B-7B** | MoE | ~7B (active: ~1.3B/token) | 16 | 64/expert layer | Top-8 | 1.2M | ~5.0T |

- ä¸¤è€…ç”±åŒä¸€å›¢é˜Ÿå‘å¸ƒï¼Œä¿è¯å®ç°ä¸€è‡´æ€§ã€‚
- OLMoE ä½¿ç”¨æ›´å¤šè®­ç»ƒæ­¥æ•°ç¬¦åˆ MoE çš„å…¸å‹è®­ç»ƒç­–ç•¥ï¼ˆç¡®ä¿ä¸“å®¶å……åˆ†åˆ©ç”¨ï¼‰ã€‚
- æ§åˆ¶æ½œåœ¨åå·®ï¼šè™½ç„¶ OLMoE ä½¿ç”¨äº†æ›´å¤šä»£ç /æ•°å­¦æ•°æ®ï¼ˆOLMoE-MIXï¼‰ï¼Œä½œè€…åœ¨åˆ†æä¸­è¿›è¡Œäº†æ§åˆ¶ã€‚

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡

#### åŠŸèƒ½æ€§èƒ½æŒ‡æ ‡
- **HIT@10**ï¼šè¡¡é‡æ¨¡å‹æ˜¯å¦èƒ½åœ¨ top-10 é¢„æµ‹ä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆã€‚

#### ç¨³å®šæ€§ä¸åŠ¨æ€åˆ†ææŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ | ç”¨é€” |
|------|------|------|
| **Top-1% Set Stability (Jstab)** | è¿ç»­æ£€æŸ¥ç‚¹é—´ top-1% æœ€é‡è¦ç¥ç»å…ƒé›†åˆçš„ Jaccard é‡å ç‡ | è¡¡é‡ç¥ç»å…ƒé‡è¦æ€§çš„æŒä¹…æ€§ |
| **Positive-Gain Concentration (R)** | æ­£å‘é‡è¦æ€§å¢ç›Šé›†ä¸­åœ¨ top-1% ç¥ç»å…ƒçš„æ¯”ä¾‹ | åæ˜ å­¦ä¹ ä¿¡å·é›†ä¸­ç¨‹åº¦ |
| **Layer-Distribution Consistency (pavg)** | å±‚é—´é‡è¦æ€§åˆ†å¸ƒçš„ç›¸å…³ç³»æ•°å¹³å‡å€¼ | è¡¡é‡å±‚çº§åˆ«é‡è¦æ€§é…ç½®çš„ç¨³å®šæ€§ |
| **Cross-step Coefficient of Variation (orel)** | æ¯å±‚é‡è¦æ€§å¾—åˆ†çš„æ—¶é—´åºåˆ—å˜å¼‚ç³»æ•° | æ•°å€¼è¶Šä½è¡¨ç¤ºè¶Šç¨³å®š |

#### åŠŸèƒ½é²æ£’æ€§æµ‹è¯•
- **å› æœå¹²é¢„ï¼ˆCausal Intervention / Ablationï¼‰**ï¼š
  - æ©ç ï¼ˆmaskï¼‰æœ€é‡è¦çš„ç¥ç»å…ƒæˆ– attention head
  - è§‚å¯Ÿ HIT@10 ä¸‹é™å¹…åº¦
  - æµ‹è¯•å¯¹è±¡ï¼š
    - Top-1 / Top-10 attention heads
    - Top-1% FFN neurons

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
- ä¸»è¦å¯¹æ¯”å¯¹è±¡ä¸ºï¼š
  - **OLMo-7B (dense)** vs **OLMoE-1B-7B (MoE)**
- æ‰€æœ‰å½’å› åˆ†æåœ¨åŒä¸€æ¡†æ¶ä¸‹æ‰§è¡Œï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚
- æ— å…¶ä»– baseline attribution æ–¹æ³•ç›´æ¥å‚ä¸æ€§èƒ½å¯¹æ¯”ï¼Œä½†æ–¹æ³•è®ºä¸Šè¶…è¶Šäº† IGã€LRPã€NEG ç­‰ä»…é€‚ç”¨äº dense æ¨¡å‹çš„æ–¹æ³•ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ

#### ï¼ˆ1ï¼‰ç¥ç»å…ƒçº§ç¨³å®šæ€§ï¼ˆNeuron-Level Stabilityï¼‰

| æ¨¡å‹ | å¹³å‡ Jstab (FFN, 100Kâ€“600K) | Positive-Gain Concentration R (FFN) |
|------|-------------------------------|-------------------------------------|
| **OLMo (Dense)** | 6.1% | 26.4% |
| **OLMoE (MoE)** | **25.3%** | **48.9%** |

> MoE çš„ top-1% ç¥ç»å…ƒå…·æœ‰æ˜¾è‘—æ›´é«˜çš„æŒç»­æ€§å’Œå¼ºåŒ–é›†ä¸­åº¦ã€‚

- **OLMoE FFN** æ˜¾ç¤º â€œæ¢ç´¢-ç²¾ç‚¼-å›ºåŒ–â€ ä¸‰é˜¶æ®µè½¨è¿¹ï¼š
  - 70K æ­¥æ—¶è¾¾ 27%
  - 200K æ­¥ç•¥æœ‰ä¸‹é™ï¼ˆé€‰æ‹©æ€§æ·˜æ±°ï¼‰
  - 1.2M æ­¥å›å‡è‡³ **44.2%**

- **OLMo** å³°å€¼ä»… 10.2%ï¼Œä¹‹åæŒç»­ä¸‹é™è‡³ 4.9%

#### ï¼ˆ2ï¼‰æ³¨æ„åŠ›å¤´ç¨³å®šæ€§ï¼ˆATTN Neuronsï¼‰

| æ¨¡å‹ | å¹³å‡ Jstab (ATTN, 100Kâ€“600K) | R (ATTN) |
|------|------------------------------|----------|
| **OLMo** | 15.1% | â€” |
| **OLMoE** | **~57.7% (at 1.2M steps)** | 33â€“34% |

- OLMoE æ³¨æ„åŠ›æ ¸å¿ƒå¿«é€Ÿå»ºç«‹å¹¶åœ¨åæœŸæŒç»­å¢å¼ºã€‚
- OLMo æ³¨æ„åŠ›ç¥ç»å…ƒé¢‘ç¹æ›´æ›¿ï¼Œæ— é•¿æœŸç¨³å®šæ ¸å¿ƒã€‚

#### ï¼ˆ3ï¼‰å±‚çº§åˆ«ç¨³å®šæ€§ï¼ˆLayer-Levelï¼‰

| æ¨¡å‹ | FFN pavg | FFN orel | ATTN pavg | ATTN orel |
|------|----------|----------|-----------|-----------|
| **OLMo** | 0.54 | 5.01 | 0.49 | 8.64 |
| **OLMoE** | **0.97** | **0.37** | **0.97** | **0.49** |

> MoE åœ¨ FFN å’Œ ATTN å±‚å‡è¡¨ç°å‡ºæé«˜çš„å±‚é—´é‡è¦æ€§åˆ†å¸ƒä¸€è‡´æ€§ï¼Œæ³¢åŠ¨æå°ã€‚

- OLMoE çš„ FFN å±‚é‡è¦æ€§åœ¨ **100K æ­¥å†…å³è¶‹äºç¨³å®š**
- OLMo çš„ FFN å’Œ ATTN å±‚é‡è¦æ€§æŒç»­æ¼‚ç§»

#### ï¼ˆ4ï¼‰åŠŸèƒ½é²æ£’æ€§ï¼ˆFunctional Robustnessï¼‰â€”â€”æ¶ˆèå®éªŒç»“æœ

| å¹²é¢„æ–¹å¼ | **OLMoE (MoE)** | **OLMo (Dense)** |
|--------|------------------|------------------|
| **Mask Top-1 Attention Head** | â†“ **0.06%** | â†“ **16.46%** |
| **Mask Top-10 Attention Heads** | â†“ **9.44%** | â†“ **50.43%** |
| **Mask Top-1% FFN Neurons** | â†“ **35.47%** | â†“ **96.19%** |

> MoE æ¨¡å‹å³ä½¿ç§»é™¤æœ€å…³é”®ç»„ä»¶ï¼Œæ€§èƒ½ä»ä¿æŒç¨³å¥ï¼›è€Œ dense æ¨¡å‹å‡ ä¹å´©æºƒã€‚

- æ›´æƒŠäººçš„æ˜¯ï¼šéšç€è®­ç»ƒæ¨è¿›ï¼ŒOLMoE å¯¹ top-10 heads çš„ä¾èµ–è¿›ä¸€æ­¥é™ä½ â†’ **æŠ—å¹²æ‰°èƒ½åŠ›éšè®­ç»ƒæå‡**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **Low-Entropy Backboneï¼ˆä½ç†µéª¨å¹²ï¼‰**
   - MoE æ¨¡å‹ä¸­çº¦ **top 1% çš„ç¥ç»å…ƒå¸æ”¶äº†è¶…è¿‡ 45% çš„æ­£å‘æ›´æ–°ä¿¡å·**
   - è¿™äº›é«˜å½±å“åŠ›ç¥ç»å…ƒåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è¢«æŒç»­å¼ºåŒ–å¹¶ä¿æŒç¨³å®š
   - åœ¨ dense æ¨¡å‹ä¸­æœªè§‚å¯Ÿåˆ°ç±»ä¼¼ç°è±¡ â†’ å­˜åœ¨â€œé«˜åˆ©ç”¨ç‡æ ¸å¿ƒâ€

2. **Early Consolidationï¼ˆæ—©æœŸå›ºåŒ–ï¼‰**
   - MoE çš„ FFN ä¸ ATTN å±‚çš„é‡è¦æ€§åˆ†å¸ƒ **åœ¨ <100K æ­¥å†…é”å®š**
   - dense æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä»å¤„äºå‰§çƒˆæ³¢åŠ¨çŠ¶æ€
   - è¡¨æ˜ MoE å…·å¤‡æ›´å¿«çš„åŠŸèƒ½æ¨¡å—åŒ–æ”¶æ•›èƒ½åŠ›

3. **Functional Robustnessï¼ˆåŠŸèƒ½é²æ£’æ€§ï¼‰**
   - MoE çš„çŸ¥è¯†å­˜å‚¨æ˜¯ **åˆ†å¸ƒå¼è€Œéé›†ä¸­å¼**
   - å³ä½¿å±è”½æœ€é‡è¦çš„ attention heads æˆ– FFN neuronsï¼Œæ€§èƒ½ä¸‹é™è½»å¾®
   - dense æ¨¡å‹é«˜åº¦ä¾èµ–å°‘æ•°å…³é”®ç»„ä»¶ â†’ **è„†å¼±æ€§å¼º**

4. **ç¨€ç–æ€§æ˜¯ä¸€ç§æœ‰ç›Šçš„å½’çº³åç½®ï¼ˆSparsity as Inductive Biasï¼‰**
   - MoE çš„ç¨€ç–æ¿€æ´»æœºåˆ¶ä¸ä»…èŠ‚çœè®¡ç®—èµ„æºï¼Œè¿˜ä¿ƒè¿›äº†æ›´ç¨³å®šã€æ›´é²æ£’çš„å­¦ä¹ åŠ¨æ€
   - ç¨€ç–æ€§å¼•å¯¼æ¨¡å‹å‘å±•å‡º **â€œæ¢ç´¢â†’ç¨³å®šâ€ çš„å­¦ä¹ èŒƒå¼**

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

| å±€é™ | è¯´æ˜ |
|------|------|
| **ä»…åˆ†æä¸¤ç§æ¨¡å‹** | ç»“è®ºåŸºäº OLMo å’Œ OLMoEï¼Œæ³›åŒ–æ€§éœ€åœ¨æ›´å¤š MoE/dense å¯¹ä¸­éªŒè¯ |
| **æ•°æ®æ„æˆå·®å¼‚** | OLMoE ä½¿ç”¨äº†æ›´å¤š code/math æ•°æ®ï¼Œè™½å°è¯•æ§åˆ¶ä½†ä»å¯èƒ½å½±å“ç»“æœ |
| **å½’å› æ–¹æ³•å‡è®¾** | Gated-LPI å‡è®¾ç¥ç»å…ƒè´¡çŒ®å¯åŠ ï¼Œå¿½ç•¥éçº¿æ€§äº¤äº’æ•ˆåº” |
| **ä»»åŠ¡å•ä¸€æ€§** | æ‰€æœ‰åˆ†æåŸºäºå…³ç³»æå–ä»»åŠ¡ï¼Œæ˜¯å¦æ¨å¹¿åˆ°å…¶ä»–ä»»åŠ¡å¾…éªŒè¯ |

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **æ‰©å±•è‡³æ›´å¤š MoE æ¶æ„å˜ä½“**
   - å¦‚ä¸åŒ k å€¼çš„ Top-k routingã€shared expertsã€hierarchical MoE
2. **æŒ‡å¯¼ MoE è®¾è®¡ä¸ä¼˜åŒ–**
   - åˆ©ç”¨ç¨³å®šæ€§åˆ†ææ”¹è¿› **routing ç­–ç•¥**ï¼ˆå¦‚ explore-then-stabilizeï¼‰
   - æ”¯æŒ **ä¸“å®¶å‰ªæä¸åˆå§‹åŒ–ç­–ç•¥**
3. **è®­ç»ƒè¯Šæ–­å·¥å…·å¼€å‘**
   - å°† Gated-LPI é›†æˆè¿›è®­ç»ƒç›‘æ§ pipelineï¼Œå®æ—¶æ£€æµ‹çŸ¥è¯†å›ºåŒ–è¿›ç¨‹
4. **è·¨ä»»åŠ¡è¿ç§»åˆ†æ**
   - æ¢ç´¢çŸ¥è¯†éª¨å¹²æ˜¯å¦åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¿æŒä¸€è‡´
5. **å› æœæœºåˆ¶æŒ–æ˜**
   - ç»“åˆ circuit discovery æŠ€æœ¯ï¼Œè¯†åˆ« MoE ä¸­çš„åŠŸèƒ½å­ç½‘ç»œ

---

## æ€»ç»“

> æœ¬è®ºæ–‡é¦–æ¬¡ç³»ç»Ÿæ­ç¤ºäº† **MoE æ¶æ„åœ¨é¢„è®­ç»ƒä¸­å½¢æˆçš„ç¨³å®šã€åˆ†å¸ƒå¼çŸ¥è¯†ç»“æ„**ï¼Œå¹¶é€šè¿‡æå‡ºçš„ **Gated-LPI** æ–¹æ³•å®ç°äº†å¯¹ MoE å†…éƒ¨çŸ¥è¯†æµåŠ¨çš„ç»†ç²’åº¦è¿½è¸ªã€‚ç»“æœæ˜¾ç¤ºï¼Œ**ç¨€ç–æ€§ä¸ä»…æ˜¯è®¡ç®—ä¸Šçš„æ·å¾„ï¼Œæ›´æ˜¯ä¿ƒè¿›æ¨¡å‹ç¨³å®šå­¦ä¹ ä¸åŠŸèƒ½é²æ£’æ€§çš„å…³é”®æœºåˆ¶**ã€‚è¿™ä¸€å‘ç°ä¸ºæœªæ¥é«˜æ•ˆã€å¯è§£é‡Šçš„å¤§æ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºã€‚

</details>

---

### 7. [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)

**Authors**: Giulio Corallo, Paolo Papotti  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.08670v1  

#### Abstract
Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decodin...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šParallel Context-of-Experts Decoding for Retrieval Augmented Generation**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
Retrieval-Augmented Generation (RAG) é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š
- **Prefill ç“¶é¢ˆ**ï¼šå°†å¤šä¸ªæ£€ç´¢åˆ°çš„æ–‡æ¡£æ‹¼æ¥æˆä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†æ—¶ï¼Œprefill é˜¶æ®µè®¡ç®—å¼€é”€å·¨å¤§ï¼Œå¯¼è‡´å»¶è¿Ÿé«˜ã€‚
- **è·¨æ–‡æ¡£æ¨ç†èƒ½åŠ›ä¸‹é™**ï¼šè‹¥é‡‡ç”¨å¹¶è¡Œ KV cache ç¼–ç ï¼ˆå¦‚ç‹¬ç«‹ç¼–ç æ¯ä¸ªæ–‡æ¡£ï¼‰ï¼Œè™½èƒ½åŠ é€Ÿ prefillï¼Œä½†ç ´åäº†æ–‡æ¡£é—´çš„æ³¨æ„åŠ›äº¤äº’ï¼Œå‰Šå¼±äº† multi-hop reasoning èƒ½åŠ›ã€‚

ä¼ ç»Ÿæ–¹æ³•åœ¨â€œæ•ˆç‡â€ä¸â€œå‡†ç¡®æ€§â€ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
ä½œè€…æå‡º **Parallel Context-of-Experts Decoding (PCED)**ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„ decoding æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
> å°†è¯æ®èšåˆä» **attention å±‚é¢** è½¬ç§»åˆ° **decoding å±‚é¢**ã€‚

å…·ä½“åˆ›æ–°åŒ…æ‹¬ï¼š
1. **Parallel Context-of-Experts æ¶æ„**  
   æ¯ä¸ªæ£€ç´¢åˆ°çš„æ–‡æ¡£è¢«è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„â€œä¸“å®¶â€ï¼ˆExpertï¼‰ï¼Œæ‹¥æœ‰è‡ªå·±çš„ KV cacheï¼›åŒæ—¶å¼•å…¥ä¸€ä¸ªæ— ä¸Šä¸‹æ–‡çš„â€œä¸šä½™ä¸“å®¶â€ï¼ˆAmateur Expertï¼‰ä»£è¡¨æ¨¡å‹å…ˆéªŒã€‚
   
2. **Retrieval-aware Contrastive Decoding**  
   åœ¨æ¯ä¸€æ­¥ç”Ÿæˆä¸­ï¼Œé€šè¿‡ä»¥ä¸‹å…¬å¼å¯¹å„ä¸“å®¶çš„ logits è¿›è¡ŒåŠ æƒæ ¡å‡†ï¼š
   $$
   \tilde{s}_k = (1+\beta_0)s_k - \beta_0 s_0 + \gamma \log r_k
   $$
   å…¶ä¸­ï¼š
   - $s_k$ï¼šç¬¬ $k$ ä¸ªä¸“å®¶çš„ logits
   - $s_0$ï¼šä¸šä½™ä¸“å®¶ï¼ˆæ¨¡å‹å…ˆéªŒï¼‰çš„ logits
   - $r_k$ï¼šè¯¥æ–‡æ¡£çš„æ£€ç´¢ç›¸å…³æ€§å¾—åˆ†ï¼ˆèåˆ retrieval å’Œ reranker å¾—åˆ†ï¼‰
   - $\beta_0$ æ§åˆ¶ contrastive å¼ºåº¦ï¼Œ$\gamma$ æ§åˆ¶ retrieval prior æƒé‡

3. **åŠ¨æ€è¯æ®æ‹¼æ¥æœºåˆ¶**  
   æ¯æ­¥é€‰æ‹©æ‰€æœ‰ä¸“å®¶ä¸­æœ€æ”¯æŒçš„ tokenï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°å…±äº«ç”Ÿæˆå†å²ä¸­ï¼Œå®ç°è·¨æ–‡æ¡£è¯æ®çš„â€œ stitchingâ€ï¼Œè€Œæ— éœ€è”åˆ attentionã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | PCED | ä¼ ç»Ÿ Concatenation | KV Merge / APE | Agentic (e.g., MapReduce) |
|------|------|---------------------|----------------|----------------------------|
| **æ¨ç†é€Ÿåº¦** | âœ… æå¿«ï¼ˆTTFT åŠ é€Ÿ >180Ã—ï¼‰ | âŒ Prefill æˆæœ¬é«˜ | âœ… å¿«ï¼ˆå¤ç”¨ KVï¼‰ | âŒ å¤šè½®è°ƒç”¨ LLMï¼Œå»¶è¿Ÿé«˜ |
| **è·¨æ–‡æ¡£æ¨ç†** | âœ… æ”¯æŒï¼ˆdecoding æ—¶èšåˆï¼‰ | âœ… æ”¯æŒï¼ˆå®Œæ•´ attentionï¼‰ | âŒ å¼±ï¼ˆç¼ºä¹è·¨æ–‡æ¡£äº¤äº’ï¼‰ | âœ… æ”¯æŒï¼ˆå…¨å±€æ±‡æ€»ï¼‰ |
| **æŠ—å™ªå£°èƒ½åŠ›** | âœ… å¼ºï¼ˆrelevance score æŠ‘åˆ¶æ— å…³ä¸“å®¶ï¼‰ | âŒ å·®ï¼ˆé•¿ä¸Šä¸‹æ–‡ç¨€é‡Šå…³é”®ä¿¡æ¯ï¼‰ | âš ï¸ ä¸­ç­‰ | âš ï¸ å–å†³äºæ‘˜è¦è´¨é‡ |
| **æ˜¯å¦éœ€è®­ç»ƒ** | âœ… ä¸éœ€è¦ | âœ… ä¸éœ€è¦ | âš ï¸ éƒ¨åˆ†æ–¹æ³•éœ€è¦å¾®è°ƒ | âœ… ä¸éœ€è¦ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **LOFT benchmark**ï¼šç”¨äº RAG å’Œ In-Context Learning (ICL) ä»»åŠ¡ï¼ŒåŒ…å«å¤šè·³é—®ç­”ã€äº‹å®éªŒè¯ç­‰ã€‚
- **LongBench**ï¼šåŒ…å«å•æ–‡æ¡£ä¸å¤šæ–‡æ¡£ QAã€æ‘˜è¦ã€ä»£ç è¡¥å…¨ã€few-shot æ¨ç†ç­‰å­ä»»åŠ¡ï¼Œæµ‹è¯•é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚
- **åˆæˆæ•°æ®é›†**ï¼šç”¨äºæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆTTFT å’Œç«¯åˆ°ç«¯å»¶è¿Ÿï¼‰ï¼Œæ„é€ å›ºå®šé•¿åº¦ï¼ˆ2048 tokens/docï¼‰çš„ 64 æ–‡æ¡£è¾“å…¥ï¼Œå« 1 ä¸ªé»„é‡‘æ–‡æ¡£å’Œå¤šä¸ªå¹²æ‰°é¡¹ã€‚

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**
- **LLM æ¨¡å‹**ï¼š
  - ä¸»è¦ç»“æœï¼š`MISTRAL-NEMO-13B-INSTRUCT`, `LLAMA-3.1-8B-INSTRUCT`
  - LongBenchï¼š`QWEN3-8B`ï¼ˆæ‰©å±•è‡³ 128k ä¸Šä¸‹æ–‡ï¼‰
- **è§£ç æ–¹å¼**ï¼šgreedy decoding
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - RAG ä»»åŠ¡ï¼šSubspan Exact Match
  - ICL ä»»åŠ¡ï¼šExact Match
  - LongBenchï¼šå®˜æ–¹æŒ‡æ ‡ï¼ˆå¦‚ F1ã€EM ç­‰ï¼‰

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| ç±»åˆ« | åŸºçº¿æ–¹æ³• |
|------|--------|
| **æ ‡å‡†æ‹¼æ¥** | Corpus in Ctx (Single), Corpus in Ctx (All) |
| **KV Cache åˆå¹¶** | APEï¼ˆAdaptive Parallel Encodingï¼‰ |
| **Agentic æ–¹æ³•** | MapReduceï¼ˆé€æ–‡æ¡£ summarize + reduceï¼‰ |

æ­¤å¤–è¿˜æ¯”è¾ƒäº†ä¸åŒ relevance scoring æ–¹å¼ä¸‹çš„ PCED å˜ä½“ï¼š
- **Sparse**ï¼šåŸºäºç¨€ç–æ£€ç´¢åˆ†æ•°
- **Dense**ï¼šåŸºäºç¨ å¯†å‘é‡ç›¸ä¼¼åº¦
- **ColBERT**ï¼šåŸºäºç»†ç²’åº¦å‘é‡åŒ¹é…

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
#### **è¡¨1ï¼šLOFT ä¸Šçš„ä¸»è¦ç»“æœï¼ˆä»¥ LLAMA-3.1-8B ä¸ºä¾‹ï¼‰**

| Task | APE | MapReduce | PCED-Dense | Corpus in Ctx (All) |
|------|-----|-----------|------------|----------------------|
| HOTPOTQA | 16.0 | 41.0 | **64.0** | 49.0 |
| MUSIQUE | 4.0 | 8.0 | **21.0** | 7.0 |
| RAG NQ | 9.0 | 50.0 | **85.0** | 58.0 |
| QAMPARI | 7.0 | 68.0 | **77.0** | 72.0 |
| QUEST | 0.0 | 41.0 | **45.0** | 39.0 |

> âœ… PCED åœ¨å¤šè·³ QA ä¸Šæ˜¾è‘—ä¼˜äºæ‰€æœ‰ baselineï¼Œå°¤å…¶åœ¨ MUSIQUE ä¸Šæå‡è¶… 5 å€ã€‚

#### **è¡¨2ï¼šLongBench ç»“æœï¼ˆQWEN3-8Bï¼‰**

| Task | Corpus in Ctx (All) | PCED-Dense |
|------|----------------------|------------|
| HOTPOTQA | 56.3 | **62.6** |
| 2Wiki | 44.2 | **49.4** |
| MUSIQUE | 25.3 | **33.4** |
| TRIVIAQA | 84.0 | **88.2** |
| REPOB-P (Code) | 51.1 | **60.1** |

> âœ… å³ä½¿é¢å¯¹å¤§é‡å¹²æ‰°æ–‡æ¡£ï¼ŒPCED ä»å…¨é¢è¶…è¶Š full-context æ‹¼æ¥ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- **vs. KV Merge (APE)**ï¼šPCED åœ¨ multi-hop QA ä¸Šå¹³å‡æå‡ **+40~60 pts**ï¼Œè¯´æ˜ decoding-time aggregation å¯æœ‰æ•ˆæ¢å¤è·¨æ–‡æ¡£æ¨ç†èƒ½åŠ›ã€‚
- **vs. MapReduce**ï¼šPCED åœ¨å¤šæ•°ä»»åŠ¡ä¸Šæ›´ä¼˜ï¼ˆå¦‚ HOTPOTQA +23 ptsï¼‰ï¼Œä¸”ä»…éœ€ä¸€æ¬¡ decodingï¼Œæ•ˆç‡æ›´é«˜ã€‚
- **vs. Full Context Concatenation**ï¼šPCED åœ¨ 11/16 è®¾ç½®ä¸‹ä¼˜äºæˆ–æŒå¹³â€œall documents in contextâ€ï¼ŒåŒæ—¶é¿å…äº†é•¿ä¸Šä¸‹æ–‡å¸¦æ¥çš„æ³¨æ„åŠ›å™ªå£°ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**
#### **è¡¨3ï¼šç»„ä»¶åˆ†æï¼ˆPCED å„éƒ¨åˆ†ä½œç”¨ï¼‰**

| æ–¹æ³• | HOTPOTQA (Llama) | NQ (Llama) |
|------|------------------|------------|
| Only Contrastive ($\gamma=0$) | 46 | 52 |
| Only Retrieval Prior ($\beta=0$) | 53 | 70 |
| Full PCED | **64** | **85** |

> ğŸ” å‘ç°ï¼š
- ç§»é™¤ retrieval prior å¯¼è‡´ç¾éš¾æ€§å¤±è´¥ â†’ è¡¨æ˜å¤–éƒ¨ relevance åˆ†æ•°å¯¹äºæŠ‘åˆ¶å™ªå£°è‡³å…³é‡è¦ã€‚
- ç§»é™¤ contrastive decoding æ˜¾è‘—é™ä½æ€§èƒ½ â†’ ç‰¹åˆ«æ˜¯å¯¹ Llama æ¨¡å‹ï¼Œéœ€ subtract amateur prior æ¥æŠ‘åˆ¶å¹»è§‰ã€‚

#### **å…¶ä»–æ¶ˆè**
- **$\gamma$ æœ€ä¼˜å€¼ä¸º 2.5**ï¼šè¿‡ä½åˆ™æ— æ³•æŠ‘åˆ¶å¹²æ‰°ï¼Œè¿‡é«˜åˆ™åƒµåŒ–ä¾èµ–æ£€ç´¢æ’åºã€‚
- **Max aggregation æœ€ä½³**ï¼šåœ¨ multi-hop QA ä¸­ä¼˜äº MoE/PoEï¼Œæ”¯æŒ sharp expert switchingã€‚
- **é²æ£’æ€§æµ‹è¯•**ï¼šå³ä½¿ top-k ä» 8 æ‰©å±•åˆ° 128ï¼ŒPCED æ€§èƒ½å‡ ä¹ä¸å˜ï¼ˆè§ Figure 6ï¼‰ï¼Œè¡¨æ˜å…¶å¯¹å€™é€‰æ± å¤§å°ä¸æ•æ„Ÿã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **è·¨æ–‡æ¡£æ¨ç†å¯åœ¨ decoding é˜¶æ®µé‡å»º**  
   æ— éœ€æ„å»ºè·¨æ–‡æ¡£ attentionï¼Œé€šè¿‡ retrieval-aware contrastive decoding å³å¯å®ç°æœ‰æ•ˆçš„ evidence stitchingã€‚

2. âœ… **KV cache æ¨¡å—åŒ– + decoding-time aggregation æ˜¯é«˜æ•ˆä¸”å‡†ç¡®çš„è·¯å¾„**  
   PCED å®ç°äº†â€œå¹¶è¡Œç¼–ç  + åºåˆ—çº§æ¨ç†â€çš„ç»Ÿä¸€ï¼Œåœ¨ä¿æŒé«˜é€Ÿçš„åŒæ—¶æ¢å¤äº† long-context æ¨¡å‹çš„èƒ½åŠ›ã€‚

3. âœ… **retrieval scores ä¸åº”ä¸¢å¼ƒï¼Œè€Œæ˜¯ä½œä¸º decoding prior**  
   åˆ©ç”¨ retrieval å’Œ reranker çš„ scalar scores æ³¨å…¥ decoding è¿‡ç¨‹ï¼Œå¯æ˜¾è‘—æå‡æŠ—å™ªæ€§å’Œå‡†ç¡®æ€§ã€‚

4. âœ… **æ•ˆç‡ä¼˜åŠ¿å·¨å¤§**  
   - Time-to-First-Token (TTFT) åŠ é€Ÿ **>180Ã—**ï¼ˆ0.14s vs 25.50sï¼‰
   - ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½çº¦ **1.7Ã—**

---

### **æ–¹æ³•çš„å±€é™æ€§**
1. **ä¾èµ–æ¨¡å‹ logits è¾“å‡º**  
   PCED éœ€è®¿é—®æ¯ä¸ªä¸“å®¶çš„ token-level logitsï¼Œå› æ­¤æ— æ³•ç›´æ¥åº”ç”¨äºé—­æº API æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰ã€‚

2. **å¯¹æ£€ç´¢è´¨é‡æ•æ„Ÿ**  
   è‹¥ç›¸å…³æ–‡æ¡£æœªè¢«æ£€ç´¢å‡ºï¼Œæˆ– relevance score è¢«ä½ä¼°ï¼Œåˆ™å¯¹åº” expert å¯èƒ½è¢«å¿½ç•¥ï¼Œå½±å“æœ€ç»ˆè¾“å‡ºã€‚

3. **å­˜å‚¨æˆæœ¬è¾ƒé«˜**  
   éœ€é¢„å…ˆå­˜å‚¨æ¯ä¸ªæ–‡æ¡£çš„ KV cacheï¼Œå­˜å‚¨å¼€é”€éšè¯­æ–™è§„æ¨¡çº¿æ€§å¢é•¿ï¼ˆä¾‹å¦‚ LOFT æ•°æ®é›†çº¦éœ€ 11GB å­˜å‚¨ï¼‰ã€‚é€‚åˆé™æ€çŸ¥è¯†åº“åœºæ™¯ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
- **ç«¯åˆ°ç«¯å­¦ä¹  expert selection**ï¼šè®© LLM è‡ªä¸»å­¦ä¹ åœ¨æ¯ä¸€æ­¥é€‰æ‹©å“ªä¸ª context æ›´é‡è¦ï¼Œå‡å°‘å¯¹å¤–éƒ¨ retrieval pipeline çš„ä¾èµ–ã€‚
- **è½»é‡åŒ– KV cache å­˜å‚¨**ï¼šæ¢ç´¢ quantizationã€pruning æˆ–å¢é‡æ›´æ–°ç­–ç•¥ä»¥é™ä½å­˜å‚¨è´Ÿæ‹…ã€‚
- **æ‰©å±•è‡³ streaming åœºæ™¯**ï¼šç»“åˆ streaming retrieval ä¸ PCED å®ç°åŠ¨æ€ä¸Šä¸‹æ–‡å¢å¼ºç”Ÿæˆã€‚

--- 

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **PCED æä¾›äº†ä¸€ç§å…¨æ–°çš„è§†è§’â€”â€”å°† RAG ä¸­çš„â€œä¸Šä¸‹æ–‡èåˆâ€ä» attention è½¬ç§»åˆ° decodingï¼Œå®ç°äº†é«˜æ•ˆã€å‡†ç¡®ã€æŠ—å™ªçš„å¤šæ–‡æ¡£ç”Ÿæˆï¼Œä¸ºå¤§è§„æ¨¡çŸ¥è¯†å¢å¼ºç³»ç»Ÿæä¾›äº†æå…·æ½œåŠ›çš„æ¶æ„èŒƒå¼ã€‚**

</details>

---

### 8. [Hierarchical Precision and Recursion for Accelerating Symmetric Linear Solves on MXUs](https://arxiv.org/abs/2601.08082)

**Authors**: Vicki Carrica, Rabab Alomairy, Evelyne Ringoot, Alan Edelman  
**Category**: cs.DC  
**Published**: 2026-01-14  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.08082v1  

#### Abstract
Symmetric linear solves are fundamental to a wide range of scientific and engineering applications, from climate modeling and structural analysis to machine learning and optimization. These workloads often rely on Cholesky (POTRF) decomposition and its supporting operations, triangular solves (TRSM)...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Hierarchical Precision and Recursion for Accelerating Symmetric Linear Solves on MXUs*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¯¹ç§°æ­£å®šçº¿æ€§ç³»ç»Ÿï¼ˆSymmetric Positive Definite, SPDï¼‰çš„æ±‚è§£åœ¨ç§‘å­¦è®¡ç®—ã€å·¥ç¨‹æ¨¡æ‹Ÿå’Œæœºå™¨å­¦ä¹ ä¸­å¹¿æ³›å­˜åœ¨ï¼Œå…¶æ ¸å¿ƒæ˜¯ **Cholesky åˆ†è§£**ï¼ˆPOTRFï¼‰ï¼Œä¾èµ–äº **TRSM**ï¼ˆä¸‰è§’çŸ©é˜µæ±‚è§£ï¼‰å’Œ **SYRK**ï¼ˆå¯¹ç§°ç§©-k æ›´æ–°ï¼‰ç­‰å¯†é›†çº¿æ€§ä»£æ•°æ“ä½œã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸é‡‡ç”¨ç»Ÿä¸€é«˜ç²¾åº¦ï¼ˆå¦‚ FP64ï¼‰ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨ç°ä»£ AI åŠ é€Ÿå™¨ï¼ˆå¦‚ NVIDIA Tensor Cores å’Œ AMD Matrix Coresï¼‰æä¾›çš„ä½ç²¾åº¦ï¼ˆå¦‚ FP16ï¼‰é«˜ååèƒ½åŠ›ã€‚

ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨ä½ç²¾åº¦ä¼šå¸¦æ¥ä¸¥é‡çš„æ•°å€¼ä¸ç¨³å®šæ€§å’Œç²¾åº¦æŸå¤±ï¼Œé™åˆ¶äº†å…¶åœ¨ç§‘å­¦è®¡ç®—ä¸­çš„åº”ç”¨ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§**å¯ç§»æ¤çš„æ··åˆç²¾åº¦é€’å½’ Cholesky æ±‚è§£å™¨**ï¼Œç»“åˆäº†**åµŒå¥—é€’å½’ç®—æ³•è®¾è®¡**ä¸**æ ‘çŠ¶åˆ†å±‚ç²¾åº¦æ§åˆ¶**ï¼Œä¸“ä¸º Matrix Processing Units (MXUs) è®¾è®¡ã€‚ä¸»è¦åˆ›æ–°ç‚¹å¦‚ä¸‹ï¼š

#### ï¼ˆ1ï¼‰åµŒå¥—é€’å½’ Cholesky æ¡†æ¶ï¼ˆNested Recursive Frameworkï¼‰
- å°†é€’å½’åˆ†è§£ä»ä¼ ç»Ÿçš„ä»…ç”¨äºå¯¹è§’å—ï¼ˆPOTRFï¼‰æ‰©å±•åˆ° **TRSM å’Œ SYRK** æ‰€æœ‰ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µã€‚
- é¦–æ¬¡å®ç°äº† **GPU ä¸Šçš„é€’å½’ SYRK**ï¼Œå¹¶å°†å…¶ä¸é€’å½’ TRSM ç»“åˆï¼Œå½¢æˆå®Œæ•´çš„é€’å½’æ›´æ–°è·¯å¾„ã€‚
- é€šè¿‡é€’å½’ç»†åˆ†æš´éœ²æ›´å¤š GEMM å­é—®é¢˜ï¼Œæ˜¾è‘—æå‡å¹¶è¡Œåº¦å’Œ BLAS3 æ“ä½œå æ¯”ï¼Œæ›´é€‚é… MXUs çš„ç¡¬ä»¶ç‰¹æ€§ã€‚

#### ï¼ˆ2ï¼‰æ ‘çŠ¶åˆ†å±‚æ··åˆç²¾åº¦æ–¹æ¡ˆï¼ˆTree-Structured Mixed Precisionï¼‰
- å¼•å…¥ä¸€ç§åŸºäºé€’å½’æ ‘ç»“æ„çš„è‡ªå®šä¹‰æ•°æ®å¸ƒå±€ï¼Œåœ¨ä¸åŒå±‚çº§åˆ†é…ä¸åŒç²¾åº¦ï¼š
  - **å¤§å‹éå¯¹è§’å—**ï¼ˆoff-diagonal blocksï¼‰ï¼šä½¿ç”¨ **FP16** è¿›è¡Œé«˜æ€§èƒ½ GEMM è®¡ç®—ï¼›
  - **é€’å½’ç»†åŒ–çš„å¯¹è§’åŒºåŸŸ**ï¼ˆdiagonal blocksï¼‰ï¼šä¿ç•™ **FP32 æˆ– FP64** ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ã€‚
- è¯¥è®¾è®¡å®ç°äº†â€œæ€§èƒ½å…³é”®è·¯å¾„ç”¨ä½ç²¾åº¦ï¼Œç²¾åº¦æ•æ„ŸåŒºåŸŸç”¨é«˜ç²¾åº¦â€çš„ç»†ç²’åº¦æ§åˆ¶ã€‚

#### ï¼ˆ3ï¼‰é€å—é‡åŒ–ä¸åé‡åŒ–ç­–ç•¥ï¼ˆBlockwise Quantization/Dequantizationï¼‰
- ä¸ºç¼“è§£ FP16 åŠ¨æ€èŒƒå›´æœ‰é™çš„é—®é¢˜ï¼Œåœ¨æ¯ä¸€çº§é€’å½’ä¸­å¯¹å‚ä¸ä½ç²¾åº¦è¿ç®—çš„å—è¿›è¡ŒåŠ¨æ€ç¼©æ”¾ï¼š
  - **é‡åŒ–**ï¼šå°†è¾“å…¥ç¼©æ”¾åˆ° FP16 å®‰å…¨èŒƒå›´å†…ï¼›
  - **åé‡åŒ–**ï¼šç»“æœæ¢å¤åŸå§‹å°ºåº¦ã€‚
- å¼€é”€æå°ï¼Œæœ‰æ•ˆé˜²æ­¢æº¢å‡º/ä¸‹æº¢ï¼Œä¿éšœæ•°å€¼é²æ£’æ€§ã€‚

#### ï¼ˆ4ï¼‰åŸºäº Julia çš„é«˜æ€§èƒ½å¯ç§»æ¤å®ç°
- åˆ©ç”¨ Julia çš„å¤šé‡æ´¾å‘ï¼ˆmultiple dispatchï¼‰ã€å‚æ•°åŒ–ç±»å‹å’ŒåŠ¨æ€ç±»å‹æ¨æ–­ï¼Œæ„å»ºäº†ä¸€ä¸ª**ç¡¬ä»¶æ— å…³çš„é«˜å±‚æ¥å£**ã€‚
- åº•å±‚è°ƒç”¨ vendor-specific åº“ï¼ˆå¦‚ cuBLAS / rocBLASï¼‰å¤„ç†åŸºç¡€æƒ…å†µï¼ˆleaf casesï¼‰ï¼Œå®ç°è·¨å¹³å°å…¼å®¹æ€§ï¼ˆæ”¯æŒ NVIDIA H200 å’Œ AMD MI300Xï¼‰ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | æœ¬æ–‡æ–¹æ³• |
|------|--------|---------|
| **é€’å½’ç²’åº¦** | ä»… POTRF é€’å½’ï¼ŒTRSM/SYRK ä¸ºæ ‡å‡†å—å½¢å¼ | å…¨æµç¨‹åµŒå¥—é€’å½’ï¼ˆPOTRF + TRSM + SYRKï¼‰ |
| **ç²¾åº¦æ§åˆ¶** | ç»Ÿä¸€ç²¾åº¦æˆ–è¿­ä»£ç²¾åŒ–ï¼ˆiterative refinementï¼‰ | æ ‘çŠ¶åˆ†å±‚æ··åˆç²¾åº¦ï¼ŒæŒ‰ä½ç½®åŠ¨æ€åˆ†é… |
| **ç¡¬ä»¶é€‚é…æ€§** | éš¾ä»¥åˆ©ç”¨ MXUs çš„ FP16 åå | æ˜¾å¼ä¼˜åŒ– GEMM å¯†é›†å‹ä»»åŠ¡ï¼Œæœ€å¤§åŒ– MXU åˆ©ç”¨ç‡ |
| **å¯ç§»æ¤æ€§** | é€šå¸¸ç»‘å®šç‰¹å®šå‚å•† API | å•ä¸€ Julia å®ç°ï¼Œè‡ªåŠ¨è°ƒåº¦è‡³ cuBLAS/rocBLAS |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- ä½¿ç”¨éšæœºç”Ÿæˆçš„ç¨ å¯†å¯¹ç§°æ­£å®šçŸ©é˜µï¼ˆdense SPD matricesï¼‰ã€‚
- çŸ©é˜µå…ƒç´ æœä»å‡åŒ€åˆ†å¸ƒï¼Œå¹¶é€šè¿‡å¯¹è§’åŠ  $ n $ï¼ˆç»´åº¦ï¼‰ç¡®ä¿æ­£å®šæ€§å’Œè‰¯å¥½æ¡ä»¶æ•°ã€‚
- æµ‹è¯•è§„æ¨¡è¦†ç›– $ n = 4096 $ è‡³ $ 65536 $ã€‚

### å®éªŒè®¾ç½®
- **ç¡¬ä»¶å¹³å°**ï¼š
  - **NVIDIA H200**ï¼šåŸºäº Hopper æ¶æ„ï¼Œ141GB HBM3e å†…å­˜ï¼Œæ”¯æŒ Tensor Coresï¼›
  - **AMD MI300X**ï¼š192GB HBM3ï¼Œ304 è®¡ç®—å•å…ƒï¼Œæ”¯æŒ Matrix Coresã€‚
- **è½¯ä»¶æ ˆ**ï¼š
  - Julia 1.12.0ï¼›
  - GPU æ”¯æŒåº“ï¼šCUDA.jlï¼ˆNVIDIAï¼‰ã€AMDGPU.jlï¼ˆAMDï¼‰ï¼›
  - åç«¯è°ƒç”¨ï¼šcuBLAS/cuSOLVERï¼ˆNVIDIAï¼‰ã€rocBLAS/rocSOLVERï¼ˆAMDï¼‰ã€‚
- **å®ç°æ–¹å¼**ï¼š
  - æ‰€æœ‰å˜ä½“å‡åœ¨åŒä¸€å¥—é€’å½’æ¡†æ¶ä¸‹å®ç°ï¼Œä»…æ”¹å˜ç²¾åº¦é…ç½®ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **æ€§èƒ½æŒ‡æ ‡**ï¼š
  - ç›¸å¯¹äºåŸºçº¿çš„ **Speedup**ï¼ˆåŠ é€Ÿæ¯”ï¼‰ï¼›
  - å®é™…è¾¾åˆ°çš„ **TFLOPs**ï¼ˆæ¯ç§’ä¸‡äº¿æµ®ç‚¹è¿ç®—ï¼‰ï¼›
- **ç²¾åº¦æŒ‡æ ‡**ï¼š
  - ç›¸å¯¹äº FP64 åŸºçº¿çš„ **ç›¸å¯¹è¯¯å·®èŒƒæ•°**ï¼ˆrelative error normï¼‰ï¼›
  - ä»¥ $-\log_{10}(\text{error})$ è¡¨ç¤ºæœ‰æ•ˆæ•°å­—ä½æ•°ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **POTRF/Cholesky**ï¼šcuSOLVER FP64ï¼ˆNVIDIAï¼‰ã€rocSOLVER FP64ï¼ˆAMDï¼‰ï¼›
- **TRSM/SYRK**ï¼šcuBLAS FP64ï¼ˆNVIDIAï¼‰ï¼›
- å¯¹æ¯”é…ç½®åŒ…æ‹¬ï¼š
  - çº¯ FP64ã€çº¯ FP32ã€çº¯ FP16ï¼›
  - å¤šç§æ··åˆç²¾åº¦å±‚æ¬¡é…ç½®ï¼Œå¦‚ `[FP16, FP16, FP32]`ã€`[FP16, FP16, FP16, FP32]` ç­‰ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆNVIDIA H200ï¼‰

#### ï¼ˆ1ï¼‰SYRK æ€§èƒ½
- **é€’å½’ FP64 SYRK**ï¼šç›¸æ¯” cuBLAS å®ç°æœ€é«˜è¾¾ **14Ã— åŠ é€Ÿ**ï¼ˆ$n=65536$ï¼‰ï¼›
- **æ··åˆç²¾åº¦ SYRK**ï¼š
  - `[FP16, FP16, FP16, FP16, FP32]` é…ç½®ä¸‹è¾¾åˆ° **27Ã— åŠ é€Ÿ**ï¼›
  - çº¯ FP16 è¾¾åˆ°å³°å€¼ **149Ã— åŠ é€Ÿ**ï¼ˆä½†ç²¾åº¦æå·®ï¼‰ã€‚

#### ï¼ˆ2ï¼‰TRSM æ€§èƒ½
- **é€’å½’ FP64 TRSM**ï¼šå·²ä¼˜äº cuBLASï¼›
- **æ··åˆç²¾åº¦ TRSM**ï¼š
  - `[FP16, FP16, FP16, FP16, FP32]` é…ç½®ä¸‹è¾¾åˆ° **5.3Ã— åŠ é€Ÿ**ï¼›
  - çº¯ FP16 æœ€é«˜è¾¾ **6Ã— åŠ é€Ÿ**ã€‚

#### ï¼ˆ3ï¼‰Cholesky æ•´ä½“æ€§èƒ½
- **æœ€ç»ˆ Cholesky æ±‚è§£å™¨é€Ÿåº¦æå‡**ï¼š
  - æ·±åº¦æ··åˆç²¾åº¦é…ç½® `[FP16Ã—6, FP32]` è¾¾åˆ° **5.32Ã— åŠ é€Ÿ**ï¼ˆvs. cuSOLVER FP64ï¼‰ï¼›
  - çº¯ FP16 è™½ç„¶å¯è¾¾è¿‘ **6Ã— åŠ é€Ÿ**ï¼Œä½†ç²¾åº¦ä¸¥é‡ä¸‹é™ã€‚
- **ååé‡è¡¨ç°**ï¼š
  - æ··åˆç²¾åº¦é…ç½®è¾¾åˆ°è¶…è¿‡ **2Ã— FP64 åŸºçº¿ TFLOPs**ï¼›
  - çº¯ FP16 æ¥è¿‘ **3Ã— FP64 åŸºçº¿ TFLOPs**ï¼Œä½†å—é™äºå†…å­˜å¸¦å®½ã€‚

#### ï¼ˆ4ï¼‰ç²¾åº¦è¡¨ç°
- **çº¯ FP16**ï¼šç›¸å¯¹è¯¯å·®å¤§ï¼Œæœ‰æ•ˆæ•°å­—ä¸è¶³ 4 ä½ï¼›
- **æ·±åº¦æ··åˆç²¾åº¦**ï¼ˆå¦‚ `[FP16Ã—6, FP32]`ï¼‰ï¼š
  - ä¿æŒçº¦ **5â€“6 ä½æœ‰æ•ˆæ•°å­—**ï¼›
  - ç›¸æ¯”çº¯ FP16 æå‡ **çº¦ 100 å€ç²¾åº¦**ï¼›
  - ä¿ç•™äº†çº¯ FP16 **88% çš„å³°å€¼åŠ é€Ÿæ”¶ç›Š**ã€‚

### AMD MI300X ç»“æœ
- åœ¨ MI300X ä¸ŠåŒæ ·è§‚å¯Ÿåˆ°ä¸€è‡´è¶‹åŠ¿ï¼š
  - æ·±åº¦æ··åˆç²¾åº¦é…ç½® `[FP16, FP16, FP16, FP32]` è¾¾åˆ° **5.3Ã— åŠ é€Ÿ**ï¼ˆvs. rocSOLVER FP64ï¼‰ï¼›
  - éªŒè¯äº†è·¨å¹³å°å¯ç§»æ¤æ€§å’Œæœ‰æ•ˆæ€§ã€‚

### æ¶ˆèå®éªŒåˆ†æ
- **é€’å½’æ·±åº¦å½±å“**ï¼ˆå›¾ 10ï¼‰ï¼š
  - åŠ é€Ÿæ¯”éšçŸ©é˜µå°ºå¯¸å¢å¤§è€Œçº¿æ€§å¢é•¿ï¼›
  - åœ¨ $n=65536$ æ—¶è¾¾åˆ° **5.32Ã— å³°å€¼åŠ é€Ÿ**ï¼›
  - å°çŸ©é˜µå› é€’å½’å¼€é”€è¿‡å¤§è€Œå¢ç›Šè¾ƒå°ã€‚
- **ç²¾åº¦å±‚æ•°å½±å“**ï¼š
  - æ›´å¤š FP16 å±‚å¸¦æ¥æ›´é«˜æ€§èƒ½ï¼Œä½†ç²¾åº¦é€æ­¥ä¸‹é™ï¼›
  - é€šè¿‡åœ¨é¡¶å±‚ä¿ç•™ FP32/FP64ï¼Œå¯åœ¨æ€§èƒ½ä¸ç²¾åº¦é—´å®ç°å¹³æ»‘æƒè¡¡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **åµŒå¥—é€’å½’ + åˆ†å±‚ç²¾åº¦ = é«˜æ€§èƒ½ä¸é«˜ç²¾åº¦å…¼å¾—**  
   é€šè¿‡å°†é€’å½’åº”ç”¨äº POTRFã€TRSM å’Œ SYRK å…¨æµç¨‹ï¼Œå¹¶ç»“åˆæ ‘çŠ¶æ··åˆç²¾åº¦æ§åˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²ç¨³å®šæ€§çš„å‰æä¸‹ï¼Œå……åˆ†é‡Šæ”¾ MXUs çš„ä½ç²¾åº¦è®¡ç®—æ½œåŠ›ã€‚

2. **GEMM å¯†é›†åŒ–æ˜¯ GPU åŠ é€Ÿçš„å…³é”®**  
   é€’å½’è®¾è®¡å°†å¤§éƒ¨åˆ†è®¡ç®—è½¬åŒ–ä¸º GEMM æ“ä½œï¼Œæå¤§æå‡äº†ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œå°¤å…¶é€‚åˆ Tensor Cores / Matrix Coresã€‚

3. **Julia æ˜¯å®ç°å¯ç§»æ¤é«˜æ€§èƒ½çº¿æ€§ä»£æ•°çš„ç†æƒ³è¯­è¨€**  
   å¤šé‡æ´¾å‘å’Œç±»å‹ç³»ç»Ÿä½¿å¾—åŒä¸€ä»£ç å¯æ— ç¼è¿è¡Œäºä¸åŒ GPU å¹³å°ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚

4. **æ··åˆç²¾åº¦éœ€â€œç²¾å‡†æŠ•æ”¾â€è€Œéå…¨å±€é™çº§**  
   å°† FP16 ç”¨äºéå¯¹è§’å—ï¼ˆè®¡ç®—å¯†é›†ä½†å®¹é”™æ€§å¼ºï¼‰ï¼Œä¿ç•™å¯¹è§’å—é«˜ç²¾åº¦ï¼Œæ˜¯ä¸€ç§é«˜æ•ˆä¸”ç¨³å¥çš„è®¾è®¡èŒƒå¼ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰å®ç°ä»ä¾èµ– vendor-specific BLAS åº“å¤„ç†åŸºç¡€æƒ…å†µï¼Œå°šæœªå®Œå…¨å®ç°â€œå…¨æ ˆç¡¬ä»¶æ— å…³â€ï¼›
- å¯¹æå°çŸ©é˜µï¼ˆ< 4Kï¼‰åŠ é€Ÿæ•ˆæœæœ‰é™ï¼Œé€’å½’å¼€é”€å ä¸»å¯¼ï¼›
- ç›®å‰ä»…æ”¯æŒç¨ å¯† SPD ç³»ç»Ÿï¼Œæœªæ‰©å±•è‡³ç¨€ç–æˆ–ä¸å®šçŸ©é˜µï¼›
- AMD å¹³å°æš‚æœªå¯ç”¨æ··åˆç²¾åº¦ GEMMï¼ˆGemmExï¼‰ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ä¸Šé™ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³æ›´å¹¿é—®é¢˜ç±»åˆ«**ï¼š
   - å—ç¨€ç–ï¼ˆblock-sparseï¼‰ã€å¸¦çŠ¶çŸ©é˜µï¼ˆbanded systemsï¼‰ï¼›
   - ä¸å®šç³»ç»Ÿçš„ LDL åˆ†è§£ã€‚
2. **å…¨æ ˆç¡¬ä»¶æ— å…³å®ç°**ï¼š
   - å®ç°é€’å½’å±‚çº§ä¸Šçš„å®Œå…¨ç¼–è¯‘å¯¼å‘è°ƒåº¦ï¼Œè¿ˆå‘ Apple Metal ç­‰æ–°æ¶æ„ã€‚
3. **è‡ªé€‚åº”ç²¾åº¦æ§åˆ¶**ï¼š
   - åŸºäºæ¡ä»¶æ•°ä¼°è®¡åŠ¨æ€è°ƒæ•´ç²¾åº¦å±‚çº§ã€‚
4. **å¤š GPU æ‰©å±•**ï¼š
   - ç»“åˆå¤§è§„æ¨¡çº¿æ€§ä»£æ•°åº“æ€æƒ³ï¼Œæ”¯æŒåˆ†å¸ƒå¼å†…å­˜ç¯å¢ƒã€‚
5. **èƒ½æ•ˆä¸å¼‚æ„è°ƒåº¦ä¼˜åŒ–**ï¼š
   - æ¢ç´¢èƒ½é‡æ„ŸçŸ¥çš„ä»»åŠ¡è°ƒåº¦ä¸å¼‚æ­¥æ‰§è¡Œæ¨¡å‹ã€‚

--- 

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼šæœ¬æ–‡é€šè¿‡**åµŒå¥—é€’å½’ç®—æ³• + æ ‘çŠ¶åˆ†å±‚æ··åˆç²¾åº¦ + Julia å¯ç§»æ¤å®ç°**ï¼Œåœ¨ MXUs ä¸Šå®ç°äº† Cholesky æ±‚è§£å™¨çš„ **5.3Ã— åŠ é€Ÿ**ï¼ŒåŒæ—¶è·å¾—æ¯”çº¯ FP16 **é«˜ 100 å€çš„ç²¾åº¦**ï¼Œä¸ºç§‘å­¦è®¡ç®—ä¸­çš„é«˜æ€§èƒ½ä¸é«˜ç²¾åº¦å¹³è¡¡æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 9. [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)

**Authors**: Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed Saifullah, Ali Jannesari  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.08166v1  

#### Abstract
Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platformsã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»ŸåµŒå…¥å¼å¹³å°çš„ **Dynamic Voltage and Frequency Scaling (DVFS)** å’Œä»»åŠ¡åˆ°æ ¸å¿ƒåˆ†é…ï¼ˆtask-to-core allocationï¼‰ç­–ç•¥å­˜åœ¨ä»¥ä¸‹å…³é”®ç¼ºé™·ï¼š
- **åŸºäºåˆ©ç”¨ç‡çš„å¯å‘å¼æ–¹æ³•**ï¼ˆå¦‚Linux `ondemand` governorï¼‰å¿½ç•¥ **stall time**ï¼Œæ— æ³•å‡†ç¡®åæ˜ çœŸå®è´Ÿè½½éœ€æ±‚ã€‚
- **è¡¨é©±åŠ¨æ–¹æ³•**ï¼ˆå¦‚Precise Schedulerï¼‰ä¾èµ–äºå¯¹æ‰€æœ‰é¢‘ç‡-æ ¸å¿ƒç»„åˆè¿›è¡Œç¦»çº¿æ€§èƒ½åˆ†æï¼Œè€—æ—¶é•¿è¾¾ **8â€“12å°æ—¶/åŸºå‡†ç¨‹åº**ï¼Œä¸”æ— æ³•é€‚åº”åŠ¨æ€å˜åŒ–çš„å·¥ä½œè´Ÿè½½ã€‚
- ç¼ºä¹å¯¹æ–°å·¥ä½œè´Ÿè½½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ¯æ¬¡æ–°å¢ç¨‹åºéƒ½éœ€è¦é‡æ–°æ‰§è¡Œå®Œæ•´çš„æ€§èƒ½åˆ†æã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡º **ZeroDVFS**ï¼Œä¸€ä¸ªåŸºäºæ¨¡å‹çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¡†æ¶ï¼Œç»“åˆ **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯­ä¹‰ç‰¹å¾æå–**ï¼Œå®ç° **é›¶æ ·æœ¬éƒ¨ç½²ï¼ˆzero-shot deploymentï¼‰** çš„çƒ­æ„ŸçŸ¥ä¸èƒ½æ•ˆä¼˜åŒ–è°ƒåº¦å™¨ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š
1. **LLMé©±åŠ¨çš„è¯­ä¹‰ç‰¹å¾æå–ï¼ˆLLM-based Semantic Feature Extractionï¼‰**
   - åˆ©ç”¨ **DeepSeek-V3ã€Claude Sonnetã€GPT-4o** ç­‰ LLM åˆ†æ OpenMP æºç ï¼Œæ— éœ€è¿è¡Œå³å¯æå– **13ä¸ªä»£ç çº§è¯­ä¹‰ç‰¹å¾**ï¼ˆå¦‚ç®—æ³•å¤æ‚åº¦ã€å†…å­˜è®¿é—®æ¨¡å¼ã€å¹¶è¡Œå¼€é”€ç­‰ï¼‰ã€‚
   - æ›¿ä»£ä¼ ç»Ÿçš„â€œåŸºå‡†æ ‡è¯†ç¬¦â€æˆ–ä»…è¯­æ³•ç‰¹å¾ï¼Œä½¿æ¨¡å‹å…·å¤‡å¯¹**æœªè§ç¨‹åº**çš„æ³›åŒ–èƒ½åŠ›ã€‚

2. **æ¨¡å‹é©±åŠ¨çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆModel-based Hierarchical MARLï¼‰**
   - é‡‡ç”¨ **Dyna-Q å¯å‘çš„æ¡†æ¶**ï¼Œç»“åˆç›´æ¥å¼ºåŒ–å­¦ä¹ ä¸åŸºäºç¯å¢ƒæ¨¡å‹çš„è§„åˆ’ï¼ˆplanningï¼‰ï¼Œæ˜¾è‘—æå‡æ ·æœ¬æ•ˆç‡ã€‚
   - å¼•å…¥ä¸¤ä¸ªåä½œæ™ºèƒ½ä½“ï¼š
     - **Profiler Agent**ï¼šè´Ÿè´£å†³å®šæ ¸å¿ƒæ•°é‡å’Œé¢‘ç‡ï¼Œä»¥æœ€å°åŒ–èƒ½è€—å’Œ **makespan**ã€‚
     - **Temperature Agent**ï¼šä¼˜å…ˆé€‰æ‹©æ¸©åº¦è¾ƒä½çš„æ ¸å¿ƒï¼Œé¿å…å±€éƒ¨çƒ­ç‚¹ã€‚
   - å°†æŒ‡æ•°çº§åŠ¨ä½œç©ºé—´åˆ†è§£ä¸ºå¯ç®¡ç†å­é—®é¢˜ï¼Œé™ä½å†³ç­–å¤æ‚åº¦ã€‚

3. **é›¶æ ·æœ¬è·¨å¹³å°è¿ç§»ï¼ˆZero-Shot Cross-Platform Transferï¼‰**
   - é€šè¿‡å°†å¹³å°æ— å…³ç‰¹å¾ï¼ˆå¦‚ç®—æ³•å¤æ‚åº¦ï¼‰ä¸å¹³å°ç›¸å…³ç‰¹å¾ï¼ˆå¦‚é¢‘ç‡ç­‰çº§ï¼‰åˆ†ç¦»ï¼Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨æ–°ç¡¬ä»¶ä¸Šç›´æ¥éƒ¨ç½²ã€‚
   - ç»“åˆå°‘é‡å¾®è°ƒï¼ˆfew-shot fine-tuningï¼‰ï¼Œå¿«é€Ÿé€‚é…ç›®æ ‡å¹³å°ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚Precise Schedulerï¼‰ | ZeroDVFS |
|------|-------------------------------|--------|
| éƒ¨ç½²å»¶è¿Ÿ | 8â€“12å°æ—¶/ç¨‹åºï¼ˆç¦»çº¿åˆ†æï¼‰ | **é¦–æ¬¡å†³ç­–3.5â€“8.0ç§’**ï¼ˆå«LLMæå–ï¼‰ |
| å†³ç­–å»¶è¿Ÿ | è¡¨æŸ¥æ‰¾ï¼šäºšæ¯«ç§’ | **åç»­å†³ç­–358ms**ï¼ˆPythonå®ç°ï¼‰ |
| èƒ½æ•ˆ | è¿‘æœ€ä¼˜ä½†é™æ€ | **7.09Ã— æ›´ä¼˜**ï¼ˆvs Linux ondemandï¼‰ |
| æ³›åŒ–èƒ½åŠ› | æ— ï¼ˆéœ€é‡æ–°åˆ†æï¼‰ | æ”¯æŒ**é›¶æ ·æœ¬éƒ¨ç½²**æ–°ç¨‹åº |
| æ ·æœ¬æ•ˆç‡ | éœ€å¤§é‡çœŸå®æ ·æœ¬ | **20Ã— æ›´å¿«æ”¶æ•›**ï¼ˆvs model-free RLï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **BOTS (Barcelona OpenMP Tasks Suite)**ï¼šåŒ…å«FFTã€StrassençŸ©é˜µä¹˜æ³•ã€N-Queensç­‰ä»»åŠ¡å¹¶è¡Œç¨‹åºã€‚
- **PolybenchC**ï¼šæ¶µç›–çº¿æ€§ä»£æ•°ã€Stencilè®¡ç®—ã€æ•°æ®æŒ–æ˜ç­‰å¾ªç¯å¹¶è¡Œç¨‹åºã€‚
- å…± **42ä¸ªåŸºå‡†ç¨‹åº**ï¼Œåœ¨å¤šä¸ªå¹³å°ä¸Šæ‰§è¡Œæ‰€æœ‰é¢‘ç‡-æ ¸å¿ƒç»„åˆï¼Œæ¯é…ç½®é‡å¤8æ¬¡ã€‚

### å®éªŒå¹³å°
| å¹³å° | æ¶æ„ | æ ¸å¿ƒæ•° | é¢‘ç‡ç­‰çº§ |
|------|------|--------|----------|
| **NVIDIA Jetson TX2** | ARM Cortex-A57 + Denver 2 | 6 | 12 |
| **Jetson Orin NX** | ARM Cortex-A78AE | 8 | 16 |
| **RubikPi** | Qualcomm Kryo 585 | 8 | 36 |
| **Intel Core i7 (8th Gen)** | x86_64 | 4 | å¤šçº§ |

### è¯„ä¼°æŒ‡æ ‡
- **Energy Efficiency**ï¼šæ€»èƒ½è€—ï¼ˆmJï¼‰
- **Makespan**ï¼šä»»åŠ¡å®Œæˆæ—¶é—´ï¼ˆsï¼‰
- **Temperature**ï¼šå¹³å‡æ ¸å¿ƒæ¸©åº¦ï¼ˆÂ°Cï¼‰
- **Decision Latency**ï¼šè°ƒåº¦å†³ç­–è€—æ—¶
- **Convergence Speed**ï¼šRLè®­ç»ƒæ”¶æ•›æ‰€éœ€episodeæ•°
- **MAPE**ï¼ˆMean Absolute Percentage Errorï¼‰ï¼šé¢„æµ‹è¯¯å·®
- **RÂ²**ï¼šè§£é‡Šæ–¹å·®æ¯”ä¾‹

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿æ–¹æ³• | ç±»å‹ |
|--------|------|
| **Linux ondemand governor** | åˆ©ç”¨ç‡é©±åŠ¨ |
| **Precise Scheduler [7]** | è¡¨é©±åŠ¨ï¼Œç¦»çº¿åˆ†æ |
| **zTT [30]** | å•æ™ºèƒ½ä½“ model-free RL |
| **DynaQ** | å•æ™ºèƒ½ä½“ model-based RL |
| **MAMF (Multi-Agent Model-Free)** | å¤šæ™ºèƒ½ä½“ model-free RL |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
- **èƒ½æ•ˆæå‡**ï¼šç›¸æ¯” **Linux ondemand governor**ï¼ŒZeroDVFS å®ç° **7.09Ã— æ›´å¥½èƒ½æ•ˆ**ã€‚
- **Makespan ä¼˜åŒ–**ï¼šç›¸æ¯” Precise Schedulerï¼Œ**makespan ç¼©çŸ­ 4.0Ã—**ã€‚
- **å†³ç­–å»¶è¿Ÿ**ï¼š
  - é¦–æ¬¡å†³ç­–ï¼ˆå«LLMæå–ï¼‰ï¼š**3.5â€“8.0ç§’**
  - åç»­å†³ç­–ï¼š**358ms**
- **é¦–æ¬¡å†³ç­–é€Ÿåº¦**ï¼šæ¯”è¡¨é©±åŠ¨æ–¹æ³•å¿« **8,300Ã—**ã€‚
- **æ”¶æ•›é€Ÿåº¦**ï¼šæ¯”çº¯ model-free æ–¹æ³•å¿« **20Ã—**ã€‚

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ï¼ˆBOTS FFT åŸºå‡†ï¼‰
| æ–¹æ³• | èƒ½è€— (mJ) | Makespan (s) | æ¸©åº¦ (Â°C) |
|------|-----------|--------------|-----------|
| **ZeroDVFS (MAMBRL D3QN)** | **9.1** | **1.13** | 42.1 |
| zTT | 31.2 | 1.93 | 43.6 |
| Precise Scheduler [7] | 75.5 | 5.96 | 44.0 |

> **ç»“è®º**ï¼šZeroDVFS åœ¨èƒ½è€—å’Œæ€§èƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰æ¨¡å‹é©±åŠ¨ vs. æ¨¡å‹æ— å…³ï¼ˆModel-based vs. Model-freeï¼‰
- **MAMBRL D3QN**ï¼ˆæ¨¡å‹é©±åŠ¨ï¼‰åœ¨ **20ä¸ªepisodeå†…æ”¶æ•›**ï¼Œè€Œ model-free æ–¹æ³•éœ€è¶…è¿‡400ä¸ªepisodeã€‚
- æ¨¡å‹é©±åŠ¨æ–¹æ³•é€šè¿‡åˆæˆæ•°æ®å¢å¼ºï¼Œæ˜¾è‘—æå‡æ ·æœ¬æ•ˆç‡ã€‚

#### ï¼ˆ2ï¼‰LLM ç‰¹å¾è´¡çŒ®åˆ†æ
- åœ¨å·²çŸ¥ç¨‹åºä¸Šï¼ŒåŠ å…¥LLMç‰¹å¾å¯¹é¢„æµ‹ç²¾åº¦æå‡æœ‰é™ï¼ˆRÂ² â‰ˆ 0.94ï¼‰ï¼Œå› å·²æœ‰ç¡¬ä»¶è®¡æ•°å™¨ä¸»å¯¼ã€‚
- **ä½†åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹ï¼ŒLLMç‰¹å¾æ˜¯å®ç°æ³›åŒ–çš„å…³é”®**ï¼Œä½¿æ¨¡å‹èƒ½é¢„æµ‹ä»æœªè§è¿‡çš„ç¨‹åºè¡Œä¸ºã€‚

#### ï¼ˆ3ï¼‰è·¨å¹³å°è¿ç§»æ•ˆæœ
| è¿ç§»è·¯å¾„ | MAPE | RÂ² |
|--------|------|-----|
| TX2 â†’ Orin NXï¼ˆé›¶æ ·æœ¬ï¼‰ | 64.5% | 0.90 |
| TX2 â†’ RubikPiï¼ˆé›¶æ ·æœ¬ï¼‰ | 73.2% | 0.80 |
| åŠ å…¥10ä¸ªå¾®è°ƒæ ·æœ¬å | â†“ è‡³ ~60% | â€” |

> å°½ç®¡ç»å¯¹è¯¯å·®è¾ƒé«˜ï¼Œä½†**ç›¸å¯¹æ’åºä¿æŒè‰¯å¥½**ï¼ˆSpearman Ï > 0.7ï¼‰ï¼Œè¶³ä»¥æ”¯æŒæœ‰æ•ˆè°ƒåº¦å†³ç­–ã€‚

#### ï¼ˆ4ï¼‰LLM ç‰¹å¾ä¸€è‡´æ€§
| ç‰¹å¾ | ä¸‰æ¨¡å‹ä¸€è‡´ç‡ (%) |
|------|------------------|
| dominant_operation | 73.8 |
| algorithmic_complexity | 59.5 |
| false_sharing_risk | 14.3 |
| cache_behavior_pattern | 16.7 |

> é«˜å…±è¯†ç‰¹å¾å¯é ï¼Œä½å…±è¯†ç‰¹å¾ç”±XGBoostè‡ªåŠ¨é™æƒå¤„ç†ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **LLM å¯æœ‰æ•ˆæå–è¯­ä¹‰ç‰¹å¾ç”¨äºæ€§èƒ½é¢„æµ‹**ï¼Œæ— éœ€ç¨‹åºæ‰§è¡Œå³å¯å®ç°é›¶æ ·æœ¬éƒ¨ç½²ã€‚
2. **æ¨¡å‹é©±åŠ¨çš„ MARL æ˜¾è‘—æå‡æ ·æœ¬æ•ˆç‡**ï¼Œç»“åˆè§„åˆ’æœºåˆ¶å®ç°å¿«é€Ÿæ”¶æ•›ã€‚
3. **ZeroDVFS åœ¨èƒ½æ•ˆå’Œæ€§èƒ½ä¸Šå…¨é¢è¶…è¶Šç°æœ‰æ–¹æ³•**ï¼Œå°¤å…¶é€‚åˆåŠ¨æ€åµŒå…¥å¼ç¯å¢ƒã€‚
4. **è·¨å¹³å°è¿ç§»å¯è¡Œ**ï¼Œå³ä½¿å­˜åœ¨æ¶æ„å·®å¼‚ï¼Œä»èƒ½ä¿æŒè¾ƒé«˜è°ƒåº¦æœ‰æ•ˆæ€§ã€‚
5. **LLM æå–æˆæœ¬æä½**ï¼šå•ç¨‹åºæˆæœ¬ **$0.0015â€“$0.018**ï¼Œè¿œä½äºä¼ ç»Ÿåˆ†æçš„äººåŠ›æˆæœ¬ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **å½“å‰ä»…æ”¯æŒå•DAG OpenMPç¨‹åº**ï¼Œä¸é€‚ç”¨äºå¹¶å‘æˆ–å¤šä»»åŠ¡æ··åˆè´Ÿè½½ã€‚
2. **é›¶æ ·æœ¬è·¨å¹³å° MAPE è¾ƒé«˜**ï¼ˆ64â€“73%ï¼‰ï¼Œä»éœ€å°‘é‡å¾®è°ƒä»¥æå‡ç²¾åº¦ã€‚
3. **LLM æå–é™äºå•æ–‡ä»¶ç¨‹åº**ï¼Œå¤šæ–‡ä»¶é¡¹ç›®éœ€æ‰‹åŠ¨æ‹¼æ¥æˆ–åˆ†å±‚åˆ†æã€‚
4. **ç¼ºä¹ç½®ä¿¡åŒºé—´é‡åŒ–**ï¼Œå®éªŒé‡å¤æ¬¡æ•°æœ‰é™ã€‚
5. **ä¾èµ–å•†ä¸šLLM API**ï¼Œè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²éœ€æœ¬åœ°åŒ–æ–¹æ¡ˆï¼ˆå¦‚CodeLlamaï¼‰ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. æ‰©å±•è‡³**å¹¶å‘å·¥ä½œè´Ÿè½½**å’Œ**å¤šæ–‡ä»¶é¡¹ç›®åˆ†æ**ã€‚
2. æ¢ç´¢**HPCä¸“ç”¨LLMå¾®è°ƒ**ï¼Œæå‡ç‰¹å¾æå–å‡†ç¡®æ€§ã€‚
3. ç ”ç©¶**GPUå¸è½½å†³ç­–**ä¸CPU-GPUååŒè°ƒåº¦ã€‚
4. å®ç°**æœ¬åœ°/è’¸é¦LLMéƒ¨ç½²**ï¼Œæ”¯æŒå®Œå…¨ç¦»çº¿è¾¹ç¼˜åœºæ™¯ã€‚
5. å¼€å±•æ›´ä¸¥æ ¼çš„ç»Ÿè®¡åˆ†æï¼ˆå¤šéšæœºç§å­ã€æ–¹å·®é‡åŒ–ï¼‰ã€‚
6. æ¢ç´¢**å¤šè½®æç¤º**ï¼ˆmulti-turn promptingï¼‰ä»¥æ·±å…¥ç†è§£å¤æ‚ä»£ç ç»“æ„ã€‚
7. å¼•å…¥**ä¸ç¡®å®šæ€§ä¼°è®¡**ï¼Œä¸ºæ¯ä¸ªç‰¹å¾æä¾›ç½®ä¿¡åº¦è¯„åˆ†ã€‚

---

> **æ€»ç»“**ï¼šZeroDVFS é€šè¿‡ **LLM + Model-based MARL** çš„åˆ›æ–°ç»„åˆï¼Œå®ç°äº†åµŒå…¥å¼ç³»ç»Ÿä¸­ **é«˜æ•ˆã€è‡ªé€‚åº”ã€é›¶æ ·æœ¬** çš„DVFSä¸ä»»åŠ¡è°ƒåº¦ï¼Œä¸ºåŠ¨æ€èµ„æºå—é™ç¯å¢ƒæä¾›äº†æå…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚

</details>

---

### 10. [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)

**Authors**: Abdelaziz Bounhar, Rania Hossam Elmohamady Elbadry, Hadi Abdine, Preslav Nakov, Michalis Vazirgiannis, Guokan Shang  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.08441v1  

#### Abstract
Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference dat...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šYaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
- **å¯†é›†æ¿€æ´»å¹²é¢„çš„å±€é™æ€§**ï¼šç°æœ‰çš„æ¿€æ´»å¹²é¢„æ–¹æ³•ï¼ˆå¦‚ CAA å’Œ BiPOï¼‰ä¾èµ–äºåœ¨åŸå§‹æ¨¡å‹çš„ dense activation ç©ºé—´ä¸­å­¦ä¹  steering vectorsï¼Œä½†ç”±äºç¥ç»å…ƒçš„ multi-semanticityï¼ˆå¤šä¹‰æ€§ï¼‰å’Œ superpositionï¼ˆå åŠ ç°è±¡ï¼‰ï¼Œè¿™äº›å‘é‡å¾€å¾€çº ç¼ å¤šä¸ªæ½œåœ¨è¡Œä¸ºå› å­ã€‚
- è¿™ç§çº ç¼ å¯¼è‡´åœ¨éœ€è¦ç»†ç²’åº¦æ§åˆ¶çš„ä»»åŠ¡ï¼ˆå¦‚æ–‡åŒ–å¯¹é½ã€truthfulness æ§åˆ¶ç­‰ï¼‰ä¸­å‡ºç°ä¸ç¨³å®šã€ä¸å¯è§£é‡Šä¸”æ•ˆæœæœ‰é™çš„é—®é¢˜ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šYaPO
- **YaPO (Yet another Policy Optimization)** æ˜¯ä¸€ç§**æ— å‚è€ƒï¼ˆreference-freeï¼‰**çš„æ–¹æ³•ï¼Œç”¨äºä»åå¥½æ•°æ®ä¸­å­¦ä¹ ç¨€ç–çš„æ¿€æ´»å¼•å¯¼å‘é‡ã€‚
- æ ¸å¿ƒæ€æƒ³æ˜¯å°† steering å­¦ä¹ è¿‡ç¨‹è½¬ç§»åˆ°ä¸€ä¸ªé¢„è®­ç»ƒçš„ **Sparse Autoencoder (SAE)** çš„éšç©ºé—´ä¸­ï¼Œåœ¨è¯¥ç©ºé—´å†…ä¼˜åŒ–ç¨€ç–ç¼–ç ï¼ˆsparse codesï¼‰æ¥ç”Ÿæˆ steering vectorsã€‚
- ç»“åˆäº† BiPO çš„å¯å­¦ä¹ æ€§ï¼ˆlearnable preference optimizationï¼‰ä¸ SAS çš„å¯è§£é‡Šæ€§ï¼ˆåŸºäºç¨€ç–ç‰¹å¾çš„æ“ä½œï¼‰ï¼Œå®ç°äº†æ›´è§£è€¦ï¼ˆdisentangledï¼‰ã€ç¨³å®šä¸”é«˜æ•ˆçš„æ§åˆ¶ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | ç‰¹ç‚¹ | å±€é™ |
|------|------|-------|
| **CAA** | ç®€å•å¹³å‡ contrastive prompts çš„æ¿€æ´»å·®å¼‚ | å›ºå®šæ–¹å‘ã€ç²—ç²’åº¦ã€æ˜“è¿‡æ­£åˆ™åŒ–é•¿æ–‡æœ¬ç”Ÿæˆ |
| **BiPO** | å¯å­¦ä¹  dense steering vectorsï¼Œé€šè¿‡ DPO-style objective ä¼˜åŒ– | å¯†é›†å‘é‡æ˜“çº ç¼ å¤šç§è¯­ä¹‰ï¼Œç¨³å®šæ€§å·® |
| **SAS** | åœ¨ SAE ç¨€ç–ç©ºé—´æ“ä½œï¼Œæå‡å¯è§£é‡Šæ€§ | ä½¿ç”¨é™æ€å¹³å‡ï¼Œç¼ºä¹åå¥½é©±åŠ¨çš„å­¦ä¹ èƒ½åŠ› |
| **YaPOï¼ˆæœ¬æ–‡ï¼‰** âœ… | âœ… åœ¨ SAE éšç©ºé—´è¿›è¡Œå¯å­¦ä¹ çš„ç¨€ç– steering<br>âœ… å®ç°è§£è€¦ã€é«˜æ•ˆã€ç¨³å®šçš„ fine-grained alignment<br>âœ… æ”¶æ•›æ›´å¿«ã€è®­ç»ƒæ›´ç¨³å®šã€æ³›åŒ–æ›´å¼º | â€”â€” |

> **åˆ›æ–°ç‚¹æ€»ç»“**ï¼š
> - é¦–æ¬¡æå‡ºåœ¨ SAE éšç©ºé—´ä¸­ç«¯åˆ°ç«¯åœ°å­¦ä¹ å¯è®­ç»ƒçš„ç¨€ç– steering vectorsã€‚
> - å¼•å…¥æ–°çš„æ–‡åŒ–å¯¹é½ benchmarkï¼Œæ¶µç›–äº”ç§è¯­è¨€å®¶æ—ã€åäº”ä¸ªæ–‡åŒ–èƒŒæ™¯ã€‚
> - æå‡ºå¹¶éªŒè¯ä¸¤ä¸ªæ›´åˆç†çš„è¯„ä¼°æŒ‡æ ‡ï¼š**RCA** å’Œ **PNLG**ï¼Œè§£å†³ä¼ ç»Ÿâ€œæœ¬åœ°åŒ–å·®è·â€è¯„ä¼°ä¸­çš„åå·®é—®é¢˜ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- è‡ªå»ºå¤šè¯­è¨€æ–‡åŒ–å¯¹é½ benchmarkï¼š
  - è¦†ç›– **5 ç§è¯­è¨€å®¶æ—**ï¼šArabic, Spanish, English, Portuguese, Chinese
  - åŒ…å« **15 ä¸ªå…·ä½“æ–‡åŒ–åŒºåŸŸ**ï¼ˆå¦‚ Egypt, KSA, Levantine, Spain, Mexico, USA ç­‰ï¼‰
  - åŒ…æ‹¬ä¸¤ç§ä»»åŠ¡å½¢å¼ï¼š
    - **MCQï¼ˆå¤šé¡¹é€‰æ‹©é¢˜ï¼‰**
    - **Open-ended generationï¼ˆå¼€æ”¾å¼ç”Ÿæˆï¼‰**
  - æ‰€æœ‰å“åº”ç”± Gemini ç”Ÿæˆåäººå·¥ç­›é€‰æ•´ç†ï¼Œç¡®ä¿è·¨å›½å®¶ä¸€è‡´æ€§ã€‚

### å®éªŒè®¾ç½®
- **åŸºç¡€æ¨¡å‹**ï¼šGemma-2-2B-it
- **å¹²é¢„æ–¹å¼**ï¼šåœ¨æ¨ç†æ—¶æ³¨å…¥ steering vector åˆ° residual stream çš„ç‰¹å®šå±‚ï¼ˆä¸ä¿®æ”¹åŸæ¨¡å‹æƒé‡ï¼‰
- **è®­ç»ƒç›®æ ‡**ï¼šé‡‡ç”¨ BiPO-style çš„åŒå‘åå¥½ä¼˜åŒ–ç›®æ ‡ï¼ˆbi-directional DPO-style objectiveï¼‰
- **SAE æ„é€ **ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„ Sparse Autoencoder å°†åŸå§‹æ¿€æ´»æŠ•å½±è‡³é«˜ç»´ç¨€ç–ç©ºé—´ï¼ˆ~1M featuresï¼‰

### è¯„ä¼°æŒ‡æ ‡
#### ï¼ˆ1ï¼‰Robust Cultural Accuracy (RCA)
$$
\text{RCA} = \mathbb{E}\left[\frac{2p_{\text{loc}} p_{\text{non}}}{p_{\text{loc}} + p_{\text{non}} + \epsilon}\right]
$$
- æ›´å¼ºè°ƒæ¨¡å‹åœ¨ **æœ¬åœ°åŒ–ï¼ˆlocalizedï¼‰** ä¸ **éæœ¬åœ°åŒ–ï¼ˆnon-localizedï¼‰** æç¤ºä¸‹çš„å¹³è¡¡è¡¨ç°ã€‚
- è¶Šé«˜è¶Šå¥½ â†’ è¡¨ç¤ºé²æ£’çš„æ–‡åŒ–é€‚åº”èƒ½åŠ›ã€‚

#### ï¼ˆ2ï¼‰Performance-Normalized Localization Gap (PNLG)
$$
\text{PNLG} = \mathbb{E}\left[\frac{p_{\text{loc}} - p_{\text{non}}}{(p_{\text{loc}} + p_{\text{non}})/2 + \epsilon}\right]
$$
- å¯¹åŸå§‹ localization gap è¿›è¡Œå½’ä¸€åŒ–ï¼Œé¿å…ä½æ€§èƒ½æ¨¡å‹å› æ•´ä½“å‡†ç¡®ç‡ä½è€Œè¡¨ç°å‡ºâ€œå° gapâ€çš„å‡è±¡ã€‚
- è¶Šä½è¶Šå¥½ â†’ è¡¨ç¤ºæ›´å¥½çš„éšå¼æ–‡åŒ–è¿ç§»èƒ½åŠ›ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æ˜¯å¦å¯å­¦ä¹  | æ˜¯å¦ç¨€ç– | æ˜¯å¦ä½¿ç”¨åå¥½æ•°æ® |
|------|------|------------|----------|------------------|
| **No Steering (Base)** | None | âŒ | âŒ | âŒ |
| **CAA** | Dense | âŒ | âŒ | âŒ |
| **SAS** | Sparse | âŒ | âœ… | âŒ |
| **BiPO** | Dense | âœ… | âŒ | âœ… |
| **YaPO**ï¼ˆæœ¬æ–‡ï¼‰ | Sparse | âœ… | âœ… | âœ… |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆä»¥é˜¿æ‹‰ä¼¯è¯­å’Œè‘¡è„ç‰™è¯­ä¸ºä¾‹ï¼‰

#### è¡¨ï¼šMCQ & Open-Ended ä»»åŠ¡ä¸Šçš„ RCA å’Œ PNLGï¼ˆHigher RCA / Lower PNLG æ›´å¥½ï¼‰

| Language | Method | RCA â†‘ | PNLG â†“ |
|---------|--------|--------|--------|
| **Arabic** | Base | 20.1 | 0.129 |
|           | CAA   | 19.2 | 0.167 |
|           | SAS   | 21.3 | 0.098 |
|           | BiPO  | 22.2 | 0.141 |
|           | **YaPO** | **23.5** | **0.098** |
| **Portuguese** | Base | 23.8 | 0.184 |
|               | CAA   | 37.5 | 0.192 |
|               | SAS   | 36.5 | 0.113 |
|               | BiPO  | 29.3 | 0.126 |
|               | **YaPO** | **40.8** | **0.165** |

> âœ… YaPO åœ¨æ‰€æœ‰è¯­è¨€ä¸Šå‡å–å¾—æœ€é«˜çš„ **RCA**ï¼Œè¡¨æ˜å…¶å…·å¤‡æœ€å¼ºçš„æ–‡åŒ–é²æ£’æ€§ï¼›
> åŒæ—¶ä¿æŒè¾ƒä½çš„ **PNLG**ï¼Œè¯´æ˜å®ƒèƒ½æœ‰æ•ˆç¼©å°æ˜¾å¼ vs éšå¼æç¤ºä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚

---

### å…¶ä»–é‡è¦å®éªŒç»“æœ

#### ï¼ˆ1ï¼‰MMLU ä¸Šçš„çŸ¥è¯†ä¿ç•™æµ‹è¯•
- æ‰€æœ‰ steering æ–¹æ³•ï¼ˆåŒ…æ‹¬ YaPOï¼‰åœ¨ MMLU ä¸Šçš„è¡¨ç°ä¸ base model å‡ ä¹ä¸€è‡´ï¼ˆçº¦ 57.58%ï¼‰ï¼Œ**æ²¡æœ‰æ˜¾è‘—é€€åŒ–**ã€‚
- è¯´æ˜ YaPO å¹²é¢„ä¸»è¦å½±å“å¯¹é½ç›¸å…³è¡Œä¸ºï¼Œ**ä¸å½±å“é€šç”¨çŸ¥è¯†èƒ½åŠ›**ã€‚

#### ï¼ˆ2ï¼‰æ•æ„Ÿæ€§åˆ†æï¼ˆSteering Multiplier Î»ï¼‰
- å›¾ 4 æ˜¾ç¤ºï¼š**CAA å’Œ SAS å¯¹ Î» æå…¶æ•æ„Ÿ**ï¼Œç¨å¤§å³å´©æºƒï¼ˆover-steeringï¼‰ï¼›
- è€Œ **YaPO å’Œ BiPO æ›´ç¨³å¥**ï¼Œå°¤å…¶ YaPO åœ¨ Î»=1.5~2.0 ä»èƒ½ç»´æŒé«˜æ€§èƒ½ï¼Œä½“ç°å…¶ç¨³å®šæ€§ä¼˜åŠ¿ã€‚

#### ï¼ˆ3ï¼‰æ³›åŒ–åˆ°å…¶ä»– alignment ä»»åŠ¡
| Task | CAA | SAS | BiPO | YaPO |
|------|-----|-----|------|------|
| Wealth-Seeking | 2.23 | 2.14 | 2.17 | **2.31** |
| Jailbreak       | 1.08 | 1.00 | 1.02 | **1.00** |
| Power-Seeking   | 2.09 | 1.81 | 1.93 | **2.03** |
| Hallucination   | 2.18 | 1.46 | 1.60 | **1.69** |
| **Average**     | **1.90** | 1.60 | 1.68 | **1.76** |

- YaPO åœ¨å¤šæ•°ä»»åŠ¡ä¸­ä»…æ¬¡äº CAAï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ **æ›´ç¨³å®šã€æ›´å°‘ä¾èµ–è¶…å‚è°ƒä¼˜**ã€‚
- CAA è™½å¾—åˆ†é«˜ä½†éå¸¸ brittleï¼ˆè„†å¼±ï¼‰ï¼Œéš¾ä»¥éƒ¨ç½²ã€‚

#### ï¼ˆ4ï¼‰æ”¶æ•›é€Ÿåº¦ä¸è®­ç»ƒç¨³å®šæ€§
- YaPO æ¯” BiPO æ”¶æ•›æ›´å¿«ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ loss æ³¢åŠ¨æ›´å°ã€‚
- å½’å› äºç¨€ç–è¡¨ç¤ºå¸¦æ¥çš„æ¢¯åº¦æ›´æ¸…æ™°ã€ä¼˜åŒ–è·¯å¾„æ›´å¹³æ»‘ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **ç¨€ç–ç©ºé—´ä¸­çš„å¯å­¦ä¹  steering æ›´ä¼˜**ï¼šåœ¨ SAE éšç©ºé—´ä¸­å­¦ä¹  sparse steering vectors èƒ½æœ‰æ•ˆç¼“è§£ neuron multi-semanticity å¸¦æ¥çš„çº ç¼ é—®é¢˜ï¼Œå®ç°æ›´ç²¾ç»†ã€ç¨³å®šçš„è¡Œä¸ºè°ƒæ§ã€‚
2. âœ… **YaPO å®ç°äº†æ€§èƒ½ä¸ç¨³å®šæ€§çš„ç»Ÿä¸€**ï¼šç›¸æ¯” dense æ–¹æ³•ï¼ˆBiPOï¼‰ï¼ŒYaPO æ›´ç¨³å®šï¼›ç›¸æ¯”é™æ€ç¨€ç–æ–¹æ³•ï¼ˆSASï¼‰ï¼ŒYaPO å¯å­¦ä¹ ä¸”æ€§èƒ½æ›´å¼ºã€‚
3. âœ… **æ— éœ€ç‰ºç‰²é€šç”¨èƒ½åŠ›å³å¯å®Œæˆé¢†åŸŸé€‚é…**ï¼šYaPO åœ¨æå‡æ–‡åŒ–å¯¹é½çš„åŒæ—¶ï¼Œ**æœªæŸå®³ MMLU ç­‰é€šç”¨ä»»åŠ¡è¡¨ç°**ï¼Œè¯æ˜å…¶ä¸ºè½»é‡çº§ã€å®šå‘å¹²é¢„ã€‚
4. âœ… **é€‚ç”¨äºå¤šç§ alignment åœºæ™¯**ï¼šé™¤æ–‡åŒ–å¯¹é½å¤–ï¼ŒYaPO åœ¨ hallucination æŠ‘åˆ¶ã€wealth/power-seeking æ§åˆ¶ç­‰æ–¹é¢ä¹Ÿå±•ç°å‡ºè‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- ä¾èµ–é«˜è´¨é‡çš„ **é¢„è®­ç»ƒ SAE**ï¼Œæ„å»ºå’Œè®­ç»ƒ SAE æœ¬èº«æœ‰ä¸€å®šæˆæœ¬ã€‚
- å½“å‰å®éªŒé›†ä¸­åœ¨ Gemma-2-2Bï¼Œå°šæœªåœ¨æ›´å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ 70B+ï¼‰ä¸Šå…¨é¢éªŒè¯ã€‚
- ç¨€ç–ç©ºé—´ç»´åº¦æé«˜ï¼ˆç™¾ä¸‡çº§ï¼‰ï¼Œè™½ç„¶ç¨€ç–ä½†ä»å­˜åœ¨å­˜å‚¨ä¸æ£€ç´¢å¼€é”€ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ›´é«˜æ•ˆçš„ SAE æ¶æ„æˆ–åœ¨çº¿å­¦ä¹ æœºåˆ¶ï¼Œé™ä½éƒ¨ç½²é—¨æ§›ã€‚
- å°† YaPO æ‰©å±•è‡³å¤šæ¨¡æ€ LLM æˆ– agent å†³ç­–ç³»ç»Ÿä¸­çš„è¡Œä¸ºè°ƒæ§ã€‚
- ç ”ç©¶å¦‚ä½•è‡ªåŠ¨è¯†åˆ«éœ€å¹²é¢„çš„ SAE feature å­é›†ï¼Œå®ç°åŠ¨æ€ã€æŒ‰éœ€ steeringã€‚
- æ„å»ºæ›´å¤§è§„æ¨¡çš„æ–‡åŒ–ä¸ä»·å€¼è§‚ benchmarkï¼Œæ”¯æŒè·¨æ–‡æ˜æ·±åº¦å¯¹é½ç ”ç©¶ã€‚

---

> ğŸ”— **ä»£ç ä¸æ•°æ®å…¬å¼€**ï¼š  
> é¡¹ç›®ä»“åº“å·²å¼€æºï¼š[https://github.com/MBZUAI-Paris/YaPO](https://github.com/MBZUAI-Paris/YaPO)

</details>

---

### 11. [Where to Split? A Pareto-Front Analysis of DNN Partitioning for Edge Inference](https://arxiv.org/abs/2601.08025)

**Authors**: Adiba Masud, Nicholas Foley, Pragathi Durga Rajarajan, Palden Lama  
**Category**: cs.DC  
**Published**: 2026-01-14  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.08025v1  

#### Abstract
The deployment of deep neural networks (DNNs) on resource-constrained edge devices is frequently hindered by their significant computational and memory requirements. While partitioning and distributing a DNN across multiple devices is a well-established strategy to mitigate this challenge, prior res...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Where to Split? A Pareto-Front Analysis of DNN Partitioning for Edge Inference*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰é¢ä¸´**è®¡ç®—èµ„æºå—é™**å’Œ**å†…å­˜ä¸è¶³**çš„æŒ‘æˆ˜ã€‚è™½ç„¶æ¨¡å‹åˆ†å‰²ï¼ˆDNN partitioningï¼‰æ˜¯ä¸€ç§å¸¸è§è§£å†³æ–¹æ¡ˆï¼Œä½†å¤§å¤šæ•°ç°æœ‰ç ”ç©¶ä»…å…³æ³¨å•ä¸€ä¼˜åŒ–ç›®æ ‡ï¼ˆå¦‚æœ€å°åŒ–å»¶è¿Ÿæˆ–æœ€å¤§åŒ–ååé‡ï¼‰ï¼Œå¿½ç•¥äº†ç°å®åœºæ™¯ä¸­**å»¶è¿Ÿä¸ååé‡ä¹‹é—´çš„å¤æ‚æƒè¡¡å…³ç³»**ï¼Œå°¤å…¶æ˜¯åœ¨ç½‘ç»œæ¡ä»¶å¤šå˜çš„æƒ…å†µä¸‹ã€‚

æ­¤å¤–ï¼Œé€šä¿¡å¼€é”€ï¼ˆå¦‚ä½¿ç”¨ PyTorch RPCï¼‰å¸¸è¢«å¿½è§†ï¼Œå¯¼è‡´å®éªŒå®¤ç†æƒ³ç¯å¢ƒä¸‹çš„æœ€ä¼˜ç­–ç•¥åœ¨çœŸå®éƒ¨ç½²ä¸­è¡¨ç°ä¸ä½³ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºå°† DNN åˆ†å‰²å»ºæ¨¡ä¸ºä¸€ä¸ª**å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜**ï¼Œå¹¶å¼•å…¥ **ParetoPipe** â€”â€” ä¸€ä¸ªå¼€æºçš„åˆ†å¸ƒå¼æ¨ç†æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- åˆ©ç”¨ **Pareto Front åˆ†æ** æ¥ç³»ç»Ÿè¯†åˆ«æ‰€æœ‰â€œä¸å¯æ”¯é…â€çš„åˆ†å‰²ç­–ç•¥é›†åˆï¼Œå³æ— æ³•åœ¨ä¸ç‰ºç‰²ä¸€ä¸ªæŒ‡æ ‡çš„å‰æä¸‹æå‡å¦ä¸€ä¸ªæŒ‡æ ‡çš„æ‰€æœ‰æ–¹æ¡ˆã€‚
- æ”¯æŒå¯¹ **latency** å’Œ **throughput** è¿›è¡Œè”åˆåˆ†æï¼Œæ­ç¤ºäºŒè€…ä¹‹é—´çš„æœ¬è´¨æƒè¡¡ã€‚
- è®¾è®¡åŒé€šä¿¡åç«¯ï¼ˆPyTorch RPC + è‡ªå®šä¹‰è½»é‡çº§ TCP Socket å®ç°ï¼‰ï¼Œä»¥ç²¾ç¡®è¯„ä¼°è¿è¡Œæ—¶å¼€é”€çš„å½±å“ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼ ç»Ÿæ–¹æ³• | æœ¬æ–‡æ–¹æ³•ï¼ˆParetoPipeï¼‰ |
|------|--------|-----------------------|
| ä¼˜åŒ–ç›®æ ‡ | å•ç›®æ ‡ï¼ˆä»… latency æˆ– throughputï¼‰ | å¤šç›®æ ‡ï¼ˆlatency & throughput è”åˆä¼˜åŒ–ï¼‰ |
| å†³ç­–ä¾æ® | â€œæœ€ä¼˜â€å•ä¸€ç‚¹ | å®Œæ•´çš„ Pareto Frontierï¼Œæä¾›å†³ç­–ç©ºé—´ |
| ç½‘ç»œå½±å“ | å¿½è§†æˆ–ç®€åŒ– | æ˜¾å¼å»ºæ¨¡ç½‘ç»œå»¶è¿Ÿä¸å¸¦å®½é™åˆ¶ |
| é€šä¿¡å¼€é”€ | ä½¿ç”¨æ ‡å‡† RPC æ¡†æ¶ï¼ˆé«˜å¼€é”€ï¼‰ | æä¾›è½»é‡çº§è‡ªå®šä¹‰é€šä¿¡åç«¯ï¼Œæ˜¾è‘—é™ä½å¼€é”€ |
| å¯å¤ç°æ€§ä¸å·¥å…·æ”¯æŒ | å°é—­æˆ–ä¸“ç”¨ç³»ç»Ÿ | å¼€æºæ¡†æ¶ï¼Œæ”¯æŒå¼‚æ„ç¡¬ä»¶ä¸çµæ´»å®éªŒ |

> âœ… **åˆ›æ–°ç‚¹æ€»ç»“**ï¼š
> - é¦–æ¬¡ç³»ç»Ÿæ€§åœ°é€šè¿‡ Pareto Front åˆ†ææ­ç¤ºè¾¹ç¼˜ç¯å¢ƒä¸‹ DNN åˆ†å‰²çš„æ€§èƒ½è¾¹ç•Œï¼›
> - å‘å¸ƒäº†å¯æ‰©å±•ã€å¼€æºçš„å·¥å…·é“¾ ParetoPipeï¼Œä¿ƒè¿›åç»­ç ”ç©¶ï¼›
> - æ­ç¤ºäº†ç½‘ç»œç“¶é¢ˆä¸‹â€œè¿‡åº¦å¸è½½â€åè€ŒåŠ£åŒ–çš„ç°è±¡ï¼ŒæŒ‘æˆ˜ç›´è§‰è®¤çŸ¥ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†ä¸æ¨¡å‹
- **æ•°æ®é›†**ï¼šCIFAR-10ï¼ˆç”¨äºå¾®è°ƒï¼‰
- **é¢„è®­ç»ƒæ¨¡å‹æ¥æº**ï¼šImageNet ä¸Šçš„æ ‡å‡† PyTorch é¢„è®­ç»ƒæƒé‡
- **æµ‹è¯•çš„ DNN æ¶æ„**ï¼ˆå…±6ä¸ªï¼‰ï¼š
  - MobileNetV2
  - ResNet18 / ResNet50
  - InceptionV3
  - AlexNet
  - VGG16  
  > â†’ è¦†ç›–ä»è½»é‡åˆ°é‡å‹ã€ä¸åŒç»“æ„å¤æ‚åº¦çš„å…¸å‹ CNN æ¨¡å‹ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
#### ç¡¬ä»¶å¹³å°ï¼ˆHeterogeneous Testbedï¼‰
| è®¾å¤‡ | é…ç½® |
|------|------|
| **Edge Workers** | 2 Ã— Raspberry Pi 4Bï¼ˆ4æ ¸ CPUï¼Œ4GB RAMï¼‰ |
| **Edge Server** | NVIDIA GeForce RTX 4090 GPUï¼Œ32æ ¸ CPUï¼Œ124GB RAM |
| **ç½‘ç»œè¿æ¥** | æœ‰çº¿ä»¥å¤ªç½‘ LANï¼ˆåŸºç¡€ RTT ~0.2â€“0.4msï¼‰ |
| **ç½‘ç»œæ¨¡æ‹Ÿå·¥å…·** | `tc`ï¼ˆLinux traffic controlï¼‰ç”¨äºæ³¨å…¥å»¶è¿Ÿï¼ˆ200msï¼‰å’Œé™é€Ÿï¼ˆ5 Mbit/sï¼‰ |

#### éƒ¨ç½²æ¨¡å¼
- **Pi-to-Pi**ï¼šä¸¤ä¸ªåˆ†åŒºå‡åœ¨æ ‘è“æ´¾ä¸Šæ‰§è¡Œ
- **Pi-to-GPU**ï¼šç¬¬ä¸€æ®µåœ¨ Pi æ‰§è¡Œï¼Œç¬¬äºŒæ®µå¸è½½è‡³ GPU æœåŠ¡å™¨

#### åˆ†å‰²ç²’åº¦
- åœ¨ **block è¾¹ç•Œ** è¿›è¡Œåˆ†å‰²ï¼ˆéé€å±‚ï¼‰ï¼Œæ¯ä¸ª block æ˜¯ä¸€ç»„åŠŸèƒ½ç›¸å…³çš„å±‚ï¼ˆå¦‚ conv blockã€inception blockï¼‰

#### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **End-to-end Latency** | å• batch æ¨ç†çš„æ€»è€—æ—¶ï¼ˆç§’ï¼‰ |
| **Throughput** | æ¯ç§’å¤„ç†å›¾åƒæ•°ï¼ˆimgs/sï¼‰ |
| **Resource Utilization** | CPU åˆ©ç”¨ç‡ã€å†…å­˜å ç”¨ã€ç½‘ç»œä¼ è¾“æ—¶é—´ |
| **Pareto Optimality** | æ˜¯å¦å±äº Pareto å‰æ²¿ä¸Šçš„ç‚¹ï¼ˆæ— æ”¯é…è§£é›†ï¼‰ |

#### åŸºçº¿å¯¹æ¯”
- **é€šä¿¡åç«¯å¯¹æ¯”**ï¼š
  - PyTorch RPCï¼ˆTensorPipe backendï¼‰
  - è‡ªç ”è½»é‡çº§ TCP Socket å®ç°
- **åˆ†å‰²ç­–ç•¥åˆ†æ**ï¼šéå†æ‰€æœ‰åˆæ³•åˆ†å‰²ç‚¹ï¼Œç»˜åˆ¶å®Œæ•´ Pareto Frontier

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰Pareto Front åˆ†æç»“æœï¼ˆFig. 3 & 4ï¼‰
- åœ¨ Pi-to-Pi åœºæ™¯ä¸­ï¼š
  - è½»é‡æ¨¡å‹ï¼ˆå¦‚ MobileNetV2ã€AlexNetï¼‰æ•´ä½“è¡¨ç°ä¼˜äºé‡å‹æ¨¡å‹ï¼ˆVGG16ã€ResNet50ï¼‰
  - æœ€ä¼˜åˆ†å‰²ç‚¹å–å†³äºå„ block çš„è®¡ç®—è´Ÿè½½åˆ†å¸ƒï¼š
    - AlexNetã€InceptionV3ã€VGG16ï¼šé€‚åˆ**å¯¹ç§°åˆ†å‰²**
    - MobileNetV2ã€ResNet18ã€ResNet50ï¼šéœ€**éå¯¹ç§°åˆ†å‰²**ï¼ˆæ—©æœŸå—æ›´é‡ï¼‰
    - ä¾‹ï¼šMobileNetV2 åœ¨ P3ï¼ˆå‰3å—ç•™æœ¬åœ°ï¼‰è¾¾åˆ°æœ€é«˜åå

- åœ¨ Pi-to-GPU åœºæ™¯ä¸­ï¼š
  - ååå¤§å¹…æå‡ï¼Œå»¶è¿Ÿæ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶å¯¹å¤§æ¨¡å‹ï¼ˆResNet50ã€VGG16ï¼‰
  - Pareto Front **éå¸¸ç¨€ç–**ï¼Œé€šå¸¸åªæœ‰ä¸€ä¸ªä¸»å¯¼ç­–ç•¥ï¼š
    - å¦‚ MobileNetV2 åº”åœ¨ P1 åˆ†å‰²ï¼ˆå‡ ä¹å…¨éƒ¨å¸è½½åˆ° GPUï¼‰

#### ï¼ˆ2ï¼‰ç½‘ç»œå‹åŠ›ä¸‹çš„æ€§èƒ½é€€åŒ–ï¼ˆFig. 5 & 6ï¼‰
- æ³¨å…¥ **200ms å»¶è¿Ÿ + 5 Mbit/s å¸¦å®½é™åˆ¶** åï¼š
  - æ‰€æœ‰æ¨¡å‹çš„ Pareto Front æ•´ä½“å³ç§»ï¼ˆå»¶è¿Ÿâ†‘ï¼‰ä¸”ä¸‹ç§»ï¼ˆååâ†“ï¼‰
  - GPU åŠ é€Ÿä¼˜åŠ¿è¢«ä¸¥é‡å‰Šå¼±ï¼šGPU å¤§éƒ¨åˆ†æ—¶é—´ç©ºç­‰ä¸­é—´æ¿€æ´»å€¼
  - **æ—©å¸è½½ç­–ç•¥ï¼ˆearly offloadingï¼‰ä»£ä»·é«˜æ˜‚**ï¼šäº§ç”Ÿå¤§é‡éœ€ä¼ è¾“çš„ä¸­é—´å¼ é‡ï¼ŒåŠ å‰§é€šä¿¡ç“¶é¢ˆ
  - ç»“è®ºï¼šåœ¨ç½‘ç»œå—é™æ—¶ï¼Œåº”**å‡å°‘é€šä¿¡é‡**ï¼Œå€¾å‘äºåœ¨è¾¹ç¼˜ä¾§å®Œæˆæ›´å¤šè®¡ç®—

#### ï¼ˆ3ï¼‰é€šä¿¡åç«¯æ€§èƒ½å¯¹æ¯”ï¼ˆFig. 7ï¼‰
| æŒ‡æ ‡ | PyTorch RPCï¼ˆP15ï¼‰ | Custom TCPï¼ˆP3ï¼‰ | æå‡ |
|------|--------------------|------------------|------|
| Throughput | 5.1 imgs/s | **7.8 imgs/s** | â†‘53% |
| End-to-end Latency | 1.14 s | **0.27 s** | â†“76% |
| RPC Overhead | æ˜¾è‘—å­˜åœ¨ | å‡ ä¹ä¸ºé›¶ | â€” |

> âœ… è‡ªå®šä¹‰é€šä¿¡åç«¯æå¤§é™ä½äº†åè°ƒå¼€é”€ï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™è¾¹ç¼˜ç¯å¢ƒã€‚

---

### ğŸ“‹ æ€§èƒ½åˆ†è§£è¡¨ï¼ˆTable II & IIIï¼‰
- **é«˜ååç­–ç•¥**ï¼šä¸¤è®¾å¤‡ CPU åˆ©ç”¨ç‡å‡æ¥è¿‘é¥±å’Œï¼ˆ>300%ï¼Œå¤šæ ¸å¹¶è¡Œï¼‰
- **ä½å»¶è¿Ÿç­–ç•¥**ï¼šå¸¸ä¼´éšèµ„æºåˆ©ç”¨ä¸å‡è¡¡ï¼ˆä¸€ç«¯å¿™ï¼Œä¸€ç«¯é—²ï¼‰
- Pi-to-GPU è®¾ç½®ä¸‹ Pareto Front å¤šä¸ºå•ç‚¹ï¼Œè¯´æ˜å¸è½½ç­–ç•¥é«˜åº¦é›†ä¸­

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **DNN åˆ†å‰²æœ¬è´¨ä¸Šæ˜¯å¤šç›®æ ‡æƒè¡¡é—®é¢˜**ï¼š
   - ä¸å­˜åœ¨å…¨å±€â€œæœ€ä¼˜â€åˆ†å‰²ç‚¹ï¼Œå¿…é¡»ç»“åˆåº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„ Pareto ç‚¹ã€‚
   
2. **ç½‘ç»œæ˜¯ç¬¬ä¸€ç±»æ€§èƒ½ç“¶é¢ˆ**ï¼š
   - åœ¨é«˜å»¶è¿Ÿæˆ–ä½å¸¦å®½æ¡ä»¶ä¸‹ï¼Œå³ä½¿æ‹¥æœ‰å¼ºå¤§ GPUï¼Œä¹Ÿæ— æ³•å‘æŒ¥å…¶ç®—åŠ›ä¼˜åŠ¿ã€‚
   - â€œè¶Šå¤šå¸è½½è¶Šå¥½â€å¹¶éæ€»æ˜¯æ­£ç¡®ï¼›æœ‰æ—¶åº”åœ¨è¾¹ç¼˜ä¿ç•™æ›´å¤šè®¡ç®—ä»¥å‡å°é€šä¿¡é‡ã€‚

3. **Block-level profiling è‡³å…³é‡è¦**ï¼š
   - ä¸åŒ block çš„è®¡ç®—æˆæœ¬å·®å¼‚å·¨å¤§ï¼Œç›²ç›®æŒ‰å±‚æ•°å¹³å‡åˆ†å‰²ä¼šå¯¼è‡´è´Ÿè½½å¤±è¡¡ã€‚
   - å¿…é¡»è¿›è¡Œç»†ç²’åº¦æ€§èƒ½å‰–ææ‰èƒ½æ‰¾åˆ°çœŸæ­£é«˜æ•ˆçš„åˆ†å‰²ç‚¹ã€‚

4. **æ ‡å‡† RPC æ¡†æ¶å¼€é”€è¿‡å¤§**ï¼š
   - PyTorch RPC å¼•å…¥æ˜¾è‘—é¢å¤–å»¶è¿Ÿï¼Œåœ¨è¾¹ç¼˜åœºæ™¯ä¸­ä¸å¯æ¥å—ã€‚
   - è½»é‡çº§é€šä¿¡æœºåˆ¶ï¼ˆå¦‚ TCP + æ‰‹åŠ¨åºåˆ—åŒ–ï¼‰æ›´å…·å®ç”¨æ€§ã€‚

5. **Pareto Front å¯æŒ‡å¯¼å®é™…éƒ¨ç½²å†³ç­–**ï¼š
   - æä¾›å¯è§†åŒ–çš„æ€§èƒ½è¾¹ç•Œï¼Œå¸®åŠ©å¼€å‘è€…ç†è§£ trade-offï¼Œå¹¶é’ˆå¯¹ç‰¹å®šç½‘ç»œç¯å¢ƒé€‰æ‹©é²æ£’ç­–ç•¥ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- å½“å‰ä»…æ”¯æŒ **two-stage pipeline**ï¼ˆä¸¤é˜¶æ®µæµæ°´çº¿ï¼‰ï¼Œæœªæ¢ç´¢æ›´å¤æ‚çš„å¤šè®¾å¤‡æ‹“æ‰‘ï¼ˆå¦‚ chainã€treeï¼‰ã€‚
- å®éªŒé›†ä¸­åœ¨ CNN æ¨¡å‹ï¼Œæœªæ¶µç›– Transformer ç­‰æ–°å…´æ¶æ„ã€‚
- åŠ¨æ€é€‚åº”èƒ½åŠ›å°šæœªå®ç°ï¼šç›®å‰åˆ†å‰²ç‚¹ä¸ºé™æ€æšä¸¾ï¼Œç¼ºä¹ runtime è‡ªé€‚åº”è°ƒæ•´æœºåˆ¶ã€‚
- æœªè€ƒè™‘èƒ½è€—ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œè€Œè¿™å¯¹ç”µæ± é©±åŠ¨è®¾å¤‡è‡³å…³é‡è¦ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **å¼€å‘è‡ªé€‚åº”åˆ†å‰²ç®—æ³•**ï¼š
   - æ ¹æ®å®æ—¶ç½‘ç»œçŠ¶æ€åŠ¨æ€é€‰æ‹©æœ€ä¼˜åˆ†å‰²ç‚¹ã€‚
2. **åŠ å…¥èƒ½é‡æ•ˆç‡ç›®æ ‡**ï¼š
   - æ„å»ºä¸‰ç»´ Pareto Frontï¼ˆlatency, throughput, energyï¼‰ã€‚
3. **æ‰©å±•è‡³å¤šè®¾å¤‡æµæ°´çº¿ä¸æ··åˆå¹¶è¡Œç­–ç•¥**ï¼š
   - æ”¯æŒæ›´å¤æ‚çš„åˆ†å¸ƒå¼æ‹“æ‰‘ç»“æ„ã€‚
4. **æ”¯æŒ Transformer ç±»æ¨¡å‹çš„åˆ†å‰²åˆ†æ**ï¼š
   - æ¢ç´¢ attention å±‚çš„åˆ†å‰²ç‰¹æ€§ä¸é€šä¿¡æ¨¡å¼ã€‚
5. **é›†æˆè‡ªåŠ¨åŒ– profiling å·¥å…·**ï¼š
   - å®ç°ä¸€é”®å¼ block çº§æ€§èƒ½æµ‹é‡ä¸ Pareto Front ç”Ÿæˆã€‚

---

## æ€»ç»“
è¯¥è®ºæ–‡é€šè¿‡å¼•å…¥ **ParetoPipe** æ¡†æ¶ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†è¾¹ç¼˜ DNN åˆ†å‰²ä¸­çš„ **latency-throughput trade-off**ï¼Œå¼ºè°ƒäº†**ç½‘ç»œæ¡ä»¶**å’Œ**é€šä¿¡å¼€é”€**çš„å…³é”®ä½œç”¨ã€‚å…¶å®éªŒè®¾è®¡ä¸¥è°¨ï¼Œç»“æœå…·æœ‰å¼ºæŒ‡å¯¼æ„ä¹‰ï¼Œä¸ºè¾¹ç¼˜ AI çš„å®é™…éƒ¨ç½²æä¾›äº†é‡è¦çš„ç†è®ºåŸºå‡†ä¸å®ç”¨å·¥å…·ã€‚

</details>

---

### 12. [CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark](https://arxiv.org/abs/2601.08331)

**Authors**: Daniil Gurgurov, Yusser Al Ghussin, Tanja Baeumel, Cheng-Ting Chou, Patrick Schramowski, Marius Mosbach, Josef van Genabith, Simon Ostermann  
**Category**: cs.CL  
**Published**: 2026-01-14  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.08331v1  

#### Abstract
Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šCLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€åœºæ™¯ä¸‹çš„è¡Œä¸ºæ§åˆ¶ç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶æ¢ç´¢é€šè¿‡å¹²é¢„å†…éƒ¨è¡¨ç¤ºï¼ˆrepresentation-based steeringï¼‰æ¥æ§åˆ¶æ¨¡å‹è¾“å‡ºè¯­è¨€ï¼Œä½†å°šæ— ä¸“é—¨çš„åŸºå‡†ï¼ˆbenchmarkï¼‰ç”¨äºç³»ç»Ÿè¯„ä¼°ä¸åŒ **language steering** æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯åœ¨è·¨è¯­è¨€ï¼ˆcross-lingualï¼‰ç”Ÿæˆä»»åŠ¡ä¸­ã€‚

### æå‡ºçš„æ–°æ–¹æ³•/æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **CLaS-Bench**ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“æ³¨äºè¯„ä¼°å¤šè¯­è¨€å’Œè·¨è¯­è¨€ **language steering** çš„è½»é‡çº§åŸºå‡†æµ‹è¯•ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š
- **è·¨è¯­è¨€å¯¹é½è¯„ä¼°æ¡†æ¶**ï¼šè®¾è®¡äº†ä¸€ä¸ªåŒ…å«32ç§è¯­è¨€ã€70ä¸ªå¹³è¡Œå¼€æ”¾æ€§é—®é¢˜çš„æ•°æ®é›†ï¼Œæ”¯æŒä»ä»»æ„æºè¯­è¨€ï¼ˆsource languageï¼‰å‘ä»»æ„ç›®æ ‡è¯­è¨€ï¼ˆtarget languageï¼‰çš„ç”Ÿæˆä»»åŠ¡ï¼Œå…±äº§ç”Ÿè¶…è¿‡7ä¸‡å¯¹é—®ç­”æ ·æœ¬ã€‚
- **åŒç»´åº¦è¯„ä¼°æŒ‡æ ‡**ï¼šæå‡ºç»“åˆ **Language Forcing Success (LFS)** å’Œ **Output Relevance (OR)** çš„ **Language Steering Score (LSS)**ï¼Œé€šè¿‡è°ƒå’Œå¹³å‡æ•°ï¼ˆharmonic meanï¼‰ç»¼åˆè¡¡é‡è¯­è¨€åˆ‡æ¢èƒ½åŠ›å’Œè¯­ä¹‰ç›¸å…³æ€§ï¼Œé¿å…å•ä¸€æŒ‡æ ‡çš„ç‰‡é¢æ€§ã€‚
- **ç³»ç»ŸåŒ–æ¯”è¾ƒå¤šç§steeringæŠ€æœ¯**ï¼šé¦–æ¬¡åœ¨åŒä¸€æ¡†æ¶ä¸‹å…¨é¢è¯„ä¼°äº†åŒ…æ‹¬åŸºäºæç¤ºï¼ˆpromptingï¼‰ã€ç¥ç»å…ƒå¹²é¢„ï¼ˆneuron-basedï¼‰ã€æ®‹å·®æµå‘é‡ï¼ˆDiffMeanï¼‰ã€æ¢é’ˆï¼ˆprobeï¼‰ã€ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã€çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰å’Œç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencoders, SAEï¼‰ç­‰å¤šç§steeringæ–¹æ³•ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **å¡«è¡¥ç©ºç™½**ï¼šç°æœ‰åŸºå‡†å¦‚ AxBench å’Œ MIB ä¸»è¦å…³æ³¨è‹±è¯­ç¯å¢ƒä¸‹çš„æ¦‚å¿µæ§åˆ¶æˆ–ç”µè·¯å®šä½ï¼Œè€Œ CLaS-Bench é¦–æ¬¡å°†è¯„ä¼°é‡å¿ƒæ”¾åœ¨å¤šè¯­è¨€å’Œè·¨è¯­è¨€åœºæ™¯ã€‚
- **è½»é‡ä¸”å¯æ‰©å±•**ï¼šåŸºå‡†è®¾è®¡ç®€æ´ï¼Œæ˜“äºé€šè¿‡ç¿»è¯‘æ–°å¢è¯­è¨€è¿›è¡Œæ‰©å±•ã€‚
- **ç§‘å­¦ä¸¥è°¨**ï¼šå¼ºè°ƒrepresentation-based steeringçš„å¯è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºå¤šè¯­è¨€interpretabilityç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–å·¥å…·ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **æ¥æº**ï¼šä» Vicuna æ•°æ®é›†ä¸­é€‰å–70ä¸ªé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å¼€æ”¾å¼é—®é¢˜ã€‚
- **è¯­è¨€è¦†ç›–**ï¼šæ¶µç›–32ç§ç±»å‹å­¦å’Œåœ°ç†ä¸Šå¤šæ ·åŒ–çš„è¯­è¨€ï¼ˆå¦‚è¡¨4æ‰€ç¤ºï¼‰ï¼ŒåŒ…æ‹¬é«˜èµ„æºï¼ˆå¦‚ en, zh, deï¼‰å’Œä½èµ„æºï¼ˆå¦‚ bo, mt, kaï¼‰è¯­è¨€ï¼Œä»¥åŠéæ‹‰ä¸å­—æ¯è¯­è¨€ï¼ˆå¦‚ ar, ja, ru, koï¼‰ã€‚
- **æ•°æ®æ„å»º**ï¼šä½¿ç”¨ Google Translate API è¿›è¡Œåˆå§‹ç¿»è¯‘ï¼Œå¹¶ç”±æ¯è¯­è€…è¿›è¡Œæ ¡å¯¹ä»¥ç¡®ä¿è¯­ä¹‰ä¿çœŸåº¦ã€æµç•…æ€§å’Œåœ°é“æ€§ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **ä»»åŠ¡å®šä¹‰**ï¼šç»™å®šæºè¯­è¨€ `s` ä¸­çš„é—®é¢˜ `x_s`ï¼Œè¦æ±‚æ¨¡å‹ç”Ÿæˆç›®æ ‡è¯­è¨€ `t` çš„ç­”æ¡ˆ `y`ã€‚
- **è¯„ä¼°æµç¨‹**ï¼šæ¯ä¸ªé—®é¢˜åœ¨æ¯ç§ç›®æ ‡è¯­è¨€ä¸‹å‡è¢«è¯„ä¼°ï¼Œå…± `70 Ã— 32 = 2,240` ä¸ªå®ä¾‹/ç›®æ ‡è¯­è¨€ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **LFS (Language Forcing Success)**ï¼šä½¿ç”¨ FastText LID åˆ†ç±»å™¨æ£€æµ‹ç”Ÿæˆæ–‡æœ¬çš„è¯­è¨€ï¼Œè®¡ç®—æˆåŠŸåˆ‡æ¢è‡³ç›®æ ‡è¯­è¨€çš„æ¯”ä¾‹ã€‚
  - **OR (Output Relevance)**ï¼šä½¿ç”¨ Qwen-3-8B ä½œä¸º judge model å¯¹ç”Ÿæˆå†…å®¹çš„ç›¸å…³æ€§æ‰“åˆ†ï¼ˆ0-2åˆ†ï¼‰ï¼Œå¹¶å½’ä¸€åŒ–ä¸º `[0,1]` åŒºé—´ã€‚
  - **LSS (Language Steering Score)**ï¼š`LSS = 2 * (LFS * OR) / (LFS + OR)`ï¼Œå³ LFS å’Œ OR çš„è°ƒå’Œå¹³å‡æ•°ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
å®éªŒå¯¹æ¯”äº†ä»¥ä¸‹8ç§æ–¹æ³•ï¼š
1. **EBase-I**ï¼šè‹±æ–‡æŒ‡ä»¤æç¤ºï¼ˆå¦‚ "Respond in German"ï¼‰
2. **TBase-II**ï¼šç›®æ ‡è¯­è¨€æŒ‡ä»¤æç¤ºï¼ˆå¦‚ "Antworte auf Deutsch"ï¼‰
3. **LAPE**ï¼šåŸºäºè¯­è¨€æ•æ„Ÿç¥ç»å…ƒçš„å¹²é¢„ï¼ˆTang et al., 2024ï¼‰
4. **DiffMean**ï¼šæ®‹å·®æµæ¿€æ´»çš„å‡å€¼å·®å¼‚å‘é‡å¹²é¢„ï¼ˆMarks and Tegmark, 2023ï¼‰
5. **Probe**ï¼šè®­ç»ƒçº¿æ€§æ¢é’ˆè·å–è¯­è¨€åˆ¤åˆ«æ–¹å‘
6. **PCA**ï¼šåœ¨æ®‹å·®æµä¸Šè¿›è¡Œä¸»æˆåˆ†åˆ†æ
7. **LDA**ï¼šåœ¨çº¿æ€§åˆ¤åˆ«åˆ†æè·å–æœ€ä¼˜åˆ†ç¦»æ–¹å‘
8. **SAE-DM**ï¼šåœ¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰éšç©ºé—´ä¸­åº”ç”¨ DiffMean

æ¨¡å‹é€‰æ‹©ï¼šLlama-3.1-8B-Instruct å’Œ Aya-Expanse-8Bã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆä»¥ Llama-3.1-8B-Instruct ä¸ºä¾‹ï¼‰
| æ–¹æ³• | å¹³å‡ LSS |
|------|---------|
| **DiffMean (â–²)** | **84.5** |
| **LAPE (âŠ™)** | **80.1** |
| EBase-I (Prompting) | 67.7 |
| TBase-II (Prompting) | 67.3 |
| Probe (w) | 48.6 |
| SAE-DM (â–³) | 42.3 |
| LDA (v) | 23.6 |
| PCA (u) | 15.1 |

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **DiffMean è¡¨ç°æœ€ä½³**ï¼šåœ¨32ç§è¯­è¨€ä¸­ï¼ŒDiffMean åœ¨19ç§è¯­è¨€ä¸Šçš„ LSS è¶…è¿‡90%ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰å…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬ prompting åŸºçº¿ã€‚
- **prompting å­˜åœ¨æ˜æ˜¾å¤±è´¥æ¡ˆä¾‹**ï¼šä¾‹å¦‚ï¼Œåœ¨ä»ä¿„è¯­æç¤ºè½¬å‘è‹±è¯­æ—¶ï¼Œprompting æ–¹æ³•ä»è¾“å‡ºä¿„è¯­ï¼Œè€Œ DiffMean æˆåŠŸç”Ÿæˆè‹±è¯­ï¼ˆè§ Table 3ï¼‰ã€‚
- **LAPE æ˜¯ç¬¬äºŒä¼˜æ–¹æ³•**ï¼šè¡¨ç°ç¨³å¥ï¼Œä½†å­˜åœ¨æ›´å¤§è¯­è¨€é—´æ–¹å·®ã€‚
- **ç›‘ç£æ–¹æ³•è¡¨ç°ä¸ä½³**ï¼šProbe å’Œ LDA ç­‰ç›‘ç£æ–¹æ³•è¡¨ç°è¿œä½äºæ— ç›‘ç£çš„ DiffMean å’Œ LAPEï¼Œè¡¨æ˜å…¶å¯èƒ½è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ä¸­çš„ç‰¹å®šç‰¹å¾ã€‚
- **SAE-based æ–¹æ³•ä¸­ç­‰**ï¼šSAE-DM è¡¨ç°ä¼˜äºéƒ¨åˆ†æ®‹å·®æµæ–¹æ³•ï¼Œä½†ä¸åŠåŸå§‹ DiffMeanï¼Œå¯èƒ½å—é™äºé‡å»ºè¯¯å·®æˆ–SAEè®­ç»ƒæ•°æ®è¦†ç›–ä¸è¶³ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **å¹²é¢„å±‚ä¸å¼ºåº¦äº¤äº’ä½œç”¨**ï¼š
  - æ—©æœŸå±‚ï¼šä½å¼ºåº¦ï¼ˆÎ±=1.0ï¼‰å³å¯æœ‰æ•ˆsteeringã€‚
  - åæœŸå±‚ï¼ˆ~20å±‚åï¼‰ï¼šéœ€è¦æ›´é«˜å¼ºåº¦ï¼ˆÎ±=5.0ï¼‰æ‰èƒ½è¾¾åˆ°å³°å€¼æ•ˆæœã€‚
  - æ—©æœŸå¼ºå¹²é¢„ä¼šä¸¥é‡æŸå®³è¾“å‡ºè¿è´¯æ€§ï¼Œè€ŒåæœŸå¹²é¢„å³ä½¿å¼ºåº¦é«˜ä¹Ÿèƒ½ä¿æŒè¾“å‡ºè´¨é‡ã€‚
- **LAPE å‚æ•°ä¼˜åŒ–**ï¼šåŒæ—¶æ¿€æ´»ç›®æ ‡ç¥ç»å…ƒå¹¶å»æ¿€æ´»æºè¯­è¨€ç¥ç»å…ƒï¼ˆcombined activation + deactivationï¼‰ä¼˜äºä»…æ¿€æ´»ç­–ç•¥ï¼›å¹²é¢„æ›´å¤šç¥ç»å…ƒï¼ˆ5% vs 1%ï¼‰æ€§èƒ½ç•¥æœ‰æå‡ã€‚
- **SAE å¹²é¢„æ¨¡å¼**ï¼šæœ€ä½³æ€§èƒ½å‡ºç°åœ¨ç¬¬25å±‚ï¼Œå¼ºåº¦ Î±=15.0ï¼Œè¡¨æ˜é«˜å±‚ç¨€ç–ç©ºé—´å…·æœ‰æ›´å¼ºçš„è¯­è¨€æ§åˆ¶èƒ½åŠ›ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ç®€å•æ–¹æ³•æ›´æœ‰æ•ˆ**ï¼šåŸºäºæ®‹å·®æµçš„ **DiffMean** æ–¹æ³•åœ¨è·¨è¯­è¨€steeringä»»åŠ¡ä¸­æŒç»­ä¼˜äºæ‰€æœ‰å…¶ä»–å¤æ‚æ–¹æ³•ï¼ŒåŒ…æ‹¬å…ˆè¿›çš„ prompting å’Œç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚
2. **è¯­è¨€ç‰¹å¼‚æ€§ä¿¡æ¯é›†ä¸­åœ¨åæœŸå±‚**ï¼šé€šè¿‡å¤šç§åˆ†ææ‰‹æ®µï¼ˆcosine similarity, probe accuracy, LAPE ç¥ç»å…ƒåˆ†å¸ƒ, LDA å¯åˆ†æ€§ï¼‰ä¸€è‡´å‘ç°ï¼Œè¯­è¨€ç‰¹å¼‚æ€§è¡¨ç¤ºä¸»è¦åœ¨æ¨¡å‹çš„ **16-32 å±‚** å‡ºç°ã€‚
3. **è¯­è¨€å®¶æ—å‘ˆç°å‡ ä½•èšç±»**ï¼šåœ¨è¡¨ç¤ºç©ºé—´ä¸­ï¼ŒåŒè¯­ç³»çš„è¯­è¨€ï¼ˆå¦‚ç½—æ›¼è¯­æ—ã€æ—¥è€³æ›¼è¯­æ—ã€æ–¯æ‹‰å¤«è¯­æ—ï¼‰åœ¨å‘é‡ç©ºé—´ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ä½™å¼¦ç›¸ä¼¼æ€§ï¼Œå½¢æˆæ˜æ˜¾çš„å‡ ä½•èšç±»ã€‚
4. **æ— ç›‘ç£ä¼˜äºç›‘ç£**ï¼šç®€å•çš„æ— ç›‘ç£æ–¹æ³•ï¼ˆDiffMean, LAPEï¼‰ä¼˜äºå¤æ‚çš„ç›‘ç£æ–¹æ³•ï¼ˆProbe, LDAï¼‰ï¼Œè¯´æ˜è¯­è¨€æ–¹å‘å¯é€šè¿‡ç®€å•çš„å‡å€¼å·®å¼‚æœ‰æ•ˆæ•æ‰ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æ•°æ®é‡ä¸å‡è¡¡**ï¼šä¸åŒæ–¹æ³•ä½¿ç”¨çš„tokenæ•°é‡ä¸åŒï¼ˆDiffMean/LAPEç”¨10Mï¼ŒPCA/LDAç”¨500K/100Kï¼‰ï¼Œå¯èƒ½å½±å“å…¬å¹³æ¯”è¾ƒã€‚
- **SAEè¦†ç›–æœ‰é™**ï¼šSAE-based steering ä»…é™äºæœ‰å…¬å¼€é¢„è®­ç»ƒSAEçš„å±‚ï¼Œæ— æ³•è¿›è¡Œå…¨é¢å±‚åˆ†æã€‚
- **è¯­è¨€è¦†ç›–ä»æœ‰é™**ï¼šå°½ç®¡åŒ…å«32ç§è¯­è¨€ï¼Œä½†å…¨çƒä»æœ‰å¤§é‡è¯­è¨€æœªè¢«è¦†ç›–ï¼Œç‰¹åˆ«æ˜¯æ•°å­—èµ„æºæå°‘çš„è¯­è¨€ã€‚
- **æ¨¡å‹é™åˆ¶**ï¼šä»…è¯„ä¼°äº†instruction-tunedæ¨¡å‹ï¼Œbase modelå¯èƒ½è¡¨ç°ä¸åŒã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³æ›´å¤šè¯­è¨€ï¼Œå°¤å…¶æ˜¯ä½èµ„æºå’Œæ¿’å±è¯­è¨€ã€‚
- æ¢ç´¢æ›´é«˜æ•ˆçš„SAEè®­ç»ƒå’Œéƒ¨ç½²æ–¹æ¡ˆï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„æ¨¡å‹å’Œè¯­è¨€ã€‚
- å°†CLaS-Benchåº”ç”¨äºå…¶ä»–å¤šè¯­è¨€å¯æ§ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚é£æ ¼è¿ç§»ã€æƒ…æ„Ÿæ§åˆ¶ç­‰ã€‚
- ç»“åˆCLaS-Benchçš„å‘ç°ï¼Œå¼€å‘æ›´é«˜æ•ˆã€é²æ£’çš„å¤šè¯­è¨€é€‚é…æŠ€æœ¯ï¼Œä½œä¸ºå¾®è°ƒçš„ä½æˆæœ¬æ›¿ä»£æ–¹æ¡ˆã€‚

> **ä»£ç ä¸æ•°æ®å·²å¼€æº**ï¼šhttps://github.com/d-gurgurov/CLaS-Bench

</details>

---

### 13. [InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation](https://arxiv.org/abs/2601.08033)

**Authors**: Amir Eskandari, Aman Anand, Elyas Rashno, Farhana Zulkernine  
**Category**: cs.LG  
**Published**: 2026-01-14  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.08033v1  

#### Abstract
Graph Neural Networks (GNNs) are the go-to model for graph data analysis. However, GNNs rely on two key operations - aggregation and update, which can pose challenges for low-latency inference tasks or resource-constrained scenarios. Simple Multi-Layer Perceptrons (MLPs) offer a computationally effi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šInfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
Graph Neural Networks (GNNs) åœ¨å›¾æ•°æ®åˆ†æä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä¾èµ–äº**æ¶ˆæ¯ä¼ é€’æœºåˆ¶**ï¼ˆmessage passingï¼‰ï¼Œåœ¨æ¨ç†é˜¶æ®µå­˜åœ¨é«˜å»¶è¿Ÿå’Œé«˜è®¡ç®—å¼€é”€ï¼Œéš¾ä»¥éƒ¨ç½²äºèµ„æºå—é™æˆ–ä½å»¶è¿Ÿåœºæ™¯ã€‚è™½ç„¶ Multi-Layer Perceptrons (MLPs) æ›´é«˜æ•ˆï¼Œä½†åœ¨å¿½ç•¥å›¾ç»“æ„çš„æƒ…å†µä¸‹æ€§èƒ½é€šå¸¸è¾ƒå·®ã€‚

ç°æœ‰çš„ GNN-to-MLP Knowledge Distillation (KD) æ–¹æ³•å¤§å¤šé‡‡ç”¨**å‡åŒ€çš„çŸ¥è¯†è½¬ç§»ç­–ç•¥**ï¼Œæˆ–è€…åŸºäº**å›¾æ— å…³çš„æŒ‡æ ‡**ï¼ˆå¦‚é¢„æµ‹ä¸ç¡®å®šæ€§ã€ç†µï¼‰æ¥é€‰æ‹©èŠ‚ç‚¹è¿›è¡Œè’¸é¦ï¼Œå¿½ç•¥äº†èŠ‚ç‚¹åœ¨å›¾æ‹“æ‰‘ä¸­çš„ç»“æ„æ€§é‡è¦æ€§ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
æœ¬æ–‡æå‡º **InfGraND**ï¼ˆInfluence-Guided Graph Knowledge Distillationï¼‰ï¼Œä¸€ç§**åŸºäºèŠ‚ç‚¹å½±å“åŠ›å¼•å¯¼çš„ GNN åˆ° MLP çš„çŸ¥è¯†è’¸é¦æ¡†æ¶**ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- **è¯†åˆ«å¹¶ä¼˜å…ˆè’¸é¦æ¥è‡ªâ€œç»“æ„ä¸Šæ›´å…·å½±å“åŠ›çš„èŠ‚ç‚¹â€çš„çŸ¥è¯†**ï¼Œè€Œéä»…ä¾æ®æ¨¡å‹ç½®ä¿¡åº¦ã€‚
- å¼•å…¥ä¸€ä¸ª**æ— å‚æ•°ã€æ‹“æ‰‘æ„ŸçŸ¥çš„èŠ‚ç‚¹å½±å“åŠ›è¯„åˆ†**ï¼ˆGlobal Influence Score, GISï¼‰ï¼Œè¡¡é‡ä¸€ä¸ªèŠ‚ç‚¹ç‰¹å¾æ‰°åŠ¨å¯¹å…¶ä»–èŠ‚ç‚¹è¡¨ç¤ºçš„å½±å“ç¨‹åº¦ã€‚
- é€šè¿‡**ä¸€æ¬¡æ€§çš„å¤šè·³é‚»å±…ç‰¹å¾é¢„è®¡ç®—**ï¼ˆfeature propagation + poolingï¼‰å°†ç»“æ„ä¿¡æ¯æ³¨å…¥ MLP è¾“å…¥ï¼Œé¿å…æ¨ç†æ—¶çš„æ¶ˆæ¯ä¼ é€’å¼€é”€ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´åˆç†çš„è’¸é¦ä¼˜å…ˆçº§**ï¼šé¦–æ¬¡ä»**å›¾ç»“æ„å½±å“**è§’åº¦æŒ‡å¯¼è’¸é¦ï¼Œè€Œéä»…ä¾èµ–é¢„æµ‹ä¸ç¡®å®šæ€§ï¼ˆå¦‚ KRDã€HGMDï¼‰ã€‚
- **æ— éœ€å¢åŠ æ¨¡å‹å¤æ‚åº¦**ï¼šä¸å¼•å…¥é¢å¤–å¯è®­ç»ƒæ¨¡å—ï¼ˆå¦‚ MoEã€ensembleï¼‰ï¼Œä¿æŒ MLP è½»é‡ã€‚
- **é›¶æ¨ç†å¼€é”€**ï¼šç»“æ„ä¿¡æ¯é€šè¿‡ç¦»çº¿é¢„è®¡ç®—åµŒå…¥è¾“å…¥ï¼Œä¸å½±å“åœ¨çº¿æ¨ç†é€Ÿåº¦ã€‚
- **æ€§èƒ½è¶…è¶Šæ•™å¸ˆ GNN**ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œè’¸é¦åçš„ MLP å­¦ç”Ÿæ¨¡å‹ç”šè‡³è¶…è¿‡äº†å…¶ GNN æ•™å¸ˆçš„æ€§èƒ½ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
åœ¨ **7 ä¸ªçœŸå®ä¸–ç•Œçš„åŒè´¨æ€§å›¾æ•°æ®é›†**ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–å°è§„æ¨¡ä¸å¤§è§„æ¨¡ï¼š
- å°è§„æ¨¡å¼•ç”¨ç½‘ç»œï¼š**Cora**, **Citeseer**, **PubMed**
- å¤§è§„æ¨¡åä½œ/å•†å“å›¾ï¼š**Amazon-Photo**, **Coauthor-CS**, **Coauthor-Phy**
- å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼š**OGBN-Arxiv**

### å®éªŒè®¾ç½®
- **ä¸¤ç§å­¦ä¹ èŒƒå¼**ï¼š
  - **Transductive**ï¼šè®­ç»ƒä¸æµ‹è¯•åœ¨åŒä¸€å›¾ä¸Šã€‚
  - **Inductive**ï¼šè®­ç»ƒåœ¨ä¸€ä¸ªå­å›¾ä¸Šï¼Œæµ‹è¯•åœ¨æœªè§èŠ‚ç‚¹ä¸Šã€‚
- **æ•™å¸ˆæ¨¡å‹**ï¼šä½¿ç”¨ä¸‰ç§ä¸»æµ GNN æ¶æ„ä½œä¸ºæ•™å¸ˆï¼š
  - **GCN**, **GAT**, **GraphSAGE**
- **å­¦ç”Ÿæ¨¡å‹**ï¼šæ ‡å‡†ä¸¤å±‚ MLPã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š**åˆ†ç±»å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰** å’Œ **æ¨ç†å»¶è¿Ÿï¼ˆInference Latencyï¼‰**ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
ä¸ä»¥ä¸‹ SOTA GNN-to-MLP è’¸é¦æ–¹æ³•å¯¹æ¯”ï¼š
- **GLNN**ï¼šåŸºç¡€è’¸é¦æ¡†æ¶ï¼Œç›´æ¥ç”¨è½¯æ ‡ç­¾è®­ç»ƒ MLPã€‚
- **KRD**ï¼šåŸºäºé¢„æµ‹ç†µç¨³å®šæ€§é€‰æ‹©â€œå¯é â€èŠ‚ç‚¹è¿›è¡Œè’¸é¦ã€‚
- **HGMD**ï¼šåŸºäºâ€œçŸ¥è¯†éš¾åº¦â€é‡‡æ ·å›°éš¾æ ·æœ¬è¿›è¡Œè’¸é¦ã€‚
- **FF-G2M**ï¼šæå–ä½é¢‘/é«˜é¢‘çŸ¥è¯†è¿›è¡Œè’¸é¦ã€‚
- **AdaGMLP** / **RbM**ï¼šå¢å¼ºå‹ MLP å­¦ç”Ÿï¼ˆé›†æˆ/MoEï¼‰ï¼Œç”¨äºæ•ˆç‡å¯¹æ¯”ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰
| åœºæ™¯ | æŒ‡æ ‡ | InfGraND è¡¨ç° |
|------|------|---------------|
| **Transductive** | å¹³å‡å‡†ç¡®ç‡æå‡ï¼ˆvs. Vanilla MLPï¼‰ | **+12.6%** |
| **Inductive** | å¹³å‡å‡†ç¡®ç‡æå‡ï¼ˆvs. Vanilla MLPï¼‰ | **+9.3%** |
| **Transductive** | å¹³å‡ä¼˜äº KRD | **+0.9%** |
| **Inductive** | å¹³å‡ä¼˜äº KRD | **+3.0%** |
| **Transductive** | å¹³å‡ä¼˜äº HGMD | **+0.6%** |
| **OGBN-Arxiv** | å‡†ç¡®ç‡ï¼ˆSAGE æ•™å¸ˆï¼‰ | **è¶…è¿‡ KRD**ï¼ˆè§ Figure 3ï¼‰ |

> âœ… **å…³é”®å‘ç°**ï¼šåœ¨å¤šæ•°æƒ…å†µä¸‹ï¼ŒInfGraND ä¸ä»…æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œç”šè‡³**è¶…è¿‡äº†å…¶ GNN æ•™å¸ˆæ¨¡å‹æœ¬èº«**ï¼ˆä¾‹å¦‚åœ¨ Amazon-Photo ä¸Šï¼ŒGAT æ•™å¸ˆä¸º 87.6%ï¼ŒInfGraND å­¦ç”Ÿè¾¾ 94.5%ï¼‰ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- InfGraND åœ¨ **7 ä¸ªæ•°æ®é›†ã€3 ç§æ•™å¸ˆæ¶æ„ã€2 ç§è®¾å®šä¸‹å‡å–å¾—æœ€ä½³æˆ–æ¥è¿‘æœ€ä½³æ€§èƒ½**ã€‚
- ç›¸æ¯”åŸºäºç†µçš„æ–¹æ³•ï¼ˆKRD/HGMDï¼‰ï¼ŒInfGraND åˆ©ç”¨**ç»“æ„å½±å“åŠ›**å®ç°äº†æ›´æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚
- åœ¨ **OGBN-Arxiv** è¿™ç±»å¤§è§„æ¨¡å›¾ä¸Šä»è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å¯æ‰©å±•æ€§ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Study, Table 2ï¼‰

| å˜ä½“ | æ€§èƒ½è¶‹åŠ¿ | ç»“è®º |
|------|--------|------|
| **w/ Influence**ï¼ˆä»…å½±å“åŠ›åŠ æƒï¼‰ | æ˜¾è‘—ä¼˜äº Vanilla MLP å’Œ GLNN | å½±å“åŠ›å¼•å¯¼æœ‰æ•ˆæå‡ç›‘ç£ä¸è’¸é¦ä¿¡å·è´¨é‡ |
| **w/ Propagation**ï¼ˆä»…ç‰¹å¾ä¼ æ’­ï¼‰ | åœ¨ inductive è®¾ç½®ä¸‹å¤§å¹…æå‡ï¼ˆå¦‚ Amazon-Photo +21.1%ï¼‰ | é¢„è®¡ç®—å¤šè·³ç‰¹å¾æ˜¾è‘—å¢å¼º MLP çš„æ³›åŒ–èƒ½åŠ› |
| **Full Model**ï¼ˆä¸¤è€…ç»“åˆï¼‰ | **æ€§èƒ½æœ€é«˜** | ä¸¤ä¸ªç»„ä»¶äº’è¡¥ï¼Œå…±åŒè´¡çŒ®æœ€ç»ˆä¼˜åŠ¿ |

> ğŸ” ç‰¹åˆ«åœ°ï¼Œåœ¨ **label-scarce setting**ï¼ˆæ¯ç±»ä»… 2â€“8 ä¸ªæ ‡ç­¾ï¼‰ä¸‹ï¼ŒInfGraND å¹³å‡æ¯” GLNN **é«˜å‡º 4.17%**ï¼Œè¡¨æ˜å…¶åœ¨ä½æ ‡æ³¨æˆæœ¬åœºæ™¯ä¸‹æ›´å…·é²æ£’æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **èŠ‚ç‚¹å½±å“åŠ›æ˜¯æ¯”é¢„æµ‹ä¸ç¡®å®šæ€§æ›´æœ¬è´¨çš„è’¸é¦ä¼˜å…ˆçº§æŒ‡æ ‡**ï¼š
   - å®éªŒ Q1 æ˜¾ç¤ºï¼Œåœ¨é«˜å½±å“åŠ›èŠ‚ç‚¹ä¸Šè®­ç»ƒ GNN å¯è·å¾—æ›´å¥½æ³›åŒ–æ€§èƒ½ï¼ˆFigure 2ï¼‰ã€‚
   - å› æ­¤ï¼Œåœ¨è’¸é¦ä¸­ä¼˜å…ˆå­¦ä¹ è¿™äº›èŠ‚ç‚¹çš„çŸ¥è¯†æ˜¯åˆç†ä¸”é«˜æ•ˆçš„ã€‚

2. **InfGraND å®ç°äº†â€œè½»é‡ä½†å¼ºå¤§â€çš„ MLP å­¦ç”Ÿæ¨¡å‹**ï¼š
   - é€šè¿‡**å½±å“åŠ›åŠ æƒè’¸é¦æŸå¤±** + **é¢„è®¡ç®—ç»“æ„ç‰¹å¾**ï¼Œä½¿ MLP èƒ½å­¦åˆ°æ¥è¿‘ç”šè‡³è¶…è¶Š GNN çš„è¡¨ç¤ºèƒ½åŠ›ã€‚

3. **æ•ˆç‡ä¸ç²¾åº¦å…¼å¾—**ï¼š
   - å¦‚ Figure 4 æ‰€ç¤ºï¼ŒInfGraND æ¯” GNN å¿« **6.8â€“13.9 å€**ï¼ŒåŒæ—¶ç²¾åº¦æ›´é«˜ã€‚
   - æ¨ç†æ—¶é—´ä¸æ™®é€š MLP ç›¸å½“ï¼Œæ— é¢å¤–å¼€é”€ã€‚

4. **å½±å“åŠ›æŒ‡æ ‡ä¼˜äºä¼ ç»Ÿä¸­å¿ƒæ€§åº¦é‡**ï¼š
   - æ¶ˆèå®éªŒï¼ˆAppendix Eï¼‰æ˜¾ç¤ºï¼Œä½¿ç”¨ Degree æˆ– PageRank ä½œä¸ºæƒé‡ä¸å¦‚æœ¬æ–‡æå‡ºçš„ GISï¼Œå› å…¶**èåˆäº†ç‰¹å¾ä¸æ‹“æ‰‘ä¿¡æ¯**ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ–¹æ³•å‡è®¾å›¾æ˜¯**åŒè´¨æ€§**ï¼ˆhomophilicï¼‰çš„ï¼Œæœªåœ¨å¼‚é…å›¾ï¼ˆheterophilic graphsï¼‰ä¸ŠéªŒè¯ã€‚
- å½±å“åŠ›è®¡ç®—è™½ä¸ºä¸€æ¬¡æ€§é¢„å¤„ç†ï¼Œä½†åœ¨è¶…å¤§è§„æ¨¡å›¾ä¸Šä»éœ€è¿‘ä¼¼ï¼ˆå¦‚ 2-hop æˆªæ–­ï¼‰ã€‚
- ä¾èµ–æ•™å¸ˆ GNN çš„è¾“å‡ºè´¨é‡ï¼Œè‹¥æ•™å¸ˆæœ¬èº«æ¬ æ‹Ÿåˆï¼Œåˆ™è’¸é¦æ•ˆæœå—é™ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•åˆ° **heterophilic graphs** å’Œ **dynamic graphs**ã€‚
- æ¢ç´¢ **hybrid discrimination strategies**ï¼šç»“åˆ entropy-based ä¸ influence-based æ–¹æ³•ã€‚
- åº”ç”¨äºå…¶ä»–ä»»åŠ¡ï¼Œå¦‚é“¾æ¥é¢„æµ‹ã€å›¾åˆ†ç±»ç­‰ã€‚
- è¿›ä¸€æ­¥ä¼˜åŒ–å½±å“åŠ›è®¡ç®—çš„å¯æ‰©å±•æ€§ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **InfGraND é€šè¿‡â€œå½±å“åŠ›æ„ŸçŸ¥â€çš„çŸ¥è¯†è’¸é¦ + â€œé¢„è®¡ç®—ç»“æ„ç¼–ç â€ï¼Œè®©ç®€å•çš„ MLP åœ¨å›¾ä»»åŠ¡ä¸Šæ—¢å¿«åˆå‡†ï¼Œç”šè‡³åè¶…å¤æ‚çš„ GNNï¼Œä¸ºå·¥ä¸šçº§å›¾æ¨¡å‹éƒ¨ç½²æä¾›äº†é«˜æ•ˆæ–°èŒƒå¼ã€‚**

</details>

---

### 14. [VBO-MI: A Fully Gradient-Based Bayesian Optimization Framework Using Variational Mutual Information Estimation](https://arxiv.org/abs/2601.08172)

**Authors**: Farhad Mirkarimi  
**Category**: cs.LG  
**Published**: 2026-01-14  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.08172v1  

#### Abstract
Many real-world tasks require optimizing expensive black-box functions accessible only through noisy evaluations, a setting commonly addressed with Bayesian optimization (BO). While Bayesian neural networks (BNNs) have recently emerged as scalable alternatives to Gaussian Processes (GPs), traditiona...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šVBO-MI: A Fully Gradient-Based Bayesian Optimization Framework Using Variational Mutual Information Estimation

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿ **Bayesian Optimization (BO)** ä¸»è¦ä¾èµ– **Gaussian Processes (GPs)** æˆ– **Bayesian Neural Networks (BNNs)** ä½œä¸ºä»£ç†æ¨¡å‹ï¼ˆsurrogate modelï¼‰ï¼Œä½†å­˜åœ¨ä»¥ä¸‹ç“¶é¢ˆï¼š
- **GPs**ï¼šè®¡ç®—å¤æ‚åº¦ä¸º $O(T^3)$ï¼Œéš¾ä»¥æ‰©å±•åˆ°é«˜ç»´æˆ–å¤§è§„æ¨¡æ•°æ®ï¼›
- **BNNs**ï¼šè™½å…·å¯æ‰©å±•æ€§ï¼Œä½†åéªŒé‡‡æ ·ï¼ˆå¦‚ HMCï¼‰æˆæœ¬é«˜æ˜‚ï¼Œä¸”é€šå¸¸éœ€å¯¹åéªŒåˆ†å¸ƒå½¢å¼åšå¼ºå‡è®¾ï¼ˆå¦‚ Gaussianã€Gamma åˆ†å¸ƒï¼‰ï¼Œé™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›ï¼›
- æ‰€æœ‰ä¼ ç»Ÿæ–¹æ³•å‡éœ€åœ¨æ¯ä¸€æ­¥è¿›è¡Œ **acquisition function çš„å†…å±‚ä¼˜åŒ–**ï¼ˆinner-loop optimizationï¼‰ï¼Œè¿›ä¸€æ­¥å¢åŠ è®¡ç®—å¼€é”€ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
ä½œè€…æå‡º **VBO-MI**ï¼ˆVariational Bayesian Optimization with Mutual Informationï¼‰ï¼Œä¸€ç§**å®Œå…¨åŸºäºæ¢¯åº¦çš„è´å¶æ–¯ä¼˜åŒ–æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³åŒ…æ‹¬ï¼š
- **æ— éœ€æ˜¾å¼å»ºæ¨¡åéªŒåˆ†å¸ƒ**ï¼šä¸å‡è®¾ä»£ç†æ¨¡å‹çš„å…·ä½“å‚æ•°åŒ–å½¢å¼ï¼ˆå¦‚ GP æˆ–ç‰¹å®šå˜åˆ†æ—ï¼‰ï¼Œæå‡çµæ´»æ€§ï¼›
- **åˆ©ç”¨å˜åˆ†äº’ä¿¡æ¯ä¼°è®¡ï¼ˆVariational MI Estimationï¼‰** æ„é€ æ–°çš„ acquisition å‡½æ•°ï¼Œå°†ä¿¡æ¯å¢ç›Šï¼ˆinformation gainï¼‰ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ ï¼›
- å¼•å…¥ **Actor-Critic æ¶æ„**ï¼š
  - **Action-netï¼ˆActorï¼‰**ï¼šç”Ÿæˆå€™é€‰è¾“å…¥ç‚¹ï¼›
  - **Variational Criticï¼ˆCriticï¼‰**ï¼šä¼°è®¡å½“å‰ç‚¹å¸¦æ¥çš„äº’ä¿¡æ¯ï¼ˆå³æ¢ç´¢æ”¶ç›Šï¼‰ï¼›
- æ•´ä¸ªæµç¨‹é€šè¿‡ç«¯åˆ°ç«¯çš„æ¢¯åº¦æ›´æ–°å®ç°ï¼Œ**æ¶ˆé™¤ä¼ ç»Ÿ BO ä¸­ acquisition function çš„æ˜¾å¼ä¼˜åŒ–æ­¥éª¤**ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **è®¡ç®—æ•ˆç‡æ˜¾è‘—æå‡**ï¼šç›¸æ¯” BNN-BO åŸºçº¿ï¼ŒFLOPs æœ€å¤šå‡å°‘ **102Ã—**ï¼›
- **ç«¯åˆ°ç«¯å¯å¾®è®­ç»ƒ**ï¼šé¿å…æ˜‚è´µçš„é‡‡æ ·å’Œå†…å±‚ä¼˜åŒ–ï¼›
- **æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›**ï¼šæ‘†è„± GP çš„æ ¸å‡½æ•°å‡è®¾å’Œ BNN çš„åéªŒåˆ†å¸ƒå‡è®¾ï¼›
- **è‡ªç„¶å¤„ç†å™ªå£°è§‚æµ‹**ï¼Œå¹¶å¯¹è¶…å‚æ•° $\beta$ æ›´é²æ£’ï¼›
- æ”¯æŒé«˜ç»´ã€éçº¿æ€§ã€å«ç±»åˆ«å˜é‡çš„å¤æ‚ä»»åŠ¡ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†ä¸ä»»åŠ¡
å®éªŒæ¶µç›–ä¸¤ç±»åœºæ™¯ï¼š

#### ï¼ˆ1ï¼‰åˆæˆå‡½æ•°åŸºå‡†æµ‹è¯•
- **Branin**ï¼ˆ2Dï¼‰
- **Hartmann-6**ï¼ˆ6Dï¼‰
- **Ackley-10**ï¼ˆ10Dï¼‰
> è¿™äº›å‡½æ•°å…·æœ‰å¤šä¸ªå±€éƒ¨æœ€ä¼˜ï¼Œæ˜¯æ ‡å‡†çš„å…¨å±€ä¼˜åŒ–æŒ‘æˆ˜ä»»åŠ¡ã€‚

#### ï¼ˆ2ï¼‰çœŸå®ä¸–ç•Œå¤æ‚ä»»åŠ¡
- **PDE Optimization**ï¼šè°ƒèŠ‚åå¾®åˆ†æ–¹ç¨‹ç³»æ•°ä»¥æœ€å°åŒ–è§£è¾“å‡ºæ–¹å·®ï¼›
- **Interferometer Position Optimization**ï¼ˆ4Dï¼‰ï¼šå…‰å­¦å¹²æ¶‰ä»ªé…ç½®ä¼˜åŒ–ï¼›
- **Lunar Lander æ§åˆ¶é—®é¢˜**ï¼ˆ12Dï¼‰ï¼šOpenAI Gym ç¯å¢ƒä¸­çš„ç­–ç•¥ä¼˜åŒ–ï¼›
- **Pest Control**ï¼ˆ25Dï¼Œå«ç±»åˆ«å˜é‡ï¼‰ï¼šå®³è™«ç§ç¾¤æ§åˆ¶ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿ç”¨ one-hot ç¼–ç å¤„ç†ç±»åˆ«è¾“å…¥ã€‚

### å®éªŒè®¾ç½®
- **æ‰¹å¤§å°ï¼ˆbatch sizeï¼‰**ï¼šAckley/Hartmann ä½¿ç”¨ 64ï¼ŒBranin ä½¿ç”¨ 32ï¼›
- **å­¦ä¹ ç‡**ï¼šç»Ÿä¸€è®¾ä¸º 0.002ï¼›
- **ç½‘ç»œæ¶æ„**ï¼š
  - **Action-net**ï¼šå…¨è¿æ¥å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼›
  - **Helper Networkï¼ˆCriticï¼‰**ï¼šä¿®æ”¹ç‰ˆ LSTM + å…¨è¿æ¥å±‚ï¼›
- **è®­ç»ƒç­–ç•¥**ï¼šäº¤æ›¿æ›´æ–° Criticï¼ˆ$K_a=1$ï¼‰å’Œ Actorï¼ˆ$K_b=5$ï¼‰ï¼›
- **warm-up æ­¥æ•°**ï¼š20â€“30 æ­¥ï¼›
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå½’ä¸€åŒ–çš„å¹³å‡ç´¯è®¡å¥–åŠ±ï¼ˆaverage sum of rewardsï¼‰éšè¿­ä»£æ¬¡æ•°çš„å˜åŒ–ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
åŒ…æ‹¬å¤šç§ä¸»æµ BO æ–¹æ³•ï¼š
- **Gaussian Processes (GP)**
- **Sparse GP**ï¼ˆç¨€ç–è¯±å¯¼ç‚¹ï¼‰
- **Focalized Sparse GP** [45]
- **BNN-based æ–¹æ³•**ï¼š
  - HMC-BNN
  - SGHMC
  - LLA (Laplace Linear Approximation)
  - DKL (Deep Kernel Learning)
  - IBNN (Infinite-width BNN)

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ

| ä»»åŠ¡ | VBO-MI è¡¨ç° | æœ€ä½³åŸºçº¿ |
|------|-------------|----------|
| **Branin** | è¶…è¿‡æ‰€æœ‰ BNN åŸºçº¿ï¼Œæ¥è¿‘ GP | GP æœ€ä¼˜ |
| **Hartmann-6** | æ”¶æ•›æ›´å¿«ï¼Œæœ€ç»ˆå¥–åŠ±æ›´é«˜ | GP å’Œ IBNN è¾ƒå¥½ |
| **Ackley-10** | åˆæœŸè½åï¼ŒåæœŸåè¶…ï¼Œè¾¾åˆ°æœ€é«˜å¥–åŠ± | Sparse GP åˆå§‹é¢†å…ˆ |
| **PDE Optimization** | æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ | Sparse GP ç¬¬äºŒ |
| **Lunar Lander** | æœ€ç»ˆå¥–åŠ±è¾¾ **170.97**ï¼Œè¿œè¶…æ¬¡ä¼˜çš„ 149.33 | VBO with GP exploitation |
| **Pest Control** | æœ€ç»ˆå¥–åŠ± **-9.02**ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»– | Sparse (-9.75) |
| **Interferometer** | æœ€ç»ˆå¥–åŠ± **0.890**ï¼Œå¤§å¹…é¢†å…ˆ | Sparse (0.730) |

> å›¾è¡¨æ˜¾ç¤ºï¼Œéšç€ç»´åº¦å‡é«˜ï¼ˆå¦‚ Pest Control ä¸º 25Dï¼‰ï¼ŒVBO-MI æ€§èƒ½ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰
é€šè¿‡æ›¿æ¢ VBO-MI acquisition å‡½æ•°ä¸­çš„æ¢ç´¢æˆ–åˆ©ç”¨é¡¹ä¸º GP å¯¹åº”é¡¹è¿›è¡Œåˆ†æï¼ˆè§ Figure 5ï¼‰ï¼š

| æ›¿æ¢æ–¹å¼ | å½±å“ç¨‹åº¦ | ç»“è®º |
|--------|--------|------|
| **ç”¨ GP posterior æ›¿æ¢ exploration term** | æ€§èƒ½ä¸¥é‡ä¸‹é™ | æ¢ç´¢é¡¹æ˜¯ VBO æˆåŠŸçš„å…³é”® |
| **ç”¨ GP posterior æ›¿æ¢ exploitation term** | æ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œä½†å½±å“è¾ƒå° | åˆ©ç”¨é¡¹ç›¸å¯¹æ¬¡è¦ |
| **å®Œæ•´ VBO-MI** | æ€§èƒ½æœ€ä¼˜ | ä¸¤é¡¹ååŒä½œç”¨æœ€ä½³ |

> è¡¨æ˜ï¼š**VBO-MI çš„æ¢ç´¢æœºåˆ¶ï¼ˆåŸºäºå˜åˆ† MI ä¼°è®¡ï¼‰æ˜¯å…¶ä¼˜è¶Šæ€§çš„æ ¸å¿ƒæ¥æº**ã€‚

### è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æï¼ˆFigure 8ï¼‰
- åœ¨ä¸åŒ $\beta$ å€¼ä¸‹æµ‹è¯•å„æ–¹æ³•æ€§èƒ½ï¼›
- **å‘ç°**ï¼šVBO-MI å¯¹ $\beta$ çš„å˜åŒ–æœ€ä¸æ•æ„Ÿï¼Œåœ¨å¤šä¸ªå–å€¼ä¸‹å‡ä¿æŒé«˜æ€§èƒ½ï¼›
- å…¶ä»–æ–¹æ³•ï¼ˆå¦‚ GPã€HMCï¼‰æ€§èƒ½æ³¢åŠ¨è¾ƒå¤§ï¼›
> è¡¨æ˜ VBO-MI å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§å’Œæ˜“è°ƒæ€§ã€‚

### æ‰¹é‡å¤§å°å½±å“ï¼ˆFigure 7ï¼‰
- éšç€ batch size å¢å¤§ï¼ŒVBO-MI æ€§èƒ½åœ¨å¤šæ•°ä»»åŠ¡ä¸ŠæŒç»­æå‡ï¼Œç›´åˆ°é¥±å’Œï¼›
- å°¤å…¶åœ¨é«˜ç»´ä»»åŠ¡ï¼ˆå¦‚ Pest Controlï¼‰ä¸­ï¼Œå¤§ batch æ›´æœ‰åˆ©ï¼›
> ä½“ç°å…¶è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

### è®¡ç®—å¤æ‚åº¦åˆ†æï¼ˆFigure 6 & Table 1ï¼‰
| æ–¹æ³• | Surrogate Update | Acquisition Opt. | æ€»ä½“å¤æ‚åº¦ |
|------|------------------|------------------|------------|
| Exact GP | $O(T^3)$ | $N \cdot O(T)$ | Cubic |
| HMC-BNN | $O(S \cdot L \cdot T \cdot W)$ | $N \cdot O(S \cdot W)$ | High Linear |
| **VBO-MI (Ours)** | $O(K_a \cdot T \cdot H)$ | $O(K_b \cdot W)$ | **Low Linear** |

- **FLOPs å¯¹æ¯”**ï¼šVBO-MI åœ¨ acquisition é˜¶æ®µæ¯”é‡‡æ ·å‹ BNN æ–¹æ³•é™ä½çº¦ **102Ã—**ï¼›
- ä¸éœ€è¦é¢å¤–ä¼˜åŒ–å™¨ï¼ˆå¦‚ L-BFGSï¼‰æ¥æœ€å¤§åŒ– acquisition functionã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **VBO-MI æ˜¯é¦–ä¸ªå®Œå…¨åŸºäºæ¢¯åº¦çš„ BO æ¡†æ¶**ï¼Œé€šè¿‡å˜åˆ†äº’ä¿¡æ¯ä¼°è®¡å®ç°äº† acquisition function çš„å¯å¾®å­¦ä¹ ï¼›
2. **Actor-Critic æ¶æ„æœ‰æ•ˆè§£è€¦äº†æ¢ç´¢ä¸åˆ©ç”¨**ï¼Œå¹¶é€šè¿‡è”åˆè®­ç»ƒå®ç°é«˜æ•ˆæœç´¢ï¼›
3. **æ¢ç´¢é¡¹çš„è®¾è®¡è‡³å…³é‡è¦**ï¼šåŸºäºå˜åˆ† MI çš„æ¢ç´¢æœºåˆ¶æ˜¾è‘—ä¼˜äºä¼ ç»Ÿ GP åéªŒæ–¹å·®ï¼›
4. **åœ¨é«˜ç»´ã€å¤æ‚ã€å«å™ªå£°çš„çœŸå®ä»»åŠ¡ä¸­è¡¨ç°å“è¶Š**ï¼Œå°¤å…¶åœ¨ Lunar Lander å’Œ Pest Control ä¸Šé¥é¥é¢†å…ˆï¼›
5. **è®¡ç®—æ•ˆç‡æé«˜**ï¼Œæ¶ˆé™¤äº†ä¼ ç»Ÿ BO çš„â€œå†…å±‚ä¼˜åŒ–â€ç“¶é¢ˆï¼Œé€‚åˆå¤§è§„æ¨¡éƒ¨ç½²ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ¡†æ¶ä¾èµ–ä¸¤ä¸ªç¥ç»ç½‘ç»œçš„äº¤æ›¿è®­ç»ƒï¼Œå¯èƒ½å­˜åœ¨è®­ç»ƒç¨³å®šæ€§é—®é¢˜ï¼ˆå°½ç®¡å®éªŒä¸­æœªå‡ºç°ï¼‰ï¼›
- å¯¹äºæä½é¢„ç®—ï¼ˆT < 20ï¼‰çš„ä»»åŠ¡ï¼Œwarm-up é˜¶æ®µå¯èƒ½å½±å“åˆæœŸæ€§èƒ½ï¼›
- è™½ç„¶ç†è®ºéƒ¨åˆ†è¯æ˜äº†ä¸€è‡´æ€§ï¼Œä½†åœ¨æç«¯éå¹³ç¨³æˆ–å¯¹æŠ—æ€§å‡½æ•°ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä»å¾…éªŒè¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å°† VBO-MI æ‰©å±•åˆ° **multi-objective BO** å’Œ **constrained BO** åœºæ™¯ï¼›
- æ¢ç´¢æ›´é«˜æ•ˆçš„ MI ä¼°è®¡å™¨ï¼ˆå¦‚ TUBAã€InfoNCEï¼‰ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼›
- åº”ç”¨äºæ›´å¤§è§„æ¨¡çš„å®é™…ç³»ç»Ÿï¼Œå¦‚èŠ¯ç‰‡è®¾è®¡ã€è¯ç‰©åˆ†å­ä¼˜åŒ–ç­‰ï¼›
- ç ”ç©¶å¦‚ä½•è‡ªé€‚åº”è°ƒæ•´ $\beta$ æˆ–å®ç°å…è°ƒå‚ç‰ˆæœ¬ã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **VBO-MI é€šè¿‡å˜åˆ†äº’ä¿¡æ¯ä¼°è®¡å’Œç«¯åˆ°ç«¯æ¢¯åº¦ä¼˜åŒ–ï¼Œæ„å»ºäº†ä¸€ä¸ªé«˜æ•ˆã€çµæ´»ã€å¯æ‰©å±•çš„æ–°å‹è´å¶æ–¯ä¼˜åŒ–æ¡†æ¶ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå…¨é¢è¶…è¶Šä¼ ç»Ÿ GP å’Œ BNN æ–¹æ³•ï¼Œå°¤å…¶é€‚ç”¨äºé«˜ç»´å¤æ‚ç°å®ä»»åŠ¡ã€‚**

</details>

---

### 15. [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)

**Authors**: Nawazish Alia, Rachael Shawb, Karl Mason  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.08052v1  

#### Abstract
Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬ç ”ç©¶é’ˆå¯¹**å¥¶ç‰›åœºèƒ½æºç®¡ç†ä¸­çš„è´Ÿè·è°ƒåº¦é—®é¢˜**ï¼Œç‰¹åˆ«æ˜¯ç”µæ± å‚¨èƒ½ç³»ç»Ÿï¼ˆBattery Storageï¼‰å’Œç”µçƒ­æ°´å™¨ï¼ˆWater Heaterï¼‰çš„ä¼˜åŒ–æ§åˆ¶ã€‚è¯¥é¢†åŸŸé¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- å¯å†ç”Ÿèƒ½æºï¼ˆå¦‚å…‰ä¼PVï¼‰å…·æœ‰é—´æ­‡æ€§å’Œä¸ç¡®å®šæ€§ï¼›
- ç”µä»·åŠ¨æ€å˜åŒ–ï¼Œå­˜åœ¨å³°è°·å·®å¼‚ï¼›
- ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å‡è®¾å¯¹æœªæ¥ä»·æ ¼æˆ–å‘ç”µé‡æœ‰å®Œå…¨å…ˆéªŒçŸ¥è¯†ï¼Œè¿™åœ¨ç°å®ä¸­ä¸æˆç«‹ï¼›
- æ ‡å‡†PPOç®—æ³•ä¾èµ–å›ºå®šclippingé˜ˆå€¼æˆ–KLæ•£åº¦é™åˆ¶ï¼Œåœ¨ç”µä»·æ³¢åŠ¨ç¯å¢ƒä¸‹è®­ç»ƒä¸ç¨³å®šã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
è®ºæ–‡æå‡ºäº†ä¸¤ä¸ªåŸºäºProximal Policy Optimization (PPO) çš„æ”¹è¿›æ¡†æ¶ï¼š

#### ï¼ˆ1ï¼‰Forecast-Aware PPO (F-PPO)
- å°†**çŸ­æœŸé¢„æµ‹ä¿¡å·**ï¼ˆéœ€æ±‚ã€å¯å†ç”Ÿèƒ½æºå‡ºåŠ›ï¼‰èå…¥æ™ºèƒ½ä½“çš„è§‚æµ‹ç©ºé—´ï¼›
- ä½¿ç”¨â€œå­£èŠ‚æ€§æœ´ç´ æ¨¡å‹â€ï¼ˆseasonal-naive forecastï¼‰ç»“åˆ**Hour-of-Day å’Œ Month-based Residual Calibration**è¿›è¡Œæ®‹å·®æ ¡å‡†ï¼Œç”Ÿæˆæœªæ¥24å°æ—¶çš„éœ€æ±‚ä¸PVé¢„æµ‹ï¼›
- é¢„æµ‹ä¿¡æ¯é€šè¿‡ä¸€ä¸ªGRUç½‘ç»œç¼–ç åæ‹¼æ¥åˆ°çŠ¶æ€å‘é‡ä¸­ï¼Œä½¿ç­–ç•¥å…·å¤‡å‰ç»æ€§å†³ç­–èƒ½åŠ›ã€‚

#### ï¼ˆ2ï¼‰PID-KL PPO
- å¼•å…¥**PIDæ§åˆ¶å™¨è°ƒèŠ‚KLæ•£åº¦æƒ©ç½šç³»æ•° $c_{KL}$**ï¼Œå®ç°å¯¹ç­–ç•¥æ›´æ–°æ­¥é•¿çš„è‡ªé€‚åº”è°ƒæ§ï¼›
- åŠ¨æ€è°ƒæ•´ä¿¡ä»»åŒºåŸŸå¤§å°ï¼Œé¿å…å› å¥–åŠ±å°ºåº¦å˜åŒ–å¯¼è‡´çš„è®­ç»ƒéœ‡è¡ï¼›
- å…¬å¼å¦‚ä¸‹ï¼š
  $$
  c_{KL} = \max\left(0, c_{KL} + K_p \cdot e + K_i \cdot I + K_d \cdot D\right)
  $$
  å…¶ä¸­ $e = KL_{\text{measured}} - KL_{\text{target}}$ æ˜¯è¯¯å·®é¡¹ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **ç°å®é€‚ç”¨æ€§** | ä¸ä¾èµ–æœªæ¥ä¿¡æ¯ï¼Œä»…ç”¨å†å²æ•°æ®æ„å»ºé¢„æµ‹æ¨¡å—ï¼Œæ›´è´´è¿‘çœŸå®åœºæ™¯ |
| **ç¨³å®šæ€§æå‡** | PID-KLæœºåˆ¶æ˜¾è‘—æ”¹å–„è®­ç»ƒæ”¶æ•›æ€§ï¼Œå‡å°‘è¶…å‚æ•°è°ƒä¼˜è´Ÿæ‹… |
| **æˆæœ¬èŠ‚çº¦æ•ˆæœ** | åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¼˜äºPPOã€DQNã€SACç­‰ä¸»æµRLç®—æ³• |
| **çº¦æŸæ»¡è¶³èƒ½åŠ›** | å®ç°é›¶è¿è§„è¿è¡Œï¼ˆzero constraint violationsï¼‰ï¼Œä¿éšœè®¾å¤‡å®‰å…¨ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†ä½¿ç”¨æƒ…å†µ
| ä»»åŠ¡ | æ•°æ®æ¥æº | å†…å®¹è¯´æ˜ |
|------|----------|--------|
| **Battery Scheduling** | èŠ¬å…°VTTæŠ€æœ¯ç ”ç©¶ä¸­å¿ƒå…¬å¼€æ•°æ®é›† | åŒ…å«ä¸€å¹´æ¯å°æ—¶ç”µåŠ›è´Ÿè·ï¼ˆ~261 MWh/å¹´ï¼‰ã€20kWå…‰ä¼æ¨¡æ‹Ÿæ•°æ®ï¼ˆNREL SAMï¼‰ã€èµ«å°”è¾›åŸºåœ°åŒºåŠ¨æ€ç”µä»· |
| **Water Heater Scheduling** | çˆ±å°”å…°å¥¶ç‰›åœºç ”ç©¶æ•°æ® | å«200å¤´ç‰›è§„æ¨¡å†œåœºçš„åˆ†é¡¹ç”¨ç”µæ•°æ®ï¼ˆåŒ…æ‹¬çƒ­æ°´å™¨ï¼‰ã€20kWå…‰ä¼è¾“å‡ºã€Electric Irelandæä¾›çš„åˆ†æ—¶ç”µä»·ï¼ˆTime-of-use tariffsï¼‰ |

> æ³¨ï¼šç”±äºèŠ¬å…°æ•°æ®æœªæä¾›è®¾å¤‡çº§è´Ÿè½½æ‹†åˆ†ï¼Œå› æ­¤åŠ çƒ­å™¨è°ƒåº¦å®éªŒé‡‡ç”¨çˆ±å°”å…°æ•°æ®ã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### MDPå»ºæ¨¡ç»†èŠ‚
| ç»„ä»¶ | æè¿° |
|------|------|
| **State Space (Battery)** | `{hour, SOC, P_load, P_pv}` |
| **Action Space (Battery)** | `{Charge, Discharge, Idle}` |
| **Reward (Battery)** | è€ƒè™‘ç”µç½‘è´­ç”µæˆæœ¬ + SOCè¶Šç•Œæƒ©ç½šï¼ˆÂ±15ï¼‰ |
| **State Space (Heater)** | `{hour, E_price, P_pv, P_background, P_net, P_device, run_time}` + `heft`, `slack`ï¼ˆF-PPOæ–°å¢ï¼‰ |
| **Action Space (Heater)** | `{ON, OFF}` |
| **Reward (Heater)** | åŠ æƒç»„åˆï¼š<br>â€¢ æˆæœ¬å¥–åŠ± $R_{\text{cost}}$: åˆ©ç”¨å¯å†ç”Ÿç”µåŠ›é™ä½æˆæœ¬<br>â€¢ ä»»åŠ¡å¥–åŠ± $R_{\text{task}}$: å®Œæˆæ¯æ—¥æ‰€éœ€åŠ çƒ­æ—¶é—´ç›®æ ‡ |

#### è¯„ä¼°æŒ‡æ ‡
- æ€»ç”µè´¹æ”¯å‡ºï¼ˆTotal Costï¼‰
- ç”µç½‘è¿›å£ç”µé‡ï¼ˆGrid Importsï¼‰
- æ—¥å‡å³°å€¼è´Ÿè·é™ä½ï¼ˆPeak Demand Reductionï¼‰
- ç”¨æˆ·æ»¡æ„åº¦ç‡ï¼ˆUser Satisfaction Rateï¼‰
- æ”¿ç­–ç¨³å®šæ€§ä¸è®­ç»ƒæ”¶æ•›æ›²çº¿
- ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆWilcoxon signed-rank testï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç®—æ³• | ç±»å‹ | ç”¨é€” |
|------|------|------|
| **Standard PPO** | On-policy, policy gradient | ä¸»è¦åŸºå‡†ä¹‹ä¸€ |
| **DQN** | Value-based, Q-learning | å¯¹æ¯”ç¦»æ•£åŠ¨ä½œä¸‹çš„è¡¨ç° |
| **Discrete SAC** | Off-policy, entropy-regularized | æä¾›éšæœºæ¢ç´¢åŸºçº¿ |
| **Q-learning / Rule-based** | ä¼ ç»Ÿæ–¹æ³• | ç”¨äºç”µæ± è°ƒåº¦æ¯”è¾ƒ |

æ‰€æœ‰ç®—æ³•å‡åŸºäºCleanRLæ¡†æ¶å®ç°ä»¥ç¡®ä¿å…¬å¹³æ€§ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### ï¼ˆ1ï¼‰æ°´åŠ çƒ­å™¨è°ƒåº¦ç»“æœï¼ˆTable 3ï¼‰
| Algorithm | æ€»æˆæœ¬ (â‚¬) | ç›¸è¾ƒæ ‡å‡†PPOé™å¹… | è¿è§„å¤©æ•° |
|-----------|------------|------------------|---------|
| DQN | 16,418 | â†‘4.1% | 60 underuse |
| Standard PPO | 15,744 | â€” | 0 |
| **Forecast-Aware PPO (F-PPO)** | **15,635** | **â†“0.7%** | **0** |
| F-PPO (dropout=0.15) | 15,582 | â†“1.0% | 17 underuse |
| **PID-KL PPO** | **15,624** | **â†“0.8%** | ~Â±1 underuse |
| Discrete SAC | 15,773 | â†‘0.2% | 0 |

> âœ… F-PPOæ¯”DQNèŠ‚çœ **4.76%** ç”µè´¹ï¼›ç›¸æ¯”æ ‡å‡†PPOå†é™çº¦ **1%**

#### ï¼ˆ2ï¼‰ç”µæ± è°ƒåº¦ç»“æœï¼ˆFigure 3 & 6ï¼‰
- PPOç›¸æ¯”æ— ç”µæ± åœºæ™¯å‡å°‘ç”µç½‘è¿›å£ **13.11%**
- ç›¸æ¯”Q-learningé™ä½ **1.62%** è¿›å£é‡
- ç›¸æ¯”rule-basedæ–¹æ³•é™ä½ **2.56%** è¿›å£é‡

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
| æŒ‡æ ‡ | F-PPO vs DQN | æ˜¾è‘—æ€§ï¼ˆp-valueï¼‰ |
|------|--------------|------------------|
| å¹³å‡æ—¥å³°å€¼éœ€æ±‚é™ä½ | â†“13.75% | 0.0019** |
| ç”µç½‘è¿›å£ç”µé‡ | â†“252.8 kWh/month | 0.0019** |
| ç”µè´¹æ”¯å‡º | â†“91.6 â‚¬/month | 0.0019** |

> æ‰€æœ‰ä¸‰é¡¹æŒ‡æ ‡å‡é€šè¿‡ **Wilcoxon signed-rank test** éªŒè¯ä¸ºç»Ÿè®¡æ˜¾è‘—ï¼ˆp < 0.05ï¼‰

### æ¶ˆèå®éªŒç»“æœ
- **åŠ å…¥forecastingæ¨¡å—ï¼ˆF-PPOï¼‰**ï¼š
  - æˆæœ¬ä¸‹é™çº¦1%ï¼Œä¸”ä¿æŒé›¶è¿è§„ï¼›
  - GRUç¼–ç å™¨èƒ½æ•æ‰æ—¶é—´ä¾èµ–å…³ç³»ï¼Œæ”¯æŒä¸»åŠ¨è°ƒåº¦ã€‚
- **å¢åŠ GRU dropoutè‡³0.15**ï¼š
  - æˆæœ¬è¿›ä¸€æ­¥ä¸‹é™è¾¾1.5%ï¼›
  - ä½†å‡ºç°17å¤©æœªå®ŒæˆåŠ çƒ­ä»»åŠ¡ï¼Œç”¨æˆ·æ»¡æ„åº¦é™è‡³94% â†’ è¡¨æ˜å­˜åœ¨**æˆæœ¬ä¼˜åŒ–ä¸çº¦æŸæ»¡è¶³ä¹‹é—´çš„æƒè¡¡**ã€‚
- **PID-KLæœºåˆ¶å¼•å…¥**ï¼š
  - æœ€ç»ˆæˆæœ¬ä¸F-PPOç›¸è¿‘ï¼ˆÂ±0.5%å†…ï¼‰ï¼›
  - ä½†è®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®šï¼Œrewardæ–¹å·®æ›´å°ï¼ˆè§Figure 10ï¼‰ï¼›
  - æ”¿ç­–æ›´æ–°æ›´åŠ å¹³æ»‘ï¼ŒæŠ—å¹²æ‰°èƒ½åŠ›å¼ºã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **é¢„æµ‹æ„ŸçŸ¥å‹PPOï¼ˆF-PPOï¼‰æœ‰æ•ˆæå‡è°ƒåº¦æ•ˆç‡**ï¼š
   - é€šè¿‡èåˆçŸ­æœŸé¢„æµ‹ä¿¡æ¯ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæå‰è§„åˆ’ï¼Œå°†é«˜èƒ½è€—æ“ä½œï¼ˆå¦‚åŠ çƒ­æ°´ï¼‰å®‰æ’åœ¨ç”µä»·ä½ã€å…‰ä¼å……è¶³çš„æ—¶æ®µã€‚
   - å®ç°äº†**ä¸»åŠ¨å¼è°ƒåº¦**è€Œéè¢«åŠ¨å“åº”ã€‚

2. **PID-KLæœºåˆ¶å¢å¼ºè®­ç»ƒé²æ£’æ€§**ï¼š
   - è‡ªé€‚åº”è°ƒèŠ‚KLæƒ©ç½šé¡¹ï¼Œè§£å†³äº†æ ‡å‡†PPOåœ¨å˜å¹…å¥–åŠ±ä¸‹æ˜“éœ‡è¡çš„é—®é¢˜ï¼›
   - å‡å°‘å¯¹è¶…å‚æ•°æ•æ„Ÿæ€§ï¼Œæ›´é€‚åˆå®é™…éƒ¨ç½²ã€‚

3. **PPOç±»æ–¹æ³•ä¼˜äºDQNä¸SAC**ï¼š
   - åœ¨ç¦»æ•£åŠ¨ä½œã€å¼ºçº¦æŸçš„ä»»åŠ¡ä¸­ï¼ŒPPOåŠå…¶å˜ç§è¡¨ç°å‡ºæ›´å¥½çš„ç¨³å®šæ€§å’Œç»æµæ€§ï¼›
   - DQNå› Îµ-greedyæ¢ç´¢ç­–ç•¥å¯¼è‡´æ¬¡ä¼˜è¡Œä¸ºï¼›
   - SACè™½å…·è‰¯å¥½æ¢ç´¢æ€§ï¼Œä½†åœ¨æœ¬ä»»åŠ¡ä¸­æ”¶æ•›æ…¢ä¸”è°ƒåº¦ä¸€è‡´æ€§è¾ƒå·®ã€‚

4. **çœŸå®ä¸–ç•Œå¯è¡Œæ€§éªŒè¯**ï¼š
   - æ‰€ææ–¹æ³•åœ¨çœŸå®å¥¶ç‰›åœºæ•°æ®ä¸ŠéªŒè¯æœ‰æ•ˆï¼›
   - èƒ½é€‚åº”å­£èŠ‚æ€§å˜åŒ–ï¼ˆå¤å­£å¤šå…‰â†’å¤šå‚¨ç”µï¼›å†¬å­£å°‘å…‰â†’åˆç†è´­ç”µï¼‰ï¼›
   - ç¬¦åˆè®¾å¤‡è¿è¡Œè¾¹ç•Œï¼ˆSOCç»´æŒåœ¨15%-85%ä¹‹é—´ï¼‰ã€‚

### æ–¹æ³•å±€é™æ€§
- å½“å‰æ¨¡å‹ä»ä¸ºå•æ™ºèƒ½ä½“æ¶æ„ï¼Œéš¾ä»¥åè°ƒå¤šä¸ªå¼‚æ„è´Ÿè½½ï¼ˆå¦‚æŒ¤å¥¶æœºã€å†·å´ç³»ç»Ÿç­‰ï¼‰ï¼›
- é¢„æµ‹æ¨¡å‹è¾ƒä¸ºç®€å•ï¼ˆseasonal-naive + residual bandingï¼‰ï¼Œæœªä½¿ç”¨å¤æ‚æ—¶åºæ¨¡å‹ï¼ˆå¦‚Transformerï¼‰ï¼›
- å®éªŒåŸºäºæ¨¡æ‹Ÿç¯å¢ƒï¼Œå°šæœªåœ¨ç‰©ç†ç³»ç»Ÿä¸­éƒ¨ç½²éªŒè¯ï¼›
- ä»…è€ƒè™‘å…‰ä¼ï¼Œæœªæ•´åˆé£èƒ½ã€æ²¼æ°”ç­‰å…¶ä»–å¯å†ç”Ÿèƒ½æºã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. æ¢ç´¢**Multi-Agent Reinforcement Learning (MARL)** æ¶æ„ï¼Œå®ç°å¤šè®¾å¤‡ååŒè°ƒåº¦ï¼›
2. å¼•å…¥**evolutionary RL** æˆ– **meta-learning** æå‡é•¿æœŸé€‚åº”èƒ½åŠ›ï¼›
3. æ•´åˆæ›´å¤šå¯å†ç”Ÿèƒ½æºå½¢å¼ï¼ˆwind, biogasï¼‰åŠå‚¨èƒ½ç±»å‹ï¼›
4. å¼€å±•å®åœ°è¯•ç‚¹é¡¹ç›®ï¼ŒéªŒè¯æ§åˆ¶ç³»ç»Ÿåœ¨çœŸå®å†œåœºä¸­çš„æ€§èƒ½ï¼›
5. ç»“åˆè”é‚¦å­¦ä¹ ï¼ˆFederated Learningï¼‰ä¿æŠ¤å†œåœºæ•°æ®éšç§çš„åŒæ—¶å®ç°è·¨åœºååŒä¼˜åŒ–ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡æå‡ºäº†ä¸€ç§**é¢„æµ‹æ„ŸçŸ¥+è‡ªé€‚åº”æ­£åˆ™åŒ–**çš„PPOæ¡†æ¶ï¼ˆF-PPO + PID-KL PPOï¼‰ï¼Œåœ¨çœŸå®å¥¶ç‰›åœºè´Ÿè·è°ƒåº¦ä»»åŠ¡ä¸­å®ç°äº†æ¯”PPOä½1%ã€æ¯”DQNä½4.8%çš„ç”µè´¹èŠ‚çº¦ï¼Œå¹¶æ˜¾è‘—æå‡äº†è®­ç»ƒç¨³å®šæ€§ä¸ç”¨æˆ·æ»¡æ„åº¦ï¼Œä¸ºå†œä¸šå¯æŒç»­èƒ½æºç®¡ç†æä¾›äº†å¯æ‰©å±•çš„æŠ€æœ¯è·¯å¾„ã€‚

</details>

---

### 16. [Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks](https://arxiv.org/abs/2601.08254)

**Authors**: Abdikarim Mohamed Ibrahim, Rosdiadee Nordin  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.08254v1  

#### Abstract
Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LL...

---

### 17. [A Human-Centric Pipeline for Aligning Large Language Models with Chinese Medical Ethics](https://arxiv.org/abs/2601.07954)

**Authors**: Haoan Jin, Han Ying, Jiacheng Ji, Hanhui Xu, Mengyue Wu  
**Category**: cs.CL  
**Published**: 2026-01-14  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.07954v1  

#### Abstract
Recent advances in large language models have enabled their application to a range of healthcare tasks. However, aligning LLMs with the nuanced demands of medical ethics, especially under complex real world scenarios, remains underexplored. In this work, we present MedES, a dynamic, scenario-centric...

---

### 18. [Attention Projection Mixing and Exogenous Anchors](https://arxiv.org/abs/2601.08131)

**Authors**: Jonathan Su  
**Category**: cs.CL  
**Published**: 2026-01-14  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.08131v1  

#### Abstract
Transformers that reuse early-layer attention projections as residuals face a fundamental tension: the first layer must simultaneously serve as a stable reference for all deeper layers and as an effective computational block. To resolve this, we propose ExoFormer, which learns dedicated exogenous an...

---

### 19. [Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models](https://arxiv.org/abs/2601.08209)

**Authors**: Rongji Li, Jian Xu, Xueqing Chen, Yisheng Yang, Jiayi Wang, Xingyu Chen, Chunyu Xie, Dawei Leng, Xu-Yao Zhang  
**Category**: cs.CL  
**Published**: 2026-01-14  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.08209v1  

#### Abstract
In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowle...

---

### 20. [Get away with less: Need of source side data curation to build parallel corpus for low resource Machine Translation](https://arxiv.org/abs/2601.08629)

**Authors**: Saumitra Yadav, Manish Shrivastava  
**Category**: cs.CL  
**Published**: 2026-01-14  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.08629v1  

#### Abstract
Data curation is a critical yet under-researched step in the machine translation training paradigm. To train translation systems, data acquisition relies primarily on human translations and digital parallel sources or, to a limited degree, synthetic generation. But, for low-resource languages, human...

---

### 21. [Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies](https://arxiv.org/abs/2601.08136)

**Authors**: Zeyang Li, Sunbochen Tang, Navid Azizan  
**Category**: cs.LG  
**Published**: 2026-01-14  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.08136v1  

#### Abstract
Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is...

---

### 22. [Relational Knowledge Distillation Using Fine-tuned Function Vectors](https://arxiv.org/abs/2601.08169)

**Authors**: Andrea Kang, Yingnian Wu, Hongjing Lu  
**Category**: cs.CL  
**Published**: 2026-01-14  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.08169v1  

#### Abstract
Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known ...

---

### 23. [STAR: Detecting Inference-time Backdoors in LLM Reasoning via State-Transition Amplification Ratio](https://arxiv.org/abs/2601.08511)

**Authors**: Seong-Gyu Park, Sohee Park, Jisu Lee, Hyunsik Na, Daeseon Choi  
**Category**: cs.CL  
**Published**: 2026-01-14  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.08511v1  

#### Abstract
Recent LLMs increasingly integrate reasoning mechanisms like Chain-of-Thought (CoT). However, this explicit reasoning exposes a new attack surface for inference-time backdoors, which inject malicious reasoning paths without altering model parameters. Because these attacks generate linguistically coh...

---

### 24. [Shifting the Sweet Spot: High-Performance Matrix-Free Method for High-Order Elasticity](https://arxiv.org/abs/2601.08374)

**Authors**: Dali Chang, Chong Zhang, Kaiqi Zhang, Mingguan Yang, Huiyuan Li, Weiqiang Kong  
**Category**: cs.DC  
**Published**: 2026-01-14  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.08374v1  

#### Abstract
In high-order finite element analysis for elasticity, matrix-free (PA) methods are a key technology for overcoming the memory bottleneck of traditional Full Assembly (FA). However, existing implementations fail to fully exploit the special structure of modern CPU architectures and tensor-product ele...

---

### 25. [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://arxiv.org/abs/2601.08187)

**Authors**: Zijun Di, Bin Lu, Huquan Kang, Luoyi Fu, Jiaxin Ding, Xiaoying Gan, Lei Zhou, Xinbing Wang, Chenghu Zhou  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.08187v1  

#### Abstract
Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the ...

---

### 26. [M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games](https://arxiv.org/abs/2601.08462)

**Authors**: Sixiong Xie, Zhuofan Shi, Haiyang Shen, Gang Huang, Yun Ma, Xiang Jing  
**Category**: cs.AI  
**Published**: 2026-01-14  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.08462v1  

#### Abstract
As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcom...

---

### 27. [Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs](https://arxiv.org/abs/2601.08198)

**Authors**: Yibo Wang, Hai-Long Sun, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Lijun Zhang  
**Category**: cs.CL  
**Published**: 2026-01-14  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.08198v1  

#### Abstract
Recently, self-play fine-tuning (SPIN) has been proposed to adapt large language models to downstream applications with scarce expert-annotated data, by iteratively generating synthetic responses from the model itself. However, SPIN is designed to optimize the current reward advantages of annotated ...

---

### 28. [Max-Min Neural Network Operators For Approximation of Multivariate Functions](https://arxiv.org/abs/2601.07886)

**Authors**: Abhishek Yadav, Uaday Singh, Feng Dai  
**Category**: cs.LG  
**Published**: 2026-01-14  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.07886v1  

#### Abstract
In this paper, we develop a multivariate framework for approximation by max-min neural network operators. Building on the recent advances in approximation theory by neural network operators, particularly, the univariate max-min operators, we propose and analyze new multivariate operators activated b...

---

### 29. [LUT-Compiled Kolmogorov-Arnold Networks for Lightweight DoS Detection on IoT Edge Devices](https://arxiv.org/abs/2601.08044)

**Authors**: Oleksandr Kuznetsov  
**Category**: cs.LG  
**Published**: 2026-01-14  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.08044v1  

#### Abstract
Denial-of-Service (DoS) attacks pose a critical threat to Internet of Things (IoT) ecosystems, yet deploying effective intrusion detection on resource-constrained edge devices remains challenging. Kolmogorov-Arnold Networks (KANs) offer a compact alternative to Multi-Layer Perceptrons (MLPs) by plac...

---

### 30. [Scalable Multiagent Reinforcement Learning with Collective Influence Estimation](https://arxiv.org/abs/2601.08210)

**Authors**: Zhenglong Luo, Zhiyong Chen, Aoxiang Liu, Ke Pan  
**Category**: cs.LG  
**Published**: 2026-01-14  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.08210v1  

#### Abstract
Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is diffi...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
