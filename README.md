# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-18 12:54:59 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput](https://arxiv.org/abs/2511.11733)

**Authors**: Jingwei Song, Wanyi Chen, Xinyuan Song,  Max, Chris Tong, Gufeng Chen, Tianyi Zhao, Eric Yang, Bill Shi, Lynn Ai  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 16.0  
**Type**: new  
**ArXiv ID**: 2511.11733v1  

Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute...

---

### 2. [JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs](https://arxiv.org/abs/2508.15468)

**Authors**: Zhiqiang Que, Chang Sun, Sudarshan Paramesvaran, Emyr Clement, Katerina Karakoulaki, Christopher Brown, Lauri Laatu, Arianna Cox, Alexander Tapper, Wayne Luk, Maria Spiropulu  
**Category**: cs.LG  
**Published**: 2025-11-18  
**Score**: 13.5  
**Type**: replace-cross  
**ArXiv ID**: 2508.15468v2  

Graph Neural Networks (GNNs), particularly Interaction Networks (INs), have shown exceptional performance for jet tagging at the CERN High-Luminosity Large Hadron Collider (HL-LHC). However, their computational complexity and irregular memory access patterns pose significant challenges for deploymen...

---

### 3. [T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization](https://arxiv.org/abs/2511.13676)

**Authors**: Hyunwoo Oh, KyungIn Nam, Rajat Bhattacharjya, Hanning Chen, Tamoghno Das, Sanggeon Yun, Suyeon Jang, Andrew Ding, Nikil Dutt, Mohsen Imani  
**Category**: cs.LG  
**Published**: 2025-11-18  
**Score**: 11.0  
**Type**: cross  
**ArXiv ID**: 2511.13676v1  

Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based look...

---

### 4. [Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression](https://arxiv.org/abs/2511.11608)

**Authors**: Mingyu Sung, Suhwan Im, Daeho Bang, Il-Min Kim, Sangseok Yun, Jae-Mo Kang  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2511.11608v1  

Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repe...

---

### 5. [Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks](https://arxiv.org/abs/2511.11664)

**Authors**: Mingyu Sung, Suhwan Im, Vikas Palakonda, Jae-Mo Kang  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2511.11664v1  

Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverag...

---

### 6. [SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices](https://arxiv.org/abs/2511.11038)

**Authors**: Jiaming Huang, Yi Gao, Fuchang Pan, Renjie Li, Wei Dong  
**Category**: cs.AI  
**Published**: 2025-11-18  
**Score**: 10.0  
**Type**: cross  
**ArXiv ID**: 2511.11038v1  

With the rapid growth of the Internet of Things (IoT), integrating artificial intelligence (AI) on extremely weak embedded devices has garnered significant attention, enabling improved real-time performance and enhanced data privacy. However, the resource limitations of such devices and unreliable n...

---

### 7. [Multi-Personality Generation of LLMs at Decoding-time](https://arxiv.org/abs/2511.01891)

**Authors**: Rongxin Chen, Yunfan Li, Yige Yuan, Bingbing Xu, Huawei Shen  
**Category**: cs.CL  
**Published**: 2025-11-18  
**Score**: 10.0  
**Type**: replace  
**ArXiv ID**: 2511.01891v2  

Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibili...

---

### 8. [EcoSpa: Efficient Transformer Training with Coupled Sparsity](https://arxiv.org/abs/2511.11641)

**Authors**: Jinqi Xiao, Cheng Luo, Lingyi Huang, Cheng Yang, Yang Sui, Huy Phan, Xiao Zang, Yibiao Ying, Zhexiang Tang, Anima Anandkumar, Bo Yuan  
**Category**: cs.LG  
**Published**: 2025-11-18  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2511.11641v1  

Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in atten...

---

### 9. [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)

**Authors**: Shulin Liu, Dong Du, Tao Yang, Yang Li, Boyu Qiu  
**Category**: cs.AI  
**Published**: 2025-11-18  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.11373v1  

Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offe...

---

### 10. [STAGE: A Symbolic Tensor grAph GEnerator for distributed AI system co-design](https://arxiv.org/abs/2511.10480)

**Authors**: Changhai Man, Joongun Park, Hanjiang Wu, Huan Xu, Srinivas Sridharan, Tushar Krishna  
**Category**: cs.AI  
**Published**: 2025-11-18  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.10480v2  

Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies...

---

### 11. [ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments](https://arxiv.org/abs/2511.11586)

**Authors**: Ao Zhou, Jianlei Yang, Tong Qiao, Yingjie Qi, Xinming Wei, Cenlin Duan, Weisheng Zhao, Chunming Hu  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.11586v1  

The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline mode...

---

### 12. [CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models](https://arxiv.org/abs/2405.13746)

**Authors**: Huiwen Wu, Xiaogang Xu, Deyi Zhang, Xiaohan Li, Jiafei Wu, Zhe Liu  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2405.13746v3  

The success of current Large-Language Models (LLMs) hinges on extensive training data that is collected and stored centrally, called Centralized Learning (CL). However, such a collection manner poses a privacy threat, and one potential solution is Federated Learning (FL), which transfers gradients, ...

---

### 13. [DeToNATION: Decoupled Torch Network-Aware Training on Interlinked Online Nodes](https://arxiv.org/abs/2502.06728)

**Authors**: Mogens Henrik From, Jacob Nielsen, Lukas Galke Poech, Peter Schneider-Kamp  
**Category**: cs.LG  
**Published**: 2025-11-18  
**Score**: 9.5  
**Type**: replace  
**ArXiv ID**: 2502.06728v4  

Training large neural network models requires extensive computational resources, often distributed across several nodes and accelerators. Recent findings suggest that it may be sufficient to only exchange the fast moving components of the gradients, while accumulating momentum locally (Decoupled Mom...

---

### 14. [KernelDNA: Dynamic Kernel Sharing via Decoupled Naive Adapters](https://arxiv.org/abs/2503.23379)

**Authors**: Haiduo Huang, Yadong Zhang, Yinghui Xu, Pengju Ren  
**Category**: cs.LG  
**Published**: 2025-11-18  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2503.23379v2  

Dynamic convolution enhances model capacity by adaptively combining multiple kernels, yet faces critical trade-offs: prior works either (1) incur significant parameter overhead by scaling kernel numbers linearly, (2) compromise inference speed through complex kernel interactions, or (3) struggle to ...

---

### 15. [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)

**Authors**: Marco Kurzynski, Shaizeen Aga, Di Wu  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2511.09861v2  

GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, su...

---

### 16. [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)

**Authors**: Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian H\"aggstr\"om, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, H\r{a}kan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2511.03092v4  

The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy....

---

### 17. [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)

**Authors**: Shien Zhu, Samuel Bohl, Robin Oester, Gustavo Alonso  
**Category**: cs.AI  
**Published**: 2025-11-18  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.10676v1  

Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, ex...

---

### 18. [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)

**Authors**: Omar Erak, Omar Alhussein, Hatem Abou-Zeid, Mehdi Bennis  
**Category**: cs.AI  
**Published**: 2025-11-18  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2509.09168v2  

Large-scale transformer models have emerged as a powerful tool for semantic communication systems, enabling edge devices to extract rich representations for robust inference across noisy wireless channels. However, their substantial computational demands remain a major barrier to practical deploymen...

---

### 19. [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)

**Authors**: Xiaoyu Lin, Aniket Ghorpade, Hansheng Zhu, Justin Qiu, Dea Rrozhani, Monica Lama, Mick Yang, Zixuan Bian, Ruohan Ren, Alan B. Hong, Jiatao Gu, Chris Callison-Burch  
**Category**: cs.CL  
**Published**: 2025-11-18  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.12452v1  

With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual...

---

### 20. [AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism](https://arxiv.org/abs/2511.11617)

**Authors**: Wendong Xu, Chujie Chen, He Xiao, Kuan Li, Jing Xiong, Chen Zhang, Wenyong Zhou, Chaofan Tao, Yang Bai, Bei Yu, Ngai Wong  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.11617v1  

Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Para...

---

### 21. [Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows](https://arxiv.org/abs/2511.11739)

**Authors**: Christina Schenk, Miguel Hern\'andez-del-Valle, Luis Calero-Lumbreras, Marcus Noack, Maciej Haranczyk  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.11739v1  

Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D print...

---

### 22. [MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity](https://arxiv.org/abs/2511.13061)

**Authors**: Vladim\'ir Macko, Vladim\'ir Bo\v{z}a  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.13061v1  

Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited me...

---

### 23. [Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation](https://arxiv.org/abs/2511.12779)

**Authors**: Zhenshuo Zhang, Minxuan Duan, Youran Ye, Hongyang R. Zhang  
**Category**: cs.LG  
**Published**: 2025-11-18  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.12779v1  

We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be...

---

### 24. [Representation Meets Optimization: Training PINNs and PIKANs for Gray-Box Discovery in Systems Pharmacology](https://arxiv.org/abs/2504.07379)

**Authors**: Nazanin Ahmadi Daryakenari, Khemraj Shukla, George Em Karniadakis  
**Category**: cs.AI  
**Published**: 2025-11-18  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2504.07379v2  

Physics-Informed Kolmogorov-Arnold Networks (PIKANs) are gaining attention as an effective counterpart to the original multilayer perceptron-based Physics-Informed Neural Networks (PINNs). Both representation models can address inverse problems and facilitate gray-box system identification. However,...

---

### 25. [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)

**Authors**: Yaocheng Zhang, Haohuan Huang, Zijun Song, Yuanheng Zhu, Qichao Zhang, Zijie Zhao, Dongbin Zhao  
**Category**: cs.CL  
**Published**: 2025-11-18  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.12159v1  

Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning ...

---

### 26. [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)

**Authors**: Burkhard Ringlein, Jan van Lunteren, Radu Stoica, Thomas Parnell  
**Category**: cs.CL  
**Published**: 2025-11-18  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.11581v1  

A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform ...

---

### 27. [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)

**Authors**: Yuhan Chen, Yuxuan Liu, Long Zhang, Pengzhi Gao, Jian Luan, Wei Liu  
**Category**: cs.CL  
**Published**: 2025-11-18  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.13091v1  

Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling acr...

---

### 28. [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/abs/2504.06261)

**Authors**: Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Erik Schultheis, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, Dan Alistarh  
**Category**: cs.CL  
**Published**: 2025-11-18  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2504.06261v4  

Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is...

---

### 29. [Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks](https://arxiv.org/abs/2511.11598)

**Authors**: Van-Vi Vo, Tien-Dung Nguyen, Duc-Tai Le, Hyunseung Choo  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.11598v1  

Efficient routing in IoT sensor networks is critical for minimizing energy consumption and latency. Traditional centralized algorithms, such as Dijkstra's, are computationally intensive and ill-suited for dynamic, distributed IoT environments. We propose a novel distributed Q-learning framework for ...

---

### 30. [OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents](https://arxiv.org/abs/2511.11672)

**Authors**: Zengyi Qin, Jinyuan Chen, Yunze Man, Shengcao Cao, Ziqi Pang, Zhuoyuan Wang, Xin Sun, Gen Lin, Han Fang, Ling Zhu, Zixin Xie, Zibu Wei, Tianshu Ran, Haoran Geng, Xander Wu, Zachary Bright, Qizhen Sun, Rui Wang, Yuyang Cai, Song Wang, Jiace Zhao, Han Cao, Yeyang Zhou, Tianrui Liu, Ray Pan, Chongye Yang, Xiang Ren, Bo Zhang, Yutong Ban, Jitendra Malik, Brian Anthony, Pieter Abbeel  
**Category**: cs.DC  
**Published**: 2025-11-18  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.11672v1  

We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
