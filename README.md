# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-09-17 12:51:55 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [TinyServe: Query-Aware Cache Selection for Efficient LLM Serving](https://arxiv.org/abs/2509.12211)

**Authors**: Dong Liu, Yanxuan Yu  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 11.0

arXiv:2509.12211v1 Announce Type: cross 
Abstract: Serving large language models (LLMs) efficiently remains challenging due to the high memory and latency overhead of key-value (KV) cache access during autoregressive decoding. We present \textbf{TinyServe}, a lightweight and extensible serving syste...

---

### 2. [PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training](https://arxiv.org/abs/2507.11683)

**Authors**: Seth Ockerman, Amal Gueroudji, Tanwi Mallick, Yixuan He, Line Pouchard, Robert Ross, Shivaram Venkataraman  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 9.5

arXiv:2507.11683v3 Announce Type: replace-cross 
Abstract: Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for modeling spatial and temporal data dependencies. However, their applications have been limited primarily to small-scale datasets because of memory constraints. While distr...

---

### 3. [Online Learning Based Efficient Resource Allocation for LoRaWAN Network](https://arxiv.org/abs/2509.10493)

**Authors**: Ruiqi Wang, Wenjun Li, Jing Ren, Tongyu Song, Xiong Wang, Sheng Wang, Shizhong Xu  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 9.0

arXiv:2509.10493v2 Announce Type: replace-cross 
Abstract: The deployment of large-scale LoRaWAN networks requires jointly optimizing conflicting metrics like Packet Delivery Ratio (PDR) and Energy Efficiency (EE) by dynamically allocating transmission parameters, including Carrier Frequency, Spread...

---

### 4. [IsoSched: Preemptive Tile Cascaded Scheduling of Multi-DNN via Subgraph Isomorphism](https://arxiv.org/abs/2509.12208)

**Authors**: Boran Zhao, Zihang Yuan, Yanbin Hu, Haiming Zhai, Haoruo Zhang, Wenzhe Zhao, Tian Xia, Pengju Ren  
**Category**: cs.DC  
**Published**: 2025-09-17  
**Score**: 9.0

arXiv:2509.12208v1 Announce Type: new 
Abstract: Deploying deep neural network (DNN) accelerators with Layer Temporal Scheduling (LTS) often incurs significant overheads (e.g., energy and latency), as intermediate activations must be cached in DRAM. To alleviate this, Tile Spatial Scheduling (TSS) r...

---

### 5. [Principled Approximation Methods for Efficient and Scalable Deep Learning](https://arxiv.org/abs/2509.00174)

**Authors**: Pedro Savarese  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 9.0

arXiv:2509.00174v2 Announce Type: replace 
Abstract: Recent progress in deep learning has been driven by increasingly larger models. However, their computational and energy demands have grown proportionally, creating significant barriers to their deployment and to a wider adoption of deep learning t...

---

### 6. [TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval](https://arxiv.org/abs/2502.20969)

**Authors**: Chien-Yu Lin, Keisuke Kamahori, Yiyu Liu, Xiaoxiang Shi, Madhav Kashyap, Yile Gu, Rulin Shao, Zihao Ye, Kan Zhu, Stephanie Wang, Arvind Krishnamurthy, Rohan Kadekodi, Luis Ceze, Baris Kasikci  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 9.0

arXiv:2502.20969v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, leading to system challenges in latency-s...

---

### 7. [When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large Models](https://arxiv.org/abs/2509.12141)

**Authors**: Weihao Zhu, Long Shi, Kang Wei, Zhen Mei, Zhe Wang, Jiaheng Wang, Jun Li  
**Category**: cs.DC  
**Published**: 2025-09-17  
**Score**: 8.5

arXiv:2509.12141v2 Announce Type: replace 
Abstract: As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE) has become prevalent thanks to its sparsely-gated mechanism, which lowers computational overhead while maintaining learning performance comparable to dense LMs. The essenc...

---

### 8. [FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving](https://arxiv.org/abs/2509.06261)

**Authors**: Kyungmin Bin, Seungbeom Choi, Jimyoung Son, Jieun Choi, Daseul Bae, Daehyeon Baek, Kihyo Moon, Minsung Jang, Hyojung Lee  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 8.5

arXiv:2509.06261v2 Announce Type: replace-cross 
Abstract: Recent advances in Post-Training Quantization (PTQ) techniques have significantly increased demand for serving quantized large language models (LLMs), enabling higher throughput and substantially reduced memory usage with minimal accuracy lo...

---

### 9. [AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs](https://arxiv.org/abs/2509.11155)

**Authors**: Santhosh G S, Saurav Prakash, Balaraman Ravindran  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 8.0

arXiv:2509.11155v1 Announce Type: new 
Abstract: The quadratic complexity of the attention mechanism remains a fundamental barrier to scaling Large Language Models (LLMs) to longer contexts, creating a critical bottleneck in both computation and memory. To address this, we introduce AQUA (Attention ...

---

### 10. [Gradient Free Deep Reinforcement Learning With TabPFN](https://arxiv.org/abs/2509.11259)

**Authors**: David Schiff, Ofir Lindenbaum, Yonathan Efroni  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 8.0

arXiv:2509.11259v1 Announce Type: new 
Abstract: Gradient based optimization is fundamental to most modern deep reinforcement learning algorithms, however, it introduces significant sensitivity to hyperparameters, unstable training dynamics, and high computational costs. We propose TabPFN RL, a nove...

---

### 11. [MinatoLoader: Accelerating Machine Learning Training Through Efficient Data Preprocessing](https://arxiv.org/abs/2509.10712)

**Authors**: Rahma Nouaji, Stella Bitchebe, Ricardo Macedo, Oana Balmau  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 8.0

arXiv:2509.10712v1 Announce Type: cross 
Abstract: Data loaders are used by Machine Learning (ML) frameworks like PyTorch and TensorFlow to apply transformations to data before feeding it into the accelerator. This operation is called data preprocessing. Data preprocessing plays an important role in...

---

### 12. [Binary Quantization For LLMs Through Dynamic Grouping](https://arxiv.org/abs/2509.03054)

**Authors**: Xinzhe Zheng, Zhen-Qun Yang, Haoran Xie, S. Joe Qin, Arlene Chen, Fangzhen Lin  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 8.0

arXiv:2509.03054v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of Natural Language Processing (NLP) tasks, but require substantial memory and computational resources. Binary quantization, which compresses model weights f...

---

### 13. [PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning](https://arxiv.org/abs/2508.21104)

**Authors**: Wenfeng Feng, Penghong Zhao, Guochao Jiang, Chuzhan Hao, Yuewei Zhang, Hao Wang, Guohua Liu  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2508.21104v2 Announce Type: replace-cross 
Abstract: Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy ...

---

### 14. [MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining](https://arxiv.org/abs/2509.06806)

**Authors**: Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.06806v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonst...

---

### 15. [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)

**Authors**: Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou  
**Category**: cs.CL  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.13305v1 Announce Type: cross 
Abstract: Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, ...

---

### 16. [The Entropy of Parallel Systems](https://arxiv.org/abs/2509.12256)

**Authors**: Temitayo Adefemi  
**Category**: cs.DC  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.12256v1 Announce Type: new 
Abstract: Ever since Claude Shannon used entropy for his "Mathematical Theory of Communication", entropy has become a buzzword in research circles with scientists applying entropy to describe any phenomena that are reminiscent of disorder. In this paper, we use...

---

### 17. [Energy-Efficient Quantized Federated Learning for Resource-constrained IoT devices](https://arxiv.org/abs/2509.12814)

**Authors**: Wilfrid Sougrinoma Compaor\'e, Yaya Etiabi, El Mehdi Amhoud, Mohamad Assaad  
**Category**: cs.DC  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.12814v1 Announce Type: cross 
Abstract: Federated Learning (FL) has emerged as a promising paradigm for enabling collaborative machine learning while preserving data privacy, making it particularly suitable for Internet of Things (IoT) environments. However, resource-constrained IoT devic...

---

### 18. [Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via Reinforcement Learning](https://arxiv.org/abs/2507.10259)

**Authors**: Chengze Du, Zhiwei Yu, Heng Xu, Haojie Wang, Bo liu, Jialong Li  
**Category**: cs.DC  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2507.10259v2 Announce Type: replace 
Abstract: The rapid growth of large language model (LLM) services imposes increasing demands on distributed GPU inference infrastructure. Most existing scheduling systems follow a reactive paradigm, relying solely on the current system state to make decisio...

---

### 19. [LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs](https://arxiv.org/abs/2509.10511)

**Authors**: Umberto Gon\c{c}alves de Sousa  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.10511v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has transformed sequential decision-making, but traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy Optimization (PPO) often struggle with efficient exploration, stability, and adaptability in dynamic env...

---

### 20. [Holographic Knowledge Manifolds: A Novel Pipeline for Continual Learning Without Catastrophic Forgetting in Large Language Models](https://arxiv.org/abs/2509.10518)

**Authors**: Justin Arndt  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.10518v1 Announce Type: new 
Abstract: We introduce the Holographic Knowledge Manifold (HKM), a four-phase pipeline that achieves zero catastrophic forgetting in AI knowledge representation while maintaining minimal memory growth and high efficiency. Leveraging fractal quantization, probab...

---

### 21. [Efficient Single-Step Framework for Incremental Class Learning in Neural Networks](https://arxiv.org/abs/2509.11285)

**Authors**: Alejandro Dopico-Castro, Oscar Fontenla-Romero, Bertha Guijarro-Berdi\~nas, Amparo Alonso-Betanzos  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.11285v1 Announce Type: new 
Abstract: Incremental learning remains a critical challenge in machine learning, as models often struggle with catastrophic forgetting -the tendency to lose previously acquired knowledge when learning new information. These challenges are even more pronounced i...

---

### 22. [Deep operator network for surrogate modeling of poroelasticity with random permeability fields](https://arxiv.org/abs/2509.11966)

**Authors**: Sangjoon Park, Yeonjong Shin, Jinhyun Choo  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.11966v1 Announce Type: new 
Abstract: Poroelasticity -- coupled fluid flow and elastic deformation in porous media -- often involves spatially variable permeability, especially in subsurface systems. In such cases, simulations with random permeability fields are widely used for probabilis...

---

### 23. [AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models](https://arxiv.org/abs/2509.12019)

**Authors**: Sangjun Lee, Seung-taek Woo, Jungyu Jin, Changhun Lee, Eunhyeok Park  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.12019v1 Announce Type: new 
Abstract: To enable broader deployment of Large Language Models (LLMs), it is essential to identify the best-performing model under strict memory constraints. We present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework that assigns layer-wis...

---

### 24. [K2-Think: A Parameter-Efficient Reasoning System](https://arxiv.org/abs/2509.07604)

**Authors**: Zhoujun Cheng, Richard Fan, Shibo Hao, Taylor W. Killian, Haonan Li, Suqi Sun, Hector Ren, Alexander Moreno, Daqian Zhang, Tianjun Zhong, Yuxin Xiong, Yuanzhe Hu, Yutao Xie, Xudong Han, Yuqi Wang, Varad Pimpalkhute, Yonghao Zhuang, Aaryamonvikram Singh, Xuezhi Liang, Anze Xie, Jianshu She, Desai Fan, Chengqian Gao, Liqun Ma, Mikhail Yurochkin, John Maggs, Xuezhe Ma, Guowei He, Zhiting Hu, Zhengzhong Liu, Eric P. Xing  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2509.07604v3 Announce Type: replace 
Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance with a 32B parameter model, matching or surpassing much larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system shows that smaller mod...

---

### 25. [Think Small, Plan Smart: Minimalist Symbolic Abstraction and Heuristic Subspace Search for LLM-Guided Task Planning](https://arxiv.org/abs/2501.15214)

**Authors**: Junfeng Tang, Yuping Yan, Zihan Ye,  Zhenshou,  Song, Zeqi Zheng, Yaochu Jin  
**Category**: cs.LG  
**Published**: 2025-09-17  
**Score**: 7.5

arXiv:2501.15214v2 Announce Type: replace-cross 
Abstract: Reliable task planning is pivotal for achieving long-horizon autonomy in real-world robotic systems. Large language models (LLMs) offer a promising interface for translating complex and ambiguous natural language instructions into actionable...

---

### 26. [LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning](https://arxiv.org/abs/2509.12875)

**Authors**: Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang, Yuantao Han, Cangyi Yang, jiaxu Zhang, Feiliang Ren  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 7.0

arXiv:2509.12875v1 Announce Type: new 
Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck st...

---

### 27. [Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture](https://arxiv.org/abs/2509.12363)

**Authors**: Ritesh Janga, Rushit Dave  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 7.0

arXiv:2509.12363v1 Announce Type: cross 
Abstract: The agricultural sector is undergoing a transformation with the integration of advanced technologies, particularly in data-driven decision-making. This work proposes a federated learning framework for smart farming, aiming to develop a scalable, eff...

---

### 28. [Toward Ownership Understanding of Objects: Active Question Generation with Large Language Model and Probabilistic Generative Model](https://arxiv.org/abs/2509.12754)

**Authors**: Saki Hashimoto, Shoichi Hasegawa, Tomochika Ishikawa, Akira Taniguchi, Yoshinobu Hagiwara, Lotfi El Hafi, Tadahiro Taniguchi  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 7.0

arXiv:2509.12754v1 Announce Type: cross 
Abstract: Robots operating in domestic and office environments must understand object ownership to correctly execute instructions such as ``Bring me my cup.'' However, ownership cannot be reliably inferred from visual features alone. To address this gap, we p...

---

### 29. [Tuning-Free LLM Can Build A Strong Recommender Under Sparse Connectivity And Knowledge Gap Via Extracting Intent](https://arxiv.org/abs/2505.10900)

**Authors**: Wenqing Zheng, Noah Fatsi, Daniel Barcklow, Dmitri Kalaev, Steven Yao, Owen Reinert, C. Bayan Bruss, Daniele Rosa  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 7.0

arXiv:2505.10900v3 Announce Type: replace-cross 
Abstract: Recent advances in recommendation with large language models (LLMs) often rely on either commonsense augmentation at the item-category level or implicit intent modeling on existing knowledge graphs. However, such approaches struggle to captu...

---

### 30. [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)

**Authors**: Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang  
**Category**: cs.AI  
**Published**: 2025-09-17  
**Score**: 7.0

arXiv:2508.06259v4 Announce Type: replace-cross 
Abstract: Current multimodal large language models (MLLMs) still face significant challenges in complex visual tasks (e.g., spatial understanding, fine-grained perception). Prior methods have tried to incorporate visual reasoning, however, they fail t...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
