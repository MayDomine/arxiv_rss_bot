# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-09-19 12:50:45 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers](https://arxiv.org/abs/2509.15113)

**Authors**: Andrei Chertkov, Artem Basharin, Mikhail Saygin, Evgeny Frolov, Stanislav Straupe, Ivan Oseledets  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 11.5

arXiv:2509.15113v1 Announce Type: new 
Abstract: The growing demand for energy-efficient, high-performance AI systems has led to increased attention on alternative computing platforms (e.g., photonic, neuromorphic) due to their potential to accelerate learning and inference. However, integrating suc...

---

### 2. [Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization](https://arxiv.org/abs/2509.14279)

**Authors**: Robert Tjarko Lange, Qi Sun, Aaditya Prasad, Maxence Faldor, Yujin Tang, David Ha  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 10.0

arXiv:2509.14279v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) demonstrate their effectiveness in scaling test-time compute for software engineering tasks. However, these approaches often focus on high-level solutions, with limited attention to optimizing low-leve...

---

### 3. [A1: Asynchronous Test-Time Scaling via Conformal Prediction](https://arxiv.org/abs/2509.15148)

**Authors**: Jing Xiong, Qiujiang Chen, Fanghua Ye, Zhongwei Wan, Chuanyang Zheng, Chenyang Zhao, Hui Shen, Alexander Hanbo Li, Chaofan Tao, Haochen Tan, Haoli Bai, Lifeng Shang, Lingpeng Kong, Ngai Wong  
**Category**: cs.CL  
**Published**: 2025-09-19  
**Score**: 10.0

arXiv:2509.15148v1 Announce Type: new 
Abstract: Large language models (LLMs) benefit from test-time scaling, but existing methods face significant challenges, including severe synchronization overhead, memory bottlenecks, and latency, especially during speculative decoding with long reasoning chain...

---

### 4. [LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.14680)

**Authors**: Tianyang Duan, Zongyuan Zhang, Songxiao Guo, Dong Huang, Yuanye Zhao, Zheng Lin, Zihan Fang, Dianxin Luan, Heming Cui, Yong Cui  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 10.0

arXiv:2509.14680v1 Announce Type: cross 
Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issue...

---

### 5. [{\lambda}Scale: Enabling Fast Scaling for Serverless Large Language Model Inference](https://arxiv.org/abs/2502.09922)

**Authors**: Minchen Yu, Rui Yang, Chaobo Jia, Zhaoyuan Su, Sheng Yao, Tingfeng Lan, Yuchen Yang, Yue Cheng, Wei Wang, Ao Wang, Ruichuan Chen  
**Category**: cs.DC  
**Published**: 2025-09-19  
**Score**: 9.5

arXiv:2502.09922v2 Announce Type: replace 
Abstract: Serverless computing has emerged as a compelling solution for cloud-based model inference. However, as modern large language models (LLMs) continue to grow in size, existing serverless platforms often face substantial model startup overhead. This ...

---

### 6. [The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning](https://arxiv.org/abs/2509.15097)

**Authors**: Mohammad Saleh Vahdatpour, Huaiyuan Chu, Yanqing Zhang  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 9.5

arXiv:2509.15097v1 Announce Type: new 
Abstract: The rising computational and energy demands of deep learning, particularly in large-scale architectures such as foundation models and large language models (LLMs), pose significant challenges to sustainability. Traditional gradient-based training meth...

---

### 7. [Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures](https://arxiv.org/abs/2509.14920)

**Authors**: Amine Barrak, Fabio Petrillo, Fehmi Jaafar  
**Category**: cs.DC  
**Published**: 2025-09-19  
**Score**: 9.0

arXiv:2509.14920v1 Announce Type: new 
Abstract: The field of distributed machine learning (ML) faces increasing demands for scalable and cost-effective training solutions, particularly in the context of large, complex models. Serverless computing has emerged as a promising paradigm to address these...

---

### 8. [FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference](https://arxiv.org/abs/2509.14968)

**Authors**: Carlos Barroso-Fern\'andez, Alejandro Calvillo-Fernandez, Antonio de la Oliva, Carlos J. Bernardos  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 9.0

arXiv:2509.14968v1 Announce Type: new 
Abstract: The upcoming generations of wireless technologies promise an era where everything is interconnected and intelligent. As the need for intelligence grows, networks must learn to better understand the physical world. However, deploying dedicated hardware...

---

### 9. [MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration](https://arxiv.org/abs/2509.15187)

**Authors**: Giorgos Armeniakos, Alexis Maras, Sotirios Xydis, Dimitrios Soudris  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 8.5

arXiv:2509.15187v1 Announce Type: new 
Abstract: The evolution of quantization and mixed-precision techniques has unlocked new possibilities for enhancing the speed and energy efficiency of NNs. Several recent studies indicate that adapting precision levels across different parameters can maintain a...

---

### 10. [SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models](https://arxiv.org/abs/2509.14269)

**Authors**: Zhang Jianbin, Yulin Zhu, Wai Lun Lo, Richard Tai-Chiu Hsung, Harris Sik-Ho Tsang, Kai Zhou  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 8.0

arXiv:2509.14269v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved great success in medical question answering and clinical decision-making, promoting the efficiency and popularization of the personalized virtual doctor in society. However, the traditional fine-tuning stra...

---

### 11. [TITAN: A Trajectory-Informed Technique for Adaptive Parameter Freezing in Large-Scale VQE](https://arxiv.org/abs/2509.15193)

**Authors**: Yifeng Peng, Xinyi Li, Samuel Yen-Chi Chen, Kaining Zhang, Zhiding Liang, Ying Wang, Yuxuan Du  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 8.0

arXiv:2509.15193v1 Announce Type: cross 
Abstract: Variational quantum Eigensolver (VQE) is a leading candidate for harnessing quantum computers to advance quantum chemistry and materials simulations, yet its training efficiency deteriorates rapidly for large Hamiltonians. Two issues underlie this b...

---

### 12. [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)

**Authors**: Qihang Chen  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 7.5

arXiv:2509.14251v1 Announce Type: new 
Abstract: Metro crew planning is a key component of smart city development as it directly impacts the operational efficiency and service reliability of public transportation. With the rapid expansion of metro networks, effective multi-line scheduling and emerge...

---

### 13. [Communication Efficient Split Learning of ViTs with Attention-based Double Compression](https://arxiv.org/abs/2509.15058)

**Authors**: Federico Alvetreti, Jary Pomponi, Paolo Di Lorenzo, Simone Scardapane  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 7.5

arXiv:2509.15058v1 Announce Type: cross 
Abstract: This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during...

---

### 14. [LiMuon: Light and Fast Muon Optimizer for Large Models](https://arxiv.org/abs/2509.14562)

**Authors**: Feihu Huang, Yuning Luo, Songcan Chen  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 7.5

arXiv:2509.14562v1 Announce Type: new 
Abstract: Large models recently are widely applied in artificial intelligence, so efficient training of large models has received widespread attention. More recently, a useful Muon optimizer is specifically designed for matrix-structured parameters of large mod...

---

### 15. [Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models](https://arxiv.org/abs/2509.14265)

**Authors**: Siyuan Chen, Zhichao Lu, Qingfu Zhang  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 7.0

arXiv:2509.14265v1 Announce Type: cross 
Abstract: Automated kernel design is critical for overcoming software ecosystem barriers in emerging hardware platforms like RISC-V. While large language models (LLMs) have shown promise for automated kernel optimization, demonstrating success in CUDA domains...

---

### 16. [Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering](https://arxiv.org/abs/2509.15024)

**Authors**: Xuanting Xie, Bingheng Li, Erlin Pan, Rui Hou, Wenyu Chen, Zhao Kang  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 7.0

arXiv:2509.15024v1 Announce Type: cross 
Abstract: Attention mechanisms have become a cornerstone in modern neural networks, driving breakthroughs across diverse domains. However, their application to graph structured data, where capturing topological connections is essential, remains underexplored ...

---

### 17. [SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models](https://arxiv.org/abs/2509.15174)

**Authors**: Huy Nghiem, Advik Sachdeva, Hal Daum\'e III  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 7.0

arXiv:2509.15174v1 Announce Type: cross 
Abstract: WARNING: This paper contains examples of offensive materials. Toxic content has become pervasive on social media platforms. We introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LL...

---

### 18. [Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning](https://arxiv.org/abs/2509.15188)

**Authors**: Yeongbin Seo, Dongha Lee, Jaehyung Kim, Jinyoung Yeo  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 7.0

arXiv:2509.15188v1 Announce Type: cross 
Abstract: Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bott...

---

### 19. [LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models](https://arxiv.org/abs/2509.15218)

**Authors**: Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu  
**Category**: cs.CL  
**Published**: 2025-09-19  
**Score**: 7.0

arXiv:2509.15218v1 Announce Type: new 
Abstract: The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to...

---

### 20. [Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking](https://arxiv.org/abs/2509.14603)

**Authors**: Xingchen Wang, Feijie Wu, Chenglin Miao, Tianchun Li, Haoyu Hu, Qiming Cao, Jing Gao, Lu Su  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 7.0

arXiv:2509.14603v1 Announce Type: new 
Abstract: Split Federated Learning (SFL) has emerged as an efficient alternative to traditional Federated Learning (FL) by reducing client-side computation through model partitioning. However, exchanging of intermediate activations and model updates introduces ...

---

### 21. [Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning](https://arxiv.org/abs/2509.15087)

**Authors**: Lei Wang, Jieming Bian, Letian Zhang, Jie Xu  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 7.0

arXiv:2509.15087v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various tasks, but fine-tuning them for domain-specific applications often requires substantial domain-specific data that may be distributed across multiple organizations. F...

---

### 22. [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)

**Authors**: Fabian Kresse, Emily Yu, Christoph H. Lampert  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 7.0

arXiv:2507.02585v2 Announce Type: replace 
Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver efficient inference on resource-constrained hardware. We extend them with a trainable, differentiable interconnect whose parameter count remains constant as input width grows, al...

---

### 23. [DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models](https://arxiv.org/abs/2509.14268)

**Authors**: Jiachen Fu, Chun-Le Guo, Chongyi Li  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 6.5

arXiv:2509.14268v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) has drawn urgent attention to the task of machine-generated text detection (MGTD). However, existing approaches struggle in complex real-world scenarios: zero-shot detectors rely heavily on scori...

---

### 24. [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)

**Authors**: Dan Zhang, Min Cai, Jonathan Li, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang  
**Category**: cs.CL  
**Published**: 2025-09-19  
**Score**: 6.5

arXiv:2509.15110v1 Announce Type: cross 
Abstract: Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We i...

---

### 25. [Fast Multipole Attention: A Scalable Multilevel Attention Mechanism for Text and Images](https://arxiv.org/abs/2310.11960)

**Authors**: Yanming Kang, Giang Tran, Hans De Sterck  
**Category**: cs.CL  
**Published**: 2025-09-19  
**Score**: 6.5

arXiv:2310.11960v4 Announce Type: replace 
Abstract: While Transformer networks benefit from a global receptive field, their quadratic cost relative to sequence length restricts their application to long sequences and high-resolution inputs. We introduce Fast Multipole Attention (FMA), a divide-and-...

---

### 26. [Scaling Hybrid Quantum-HPC Applications with the Quantum Framework](https://arxiv.org/abs/2509.14470)

**Authors**: Srikar Chundury, Amir Shehata, Seongmin Kim, Muralikrishnan Gopalakrishnan Meena, Chao Lu, Kalyana Gottiparthi, Eduardo Antonio Coello Perez, Frank Mueller, In-Saeng Suh  
**Category**: cs.DC  
**Published**: 2025-09-19  
**Score**: 6.5

arXiv:2509.14470v1 Announce Type: cross 
Abstract: Hybrid quantum-high performance computing (Q-HPC) workflows are emerging as a key strategy for running quantum applications at scale in current noisy intermediate-scale quantum (NISQ) devices. These workflows must operate seamlessly across diverse s...

---

### 27. [HD3C: Efficient Medical Data Classification for Embedded Devices](https://arxiv.org/abs/2509.14617)

**Authors**: Jianglan Wei, Zhenyu Zhang, Pengcheng Wang, Mingjie Zeng, Zhigang Zeng  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 6.5

arXiv:2509.14617v1 Announce Type: new 
Abstract: Energy-efficient medical data classification is essential for modern disease screening, particularly in home and field healthcare where embedded devices are prevalent. While deep learning models achieve state-of-the-art accuracy, their substantial ene...

---

### 28. [Improving Internet Traffic Matrix Prediction via Time Series Clustering](https://arxiv.org/abs/2509.15072)

**Authors**: Martha Cash, Alexander Wyglinski  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 6.5

arXiv:2509.15072v1 Announce Type: new 
Abstract: We present a novel framework that leverages time series clustering to improve internet traffic matrix (TM) prediction using deep learning (DL) models. Traffic flows within a TM often exhibit diverse temporal behaviors, which can hinder prediction accu...

---

### 29. [HAM: Hierarchical Adapter Merging for Scalable Continual Learning](https://arxiv.org/abs/2509.13211)

**Authors**: Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco  
**Category**: cs.LG  
**Published**: 2025-09-19  
**Score**: 6.5

arXiv:2509.13211v3 Announce Type: replace 
Abstract: Continual learning is an essential capability of human cognition, yet it poses significant challenges for current deep learning models. The primary issue is that new knowledge can interfere with previously learned information, causing the model to...

---

### 30. [SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models](https://arxiv.org/abs/2509.14270)

**Authors**: Karan Dua, Puneet Mittal, Ranjeet Gupta, Hitesh Laxmichand Patel  
**Category**: cs.AI  
**Published**: 2025-09-19  
**Score**: 6.0

arXiv:2509.14270v1 Announce Type: cross 
Abstract: High-quality Text-to-Speech (TTS) model training requires extensive and diverse text and speech data. It is challenging to procure such data from real sources due to issues of domain specificity, licensing, and scalability. Large language models (LL...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
