# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-01-30 06:18:23 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Causal Autoregressive Diffusion Language Model](https://arxiv.org/abs/2601.22031)

**Authors**: Junhao Ruan, Bei Li, Yongjing Yin, Pengcheng Huang, Xin Chen, Jingang Wang, Xunliang Cai, Tong Xiao, JingBo Zhu  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2601.22031v1  

#### Abstract
In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervisi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šCausal Autoregressive Diffusion Language Model

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰ä¸»æµçš„ **Autoregressive Models (ARMs)** è™½ç„¶è®­ç»ƒç¨³å®šã€æ•°æ®æ•ˆç‡é«˜ï¼Œä½†å…¶**è‡ªå›å½’ç”Ÿæˆæ–¹å¼å¯¼è‡´æ¨ç†é€Ÿåº¦æ…¢**ï¼Œéš¾ä»¥æ»¡è¶³ä½å»¶è¿Ÿåº”ç”¨éœ€æ±‚ã€‚è€Œç°æœ‰çš„ **Text Diffusion Models**ï¼ˆå¦‚ MDLM å’Œ BD3LMï¼‰è™½ç„¶æ”¯æŒå¹¶è¡Œæ¨ç†ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´ä»¥ä¸‹ç“¶é¢ˆï¼š
- **MDLM** ä½¿ç”¨åŒå‘æ³¨æ„åŠ›ï¼ˆFull Attentionï¼‰ï¼Œæ— æ³•åˆ©ç”¨ **KV Cache**ï¼Œæ¨ç†æ•ˆç‡ä½ä¸‹ï¼›
- **BD3LM** å¼•å…¥å—çº§æ‰©æ•£æœºåˆ¶ï¼Œè™½æ¢å¤éƒ¨åˆ†å› æœç»“æ„ï¼Œä½†å¸¦æ¥æ˜¾è‘—çš„**è®­ç»ƒå¼€é”€**ï¼ˆå¤æ‚æ©ç ã€è¾“å…¥å¤åˆ¶ï¼‰å’Œ**å›ºå®šå—å¤§å°é™åˆ¶åŠ¨æ€å¹¶è¡Œæ€§**ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡º **CARD (Causal Autoregressive Diffusion)** æ¡†æ¶ï¼Œå°†ç¦»æ•£æ‰©æ•£è¿‡ç¨‹é‡æ„åœ¨ä¸¥æ ¼çš„**å› æœæ³¨æ„åŠ›æ¶æ„**ä¸‹ï¼Œå®ç°è®­ç»ƒæ•ˆç‡ä¸æ¨ç†é€Ÿåº¦çš„ç»Ÿä¸€ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **Shifted Causal Attention Mechanism**  
  åœ¨è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªä½ç½® $x_n$ é¢„æµ‹åŸå§‹ tokenï¼Œæ¡ä»¶ä¸ºå…¶å‰é¢è¢«å™ªå£°æ±¡æŸ“çš„å†å² $x_{<n}'$ã€‚é€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­å³å¯è·å¾—å…¨åºåˆ—å¯†é›†ç›‘ç£ä¿¡å·ï¼Œé¿å…äº†å—çº§å‘é‡åŒ–å¸¦æ¥çš„è®¡ç®—å†—ä½™ã€‚

- **Soft Tail Masking**  
  å°†å™ªå£°é›†ä¸­äºåºåˆ—å°¾éƒ¨çš„ä¸€ä¸ªåŠ¨æ€çª—å£å†…ï¼ˆè€Œéå‡åŒ€æˆ–ä¸¥æ ¼å°¾éƒ¨ï¼‰ï¼Œæ—¢ä¿ç•™äº†è¶³å¤Ÿçš„å¹²å‡€å†å²ä¸Šä¸‹æ–‡ä»¥ä¿è¯æ—©æœŸ token å¯é¢„æµ‹æ€§ï¼Œåˆç»´æŒå±€éƒ¨é‚»è¿‘ä¿¡æ¯ç”¨äºè¯­è¨€å»ºæ¨¡ã€‚

- **Context-aware Reweighting**  
  è®¾è®¡åŸºäºä¸Šä¸‹æ–‡æ¨¡ç³Šåº¦çš„æŸå¤±é‡åŠ æƒæœºåˆ¶ï¼Œä»ä¸‰ä¸ªç»´åº¦è¯„ä¼°ä¸Šä¸‹æ–‡è´¨é‡ï¼š
  - **Quantity**ï¼šå†å²ä¸­è¢«æ©ç çš„æ•°é‡
  - **Distance**ï¼šæœ€è¿‘æ©ç çš„è·ç¦»ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
  - **Density**ï¼šè¿ç»­æ©ç æ®µè½é•¿åº¦  
  ç»¼åˆä¸ºå±€éƒ¨æ¨¡ç³Šå¾—åˆ† $S_{\text{focal}}$ï¼Œç”¨äºåŠ¨æ€é™ä½é«˜ç†µä¸Šä¸‹æ–‡ä¸‹çš„æŸå¤±æƒé‡ï¼Œæå‡ä¼˜åŒ–ç¨³å®šæ€§ã€‚

- **Confidence-based Block Inference**  
  æ”¯æŒåŠ¨æ€å¹¶è¡Œè§£ç ï¼šæ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼å†³å®šæ¯æ¬¡è¿­ä»£æ›´æ–°å¤šå°‘ä¸ª [MASK] tokenï¼Œå¹¶ç»“åˆ KV Cache å®ç°ä»»æ„é•¿åº¦è¾“å‡ºçš„é«˜æ•ˆç”Ÿæˆã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | ARM | MDLM | BD3LM | CARD (Ours) |
|------|-----|-------|--------|-------------|
| å› æœç»“æ„ | âœ… | âŒ | âš ï¸ï¼ˆå—é—´å› æœï¼‰ | âœ… |
| KV Cache æ”¯æŒ | âœ… | âŒ | âŒ | âœ… |
| å¹¶è¡Œæ¨ç†èƒ½åŠ› | âŒ | âœ…ï¼ˆå¤šæ­¥è¿­ä»£ï¼‰ | âœ…ï¼ˆå—å†…å¹¶è¡Œï¼‰ | âœ…âœ…ï¼ˆåŠ¨æ€å¤š token/stepï¼‰ |
| è®­ç»ƒæ•ˆç‡ | âœ… | âš ï¸ï¼ˆ1.5Ã— å¼€é”€ï¼‰ | âŒï¼ˆ3Ã— å¼€é”€ï¼‰ | âœ…ï¼ˆä¸ ARM ç›¸å½“ï¼‰ |
| æ•°æ®åˆ©ç”¨ç‡ | âœ… | âš ï¸ï¼ˆ~50%ï¼‰ | âš ï¸ï¼ˆé‡å¤è¾“å…¥ï¼‰ | âœ…ï¼ˆæ¥è¿‘ 100%ï¼‰ |

> âœ… CARD æˆåŠŸèåˆäº† ARMs çš„è®­ç»ƒé«˜æ•ˆæ€§å’Œæ‰©æ•£æ¨¡å‹çš„å¹¶è¡Œæ¨ç†ä¼˜åŠ¿ï¼ŒåŒæ—¶è§£å†³äº†ä¼ ç»Ÿæ‰©æ•£æ–¹æ³•ä¸­çš„ä¿¡æ¯åå¡Œï¼ˆInformation Collapseï¼‰å’Œä¼˜åŒ–ä¸ç¨³å®šé—®é¢˜ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **é¢„è®­ç»ƒæ•°æ®**ï¼šFineWeb çš„å­é›†ï¼Œå…± **300B tokens**
- **å¾®è°ƒä¸è¯„ä¼°ä»»åŠ¡**ï¼š
  - å­¦ç§‘çŸ¥è¯†ç†è§£ï¼šARC-Challenge/Easy, MMLU-redux, SciQ
  - å¸¸è¯†æ¨ç†ï¼šCommonsenseQA, HellaSwag, PIQA
  - ä¸Šä¸‹æ–‡æ¶ˆæ­§ï¼šWinogrande
  - é€šç”¨è¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼šAG News, arXiv, LAMBADA, LM1B, OpenWebText, PTB, PubMed, WikiTextï¼ˆé›¶æ ·æœ¬ PPL æµ‹è¯•ï¼‰

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹è§„æ¨¡**ï¼šç»Ÿä¸€ä¸º **1B å‚æ•°é‡çº§**
- **è®­ç»ƒé…ç½®**ï¼š
  - åºåˆ—é•¿åº¦ï¼š128
  - ä¼˜åŒ–å™¨ï¼šAdamW + Cosine LR Schedulerï¼ˆå³°å€¼å­¦ä¹ ç‡ 3e-4ï¼‰
  - ç²¾åº¦ï¼šbfloat16 æ··åˆç²¾åº¦
  - æ³¨æ„åŠ›å®ç°ï¼šFlash Attention 2
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡ï¼ˆZero-shotï¼‰
  - é›¶æ ·æœ¬å›°æƒ‘åº¦ï¼ˆZero-shot PPLï¼‰
  - æ¨ç†ååé‡ï¼ˆtokens/secï¼‰
  - GenPPLï¼ˆç”Ÿæˆæ–‡æœ¬è´¨é‡è¯„ä¼°ï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Autoregressive Models (ARM)**ï¼šæ ‡å‡† GPT æ¶æ„
- **MDLM**ï¼šç®€åŒ–æ©ç ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆSimplified Masked Discrete Diffusionï¼‰
- **BD3LM**ï¼šå—çº§æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆBlock Diffusionï¼‰

æ‰€æœ‰åŸºçº¿å‡é‡‡ç”¨æœ€å…ˆè¿›çš„å·¥ç¨‹ä¼˜åŒ–ï¼ˆå¦‚ Flex Attentionã€torch.compileï¼‰ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### è¡¨ 1ï¼šä¸‹æ¸¸ä»»åŠ¡å¹³å‡å‡†ç¡®ç‡å¯¹æ¯”ï¼ˆ1B æ¨¡å‹ï¼Œ300B tokensï¼‰
| Model | AVG Accuracy |
|-------|--------------|
| ARM | 56.39 |
| BD3LM | 47.45 |
| MDLM | 47.55 |
| **CARD (Ours)** | **53.23** |

> CARD ç›¸æ¯”ç°æœ‰æ‰©æ•£åŸºçº¿æå‡ **+5.7% ç»å¯¹å¢ç›Š**ï¼Œæ¥è¿‘ ARM æ€§èƒ½ã€‚

#### è¡¨ 2ï¼šé›¶æ ·æœ¬å›°æƒ‘åº¦ï¼ˆPPLï¼‰å¯¹æ¯”ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
| Model | AVG PPL |
|-------|---------|
| ARM | 38.68 |
| BD3LM | 49.78 |
| MDLM | 49.84 |
| **CARD (Ours)** | **34.40** |

> CARD åœ¨ **8 ä¸ªé¢†åŸŸä¸­æœ‰ 6 ä¸ªä¼˜äº ARM**ï¼Œå°¤å…¶åœ¨é•¿ç¨‹ä¾èµ–ä»»åŠ¡ï¼ˆå¦‚ LAMBADAï¼‰è¡¨ç°æ›´ä¼˜ã€‚

#### è¡¨ 3ï¼šå°æ¨¡å‹å¤ç°å®éªŒï¼ˆ110M å‚æ•°ï¼Œ33B tokensï¼Œä½¿ç”¨ EMAï¼‰
| Model | PPL |
|-------|-----|
| ARM | 21.12 |
| BD3LM | 35.06 |
| MDLM | 37.48 |
| **CARD (Ours)** | **21.54** |

> å³ä½¿åœ¨å¼ºå¹³æ»‘ï¼ˆEMAï¼‰æ¡ä»¶ä¸‹ï¼ŒCARD ä»èƒ½é€¼è¿‘ ARM çš„å¯†åº¦ä¼°è®¡èƒ½åŠ›ï¼Œè¿œè¶…å…¶ä»–æ‰©æ•£æ¨¡å‹ã€‚

#### è¡¨ 5ï¼šæ¨ç†åŠ é€Ÿæ•ˆæœï¼ˆGenPPL åˆ†æï¼‰
| Decoding Config | Throughput (tok/s) | Speedup | AVG PPL â†“ |
|----------------|--------------------|--------|----------|
| ARM (Baseline) | 3,771 | 1.00Ã— | 11.19 |
| CARD (Block=16, Steps=16) | 6,441 | 1.71Ã— | 12.65 |
| CARD (Block=32, Steps=8) | 15,064 | **4.01Ã—** | 18.38 |

> CARD å®ç° **æœ€é«˜è¾¾ 4Ã— æ¨ç†åŠ é€Ÿ**ï¼Œåœ¨é€‚åº¦è®¾ç½®ä¸‹ä»…è½»å¾®ç‰ºç‰²è´¨é‡ï¼ˆPPL +1.5ï¼‰ï¼Œå…·å¤‡å®ç”¨ä»·å€¼ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 4ï¼‰

| è®¾ç½® | AVG Accuracy |
|------|--------------|
| CARD (Ours) | 53.21 |
| w/o Relaxed Window (Strict Tail) | 52.50 |
| w/o Tail Preference (Random) | 51.62 |
| w/o Context-aware Weighting | 51.66 |

> ç»“æœè¡¨æ˜ï¼š
- **Tail-biased noise åˆ†å¸ƒè‡³å…³é‡è¦**ï¼Œéšæœºæ©ç ä¸¥é‡æŸå®³æ€§èƒ½ï¼›
- **Soft Tail Window** æ˜æ˜¾ä¼˜äºä¸¥æ ¼å°¾éƒ¨æ©ç ï¼Œè¯´æ˜æ··åˆçŠ¶æ€æœ‰åŠ©äºå±€éƒ¨å»ºæ¨¡ï¼›
- **Context-aware Reweighting** å¯¹ç¨³å®šè®­ç»ƒæœ‰æ˜¾è‘—ä½œç”¨ï¼Œç§»é™¤åä¸‹é™çº¦ 1.55 ptsã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. âœ… **CARD æˆåŠŸå¼¥åˆäº† ARMs ä¸æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„é¸¿æ²Ÿ**ï¼šåœ¨ä¿æŒ ARM çº§è®­ç»ƒæ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾ 4Ã— çš„å¹¶è¡Œæ¨ç†åŠ é€Ÿã€‚
2. âœ… **å› æœæ‰©æ•£æ˜¯å¯è¡Œä¸”é«˜æ•ˆçš„**ï¼šé€šè¿‡ Soft Tail Masking å’Œ Context-aware Reweightingï¼Œæœ‰æ•ˆç¼“è§£äº†â€œä¿¡æ¯åå¡Œâ€é—®é¢˜ï¼Œä½¿çº¯å› æœç»“æ„ä¸‹çš„æ‰©æ•£è®­ç»ƒå˜å¾—ç¨³å®šã€‚
3. âœ… **æ›´é«˜çš„æ•°æ®æ½œåŠ›ï¼ˆData Potentialï¼‰**ï¼šåœ¨å¤šè½®è®­ç»ƒåœºæ™¯ä¸‹ï¼ŒCARD ä¸åƒ ARM é‚£æ ·å¿«é€Ÿé¥±å’Œï¼Œè€Œæ˜¯æŒç»­ä»é‡å¤æ•°æ®ä¸­æå–ä¿¡å·ï¼Œåœ¨æ•°æ®å—é™åœºæ™¯æ›´å…·ä¼˜åŠ¿ï¼ˆè§ Figure 4ï¼‰ã€‚
4. âœ… **æ›´å¼ºçš„è¯­è¨€å»ºæ¨¡æ³›åŒ–èƒ½åŠ›**ï¼šç”±äºè®­ç»ƒç›®æ ‡è¦æ±‚ä»æŸåä¸Šä¸‹æ–‡ä¸­é‡å»º tokenï¼Œä¿ƒä½¿æ¨¡å‹æ•æ‰å…¨å±€ç»“æ„è€Œéå±€éƒ¨ç»Ÿè®¡æ¨¡å¼ï¼Œå› æ­¤åœ¨é•¿ç¨‹ä¾èµ–ä»»åŠ¡ä¸Šè¶…è¶Š ARMã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æç«¯åŠ é€Ÿè®¾ç½®å¯èƒ½å¯¼è‡´é€€åŒ–è¡Œä¸º**ï¼šå½“ block size è¿‡å¤§ä¸” step limit è¿‡å°æ—¶ï¼Œæ¨¡å‹æ˜“é™·å…¥é‡å¤å¾ªç¯ï¼ˆå¦‚â€œapplying the gel...â€åå¤å‡ºç°ï¼‰ï¼Œéœ€è°¨æ…è°ƒèŠ‚æ¨ç†å‚æ•°ã€‚
- **å½“å‰å®ç°ä»ä¾èµ–ç»éªŒæ€§è¶…å‚è®¾è®¡**ï¼šå¦‚ tail factor $\lambda$ã€decay $p$ã€threshold $\tau$ ç­‰å°šæ— ç†è®ºæœ€ä¼˜é€‰æ‹©ã€‚
- **æœªæ¢ç´¢æ›´å¤§è§„æ¨¡ä¸‹çš„æ‰©å±•è§„å¾‹**ï¼šç›®å‰å®éªŒé›†ä¸­åœ¨ 1B æ¨¡å‹ï¼Œå°šæœªéªŒè¯åœ¨ 10B+ è§„æ¨¡çš„è¡¨ç°ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **å¯å­¦ä¹ çš„å™ªå£°è°ƒåº¦ç­–ç•¥**ï¼ˆadaptive noise placementï¼‰
- å¼•å…¥ **è¯¾ç¨‹å­¦ä¹ æœºåˆ¶** æ§åˆ¶è®­ç»ƒéš¾åº¦æ›²çº¿
- æ‰©å±•è‡³ **å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡**ï¼ˆå¦‚å›¾æ–‡ç”Ÿæˆï¼‰
- ç ”ç©¶ **æ›´é²æ£’çš„ confidence scoring å‡½æ•°** ä»¥è¿›ä¸€æ­¥æå‡å¹¶è¡Œè§£ç è´¨é‡
- æ¢ç´¢ **å®Œå…¨å…è®­ç»ƒåŠ é€Ÿæ–¹æ¡ˆ**ï¼ˆtraining-free accelerationï¼‰ä¸ CARD çš„ç»“åˆå¯èƒ½æ€§

---

> ğŸ”š **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **CARD æ˜¯é¦–ä¸ªçœŸæ­£æ„ä¹‰ä¸Šå°†å› æœè‡ªå›å½’è®­ç»ƒæ•ˆç‡ä¸æ‰©æ•£æ¨¡å‹å¹¶è¡Œæ¨ç†ä¼˜åŠ¿ç»Ÿä¸€èµ·æ¥çš„æ¡†æ¶ï¼Œåœ¨æ€§èƒ½ã€æ•ˆç‡ä¸æ•°æ®åˆ©ç”¨ä¹‹é—´è¾¾åˆ°äº†æ–°çš„å¹³è¡¡ï¼Œä¸ºä¸‹ä¸€ä»£é«˜æ•ˆ LLM æä¾›äº†ä¸€æ¡åšå®è·¯å¾„ã€‚**

</details>

---

### 2. [Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts](https://arxiv.org/abs/2601.22156)

**Authors**: Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2601.22156v1  

#### Abstract
Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratc...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šHybrid Linear Attention Done Right**

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å½“å‰ä¸»æµçš„ **Transformer** æ¨¡å‹ä¾èµ– **softmax attention**ï¼Œå…¶è®¡ç®—å¤æ‚åº¦ä¸º $O(T^2)$ï¼Œåœ¨å¤„ç† **é•¿ä¸Šä¸‹æ–‡ï¼ˆlong-contextï¼‰** æ—¶æ•ˆç‡æä½ã€‚è™½ç„¶ **RNN-attention hybrid models**ï¼ˆå¦‚ Mambaã€Jambaï¼‰é€šè¿‡æ··åˆæ¶æ„å®ç°äº†çº¿æ€§å¤æ‚åº¦ï¼Œåœ¨é•¿åºåˆ—ä¸Šæ¨ç†æ›´å¿«ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦ä»å¤´å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å­¦æœ¯ç•Œçš„ç ”ç©¶ã€‚

æ­¤å¤–ï¼Œç°æœ‰çš„å°† **Transformer è’¸é¦ä¸º hybrid æ¨¡å‹** çš„æ–¹æ³•å­˜åœ¨ä¸¤å¤§ç¼ºé™·ï¼š
1. **æ•°æ®éœ€æ±‚å·¨å¤§**ï¼šå¤šæ•°æ–¹æ³•éœ€æ•°åäº¿ç”šè‡³æ•°ç™¾äº¿ tokens çš„è®­ç»ƒæ•°æ®ï¼ˆå¦‚ Jet-Nemotron éœ€ 400B tokensï¼‰ã€‚
2. **é•¿ä¸Šä¸‹æ–‡æ€§èƒ½å·®**ï¼šè’¸é¦åçš„æ¨¡å‹åœ¨çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°å°šå¯ï¼Œä½†åœ¨é•¿ä¸Šä¸‹æ–‡ï¼ˆå¦‚ >128Kï¼‰çš„ **recall-intensive ä»»åŠ¡** ä¸Šæ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹**

æœ¬æ–‡æå‡ºäº† **HALO**ï¼ˆHybrid Attention via Layer Optimizationï¼‰å’Œ **HypeNet**ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚

#### **(1) HALOï¼šé«˜æ•ˆè·¨æ¶æ„è’¸é¦æµç¨‹**
- **ç›®æ ‡**ï¼šå°†å·²æœ‰çš„ **Transformer æ¨¡å‹**ï¼ˆå¦‚ Qwen3ï¼‰é«˜æ•ˆè’¸é¦ä¸º **RNN-attention hybrid æ¨¡å‹**ã€‚
- **æ ¸å¿ƒæ­¥éª¤**ï¼š
  1. **Attention Weight Transfer**ï¼šå°†æ¯ä¸ª attention å±‚çš„æƒé‡åˆå§‹åŒ–ä¸ºå¯¹åº”çš„ RNN å±‚ã€‚
  2. **Hidden State Alignment**ï¼šç‹¬ç«‹è®­ç»ƒæ¯ä¸ª RNN å±‚ï¼Œä½¿å…¶è¾“å‡ºå¯¹é½åŸ attention å±‚ã€‚
  3. **Attention Layer Selection**ï¼šæå‡ºåŸºäº **recall vs. CSR æ€§èƒ½å˜åŒ–** çš„é‡è¦æ€§è¯„åˆ†æœºåˆ¶ï¼Œé€‰æ‹©æœ€å…³é”®çš„ attention å±‚ä¿ç•™ã€‚
  4. **Knowledge Distillation**ï¼šç«¯åˆ°ç«¯è’¸é¦ï¼Œä»¥åŸå§‹ Transformer ä¸º teacherï¼Œhybrid æ¨¡å‹ä¸º studentã€‚
  5. **Finetuning**ï¼šåœ¨æ›´é•¿ä¸Šä¸‹æ–‡ä¸Šå¾®è°ƒä»¥ä¼˜åŒ–é•¿ç¨‹èƒ½åŠ›ã€‚

> âœ… **ä¼˜åŠ¿**ï¼šä»…éœ€ **2.3B tokens** å³å¯å®Œæˆè½¬æ¢ï¼Œè¿œä½äºç°æœ‰æ–¹æ³•ï¼ˆå¦‚ KL-LS éœ€ 25Bï¼ŒJet-Nemotron éœ€ 400Bï¼‰ã€‚

#### **(2) HypeNetï¼šé«˜æ€§èƒ½ hybrid æ¶æ„**
- **æ ¸å¿ƒåˆ›æ–°**ï¼š
  - **HyPE**ï¼ˆHybrid Positional Encodingï¼‰ï¼š
    - **RNN å±‚ç”¨ RoPE**ï¼ˆæ³¨å…¥ä¸°å¯Œä½ç½®ä¿¡æ¯ï¼‰
    - **Attention å±‚ç”¨ NoPE**ï¼ˆæå‡é•¿åº¦å¤–æ¨èƒ½åŠ›ï¼‰
    - ç»“åˆä¸¤è€…ä¼˜åŠ¿ï¼Œå®ç°æ›´å¼ºçš„ **length generalization**ã€‚
  - **Attention Logits Scaling**ï¼šå¼•å…¥ä½ç½®ç›¸å…³çš„ç¼©æ”¾å› å­ $s_t = \log(t + a)$ï¼Œç¼“è§£æ³¨æ„åŠ›ç†µéšé•¿åº¦å¢é•¿çš„é—®é¢˜ã€‚
  - **å…¶ä»–æ¶æ„æ”¹è¿›**ï¼š
    - **QK-Normalization** åœ¨ RNN å±‚ä¸­åŠ å…¥ã€‚
    - **GQA to MHA**ï¼šè§£é™¤ RNN å±‚ä¸­ K/V å…±äº«ï¼Œå¢å¼ºè¡¨è¾¾åŠ›ã€‚
    - **Output Gate**ï¼šåœ¨ RNN å’Œ attention å±‚å‡åŠ å…¥é—¨æ§æœºåˆ¶ï¼Œæå‡å»ºæ¨¡èƒ½åŠ›ã€‚

> âœ… **ä¼˜åŠ¿**ï¼šHypeNet åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ hybrid æ¨¡å‹ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| æ–¹æ³• | æ‰€éœ€ Tokens | æ˜¯å¦å¼€æºæ•°æ® | é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ | æ•ˆç‡ |
|------|-------------|---------------|----------------|--------|
| Jet-Nemotron (Gu et al., 2025) | 400B | å¦ | å·®ï¼ˆNIAH@256K â‰ˆ 0%ï¼‰ | ä¸­ç­‰ |
| KL-LS (Li et al., 2025) | 25B | å¦ | ä¸­ç­‰ï¼ˆNIAH@256K â‰ˆ 44.3%ï¼‰ | ä¸­ç­‰ |
| **HALO + HypeNet (Ours)** | **2.3B** | **æ˜¯ï¼ˆFineWeb-eduï¼‰** | **ä¼˜ï¼ˆNIAH@256K â‰ˆ 74.3%ï¼‰** | **é«˜ï¼ˆ3.4Ã— prefill speedupï¼‰** |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **è®­ç»ƒæ•°æ®**ï¼š`FineWeb-edu`ï¼ˆé«˜è´¨é‡äº’è”ç½‘æ–‡æœ¬å­é›†ï¼‰ï¼Œä»ä¸­éšæœºé‡‡æ ·ï¼š
  - Stage 1: 320M tokens
  - Stage 2: 1B tokens
  - Stage 3: 1B tokensï¼ˆé•¿ä¸Šä¸‹æ–‡å¾®è°ƒï¼‰
  - **æ€»è®¡ï¼š2.3B tokens**
- **å±‚é€‰æ‹©è¯„ä¼°æ•°æ®**ï¼šç”¨äºè®¡ç®— attention å±‚é‡è¦æ€§çš„ä»»åŠ¡é›†åˆï¼š
  - **CSR**ï¼šHellaSwag, ARC-Easy, ARC-Challenge
  - **Recall**ï¼šSQuAD, FDA, SWDE

### **å®éªŒè®¾ç½®**
- **åŸºç¡€æ¨¡å‹**ï¼šQwen3-1.7B, Qwen3-4B, Qwen3-8B
- **è½¬æ¢åæ¨¡å‹**ï¼šHypeNet-2B, HypeNet-5B, HypeNet-9Bï¼ˆå‚æ•°ç•¥å¢ ~10%ï¼‰
- **RNN Mixer**ï¼šé»˜è®¤ä½¿ç”¨ **Lightning Attention**ï¼Œä¹Ÿæµ‹è¯•äº† Mamba2ã€GLAã€GDNã€RWKV-7
- **ç¡¬ä»¶**ï¼šå•å¼  NVIDIA A800 GPUï¼ŒBFloat16 ç²¾åº¦
- **Batch Size**ï¼š1ï¼ˆå…¬å¹³æ¯”è¾ƒååé‡ï¼‰

### **è¯„ä¼°æŒ‡æ ‡**
- **Commonsense Reasoning (CSR)**ï¼š
  - MMLU, HellaSwag, ARC-Easy/Challenge, WinoGrande, PIQA, LAMBADA
  - æŠ¥å‘Š **normalized accuracy**
- **é•¿ä¸Šä¸‹æ–‡å¬å›èƒ½åŠ›**ï¼š
  - **Needle-in-a-Haystack (NIAH)**ï¼šåœ¨è¶…é•¿æ–‡æœ¬ä¸­å®šä½å…³é”®ä¿¡æ¯ï¼Œæµ‹è¯•é•¿åº¦ä» 4K åˆ° 1Mã€‚
- **æ•ˆç‡æŒ‡æ ‡**ï¼š
  - **Throughput (tokens/s)**ï¼šè§£ç é€Ÿåº¦
  - **Time per Output Token (TPOT)**ï¼šç”Ÿæˆå»¶è¿Ÿ
  - **Prefill Time**ï¼šå¤„ç†é•¿è¾“å…¥çš„æ—¶é—´

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **Teacher Model**ï¼šQwen3ï¼ˆåŸç”Ÿ Transformerï¼‰
- **SOTA è’¸é¦æ–¹æ³•**ï¼š
  - Jet-Nemotron (Gu et al., 2025)
  - KL-LS (Li et al., 2025)
  - RADLADS (Goldstein et al., 2025)
- **å…¶ä»– baseline**ï¼š
  - å‡åŒ€åˆ†å¸ƒ attention å±‚
  - ååŠéƒ¨åˆ†é›†ä¸­ attention å±‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆQwen3-1.7B â†’ HypeNet-2Bï¼‰**

#### **è¡¨ï¼šNIAH é•¿ä¸Šä¸‹æ–‡å¬å›æ€§èƒ½å¯¹æ¯”ï¼ˆæœ€é«˜å€¼åŠ ç²—ï¼‰**

| Model | NIAH @ 128K | NIAH @ 256K |
|-------|------------|------------|
| Qwen3 (teacher) | 96.4 | 17.0 |
| Jet-Nemotron | 14.8 | 0.0 |
| KL-LS (GDN) | 24.8 | 11.0 |
| **HypeNet + HALO (ours)** | **79.9** | **74.3** |

> ğŸ“Œ **ç»“è®º**ï¼šHypeNet åœ¨ 256K ä¸Šä»ä¿æŒ **74.3% å‡†ç¡®ç‡**ï¼Œè€Œ Qwen3 ä»…ä¸º 17%ï¼Œå…¶ä»–è’¸é¦æ–¹æ³•å‡ ä¹å¤±æ•ˆã€‚

#### **æ•ˆç‡å¯¹æ¯”ï¼ˆå›¾1 & å›¾6ï¼‰**
- **å†…å­˜å ç”¨æ›´ä½**ï¼šHypeNet åœ¨ 1M context ä¸‹ä»å¯è¿è¡Œï¼ŒQwen3 å†…å­˜æº¢å‡ºã€‚
- **ååé‡æ›´é«˜**ï¼š
  - **è§£ç é€Ÿåº¦æå‡ up to 3.0Ã—**
  - **prefill é€Ÿåº¦æå‡ up to 3.4Ã—**ï¼ˆ512K contextï¼‰
- **TPOT æ›´ä½**ï¼šéšç€ context å¢é•¿ï¼ŒHypeNet å¢é•¿ç¼“æ…¢ï¼ŒQwen3 å¢é•¿å‰§çƒˆã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

#### **(1) HypeNet æ¶æ„æ¶ˆèï¼ˆTable 3ï¼‰**
ç§»é™¤ä»»ä¸€ç»„ä»¶å‡å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶ï¼š
- ç§»é™¤ **RNN RoPE**ï¼ˆå³ä¸ç”¨ HyPEï¼‰ï¼šNIAH@128K ä» 79.9 â†’ 47.9
- ç§»é™¤ **RNN QK-Norm**ï¼šNIAH@128K â†’ 17.3
- ç§»é™¤ **RNN Output Gate**ï¼šNIAH@128K â†’ 74.5

> âœ… **éªŒè¯äº† HyPE å’Œå„æ¨¡å—çš„æœ‰æ•ˆæ€§**ã€‚

#### **(2) Layer Selection æ–¹æ³•å¯¹æ¯”ï¼ˆTable 4ï¼‰**
| Method | CSR Score | NIAH @ 128K |
|--------|-----------|-------------|
| Evenly distribute | 54.0 | 61.9 |
| Latter-half only | 55.8 | 39.2 |
| KL-LS | 55.3 | 58.3 |
| **HALO (ours)** | **55.9** | **79.9** |

> âœ… **è¯æ˜æ‰€æ selection æ–¹æ³•æœ€ä¼˜**ã€‚

#### **(3) ä¸åŒ RNN Mixer å¯¹æ¯”ï¼ˆFigure 5ï¼‰**
- **Lightning Attention** è¡¨ç°æœ€ä½³ï¼Œä¼˜äº Mamba2ã€GLAã€GDNã€‚
- å¯èƒ½åŸå› ï¼šå…¶ **data-independent forget gate** æ›´åˆ©äºé•¿åº¦æ³›åŒ–ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **HALO æ˜¯ç›®å‰æœ€é«˜æ•ˆçš„ Transformer-to-Hybrid è’¸é¦æ–¹æ³•**ï¼Œä»…éœ€ **2.3B tokens** å³å¯å®Œæˆè½¬æ¢ã€‚
2. **HyPE æ˜¯å…³é”®åˆ›æ–°**ï¼šRNN ç”¨ RoPE + Attention ç”¨ NoPE çš„ç»„åˆæ˜¾è‘—æå‡ **length generalization**ã€‚
3. **Lightning Attention æ˜¯æœ€é€‚åˆ HypeNet çš„ RNN mixer**ï¼Œç®€å•ä¸”é«˜æ•ˆã€‚
4. **HypeNet åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹å…¼å…·é«˜æ€§èƒ½ä¸é«˜æ•ˆç‡**ï¼š
   - æ€§èƒ½æ¥è¿‘åŸç”Ÿ Transformer
   - æ¨ç†é€Ÿåº¦æ˜¾è‘—æ›´å¿«
   - å†…å­˜å ç”¨æ›´ä½

---

### **å±€é™æ€§**
1. **æŒ‡ä»¤è·Ÿéšèƒ½åŠ›å¯èƒ½é€€åŒ–**ï¼šç”±äºè½¬æ¢è¿‡ç¨‹æœªä½¿ç”¨ instruction tuning æ•°æ®ï¼Œæ¨¡å‹çš„å¯¹é½è¡Œä¸ºå¯èƒ½ä¸å¦‚åŸæ¨¡å‹ã€‚
2. **ä»…é€‚ç”¨äº Transformer æ¶æ„**ï¼šå½“å‰æµç¨‹é’ˆå¯¹ Transformer è®¾è®¡ï¼Œéš¾ä»¥ç›´æ¥è¿ç§»åˆ°å…¶ä»–æ¶æ„ã€‚
3. **ä¸æ”¯æŒ Short Convolution**ï¼šå®éªŒå‘ç°æ·»åŠ  short conv å±‚æ— ç›Šç”šè‡³æœ‰å®³ï¼Œé™åˆ¶äº†æŸäº› RNN å˜ä½“çš„åº”ç”¨ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **æ¢å¤å¯¹é½èƒ½åŠ›**ï¼šæ¢ç´¢å¦‚ä½•åœ¨è’¸é¦åé«˜æ•ˆæ¢å¤ instruction-following å’Œ safety behaviorã€‚
2. **æ‰©å±•è‡³å¤šæ¨¡æ€æ¨¡å‹**ï¼šå°† HALO åº”ç”¨äº vision-language æ¨¡å‹ã€‚
3. **è‡ªåŠ¨åŒ– layer selection**ï¼šè®¾è®¡æ›´æ™ºèƒ½çš„ç®—æ³•åŠ¨æ€å†³å®šæ¯å±‚æ˜¯å¦ä½¿ç”¨ attentionã€‚
4. **æ¢ç´¢æ›´ä¼˜çš„ RNN mixer**ï¼šç»“åˆ state space models ä¸ linear attention çš„ä¼˜åŠ¿ã€‚

---

> ğŸ”— **ä»£ç ä¸æ¨¡å‹**ï¼šhttps://github.com/THUNLP/hybrid-linear-attention

</details>

---

### 3. [Amortized Spectral Kernel Discovery via Prior-Data Fitted Network](https://arxiv.org/abs/2601.21731)

**Authors**: Kaustubh Sharma, Srijan Tiwari, Ojasva Nema, Parikshit Pareek  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2601.21731v1  

#### Abstract
Prior-Data Fitted Networks (PFNs) enable efficient amortized inference but lack transparent access to their learned priors and kernels. This opacity hinders their use in downstream tasks, such as surrogate-based optimization, that require explicit covariance models. We introduce an interpretability-...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šAmortized Spectral Kernel Discovery via Prior-Data Fitted Network**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
Prior-Data Fitted Networks (PFNs) è™½ç„¶åœ¨**amortized Bayesian inference**ï¼ˆæ‘Šé”€è´å¶æ–¯æ¨æ–­ï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­å®ç°å¿«é€Ÿé¢„æµ‹ï¼Œä½†å…¶å†…éƒ¨å­¦åˆ°çš„å…ˆéªŒï¼ˆå¦‚ spectral density å’Œ covariance kernelï¼‰æ˜¯éšå¼çš„ã€ä¸å¯è§£é‡Šçš„ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨éœ€è¦æ˜¾å¼åæ–¹å·®æ¨¡å‹çš„ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä¾‹å¦‚ surrogate-based optimization æˆ–ç§‘å­¦å»ºæ¨¡ã€‚

æœ¬æ–‡æ—¨åœ¨è§£å†³ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š
- **æœºåˆ¶é€æ˜æ€§ç¼ºå¤±**ï¼šPFNs å¦‚ä½•ç¼–ç é¢‘è°±ä¿¡æ¯ï¼Ÿè¿™äº›ä¿¡æ¯æ˜¯å¦å¯è¢«æå–ï¼Ÿ
- **ç¼ºä¹æ˜¾å¼å¯è¿ç§»è¡¨ç¤º**ï¼šèƒ½å¦ä»é¢„è®­ç»ƒ PFN ä¸­è§£ç å‡ºæ˜¾å¼çš„ spectral density å’Œ stationary kernelï¼Ÿ

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
ä½œè€…æå‡ºäº†ä¸€ç§**åŸºäºæœºåˆ¶åˆ†æçš„è§£ç æ¡†æ¶**ï¼Œç”¨äºä»é¢„è®­ç»ƒçš„ PFN ä¸­**æ‘Šé”€åœ°ï¼ˆamortizedï¼‰å‘ç°é¢‘è°±ç»“æ„**ï¼Œå¹¶æ„å»ºæ˜¾å¼çš„ stationary kernelã€‚æ ¸å¿ƒæ€æƒ³å¦‚ä¸‹ï¼š

1. **æœºåˆ¶æ€§æ´å¯Ÿï¼ˆMechanistic Insightï¼‰**  
   åœ¨é‡‡ç”¨ **Decoupled-Value Attention (DVA)** æ¶æ„çš„ PFN ä¸­ï¼Œæ³¨æ„åŠ›è¾“å‡º $ H $ ç¼–ç äº†è¾“å…¥å‡½æ•°çš„é¢‘è°±ç»“æ„ï¼Œè€Œå€¼ç¼–ç  $ V $ ä¸»è¦æºå¸¦ä¿¡å·å¹…åº¦ä¿¡æ¯ã€‚é€šè¿‡ t-SNE å¯è§†åŒ–å‘ç°ï¼Œ$ H $ å½¢æˆäº†ä¸€ä¸ªä¸é¢‘ç‡å¼ºç›¸å…³çš„å¹³æ»‘æµå½¢ï¼ˆmanifoldï¼‰ï¼Œç›¸å…³ç³»æ•°è¾¾ 0.864ã€‚

2. **Filter Bank Decoder æ¶æ„**  
   è®¾è®¡äº†ä¸€ä¸ªåŒè·¯å¾„è§£ç å™¨ï¼Œå°†å†»ç»“çš„ PFN è¾“å‡ºæ˜ å°„ä¸ºæ˜¾å¼çš„ spectral densityï¼š
   - åˆ©ç”¨ **Multi-Query Attention (MQA) pooling** æå– $ H $ å’Œ $ V $ çš„å¤šè§†è§’æ‘˜è¦ã€‚
   - é‡‡ç”¨â€œæ»¤æ³¢å™¨ç»„â€ï¼ˆfilter bankï¼‰å½¢å¼ï¼Œåœ¨ç¦»æ•£é¢‘ç‡åŸŸä¸Šé¢„æµ‹ï¼š
     - é¢‘ç‡ bin æ˜¯å¦æ¿€æ´»ï¼ˆBin Classifierï¼‰
     - ä¸­å¿ƒé¢‘ç‡åç§»ä¸å¸¦å®½ï¼ˆOffset-Bandwidth Regressorï¼‰
     - å¤š realization ä¸‹çš„ç›¸å¯¹æƒé‡ï¼ˆWeight Regressorï¼‰

3. **Bochnerâ€™s Theorem æ˜¾å¼æ ¸é‡å»º**  
   å°†è§£ç å¾—åˆ°çš„ spectral density é€šè¿‡å‚…é‡Œå¶é€†å˜æ¢ï¼ˆBochner å®šç†ï¼‰è½¬æ¢ä¸º stationary kernelï¼š
   $$
   k(\tau) = \sum_b [\mathbb{I}(p_b > \gamma)] w_b \exp(-2\pi^2 \sigma_b^2 \tau^2) \cos(2\pi \mu_b \tau)
   $$

4. **Analytical Scaling è§£å†³å°ºåº¦æ¨¡ç³Š**  
   åœ¨ single-realization åœºæ™¯ä¸‹ï¼Œç”±äºå½’ä¸€åŒ–å¯¼è‡´å…¨å±€å°ºåº¦ä¸å¯è¯†åˆ«ï¼Œå¼•å…¥è§£æä¼°è®¡å™¨ï¼š
   $$
   \hat{\alpha} = \|f\|^2 / \text{tr}(K)
   $$
   æ¥æ¢å¤èƒ½é‡ä¸€è‡´æ€§ï¼Œé¿å…ç½‘ç»œâ€œå¹»è§‰â€å°ºåº¦ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| æ–¹æ³• | æ˜¯å¦éœ€è¿­ä»£ä¼˜åŒ– | æ˜¯å¦è¾“å‡ºæ˜¾å¼ kernel | æ¨ç†é€Ÿåº¦ | å¯è§£é‡Šæ€§ |
|------|----------------|--------------------|----------|----------|
| **DKL / RFF** | æ˜¯ï¼ˆæ¯ä»»åŠ¡ä¼˜åŒ–ï¼‰ | å¦ï¼ˆéšå¼ï¼‰ | æ…¢ï¼ˆç§’çº§ï¼‰ | ä½ |
| **Automatic Statistician** | æ˜¯ï¼ˆç»„åˆæœç´¢ï¼‰ | æ˜¯ | ææ…¢ | é«˜ |
| **PFNï¼ˆåŸç”Ÿï¼‰** | å¦ | å¦ | å¿«ï¼ˆ~10â»Â³ ç§’ï¼‰ | æ—  |
| **æœ¬æ–‡æ–¹æ³•ï¼ˆDecoder + PFNï¼‰** | **å¦** | **æ˜¯** | **å¿«ï¼ˆ~10â»Â³ ç§’ï¼‰** | **é«˜** |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼š
> - **é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰é¢‘è°±æ¨æ–­**ï¼šæ— éœ€æµ‹è¯•æ—¶ä¼˜åŒ–ã€‚
> - **æ˜¾å¼å¯è§£é‡Š kernel**ï¼šæ”¯æŒä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ä¼˜åŒ–ã€æ§åˆ¶ï¼‰ã€‚
> - **æ¨ç†åŠ é€Ÿä¸‰ä¸ªæ•°é‡çº§**ï¼šç›¸è¾ƒ DKL/RFF ä»ç§’çº§é™è‡³æ¯«ç§’çº§ã€‚
> - **ç†è®ºä¿è¯**ï¼šmulti-realization ä¸‹ spectral weights å¯è¯†åˆ«ï¼›single-realization ä¸‹ shape å¯æ¢å¤ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
å…¨éƒ¨ä¸º**åˆæˆæ•°æ®é›†**ï¼Œç”± GP é‡‡æ ·ç”Ÿæˆï¼Œæ¶µç›–å¤šç§ kernel ç±»å‹ï¼š

- **Spectral Mixture (SM)**ï¼šå« 1â€“4 ä¸ªé«˜æ–¯åˆ†é‡ï¼Œä¸­å¿ƒé¢‘ç‡ $ \mu \sim U[0.5, 3.0] $ï¼Œå¸¦å®½ $ \sigma \sim U[0.01, 0.05] $
- **æ ‡å‡† kernel family**ï¼šRBFã€Periodicã€RBF+Periodicã€RBFÃ—Periodic
- **é«˜ç»´æ‰©å±•**ï¼š5D å’Œ 10D additive kernelsï¼ˆä»…å°‘æ•°ç»´åº¦æ´»è·ƒï¼‰
- **out-of-distribution**ï¼šMatÃ©rn kernelsï¼ˆheavy-tailed spectral densityï¼‰

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### **è¯„ä¼°åœºæ™¯**
| åœºæ™¯ | è¾“å…¥ | è¾“å‡ºç›®æ ‡ |
|------|------|-----------|
| **Multi-Realization** | $ M=16 $ ä¸ªç‹¬ç«‹å‡½æ•°æ ·æœ¬ | å®Œæ•´ spectral densityï¼ˆé¢‘ç‡ã€å¸¦å®½ã€æƒé‡ï¼‰ |
| **Single-Realization** | å•ä¸ªå‡½æ•°æ ·æœ¬ | é¢‘ç‡ä¸å¸¦å®½ï¼ˆæƒé‡è®¾ä¸ºå‡åŒ€ï¼‰+ è§£æç¼©æ”¾ |

#### **è¯„ä¼°æŒ‡æ ‡**
- **Spectral Recovery**ï¼š
  - Wasserstein è·ç¦»ï¼ˆé¢‘è°±å¯†åº¦åŒ¹é…åº¦ï¼‰
- **Kernel Quality**ï¼š
  - GP å›å½’ MSEï¼ˆä½¿ç”¨è§£ç  kernel ä½œä¸º covarianceï¼‰
  - ä¸ Oracle GPã€PFNã€DKLã€RFF å¯¹æ¯”
- **æ•ˆç‡**ï¼š
  - æ¨ç†æ—¶é—´ï¼ˆsecondsï¼‰

#### **è®­ç»ƒç­–ç•¥**
- **Curriculum Learning**ï¼šä»ç®€å•ï¼ˆ1 åˆ†é‡ï¼‰åˆ°å¤æ‚ï¼ˆ4 åˆ†é‡ï¼‰é€æ­¥è®­ç»ƒ
- **æŸå¤±å‡½æ•°**ï¼š
  - Bin Classificationï¼šBinary Cross-Entropyï¼ˆæ­£æ ·æœ¬åŠ æƒï¼‰
  - Parameter Regressionï¼šMasked MSEï¼ˆä»…å¯¹æ¿€æ´» bin è®¡ç®—ï¼‰

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| åŸºçº¿æ–¹æ³• | æè¿° |
|---------|------|
| **Oracle GP** | ä½¿ç”¨çœŸå®ç”Ÿæˆ kernel çš„ GP å›å½’ï¼ˆç†æƒ³ä¸Šé™ï¼‰ |
| **PFN** | åŸå§‹ PFN æ¨¡å‹ç›´æ¥é¢„æµ‹ï¼ˆé»‘ç›’ï¼‰ |
| **DKL (Deep Kernel Learning)** | ç¥ç»ç½‘ç»œ + base kernelï¼Œéœ€ marginal likelihood ä¼˜åŒ– |
| **RFF (Random Fourier Features)** | å›ºå®šé¢‘è°±é‡‡æ ·ï¼Œéœ€ä¼˜åŒ–ç‰¹å¾æƒé‡ |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **Multi-Realization Setting**
- **å›¾3**ï¼šéšç€å‡½æ•°æ ·æœ¬æ•° $ M $ å¢åŠ ï¼ŒWasserstein è·ç¦»å•è°ƒä¸‹é™ï¼ŒéªŒè¯äº† Theorem 4.3 çš„ identifiabilityã€‚
- **å›¾4 & è¡¨6**ï¼šDecoder æˆåŠŸæ¢å¤å¤šå³° spectral mixtureï¼Œå³ä½¿åœ¨é‡å é¢‘ç‡ä¸‹ä¹Ÿèƒ½å‡†ç¡®å®šä½å³°å€¼ã€‚
- **è¡¨6**ï¼šåœ¨ in-distributionï¼ˆSMï¼‰ä»»åŠ¡ä¸Šï¼ŒDecoder MSE ä¸ Oracle GP åŒé˜¶ï¼ˆ~10â»â´ï¼‰ï¼Œæ˜¾è‘—ä¼˜äº DKL/RFFã€‚

#### **Single-Realization Settingï¼ˆKernel Cookbookï¼‰**
| True Kernel | Decoder MSE | PFN MSE | DKL MSE | RFF MSE |
|------------|-------------|--------|--------|--------|
| RBF        | 2.52e-4     | 6.56e-5 | 2.52e-4 | 4.33e-2 |
| Periodic   | 2.78e-2     | 1.01e-2 | 3.80e-2 | 5.51e-2 |
| SM(Q=4)    | 2.54e-3     | 2.46e-4 | 2.70e-2 | 5.72e-2 |

> ğŸ”¹ Decoder æ€§èƒ½æ¥è¿‘ PFNï¼Œä¸”æ˜¾è‘—ä¼˜äº DKL å’Œ RFFï¼Œå°¤å…¶åœ¨å¤æ‚ kernel ä¸Šã€‚
> ğŸ”¹ æ‰€æœ‰ä»»åŠ¡æ¨ç†æ—¶é—´ï¼š< 0.006 ç§’ vs. DKL/RFF çš„ ~2 ç§’ â†’ **~1000Ã— åŠ é€Ÿ**

#### **é«˜ç»´æ‰©å±•ï¼ˆ5D & 10Dï¼‰**
| Kernel (Additive) | Oracle GP MSE | Amortized MSE |
|-------------------|---------------|----------------|
| RBF(5D)           | 1.3e-4        | 2.9e-4         |
| SM(Q=4,10D)       | 2.06e-4       | 4.56e-4        |
| Periodic(10D)     | 2.71e-4       | 2.32e-1        |

> ğŸ”¹ åœ¨ additive kernels ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè¯¯å·®åŒé˜¶ã€‚
> ğŸ”¹ Periodic kernel æŒ‘æˆ˜è¾ƒå¤§ï¼ˆéœ€å…¨å±€ç»“æ„å»ºæ¨¡ï¼‰ï¼Œè¯¯å·®é«˜å‡ºçº¦ 3 ä¸ªæ•°é‡çº§ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

#### **Pooling æ–¹å¼å¯¹æ¯”ï¼ˆTable 4, Figure 9ï¼‰**
| Pooling Method | Easy (1 param) | Hard (4 params) | Very Hard (6 params) |
|----------------|----------------|------------------|------------------------|
| Mean Pooling   | 0.999          | 0.560            | 0.333                  |
| Attention Pooling | 1.000        | 0.610            | 0.404                  |

> âœ… Attention pooling åœ¨å¤æ‚ä»»åŠ¡ä¸­å¸¦æ¥ +3.8% ~ +5.8% æå‡ï¼Œè¯æ˜å…¶å¯¹å¤šæˆåˆ†åˆ†ç¦»çš„æœ‰æ•ˆæ€§ã€‚

#### **Representation æ¢æµ‹ï¼ˆTable 3, Figure 7â€“8ï¼‰**
- $ H $ï¼ˆlatent outputï¼‰åœ¨é¢‘ç‡æ¢æµ‹ä¸Š RÂ² è¾¾ 0.998ï¼Œè¿œè¶… $ V $ï¼ˆ0.21ï¼‰
- $ H+V $ è”åˆä½¿ç”¨è¿›ä¸€æ­¥æå‡æƒé‡é¢„æµ‹èƒ½åŠ›
- éªŒè¯äº† $ H $ æ˜¯é¢‘è°±ä¿¡æ¯çš„ä¸»è¦è½½ä½“

#### **ç›¸ä½ä¸å˜æ€§åˆ†æï¼ˆFigure 10ï¼‰**
- $ V $ å¯¹ç›¸ä½å˜åŒ–æ•æ„Ÿï¼Œ$ H $ æ›´ç¨³å®š
- å»ºè®®è”åˆä½¿ç”¨ $ H $ å’Œ $ V $ ä»¥å¢å¼ºé²æ£’æ€§

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **PFNs éšå¼æ‰§è¡Œé¢‘è°±ä¼°è®¡**  
   DVA-PFN çš„ attention è¾“å‡º $ H $ ç¼–ç äº†è¾“å…¥å‡½æ•°çš„é¢‘è°±ç»“æ„ï¼Œå½¢æˆå¯è§£é‡Šçš„ manifoldã€‚

2. **æ‘Šé”€é¢‘è°±å‘ç°æ˜¯å¯è¡Œçš„**  
   é€šè¿‡è®¾è®¡ decoderï¼Œå¯ä»¥ä»å†»ç»“ PFN ä¸­é«˜æ•ˆæå–æ˜¾å¼ spectral density å’Œ stationary kernelã€‚

3. **multi-realization ä¸‹ spectral weights å¯è¯†åˆ«**  
   ç†è®ºè¯æ˜å¹¶å®éªŒéªŒè¯ï¼šå¤šä¸ªç‹¬ç«‹æ ·æœ¬å…è®¸ä¸€è‡´ä¼°è®¡ second-order statisticsï¼Œä»è€Œæ¢å¤å®Œæ•´ spectral mixtureã€‚

4. **single-realization ä¸‹ä»… shape å¯è¯†åˆ«**  
   å½’ä¸€åŒ–å¯¼è‡´å…¨å±€å°ºåº¦æ¨¡ç³Šï¼Œä½†é¢‘ç‡ä¸å¸¦å®½ä»å¯æ¢å¤ï¼Œæƒé‡éœ€è®¾ä¸ºå‡åŒ€ + è§£æç¼©æ”¾ã€‚

5. **æ€§èƒ½åª²ç¾ PFNï¼Œé€Ÿåº¦åª²ç¾å‰å‘ä¼ æ’­**  
   è§£ç  kernel æ”¯æŒçš„ GP å›å½’ç²¾åº¦æ¥è¿‘ PFNï¼Œä¸”æ¯” DKL/RFF å¿« ~1000Ã—ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ– PFN é¢„è®­ç»ƒè´¨é‡**ï¼šè‹¥ PFN æœªå……åˆ†å­¦ä¹  spectral ç»“æ„ï¼Œåˆ™ decoder æ•ˆæœå—é™ã€‚
- **ä»…é€‚ç”¨äº stationary kernels**ï¼šæ— æ³•å¤„ç†éå¹³ç¨³è¿‡ç¨‹ã€‚
- **é«˜ç»´ç»„åˆçˆ†ç‚¸**ï¼šåœ¨ fully interacting é«˜ç»´ kernel ä¸Šè¡¨ç°ä¸‹é™ï¼ˆå¦‚ product kernelsï¼‰ã€‚
- **periodic structure å»ºæ¨¡å›°éš¾**ï¼šglobal pattern éš¾ä»¥é€šè¿‡å±€éƒ¨æ³¨æ„åŠ›å®Œå…¨æ•æ‰ã€‚
- **ä¸ä¿è¯ç²¾ç¡®é‡æ„åŸå§‹ kernel**ï¼šè¾“å‡ºçš„æ˜¯ surrogate kernelï¼Œæ•è·ä¸»å¯¼äºŒé˜¶ç»Ÿè®¡é‡ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **é›†æˆåˆ°ä¸‹æ¸¸ä»»åŠ¡**ï¼šå°† decoded kernel ç”¨äº surrogate-based optimizationã€controlã€ç‰©ç†å»ºæ¨¡ç­‰ã€‚
2. **æ··åˆæ¶æ„è®¾è®¡**ï¼šç»“åˆ amortized inference ä¸ adaptive context selectionï¼Œæå‡å¤§æ•°æ®åœºæ™¯ä¸‹çš„ scalabilityã€‚
3. **ç»´åº¦æ„ŸçŸ¥å»ºæ¨¡**ï¼šæ”¹è¿› PFN æ¶æ„ä»¥æ˜¾å¼ç¼–ç ç»´åº¦ä¿¡æ¯ï¼Œæ”¯æŒ per-dimension spectral decodingã€‚
4. **éå¹³ç¨³ kernel æ‰©å±•**ï¼šæ¢ç´¢æ—¶å˜ spectral density çš„æ‘Šé”€å‘ç°ã€‚
5. **çœŸå®ä¸–ç•Œä¿¡å·åº”ç”¨**ï¼šè¿ç§»åˆ° EEGã€è¯­éŸ³ã€æ°”å€™ç­‰å®é™… time series æ•°æ®ã€‚

--- 

> ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> æœ¬æ–‡æ­ç¤ºäº† PFN å†…éƒ¨è•´å«å¯è§£é‡Šçš„é¢‘è°±ç»“æ„ï¼Œå¹¶æå‡ºé¦–ä¸ª**æ‘Šé”€é¢‘è°±æ ¸å‘ç°æ¡†æ¶**ï¼Œå®ç°äº†**é›¶æ ·æœ¬ã€é«˜é€Ÿã€æ˜¾å¼çš„ kernel å‘ç°**ï¼Œä¸º AutoML ä¸ç§‘å­¦å‘ç°æä¾›äº†æ–°çš„å¯è§£é‡Šå·¥å…·ã€‚

</details>

---

### 4. [NetMamba+: A Framework of Pre-trained Models for Efficient and Accurate Network Traffic Classification](https://arxiv.org/abs/2601.21792)

**Authors**: Tongze Wang, Xiaohui Xie, Wenduo Wang, Chuyi Wang, Jinzhou Liu, Boyan Huang, Yannan Hu, Youjian Zhao, Yong Cui  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2601.21792v1  

#### Abstract
With the rapid growth of encrypted network traffic, effective traffic classification has become essential for network security and quality of service management. Current machine learning and deep learning approaches for traffic classification face three critical challenges: computational inefficienc...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šNetMamba+: A Framework of Pre-trained Models for Efficient and Accurate Network Traffic Classification

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ç½‘ç»œæµé‡åˆ†ç±»é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š
- **è®¡ç®—æ•ˆç‡ä½ä¸‹**ï¼šåŸºäºTransformerçš„æ¨¡å‹å› è‡ªæ³¨æ„åŠ›æœºåˆ¶å…·æœ‰ $O(L^2)$ å¤æ‚åº¦ï¼Œåœ¨å¤„ç†é•¿åºåˆ—æ—¶è®¡ç®—å’Œå†…å­˜å¼€é”€å·¨å¤§ï¼Œéš¾ä»¥éƒ¨ç½²äºå®æ—¶ç³»ç»Ÿã€‚
- **æµé‡è¡¨å¾ä¸å……åˆ†**ï¼šç°æœ‰æ–¹æ³•åœ¨ä¿ç•™åŸå§‹å­—èŠ‚ä¿¡æ¯çš„åŒæ—¶å¼•å…¥åå·®ï¼ˆå¦‚å¿½ç•¥headerç‰¹å¾ã€ä¸å½“åˆ†å—ï¼‰ï¼Œå¯¼è‡´å…³é”®è¯­ä¹‰ä¸¢å¤±ã€‚
- **å¯¹é•¿å°¾åˆ†å¸ƒæ•°æ®é€‚åº”å·®**ï¼šçœŸå®åœºæ™¯ä¸­ç±»åˆ«åˆ†å¸ƒé«˜åº¦ä¸å¹³è¡¡ï¼ˆlong-tailed distributionï¼‰ï¼Œä¸»æµæ–¹æ³•å¯¹æ­¤ç¼ºä¹æœ‰æ•ˆåº”å¯¹ç­–ç•¥ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
NetMamba+ æ˜¯é¦–ä¸ªå°† **Mamba æ¶æ„**åº”ç”¨äºç½‘ç»œæµé‡åˆ†ç±»çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œæå‡ºä»¥ä¸‹ä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯ï¼š

#### ï¼ˆ1ï¼‰é«˜æ•ˆæ¨¡å‹æ¶æ„è®¾è®¡
- å¼•å…¥ **Mamba**ï¼ˆçº¿æ€§æ—¶é—´å¤æ‚åº¦çš„SSMæ¨¡å‹ï¼‰ä½œä¸ºä¸»å¹²ï¼Œæ›¿ä»£ä¼ ç»ŸTransformerï¼Œæ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿã€‚
- å¯é€‰ä½¿ç”¨é›†æˆ **Flash Attention** çš„ç°ä»£Transformerå˜ä½“ï¼ˆNetTransï¼‰ï¼Œé€šè¿‡IOæ„ŸçŸ¥ä¼˜åŒ–æå‡æ•ˆç‡ã€‚
- é‡‡ç”¨ **pre-normalization** å’Œ **GeGLU-activated FFN** æå‡è®­ç»ƒç¨³å®šæ€§å’Œå‡†ç¡®ç‡ã€‚

#### ï¼ˆ2ï¼‰å¤šæ¨¡æ€æµé‡è¡¨å¾æ–¹æ¡ˆï¼ˆMultimodal Traffic Representationï¼‰
- åŒæ—¶å»ºæ¨¡ä¸‰ç§è¾“å…¥æ¨¡æ€ï¼š
  - **Byte Stride**ï¼šä»åŒ…å¤´å’Œè½½è·æå–å›ºå®šé•¿åº¦å­—èŠ‚æ®µï¼Œä½¿ç”¨ä¸€ç»´strideåˆ‡å‰²é¿å…å›¾åƒå¼patchå¸¦æ¥çš„è¯­ä¹‰åå·®ã€‚
  - **Packet Size Sequence**
  - **Inter-arrival Time Sequence**
- é€šè¿‡æ—©æœŸèåˆï¼ˆearly fusionï¼‰æ•´åˆè·¨æ¨¡æ€ä¿¡æ¯ï¼Œå¹¶åŠ å…¥ä½ç½®ç¼–ç å’Œæ¨¡æ€æ ‡è¯†ç¬¦ã€‚

#### ï¼ˆ3ï¼‰æ ‡ç­¾åˆ†å¸ƒæ„ŸçŸ¥å¾®è°ƒç­–ç•¥ï¼ˆLabel Distribution-Aware Fine-tuning, LDAï¼‰
- é’ˆå¯¹é•¿å°¾åˆ†å¸ƒï¼Œæå‡ºç»“åˆ **ç±»é‡åŠ æƒï¼ˆclass re-weightingï¼‰** å’Œ **å¤§é—´éš”å­¦ä¹ ï¼ˆlarge margin learningï¼‰** çš„æ–°å‹æŸå¤±å‡½æ•°ï¼š
  $$
  \mathcal{L}_{\text{LDA}} = -\frac{1-\beta}{1-\beta^{n_y}} \log \frac{e^{z_y - \Delta_y}}{\sum_j e^{z_j - \Delta_j}}
  $$
  å…¶ä¸­ $\Delta_j = C / n_j^{1/4}$ å¯¹å°‘æ•°ç±»æ–½åŠ æ›´å¤§marginã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | NetMamba+ ä¼˜åŠ¿ |
|------|----------------|
| **æ€§èƒ½** | åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°SOTAï¼ŒF1æœ€é«˜æå‡è¾¾ **6.44%** |
| **æ•ˆç‡** | æ¨ç†ååé‡æ¯”æœ€ä½³åŸºçº¿é«˜ **1.7Ã—**ï¼ŒGPUå†…å­˜å ç”¨ä½ |
| **é²æ£’æ€§** | å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›å¼ºï¼Œä»…ç”¨10%æ ‡æ³¨æ•°æ®ä»ä¼˜äºå¤šæ•°å…¨ç›‘ç£æ–¹æ³• |
| **å®ç”¨æ€§** | å®ç°åœ¨çº¿ç³»ç»Ÿï¼Œå®æµ‹å¹³å‡ååè¾¾ **261.87 Mb/s** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
| ç±»å‹ | æ•°æ®é›† | ä¸‹æ¸¸ä»»åŠ¡ | ç‰¹ç‚¹ |
|------|--------|----------|------|
| **é¢„è®­ç»ƒ** | Browser [2], Kitsune [52] | â€” | åŒ…å«å¤šç§åŠ å¯†åè®®ï¼ˆTLS, QUICç­‰ï¼‰å’Œæ”»å‡»æµé‡ |
| **å¾®è°ƒ** | CipherSpectrum, CSTNET-TLS1.3, CICIoT2022, USTC-TFC2016, ISCXVPN2016 ç­‰å…±11ä¸ª | åº”ç”¨åˆ†ç±»ã€æ”»å‡»æ£€æµ‹ã€æ¶æ„è½¯ä»¶è¯†åˆ«ã€VPNè¯†åˆ« | è¦†ç›–å››å¤§å…¸å‹åˆ†ç±»ä»»åŠ¡ï¼Œéƒ¨åˆ†ä¸ºçœŸå®ä¼ä¸šæ•°æ®ï¼ˆå¦‚Huawei-VPNï¼‰ |

> æ‰€æœ‰å¾®è°ƒæ•°æ®é›†å‡æŒ‰8:1:1åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼Œå¹¶é™åˆ¶æ¯ç±»æœ€å¤§æµæ•°ä»¥ç¼“è§£ä¸å¹³è¡¡ã€‚

### å®éªŒè®¾ç½®
- **æ¨¡å‹å®ç°**ï¼š
  - NetMamba+ï¼š4å±‚Encoder + 2å±‚Decoderï¼Œå‚æ•°çº¦2.6Mï¼ˆé¢„è®­ç»ƒï¼‰/1.9Mï¼ˆå¾®è°ƒï¼‰
  - å®ç°å·¥å…·ï¼šPyTorch 2.1.1ï¼Œç¡¬ä»¶å¹³å°ä¸ºA100 GPU Ã—4
- **è®­ç»ƒé…ç½®**ï¼š
  - é¢„è®­ç»ƒï¼šbatch_size=128ï¼Œæ­¥æ•°15ä¸‡ï¼ŒAdamWä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡$1e^{-3}$
  - å¾®è°ƒï¼šbatch_size=64ï¼Œepoch=120ï¼Œå­¦ä¹ ç‡$2e^{-3}$

### è¯„ä¼°æŒ‡æ ‡
| åœºæ™¯ | æŒ‡æ ‡ |
|------|------|
| **In-distribution åˆ†ç±»** | Accuracy (AC), Precision (PR), Recall (RC), weighted F1 Score |
| **Out-of-distribution (OOD) æ£€æµ‹** | AUROC, FPR95 |
| **æ•ˆç‡è¯„ä¼°** | æ¨ç†ååé‡ï¼ˆsamples/secï¼‰ã€GPUå†…å­˜æ¶ˆè€—ï¼ˆMBï¼‰ |
| **å°‘æ ·æœ¬è¯„ä¼°** | ä¸åŒæ¯”ä¾‹ï¼ˆ10%, 40%, 70%, 100%ï¼‰æ ‡æ³¨æ•°æ®ä¸‹çš„æ€§èƒ½ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | ä»£è¡¨æ–¹æ³• |
|------|--------|
| ä¼ ç»Ÿæœºå™¨å­¦ä¹  | AppScanner, FlowPrint |
| æ·±åº¦å­¦ä¹ ï¼ˆç›‘ç£ï¼‰ | Seq2Img, FS-Net, FlowPic, mini-FlowPic, TFE-GNN |
| Transformeré¢„è®­ç»ƒæ¨¡å‹ | ET-BERT, YaTC, TrafficFormer |
| æ¶æ„å˜ä½“ | NetTransVï¼ˆvanilla Transformerï¼‰ã€NetTransLï¼ˆLinear Transformerï¼‰ã€NetMambaBï¼ˆåŒå‘Mambaï¼‰ã€NetMambaCï¼ˆçº§è”Mambaï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
| æ¨¡å‹ | å¹³å‡Accuracy | æœ€é«˜F1æå‡ | æ¨ç†ååï¼ˆç›¸å¯¹æå‡ï¼‰ | å†…å­˜ä½¿ç”¨ |
|------|--------------|------------|-----------------------|-----------|
| **NetMamba+** | **0.8498 ~ 0.9765** | **+6.44%** vs SOTA | **1.7Ã— > YaTC** | ä¿æŒä½ä½ |
| NetMamba | 0.8779 ~ 0.9779 | â€” | ç›¸å½“ | æ›´ä½ |
| NetTrans | 0.8543 ~ 0.9823 | â€” | ç›¸å½“ | ç•¥é«˜ |

> âœ… åœ¨æ‰€æœ‰5ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šï¼ŒNetMamba+å‡å–å¾—æœ€ä¼˜æˆ–æ¬¡ä¼˜æ€§èƒ½ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **æ€§èƒ½æ–¹é¢**ï¼š
  - åœ¨å®Œå…¨åŠ å¯†æµé‡ï¼ˆCipherSpectrumï¼‰ä¸Šï¼ŒNetMamba+ å‡†ç¡®ç‡è¾¾ **96.52%**ï¼Œè¿œè¶…ç¬¬äºŒåFS-Netï¼ˆ90.00%ï¼‰ã€‚
  - åœ¨åä¸ºçœŸå®VPNæ•°æ®é›†ä¸Šï¼ŒNetMamba+ è¾¾åˆ° **94.5%** å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºET-BERTï¼ˆ<80%ï¼‰ï¼Œè¯´æ˜headerä¿¡æ¯çš„é‡è¦æ€§ã€‚
- **æ•ˆç‡æ–¹é¢**ï¼š
  - æ¨ç†é€Ÿåº¦æ¯”ET-BERTå¿« **47.4å€**ï¼Œæ¯”TrafficFormerå¿«è¿‘ **10å€**ã€‚
  - GPUå†…å­˜ä½¿ç”¨ä½äºå¤§å¤šæ•°æ·±åº¦æ¨¡å‹ï¼Œä»…æ¬¡äºè½»é‡çº§NetTransLå’ŒFS-Netã€‚

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰åŸå§‹å­—èŠ‚ç»„ä»¶æ¶ˆèï¼ˆTable VIï¼‰
| ç§»é™¤ç»„ä»¶ | æ€§èƒ½ä¸‹é™ï¼ˆACCï¼‰ |
|---------|----------------|
| Header | â†“14.86% ~ 47.50% |
| Payload | å½±å“ä¸ç¨³å®šï¼ˆÂ±1~3%ï¼‰ |
| Stride Cutting â†’ Patch Splitting | â†“â‰¤2.27% |
| Pre-training | â†“2.5% ~ 6.5% |

> ç»“è®ºï¼šHeaderä¿¡æ¯è‡³å…³é‡è¦ï¼›strideåˆ‡å‰²ä¼˜äºpatchåˆ†å‰²ï¼›é¢„è®­ç»ƒæ˜¾è‘—æå‡æ³›åŒ–èƒ½åŠ›ã€‚

#### ï¼ˆ2ï¼‰å¤šæ¨¡æ€è¾“å…¥æ¶ˆèï¼ˆTable VIIï¼‰
| è¾“å…¥æ¨¡æ€ | è¡¨ç°ç‰¹ç‚¹ |
|----------|----------|
| Byte Only | åŸºç¡€å¼ºï¼Œä½†åœ¨åŠ å¯†æµé‡ä¸­å—é™ |
| Size Only | åœ¨CipherSpectrumè¡¨ç°ä¼˜å¼‚ï¼ˆ89.06%ï¼‰ |
| Interval Only | æ•ˆæœè¾ƒå·® |
| **All (NetMamba+)** | **å…¨é¢é¢†å…ˆ**ï¼Œè¯æ˜å¤šæ¨¡æ€èåˆæœ‰æ•ˆæ€§ |

#### ï¼ˆ3ï¼‰LDAå¾®è°ƒæ¶ˆèï¼ˆTable VIIIï¼‰
| æ•°æ®é›† | ä½¿ç”¨LDAåF1æå‡ |
|--------|------------------|
| Huawei-VPN | +0.89% (NetTrans), +0.90% (NetMamba) |
| CP-Android | +1.75%, +1.89% |
| CP-iOS | +2.97%, +2.40% |

> ç»“è®ºï¼šLDAå¾®è°ƒåœ¨é•¿å°¾æ•°æ®ä¸ŠæŒç»­å¸¦æ¥å¢ç›Šï¼Œå°¤å…¶åœ¨ç§»åŠ¨ç«¯åº”ç”¨åˆ†ç±»ä¸­æ•ˆæœæ˜æ˜¾ã€‚

#### ï¼ˆ4ï¼‰å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFigure 8ï¼‰
- å³ä½¿åªä½¿ç”¨ **10%æ ‡æ³¨æ•°æ®**ï¼ŒNetMamba+ ä»ä¼˜äºå¤šæ•°ä½¿ç”¨å…¨é‡æ•°æ®çš„ç›‘ç£æ¨¡å‹ã€‚
- æ˜¾è‘—ä¼˜äºéé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚TFE-GNNï¼‰åœ¨å°æ ·æœ¬ä¸‹æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚

#### ï¼ˆ5ï¼‰OODæ£€æµ‹ï¼ˆTable IXï¼‰
| ä»»åŠ¡ | AUROC (NetMamba+) | FPR95 |
|------|--------------------|--------|
| Unknown Application | **0.9455** | 0.3670 |
| Unknown Attack | **0.9825** | **0.0463** |
| Unknown VPN | **0.9720** | **0.0715** |
| Unknown Malware | **0.9668** | **0.1072** |

> NetMamba+å…·å¤‡å¼ºå¤§æœªçŸ¥æµé‡è¯†åˆ«èƒ½åŠ›ï¼Œé€‚åˆå®é™…å®‰å…¨éƒ¨ç½²ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Mambaé€‚ç”¨äºç½‘ç»œæµé‡å»ºæ¨¡**ï¼šé¦–æ¬¡æˆåŠŸå°†Mambaå¼•å…¥ç½‘ç»œé¢†åŸŸï¼ŒéªŒè¯å…¶åœ¨çº¿æ€§å¤æ‚åº¦ä¸‹ä»èƒ½æ•æ‰æ·±å±‚è¯­ä¹‰ã€‚
2. **å¤šæ¨¡æ€è¾“å…¥ä¼˜äºå•ä¸€æ¨¡æ€**ï¼šèåˆbyteã€sizeã€intervalä¸‰ç±»ä¿¡å·å¯äº’è¡¥ä¿¡æ¯ï¼Œå°¤å…¶åœ¨åŠ å¯†ç¯å¢ƒä¸‹æå‡æ˜¾è‘—ã€‚
3. **strideåˆ‡å‰²ä¼˜äºpatchåˆ†å‰²**ï¼šæ›´ç¬¦åˆç½‘ç»œæµé‡çš„é¡ºåºç‰¹æ€§ï¼Œå‡å°‘æ— å…³å­—èŠ‚å…³è”å¼•å…¥çš„å™ªå£°ã€‚
4. **LDAå¾®è°ƒæœ‰æ•ˆç¼“è§£é•¿å°¾é—®é¢˜**ï¼šæ— éœ€é‡‡æ ·å³å¯æå‡å°‘æ•°ç±»è¯†åˆ«èƒ½åŠ›ï¼Œå·¥ç¨‹å‹å¥½ã€‚
5. **é¢„è®­ç»ƒæå¤§å¢å¼ºå°‘æ ·æœ¬èƒ½åŠ›**ï¼šåˆ©ç”¨æµ·é‡æ— æ ‡ç­¾æ•°æ®å­¦ä¹ é€šç”¨è¡¨å¾ï¼Œå¤§å¹…é™ä½å¯¹æ ‡æ³¨æ•°æ®ä¾èµ–ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å¯¹åˆ†å¸ƒåç§»æ•æ„Ÿ**ï¼šåœ¨æ—¶é—´åˆ†å¸ƒæ¼‚ç§»åœºæ™¯ä¸‹ï¼ˆå¦‚CSTNET-TLS1.3ï¼‰ï¼Œå‡†ç¡®ç‡ä¸‹é™è¾¾8.47%ï¼Œéœ€è¿›ä¸€æ­¥å¢å¼ºé²æ£’æ€§ã€‚
- **æœªæ”¯æŒåŠ¨æ€æµé•¿**ï¼šå½“å‰è¾“å…¥æˆªæ–­è‡³å‰$M_b$ä¸ªåŒ…ï¼Œå¯èƒ½ä¸¢å¤±åç»­é‡è¦è¡Œä¸ºæ¨¡å¼ã€‚
- **ä¾èµ–é«˜è´¨é‡åŒ¿ååŒ–**ï¼šè‹¥IP/portæœªå½»åº•è„±æ•ï¼Œå­˜åœ¨éšç§æ³„éœ²é£é™©ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¢å¼ºæ¨¡å‹åœ¨**åˆ†å¸ƒå¤–ï¼ˆOODï¼‰å’Œæ¦‚å¿µæ¼‚ç§»**åœºæ™¯ä¸‹çš„é€‚åº”èƒ½åŠ›ã€‚
- æ¢ç´¢**æµçº§åˆ«å¢é‡å­¦ä¹ **æœºåˆ¶ï¼Œæ”¯æŒæ— é™é•¿æµå¤„ç†ã€‚
- æ‰©å±•è‡³å…¶ä»–ç½‘ç»œä»»åŠ¡ï¼šå¦‚QoSé¢„æµ‹ã€ç½‘ç»œæ€§èƒ½è¯Šæ–­ã€LLM-based traffic reasoningã€‚
- å¼€å‘ç«¯ä¾§è½»é‡åŒ–ç‰ˆæœ¬ï¼Œé€‚é…è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚

> ğŸ”— ä»£ç å·²å¼€æºï¼š[https://github.com/wangtz19/NetMamba](https://github.com/wangtz19/NetMamba)

</details>

---

### 5. [ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling](https://arxiv.org/abs/2601.21198)

**Authors**: Yuchen Yang, Yaru Zhao, Pu Yang, Shaowei Wang, Zhi-Hua Zhou  
**Category**: cs.DC  
**Published**: 2026-01-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.21198v1  

#### Abstract
While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on los...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
ç°ä»£ **Mixture-of-Experts (MoE)** æ¶æ„è™½ç„¶æ˜¾è‘—æå‡äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†å…¶åºå¤§çš„å†…å­˜å ç”¨ä¸¥é‡é˜»ç¢äº†åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºã€Jetsonã€æ ‘è“æ´¾ç­‰ï¼‰ä¸Šçš„éƒ¨ç½²ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–**æœ‰æŸé‡åŒ–ï¼ˆlossy quantizationï¼‰** æˆ–**ä¸“å®¶å¸è½½ï¼ˆexpert offloadingï¼‰** æ¥å‡å°‘å†…å­˜å¼€é”€ï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

- **æœ‰æŸé‡åŒ–**ä¼šæ”¹å˜æ¨¡å‹è¡Œä¸ºï¼Œå¯èƒ½å¼•å…¥å®‰å…¨æ¼æ´ï¼ˆå¦‚æ¶æ„ä»£ç ç”Ÿæˆã€è¿‡åº¦æ‹’ç»ç­‰ï¼‰ï¼Œä¸”ç°æœ‰è¯„ä¼°æŒ‡æ ‡éš¾ä»¥æ•æ‰è¿™äº›é£é™©ã€‚
- **ç°æœ‰å¸è½½ç³»ç»Ÿ**å‡è®¾ CPU å’Œ GPU å†…å­˜åˆ†ç¦»ï¼Œè€Œä¸»æµè¾¹ç¼˜å¹³å°é‡‡ç”¨ **ç»Ÿä¸€å†…å­˜æ¶æ„ï¼ˆUMAï¼‰**ï¼Œå¯¼è‡´ I/O ç“¶é¢ˆåŠ å‰§ï¼Œè®¡ç®—èµ„æºä¸¥é‡æµªè´¹ã€‚

å› æ­¤ï¼Œè®ºæ–‡æ—¨åœ¨è§£å†³ï¼š**å¦‚ä½•åœ¨ä¸æ”¹å˜ MoE æ¨¡å‹è¯­ä¹‰çš„å‰æä¸‹ï¼Œé«˜æ•ˆåœ°åœ¨ç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œæ¨ç†ï¼Ÿ**

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
ä½œè€…æå‡º **ZIPMoE** â€”â€” ä¸€ç§é«˜æ•ˆçš„ã€è¯­ä¹‰æ— æŸï¼ˆsemantically losslessï¼‰çš„ on-device MoE æ¨ç†å¼•æ“ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº **â€œç¼“å­˜-è°ƒåº¦ååŒè®¾è®¡â€ï¼ˆcaching-scheduling co-designï¼‰**ï¼Œå…·ä½“åŒ…æ‹¬ï¼š

#### **(1) åŸºäºç»Ÿè®¡å†—ä½™çš„æ— æŸå‹ç¼©ï¼ˆLossless Compressionï¼‰**
- è§‚å¯Ÿåˆ° BF16 å‚æ•°ä¸­ **æŒ‡æ•°ä½ï¼ˆexponent bitsï¼‰å…·æœ‰é«˜åº¦ä½ç†µç‰¹æ€§**ï¼Œè€Œç¬¦å·ä½å’Œå°¾æ•°ä½æ¥è¿‘éšæœºåˆ†å¸ƒã€‚
- å°†æ¯ä¸ªå‚æ•°å¼ é‡æ‹†åˆ†ä¸º **E-chunksï¼ˆæŒ‡æ•°ä½ï¼Œå¯å‹ç¼©ï¼‰** å’Œ **SM-chunksï¼ˆç¬¦å·+å°¾æ•°ä½ï¼Œä¸å¯å‹ç¼©ï¼‰**ã€‚
- å¯¹ E-chunks ä½¿ç”¨ **ZSTD/LZ4HC** è¿›è¡Œæ— æŸå‹ç¼©ï¼Œå‹ç¼©ç‡å¯è¾¾ **68%~74%**ï¼Œæ¥è¿‘é¦™å†œç†µä¸‹é™ã€‚

#### **(2) åˆ†å±‚å‹ç¼©æ„ŸçŸ¥ç¼“å­˜ç®¡ç†ï¼ˆCompression-Aware Cache Managementï¼‰**
- è®¾è®¡å››ç±»ç¼“å­˜æ± ï¼š
  - `F`ï¼šå®Œæ•´å¼ é‡
  - `C`ï¼šå·²è§£å‹çš„ E-chunk + SM-chunk
  - `S`ï¼šä»… SM-chunk
  - `E`ï¼šä»…å‹ç¼©åçš„ E-chunk
- å¼•å…¥ **åŸºäºæ’åçš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ï¼ˆrank-based workload modelingï¼‰**ï¼Œé€šè¿‡å†å²æ¿€æ´»é¢‘ç‡é¢„æµ‹ä¸“å®¶è®¿é—®æ¨¡å¼ã€‚
- ä½¿ç”¨ **åŠ¨æ€è§„åˆ’ï¼ˆDynamic Programmingï¼‰** ä¼˜åŒ–å„ç¼“å­˜æ± çš„å†…å­˜åˆ†é…ï¼Œæœ€å°åŒ–é¢„æœŸ **makespan**ã€‚

#### **(3) ç¼“å­˜äº²å’Œè°ƒåº¦å™¨ï¼ˆCache-Affinity Schedulerï¼‰**
- å°†æ¯ä¸ªä¸“å®¶é‡å»ºä»»åŠ¡å»ºæ¨¡ä¸º **DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰**ï¼ŒèŠ‚ç‚¹è¡¨ç¤º I/Oã€è§£å‹ã€æ¢å¤æ“ä½œã€‚
- æå‡º **Type-I / Type-II ä»»åŠ¡åˆ’åˆ†**ï¼š
  - Type-Iï¼šéœ€åŠ è½½ SM-chunkï¼ˆI/O å¯†é›†ï¼‰
  - Type-IIï¼šä»…éœ€è§£å‹ E-chunkï¼ˆè®¡ç®—å¯†é›†ï¼‰
- è°ƒåº¦å™¨é€šè¿‡ **å—æ„é€ ï¼ˆblock constructionï¼‰** ç­–ç•¥ï¼Œå°† Type-II ä»»åŠ¡æ’å…¥ Type-I çš„ I/O ç©ºéš™ä¸­ï¼Œå®ç° **I/O ä¸ CPU è§£å‹å¹¶è¡ŒåŒ–**ã€‚
- ç†è®ºè¯æ˜è¯¥è°ƒåº¦ç®—æ³•çš„ **makespan ä¸è¶…è¿‡æœ€ä¼˜å€¼çš„ (3 - 1/L) å€**ï¼ˆL ä¸ºè§£å‹çº¿ç¨‹æ•°ï¼‰ã€‚

#### **(4) é«˜æ•ˆçš„ GPU å¼ é‡æ¢å¤å†…æ ¸**
- ä½¿ç”¨ **memory-coalesced CUDA kernel** å¹¶è¡Œåˆå¹¶ E-chunk å’Œ SM-chunkï¼Œæ¢å¤ä¸º BF16 å¼ é‡ã€‚
- åˆ©ç”¨ UMA ç‰¹æ€§ï¼Œé‡‡ç”¨ **zero-copy èŒƒå¼**ï¼Œé¿å…å†—ä½™æ•°æ®æ‹·è´ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ZIPMoE | ç°æœ‰æ–¹æ³•ï¼ˆMoE-Infinity, DeepSpeed, Accelerateï¼‰ |
|------|--------|---------------------------------------------|
| **æ¨¡å‹ä¿çœŸåº¦** | âœ… æ— æŸå‹ç¼©ï¼Œä¿æŒåŸå§‹è¡Œä¸º | âŒ ä¾èµ–æœ‰æŸé‡åŒ–ï¼Œå¯èƒ½å¼•å…¥å®‰å…¨éšæ‚£ |
| **ç¡¬ä»¶é€‚é…æ€§** | âœ… é’ˆå¯¹ UMA æ¶æ„ä¼˜åŒ– | âŒ å‡è®¾åˆ†ç¦»å†…å­˜ï¼Œä¸é€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡ |
| **I/O æ•ˆç‡** | âœ… å‹ç¼©åä¼ è¾“ + å¹¶è¡Œè§£å‹éšè—å»¶è¿Ÿ | âŒ å¤§é‡åŸå§‹å‚æ•°ä¼ è¾“ï¼ŒI/O æˆç“¶é¢ˆ |
| **CPU åˆ©ç”¨ç‡** | âœ… å……åˆ†åˆ©ç”¨å¤šæ ¸ CPU è¿›è¡Œå¹¶è¡Œè§£å‹ | âŒ CPU å¤šå¤„äºç©ºé—²çŠ¶æ€ |
| **ç¼“å­˜ç­–ç•¥** | âœ… åˆ†å±‚å‹ç¼©æ„ŸçŸ¥ç¼“å­˜ + åŠ¨æ€è§„åˆ’ä¼˜åŒ– | âŒ ä¼ ç»Ÿ FIFO/LRUï¼Œæœªè€ƒè™‘å‹ç¼©çŠ¶æ€ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ¨¡å‹ä¸æ•°æ®é›†**
- **æ¨¡å‹**ï¼š
  - **DeepSeekV2-Lite**ï¼ˆdecoder-onlyï¼‰
  - **Qwen1.5-MoE**ï¼ˆdecoder-onlyï¼‰
  - **SwitchTransformers-Large-128**ï¼ˆencoder-decoderï¼‰
- **æ•°æ®é›†**ï¼šä» **ShareGPT** ä¸­éšæœºé‡‡æ ·æç¤ºè¯ï¼Œç¡®ä¿æ‰€æœ‰åŸºçº¿ç³»ç»Ÿè¾“å…¥ä¸€è‡´ã€‚

### **å®éªŒå¹³å°**
- **è¾¹ç¼˜è®¾å¤‡**ï¼š
  - **Jetson AGX Orin 64GB**ï¼ˆHardware 1ï¼‰
  - **Jetson AGX Orin 32GB**ï¼ˆHardware 2ï¼‰
- **å­˜å‚¨**ï¼šSamsung 970 EVO NVMe SSDï¼ˆè¯»é€Ÿ 3.5GB/sï¼‰
- **æ“ä½œç³»ç»Ÿ**ï¼šUbuntu 22.04 + JetPack 6.2.1

### **è¯„ä¼°æŒ‡æ ‡**
- **TPOT**ï¼ˆTime Per Output Tokenï¼‰ï¼šè¡¡é‡ç”Ÿæˆé€Ÿåº¦
- **TTFT**ï¼ˆTime To First Tokenï¼‰ï¼šè¡¡é‡å“åº”å»¶è¿Ÿ
- **ç«¯åˆ°ç«¯å»¶è¿Ÿ**ï¼ˆEnd-to-End Latencyï¼‰
- **ååé‡**ï¼ˆThroughput, token/sï¼‰
- **å†…å­˜å ç”¨**ï¼ˆMemory Footprintï¼‰

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
1. **MoE-Infinity**ï¼šåŸºäºç¨€ç–æ€§çš„ä¸“å®¶ç¼“å­˜ä¸é¢„å–
2. **DeepSpeed**ï¼šæ”¯æŒ SSD å¸è½½çš„ ZeRO-3
3. **Accelerate**ï¼šHuggingFace æä¾›çš„è½»é‡çº§å¸è½½åº“

æ‰€æœ‰ç³»ç»Ÿå‡é…ç½®ä¸º SSD å¸è½½æ¨¡å¼ï¼Œå¹¶å¯¹é½ DRAM å†…å­˜å ç”¨ä»¥ä¿è¯å…¬å¹³æ¯”è¾ƒã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
| æŒ‡æ ‡ | ZIPMoE è¡¨ç° |
|------|------------|
| **æœ€å¤§æ¨ç†å»¶è¿Ÿé™ä½** | **72.77%** |
| **æœ€é«˜ååæå‡** | **6.76Ã—** |
| **å¹³å‡ TPOT é™ä½** | **62.65% ~ 97.97%**ï¼ˆdecoder-onlyï¼‰ |
| **å¹³å‡ TTFT é™ä½** | **53.25% ~ 87.90%** |
| **ç«¯åˆ°ç«¯åŠ é€Ÿæ¯”** | **3.03Ã— ~ 42.49Ã—**ï¼ˆdecoder-onlyï¼‰ |

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
#### **(1) å®æ—¶å“åº”æ€§ï¼ˆTPOT & TTFTï¼‰**
- åœ¨å¿…é¡»å¸è½½çš„åœºæ™¯ä¸‹ï¼ŒZIPMoE æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼š
  - **TPOT å‡å°‘æœ€å¤šè¾¾ 97.97%**
  - **TTFT æœ€å¤šå‡å°‘ 87.90%**
- å³ä½¿å¯¹äº encoder-decoder æ¨¡å‹ï¼ˆå¦‚ SwitchTransformersï¼‰ï¼Œä»èƒ½å®ç° **81.24% TPOT é™ä½** å’Œ **83.45% TTFT é™ä½**ã€‚

#### **(2) ååé‡ï¼ˆThroughputï¼‰**
- åœ¨ batch size = 1~16 ä¸‹æµ‹è¯•ï¼š
  - **decoder-only æ¨¡å‹**ï¼šZIPMoE è¾¾åˆ° **1.79Ã— ~ 42.49Ã—** ååæå‡
  - **encoder-decoder æ¨¡å‹**ï¼šè¾¾åˆ° **1.31Ã— ~ 5.82Ã—** æå‡
- éšç€ batch size å¢åŠ ï¼ŒZIPMoE çš„å¹¶è¡Œä¼˜åŠ¿è¿›ä¸€æ­¥æ”¾å¤§ã€‚

#### **(3) ç«¯åˆ°ç«¯å»¶è¿Ÿ**
- åœ¨ä¸åŒè¾“å‡ºé•¿åº¦ä¸‹ï¼ŒZIPMoE å§‹ç»ˆé¢†å…ˆï¼š
  - **decoder-only**ï¼šåŠ é€Ÿ **3.03Ã— ~ 42.49Ã—**
  - **encoder-decoder**ï¼šåŠ é€Ÿ **1.11Ã— ~ 5.64Ã—**

---

### **æ¶ˆèå®éªŒç»“æœ**
- **ç§»é™¤ç¼“å­˜è§„åˆ’æ¨¡å—**ï¼ˆw/o Cache Planningï¼‰ï¼š
  - ä½¿ç”¨ FIFO/LRU æ›¿ä»£ï¼Œæ€§èƒ½æ˜æ˜¾ä¸‹é™ã€‚
- **æ›¿æ¢ç¼“å­˜ç­–ç•¥**ï¼š
  - FIFOã€Markingã€LRU è¡¨ç°ç›¸è¿‘ï¼Œå‡ä¸å¦‚ ZIPMoE è‡ªç ”ç­–ç•¥ã€‚
- **åŠ å…¥ç¼“å­˜è§„åˆ’å**ï¼š
  - å®ç° **Pareto æœ€ä¼˜**ï¼ŒåŒæ—¶é™ä½å»¶è¿Ÿå¹¶æé«˜ååã€‚

> å›¾ 10 æ˜¾ç¤ºï¼ŒZIPMoE åœ¨ç›¸åŒååä¸‹å»¶è¿Ÿæ›´ä½ï¼Œæˆ–åœ¨ç›¸åŒå»¶è¿Ÿä¸‹ååæ›´é«˜ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **MoE å‚æ•°ä¸­å­˜åœ¨æ˜¾è‘—çš„ç»Ÿè®¡å†—ä½™**ï¼Œç‰¹åˆ«æ˜¯ BF16 çš„ exponent bits å¯è¢«é«˜æ•ˆæ— æŸå‹ç¼©ã€‚
2. **è¾¹ç¼˜è®¾å¤‡çš„ UMA æ¶æ„åº”è¢«ä¸»åŠ¨åˆ©ç”¨**ï¼Œè€Œéè¢«åŠ¨é€‚åº”ã€‚ZIPMoE é€šè¿‡ zero-copy å’Œå¹¶è¡Œè§£å‹å……åˆ†å‘æŒ¥å…¶æ½œåŠ›ã€‚
3. **I/O ç“¶é¢ˆå¯é€šè¿‡â€œè®¡ç®—æ¢ I/Oâ€ç­–ç•¥çªç ´**ï¼šç”¨ CPU å¤šæ ¸å¹¶è¡Œè§£å‹æ¥éšè— SSD è¯»å–å»¶è¿Ÿã€‚
4. **åˆ†å±‚ç¼“å­˜ + åŠ¨æ€è§„åˆ’è°ƒåº¦** å¯å®ç°ç»†ç²’åº¦å†…å­˜æ§åˆ¶ï¼Œåœ¨æœ‰é™å†…å­˜ä¸‹æœ€å¤§åŒ–å‘½ä¸­ç‡ä¸æ€§èƒ½ã€‚
5. **ZIPMoE å°† MoE æ¨ç†ä» I/O-bound è½¬å˜ä¸º compute-centric æµç¨‹**ï¼Œé‡Šæ”¾äº†è¾¹ç¼˜è®¾å¤‡çš„çœŸæ­£ç®—åŠ›ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
1. **ä¾èµ–ç‰¹å®šå‹ç¼©æ ¼å¼**ï¼šç›®å‰ä»…é’ˆå¯¹ BF16 çš„ exponent bits è®¾è®¡ï¼Œè‹¥ä½¿ç”¨ FP8 æˆ–å…¶ä»–æ ¼å¼éœ€é‡æ–°åˆ†æã€‚
2. **å¯¹ SSD æ€§èƒ½æ•æ„Ÿ**ï¼šåœ¨æä½å¸¦å®½å­˜å‚¨è®¾å¤‡ä¸Šï¼Œå³ä½¿å‹ç¼©å I/O ä»å¯èƒ½æ˜¯ç“¶é¢ˆã€‚
3. **è°ƒåº¦å¤æ‚åº¦è¾ƒé«˜**ï¼šç¼“å­˜äº²å’Œè°ƒåº¦éœ€è¦ç¦»çº¿ profiling æ”¯æŒï¼Œå¢åŠ éƒ¨ç½²å¤æ‚æ€§ã€‚
4. **æœªè€ƒè™‘åŠ¨æ€å†…å­˜ç«äº‰**ï¼šåœ¨å¤šä»»åŠ¡ç¯å¢ƒä¸‹ï¼Œç¼“å­˜é¢„ç®—å¯èƒ½è¢«å…¶ä»–åº”ç”¨æŠ¢å ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **æ‰©å±•è‡³æ›´å¤šé‡åŒ–æ ¼å¼**ï¼šç ”ç©¶ FP8ã€NF4 ç­‰æ ¼å¼ä¸‹çš„å‹ç¼©æ½œåŠ›ã€‚
2. **æ”¯æŒåŠ¨æ€ç¼“å­˜è°ƒæ•´**ï¼šæ ¹æ®è¿è¡Œæ—¶è´Ÿè½½è‡ªåŠ¨è°ƒèŠ‚å„ç¼“å­˜æ± å¤§å°ã€‚
3. **ç»“åˆè½»é‡çº§é¢„å–æœºåˆ¶**ï¼šè¿›ä¸€æ­¥å‡å°‘å†·å¯åŠ¨å»¶è¿Ÿã€‚
4. **è·¨è®¾å¤‡ååŒæ¨ç†**ï¼šæ¢ç´¢ ZIPMoE åœ¨è®¾å¤‡-äº‘ååŒåœºæ™¯ä¸­çš„åº”ç”¨ã€‚
5. **ç¡¬ä»¶ååŒè®¾è®¡**ï¼šå®šåˆ¶ FPGA/ASIC åŠ é€Ÿè§£å‹ä¸å¼ é‡é‡ç»„è¿‡ç¨‹ã€‚

--- 

> **æ€»ç»“**ï¼šZIPMoE æ˜¯é¦–ä¸ªé¢å‘è¾¹ç¼˜è®¾å¤‡ã€åŸºäº**æ— æŸå‹ç¼©**ä¸**ç¼“å­˜-è°ƒåº¦ååŒè®¾è®¡**çš„ MoE æ¨ç†ç³»ç»Ÿï¼Œä¸ä»…å¤§å¹…æå‡æ€§èƒ½ï¼Œæ›´é‡è¦çš„æ˜¯**ä¿éšœäº†æ¨¡å‹è¡Œä¸ºçš„å®Œæ•´æ€§ä¸å®‰å…¨æ€§**ï¼Œä¸ºå®‰å…¨ã€é«˜æ•ˆçš„ on-device AI æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 6. [Snowball: A Scalable All-to-All Ising Machine with Dual-Mode Markov Chain Monte Carlo Spin Selection and Asynchronous Spin Updates for Fast Combinatorial Optimization](https://arxiv.org/abs/2601.21058)

**Authors**: Seungki Hong, Kyeongwon Jeong, Taekwang Jang  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.21058v1  

#### Abstract
Ising machines have emerged as accelerators for combinatorial optimization. To enable practical deployment, this work aims to reduce time-to-solution by addressing three challenges: (1) hardware topology, (2) spin selection and update algorithms, and (3) scalable coupling-coefficient precision. Rest...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Snowball: A Scalable All-to-All Ising Machine with Dual-Mode Markov Chain Monte Carlo Spin Selection and Asynchronous Spin Updates for Fast Combinatorial Optimization*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹å½“å‰ **Ising Machine** åœ¨è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜æ—¶é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼š
1. **ç¡¬ä»¶æ‹“æ‰‘é™åˆ¶**ï¼šç¨€ç–è¿æ¥çš„ç¡¬ä»¶ï¼ˆå¦‚ Chimera æ‹“æ‰‘ï¼‰éœ€è¦é€šè¿‡ *minor embedding* æ˜ å°„å¯†é›†é—®é¢˜å›¾ï¼Œå¯¼è‡´ç‰©ç†è‡ªæ—‹æ•°é‡è†¨èƒ€ã€èµ„æºæµªè´¹å’Œæ˜ å°„å¤±è´¥é£é™©ã€‚
2. **æ”¶æ•›è¡Œä¸ºä¸ä½³**ï¼šå¹¶è¡Œæ›´æ–°ç­–ç•¥ï¼ˆå¦‚åŒæ­¥å…¨è‡ªæ—‹æ›´æ–°ï¼‰å®¹æ˜“å¼•å‘æŒ¯è¡ï¼ˆoscillationï¼‰æˆ–åœæ»ï¼Œç ´åè¯¦ç»†å¹³è¡¡ï¼ˆdetailed balanceï¼‰ï¼Œå½±å“ç®—æ³•æ”¶æ•›ã€‚
3. **è€¦åˆç³»æ•°ç²¾åº¦ä¸è¶³**ï¼šæ¨¡æ‹Ÿå®ç°ï¼ˆå¦‚é‡å­é€€ç«ã€æ¨¡æ‹Ÿ CMOSï¼‰å—é™äºä½æ¯”ç‰¹å®½åº¦ï¼Œå¯¼è‡´èƒ½é‡æ™¯è§‚å¤±çœŸæˆ–éœ€å¼•å…¥è¾…åŠ©è‡ªæ—‹ï¼Œé™ä½å¯æ‰©å±•æ€§å’Œè§£è´¨é‡ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°
æœ¬æ–‡æå‡ºäº† **Snowball** â€”â€” ä¸€ç§æ•°å­—ã€å¯æ‰©å±•ã€å…¨è¿æ¥ï¼ˆall-to-allï¼‰çš„ Ising Machine æ¶æ„ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- âœ… **Dual-Mode Markov Chain Monte Carlo (MCMC) è‡ªæ—‹é€‰æ‹©æœºåˆ¶**
  - æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    - **Random-Scan Mode**ï¼šéšæœºé€‰æ‹©å•ä¸ªè‡ªæ—‹è¿›è¡Œå¼‚æ­¥æ›´æ–°ï¼Œä¿è¯è¯¦ç»†å¹³è¡¡ä¸éå†æ€§ï¼ˆergodicityï¼‰ï¼Œç¡®ä¿æ”¶æ•›åˆ°å‰å¸ƒæ–¯åˆ†å¸ƒã€‚
    - **Roulette-Wheel Mode**ï¼šå¹¶è¡Œè®¡ç®—æ‰€æœ‰è‡ªæ—‹ç¿»è½¬æ¦‚ç‡ï¼ŒæŒ‰æƒé‡æŠ½æ ·é€‰æ‹©ä¸€ä¸ªè‡ªæ—‹è¿›è¡Œç¡®å®šæ€§ç¿»è½¬ï¼Œå‡å°‘æ‹’ç»ç‡ï¼ŒåŠ é€Ÿä½èƒ½æ€æ¢ç´¢ã€‚
  - åŒæ¨¡è®¾è®¡å…è®¸åœ¨æ”¶æ•›ç¨³å®šæ€§ä¸æ±‚è§£é€Ÿåº¦ä¹‹é—´çµæ´»æƒè¡¡ã€‚

- âœ… **å¼‚æ­¥è‡ªæ—‹æ›´æ–°ï¼ˆAsynchronous Spin Updatesï¼‰**
  - æ¯æ¬¡ä»…æ›´æ–°ä¸€ä¸ªè‡ªæ—‹ï¼Œå¹¶ç«‹å³ä¼ æ’­å…¶å¯¹å±€éƒ¨åœº $ u_i $ çš„å½±å“ï¼Œé¿å…å‘¨æœŸæ€§æŒ¯è¡ï¼ˆå¦‚ period-2 dynamicsï¼‰ï¼Œæå‡æ”¶æ•›ç¨³å®šæ€§ã€‚

- âœ… **é«˜ç²¾åº¦ã€å¯æ‰©å±•çš„è€¦åˆç³»æ•°è¡¨ç¤º**
  - é‡‡ç”¨ **bit-plane decomposition** æŠ€æœ¯å°†é«˜ç²¾åº¦ $ J_{ij} $ åˆ†è§£ä¸ºå¤šä¸ªç¬¦å·ä½å¹³é¢ï¼Œåœ¨ FPGA ä¸Šé«˜æ•ˆå­˜å‚¨ä¸å¤„ç†ï¼Œæ”¯æŒå®½åŠ¨æ€èŒƒå›´ä¸”å†…å­˜å¢é•¿çº¿æ€§äºæ¯”ç‰¹æ•°ã€‚

- âœ… **å¢é‡å¼å±€éƒ¨åœºæ›´æ–°ï¼ˆIncremental Local-Field Updateï¼‰**
  - åˆ©ç”¨åˆ—ä¸»åºï¼ˆcolumn-majorï¼‰bit-plane ç¼“å†²åŒºï¼Œåœ¨å•ä¸ªè‡ªæ—‹ç¿»è½¬åä»¥ $ O(N) $ æ—¶é—´å®Œæˆæ‰€æœ‰å—å½±å“å±€éƒ¨åœºçš„æ›´æ–°ï¼Œé¿å…æ¯æ¬¡é‡æ–°è®¡ç®— $ O(N^2) $ çš„å¼€é”€ã€‚

- âœ… **ç‰‡ä¸ŠçŠ¶æ€æ— å…³ RNGï¼ˆStateless RNGï¼‰**
  - ä½¿ç”¨åŸºäºç§å­å’Œç´¢å¼•çš„çº¯å‡½æ•°ç”Ÿæˆéšæœºæ•°ï¼Œæ”¯æŒé«˜åº¦å¹¶è¡ŒåŒ–ä¸”æ— å…±äº«çŠ¶æ€äº‰ç”¨ï¼Œé€‚åˆ FPGA å®ç°ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | Snowball | å…¸å‹æ¨¡æ‹Ÿ/ä¸“ç”¨ç¡¬ä»¶ï¼ˆå¦‚ D-Wave, CIM, SBï¼‰ |
|------|----------|----------------------------------------|
| è¿æ¥æ€§ | å…¨è¿æ¥ï¼ˆall-to-allï¼‰ | å—é™æ‹“æ‰‘ï¼ˆéœ€ minor embeddingï¼‰ |
| è€¦åˆç²¾åº¦ | é«˜ç²¾åº¦ã€å¯é…ç½®ï¼ˆbit-planeï¼‰ | ä½ç²¾åº¦ã€æ˜“å—å™ªå£°å¹²æ‰° |
| æ›´æ–°æ–¹å¼ | å¼‚æ­¥ã€åŒæ¨¡å¼ MCMC | åŒæ­¥æˆ–è¿ç»­åŠ¨åŠ›å­¦ï¼Œå¯èƒ½ä¸æ»¡è¶³è¯¦ç»†å¹³è¡¡ |
| å¯ç¼–ç¨‹æ€§ | æ•°å­—æ¶æ„ï¼Œçµæ´»è°ƒåº¦ | å›ºå®šç‰©ç†è¿‡ç¨‹ï¼Œçµæ´»æ€§å·® |
| å¯æ‰©å±•æ€§ | åŸºäº FPGAï¼Œæ˜“äºæ‰©å±• | å—é™äºç‰©ç†å™¨ä»¶è§„æ¨¡ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **Gset [59]**ï¼šæ ‡å‡†ç»„åˆä¼˜åŒ–åŸºå‡†å¥—ä»¶ï¼ŒåŒ…å«å¤šç§å›¾ç»“æ„ï¼ˆErdÅ‘sâ€“RÃ©nyi, Small-world, Torusï¼‰ï¼Œç”¨äºè¯„ä¼° **è§£è´¨é‡ï¼ˆcut valueï¼‰**ã€‚
- **K2000**ï¼šäººå·¥æ„é€ çš„å®Œå…¨å›¾ï¼ˆcomplete graphï¼‰ï¼Œ$ |V|=2000 $ï¼Œè¾¹æƒ $ J_{ij} \in \{-1, +1\} $ éšæœºç”Ÿæˆï¼Œè¾¹å¯†åº¦ 100%ï¼Œç”¨äºæµ‹è¯•å¤§è§„æ¨¡å…¨è¿æ¥åœºæ™¯ä¸‹çš„ **time-to-solution (TTS)**ã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **å¹³å°**ï¼šAMD Alveo U250 FPGA åŠ é€Ÿå¡ï¼Œè¿è¡Œé¢‘ç‡ 300 MHzã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Cut Value**ï¼šMax-Cut é—®é¢˜çš„ç›®æ ‡å‡½æ•°å€¼ï¼Œè¶Šé«˜è¶Šå¥½ã€‚
  - **Time-to-Solution (TTS)**ï¼šè¾¾åˆ°ç›®æ ‡æˆåŠŸç‡ $ p=0.99 $ æ‰€éœ€çš„æœŸæœ›è¿è¡Œæ—¶é—´ï¼š
    $$
    \text{TTS}(p) = t_a \cdot \frac{\ln(1-p)}{\ln(1-P_a(t_a))}
    $$
    å…¶ä¸­ $ t_a $ æ˜¯å•æ¬¡è¿è¡Œæ—¶é—´ï¼Œ$ P_a(t_a) $ æ˜¯å•æ¬¡æˆåŠŸæ¦‚ç‡ã€‚
- **å†·å´ç­–ç•¥**ï¼šé‡‡ç”¨çº¿æ€§é€€ç«ï¼ˆsimulated annealingï¼‰æ§åˆ¶æ¸©åº¦ä¸‹é™ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Neal [15]**ï¼šD-Wave çš„ CPU å®ç°çš„æ¨¡æ‹Ÿé€€ç«å™¨ï¼ˆSimulated Annealingï¼‰ã€‚
- **CIM [28]**ï¼šç›¸å¹² Ising Machineï¼ˆå…‰å­¦ç³»ç»Ÿï¼‰ã€‚
- **SB [21]**ï¼šåŸºäºåˆ†å²”çš„ FPGA æ¨¡æ‹Ÿé€€ç«ã€‚
- **STATICA [54]**ï¼šCMOS å…¨æ•°å­—é€€ç«å¤„ç†å™¨ï¼ˆæ”¯æŒä¸€æ¬¡æ›´æ–°æ‰€æœ‰è‡ªæ—‹ï¼‰ã€‚
- **ReAIM [11]**ï¼šåŸºäº ReRAM çš„å­˜å†…è®¡ç®— Ising Machineï¼ˆå½“å‰æœ€å…ˆè¿›ä¹‹ä¸€ï¼‰ã€‚

> æ³¨ï¼šéƒ¨åˆ†åŸºçº¿ç»“æœæ¥è‡ªä»¿çœŸæˆ–å°è§„æ¨¡åŸå‹ï¼Œè€Œ Snowball åœ¨çœŸå® FPGA ç¡¬ä»¶ä¸Šå®ç°å¹¶æµ‹é‡ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… åœ¨ Gset ä¸Šçš„è§£è´¨é‡ï¼ˆCut Valueï¼‰
- Snowball çš„ **RWAï¼ˆroulette-wheelï¼‰** å’Œ **RSAï¼ˆrandom-scanï¼‰** æ¨¡å¼åœ¨å¤šæ•°å®ä¾‹ä¸Šå–å¾—æœ€é«˜ cut valueã€‚
- è¡¨æ˜å…¶ç®—æ³•åœ¨ä¿æŒé«˜è´¨é‡è§£æ–¹é¢å…·æœ‰ç«äº‰åŠ›ã€‚

#### âœ… åœ¨ K2000 Max-Cut ä¸Šçš„ TTS(0.99)

| Ising Machine | Hardware Type | $ t_a $ [ms] | $ P_a(t_a) $ | **TTS(0.99) [ms]** |
|---------------|----------------|--------------|----------------|--------------------|
| Neal [15]     | CPU            | 4610         | 0.38           | **44413**          |
| CIM [28]      | Optics         | 5            | 0.02           | **1139.74**        |
| SB [21]       | FPGA           | 0.5          | 0.04           | **56.14**          |
| STATICA [54]  | CMOS           | 0.13         | 0.07           | **8.23**           |
| ReAIM [11]    | CMOS (sim)     | 0.15         | 0.47           | **1.11**           |
| **Snowball (seq)** | FPGA       | **0.128**    | **0.99**       | **0.128**          |
| **Snowball (par)** | FPGA       | **0.085**    | **0.99**       | **0.085**          |

> âš¡ï¸ **Snowball å®ç°äº†æ¯” ReAIM å¿« 8Ã— çš„ TTS(0.99)ï¼Œç›¸æ¯” Neal åŠ é€Ÿè¶…è¿‡ 20ä¸‡å€ï¼**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- Snowball çš„ TTS(0.99) è¾¾åˆ° **0.085 msï¼ˆå¹¶è¡Œæ¨¡å¼ï¼‰**ï¼Œæ˜¯ç›®å‰æŠ¥å‘Šä¸­æœ€ä¼˜çš„ç»“æœã€‚
- å³ä½¿ä¸ä¸“ä¸ºé«˜é€Ÿè®¾è®¡çš„ STATICA å’Œ ReAIM ç›¸æ¯”ï¼Œä»åˆ†åˆ«å¿«çº¦ **97Ã—** å’Œ **13Ã—**ã€‚
- æˆåŠŸæ¦‚ç‡é«˜è¾¾ 0.99ï¼Œæ„å‘³ç€å‡ ä¹æ¯æ¬¡è¿è¡Œéƒ½èƒ½æ‰¾åˆ°ç›®æ ‡è§£ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ˆé€šå¸¸ <0.8ï¼‰ã€‚

### æ¶ˆèå®éªŒä¸åˆ†æï¼ˆéšå«éªŒè¯ï¼‰
- **å¢é‡æ›´æ–°æœ‰æ•ˆæ€§**ï¼šå›¾14æ˜¾ç¤ºï¼Œâ€œnaiveâ€ï¼ˆæ— å¢é‡æ›´æ–°ï¼‰çš„ç«¯åˆ°ç«¯å»¶è¿Ÿè¿œé«˜äºå®é™…å®ç°ï¼Œè¯æ˜å¢é‡æ›´æ–°æå¤§ç¼“è§£äº†å†…å­˜å¸¦å®½ç“¶é¢ˆã€‚
- **è®¡ç®—ç»‘å®šç‰¹æ€§**ï¼šå›¾14ä¸­ kernel-only ä¸ end-to-end æ›²çº¿å‡ ä¹é‡åˆï¼Œè¯´æ˜ç³»ç»Ÿæ˜¯ compute-bound è€Œé memory-boundï¼Œè¡¨æ˜æ¶æ„è®¾è®¡é«˜æ•ˆã€‚
- **é«˜ç²¾åº¦é‡å»ºèƒ½åŠ›**ï¼šå›¾15å±•ç¤º Snowball å¯åœ¨ 16-bit ç²¾åº¦ä¸‹é‡å»º 64Ã—64 å±€éƒ¨åœºï¼Œåƒç´ çº§å‡†ç¡®ç‡è¾¾ **99.5%**ï¼ŒéªŒè¯ bit-plane æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å…¨è¿æ¥ + æ•°å­—æ¶æ„æ˜¯é«˜æ€§èƒ½ Ising Machine çš„å¯è¡Œè·¯å¾„**ï¼šå°½ç®¡å¸ƒçº¿å¤æ‚ï¼Œä½†é€šè¿‡ bit-plane å’Œè¡Œåˆ—ç¼“å†²ç­–ç•¥å¯åœ¨ FPGA ä¸Šé«˜æ•ˆå®ç°ã€‚
2. **å¼‚æ­¥ + åŒæ¨¡å¼ MCMC å¹³è¡¡äº†æ”¶æ•›æ€§ä¸é€Ÿåº¦**ï¼š
   - Random-scan ä¿è¯ç†è®ºæ”¶æ•›ï¼›
   - Roulette-wheel å‡å°‘æ‹’ç»ï¼ŒåŠ å¿«æœç´¢ã€‚
3. **å¢é‡æ›´æ–°æ˜¯å®ç°é«˜æ•ˆå…¨è¿æ¥çš„å…³é”®**ï¼šå°†æ¯æ­¥è®¡ç®—ä» $ O(N^2) $ é™è‡³ $ O(N) $ï¼Œä½¿å¤§è§„æ¨¡æ¨¡æ‹Ÿæˆä¸ºç°å®ã€‚
4. **Snowball åœ¨çœŸå®ç¡¬ä»¶ä¸Šå®ç°äº† SOTA æ€§èƒ½**ï¼šç›¸è¾ƒ ReAIM å®ç° **8Ã— çš„ TTS åŠ é€Ÿ**ï¼Œä¸”è§£è´¨é‡æ›´ä¼˜ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ– FPGA èµ„æº**ï¼šè™½ç„¶å¯æ‰©å±•ï¼Œä½†åœ¨æ›´å¤§è§„æ¨¡ï¼ˆå¦‚ $ N > 10^4 $ï¼‰æ—¶ä»å—é™äº BRAM å®¹é‡å’Œé€»è¾‘å•å…ƒã€‚
- **æœªé›†æˆ parallel tempering**ï¼šæ–‡ä¸­æŒ‡å‡º replica exchange å¯¹å¤§è§„æ¨¡ç³»ç»Ÿæ•ˆç‡ä¸‹é™ï¼Œæ•…æœªé‡‡ç”¨ï¼Œå¯èƒ½é™åˆ¶æŸäº›ç²—ç³™èƒ½é‡æ™¯è§‚çš„ç©¿è¶Šèƒ½åŠ›ã€‚
- **å°šæœªå¤šèŠ¯ç‰‡æ‰©å±•**ï¼šå½“å‰å®ç°åœ¨å•å— FPGA ä¸Šï¼Œæœªæ¥éœ€ç ”ç©¶ chiplet æˆ–å¤šå¡äº’è”ä»¥è¿›ä¸€æ­¥æ‰©å¤§è§„æ¨¡ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **multi-FPGA æˆ– chiplet-based æ‰©å±•æ¶æ„**ï¼Œæ”¯æŒç™¾ä¸‡çº§è‡ªæ—‹ã€‚
- ç»“åˆ **hybrid annealing strategies**ï¼Œå¦‚ adaptive cooling æˆ–è½»é‡çº§ replica exchangeã€‚
- å°† Snowball æ¶æ„åº”ç”¨äºæ›´å¤š NP-hard é—®é¢˜ï¼ˆå¦‚ TSPã€SATã€QUBOï¼‰çš„å®é™…éƒ¨ç½²ã€‚
- å¼€å‘æ›´é«˜å±‚æ¬¡çš„ç¼–è¯‘å·¥å…·é“¾ï¼Œè‡ªåŠ¨å°†ç»„åˆä¼˜åŒ–é—®é¢˜æ˜ å°„åˆ° Snowball æ¶æ„ã€‚

--- 

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> Snowball é€šè¿‡ **æ•°å­—å…¨è¿æ¥æ¶æ„ + åŒæ¨¡å¼ MCMC + å¢é‡æ›´æ–° + bit-plane ç²¾åº¦ç®¡ç†**ï¼Œåœ¨çœŸå® FPGA ä¸Šå®ç°äº†æ¯”ç°æœ‰æœ€å…ˆè¿› Ising Machine å¿« **8å€ä»¥ä¸Š** çš„ time-to-solutionï¼Œä¸ºé«˜æ€§èƒ½ç»„åˆä¼˜åŒ–ç¡¬ä»¶æä¾›äº†æ–°çš„èŒƒå¼ã€‚

</details>

---

### 7. [L$^3$: Large Lookup Layers](https://arxiv.org/abs/2601.21461)

**Authors**: Albert Tseng, Christopher De Sa  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.21461v1  

#### Abstract
Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP "experts." However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable train...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡ã€ŠLÂ³: Large Lookup Layersã€‹æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
ç°ä»£ç¨€ç–è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ MoEï¼‰é€šè¿‡åŠ¨æ€è·¯ç”±ï¼ˆcontext-dependent routingï¼‰å®ç°é«˜å‚æ•°é‡ä¸‹çš„é«˜æ•ˆè®¡ç®—ï¼Œä½†å­˜åœ¨ä»¥ä¸‹ç³»ç»Ÿçº§æŒ‘æˆ˜ï¼š
- **ç¡¬ä»¶æ•ˆç‡ä½**ï¼šä¸“å®¶é€‰æ‹©ä¾èµ–ä¸Šä¸‹æ–‡ï¼Œæ— æ³•æå‰ç¡®å®šæ¿€æ´»å‚æ•°ï¼Œå¯¼è‡´éš¾ä»¥è¿›è¡Œå‚æ•°å¸è½½ï¼ˆoffloadingï¼‰å’Œé¢„å–ï¼ˆprefetchingï¼‰ã€‚
- **è®­ç»ƒä¸ç¨³å®š**ï¼šéœ€è¦è¾…åŠ©æŸå¤±ï¼ˆauxiliary lossesï¼‰æ¥å¹³è¡¡è´Ÿè½½ï¼Œé˜²æ­¢è·¯ç”±åç¼©ï¼ˆrouting collapseï¼‰ã€‚
- **ç³»ç»Ÿå¼€é”€å¤§**ï¼šä¸“å®¶åˆ†å¸ƒä¸å‡å¯èƒ½å¯¼è‡´è®¾å¤‡åˆ©ç”¨ç‡ä½ä¸‹ï¼Œä¸”éš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡éƒ¨ç½²ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„ **tokenizer embedding è¡¨** æ˜¯ä¸€ç§é™æ€ç¨€ç–ç»“æ„ï¼Œå…·æœ‰æé«˜çš„ç³»ç»Ÿå‹å¥½æ€§ï¼ˆå¦‚å¯é¢„æµ‹è®¿é—®æ¨¡å¼ï¼‰ï¼Œä½†ç¼ºä¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»…ç”¨äºåˆå§‹åµŒå…¥ã€‚

æœ¬æ–‡æå‡ºï¼š**èƒ½å¦è®¾è®¡ä¸€ç§æ—¢å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€åˆä¿æŒç³»ç»Ÿå‹å¥½çš„æ–°å‹ç¨€ç–æ¶æ„ï¼Ÿ**

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
ä½œè€…æå‡ºäº† **Large Lookup Layer (LÂ³)**ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç¨€ç–åŒ–èŒƒå¼ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
> å°†ä¼ ç»Ÿ tokenizer embedding è¡¨çš„æ¦‚å¿µæ¨å¹¿åˆ° decoder å±‚ä¸­ï¼Œå¹¶å¼•å…¥**ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é™æ€æŸ¥æ‰¾æœºåˆ¶**ã€‚

#### **LÂ³ çš„ä¸¤å¤§æ ¸å¿ƒç»„ä»¶**ï¼š
1. **é™æ€è·¯ç”±ï¼ˆStatic Routingï¼‰**  
   - æ¯ä¸ª token ID é¢„åˆ†é…ä¸€ç»„å›ºå®šçš„ embeddingsï¼ˆç§°ä¸ºâ€œlookup tableâ€ï¼‰ï¼Œè¿™äº› embeddings åœ¨è®­ç»ƒå‰å°±å·²ç¡®å®šã€‚
   - è·¯ç”±ä»…ä¾èµ–äº token IDï¼Œè€Œééšè—çŠ¶æ€ï¼Œå› æ­¤å¯ä»¥**åœ¨ token ç”Ÿæˆæ—¶ç«‹å³çŸ¥é“æ‰€éœ€å‚æ•°**ï¼Œæ”¯æŒé«˜æ•ˆçš„ CPU å¸è½½ä¸é¢„å–ã€‚

2. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥èšåˆï¼ˆContext-Aware Aggregationï¼‰**  
   - ä½¿ç”¨å½“å‰ token çš„ hidden state å¯¹å…¶å¯¹åº”çš„ embeddings è¿›è¡Œ attention æ“ä½œï¼Œä»è€Œå®ç°**åŸºäºä¸Šä¸‹æ–‡çš„ä¿¡æ¯æ£€ç´¢**ã€‚
   - å½¢å¼ä¸Šç±»ä¼¼äº Key-Value Attentionï¼Œå…¶ä¸­ Key å’Œ Value æ¥è‡ª lookup tableï¼ŒQuery æ¥è‡ª hidden stateã€‚

3. **ä¿¡æ¯è®ºé©±åŠ¨çš„ embedding åˆ†é…ç®—æ³•**  
   - æå‡ºåŸºäº **LZW å‹ç¼©ç®—æ³•** çš„ embedding åˆ†é…ç­–ç•¥ï¼Œå°†é«˜é¢‘å‡ºç°çš„ token åç¼€èµ‹äºˆæ›´å¤š embeddingsã€‚
   - å®ç°æ–¹å¼ï¼šæ‰«æè¯­æ–™åº“ï¼Œç»Ÿè®¡æœ€é•¿æœªè§åç¼€é¢‘ç‡ï¼ŒæŒ‰é¢‘æ¬¡é™åºä¸º token åˆ†é… embeddingsï¼Œä¸Šé™ä¸º $ k=512 $ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | MoE | LÂ³ |
|------|-----|----|
| **è·¯ç”±æœºåˆ¶** | åŠ¨æ€ï¼ˆcontext-dependentï¼‰ | é™æ€ï¼ˆtoken-basedï¼‰ |
| **å‚æ•°å¯é¢„æµ‹æ€§** | å·®ï¼ˆéœ€è¿è¡Œè‡³ router æ‰çŸ¥ï¼‰ | æä½³ï¼ˆtoken ä¸€å‡ºå³çŸ¥ï¼‰ |
| **ç³»ç»Ÿå‹å¥½æ€§** | å·®ï¼ˆéš¾ offloadï¼Œè´Ÿè½½ä¸å‡ï¼‰ | å¼ºï¼ˆæ”¯æŒé›¶å¼€é”€ offloadï¼‰ |
| **è®­ç»ƒç¨³å®šæ€§** | éœ€è¦ load balancing loss | ä¸éœ€è¦ |
| **ç¨€ç–è½´æ­£äº¤æ€§** | å‚æ•°ç¨€ç–ä¸»è·¯å¾„ | æ–°å¢ç‹¬ç«‹ç¨€ç–ç»´åº¦ï¼Œå¯ä¸ MoE å…±å­˜ |
| **æ€§èƒ½æå‡** | æ˜¾è‘—ä½†æœ‰å¯åŠ¨æˆæœ¬ | ç«‹ç«¿è§å½±ï¼Œå…¨ç¨‹é¢†å…ˆ |

> âœ… **LÂ³ å¹¶éæ›¿ä»£ MoEï¼Œè€Œæ˜¯æä¾›äº†ä¸€æ¡æ–°çš„ã€æ›´é«˜æ•ˆçš„ç¨€ç–æ‰©å±•è·¯å¾„**ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **è®­ç»ƒæ•°æ®**ï¼šFineWeb-Eduï¼ˆçº¦ 350B tokens å­é›†ï¼‰
- **è¯„ä¼°æ•°æ®**ï¼š
  - WikiText2ï¼ˆç”¨äº PPL æµ‹è¯•ï¼‰
  - å¤šé¡¹ zero-shot ä¸‹æ¸¸ä»»åŠ¡ï¼šARC-C, ARC-E, HellaSwag, PIQA, Winogrande

### **å®éªŒè®¾ç½®**
- **æ¨¡å‹æ¶æ„åŸºç¡€**ï¼šåŸºäº Llama 3 é£æ ¼çš„ decoder-only transformer
- **æ¨¡å‹è§„æ¨¡**ï¼š
  - 800Mï¼ˆ400M decoderï¼‰ã€1.5Bã€2.6B **active parameters**
  - æ€»å‚æ•°å¯è¾¾ 7Bï¼ˆå« LÂ³ embeddingsï¼‰
- **ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š2048
- **Tokenizer**ï¼šBPEï¼Œvocab size = 180K
- **LÂ³ è®¾ç½®**ï¼š
  - æ¯ä¸ª token æœ€å¤š $ k=512 $ ä¸ª embeddings
  - æ€» embedding æ•° $ u = 710K $
  - LÂ³ å±‚æ’å…¥ä½ç½®ï¼šdecoder å±‚ä¹‹é—´ï¼ˆå¦‚ç¬¬ 4ã€16 å±‚åï¼‰

### **è¯„ä¼°æŒ‡æ ‡**
- **è®­ç»ƒé˜¶æ®µ**ï¼šè®­ç»ƒé›† perplexityï¼ˆPPLï¼‰
- **æ¨ç†é˜¶æ®µ**ï¼š
  - Zero-shot accuracy on multiple benchmarks
  - æ¨ç†ååé‡ï¼ˆtokens/secï¼‰
  - offloading å¼€é”€åˆ†æ

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| ç±»å‹ | æ¨¡å‹ |
|------|------|
| **Dense Baseline** | åŒ active FLOPs çš„å…¨è¿æ¥æ¨¡å‹ |
| **MoE** | Iso-FLOPã€iso-depthã€iso-sparse MoEï¼ˆå¦‚ 22â€“44 ä¸“å®¶/å±‚ï¼‰ |
| **æ¶ˆèå®éªŒ** | ä¸åŒ LÂ³ å±‚æ•°ã€embedding åˆ†é…ç­–ç•¥ã€weight tying ç­‰ |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **(1) è®­ç»ƒ perplexity æå‡æ˜¾è‘—**
- åœ¨æ‰€æœ‰æ¨¡å‹å°ºåº¦ä¸‹ï¼Œæ·»åŠ  LÂ³ å±‚å‡å¸¦æ¥æŒç»­ä¸”ç¨³å®šçš„ PPL ä¸‹é™ï¼š
  - **2.6B æ¨¡å‹**ï¼šä» 15.43ï¼ˆdenseï¼‰é™è‡³ **14.51**ï¼ˆ+2LÂ³ï¼‰ï¼Œç›¸å¯¹ä¸‹é™ **6%**
  - **1.5B æ¨¡å‹**ï¼šä» 18.83 â†’ 16.72ï¼ˆ+2LÂ³ï¼‰
  - **800M æ¨¡å‹**ï¼šä» 22.02 â†’ 19.59ï¼ˆ+3LÂ³*ï¼‰

> å›¾ 6 æ˜¾ç¤º LÂ³ æ”¹è¿›è´¯ç©¿æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼Œæ— åˆæœŸæ€§èƒ½ä¸‹é™ï¼ˆè€Œ MoE å¸¸å›  router å­¦ä¹ å›°éš¾å‡ºç°å†·å¯åŠ¨é—®é¢˜ï¼‰ã€‚

#### **(2) ä¸‹æ¸¸ä»»åŠ¡å…¨é¢é¢†å…ˆ**
| æ¨¡å‹ | ACTIVE PARAMS | TOTAL PARAMS | ARCC â†‘ | HellaSwag â†‘ | PIQA â†‘ | Winogrande â†‘ |
|------|----------------|---------------|--------|-------------|--------|--------------|
| Dense 2.6B | 2.6B | 2.6B | 36.35 | 44.20 | 71.22 | 57.30 |
| LÂ³ 2.6B (2 layers) | 2.6B | 7B | **38.21** | **44.95** | **71.71** | **58.80** |

> æ‰€æœ‰ä»»åŠ¡å‡æœ‰æå‡ï¼Œå°¤å…¶é€»è¾‘æ¨ç†ç±»ä»»åŠ¡ï¼ˆå¦‚ ARCC +1.86 ptsï¼‰ã€‚

#### **(3) æ¨ç†é€Ÿåº¦å‡ ä¹æ— æŸ**
| æ¨¡å‹ | FIRST LAYER OFFLOADED? | TOKENS/S |
|------|-------------------------|----------|
| Dense 2.6B | â€“ | 334.98 |
| LÂ³ 2.6B (CPU offloaded) | Yes (1st LÂ³) | 302.11 |
| LÂ³ 2.6B | Yes (4th layer) | **332.74** |

> âš¡ **ä»…éœ€ 4 ä¸ª decoder å±‚å³å¯å®Œå…¨æ©ç›– offloading å»¶è¿Ÿ**ï¼Œè¯´æ˜ LÂ³ çš„é™æ€è·¯ç”±æå¤§æå‡äº†ç³»ç»Ÿæ•ˆç‡ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

#### **vs. MoEï¼šLÂ³ æ›´é«˜æ•ˆä¸”ç¨³å®š**
- å›¾ 8 æ˜¾ç¤ºï¼Œåœ¨ç›¸åŒ FLOPs å’Œç¨€ç–ç‡ä¸‹ï¼š
  - **800M å°ºåº¦**ï¼šä¸€ä¸ª LÂ³ å±‚ä¼˜äºä¸¤ä¸ª LÂ³ ç­‰æ•ˆçš„ MoE
  - **1.5B å°ºåº¦**ï¼šä¸€ä¸ª LÂ³ å±‚ç­‰æ•ˆçš„ MoE ç”šè‡³ä¸å¦‚ dense baseline
- ç»“è®ºï¼š**MoE éœ€è¦è¶³å¤Ÿå¤§çš„ token budget æˆ–ç¨€ç–ç‡æ‰èƒ½å‘æŒ¥ä¼˜åŠ¿ï¼Œè€Œ LÂ³ æ•ˆæœç«‹ç«¿è§å½±**ã€‚

#### **vs. Uniform Allocation**
- ä½¿ç”¨ LZW åˆ†é…æ¯”å‡åŒ€åˆ†é…ï¼ˆæ¯ä¸ª token å›ºå®š 4 ä¸ª embeddingsï¼‰å¸¦æ¥ **è¿‘ä¸¤å€çš„ PPL æ”¹å–„å¹…åº¦**ï¼ˆå›¾ 7Cï¼‰ã€‚
- è‹¥ä¸é™åˆ¶ $ k $ï¼ŒæŸäº›é«˜é¢‘ token å¯èƒ½è·å¾—è¶… 20K embeddingsï¼Œé€ æˆ worst-case å†…å­˜çˆ†ç‚¸ï¼›é™åˆ¶ $ k=512 $ åæ€§èƒ½ä»æ¥è¿‘æœ€ä¼˜ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

| å®éªŒ | å‘ç° |
|------|------|
| **ä¸åŒ LÂ³ å±‚æ•° vs. embedding è§„æ¨¡**ï¼ˆå›¾ 7Lï¼‰ | 2Ã—710K â‰ˆ 1Ã—1.42Mï¼Œä½†å•ä¸€å¤§å±‚ä¸åˆ©äº offloadingï¼›æ¨èå¤šå°å±‚å¹³è¡¡æ€§èƒ½ä¸ç³»ç»Ÿæ•ˆç‡ |
| **embedding åˆ†é…ç­–ç•¥**ï¼ˆå›¾ 7Cï¼‰ | LZW > capped LZW (k=512) > k=256 >> uniformï¼›**LZW æ˜¯å…³é”®** |
| **Weight Tying (Wâ‚– = Wáµ¥)**ï¼ˆå›¾ 7Rï¼‰ | å‡ ä¹ä¸å½±å“è´¨é‡ï¼Œä½†ä½¿æœ‰æ•ˆç¨€ç–ç‡ç¿»å€ã€ä¼ è¾“ä½“ç§¯å‡åŠ â†’ **å¼ºçƒˆå»ºè®®éƒ¨ç½²æ—¶å¯ç”¨** |
| **LÂ³ æ’å…¥ä½ç½®**ï¼ˆå›¾ 9ï¼‰ | å¤ªæ—©ï¼ˆå‰å‡ å±‚ï¼‰ä¸Šä¸‹æ–‡ä¸è¶³ï¼›å¤ªæ™šå½±å“æœ‰é™ï¼›ä¸­é—´å±‚ï¼ˆå¦‚ç¬¬ 10 å±‚ï¼‰æœ€ä½³ |

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **LÂ³ æˆåŠŸè§£é”äº†ä¸€ä¸ªæ–°çš„ç¨€ç–åŒ–ç»´åº¦**ï¼šå®ƒä¸ä¾èµ–åŠ¨æ€è·¯ç”±ï¼Œè€Œæ˜¯é€šè¿‡â€œé™æ€æŸ¥æ‰¾ + ä¸Šä¸‹æ–‡èšåˆâ€å®ç°é«˜æ•ˆä¿¡æ¯ç¼“å­˜ã€‚
2. âœ… **LÂ³ æ˜¾è‘—ä¼˜äº iso-FLOP dense å’Œ MoE æ¨¡å‹**ï¼Œæ— è®ºæ˜¯åœ¨è®­ç»ƒç¨³å®šæ€§ã€ä¸‹æ¸¸æ€§èƒ½è¿˜æ˜¯æ¨ç†æ•ˆç‡æ–¹é¢ã€‚
3. âœ… **LÂ³ å®Œå…¨å…¼å®¹ç°æœ‰ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯**ï¼šæ”¯æŒ fast sorting kernelsï¼ˆå¦‚ FlexAttentionï¼‰ã€zero-overhead CPU offloadingã€context parallelismã€‚
4. âœ… **Tuned Lens åˆ†æè¡¨æ˜**ï¼šLÂ³ å±‚å¼•èµ· KL divergence çš„éª¤é™ï¼Œè¯´æ˜æ¨¡å‹ç¡®å®åœ¨åˆ©ç”¨ LÂ³ ç¼“å­˜æœ¬éœ€æ·±å±‚è®¡ç®—çš„ä¿¡æ¯ï¼ˆå›¾ 10ï¼‰ã€‚
5. âœ… **LZW-based allocation æœ‰æ•ˆæ¨¡æ‹Ÿäº† context-dependent behavior**ï¼šé«˜é¢‘ token è·å¾—æ›´å¤š embeddingsï¼ŒKL åˆ†å¸ƒéš embedding æ•°å¹³æ»‘ä¸Šå‡ï¼ˆå›¾ 11ï¼‰ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- â— **LÂ³ çš„å¢ç›Šæ¥æºäºâ€œä¿¡æ¯ç¼“å­˜â€è€Œéâ€œè®¡ç®—å¢å¼ºâ€**ï¼šå®ƒæ›´é€‚åˆåŠ é€Ÿé‡å¤æ¨¡å¼å¤„ç†ï¼Œå¯¹å…¨æ–°ç»„åˆæ³›åŒ–å¸®åŠ©æœ‰é™ã€‚
- â— **embedding åˆ†é…ä¾èµ–è¯­æ–™å…ˆéªŒ**ï¼šè‹¥æµ‹è¯•åˆ†å¸ƒåç§»ä¸¥é‡ï¼Œå¯èƒ½å½±å“æ€§èƒ½ã€‚
- â— **ç›®å‰ä»…ä½œä¸ºè¾…åŠ©å±‚æ’å…¥**ï¼šå°šæœªå®Œå…¨å–ä»£ MLP æˆ– MoEï¼Œæœªæ¥éœ€æ¢ç´¢ç«¯åˆ°ç«¯é›†æˆã€‚
- â— **æç«¯ token çš„ worst-case å¼€é”€ä»å­˜åœ¨é£é™©**ï¼šè™½æœ‰ $ k $ é™åˆ¶ï¼Œä½†åœ¨é«˜å¹¶å‘åœºæ™¯ä»éœ€æ³¨æ„ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. ğŸ”® **ç»“åˆ MoE ä¸ LÂ³**ï¼šåœ¨åŒä¸€æ¨¡å‹ä¸­åŒæ—¶ä½¿ç”¨ä¸¤ç§ç¨€ç–æœºåˆ¶ï¼Œæ¢ç´¢ååŒæ•ˆåº”ã€‚
2. ğŸ”® **è·¨ä»»åŠ¡è¿ç§» LÂ³ embeddings**ï¼šå— Cartridges å¯å‘ï¼Œå°è¯•â€œswap LÂ³ layers across tasksâ€ã€‚
3. ğŸ”® **åŠ¨æ€è°ƒæ•´ embedding åˆ†é…**ï¼šåœ¨çº¿å­¦ä¹  token-specific embedding æ•°é‡ã€‚
4. ğŸ”® **åº”ç”¨äº encoder-decoder æˆ– long-context æ¨¡å‹**ï¼šæ¢ç´¢åœ¨æ£€ç´¢ã€æ‘˜è¦ç­‰ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚
5. ğŸ”® **è¿›ä¸€æ­¥å‹ç¼© embedding å­˜å‚¨**ï¼šç»“åˆé‡åŒ–ï¼ˆquantizationï¼‰æˆ–å“ˆå¸Œï¼ˆhashingï¼‰é™ä½å†…å­˜å ç”¨ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **LÂ³ æå‡ºäº†ä¸€ç§ç³»ç»Ÿå‹å¥½ã€æ€§èƒ½ä¼˜è¶Šçš„æ–°ç¨€ç–æ¶æ„ï¼Œé€šè¿‡â€œé™æ€æŸ¥æ‰¾ + ä¸Šä¸‹æ–‡æ³¨æ„åŠ›â€çš„è®¾è®¡ï¼Œåœ¨ä¸ç‰ºç‰²å»ºæ¨¡èƒ½åŠ›çš„å‰æä¸‹ï¼Œå®ç°äº†æ¯” MoE æ›´å¿«ã€æ›´ç¨³ã€æ›´æ˜“éƒ¨ç½²çš„ç¨€ç–æ‰©å±•è·¯å¾„**ã€‚

</details>

---

### 8. [Investigating Batch Inference in a Sequential Monte Carlo Framework for Neural Networks](https://arxiv.org/abs/2601.21983)

**Authors**: Andrew Millard, Joshua Murphy, Peter Green, Simon Maskell  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.21983v1  

#### Abstract
Bayesian inference allows us to define a posterior distribution over the weights of a generic neural network (NN). Exact posteriors are usually intractable, in which case approximations can be employed. One such approximation - variational inference - is computationally efficient when using mini-bat...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Investigating Batch Inference in a Sequential Monte Carlo Framework for Neural Networks*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿçš„ Sequential Monte Carlo (SMC) æ–¹æ³•åœ¨åº”ç”¨äº Neural Network (NN) çš„ Bayesian æ¨æ–­æ—¶ï¼Œé€šå¸¸éœ€è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®¡ç®— likelihood å’Œ gradientï¼Œå¯¼è‡´**è®¡ç®—æˆæœ¬æé«˜**ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡æ¨¡å‹å’Œæ•°æ®é›†ã€‚è€Œè™½ç„¶ mini-batch éšæœºä¼°è®¡å¯ä»¥é™ä½å¼€é”€ï¼Œä½†ç›´æ¥å°†å…¶å¼•å…¥ SMC å®¹æ˜“ç ´åé‡‡æ ·ç¨³å®šæ€§ã€‚

æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€çŸ›ç›¾ï¼šå¦‚ä½•åœ¨ä¿æŒ SMC é‡‡æ ·è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½å…¶è®¡ç®—è´Ÿæ‹…ã€‚

### âœ… æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
æå‡ºå¹¶ç³»ç»Ÿç ”ç©¶äº†å¤šç§ **Data Annealing (DA)** ç­–ç•¥ï¼Œåœ¨ SMC æ¡†æ¶ä¸­é€æ­¥å¼•å…¥ mini-batch æ•°æ®è¿›è¡Œ likelihood å’Œ gradient ä¼°è®¡ï¼š

- **æ¸è¿›å¼æ•°æ®å¼•å…¥æœºåˆ¶**ï¼šä»ä¸€ä¸ªå°çš„åˆå§‹ mini-batch å¼€å§‹è®­ç»ƒï¼Œéšç€è¿­ä»£é€æ­¥æ·»åŠ æ–°çš„ mini-batchï¼Œç›´åˆ°ä½¿ç”¨ full-batchã€‚
- æ¢ç´¢äº†äº”ç§**é¢„å®šä¹‰è°ƒåº¦ç­–ç•¥**ï¼ˆå¦‚ Constantã€Linearã€Constant-to-refineï¼‰ä»¥åŠä¸€ç§åŸºäº posterior ç»Ÿè®¡é‡çš„è‡ªé€‚åº”ç­–ç•¥ï¼š
  - **Smooth DA (SDA)**ï¼šé€šè¿‡æ§åˆ¶ç›®æ ‡åˆ†å¸ƒ Shannon entropy çš„å˜åŒ–ç‡æ¥è‡ªé€‚åº”å†³å®šä½•æ—¶å¢åŠ æ–°æ•°æ®ã€‚

è¯¥æ–¹æ³•ç»“åˆäº† mini-batch çš„é«˜æ•ˆæ€§å’Œ full-batch çš„é«˜ç²¾åº¦ä¼˜åŠ¿ï¼Œåœ¨æ¨ç†æ—©æœŸå¿«é€Ÿç§»åŠ¨ç²’å­è‡³é«˜æ¦‚ç‡åŒºåŸŸï¼ŒåæœŸç²¾ç»†ä¼˜åŒ–ã€‚

### âœ… ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **æ•ˆç‡** | ç›¸æ¯” full-batch SMCï¼Œè®­ç»ƒé€Ÿåº¦æå‡æœ€é«˜è¾¾ **6Ã—**ï¼ŒåŒæ—¶ä»…é€ æˆæå°å‡†ç¡®ç‡æŸå¤±ã€‚ |
| **çµæ´»æ€§** | æ”¯æŒå¤šç§ annealing è°ƒåº¦æ–¹å¼ï¼Œå¯çµæ´»æƒè¡¡é€Ÿåº¦ä¸ç²¾åº¦ã€‚ |
| **å…¼å®¹æ€§** | å¯ä¸æ¢¯åº¦é©±åŠ¨çš„ Markov Kernelï¼ˆå¦‚ HMCã€LDï¼‰æ— ç¼é›†æˆï¼Œé€‚ç”¨äºç°ä»£ NN æ¶æ„ã€‚ |
| **æ— éœ€å˜åˆ†å‡è®¾** | ä¸ä¾èµ–å‚æ•°åŒ–å˜åˆ†åˆ†å¸ƒï¼ˆå¦‚ VIï¼‰ï¼Œé¿å…å› åˆ†å¸ƒæ—é€‰æ‹©ä¸å½“å¸¦æ¥çš„è¿‘ä¼¼åå·®ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- **MNIST**ï¼š70,000 å¼ æ‰‹å†™æ•°å­—ç°åº¦å›¾åƒï¼Œç”¨äº LeNet-5 æ¶æ„ï¼ˆD=61,706 å‚æ•°ï¼‰ã€‚
- **FashionMNIST**ï¼š70,000 å¼ æœè£…ç±»åˆ«ç°åº¦å›¾åƒï¼Œæ›´å…·æŒ‘æˆ˜æ€§ï¼›é‡‡ç”¨æ›´å¤§ CNN æ¶æ„ï¼ˆD=96,658 å‚æ•°ï¼‰ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¡†æ¶å®ç°**ï¼šåŸºäº JAX å®ç°ï¼Œè¿è¡Œäº NVIDIA A100 GPUã€‚
- **è¿­ä»£æ¬¡æ•°**ï¼šå…± 200 iterationsï¼Œé‡å¤ 5 æ¬¡éšæœºç§å­å–å‡å€¼ä¸æ ‡å‡†å·®ã€‚
- **Mini-batch å¤§å°**ï¼šå›ºå®šä¸º $ K = C = 500 $ã€‚
- **Proposal æ–¹æ³•**ï¼š
  - **Langevin Dynamics (LD)**
  - **Hamiltonian Monte Carlo (HMC)**ï¼Œä½¿ç”¨ $ S=3 $ æ­¥ leapfrogã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - Test Lossï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰
  - Test Accuracy (%)
  - Runtime (s)

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ¯”è¾ƒäº†ä»¥ä¸‹ **å…­ç§ batching scheme**ï¼š

| åç§° | æè¿° |
|------|------|
| **Constant** | å›ºå®šä½¿ç”¨ mini-batchï¼ˆæœ€å¿«ä½†æœ€ç²—ç³™ï¼‰ |
| **Full-batch (FB)** | å§‹ç»ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆæœ€ç²¾ç¡®ä½†æœ€æ…¢ï¼‰ |
| **Constant-to-refine (CTR)** | å‰æœŸç”¨ mini-batchï¼Œæœ€åé˜¶æ®µåˆ‡æ¢ä¸º full-batch |
| **Linear** | æ¯æ­¥åŠ ä¸€ä¸ª mini-batchï¼Œçº¿æ€§å¢é•¿è‡³ full-batch |
| **Automated** | ç±»ä¼¼ Linearï¼Œä½†åœ¨ 90% è¿­ä»£å¤„è¾¾åˆ° full-batch |
| **Smooth DA (SDA)** | è‡ªé€‚åº”ç­–ç•¥ï¼Œä¾æ® entropy å˜åŒ–åŠ¨æ€å†³å®šæ˜¯å¦å¢å¤§æ•°æ® |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ‘˜è‡ª Table 1ï¼‰

#### åœ¨ MNIST ä¸Šçš„è¡¨ç°ï¼ˆä»¥ HMC + CTR ä¸ºä¾‹ï¼‰ï¼š
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| Test Accuracy | **98.00% Â± 0.15%** |
| Runtime | **1265.94 s** |
| ç›¸æ¯” FB åŠ é€Ÿæ¯” | ~8.4Ã— |

> æ³¨ï¼šFB çš„ runtime ä¸º 10,658.68 sï¼Œå‡†ç¡®ç‡ä¸º 98.00%ï¼Œè¯´æ˜ CTR å‡ ä¹è¾¾åˆ°ç›¸åŒç²¾åº¦ä½†å¿« 8 å€ä»¥ä¸Šã€‚

#### åœ¨ FashionMNIST ä¸Šè¡¨ç°æœ€ä½³æ–¹æ¡ˆï¼ˆHMC + CTRï¼‰ï¼š
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| Test Accuracy | **89.62% Â± 0.30%** |
| Runtime | **1531.53 s** |
| ç›¸æ¯” FB åŠ é€Ÿæ¯” | ~7Ã— |

---

### ğŸ” ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

| æ–¹æ³• | å‡†ç¡®ç‡ | é€Ÿåº¦ | ç»¼åˆè¡¨ç° |
|------|--------|-------|-----------|
| **Full-batch (FB)** | æœ€é«˜ï¼ˆâ‰ˆ97.5â€“98.0%ï¼‰ | ææ…¢ï¼ˆ~8700â€“10800 sï¼‰ | ç²¾åº¦æœ€ä¼˜ä½†ä¸å®ç”¨ |
| **Constant** | è¾ƒä½ï¼ˆ~92â€“83%ï¼‰ | æå¿«ï¼ˆ~400â€“500 sï¼‰ | å¿«ä½†æ¬ æ‹Ÿåˆ |
| **CTR** | æ¥è¿‘ FBï¼ˆ~98.0%, 89.6%ï¼‰ | ä¸­ç­‰ï¼ˆ~1200â€“1500 sï¼‰ | âœ… **æœ€ä½³æŠ˜è¡·æ–¹æ¡ˆ** |
| **Linear / Automated** | ç•¥ä½äº CTR | æ›´é•¿è¿è¡Œæ—¶é—´ | ä¸å¦‚ CTR é«˜æ•ˆ |
| **SDA** | å±…ä¸­ | æˆæœ¬è¾ƒé«˜ | è‡ªé€‚åº”æœªå¸¦æ¥æ˜æ˜¾æ”¶ç›Š |

> ğŸ’¡ **CTR è¡¨ç°çªå‡º**ï¼šå®ƒåœ¨æœ€åé˜¶æ®µåˆ‡æ¢ä¸º full-batchï¼Œä½¿å¾—æœ€ç»ˆ gradient estimate æ— ç¼©æ”¾è¯¯å·®ï¼Œä»è€Œè·å¾—æ¥è¿‘ FB çš„æ”¶æ•›è´¨é‡ï¼ŒåŒæ—¶èŠ‚çœäº†å‰æœŸå¤§é‡è®¡ç®—èµ„æºã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒä¸åˆ†æï¼ˆéšå«åœ¨ä¸åŒ scheme å¯¹æ¯”ä¸­ï¼‰
- **mini-batch åˆæœŸçš„æœ‰æ•ˆæ€§**ï¼šæ—©æœŸä½¿ç”¨å° batch èƒ½å¿«é€Ÿå¼•å¯¼ç²’å­è¿›å…¥é«˜å¯†åº¦åŒºåŸŸï¼ŒéªŒè¯äº†â€œå…ˆç²—åç²¾â€ç­–ç•¥çš„åˆç†æ€§ã€‚
- **full-batch æ”¶æ•›çš„é‡è¦æ€§**ï¼šä»…é  mini-batchï¼ˆå¦‚ Constantï¼‰æ— æ³•å……åˆ†æ”¶æ•›ï¼Œéœ€åæœŸ full-batch refinement æ‰èƒ½è¾¾åˆ°é«˜æ€§èƒ½ã€‚
- **è‡ªé€‚åº” SDA æ•ˆæœæœ‰é™**ï¼šå°½ç®¡ç†è®ºä¸Šæ›´æ™ºèƒ½ï¼Œä½†å®é™…ä¸­å¹¶æœªè¶…è¶Šç®€å•çº¿æ€§æˆ–ä¸¤é˜¶æ®µç­–ç•¥ï¼Œä¸”å®ç°å¤æ‚ã€è®¡ç®—å¼€é”€å¤§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Data Annealing æ˜¾è‘—åŠ é€Ÿ SMC æ¨ç†**ï¼š
   - æœ€é«˜å®ç° **6â€“8Ã— çš„è®­ç»ƒåŠ é€Ÿ**ï¼ŒåŒæ—¶ä¿æŒæµ‹è¯•å‡†ç¡®ç‡å‡ ä¹ä¸å˜ï¼ˆ<0.5% å·®å¼‚ï¼‰ã€‚
2. **Constant-to-refine (CTR) æ˜¯æœ€ä¼˜ç­–ç•¥ä¹‹ä¸€**ï¼š
   - å‰æœŸä½¿ç”¨ mini-batch å¿«é€Ÿæ¢ç´¢ï¼Œæœ«æœŸ full-batch ç²¾ç»†è°ƒæ•´ï¼Œå®ç°äº†**é€Ÿåº¦ä¸ç²¾åº¦çš„æœ€ä½³å¹³è¡¡**ã€‚
3. **HMC æ˜æ˜¾ä¼˜äº LD**ï¼š
   - å³ä½¿åªä½¿ç”¨ $ S=3 $ æ­¥ leapfrogï¼ŒHMC åœ¨æ‰€æœ‰ setting ä¸‹éƒ½å–å¾—æ›´é«˜å‡†ç¡®ç‡ï¼Œruntime å¢åŠ æœ‰é™ã€‚
4. **è‡ªé€‚åº” SDA å¹¶æœªæ˜¾è‘—èƒœå‡º**ï¼š
   - åŸºäº entropy æ§åˆ¶çš„ SDA è™½å…·ç†è®ºå¸å¼•åŠ›ï¼Œä½†å®è·µä¸­ä¸å¦‚æ‰‹å·¥è®¾è®¡çš„ schedule é«˜æ•ˆã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–äººå·¥è®¾å®šè°ƒåº¦å‚æ•°**ï¼šå¦‚ CTR ä¸­çš„åˆ‡æ¢ç‚¹ï¼ˆ0.9Kï¼‰ã€batch size ç­‰ä»éœ€è°ƒå‚ã€‚
- **SMC æœ¬èº«å†…å­˜æ¶ˆè€—å¤§**ï¼šéœ€ç»´æŠ¤å¤šä¸ª particleï¼ˆå³å¤šä¸ª NN å‰¯æœ¬ï¼‰ï¼Œé™åˆ¶äº†å…¶åœ¨è¶…å¤§è§„æ¨¡æ¨¡å‹ä¸Šçš„åº”ç”¨ã€‚
- **Smooth DA å®ç°å¤æ‚**ï¼šæ¶‰åŠ variance/covariance ä¼°è®¡ï¼Œå¯¹æƒé‡ç¨³å®šæ€§æ•æ„Ÿï¼Œé²æ£’æ€§æœ‰å¾…æé«˜ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **è‡ªé€‚åº” trajectory length**ï¼šæ¢ç´¢åœ¨ HMC proposal ä¸­å¼•å…¥ç±»ä¼¼ NUTS çš„æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´ leapfrog æ­¥æ•°ã€‚
2. **æ›´é«˜æ•ˆçš„ L-kernel è®¾è®¡**ï¼šç»“åˆ approximately optimal L-kernels [14] è¿›ä¸€æ­¥æå‡é‡‡æ ·æ•ˆç‡ã€‚
3. **åˆ†å¸ƒå¼ SMC å®ç°**ï¼šåˆ©ç”¨å¤šè®¾å¤‡å¹¶è¡Œå¤„ç† particlesï¼Œç¼“è§£å†…å­˜å‹åŠ›ã€‚
4. **å°† DA æ€è·¯æ¨å¹¿è‡³å…¶ä»–é‡‡æ ·å™¨**ï¼šå¦‚åº”ç”¨äº SGLDã€SGHMC æˆ– SVGD ç­‰éšæœºæ¢¯åº¦é‡‡æ ·æ–¹æ³•ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯
> æœ¬æ–‡é€šè¿‡å¼•å…¥ **Data Annealing** ç­–ç•¥ï¼Œåœ¨ SMC æ¡†æ¶ä¸­å®ç°äº† **é«˜æ•ˆä¸”å‡†ç¡®çš„ Bayesian NN æ¨æ–­**ï¼Œå…¶ä¸­ **Constant-to-refine æ–¹æ¡ˆä»¥ 6â€“8 å€åŠ é€Ÿé€¼è¿‘ full-batch æ€§èƒ½**ï¼Œä¸ºå¤§è§„æ¨¡è´å¶æ–¯æ·±åº¦å­¦ä¹ æä¾›äº†å¯è¡Œè·¯å¾„ã€‚

</details>

---

### 9. [Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention](https://arxiv.org/abs/2601.21768)

**Authors**: Alon Rozental  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.21768v1  

#### Abstract
Large language models (LLMs) have revolutionized natural language processing, yet they remain constrained by fixed, non-differentiable tokenizers like Byte Pair Encoding (BPE), which hinder end-to-end optimization and adaptability to noisy or domain-specific data. We introduce Zonkey, a hierarchical...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠZonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attentionã€‹æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿ **Large Language Models (LLMs)** å­˜åœ¨ä»¥ä¸‹ç“¶é¢ˆï¼š
- **å›ºå®šã€éå¯å¾®åˆ†çš„ Tokenizer**ï¼ˆå¦‚ BPEï¼‰å¯¼è‡´æ— æ³•ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œéš¾ä»¥é€‚åº”å™ªå£°æ–‡æœ¬æˆ–ç‰¹å®šé¢†åŸŸæ•°æ®ï¼›
- **ç¦»æ•£ token åŒ–é™åˆ¶äº†æ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„åº”ç”¨**ï¼Œå› ä¸ºæ‰©æ•£è¿‡ç¨‹ä¾èµ–è¿ç»­ç©ºé—´ä¸”å¯¹é•¿åº¦æ•æ„Ÿï¼›
- å±‚æ¬¡åŒ–å»ºæ¨¡ä¸­ç¼ºä¹çµæ´»çš„å¯å˜é•¿åº¦å¤„ç†æœºåˆ¶ï¼Œé€šå¸¸ä¾èµ–ç¡¬æˆªæ–­ï¼ˆEOS tokensï¼‰æˆ–å›ºå®šçª—å£ã€‚

Zonkey é’ˆå¯¹ä¸Šè¿°é—®é¢˜æå‡ºäº†ä¸€ç§å…¨æ–°çš„ **å…¨å¯å¾®åˆ†ã€å±‚æ¬¡åŒ–çš„æ‰©æ•£è¯­è¨€æ¨¡å‹æ¡†æ¶**ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹

#### ï¼ˆ1ï¼‰Differentiable Tokenization via Segment Splitter
- å¼•å…¥ **Segment Splitter** ä½œä¸ºå¯å­¦ä¹ çš„ã€æ¦‚ç‡æ€§çš„åˆ†è¯å™¨ï¼Œç›´æ¥ä»å­—ç¬¦çº§è¾“å…¥å­¦ä¹ â€œbeginning-of-sequenceâ€ï¼ˆBOSï¼‰æ¦‚ç‡ï¼›
- åˆ†å‰²å†³ç­–åŸºäºä¸Šä¸‹æ–‡åŠ¨æ€ç”Ÿæˆï¼ˆä¾‹å¦‚ç©ºæ ¼åæ›´å¯èƒ½ä¸ºè¯è¾¹ç•Œï¼Œå¥å·åæ›´å¯èƒ½ä¸ºå¥å­èµ·å§‹ï¼‰ï¼Œæ— éœ€æ˜¾å¼æ ‡æ³¨ï¼›
- åˆ©ç”¨ **existence probabilities** å’Œ **existence shares** å®ç°æ¢¯åº¦åå‘ä¼ æ’­ï¼Œä½¿æ•´ä¸ª tokenization è¿‡ç¨‹å¯å¾®ã€‚

> ğŸ’¡ *è¿™æ˜¯é¦–ä¸ªå°†åˆ†è¯è¿‡ç¨‹å®Œå…¨çº³å…¥ç«¯åˆ°ç«¯è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¹‹ä¸€ã€‚*

#### ï¼ˆ2ï¼‰Probabilistic Attention
- å°†åºåˆ—è§†ä¸ºç†è®ºä¸Šæ— é™é•¿ï¼Œæ¯ä¸ªä½ç½® $k$ å…·æœ‰å­˜åœ¨æ¦‚ç‡ $p_k \in (0,1]$ï¼›
- åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­å¼•å…¥ **existence ratio $\log(p_k / p_q)$** è°ƒæ•´ attention scoresï¼Œå®ç°è½¯æ©ç ï¼ˆsoft maskingï¼‰ï¼›
- æ”¯æŒè‡ªç„¶è¡°å‡è€Œéå¼ºåˆ¶ä½¿ç”¨ EOSï¼Œå…è®¸å˜é‡é•¿åº¦è¾“å‡ºå¹¶ä¿æŒæ¢¯åº¦æµåŠ¨ã€‚

> âš™ï¸ æ­¤æœºåˆ¶ç»Ÿä¸€äº† paddingã€truncation å’Œ variable-length modeling çš„å¤„ç†æ–¹å¼ã€‚

#### ï¼ˆ3ï¼‰Hierarchical Diffusion with DDMM
- æå‡º **Denoising Diffusion Mixed Model (DDMM)**ï¼Œç»“åˆ DDPM çš„ç¨³å®šæ€§ä¸ DDIM çš„é«˜æ•ˆæ€§ï¼›
- åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œå»å™ªé‡å»ºï¼Œæ”¯æŒå¤šå±‚çº§æŠ½è±¡ï¼ˆcharacter â†’ word-like â†’ sentence-likeï¼‰ï¼›
- ä½¿ç”¨ contrastive cosine loss å’Œ mixed-step objective æå‡é‡å»ºè´¨é‡ä¸é²æ£’æ€§ã€‚

#### ï¼ˆ4ï¼‰Differentiable Stitcher
- è®¾è®¡è½»é‡çº§ã€å¯å¾®çš„ **Stitcher æ¨¡å—**ï¼Œç”¨äºæ— ç¼æ‹¼æ¥é‡å æ®µè½ï¼›
- åˆ©ç”¨ cross-attention å¯¹é½é‡å åŒºåŸŸï¼Œçº æ­£å»å™ªè¯¯å·®ï¼Œå¢å¼ºå…¨å±€ä¸€è‡´æ€§ï¼›
- æ”¯æŒé€’å½’å±‚çº§æ„å»ºï¼ˆlevel-$l$ è¾“å‡ºä½œä¸º level-$l+1$ è¾“å…¥ï¼‰ã€‚

#### ï¼ˆ5ï¼‰End-to-End Trainable Pipeline
- æ•´ä¸ªæµç¨‹ä»åŸå§‹å­—ç¬¦åˆ°æ–‡æ¡£çº§è¡¨ç¤ºå‡å¯å¾®ï¼š  
  `Characters â†’ Segment Splitter â†’ Compressor â†’ DDMM Denoising â†’ Stitcher â†’ Higher Level`
- æ‰€æœ‰æ¨¡å—å…±äº«æŸå¤±ç›‘ç£ä¿¡å·ï¼Œå½¢æˆé—­ç¯ä¼˜åŒ–ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ BPE + Transformerï¼‰ | Zonkey |
|------|-------------------------------|--------|
| Tokenization | å›ºå®šã€éå¯å¾®ã€OOV é—®é¢˜ä¸¥é‡ | å¯å­¦ä¹ ã€å¯å¾®ã€è‡ªé€‚åº”åˆ†å‰² |
| åºåˆ—é•¿åº¦ | ä¾èµ– EOS å’Œ padding | è½¯è¡°å‡ï¼Œæ”¯æŒä»»æ„é•¿åº¦ |
| å±‚æ¬¡ç»“æ„ | å¤šé˜¶æ®µè®­ç»ƒæˆ–å›ºå®šè§„åˆ™èšåˆ | ç«¯åˆ°ç«¯é€’å½’å‹ç¼©ä¸é‡å»º |
| æ‰©æ•£æ¨¡å‹é€‚ç”¨æ€§ | ç¦»æ•£ token ä¸å…¼å®¹æ ‡å‡† diffusion | æ½œåœ¨ç©ºé—´è¿ç»­ diffusionï¼Œæ”¯æŒ denoising |
| åŸŸé€‚åº”èƒ½åŠ› | éš¾ä»¥è°ƒæ•´ tokenizer | åŠ¨æ€åˆ†å‰²å¤©ç„¶é€‚åˆå™ªå£°/é¢†åŸŸè¿ç§» |

> âœ… Zonkey æ˜¯è¿ˆå‘ **fully gradient-based LLMs** çš„é‡è¦ä¸€æ­¥ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- ä¸»è¦è®­ç»ƒæ•°æ®ï¼š**Wikipedia è‹±æ–‡è¯­æ–™å­é›†**
- å• GPU è®­ç»ƒç¯å¢ƒï¼Œèµ„æºå—é™ä¸‹éªŒè¯å¯è¡Œæ€§

### âš™ï¸ å®éªŒè®¾ç½®
- **å±‚çº§ç»“æ„**ï¼šå®ç°äº†ä¸¤çº§å±‚æ¬¡ï¼ˆlevel 0: å­—ç¬¦ â†’ ç±»è¯å•å…ƒï¼›level 1: ç±»è¯ â†’ ç±»å¥ï¼‰
- **æœ€å¤§æ®µé•¿åº¦**ï¼ˆ`max_seq_len[l]`ï¼‰è®¾ä¸º 32
- **å‹ç¼©å‘é‡æ•°**ï¼ˆ`num_compression_vectors[l]`ï¼‰è®¾ä¸º 4
- **å™ªå£°è°ƒåº¦**ï¼šçº¿æ€§ scheduleï¼Œä»é«˜å™ªå£°é€æ­¥é™è‡³æ¥è¿‘é›¶
- **è®­ç»ƒç›®æ ‡**ï¼šå¤šä»»åŠ¡è”åˆä¼˜åŒ–ï¼ˆè§ä¸‹æ–‡æŸå¤±å‡½æ•°ï¼‰

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡ï¼ˆQualitativeä¸ºä¸»ï¼‰
ç”±äºæ¨¡å‹æ— å›ºå®š vocabularyã€è¾“å‡ºä¸ºè¿ç»­ latent è¡¨ç¤ºï¼Œ**æ ‡å‡† token-level æŒ‡æ ‡ï¼ˆå¦‚ perplexityï¼‰ä¸å¯ç”¨**ï¼Œå› æ­¤é‡‡ç”¨ï¼š
- **å®šæ€§åˆ†æ**ï¼š
  - ç”Ÿæˆæ–‡æœ¬çš„è¿è´¯æ€§ï¼ˆcoherenceï¼‰
  - æ˜¯å¦å‡ºç°è¯çº§/å¥çº§ç»“æ„ï¼ˆemergent hierarchyï¼‰
  - è‡ªé€‚åº”åˆ†è¯æ˜¯å¦ç¬¦åˆè¯­è¨€ç›´è§‰ï¼ˆå¦‚ç©ºæ ¼/å¥å·å¤„åˆ†å‰²ï¼‰
- **è¾…åŠ©å®šé‡æŒ‡æ ‡**ï¼ˆä»…ç”¨äºè®­ç»ƒç›‘æ§ï¼‰ï¼š
  - Reconstruction lossï¼ˆcontrastive cosine similarityï¼‰
  - MLM lossï¼ˆmasked prediction accuracyï¼‰
  - Average BOS probability
  - Collapse prevention loss

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
æœªæä¾›ä¸ä¸»æµ LLMï¼ˆå¦‚ GPTã€LLaMAï¼‰çš„ç›´æ¥é‡åŒ–æ¯”è¾ƒï¼ŒåŸå› å¦‚ä¸‹ï¼š
- æ¶æ„å·®å¼‚å·¨å¤§ï¼ˆdiffusion vs autoregressiveï¼‰
- è¾“å‡ºå½¢å¼ä¸åŒï¼ˆlatent vectors vs discrete tokensï¼‰

ä½†æ–‡ä¸­æŒ‡å‡ºå…¶è®¾è®¡ä¼˜äºä»¥ä¸‹æ–¹å‘ï¼š
- **ByT5 / CANINE**ï¼šè™½æ“ä½œäºå­—èŠ‚/å­—ç¬¦ï¼Œä½†æ— åŠ¨æ€åˆ†è¯ä¸å±‚æ¬¡æ‰©æ•£
- **Byte Latent Transformer (BLT)**ï¼šè™½æœ‰ learnable patchingï¼Œä½†ä»¥ entropy å¹³è¡¡ä¸ºç›®æ ‡ï¼Œä¸å¦‚ Zonkey ä¸ä¸‹æ¸¸ä»»åŠ¡å¯¹é½
- **Hierarchical Diffusion Language Models (HDLM)**ï¼šä»åŸºäºç¦»æ•£ tokenï¼Œç¼ºä¹å…¨å¯å¾®æ€§

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### âœ… å…³é”®æ€§èƒ½è¡¨ç°ï¼ˆQualitative Resultsï¼‰

| æŒ‡æ ‡ | ç»“æœæè¿° |
|------|----------|
| **æ–‡æœ¬ç”Ÿæˆè´¨é‡** | å¯ä»çº¯å™ªå£°ç”Ÿæˆè¯­æ³•åˆç†ã€ä¸»é¢˜ä¸€è‡´çš„è‹±æ–‡å¥å­ï¼ˆå¦‚ï¼šâ€œQuantum computing harnesses quantum mechanics...â€ï¼‰ |
| **å±‚æ¬¡ç»“æ„æ¶Œç°** | Level 0 è‡ªåŠ¨åœ¨ç©ºæ ¼å¤„åˆ†å‰² â†’ ç±»è¯å•ä½ï¼›Level 1 åœ¨å¥å·ååˆ†å‰² â†’ ç±»å¥å•ä½ï¼Œæ— éœ€ç›‘ç£ |
| **è‡ªé€‚åº”åˆ†è¯å¯è§†åŒ–** | BOS æ¦‚ç‡çƒ­å›¾æ˜¾ç¤ºåœ¨ punctuation å’Œ space å¤„æ˜¾è‘—å‡é«˜ï¼Œè¡¨æ˜è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰² |
| **å˜é‡é•¿åº¦ç”Ÿæˆ** | å¯ç”Ÿæˆä¸åŒé•¿åº¦æ–‡æœ¬ï¼Œå°¾éƒ¨é€šè¿‡ $p_{\text{exist}} < \epsilon$ è‡ªåŠ¨æˆªæ–­ |
| **infilling èƒ½åŠ›** | æ”¯æŒéé¡ºåºè¡¥å…¨ï¼Œå¦‚ç»™å®šå‰ç¼€ â€œBob is aâ€ å’Œåç¼€ â€œplayer in the NBAâ€ï¼ŒæˆåŠŸè¡¥å…¨ä¸º â€œbasketballâ€ |

### ğŸ”¬ æ¶ˆèå®éªŒï¼ˆAblation Studiesï¼‰
å°½ç®¡æœªåˆ—å‡ºå®Œæ•´è¡¨æ ¼ï¼Œä½†æ–‡ä¸­é€šè¿‡æ¨¡å—åŠŸèƒ½è¯´æ˜äº†å„ç»„ä»¶å¿…è¦æ€§ï¼š

| æ¨¡å— | ç§»é™¤åæœ |
|------|--------|
| **Probabilistic Attention** | æ¢¯åº¦ä¸­æ–­ï¼Œvariable-length modeling ä¸ç¨³å®š |
| **Existence Shares** | Splitter å€¾å‘äºé›†ä¸­åœ¨ç®€å•åŒºåŸŸé¢‘ç¹åˆ†å‰²ï¼Œå¿½ç•¥å¤æ‚éƒ¨åˆ† |
| **Mixed-step Objective in DDMM** | å»å™ªè·¯å¾„ä¿å®ˆï¼Œæ”¶æ•›æ…¢ï¼Œéš¾ä»¥è·³è·ƒæ¢å¤ |
| **Stitcher Overlap Losses** | æ®µé—´ä¸ä¸€è‡´å¢åŠ ï¼Œå½±å“é«˜å±‚æŠ½è±¡ç¨³å®šæ€§ |

> å®éªŒè¡¨æ˜ï¼šæ‰€æœ‰æ¨¡å—ååŒä½œç”¨ï¼Œå…±åŒä¿ƒæˆ emergent linguistic structureã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å¯å¾®åˆ† tokenization æ˜¯å¯è¡Œçš„**ï¼šSegment Splitter èƒ½è‡ªå‘å­¦ä¹ ç±»è¯­è¨€è¾¹ç•Œçš„åˆ†å‰²ç­–ç•¥ï¼›
2. **Probabilistic Attention æ”¯æŒæ— é™åºåˆ—å»ºæ¨¡**ï¼šè½¯å­˜åœ¨æ¦‚ç‡æ›¿ä»£ç¡¬ mask/EOSï¼Œæå‡çµæ´»æ€§ä¸å¯å¯¼æ€§ï¼›
3. **å±‚æ¬¡åŒ–æ‰©æ•£å¯åœ¨ latent space æˆåŠŸè¿è¡Œ**ï¼šDDMM å®ç°ç¨³å®šå»å™ªä¸é«˜è´¨é‡é‡å»ºï¼›
4. **emergent hierarchy æ— éœ€ç›‘ç£**ï¼šè¯çº§ä¸å¥çº§æŠ½è±¡åœ¨ reconstruction pressure ä¸‹è‡ªç„¶æµ®ç°ï¼›
5. **end-to-end training æ¨åŠ¨å…¨å±€æœ€ä¼˜**ï¼šå„æ¨¡å—ï¼ˆsplitter, compressor, denoiser, stitcherï¼‰ç›¸äº’æ¿€åŠ±ï¼Œå½¢æˆæ­£åé¦ˆå¾ªç¯ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **å½“å‰ä»…ä¸º proof-of-concept**ï¼š
   - ä»…è®­ç»ƒè‡³ sentence-level coherenceï¼ˆlevel 0â€“1ï¼‰
   - ç¼ºä¹ paragraph æˆ– document çº§åˆ«ç”Ÿæˆèƒ½åŠ›
2. **è®¡ç®—èµ„æºé™åˆ¶**ï¼š
   - å• GPU è®­ç»ƒï¼Œå°šæœªæ‰©å±•åˆ°æ›´å¤§è§„æ¨¡
   - å±‚çº§æ·±åº¦å—é™
3. **ç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°**ï¼š
   - æ— æ³•ä¸ä¸»æµ LLM è¿›è¡Œå…¬å¹³çš„ perplexity æˆ– BLEU å¯¹æ¯”
4. **æ¨ç†é€Ÿåº¦å°šå¾…éªŒè¯**ï¼š
   - å°½ç®¡å®£ç§°å¹¶è¡Œç”Ÿæˆæ½œåŠ›å¤§ï¼Œä½†å®é™… latency æœªæµ‹é‡

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **Scaling to deeper hierarchies**ï¼š
   - æ„å»º paragraph-level åŠä»¥ä¸ŠæŠ½è±¡
   - å®ç° long-context generationï¼ˆ>10k tokensï¼‰
2. **å¤§è§„æ¨¡è®­ç»ƒä¸ benchmarking**ï¼š
   - åœ¨æ›´å¤§è¯­æ–™ä¸Šè®­ç»ƒï¼Œå¼€å±• human evaluation ä¸ automatic metric å¯¹æ¯”
3. **Domain Adaptation å®éªŒ**ï¼š
   - æµ‹è¯•åœ¨ noisy textã€codeã€medical ç­‰é¢†åŸŸçš„é€‚åº”èƒ½åŠ›
4. **Integration with Autoregressive Heads**ï¼š
   - æ¢ç´¢ hybrid generationï¼ˆdiffusion + ARï¼‰ä»¥å…¼é¡¾å¤šæ ·æ€§ä¸ç²¾ç¡®æ€§
5. **Open-source ç”Ÿæ€å»ºè®¾**ï¼š
   - å·²å¼€æºä»£ç ï¼ˆGitHub: [ARozental/Zonkey](https://github.com/ARozental/Zonkey)ï¼‰ï¼Œé¼“åŠ±ç¤¾åŒºå¤ç°ä¸æ”¹è¿›

---

## æ€»ç»“

Zonkey æå‡ºäº†ä¸€ç§é©å‘½æ€§çš„ **å…¨å¯å¾®åˆ†ã€å±‚æ¬¡åŒ–æ‰©æ•£è¯­è¨€æ¨¡å‹æ¶æ„**ï¼Œçªç ´äº†ä¼ ç»Ÿ LLM ä¸­ **å›ºå®š tokenizer ä¸ç¦»æ•£ token çš„é™åˆ¶**ã€‚å®ƒé€šè¿‡ **Segment Splitter + Probabilistic Attention + DDMM + Stitcher** å››å¤§æ ¸å¿ƒç»„ä»¶ï¼Œé¦–æ¬¡å®ç°äº†ä»å­—ç¬¦åˆ°å¥å­çš„ç«¯åˆ°ç«¯å¯å¯¼ pipelineï¼Œå¹¶å±•ç¤ºäº† **emergent linguistic structure** ä¸ **variable-length diffusion generation** çš„æ½œåŠ›ã€‚

è™½ç„¶ç›®å‰ä»å¤„äºåŸå‹é˜¶æ®µï¼Œä½†å…¶è®¾è®¡ç†å¿µä¸ºä¸‹ä¸€ä»£ **scalableã€adaptiveã€çœŸæ­£ gradient-based language models** æŒ‡æ˜äº†æ–°æ–¹å‘ã€‚

</details>

---

### 10. [ECO: Quantized Training without Full-Precision Master Weights](https://arxiv.org/abs/2601.22101)

**Authors**: Mahdi Nikdan, Amir Zandieh, Dan Alistarh, Vahab Mirrokni  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.22101v1  

#### Abstract
Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\textit...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**ECO: Quantized Training without Full-Precision Master Weights**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡åŒ–è®­ç»ƒä¸­ï¼Œå°½ç®¡å‰å‘å’Œåå‘è®¡ç®—å·²å¹¿æ³›é‡‡ç”¨ä½ç²¾åº¦ï¼ˆå¦‚ FP8 æˆ– INT4ï¼‰ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»ä¾èµ–**å…¨ç²¾åº¦ä¸»æƒé‡ï¼ˆfull-precision master weightsï¼‰** æ¥ç´¯ç§¯æ¢¯åº¦æ›´æ–°ã€‚è¿™ç§é«˜ç²¾åº¦å‚æ•°å‰¯æœ¬ï¼ˆé€šå¸¸ä¸º FP32ï¼‰å¸¦æ¥äº†æ˜¾è‘—çš„å†…å­˜å¼€é”€ï¼Œå°¤å…¶æ˜¯åœ¨ **Sparse Mixture of Experts (SMoE)** æ¨¡å‹ä¸­ï¼Œä¸»æƒé‡å’Œä¼˜åŒ–å™¨çŠ¶æ€å…±åŒä¸»å¯¼äº†æ˜¾å­˜å ç”¨ã€‚

è¯¥é—®é¢˜é™åˆ¶äº†é‡åŒ–å¸¦æ¥çš„å®é™…å†…å­˜æ”¶ç›Šâ€”â€”å³ä½¿æ¿€æ´»å€¼å’Œéƒ¨åˆ†è®¡ç®—è¢«é‡åŒ–ï¼Œæ¨¡å‹æƒé‡çš„å†…å­˜å ç”¨ä»æ¥è¿‘åŸå§‹é«˜ç²¾åº¦æ°´å¹³ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šError-Compensating Optimizer (ECO)

ä½œè€…æå‡º **ECOï¼ˆError-Compensating Optimizerï¼‰**ï¼Œä¸€ç§æ— éœ€ä¸»æƒé‡çš„é‡åŒ–è®­ç»ƒæ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> **å°†æ¯æ¬¡æƒé‡é‡åŒ–åäº§ç”Ÿçš„è¯¯å·®æ³¨å…¥åˆ°ä¼˜åŒ–å™¨çš„åŠ¨é‡ç¼“å†²åŒºï¼ˆmomentum bufferï¼‰ä¸­ï¼Œå½¢æˆä¸€ä¸ªâ€œæ— é¢å¤–å†…å­˜å¼€é”€â€çš„è¯¯å·®åé¦ˆï¼ˆerror feedbackï¼‰æœºåˆ¶ã€‚**

å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š
1. åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œç›´æ¥å¯¹é‡åŒ–åçš„å‚æ•°åº”ç”¨ä¼˜åŒ–å™¨æ›´æ–°ï¼ˆå¦‚ SGDM æˆ– Adamï¼‰ã€‚
2. æ›´æ–°åå¾—åˆ°ä¸´æ—¶é«˜ç²¾åº¦å‚æ•°ï¼Œç«‹å³è¿›è¡Œé‡åŒ–ã€‚
3. è®¡ç®—é‡åŒ–è¯¯å·® $ e_{t+1} = \theta_{t+1} - q(\theta_{t+1}) $ã€‚
4. å°†è¯¥è¯¯å·®æŒ‰æ¯”ä¾‹ $ \alpha = (1-\beta)/\eta $ æ³¨å…¥å½“å‰çš„ momentum buffer ä¸­ï¼Œä½¿å¾—ä¸¢å¤±çš„æ›´æ–°ä¿¡æ¯å¾—ä»¥ä¿ç•™å¹¶åœ¨åç»­æ­¥éª¤ä¸­è¡¥å¿ã€‚

è¿™ä¸€è®¾è®¡çš„å…³é”®åœ¨äºï¼š**å¤ç”¨ç°æœ‰çš„ momentum buffer å­˜å‚¨é‡åŒ–è¯¯å·®ï¼Œé¿å…å¼•å…¥é¢å¤–çš„ error bufferï¼Œå®ç°é›¶é¢å¤–å†…å­˜å¼€é”€ã€‚**

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹é¢ | ECO çš„ä¼˜åŠ¿ |
|------|-----------|
| **å†…å­˜æ•ˆç‡** | å®Œå…¨æ¶ˆé™¤ä¸»æƒé‡å­˜å‚¨ï¼Œé™æ€å†…å­˜æœ€å¤šå¯å‡å°‘ **25%**ï¼ˆå°¤å…¶åœ¨ SMoE æ¨¡å‹ä¸­ï¼‰ã€‚ |
| **é€šç”¨æ€§ä¸å…¼å®¹æ€§** | æ”¯æŒä¸»æµä¼˜åŒ–å™¨ï¼ˆSGDMã€Adamï¼‰ï¼Œå¯ä¸ FP8ã€INT4 ç­‰å¤šç§é‡åŒ–æ ¼å¼ç»“åˆï¼Œé€‚ç”¨äº Dense å’Œ MoE æ¶æ„ã€‚ |
| **ç†è®ºä¿éšœ** | åœ¨æ ‡å‡†éå‡¸å‡è®¾ä¸‹ï¼Œè¯æ˜ ECO æ”¶æ•›è‡³æœ€ä¼˜è§£çš„ä¸€ä¸ªå¸¸æ•°åŠå¾„é‚»åŸŸï¼Œä¸”å™ªå£°åœ°æ¿ï¼ˆnoise floorï¼‰ä»…æ¯”ä½¿ç”¨ä¸»æƒé‡çš„æ–¹æ³•å·®ä¸€ä¸ªå¸¸æ•°å› å­ï¼›è€Œæœ´ç´ ç§»é™¤ä¸»æƒé‡ä¼šå¯¼è‡´è¯¯å·®éšå­¦ä¹ ç‡è¡°å‡å‘æ•£ã€‚ |
| **å®ç°ç®€å•** | ä»…éœ€åœ¨æ¯æ­¥æœ«å°¾æ·»åŠ ä¸€æ¬¡ element-wise çš„è¯¯å·®æ³¨å…¥æ“ä½œï¼Œæ— éœ€è°ƒå‚æˆ–å¤æ‚é‡æ„ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **Scaling Law å®éªŒ**ï¼šä½¿ç”¨ `C4` æ•°æ®é›†ï¼Œè®­ç»ƒ token æ•°ä¸ºæ¨¡å‹å‚æ•°é‡çš„ 100 å€ï¼ˆå³ 5Ã— Chinchilla æœ€ä¼˜è§„æ¨¡ï¼‰ã€‚
- **Gemma-3 1B é¢„è®­ç»ƒ**ï¼šåŸºäº `C4` æ•°æ®é›†è®­ç»ƒ 40B tokensã€‚
- **SMoE 2.1B é¢„è®­ç»ƒ**ï¼šåœ¨ `LM1B` æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œtoken æ•°ä¸ºæ´»è·ƒå‚æ•°æ•°é‡çš„ 100 å€ã€‚
- **DeepSeek-MoE-16B å¾®è°ƒ**ï¼šåœ¨ `OpenAssistant-Guanaco` æ•°æ®é›†ä¸Šå¾®è°ƒ 3 è½®ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
| é¡¹ç›® | è®¾ç½®è¯¦æƒ… |
|------|--------|
| **æ¨¡å‹èŒƒå›´** | 30Mâ€“800M å°æ¨¡å‹ã€Gemma-3 1Bã€SMoE 2.1Bã€DeepSeek-MoE-16B |
| **é‡åŒ–é…ç½®** |  
- FP8 å®éªŒï¼šä¸»è¦é’ˆå¯¹ Transformer å—ä¸­çš„çº¿æ€§å±‚ï¼ˆæ’é™¤åµŒå…¥å±‚ï¼‰
- INT4 å®éªŒï¼šç”¨äº DeepSeek-MoE-16B çš„ weight-only QAT
- é‡åŒ–æ–¹å¼ï¼šæ”¯æŒ **Round-to-Nearest (RTN)** å’Œ **Stochastic Rounding (SR)** |
| **ä¼˜åŒ–å™¨** | AdamWï¼ˆ$ \beta_1=0.9, \beta_2=0.98 $ï¼‰ï¼Œå­¦ä¹ ç‡é‡‡ç”¨çº¿æ€§é¢„çƒ­ + ä½™å¼¦é€€ç« |
| **è¯„ä¼°æŒ‡æ ‡** |  
- ä¸»è¦æŒ‡æ ‡ï¼š**Validation Loss**
- ä¸‹æ¸¸ä»»åŠ¡ï¼š**Zero-shot Accuracy**ï¼ˆARC-C, ARC-E, GSM8K, HellaSwag, PIQA, MMLUï¼‰
- å†…å­˜æŒ‡æ ‡ï¼š**Static Memory Usage (MB)**ï¼Œé‡ç‚¹å…³æ³¨å³°å€¼å†…å­˜ä¸­ä¸»æƒé‡å æ¯” |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿åç§° | æè¿° |
|--------|------|
| **BF16 w/ MW** | FP32 ä¸»æƒé‡ + BF16 è®¡ç®—ï¼Œæ ‡å‡†æ··åˆç²¾åº¦åŸºå‡† |
| **FP8 w/ MW + RTN/SR** | FP32 ä¸»æƒé‡ + FP8 å‰å‘ä¼ æ’­ï¼ˆåˆ†åˆ«ç”¨ RTN/SRï¼‰ |
| **FP8 w/o MW + RTN/SR** | æ— ä¸»æƒé‡ï¼Œç›´æ¥é‡åŒ–æ›´æ–°ï¼ˆæœ´ç´ æ–¹æ³•ï¼‰ |
| **FP8 w/o MW ECO + RTN/SR** | æœ¬æ–‡æå‡ºçš„ ECO æ–¹æ³•ï¼ˆå¸¦/ä¸å¸¦åŠ¨é‡è¯¯å·®æ³¨å…¥ï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### âœ… è¡¨æ ¼ 1 & å›¾ 3ï¼šéªŒè¯æŸå¤±ï¼ˆValidation Lossï¼‰å¯¹æ¯”ï¼ˆè¶Šä½è¶Šå¥½ï¼‰

| æ–¹æ³• | 30M | 800M | Gemma-3 1B | SMoE 2.1B | DeepSeek-MoE-16B |
|------|-----|------|------------|-----------|------------------|
| **FP8 w/ MW + SR** | 3.3309 | N/A | ~3.09 | ~2.92 | â€” |
| **FP8 w/o MW + SR** | 3.4008 | 2.9471 | >3.1 (å‘æ•£) | >3.1 (å‘æ•£) | â€” |
| **FP8 w/o MW ECO + SR** | **3.3317** | **2.5399** | **~2.76** | **~2.76** | â€” |
| **INT4 w/o MW ECO + SR** | â€” | â€” | â€” | â€” | **åŒ¹é…ä¸»æƒé‡åŸºçº¿** |

> ğŸ’¡ **è§‚å¯Ÿ**ï¼šECO å‡ ä¹å®Œå…¨æ¢å¤äº†ä¸»æƒé‡æ–¹æ³•çš„ç²¾åº¦ï¼Œè€Œæœ´ç´ ç§»é™¤ä¸»æƒé‡åˆ™ä¸¥é‡å‘æ•£æˆ–æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚

---

#### âœ… è¡¨æ ¼ 2ï¼šDeepSeek-MoE-16B é›¶æ ·æœ¬è¯„æµ‹å‡†ç¡®ç‡

| æ–¹æ³• | ARC-C | ARC-E | GSM8K | HellaSwag | PIQA | MMLU |
|------|-------|-------|--------|-----------|------|------|
| **INT4 w/ MW + SR** | 48.55 | 71.13 | 16.15 | 78.78 | 80.90 | 38.57 |
| **INT4 w/o MW ECO + SR** | **48.55** | **71.17** | **16.00** | **78.84** | **81.50** | **38.41** |

> âœ… **ç»“è®º**ï¼šECO åœ¨ **INT4 ç²¾åº¦ä¸‹å®ç°äº†ä¸ä¸»æƒé‡æ–¹æ³•å‡ ä¹ä¸€è‡´çš„é›¶æ ·æœ¬æ€§èƒ½**ï¼Œéƒ¨åˆ†æŒ‡æ ‡ç”šè‡³ç•¥æœ‰æå‡ã€‚

---

#### âœ… å†…å­˜èŠ‚çœæ•ˆæœï¼ˆå›¾ 1ï¼‰
- **é™æ€å†…å­˜å‡å°‘é«˜è¾¾ 25%**ï¼Œç‰¹åˆ«æ˜¯åœ¨ SMoE æ¨¡å‹ä¸­ï¼ˆå› ä¸“å®¶å‚æ•°å¤šï¼Œä¸»æƒé‡å æ¯”è¾ƒå¤§ï¼‰ã€‚
- ECO æ˜¾è‘—æ”¹å–„äº† **memory vs. validation loss çš„ Pareto frontier**ï¼Œå³åœ¨ç›¸åŒå†…å­˜é¢„ç®—ä¸‹è¾¾åˆ°æ›´ä½æŸå¤±ï¼Œæˆ–åœ¨ç›¸åŒæ€§èƒ½ä¸‹ä½¿ç”¨æ›´å°‘å†…å­˜ã€‚

---

#### ğŸ”¬ æ¶ˆèå®éªŒä¸åˆ†æ
- **è¯¯å·®è¿ç»­æ€§éªŒè¯ï¼ˆå›¾ 2ï¼‰**ï¼šç›¸é‚»æ­¥éª¤é—´çš„é‡åŒ–è¯¯å·® $ e_t $ ä¸ $ e_{t+1} $ å…·æœ‰é«˜åº¦ç›¸ä¼¼æ€§ï¼ˆcosine similarity > 0.999ï¼‰ï¼Œæ”¯æŒäº† ECO ä¸­â€œç”¨ $ e_{t+1} $ è¿‘ä¼¼ $ e_t $â€çš„å¯å‘å¼æœ‰æ•ˆæ€§ã€‚
- **SR vs. RTN å¯¹æ¯”**ï¼šECO åœ¨ä½¿ç”¨ **Stochastic Rounding (SR)** æ—¶è¡¨ç°æœ€ä½³ï¼Œå› å…¶æä¾›æ— åä¼°è®¡ï¼›è€Œ RTN å¼•å…¥ç³»ç»Ÿåå·®ï¼Œå¯¼è‡´æ›´é«˜å™ªå£°åœ°æ¿ï¼ˆä¸ç†è®ºä¸€è‡´ï¼‰ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **ä¸»æƒé‡å¹¶éå¿…éœ€**ï¼šé€šè¿‡å°†é‡åŒ–è¯¯å·®æ³¨å…¥ momentum bufferï¼ŒECO æˆåŠŸå®ç°äº†æ— éœ€ä¸»æƒé‡çš„ç¨³å®šé‡åŒ–è®­ç»ƒã€‚
2. **ECO å®ç°è¿‘æ— æŸå‹ç¼©**ï¼šåœ¨ FP8 å’Œ INT4 ç²¾åº¦ä¸‹ï¼ŒECO å¯ä»¥ **å‡ ä¹å®Œå…¨åŒ¹é…ä¸»æƒé‡åŸºçº¿çš„è®­ç»ƒæŸå¤±å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½**ã€‚
3. **æ˜¾è‘—å†…å­˜èŠ‚çœ**ï¼šåœ¨å…¸å‹è®¾ç½®ä¸‹å¯å‡å°‘ **~25% çš„é™æ€å†…å­˜å ç”¨**ï¼Œç‰¹åˆ«æœ‰åˆ©äºå¤§è§„æ¨¡ MoE æ¨¡å‹éƒ¨ç½²ã€‚
4. **ç†è®ºä¸å®è·µä¸€è‡´**ï¼šç†è®ºè¯æ˜ ECO æ”¶æ•›äºå¸¸æ•°åŠå¾„é‚»åŸŸï¼Œè€Œæœ´ç´ æ–¹æ³•è¯¯å·®ä¼šéšå­¦ä¹ ç‡è¡°å‡è¶‹äºæ— ç©·ï¼Œå®éªŒè¯å®äº†è¿™ä¸€ç‚¹ã€‚

---

### âš ï¸ å±€é™æ€§
1. **ä¾èµ– Stochastic Rounding (SR)**ï¼šECO çš„æœ€å¼ºç†è®ºä¿è¯ä¾èµ–äº SR æä¾›çš„é›¶å‡å€¼è¯¯å·®ç‰¹æ€§ã€‚è‹¥ç¡¬ä»¶ä»…æ”¯æŒ RTNï¼Œåˆ™æ€§èƒ½ä¸Šé™å—é™ã€‚
2. **RTN åœºæ™¯ä»æœ‰å·®è·**ï¼šè™½ç„¶ä¼˜äºæœ´ç´ æ–¹æ³•ï¼Œä½†åœ¨çº¯ RTN è®¾ç½®ä¸‹ä»ç•¥é€Šäºä¸»æƒé‡ + RTN çš„æœ€ä¼˜ç»„åˆã€‚
3. **æœªæ¢ç´¢æç«¯ä½ä½å®½ï¼ˆå¦‚ INT2ï¼‰**ï¼šç›®å‰å®éªŒé›†ä¸­åœ¨ FP8 / INT4ï¼Œæ›´ä½ç²¾åº¦ä¸‹çš„ç¨³å®šæ€§å°šå¾…éªŒè¯ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³å…¶ä»–ä¼˜åŒ–å™¨ç»“æ„**ï¼šå¦‚ Adafactorã€Lion ç­‰ä½å†…å­˜ä¼˜åŒ–å™¨ï¼Œè¿›ä¸€æ­¥é™ä½æ•´ä½“è®­ç»ƒæˆæœ¬ã€‚
2. **ç»“åˆ optimizer state quantization**ï¼šå°† ECO ä¸ 4-bit momentum å‹ç¼©ï¼ˆå¦‚ 4-bit Adamï¼‰ç»“åˆï¼Œå®ç°ç«¯åˆ°ç«¯æä½å†…å­˜è®­ç»ƒã€‚
3. **ç¡¬ä»¶ååŒè®¾è®¡**ï¼šæ¨åŠ¨ SR åœ¨ GPU/NPU ä¸Šçš„åŸç”Ÿæ”¯æŒï¼Œä½¿ ECO æ›´æ˜“è½åœ°ã€‚
4. **åº”ç”¨äºæ›´å¤§è§„æ¨¡æ¨¡å‹**ï¼šå¦‚ 100B+ å‚æ•°çš„ MoE æ¨¡å‹ï¼Œåœ¨çœŸå®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒä¸­éªŒè¯å¯æ‰©å±•æ€§ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯
> **ECO æ˜¯é¦–ä¸ªé€šç”¨ã€é«˜æ•ˆã€æ— éœ€ä¸»æƒé‡çš„å¤§æ¨¡å‹é‡åŒ–è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åŠ¨é‡ç¼“å†²åŒºå¤ç”¨å®ç°è¯¯å·®åé¦ˆï¼Œåœ¨å‡ ä¹ä¸å¤±æ•ˆçš„å‰æä¸‹å‡å°‘é«˜è¾¾ 25% çš„é™æ€å†…å­˜ï¼Œä¸ºä¸‹ä¸€ä»£ä½ç²¾åº¦è®­ç»ƒæä¾›äº†åšå®åŸºç¡€ã€‚**

</details>

---

### 11. [SENDAI: A Hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework](https://arxiv.org/abs/2601.21664)

**Authors**: Xingyue Zhang, Yuxuan Bao, Mars Liyao Gao, J. Nathan Kutz  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.21664v1  

#### Abstract
Bridging the gap between data-rich training regimes and observation-sparse deployment conditions remains a central challenge in spatiotemporal field reconstruction, particularly when target domains exhibit distributional shifts, heterogeneous structure, and multi-scale dynamics absent from available...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSENDAI: A Hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³**åœ¨è§‚æµ‹æåº¦ç¨€ç–ï¼ˆsparse observationsï¼‰ä¸”ç›®æ ‡åŸŸå­˜åœ¨åˆ†å¸ƒåç§»ï¼ˆdistributional shiftï¼‰æ—¶ï¼Œå¦‚ä½•å‡†ç¡®é‡å»ºæ—¶ç©ºåœºï¼ˆspatiotemporal fieldï¼‰**çš„é—®é¢˜ã€‚è¿™åœ¨é¥æ„Ÿé¢†åŸŸå°¤ä¸ºå…³é”®ï¼Œä¾‹å¦‚å«æ˜Ÿå½±åƒå¸¸å› äº‘å±‚é®æŒ¡ã€ä¼ æ„Ÿå™¨æ•…éšœç­‰å¯¼è‡´å¤§é‡æ•°æ®ç¼ºå¤±ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦å¯†é›†è§‚æµ‹æ‰èƒ½å®ç°é«˜è´¨é‡é‡å»ºã€‚

æ­¤å¤–ï¼Œç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•å¾€å¾€ä¾èµ–å¤§è§„æ¨¡GPUè®­ç»ƒã€é«˜å¯†åº¦æ ‡æ³¨æ•°æ®ï¼Œéš¾ä»¥éƒ¨ç½²äºèµ„æºå—é™æˆ–å®æ—¶ç›‘æ§åœºæ™¯ï¼ˆå¦‚å†œä¸šç›‘æµ‹ã€ç¾å®³å“åº”ï¼‰ï¼Œä¸”å…¶éšç©ºé—´è¡¨ç¤ºå¸¸æ··åˆå¤šç§ç‰©ç†æ•ˆåº”ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡ºäº† **SENDAI**ï¼ˆSparse-measurement, EfficieNt Data AssImilationï¼‰æ¡†æ¶ï¼Œä¸€ç§**åˆ†å±‚å¼ç¨€ç–æµ‹é‡æ•°æ®åŒåŒ–æ¶æ„**ï¼Œç»“åˆæ¨¡æ‹Ÿå…ˆéªŒä¸å­¦ä¹ å‹å·®å¼‚æ ¡æ­£ï¼Œä»æç¨€ç–ä¼ æ„Ÿå™¨è§‚æµ‹ä¸­é‡å»ºå®Œæ•´ç©ºé—´çŠ¶æ€ã€‚

å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ä¸‰ä¸ªå±‚é¢ï¼š

#### ï¼ˆ1ï¼‰æç«¯ç¨€ç–æ¡ä»¶ä¸‹çš„æœ‰æ•ˆé‡å»ºï¼ˆExtreme Sparsity Reconstructionï¼‰
- ä»…ä½¿ç”¨ **64ä¸ªä¼ æ„Ÿå™¨**ï¼ˆè¦†ç›–çº¦ **1.56%** çš„ç©ºé—´åŒºåŸŸï¼‰å³å¯å®Œæˆå…¨å›¾é‡å»ºã€‚
- è¿œä½äºä¼ ç»Ÿæ–¹æ³•æ‰€éœ€çš„è§‚æµ‹å¯†åº¦é˜ˆå€¼ï¼Œé€‚ç”¨äºçœŸå®ä¸–ç•Œä¸­ä¼ æ„Ÿå™¨å¸ƒè®¾ç¨€ç–çš„åœºæ™¯ã€‚

#### ï¼ˆ2ï¼‰è½»é‡åŒ–è®¡ç®—è®¾è®¡ï¼ˆComputational Efficiencyï¼‰
- æ¶æ„è½»é‡ï¼Œå¯åœ¨æ ‡å‡†CPUä¸Šè¿è¡Œï¼Œå•ç«™ç‚¹è®­ç»ƒæ—¶é—´ä»…éœ€æ•°åˆ†é’Ÿã€‚
- æ”¯æŒåœ¨è¾¹ç¼˜è®¾å¤‡ã€ä½å¸¦å®½æˆ–æ·±ç©ºæ¢æµ‹ç­‰èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²ã€‚

#### ï¼ˆ3ï¼‰åˆ†å±‚é¢‘ç‡å‰¥ç¦»ç­–ç•¥ï¼ˆHierarchical Frequency Peelingï¼‰
- å°†é‡å»ºä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªäº’è¡¥è·¯å¾„ï¼š
  - **ä½é¢‘è·¯å¾„ï¼ˆLF Pathwayï¼‰**ï¼šåŸºäº **SHRED** æ¡†æ¶ï¼Œåˆ©ç”¨ **LSTM Encoder + æµ…å±‚MLP Decoder** å­¦ä¹ ä¸»å¯¼åŠ¨æ€ï¼Œå¹¶é€šè¿‡ **Latent GAN å¯¹é½** å®ç°æ¨¡æ‹Ÿåˆ°çœŸå®æ•°æ®çš„åˆ†å¸ƒè¿ç§»é€‚åº”ã€‚
  - **é«˜é¢‘è·¯å¾„ï¼ˆHF Pathwayï¼‰**ï¼šå¼•å…¥**é€å±‚é¢‘ç‡å‰¥ç¦»æœºåˆ¶**ï¼Œç»“åˆ **coordinate-based Implicit Neural Representation (INR)** åˆ†ç¦»å¹¶å»ºæ¨¡ç»†ç²’åº¦ç»“æ„ï¼ˆå¦‚è¾¹ç•Œã€æ¢¯åº¦ã€å±€éƒ¨å¼‚å¸¸ï¼‰ã€‚
- å¼•å…¥ **spectral constraint** å’Œ **frequency exclusion mechanism**ï¼Œç¡®ä¿å„å±‚æå–ä¸åŒé¢‘æ®µæˆåˆ†ï¼Œé¿å…æ¨¡å¼æ··å ï¼ˆmode interferenceï¼‰ï¼Œæå‡æ¨¡å‹ç¨³å®šæ€§å’Œå¯è§£é‡Šæ€§ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | SENDAIä¼˜åŠ¿ |
|------|-----------|
| **æ•°æ®æ•ˆç‡** | åœ¨ä»…1.56%è§‚æµ‹ä¸‹ä»èƒ½ä¿æŒé«˜ä¿çœŸé‡å»ºï¼Œè¿œè¶…ä¼ ç»Ÿæ–¹æ³•éœ€æ±‚ |
| **è®¡ç®—æ•ˆç‡** | å¯åœ¨CPUè¿è¡Œï¼Œè®­ç»ƒ/æ¨ç†é€Ÿåº¦å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨ |
| **ç»“æ„ä¿æŒèƒ½åŠ›** | æ˜¾è‘—ä¼˜äºæ’å€¼ç±»æ–¹æ³•ï¼Œèƒ½ä¿ç•™ç”°å—æ‹“æ‰‘ã€åœŸåœ°è¦†ç›–æ–­ç‚¹ã€ç©ºé—´æ¢¯åº¦ç­‰è¯Šæ–­ç›¸å…³ç»“æ„ |
| **å¯è§£é‡Šæ€§** | é«˜é¢‘ä¿®æ­£é¡¹å…·æœ‰è°±åˆ†ç¦»ç‰¹æ€§ï¼Œä¾¿äºåç»­åæ¼”é—´æ¥å˜é‡ï¼ˆå¦‚åœŸå£¤æ¹¿åº¦ã€åœ°è¡¨æ¸©åº¦ï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **MODIS å«æ˜Ÿé¥æ„Ÿæ•°æ®**ï¼ˆTerra/Aquaå¹³å°ï¼‰
- ä¸»è¦çŠ¶æ€å˜é‡ä¸º **NDVI**ï¼ˆNormalized Difference Vegetation Indexï¼‰
- æ•°æ®æ¥æºï¼šGoogle Earth Engine
- æ—¶é—´èŒƒå›´ï¼š2023å¹´
- ç©ºé—´åˆ†è¾¨ç‡ï¼šé‡é‡‡æ ·è‡³ç»Ÿä¸€çš„ **64Ã—64 åƒç´ ç½‘æ ¼**ï¼ˆå¯¹åº”å®é™…é¢ç§¯15kmÃ—15kmï¼‰

### å®éªŒè®¾ç½®
- **ç ”ç©¶åœ°ç‚¹**ï¼šå…¨çƒå…­ä¸ªå¼‚è´¨æ€§æ˜¾è‘—çš„ç«™ç‚¹ï¼Œæ¶µç›–å¤šç§æ°”å€™ç±»å‹ï¼š
  - åœ°ä¸­æµ·æ°”å€™ï¼ˆCentral Valley, Guadalquivir Valleyï¼‰
  - æ¸©å¸¦å¤§é™†æ€§æ°”å€™ï¼ˆCorn Beltï¼‰
  - å¹²æ—±åŒºï¼ˆImperial Valley, Tarim Basinï¼‰
  - äºšçƒ­å¸¦æ°”å€™ï¼ˆRiverinaï¼‰
- **å­£èŠ‚åˆ’åˆ†ç­–ç•¥**ï¼š
  - ä½¿ç”¨ä¸€ä¸ªå­£èŠ‚ä½œä¸ºâ€œæ¨¡æ‹Ÿâ€æ•°æ®ï¼ˆsimulationï¼‰ï¼Œå¦ä¸€ä¸ªå­£èŠ‚ä½œä¸ºâ€œçœŸå®â€è§‚æµ‹ï¼ˆground truthï¼‰
  - åˆ©ç”¨å­£èŠ‚æ›´æ›¿æ¨¡æ‹Ÿ **sim2real domain shift**
- **ä¼ æ„Ÿå™¨é…ç½®**ï¼š
  - å›ºå®šä½¿ç”¨ **64ä¸ªéšæœºå¸ƒç½®çš„ä¼ æ„Ÿå™¨**ï¼ˆå æ€»åƒç´ 1.56%ï¼‰
  - è¾“å…¥ä¸ºä¼ æ„Ÿå™¨å†å²åºåˆ—ï¼ˆtime-delay embeddingï¼Œé•¿åº¦L=5ï¼‰

### è¯„ä¼°æŒ‡æ ‡
- **SSIM**ï¼ˆStructural Similarity Index Measureï¼‰ä¸ºä¸»æŒ‡æ ‡ï¼Œå¼ºè°ƒå¯¹ç©ºé—´ç»“æ„ã€çº¹ç†ã€è¾¹ç•Œçš„ä¿æŒèƒ½åŠ›
- **RMSE** ä½œä¸ºè¾…åŠ©å®šé‡è¯¯å·®æŒ‡æ ‡
- ç‰¹åˆ«æŒ‡å‡ºï¼šæŸäº›æ–¹æ³•ï¼ˆå¦‚Krigingï¼‰å¯èƒ½RMSEè¾ƒä½ä½†SSIMæå·®ï¼Œè¯´æ˜å…¶è¿‡åº¦å¹³æ»‘ã€ç ´åæ‹“æ‰‘ç»“æ„

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æè¿° |
|------|------|------|
| **SG+IDW** | ä¼ ç»Ÿæµç¨‹ | Savitzky-Golayæ»¤æ³¢ + åè·ç¦»åŠ æƒæ’å€¼ |
| **HANTS+IDW** | ä¼ ç»Ÿæµç¨‹ | è°æ³¢åˆ†æå»äº‘ + IDWæ’å€¼ï¼Œæ˜¾å¼å»ºæ¨¡æ¤è¢«å­£èŠ‚æ€§ |
| **Kriging** | åœ°ç»Ÿè®¡å­¦ | é«˜æ–¯è¿‡ç¨‹å›å½’ï¼ˆRBFæ ¸ï¼‰ï¼Œæ¯å¸§ç‹¬ç«‹æ‹Ÿåˆ |
| **MMGN** | æ·±åº¦å­¦ä¹  | åŸºäºGaboræ»¤æ³¢çš„éšå¼ç¥ç»ç½‘ç»œï¼Œæœ€æ–°è¿ç»­åœºé‡å»ºæ–¹æ³• |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… SENDAI Jr.ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…ä½é¢‘è·¯å¾„ï¼‰
é€‚ç”¨äºä½é¢‘ä¸»å¯¼å˜åŒ–åœºæ™¯ï¼ˆå¦‚å¹³ç¨³å†œä¸šåŒºï¼‰ï¼š

| ç«™ç‚¹ | æœ€ä½³åŸºçº¿ SSIM | SENDAI Jr. SSIM | æå‡å¹…åº¦ |
|------|----------------|------------------|---------|
| Central Valley | 0.2612 | **0.5747** | **+120%** |
| Corn Belt | 0.1588 | **0.4530** | **+185.3%** |
| Guadalquivir Valley | 0.1849 | **0.3655** | **+97.7%** |

> ğŸ’¡ åœ¨Corn Beltè¾¾åˆ° **185% SSIMæå‡**ï¼Œæ˜¯æ–‡ä¸­æœ€å¤§æ”¹è¿›ã€‚

#### âœ… å®Œæ•´SENDIAï¼ˆå«é«˜é¢‘å‰¥ç¦»è·¯å¾„ï¼‰
ç”¨äºå¤æ‚åœ°å½¢ä¸é«˜é¢‘åŠ¨æ€åœºæ™¯ï¼š

| ç«™ç‚¹ | Cheap2Rich SSIM | SENDAI SSIM | æå‡å¹…åº¦ |
|------|------------------|-------------|----------|
| Imperial Valley | 0.4041 | **0.4668** | +15.5% |
| Tarim Basin | 0.3505 | **0.4777** | **+36.3%** |
| Riverina | 0.2761 | **0.3354** | +21.5% |

> ğŸ’¡ åœ¨ **Tarim Basin** ä¸Šç›¸æ¯”Cheap2Richæå‡è¾¾ **36.3% SSIM**ï¼Œä½“ç°é«˜é¢‘å‰¥ç¦»çš„æœ‰æ•ˆæ€§ã€‚

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- æ‰€æœ‰åŸºçº¿æ–¹æ³•å‡ä¸¥é‡ç ´åç©ºé—´ç»“æ„ï¼š
  - **IDWç±»æ–¹æ³•** å‡ºç°â€œç‰›çœ¼â€ä¼ªå½±ï¼ˆbullseye artifactsï¼‰
  - **Kriging** å¯¼è‡´è¿‡åº¦å¹³æ»‘ï¼Œå®Œå…¨æŠ¹é™¤è¾¹ç•Œä¿¡æ¯ï¼ˆå¦‚Tarim Basinçš„å±±ç›†äº¤ç•Œï¼‰
- MMGNè™½ä¸ºæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œåœ¨æç«¯ç¨€ç–æ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ï¼ŒSSIMæ™®éä½äº0.13
- SENDAIä¸ä»…SSIMé¢†å…ˆï¼Œ**RMSEä¹ŸåŒæ­¥ä¸‹é™**ï¼Œè¡¨æ˜å…¶ä¸æ˜¯ä»¥ç‰ºç‰²ç²¾åº¦æ¢å–ç»“æ„ä¿çœŸ

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰é¢‘ç‡å‰¥ç¦» vs è”åˆå‘ç°ï¼ˆJoint Discoveryï¼‰
- åœ¨åˆæˆæ—…è¡Œæ³¢ç³»ç»Ÿä¸­æµ‹è¯•ï¼š
  - è”åˆå‘ç°å¯¼è‡´é¢‘è°±èƒ½é‡æ‰©æ•£ï¼Œå‡ºç°éç›®æ ‡æ¨¡å¼æ³„æ¼
  - å±‚æ¬¡å‰¥ç¦»åˆ™è¾“å‡ºé¢‘è°±çº¯å‡€ï¼Œå„å±‚åˆ†åˆ«æ•è·ç›®æ ‡é¢‘ç‡ï¼ˆk=5, k=11ï¼‰ï¼Œæ— å¹²æ‰°
- ç»“è®ºï¼šå±‚æ¬¡ç»“æ„æœ‰åŠ©äº**ç‰©ç†å¯è§£é‡Šæ€§**ä¸**æ¨¡å—åŒ–æ‰©å±•**

#### ï¼ˆ2ï¼‰INRè§£ç å™¨ vs ç›´æ¥MLPæ˜ å°„**
- ç›´æ¥MLPæ˜“äº§ç”Ÿâ€œç‚¹çŠ¶â€ä¼ªå½±ï¼ˆdotted artifactsï¼‰ï¼Œå±€é™äºä¼ æ„Ÿå™¨ä½ç½®é™„è¿‘
- INRç»“åˆ **Fourier positional encoding** åï¼Œç”Ÿæˆç©ºé—´è¿è´¯çš„å¹³æ»‘æ’å€¼ï¼Œæ˜¾è‘—æ”¹å–„è§†è§‰è´¨é‡

#### ï¼ˆ3ï¼‰ä¼ æ„Ÿå™¨æ•°é‡æ•æ„Ÿæ€§åˆ†æ**
- å›¾5aæ˜¾ç¤ºï¼šSSIMéšä¼ æ„Ÿå™¨æ•°é‡å¢åŠ å•è°ƒä¸Šå‡
- ä½†åœ¨ **64ä¼ æ„Ÿå™¨ï¼ˆ1.56%ï¼‰** å·²æ¥è¿‘é¥±å’Œï¼Œè¾¹é™…å¢ç›Šé€’å‡
- è¡¨æ˜è¯¥é…ç½®åœ¨å®ç”¨æ€§ä¸æ€§èƒ½ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **åˆ†å±‚å»ºæ¨¡ä¼˜äºç«¯åˆ°ç«¯å­¦ä¹ **ï¼šå°†é‡å»ºä»»åŠ¡æŒ‰é¢‘ç‡å°ºåº¦æ‹†è§£ï¼ˆLF + HFï¼‰ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰å¤šå°ºåº¦åŠ¨æ€ï¼Œå°¤å…¶é€‚ç”¨äºå…·æœ‰å°–é”è¾¹ç•Œå’Œäºšå­£èŠ‚åŠ¨æ€çš„æ™¯è§‚ã€‚
2. **ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰æ¯”RMSEæ›´é‡è¦**ï¼šå¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ä½œç‰©åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹ï¼‰ï¼Œä¿æŒæ‹“æ‰‘ç»“æ„æ¯”é™ä½åƒç´ çº§è¯¯å·®æ›´å…·ä»·å€¼ã€‚
3. **è½»é‡æ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šé‡å‹DLæ¨¡å‹**ï¼šå°½ç®¡ä¸ä½¿ç”¨Transformeræˆ–å¤§è§„æ¨¡å‚æ•°ç½‘ç»œï¼ŒSENDAIå‡­å€Ÿç»“æ„è®¾è®¡åœ¨ç¨€ç–æ¡ä»¶ä¸‹å…¨é¢è¶…è¶ŠMMGNç­‰å…ˆè¿›INRæ–¹æ³•ã€‚
4. **é«˜é¢‘ä¿®æ­£å…·æœ‰ç‰©ç†æ„ä¹‰**ï¼šåœ¨Tarim Basinæ¡ˆä¾‹ä¸­ï¼ŒHF1åæ˜ æ°”å€™é©±åŠ¨çš„ç‰©å€™åç§»ï¼ŒHF2æ­ç¤ºæ°´æ–‡æŒä¹…æ€§æ§åˆ¶ï¼ŒHF3å¯¹åº”åœŸå£¤å¼‚è´¨æ€§ï¼ŒéªŒè¯äº†åˆ†å±‚å‰¥ç¦»çš„å¯è§£é‡Šæ½œåŠ›ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
1. **å‡è®¾ç©ºé—´ç»“æ„é™æ€**ï¼šå½“å‰æ¡†æ¶å‡è®¾æ¨¡æ‹ŸæœŸä¸çœŸå®æœŸçš„ç©ºé—´æ ¼å±€ä¸å˜ï¼Œæ— æ³•å¤„ç†åœŸåœ°è¦†è¢«å˜åŒ–ã€äººä¸ºæ‰°åŠ¨ç­‰æƒ…å†µã€‚
2. **ä¼ æ„Ÿå™¨å¸ƒå±€éšæœº**ï¼šæœªé‡‡ç”¨ä¿¡æ¯è®ºæŒ‡å¯¼çš„ä¸»åŠ¨é‡‡æ ·ç­–ç•¥ï¼ˆå¦‚åŸºäºä¸ç¡®å®šæ€§æˆ–ç†µæœ€å¤§åŒ–ï¼‰ï¼Œæœªæ¥å¯é€šè¿‡ä¼˜åŒ–å¸ƒç‚¹è¿›ä¸€æ­¥å‡å°‘æ‰€éœ€ä¼ æ„Ÿå™¨æ•°é‡ã€‚
3. **è·¨å¤§é™†æ³›åŒ–å¾…éªŒè¯**ï¼šç›®å‰ä¸ºåŒºåŸŸç‹¬ç«‹è®­ç»ƒï¼Œå°šæœªè¿›è¡Œä¸¥æ ¼çš„è·¨æ´²è¿ç§»æµ‹è¯•ï¼Œé€šç”¨æ€§æœ‰å¾…åŠ å¼ºã€‚
4. **å•å˜é‡é‡å»º**ï¼šå½“å‰ä»…å¤„ç†NDVIï¼Œæœªè€ƒè™‘å¤šå˜é‡è”åˆé‡å»ºï¼ˆå¦‚NDVI + LST + SMï¼‰ï¼Œæœªèƒ½æŒ–æ˜å˜é‡é—´è€¦åˆå…³ç³»ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‹“å±•è‡³éç¨³æ€åœºæ™¯**ï¼šå¼•å…¥æ—¶é—´æ¼”åŒ–ç¼–ç å™¨æˆ–åŠ¨æ€å›¾æ¨¡å‹ï¼Œåº”å¯¹åœŸåœ°åˆ©ç”¨å˜åŒ–ä¸ç®¡ç†å¹²é¢„ã€‚
2. **ä¿¡æ¯å¯¼å‘çš„ä¼ æ„Ÿå™¨è®¾è®¡**ï¼šç»“åˆäº’ä¿¡æ¯æˆ–è´å¶æ–¯å®éªŒè®¾è®¡ï¼ˆBayesian optimal designï¼‰ä¼˜åŒ–ä¼ æ„Ÿå™¨å¸ƒå±€ã€‚
3. **è·¨åŒºåŸŸè¿ç§»å­¦ä¹ **ï¼šæ¢ç´¢å…±äº«latent spaceæˆ–meta-learningç­–ç•¥ï¼Œå®ç°è·¨æ°”å€™åŒºå¿«é€Ÿé€‚é…ã€‚
4. **å¤šå˜é‡è”åˆé‡å»º**ï¼šæ„å»ºmulti-task INRæ¡†æ¶ï¼Œåˆ©ç”¨å˜é‡é—´çš„ç‰©ç†å…³è”æå‡æ•´ä½“é‡å»ºä¸€è‡´æ€§ã€‚
5. **åº”ç”¨äºå…¶ä»–åœ°çƒç§‘å­¦ä»»åŠ¡**ï¼šå¦‚åœŸå£¤æ¹¿åº¦ã€åœ°è¡¨æ¸©åº¦ã€ç§¯é›ªæ·±åº¦ã€æ´ªæ°´åˆ¶å›¾ç­‰ï¼Œæ¨åŠ¨ç‰©ç†å¼•å¯¼çš„ç¨€ç–æ„ŸçŸ¥èŒƒå¼ã€‚

---

## æ€»ç»“
**SENDAI** æå‡ºäº†ä¸€ç§æ–°é¢–ã€é«˜æ•ˆä¸”ç»“æ„æ„ŸçŸ¥çš„ç¨€ç–æ•°æ®åŒåŒ–æ¡†æ¶ï¼ŒæˆåŠŸè§£å†³äº†é¥æ„Ÿä¸­â€œå°‘è§‚æµ‹ + å¤§åç§»â€çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚å…¶å®éªŒç»“æœè¯æ˜ï¼Œåœ¨ä»… **1.56% è§‚æµ‹å¯†åº¦** ä¸‹ï¼Œä»èƒ½å®ç°é«˜è¾¾ **185% SSIM æå‡**ï¼Œå¹¶åœ¨å¤šä¸ªå¼‚è´¨æ€§åœ°è²Œä¸­ç¨³å¥è¡¨ç°ã€‚è¯¥å·¥ä½œä¸ä»…æä¾›äº†å®ç”¨å·¥å…·ï¼Œä¹Ÿä¸ºæœªæ¥èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ™ºèƒ½é¥æ„Ÿç³»ç»Ÿè®¾è®¡æŒ‡æ˜äº†æ–¹å‘ã€‚

</details>

---

### 12. [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)

**Authors**: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.21590v1  

#### Abstract
Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown t...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šScalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰ï¼Œ**Reinforcement Learning (RL)** åè®­ç»ƒæ˜¯æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„ä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è¡¨æ˜ï¼ŒRL çš„æ€§èƒ½å¢ç›Šä¸»è¦æ¥è‡ªäºå¯¹ç”Ÿæˆåˆ†å¸ƒçš„â€œ**Distribution Sharpening**â€ï¼ˆåˆ†å¸ƒé”åŒ–ï¼‰ï¼Œè€ŒéçœŸæ­£ä¹ å¾—æ–°çš„æ¨ç†èƒ½åŠ›ã€‚è¿™æ„å‘³ç€ï¼Œé«˜è´¨é‡çš„æ¨ç†è·¯å¾„å…¶å®å·²ç»å­˜åœ¨äºåŸºç¡€æ¨¡å‹ä¸­ï¼Œåªæ˜¯æ¦‚ç‡è¾ƒä½ã€‚

å°½ç®¡å·²æœ‰ç ”ç©¶æå‡ºé€šè¿‡ä» LLM çš„ **power distribution** ä¸­é‡‡æ ·æ¥å®ç°æ— éœ€è®­ç»ƒçš„æ¨ç†å¢å¼ºï¼ˆå¦‚ Karan & Du, 2025ï¼‰ï¼Œä½†å…¶ä¾èµ– **Markov Chain Monte Carlo (MCMC)** æ–¹æ³•è¿›è¡Œé‡‡æ ·ï¼Œè®¡ç®—æˆæœ¬æé«˜ï¼Œå¯¼è‡´æ¨ç†å»¶è¿Ÿä¸¥é‡ï¼Œéš¾ä»¥åœ¨å®é™…åœºæ™¯ä¸­åº”ç”¨ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **Scalable Power Sampling (SPS)**ï¼Œä¸€ç§**æ— éœ€è®­ç»ƒã€æ— éœ€éªŒè¯å™¨ï¼ˆtraining-free and verifier-freeï¼‰** çš„é«˜æ•ˆæ¨ç†ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ MCMC æ–¹æ³•çš„é«˜å»¶è¿Ÿé—®é¢˜ã€‚

å…¶æ ¸å¿ƒç†è®ºåˆ›æ–°åœ¨äºï¼š
- **å®šç† 3.1**ï¼šè¯æ˜äº†å…¨å±€çš„ power distribution å¯ä»¥è¢«åˆ†è§£ä¸ºä¸€ä¸ª**token-level çš„ç¼©æ”¾ä½æ¸©åº¦åˆ†å¸ƒï¼ˆscaled low-temperature distributionï¼‰**ã€‚
- å…·ä½“è€Œè¨€ï¼Œ`p_power(x_t | q, x_{<t}) âˆ p(x_t | q, x_{<t})^Î± * S_t(x_t)`ï¼Œå…¶ä¸­ `S_t(x_t)` æ˜¯ä¸€ä¸ª**ç¼©æ”¾å› å­ï¼ˆscaling factorï¼‰**ï¼Œå®ƒæ•è·äº†é€‰æ‹©è¯¥ token åå¯¹æœªæ¥è½¨è¿¹è´¨é‡çš„æœŸæœ›ã€‚
- è¿™æ„å‘³ç€ï¼ŒåŸæœ¬éœ€è¦å…¨å±€ä¼˜åŒ–çš„ power samplingï¼Œå¯ä»¥é€šè¿‡åœ¨æ¯ä¸€æ­¥å¯¹ token æ¦‚ç‡è¿›è¡Œå±€éƒ¨é‡åŠ æƒæ¥è¿‘ä¼¼ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ•ˆç‡æé«˜**ï¼šç›¸æ¯” MCMC æ–¹æ³•ï¼Œ**æ¨ç†å»¶è¿Ÿé™ä½äº†è¶…è¿‡ 10 å€**ï¼Œä½¿å…¶å…·å¤‡äº†å®é™…éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚
- **æ€§èƒ½å“è¶Š**ï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼Œæ€§èƒ½**åŒ¹é…ç”šè‡³è¶…è¶Šäº†ä¸€æ¬¡æ€§çš„ GRPO (Group Relative Policy Optimisation)**ï¼Œè€Œ GRPO éœ€è¦å¤æ‚çš„ RL è®­ç»ƒæµç¨‹å’Œå¤–éƒ¨å¥–åŠ±ä¿¡å·ã€‚
- **å®Œå…¨æ— è®­ç»ƒ**ï¼šä¸ä¾èµ–ä»»ä½•å‚æ•°æ›´æ–°æˆ–å¤–éƒ¨å¥–åŠ±ï¼Œä»…é€šè¿‡æ”¹è¿›æ¨ç†æ—¶çš„é‡‡æ ·ç­–ç•¥å³å¯é‡Šæ”¾åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚
- **ä¿ç•™å¤šæ ·æ€§**ï¼šç›¸æ¯” GRPO ç­‰ RL æ–¹æ³•å®¹æ˜“å¯¼è‡´çš„â€œdiversity collapseâ€ï¼ˆå¤šæ ·æ€§å´©æºƒï¼‰ï¼ŒSPS åœ¨æå‡ `pass@1` çš„åŒæ—¶ï¼Œæ›´å¥½åœ°ä¿ç•™äº†ç”Ÿæˆè·¯å¾„çš„å¤šæ ·æ€§ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
å®éªŒåœ¨ä¸‰ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„åŸºå‡†ä¸Šè¿›è¡Œï¼š
- **MATH500**ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œè¯„ä¼°å¤šæ­¥é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œä½¿ç”¨æœ€ç»ˆç­”æ¡ˆç²¾ç¡®åŒ¹é…ä½œä¸ºæŒ‡æ ‡ã€‚
- **HumanEval**ï¼šä»£ç ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…å« 164 ä¸ª Python ç¼–ç¨‹é¢˜ï¼Œé€šè¿‡åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆfunctional correctnessï¼‰è¯„ä¼°ã€‚
- **GPQA-Diamond**ï¼šçŸ¥è¯†å¯†é›†å‹é—®ç­”ä»»åŠ¡ï¼ŒåŒ…å« 198 é“ç”±ä¸“å®¶ç¼–å†™çš„ç”Ÿç‰©ã€ç‰©ç†ã€åŒ–å­¦éš¾é¢˜ï¼Œç”¨äºæŒ‘æˆ˜éä¸“å®¶æ°´å¹³ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹**ï¼šåœ¨å››ä¸ªä¸åŒçš„ LLM ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼š
  - `Qwen2.5-7B`ï¼ˆé€šç”¨æ¨¡å‹ï¼‰
  - `Qwen2.5-Math-7B` å’Œ `DeepSeek-Math-7B`ï¼ˆæ•°å­¦å¾®è°ƒæ¨¡å‹ï¼‰
  - `DeepSeek-Math-7B-RL`ï¼ˆç»è¿‡ GRPO RL å¾®è°ƒçš„æ¨¡å‹ï¼‰
- **è¯„ä¼°æŒ‡æ ‡**ï¼šä¸»è¦ä½¿ç”¨ `pass@1` å‡†ç¡®ç‡ï¼Œå¹¶è¾…ä»¥ `pass@K` åˆ†æå¤šæ ·æ€§å’Œæ¨ç†æ•ˆç‡ï¼ˆæ¯æç¤ºçš„æ¨ç†æ—¶é—´ï¼Œå•ä½ï¼šåˆ†é’Ÿï¼‰ã€‚
- **è¶…å‚æ•°**ï¼šSPS æ–¹æ³•ä¸­ï¼Œpower exponent `Î±=4`ï¼Œå€™é€‰ token æ•° `K_t=8`ï¼ŒMonte Carlo rollout æ•° `M_t=8`ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Base**ï¼šåŸºç¡€æ¨¡å‹çš„æ ‡å‡†è§£ç ã€‚
- **Low-temperature**ï¼šä½æ¸©é‡‡æ · (`T=0.25`)ã€‚
- **Best-of-N**ï¼šç”Ÿæˆ 32 ä¸ªæ ·æœ¬ï¼Œé€‰æ‹©åŸºç¡€æ¨¡å‹å¾—åˆ†æœ€é«˜çš„ä¸€ä¸ªã€‚
- **MCMC Power Sampling**ï¼šKaran & Du (2025) æå‡ºçš„åŸºäº MCMC çš„ power sampling æ–¹æ³•ã€‚
- **GRPO (MATH)**ï¼šåœ¨ MATH æ•°æ®é›†ä¸Šè¿›è¡Œ RL å¾®è°ƒåçš„æ¨¡å‹ï¼Œä½œä¸ºæ€§èƒ½ä¸Šé™çš„å‚ç…§ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
æ ¹æ® **Table 1** çš„ `pass@1` ç»“æœï¼š

| Model / Method | MATH500 | HumanEval | GPQA |
| :--- | :--- | :--- | :--- |
| **Qwen2.5-Math-7B + Ours** | **0.758** | **0.604** | **0.409** |
| Qwen2.5-Math-7B + MCMC | 0.748 | 0.573 | 0.389 |
| Qwen2.5-Math-7B + GRPO | 0.785 | 0.537 | 0.399 |

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **æ€§èƒ½å¯¹æ¯”**ï¼š
  - SPS (**Ours**) åœ¨æ‰€æœ‰æ¨¡å‹å’Œä»»åŠ¡ä¸Šå‡**æ˜¾è‘—ä¼˜äº** `Low-temperature` å’Œ `Best-of-N`ã€‚
  - SPS æ€§èƒ½**åŒ¹é…æˆ–è¶…è¶Š**äº†è®¡ç®—æˆæœ¬é«˜æ˜‚çš„ `MCMC Power Sampling`ã€‚
  - æœ€ä»¤äººç©ç›®çš„æ˜¯ï¼Œåœ¨ `HumanEval` å’Œ `GPQA` ä»»åŠ¡ä¸Šï¼ŒSPS **è¶…è¶Šäº†ç»è¿‡ RL å¾®è°ƒçš„ GRPO æ¨¡å‹**ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚
- **æ•ˆç‡å¯¹æ¯”**ï¼š
  - å¦‚ **Figure 2** æ‰€ç¤ºï¼ŒSPS çš„æ¨ç†æ—¶é—´è¿œä½äº MCMCã€‚ä¾‹å¦‚ï¼Œåœ¨ `Qwen2.5-Math-7B` ä¸Šå¤„ç† MATH500 ä»»åŠ¡ï¼ŒMCMC å¹³å‡è€—æ—¶ **2.5 åˆ†é’Ÿ/æç¤º**ï¼Œè€Œ SPS ä»…éœ€ **0.22 åˆ†é’Ÿ/æç¤º**ï¼Œå®ç°äº†çº¦ **11.4 å€**çš„é€Ÿåº¦æå‡ã€‚
- **å¯¹å·²å¾®è°ƒæ¨¡å‹çš„æœ‰æ•ˆæ€§**ï¼š
  - å¦‚ **Table 2** æ‰€ç¤ºï¼Œå°† SPS åº”ç”¨äºå·²ç»è¿‡ GRPO å¾®è°ƒçš„ `DeepSeek-Math-7B-RL` æ¨¡å‹ï¼Œä¾ç„¶èƒ½å¸¦æ¥å°å¹…ä½†ç¨³å®šçš„æ€§èƒ½æå‡ï¼Œè¯´æ˜ SPS ä¸ RL å¾®è°ƒæ˜¯æ­£äº¤ä¸”å¯å åŠ çš„æŠ€æœ¯ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼ˆFigure 4 & 6ï¼‰ï¼š
  - æ€§èƒ½å¯¹ `K_t` å’Œ `M_t` ç›¸å¯¹é²æ£’ï¼Œä¸­ç­‰å€¼ï¼ˆå¦‚ 8ï¼‰å³å¯å–å¾—è‰¯å¥½æ•ˆæœã€‚
  - æ€§èƒ½å¯¹ `Î±` æ•æ„Ÿï¼Œ`Î±=4` æˆ– `5` æ—¶æ•ˆæœæœ€ä½³ï¼Œ`Î±=8` æ—¶å› è¿‡åº¦é”åŒ–åè€Œæ€§èƒ½ä¸‹é™ã€‚
- **å¤šæ ·æ€§åˆ†æ**ï¼ˆFigure 3 & 5ï¼‰ï¼š
  - GRPO è™½ç„¶æå‡äº† `pass@1`ï¼Œä½†éšç€ `K` å¢å¤§ï¼Œæ€§èƒ½å¢é•¿è¿…é€Ÿé¥±å’Œï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„ **diversity collapse**ã€‚
  - SPS å’Œ MCMC æ–¹æ³•åˆ™ä¿æŒäº†ä¸åŸºç¡€æ¨¡å‹ç›¸ä¼¼çš„ `pass@K` å¢é•¿æ›²çº¿ï¼Œè¯´æ˜å®ƒä»¬åœ¨é”åŒ–åˆ†å¸ƒçš„åŒæ—¶ï¼ŒæˆåŠŸä¿ç•™äº†ç”Ÿæˆçš„å¤šæ ·æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **RL çš„å¢ç›Šæœ¬è´¨æ˜¯åˆ†å¸ƒé”åŒ–**ï¼šè®ºæ–‡çš„å®è¯ç»“æœå¼ºæœ‰åŠ›åœ°æ”¯æŒäº†â€œRL post-training çš„ä¸»è¦ä½œç”¨æ˜¯åˆ†å¸ƒé”åŒ–â€çš„å‡è®¾ã€‚å› ä¸ºä¸€ä¸ªå®Œå…¨æ— è®­ç»ƒã€æ— å¥–åŠ±çš„æ–¹æ³•ï¼ˆSPSï¼‰å°±èƒ½è¾¾åˆ°ç”šè‡³è¶…è¶Š RL æ¨¡å‹çš„æ€§èƒ½ã€‚
2. **åŸºç¡€æ¨¡å‹è•´å«å·¨å¤§æ½œåŠ›**ï¼šé«˜è´¨é‡çš„æ¨ç†èƒ½åŠ›å¹¶é RL â€œæ•™ä¼šâ€æ¨¡å‹çš„ï¼Œè€Œæ˜¯æœ¬æ¥å°±â€œæ½œè—â€åœ¨åŸºç¡€æ¨¡å‹ä¸­ã€‚SPS é€šè¿‡æ›´æ™ºèƒ½çš„é‡‡æ ·ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°â€œè§£é”â€äº†è¿™äº›èƒ½åŠ›ã€‚
3. **é«˜æ•ˆçš„è¿‘ä¼¼æ˜¯å¯è¡Œçš„**ï¼šé€šè¿‡ç†è®ºæ¨å¯¼ï¼Œå¯ä»¥å°†å¤æ‚çš„å…¨å±€ power sampling è¿‘ä¼¼ä¸ºé«˜æ•ˆçš„ã€è‡ªå›å½’çš„ token-level æ“ä½œï¼Œä»è€Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œå½»åº•æ‘†è„± MCMC çš„è¿­ä»£å¼€é”€ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **è®¡ç®—å¼€é”€ä»é«˜äºæ ‡å‡†è§£ç **ï¼šè™½ç„¶æ¯” MCMC å¿« 10 å€ä»¥ä¸Šï¼Œä½† SPS ç”±äºéœ€è¦è¿›è¡Œ Monte Carlo rolloutï¼Œå…¶æ¨ç†æˆæœ¬ä»ç„¶æ˜¾è‘—é«˜äºæ ‡å‡†çš„ `low-temperature` é‡‡æ ·ã€‚
- **ä¾èµ–äºåŸºç¡€æ¨¡å‹çš„è´¨é‡**ï¼šå¦‚æœåŸºç¡€æ¨¡å‹æœ¬èº«å­˜åœ¨ä¸¥é‡çš„å®‰å…¨å¯¹é½é—®é¢˜æˆ–åè§ï¼ŒSPS çš„â€œé”åŒ–â€æ•ˆåº”å¯èƒ½ä¼šæ”¾å¤§è¿™äº›æœ‰å®³è¡Œä¸ºã€‚
- **ç†è®ºè¿‘ä¼¼çš„è¯¯å·®**ï¼šå°½ç®¡æœ‰ç†è®ºæ”¯æ’‘ï¼Œä½†ç”¨æœ‰é™çš„ rollout (`M_t`) æ¥ä¼°è®¡ `S_t(x_t)` ä»ä¼šå¼•å…¥æ–¹å·®å’Œåå·®ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **é™ä½æ–¹å·®**ï¼šæ¢ç´¢ä½¿ç”¨æ§åˆ¶å˜é‡ï¼ˆcontrol variatesï¼‰ç­‰æŠ€æœ¯æ¥å‡å°‘ Monte Carlo ä¼°è®¡çš„æ–¹å·®ã€‚
- **è‡ªé€‚åº”è®¡ç®—é¢„ç®—**ï¼šæ ¹æ®ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´ `K_t` å’Œ `M_t`ï¼Œä»¥å®ç°æ›´å¥½çš„ç²¾åº¦-æ•ˆç‡æƒè¡¡ã€‚
- **ç»“åˆæ¨æµ‹è§£ç ï¼ˆspeculative decodingï¼‰**ï¼šåˆ©ç”¨æ›´å¿«çš„è‰ç¨¿æ¨¡å‹æ¥åŠ é€Ÿ rollout è¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥æ‘Šé”€æˆæœ¬ã€‚
- **æ‰©å±•åˆ° Agent åœºæ™¯**ï¼šå°† SPS åº”ç”¨äºæ›´å¤æ‚çš„åŸºäº LLM çš„æ™ºèƒ½ä½“ï¼ˆagentic settingsï¼‰å†³ç­–è¿‡ç¨‹ä¸­ã€‚

</details>

---

### 13. [MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts](https://arxiv.org/abs/2601.21866)

**Authors**: Evandro S. Ortigossa, Guy Lutsker, Eran Segal  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.21866v1  

#### Abstract
Real-world multivariate time series can exhibit intricate multi-scale structures, including global trends, local periodicities, and non-stationary regimes, which makes long-horizon forecasting challenging. Although sparse Mixture-of-Experts (MoE) approaches improve scalability and specialization, th...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç°å®ä¸–ç•Œä¸­çš„å¤šå˜é‡æ—¶é—´åºåˆ—é€šå¸¸å…·æœ‰å¤æ‚çš„å¤šå°ºåº¦ç»“æ„ï¼Œå¦‚å…¨å±€è¶‹åŠ¿ã€å±€éƒ¨å‘¨æœŸæ€§å’Œéå¹³ç¨³åŠ¨æ€ï¼Œè¿™ä½¿å¾—**é•¿æ—¶ç¨‹é¢„æµ‹**ï¼ˆlong-horizon forecastingï¼‰æå…·æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäº **sparse Mixture-of-Experts (MoE)** çš„æ–¹æ³•è™½ç„¶æå‡äº†å¯æ‰©å±•æ€§å’Œä¸“ä¸šåŒ–èƒ½åŠ›ï¼Œä½†å…¶ä¸“å®¶ç½‘ç»œæ™®éé‡‡ç”¨åŒè´¨åŒ–çš„ **MLP** ç»“æ„ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰æ—¶é—´åºåˆ—ä¸­å¤šæ ·åŒ–çš„åŠ¨æ€æ¨¡å¼ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **MoHETS** â€”â€”ä¸€ç§åŸºäº **encoder-only Transformer** çš„æ–°å‹æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œå…¶æ ¸å¿ƒæ˜¯å¼•å…¥ **Mixture-of-Heterogeneous-Experts (MoHE)** å±‚ï¼Œå®ç°å¯¹å¼‚æ„æ—¶é—´æ¨¡å¼çš„ä¸“ä¸šåŒ–å»ºæ¨¡ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š
- **MoHE æ¶æ„è®¾è®¡**ï¼š
  - å°†æ—¶é—´åºåˆ—åˆ‡åˆ†ä¸º **temporal patches**ï¼Œå¹¶ç”±è·¯ç”±æœºåˆ¶å°†ä¸åŒ patch åˆ†é…ç»™ä¸åŒçš„ä¸“å®¶ç½‘ç»œã€‚
  - **å…±äº«ä¸“å®¶**ï¼ˆshared expertï¼‰ï¼šé‡‡ç”¨ **depthwise-convolution**ï¼ˆDwConvFFNï¼‰ï¼Œè´Ÿè´£å»ºæ¨¡åºåˆ—çº§çš„è¿ç»­æ€§ä¸é•¿æœŸè¶‹åŠ¿ã€‚
  - **è·¯ç”±ä¸“å®¶**ï¼ˆrouted expertsï¼‰ï¼šé‡‡ç”¨ **Fourier-based networks**ï¼ˆFA-FFNï¼‰ï¼Œåœ¨é¢‘åŸŸä¸­æ•æ‰ patch çº§åˆ«çš„å‘¨æœŸæ€§ç»“æ„ã€‚
  - è¿™ç§â€œå…±äº« + è·¯ç”±â€çš„å¼‚æ„ä¸“å®¶ç»„åˆå®ç°äº†å¯¹å…¨å±€è¶‹åŠ¿ä¸å±€éƒ¨å‘¨æœŸæ€§çš„ç»“æ„æ€§è§£è€¦ã€‚

- **å¤–ç”Ÿå˜é‡èåˆæœºåˆ¶**ï¼š
  - å¼•å…¥ **multimodal cross-attention** æ¨¡å—ï¼Œé€šè¿‡å¯¹å¤–ç”Ÿåå˜é‡ï¼ˆcovariatesï¼‰è¿›è¡Œ patch embeddingï¼Œå¹¶ä¸ä¸»åºåˆ—è¿›è¡Œè·¨æ¨¡æ€æ³¨æ„åŠ›äº¤äº’ï¼Œå¢å¼ºæ¨¡å‹å¯¹éå¹³ç¨³åŠ¨æ€çš„é²æ£’æ€§ã€‚

- **è½»é‡åŒ–è§£ç å™¨è®¾è®¡**ï¼š
  - æ›¿ä»£ä¼ ç»Ÿçš„å‚æ•°å¯†é›†å‹çº¿æ€§æŠ•å½±å¤´ï¼ˆlinear projection headï¼‰ï¼Œæå‡º **convolutional patch decoder**ï¼Œåˆ©ç”¨è½»é‡å·ç§¯ç»“æ„è¿›è¡Œä¸Šé‡‡æ ·ï¼Œæå‡å‚æ•°æ•ˆç‡ã€å‡å°‘è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œå¹¶æ”¯æŒå•ä¸€æ¨¡å‹æ³›åŒ–äºä»»æ„é¢„æµ‹é•¿åº¦ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›**ï¼šå¼‚æ„ä¸“å®¶è®¾è®¡ä½¿æ¨¡å‹èƒ½é’ˆå¯¹ä¸åŒæ—¶é—´æ¨¡å¼ï¼ˆè¶‹åŠ¿ vs å‘¨æœŸï¼‰ä½¿ç”¨æœ€é€‚åˆçš„ç®—å­ï¼Œé¿å…äº† MLP å¯¹å¤æ‚ä¿¡å·å»ºæ¨¡çš„â€œä¸€åˆ€åˆ‡â€é—®é¢˜ã€‚
- **æ›´é«˜çš„å‚æ•°æ•ˆç‡ä¸ç¨³å®šæ€§**ï¼šå·ç§¯è§£ç å™¨æ˜¾è‘—å‡å°‘äº†è¾“å‡ºå±‚å‚æ•°é‡ï¼Œç¼“è§£è¿‡æ‹Ÿåˆé£é™©ã€‚
- **æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›**ï¼šæ”¯æŒä»»æ„é¢„æµ‹é•¿åº¦çš„è‡ªå›å½’æ»šåŠ¨é¢„æµ‹ï¼Œæ— éœ€ä¸ºæ¯ä¸ª horizon å•ç‹¬è®­ç»ƒæ¨¡å‹ã€‚
- **æ›´å¼ºçš„é²æ£’æ€§**ï¼šç»“åˆå¤–ç”Ÿå˜é‡ä¸ Huber lossï¼Œæœ‰æ•ˆåº”å¯¹å¼‚å¸¸å€¼ä¸éå¹³ç¨³æ€§ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
åœ¨ **7 ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¤šå˜é‡æ—¶é—´åºåˆ—åŸºå‡†æ•°æ®é›†** ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ¶µç›–å¤šç§é¢†åŸŸä¸é¢‘ç‡ï¼š
- **ç”µåŠ›è´Ÿè·**ï¼šETTh1, ETTh2ï¼ˆå°æ—¶çº§ï¼‰ã€ETTm1, ETTm2ï¼ˆ15åˆ†é’Ÿçº§ï¼‰
- **æ°”è±¡æ•°æ®**ï¼šWeatherï¼ˆ10åˆ†é’Ÿçº§ï¼Œ21ç»´ï¼‰
- **ç”¨ç”µé‡**ï¼šECLï¼ˆå°æ—¶çº§ï¼Œ321ç»´ï¼‰
- **äº¤é€šæµé‡**ï¼šTrafficï¼ˆå°æ—¶çº§ï¼Œ862ç»´ï¼‰

è¿™äº›æ•°æ®é›†å…·æœ‰ä¸åŒçš„ç»´åº¦ã€é¢‘ç‡å’Œå¯é¢„æµ‹æ€§ï¼ˆforecastabilityï¼‰ï¼Œè¦†ç›–äº†ä»ä½ç»´åˆ°é«˜ç»´ã€ä»ç»†ç²’åº¦åˆ°ç²—ç²’åº¦çš„å¤šæ ·åŒ–åœºæ™¯ã€‚

### å®éªŒè®¾ç½®
- **è¾“å…¥é•¿åº¦**ï¼ˆlook-back windowï¼‰ï¼šç»Ÿä¸€è®¾ä¸º $ L = 672 $ï¼ˆçº¦4å‘¨çš„å°æ—¶æ•°æ®ï¼‰ï¼Œä»¥å¹³è¡¡ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è®¡ç®—å¼€é”€ã€‚
- **é¢„æµ‹é•¿åº¦**ï¼ˆforecasting horizonï¼‰ï¼šæµ‹è¯• $ H \in \{96, 192, 336, 720\} $ å¤šä¸ªé•¿æ—¶ç¨‹è®¾å®šã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **MSE**ï¼ˆMean Squared Errorï¼‰
  - **MAE**ï¼ˆMean Absolute Errorï¼‰
- **è®­ç»ƒç­–ç•¥**ï¼š
  - ä½¿ç”¨ **Huber loss** ä½œä¸ºä¸»æŸå¤±å‡½æ•°ï¼Œæå‡å¯¹å¼‚å¸¸å€¼çš„é²æ£’æ€§ã€‚
  - å¼•å…¥ **auxiliary load balancing loss** é˜²æ­¢ MoE ä¸­çš„è·¯ç”±å´©æºƒï¼ˆrouting collapseï¼‰ã€‚
  - é‡‡ç”¨ **autoregressive rolling forecasting** è¿›è¡Œæ¨ç†ï¼Œæ”¯æŒä»»æ„é•¿åº¦é¢„æµ‹ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
å…±å¯¹æ¯”äº† **15 ç§å…ˆè¿›åŸºçº¿æ¨¡å‹**ï¼ŒåŒ…æ‹¬ï¼š
- **Transformer ç±»**ï¼šiTransformer, PatchTST, Crossformer, FEDformer, TimeXer, Timer-XL
- **MoE ç±»**ï¼šTime-MoE, Moirai-MoE
- **å…¶ä»– SOTA æ¨¡å‹**ï¼šSOFTS, TimeMixer, TimesNet, TiDE, DLinear
- **å¤§æ¨¡å‹/åŸºç¡€æ¨¡å‹**ï¼šTime-MoE, Moirai, MOMENT, Chronos

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
- åœ¨ **æ‰€æœ‰ 7 ä¸ªæ•°æ®é›†å’Œå¤šä¸ªé¢„æµ‹é•¿åº¦ä¸‹**ï¼ŒMoHETS å‡å–å¾— **state-of-the-art æ€§èƒ½**ã€‚
- å¹³å‡ **MSE ç›¸æ¯”æœ€å¼ºåŸºçº¿é™ä½ 12%**ï¼ˆåŸæ–‡ç§° 12.3%ï¼‰ã€‚
- å…·ä½“æå‡ç¤ºä¾‹ï¼š
  - åœ¨ **ETTh1** ä¸Šï¼Œç›¸æ¯” TimeXerï¼Œå¹³å‡ MSE ä¸‹é™ **12.3%**
  - åœ¨ **Weather** ä¸Šï¼Œç›¸æ¯” TimeMixerï¼Œä¸‹é™ **10%**
  - åœ¨ **Traffic** ä¸Šï¼Œç›¸æ¯” SOFTSï¼Œä¸‹é™ **5.1%**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
| æ¨¡å‹ | å¹³å‡ MSE | æ’å |
|------|--------|-----|
| **MoHETS** | **0.297** | **1st** |
| SOFTS | 0.334 | 2nd |
| TimeXer | 0.334 | 3rd |
| iTransformer | 0.342 | 4th |
| TimeMixer | 0.353 | 5th |

> MoHETS åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå‡æ’åç¬¬ä¸€ï¼Œä¸”åœ¨å¼ºå­£èŠ‚æ€§æ•°æ®ï¼ˆå¦‚ ETTh1, ETTm2ï¼‰ä¸Šä¼˜åŠ¿å°¤ä¸ºæ˜æ˜¾ã€‚

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰Transformer ä¸»å¹²ç»“æ„
- **Encoder-only** æ˜¾è‘—ä¼˜äº decoder-only è®¾è®¡ï¼š
  - åœ¨ ETTh1 ä¸Š MSE é™ä½ **8.1%**
  - åœ¨ Traffic ä¸Šé™ä½ **9.6%**
- åŸå› ï¼šencoder æ›´é€‚åˆå¤„ç†é•¿ä¸Šä¸‹æ–‡ï¼Œä¸”æ›´çµæ´»åœ°æ”¯æŒå˜é•¿è¾“å‡ºã€‚

#### ï¼ˆ2ï¼‰MoHE ä¸“å®¶é…ç½®
| é…ç½® | ETTh1 MSE | Traffic MSE |
|------|----------|-----------|
| All MLP-FFN (æ ‡å‡† MoE) | 0.399 | 0.498 |
| **MoHETS (DwConv + FA-FFN)** | **0.383** | **0.406** |
| All FA-FFN | 0.391 | 0.428 |
| Conv + MLP | 0.398 | 0.435 |

> å¼‚æ„ä¸“å®¶ç»„åˆåœ¨ ETTh1 ä¸Šç›¸å¯¹å…¨ MLP æå‡ **4.0%**ï¼Œåœ¨ Traffic ä¸Šæå‡ **18.5%**ï¼ŒéªŒè¯äº†å…¶äº’è¡¥æ€§ã€‚

#### ï¼ˆ3ï¼‰å½’ä¸€åŒ–ç­–ç•¥
- **æ··åˆå½’ä¸€åŒ–**ï¼ˆinput/output ä½¿ç”¨ GroupNormï¼Œå…¶ä½™ç”¨ RMSNormï¼‰è¡¨ç°æœ€ä½³ã€‚
- ç›¸æ¯”å…¨ RMSNormï¼Œåœ¨ ETTh1 ä¸Š MSE æ”¹å–„ **12.1%**ã€‚

#### ï¼ˆ4ï¼‰è¾“å‡ºå¤´è®¾è®¡
- **å·ç§¯è§£ç å™¨**ï¼ˆconv-based headï¼‰æ˜¾è‘—ä¼˜äºä¼ ç»Ÿ MLP æŠ•å½±å¤´ï¼š
  - åœ¨å¤šæ•°ä»»åŠ¡ä¸Š MSE æ›´ä½
  - å‚æ•°é‡å‡å°‘ **42%**
  - è®­ç»ƒæ›´ç¨³å®šï¼ˆloss æ›²çº¿æ›´å¹³æ»‘ï¼‰

#### ï¼ˆ5ï¼‰å¤–ç”Ÿå˜é‡å½±å“
- åŠ å…¥æ—¥å†ç­‰åå˜é‡åæ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨ä½ç»´æ•°æ®ï¼ˆå¦‚ ETTï¼‰ä¸Šæ›´æ˜¾è‘—ã€‚
- è¡¨æ˜è¯­ä¹‰ä¿¡æ¯ï¼ˆå¦‚èŠ‚å‡æ—¥ï¼‰å¯¹å»ºæ¨¡éå¹³ç¨³æ€§è‡³å…³é‡è¦ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å¼‚æ„ä¸“å®¶ä¼˜äºåŒè´¨ä¸“å®¶**ï¼šå°† **depthwise convolution** ç”¨äºå»ºæ¨¡è¶‹åŠ¿ã€**Fourier ç½‘ç»œ** ç”¨äºå»ºæ¨¡å‘¨æœŸæ€§ï¼Œæ˜¯ä¸€ç§ç¬¦åˆæ—¶é—´åºåˆ—æœ¬è´¨çš„æ¶æ„è®¾è®¡ï¼Œæ˜¾è‘—ä¼˜äºç»Ÿä¸€ä½¿ç”¨ MLP çš„ MoEã€‚
2. **MoHE å®ç°äº†æœ‰æ•ˆçš„ä¸“ä¸šåŒ–**ï¼šè·¯ç”±æœºåˆ¶èƒ½å¤Ÿè‡ªåŠ¨å°†é«˜é¢‘ patch åˆ†é…ç»™ Fourier ä¸“å®¶ï¼Œä½é¢‘/è¶‹åŠ¿ patch ä¿ç•™ç»™å…±äº«å·ç§¯è·¯å¾„ï¼Œå½¢æˆâ€œåŠŸèƒ½åˆ†å·¥â€ã€‚
3. **è½»é‡å·ç§¯è§£ç å™¨æ›´é«˜æ•ˆç¨³å®š**ï¼šç›¸æ¯”çº¿æ€§å¤´ï¼Œå·ç§¯è§£ç å™¨ä¸ä»…å‚æ•°æ›´å°‘ï¼Œè¿˜èƒ½ä¿æŒå±€éƒ¨ç»“æ„ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
4. **å¤–ç”Ÿå˜é‡ + Huber loss æå‡é²æ£’æ€§**ï¼šäºŒè€…å…±åŒå¢å¼ºäº†æ¨¡å‹å¯¹å™ªå£°ã€å¼‚å¸¸å€¼å’Œåˆ†å¸ƒåç§»çš„é€‚åº”èƒ½åŠ›ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æ¨ç†å»¶è¿Ÿé—®é¢˜**ï¼šåœ¨é«˜ç»´æ•°æ®ï¼ˆå¦‚ Trafficï¼‰å’Œè¶…é•¿é¢„æµ‹ï¼ˆH=720ï¼‰æ—¶ï¼Œæ¨ç†é€Ÿåº¦ä»è¾ƒæ…¢ã€‚
- **æœªä½¿ç”¨é¢„è®­ç»ƒ**ï¼šå½“å‰ MoHETS æ˜¯ä»é›¶è®­ç»ƒï¼Œæœªæ¢ç´¢åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šçš„é¢„è®­ç»ƒæ½œåŠ›ã€‚
- **å¯¹ patch é•¿åº¦æ•æ„Ÿ**ï¼šæœ€ä¼˜ patch size ä¾èµ–äºæ•°æ®é¢‘ç‡ï¼Œéœ€æ‰‹åŠ¨è°ƒå‚ã€‚
- **ç¼ºä¹å¯¹ç¼ºå¤±åå˜é‡çš„å¤„ç†æœºåˆ¶**ï¼šå½“å‰å‡è®¾åå˜é‡å®Œæ•´å¯ç”¨ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **å¼•å…¥ KV Caching**ï¼šå€Ÿé‰´ NLP ä¸­çš„ç¼“å­˜æœºåˆ¶ï¼ŒåŠ é€Ÿé•¿åºåˆ—è‡ªå›å½’æ¨ç†ã€‚
- **å¤šåˆ†è¾¨ç‡æŠ•å½±å¤´**ï¼šä¸ºä¸åŒé¢„æµ‹é•¿åº¦è®¾è®¡ä¸“ç”¨è§£ç å™¨ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–ç²¾åº¦ä¸æ•ˆç‡æƒè¡¡ã€‚
- **è‡ªé€‚åº”åå˜é‡å¤„ç†**ï¼šå¼€å‘èƒ½å¤„ç†ç¼ºå¤±æˆ–é”™ä½å¤–ç”Ÿå˜é‡çš„æ¨¡å—ã€‚
- **æ‰©å±• MoHE æ¶æ„**ï¼šå°è¯•å¼•å…¥å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æˆ–æ³¨æ„åŠ›ä¸“å®¶ï¼Œå»ºæ¨¡æ›´å¤æ‚çš„æ—¶ç©ºä¾èµ–ã€‚
- **å¤§è§„æ¨¡é¢„è®­ç»ƒ**ï¼šåœ¨ç±»ä¼¼ Time-MoE æˆ– TimesFM çš„æµ·é‡æ•°æ®ä¸Šé¢„è®­ç»ƒ MoHETSï¼Œæ¢ç´¢é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚

--- 

> **æ€»ç»“**ï¼šMoHETS é€šè¿‡ **MoHE**ã€**cross-attention with covariates** å’Œ **convolutional patch decoder** ä¸‰å¤§åˆ›æ–°ï¼Œåœ¨é•¿æ—¶ç¨‹å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çªç ´ï¼Œä¸ä»…æ€§èƒ½é¢†å…ˆï¼Œè€Œä¸”è®¾è®¡ç†å¿µæ›´è´´åˆæ—¶é—´åºåˆ—çš„æœ¬è´¨ç‰¹æ€§ï¼Œä¸ºæœªæ¥çš„æ—¶é—´åºåˆ—å»ºæ¨¡èŒƒå¼æä¾›äº†é‡è¦å¯ç¤ºã€‚

</details>

---

### 14. [TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2601.21711)

**Authors**: Huiyuan Lai, Malvina Nissim  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.21711v1  

#### Abstract
Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with re...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨é‡‡ç”¨é•¿é“¾å¼æ€ç»´ï¼ˆlong Chain-of-Thought, CoTï¼‰æ—¶ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š

- **è®­ç»ƒæ•ˆç‡ä½**ï¼šå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒéœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºï¼ˆå¦‚æ•°ä¸‡ A100 GPU å°æ—¶ï¼‰ï¼Œå°¤å…¶åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ã€‚
- **æ¨ç†å†—ä½™ï¼ˆOverthinkingï¼‰**ï¼šæ¨¡å‹å¸¸ç”Ÿæˆè¿‡é•¿ä¸”åŒ…å«å†—ä½™æ­¥éª¤çš„æ€è€ƒè¿‡ç¨‹ï¼Œå¯¼è‡´æ¨ç† token æ¶ˆè€—è¿‡é«˜ï¼Œå½±å“æ•ˆç‡ã€‚

æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨æå‡æ•ˆç‡çš„åŒæ—¶å¾€å¾€ç‰ºç‰²å‡†ç¡®æ€§ï¼Œæˆ–é™åˆ¶ç”¨æˆ·å¯¹æ¨ç†æ¨¡å¼çš„é€‰æ‹©è‡ªç”±ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡º **TACLer**ï¼ˆTailored Curriculum Reinforcement Learningï¼‰ï¼Œä¸€ç§é¢å‘é«˜æ•ˆæ¨ç†çš„æ–°å‹æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰**Tailored Curriculum Learningï¼ˆå®šåˆ¶åŒ–è¯¾ç¨‹å­¦ä¹ ï¼‰**
- ä¸åŒäºä¼ ç»ŸåŸºäºè¾“å…¥é•¿åº¦æˆ–äººå·¥æ ‡æ³¨éš¾åº¦çš„è¯¾ç¨‹å­¦ä¹ ï¼ŒTACLer æ ¹æ®æ¨¡å‹è‡ªèº«åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨ç°åŠ¨æ€è¯„ä¼°â€œå›°éš¾åº¦â€ã€‚
- å°†è®­ç»ƒæ ·æœ¬åˆ†ä¸ºä¸‰ç±»ï¼š
  - G1ï¼šç­”æ¡ˆæ­£ç¡®
  - G2ï¼šæ¨ç†å®Œæ•´ä½†ç­”æ¡ˆé”™è¯¯
  - G3ï¼šæ¨ç†è¢«æˆªæ–­ï¼ˆæœªå®Œæˆï¼‰
- åœ¨å¤šé˜¶æ®µ RL è®­ç»ƒä¸­ï¼Œé€æ­¥å¼•å…¥æ›´éš¾çš„æ•°æ®ï¼ˆä» G1/G2 â†’ G3ï¼‰ï¼Œå®ç°â€œç”±æ˜“åˆ°éš¾â€çš„æ¸è¿›å¼å­¦ä¹ ï¼Œæ˜¾è‘—æå‡å­¦ä¹ æ•ˆç‡ã€‚

#### ï¼ˆ2ï¼‰**Hybrid Thinking/NoThinking æ¨ç†èŒƒå¼**
- æ”¯æŒä¸¤ç§æ¨ç†æ¨¡å¼å…±å­˜ï¼š
  - **Thinking Mode**ï¼šç”Ÿæˆè¯¦ç»† CoT è¿‡ç¨‹ï¼Œè¿½æ±‚é«˜å‡†ç¡®ç‡ã€‚
  - **NoThinking Mode**ï¼šç›´æ¥è¾“å‡ºç®€æ´è§£ç­”ï¼Œè¿½æ±‚é«˜æ•ˆç‡ã€‚
- é€šè¿‡è”åˆè®­ç»ƒä¸¤ç§æ¨¡å¼ï¼Œå®ç°â€œå‹ç¼©æ•ˆåº”â€ï¼ˆcompression effectï¼‰ï¼šThinking æ¨¡å¼çš„é«˜è´¨é‡æ¨ç†èƒ½åŠ›è¢«æç‚¼åˆ° NoThinking æ¨¡å¼ä¸­ï¼Œä½¿å…¶åœ¨çŸ­å“åº”ä¸‹ä»ä¿æŒé«˜ç²¾åº¦ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | TACLer ä¼˜åŠ¿ |
|------|-------------|
| **è®­ç»ƒæ•ˆç‡** | å‡å°‘è¶…è¿‡ 50% çš„è®­ç»ƒè®¡ç®—é‡ï¼ˆvs. DeepScaleR/FastCuRLï¼‰ |
| **æ¨ç†æ•ˆç‡** | æ¨ç† token ä½¿ç”¨å‡å°‘ >42% |
| **å‡†ç¡®æ€§** | å¹³å‡å‡†ç¡®ç‡æå‡ >9%ï¼ˆvs. åŸºçº¿ R1-Qwenï¼‰ |
| **çµæ´»æ€§** | ç”¨æˆ·å¯æŒ‰éœ€é€‰æ‹© Thinking æˆ– NoThinking æ¨¡å¼ |
| **é€šç”¨æ€§** | æ–¹æ³•ä¸ä¾èµ–ç»å¯¹éš¾åº¦å®šä¹‰ï¼Œæ›´å…·æ¨¡å‹è‡ªé€‚åº”æ€§ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- **è®­ç»ƒæ•°æ®**ï¼š`DeepScaleR-Dataset`ï¼Œçº¦ 40k æ¥è‡ª AIME 1983â€“2023 ç­‰ç«èµ›çš„æ•°å­¦é¢˜ã€‚
- **æµ‹è¯•æ•°æ®é›†**ï¼ˆå››å¤§æ•°å­¦æ¨ç†åŸºå‡†ï¼‰ï¼š
  - `MATH500`
  - `AMC`
  - `AIME 2024`
  - `AIME 2025`

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### **æ¨¡å‹æ¶æ„**
- åŸºåº§æ¨¡å‹ï¼š`DeepSeek-R1-Distill-Qwen-1.5B`ï¼ˆç®€ç§° R1-Qwenï¼‰
- ä¼˜åŒ–ç®—æ³•ï¼š`GRPO`ï¼ˆGroup Relative Policy Optimizationï¼‰ï¼Œå¹¶ç§»é™¤ KL lossã€è°ƒæ•´ clip bounds ä»¥å¢å¼ºæ¢ç´¢ã€‚

#### **è®­ç»ƒç­–ç•¥**
- ä¸‰é˜¶æ®µè®­ç»ƒï¼Œæ¯é˜¶æ®µå‡ä½¿ç”¨ **8K context length** å’Œ **rollout=8**ï¼Œä¿æŒè½»é‡ã€‚
- Curriculum è°ƒåº¦åŸºäºå‰ä¸€é˜¶æ®µæ¨¡å‹å¯¹å…¨æ•°æ®é›†çš„æ¨ç†ç»“æœè¿›è¡Œéš¾åº¦åˆ’åˆ†ã€‚

#### **è¯„ä¼°æŒ‡æ ‡**
- **Accuracy (ACC)**ï¼šæœ€ç»ˆç­”æ¡ˆæ­£ç¡®ç‡
- **Response Length â†“**ï¼šç”Ÿæˆ token æ•°é‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- **Training Compute**ï¼šæ€» GPU å°æ—¶æ¶ˆè€—ï¼ˆä¼°ç®—ï¼‰

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

#### **é•¿ CoT æ¨ç†æ¨¡å‹ï¼ˆThinking Modeï¼‰**
- `STILL-3`
- `DeepScaleR`
- `FastCuRL`

#### **é«˜æ•ˆæ¨ç†æ¨¡å‹ï¼ˆEfficient Reasoning / NoThinking Modeï¼‰**
- `OverThink`
- `DAST`
- `O1-Pruner`
- `TLMRE`
- `ModelMerging`
- `AdaptThink`
- `AutoThink`

æ‰€æœ‰å¯¹æ¯”å‡åŸºäºç›¸åŒè®­ç»ƒæ•°æ®ï¼ˆDeepScaleR-Datasetï¼‰ï¼Œç¡®ä¿å…¬å¹³æ€§ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… **Thinking Mode ä¸‹çš„ç»“æœï¼ˆTable 1ï¼‰**

| æ¨¡å‹ | å¹³å‡ ACC â†‘ | å¹³å‡ Length â†“ | ç›¸å¯¹ R1-Qwen |
|------|------------|----------------|---------------|
| R1-Qwen | 70.3 | 9300 | â€” |
| DeepScaleR | 80.8 | 6120 | +10.5% ACC, -34.1% Length |
| **TACLer (Thinking)** | **81.5** | **5370** | **+11.2% ACC, -42.7% Length** |

> ğŸ“Œ **TACLer åœ¨ä¸‰é¡¹æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡ï¼Œå¹³å‡ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œä¸”å“åº”é•¿åº¦æœ€çŸ­ã€‚**

---

#### âœ… **NoThinking Mode ä¸‹çš„ç»“æœï¼ˆTable 2ï¼‰**

| æ¨¡å‹ | å¹³å‡ ACC â†‘ | å¹³å‡ Length â†“ |
|------|------------|----------------|
| R1-Qwen (NoThink) | 57.1 | 5430 |
| AutoThink | 60.4 | 3070 |
| **TACLer (NoThinking)** | **66.6** | **3720** |

> ğŸ“Œ **TACLer åœ¨ NoThinking æ¨¡å¼ä¸‹å‡†ç¡®ç‡è¿œè¶…æ‰€æœ‰åŸºçº¿ï¼ˆ+9.5% vs. R1-Qwenï¼‰ï¼ŒåŒæ—¶å“åº”é•¿åº¦ä»…ä¸ºä¸€åŠå·¦å³ã€‚**

---

### **æ¶ˆèå®éªŒç»“æœ**

#### ğŸ” **Hybrid Mode vs. Pure-NoThinkingï¼ˆTable 3ï¼‰**
- å•ç‹¬è®­ç»ƒ NoThinking æ¨¡å¼è™½åœ¨è¯¥æ¨¡å¼ä¸‹ç•¥å‡†ï¼Œä½†å“åº”æ›´é•¿ï¼ˆ2325 vs. 1472 tokens on MATH500ï¼‰ã€‚
- TACLer å› è”åˆè®­ç»ƒ Thinking æ¨¡å¼ï¼Œè·å¾—â€œå‹ç¼©æ•ˆåº”â€ï¼Œå®ç°æ›´**ç®€æ´ä¸”å‡†ç¡®**çš„ NoThinking è¾“å‡ºã€‚

#### ğŸ” **Curriculum Learning vs. Direct-Trainï¼ˆTable 4ï¼‰**
- ç›´æ¥è®­ç»ƒï¼ˆDirect-Trainï¼‰åœ¨ NoThinking ä¸‹å“åº”ç¨çŸ­ï¼Œä½†åœ¨ Thinking æ¨¡å¼ä¸‹ä¸å¦‚ TACLer ç²¾ç‚¼ã€‚
- **TACLer åœ¨ä¸¤ç§æ¨¡å¼ä¸‹å‡å–å¾—æ›´é«˜å‡†ç¡®ç‡**ï¼ŒéªŒè¯äº†è¯¾ç¨‹å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚

#### ğŸ” **Oracle Union åˆ†æï¼ˆTable 5ï¼‰**
- å®šä¹‰â€œOracle Unionâ€ä¸ºä» Thinking å’Œ NoThinking ä¸­é€‰æœ€ä¼˜ç»“æœçš„ç†è®ºä¸Šé™ã€‚
- TACLer å¹³å‡è¾¾åˆ°è¯¥ä¸Šé™çš„ **86.3%ï¼ˆNoThinkingï¼‰å’Œ 90.2%ï¼ˆThinkingï¼‰**ï¼Œè¯´æ˜å…¶å·²æ¥è¿‘ç†è®ºæ€§èƒ½è¾¹ç•Œã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **Tailored Curriculum Learning æ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡**
   - åŠ¨æ€åŸºäºæ¨¡å‹æŒæ¡ç¨‹åº¦å®‰æ’è®­ç»ƒé¡ºåºï¼Œé¿å…æ—©æœŸå¤„ç†è¿‡éš¾æ ·æœ¬å¯¼è‡´çš„å“åº”æˆªæ–­å’Œè®¡ç®—æµªè´¹ã€‚
   - å®ç° **>50% çš„è®­ç»ƒè®¡ç®—èŠ‚çœ**ã€‚

2. **Hybrid Reasoning Mode å®ç°æ•ˆç‡ä¸æ€§èƒ½çš„åŒèµ¢**
   - Thinking ä¸ NoThinking å¯ç›¸äº’ä¿ƒè¿›ï¼šå‰è€…æå‡åè€…å‡†ç¡®æ€§ï¼Œåè€…åå“ºå‰è€…çš„ç®€æ´æ€§ã€‚
   - åœ¨ NoThinking æ¨¡å¼ä¸‹ä»èƒ½è¶…è¶Šå¤šæ•° Thinking æ¨¡å‹çš„æ€§èƒ½ã€‚

3. **TACLer å®ç° SOTA æ€§èƒ½ä¸æè‡´æ•ˆç‡**
   - åœ¨å››å¤§æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼Œ**å‡†ç¡®ç‡å¹³å‡æå‡ >9%ï¼Œæ¨ç† token å‡å°‘ >42%**ã€‚
   - åŒæ—¶ä¼˜äºæ‰€æœ‰ Thinking å’Œ Efficient Reasoning åŸºçº¿ã€‚

4. **æ­£ç¡®æ¨ç†é€šå¸¸æ›´ç®€æ´**
   - å®éªŒå‘ç°ï¼š**æ­£ç¡®å›ç­”çš„å“åº”é•¿åº¦æ™®éçŸ­äºé”™è¯¯å›ç­”**ï¼Œæ”¯æŒâ€œè¿‡åº¦å†—é•¿å³é”™è¯¯â€çš„å‡è®¾ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

- **é¢å¤–æ¨ç†å¼€é”€**ï¼šåœ¨æ¯ä¸ªè®­ç»ƒé˜¶æ®µå‰éœ€å¯¹å…¨æ•°æ®é›†è¿›è¡Œ inference ä»¥æ„å»º curriculumï¼Œå¸¦æ¥ä¸€å®šè®¡ç®—å¼€é”€ï¼ˆçº¦ 5 å°æ—¶/H100 GPUï¼‰ã€‚
- å½“å‰ä»…éªŒè¯äºæ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œæ³›åŒ–æ€§æœ‰å¾…åœ¨å…¶ä»–é¢†åŸŸï¼ˆå¦‚ä»£ç ã€å¸¸è¯†æ¨ç†ï¼‰è¿›ä¸€æ­¥éªŒè¯ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

- å°† TACLer æ¡†æ¶æ‰©å±•è‡³æ›´å¤šæ¨ç†ä»»åŠ¡ï¼ˆå¦‚ Multi-hop QAã€Program Synthesisï¼‰ã€‚
- æ¢ç´¢è‡ªåŠ¨åŒ– curriculum labelingï¼Œé™ä½ inference å¼€é”€ã€‚
- ç ”ç©¶å¦‚ä½•å°† hybrid reasoning èƒ½åŠ›è¿ç§»åˆ° zero-shot åœºæ™¯ã€‚
- æ„å»ºé€šç”¨çš„â€œThinking Switchâ€æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·åŠ¨æ€æ§åˆ¶æ¨ç†æ·±åº¦ã€‚

---

> ğŸ’¡ **Impact Statement**  
> TACLer é€šè¿‡é™ä½è®¡ç®—æˆæœ¬ä¸æå‡æ¨ç†æ•ˆç‡ï¼Œä½¿é«˜æ€§èƒ½æ¨ç†æ¨¡å‹æ›´æ˜“äºéƒ¨ç½²ï¼Œæ¨åŠ¨å¯æŒç»­ AI å‘å±•ï¼Œå¹¶ä¸ºæ„å»ºå®‰å…¨ã€å¯é ã€å¯æ§çš„æ™ºèƒ½ç³»ç»Ÿæä¾›åŸºç¡€ã€‚

</details>

---

### 15. [A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine](https://arxiv.org/abs/2601.22124)

**Authors**: Anran Li, Yuanyuan Chen, Wenjun Long, Yu Yin, Yan Hu, Hyunjae Kim, Weipeng Zhou, Yujia Zhou, Hongyi Peng, Yang Ren, Xuguang Ai, Zhenyue Qin, Ming Hu, Xiaoxiao Li, Han Yu, Yih-Chung Tham, Lucila Ohno-Machado, Hua Xu, Qingyu Chen  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22124v1  

#### Abstract
Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰åŒ»å­¦é¢†åŸŸçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®­ç»ƒæ™®éé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š
- **éšç§ä¸åˆè§„é™åˆ¶**ï¼šç”±äºæ‚£è€…æ•°æ®æ•æ„Ÿï¼ŒåŒ»ç–—æœºæ„é—´éš¾ä»¥å…±äº«åŸå§‹æ•°æ®ï¼Œå¯¼è‡´å¤šæ•°LLMä»…åŸºäºå•ä¸€æœºæ„çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ³›åŒ–èƒ½åŠ›å·®ã€‚
- **è”é‚¦å­¦ä¹ ï¼ˆFederated Learning, FLï¼‰åœ¨LLMä¸Šçš„åº”ç”¨ç“¶é¢ˆ**ï¼š
  - ä¼ ç»ŸFLéœ€è¦ä¼ è¾“å®Œæ•´çš„æ¨¡å‹å‚æ•°ï¼Œåœ¨æ•°åäº¿å‚æ•°è§„æ¨¡çš„LLMä¸­é€šä¿¡å¼€é”€å·¨å¤§ï¼Œä¸é€‚ç”¨äºä¸´åºŠç¯å¢ƒã€‚
  - åŒ»ç–—æ•°æ®å…·æœ‰é«˜åº¦å¼‚è´¨æ€§ï¼ˆnon-IIDï¼‰ï¼Œå¦‚ä¸åŒäººç¾¤ã€ç–¾ç—…åˆ†å¸ƒã€æ ‡æ³¨ä¹ æƒ¯ç­‰ï¼Œæ ‡å‡†FLèšåˆç­–ç•¥å®¹æ˜“å¯¼è‡´æ”¶æ•›ä¸ç¨³å®šå’Œæ€§èƒ½åå·®ã€‚

### æå‡ºçš„æ–°æ–¹æ³•
æœ¬æ–‡æå‡º **Fed-MedLoRA** å’Œå…¶å¢å¼ºç‰ˆæœ¬ **Fed-MedLoRA+**ï¼Œæ˜¯é¦–ä¸ªé¢å‘åŒ»å­¦é¢†åŸŸçš„**æ¨¡å‹æ— å…³ä¸”å‚æ•°é«˜æ•ˆçš„è”é‚¦å­¦ä¹ æ¡†æ¶**ï¼Œç”¨äºé€‚é…å¤§è¯­è¨€æ¨¡å‹ã€‚

#### æ ¸å¿ƒæŠ€æœ¯æ€è·¯
- **Fed-MedLoRA**ï¼šé‡‡ç”¨ **Low-Rank Adaptation (LoRA)** æŠ€æœ¯ï¼Œå®¢æˆ·ç«¯åªè®­ç»ƒå¹¶ä¸Šä¼ ä½ç§©é€‚é…å™¨ï¼ˆadapterï¼‰å‚æ•°ï¼Œè€Œéæ•´ä¸ªæ¨¡å‹æƒé‡ï¼Œæ˜¾è‘—é™ä½é€šä¿¡æˆæœ¬ã€‚
- **Fed-MedLoRA+**ï¼šåœ¨Fed-MedLoRAåŸºç¡€ä¸Šå¼•å…¥**è‡ªé€‚åº”ã€æ•°æ®æ„ŸçŸ¥çš„èšåˆæœºåˆ¶ï¼ˆinfluence-aware aggregationï¼‰**ã€‚æœåŠ¡å™¨åˆ©ç”¨ä¸€ä¸ªå°çš„å…¬å…±éªŒè¯é›†è¯„ä¼°æ¯ä¸ªå®¢æˆ·ç«¯æ›´æ–°çš„å½±å“ï¼Œå¹¶æ®æ­¤åŠ æƒèšåˆï¼Œæå‡å¯¹è·¨ç«™ç‚¹å¼‚æ„æ€§çš„é²æ£’æ€§ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **é€šä¿¡æ•ˆç‡** | é€šä¿¡é‡å‡å°‘ **98.5%**ï¼Œä»å…¨æ¨¡å‹æ›´æ–°çš„æ•°ç™¾GBé™è‡³æ•°GBçº§åˆ«ï¼Œé€‚åˆå¸¦å®½å—é™çš„åŒ»ç–—ç½‘ç»œã€‚ |
| **è®¡ç®—å¯è¡Œæ€§** | å¯åœ¨æ¶ˆè´¹çº§GPUï¼ˆå¦‚RTX 4090ï¼‰ä¸Šå®Œæˆ8Bå‚æ•°æ¨¡å‹çš„è®­ç»ƒï¼Œæ¨ç†å¯åœ¨ç¬”è®°æœ¬ç”µè„‘ï¼ˆå¦‚Apple M3 Proï¼‰è¿è¡Œã€‚ |
| **æ¨¡å‹æ€§èƒ½** | æ˜¾è‘—ä¼˜äºå•æœºæ„å¾®è°ƒã€é›¶æ ·æœ¬LLMåŠä¸“ç”¨BERTæ¨¡å‹ï¼Œæ¥è¿‘é›†ä¸­å¼è®­ç»ƒçš„â€œä¸Šç•Œâ€æ€§èƒ½ã€‚ |
| **å®ç”¨æ€§** | æ”¯æŒä¸å®Œæ•´æˆ–ä¸å‡è¡¡çš„ä»»åŠ¡æ ‡æ³¨ï¼ˆä¾‹å¦‚æŸäº›ç«™ç‚¹ä»…æœ‰NERæ— REæ ‡ç­¾ï¼‰ï¼Œæ›´è´´è¿‘çœŸå®å¤šä¸­å¿ƒåä½œåœºæ™¯ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
å…±ä½¿ç”¨äº”ä¸ªçœŸå®ä¸–ç•Œä¸´åºŠæ–‡æœ¬æ•°æ®é›†ï¼Œæ¶µç›–å››ç§å®ä½“ç±»åˆ«ï¼ˆProblem, Treatment, Test, Drugï¼‰å’Œ16ç§å…³ç³»ç±»å‹ï¼š
- **MIMIC-III**ï¼šé‡ç—‡ç›‘æŠ¤ç”µå­ç—…å†ã€‚
- **MTSamples**ï¼šå…¬å¼€è½¬å½•çš„åŒ»ç–—æŠ¥å‘Šæ ·æœ¬ã€‚
- **UTP (UT Physicians)**ï¼šå¾·å…‹è¨æ–¯å¤§å­¦åŒ»ç”Ÿç¬”è®°ã€‚
- **I2B2**ï¼šæ•´åˆç”Ÿç‰©å­¦ä¸åºŠè¾¹ä¿¡æ¯å­¦çš„ä¸´åºŠæ–‡æœ¬ã€‚
- **YNHH (Yale New Haven Health)**ï¼šç ”ç©¶å›¢é˜Ÿæ‰‹åŠ¨æ ‡æ³¨çš„4ä»½è€¶é²çº½é»‘æ–‡å¥åº·ç³»ç»Ÿçš„çœŸå®æ‚£è€…è®°å½•ï¼Œç”¨äºæ¨¡æ‹Ÿæ–°ç«™ç‚¹ä½èµ„æºè¿ç§»åœºæ™¯ã€‚

### å®éªŒè®¾ç½®
åˆ†ä¸ºä¸‰ç§è¯„ä¼°åœºæ™¯ä»¥å…¨é¢éªŒè¯æ¨¡å‹èƒ½åŠ›ï¼š

| åœºæ™¯ | æè¿° |
|------|------|
| **In-domain Training & Testing** | åœ¨å¤šä¸ªç«™ç‚¹ï¼ˆå¦‚MIMIC-III + MTSamplesï¼‰ä¸Šè¿›è¡Œè”é‚¦è®­ç»ƒï¼Œå¹¶åœ¨å¯¹åº”æµ‹è¯•é›†ä¸Šè¯„ä¼°åŸŸå†…æ€§èƒ½ã€‚ |
| **External Validation** | å°†è”é‚¦è®­ç»ƒåçš„æ¨¡å‹ç›´æ¥åº”ç”¨äºæœªè§è¿‡çš„å¤–éƒ¨æ•°æ®é›†ï¼ˆå¦‚I2B2æˆ–UTPï¼‰ï¼Œæ£€éªŒè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚ |
| **Low-resource New-site Adaptation** | ä½¿ç”¨YNHHå°æ ·æœ¬æ•°æ®é›†ï¼Œæµ‹è¯•è”é‚¦æ¨¡å‹èƒ½å¦å¸®åŠ©ä¸€ä¸ªæ–°åŠ å…¥ä¸”æ ‡æ³¨æå°‘çš„æœºæ„å¿«é€Ÿå¯åŠ¨æœ¬åœ°ä»»åŠ¡ã€‚ |

### è¯„ä¼°æŒ‡æ ‡
- **NER (å‘½åå®ä½“è¯†åˆ«)** å’Œ **RE (å…³ç³»æŠ½å–)** ä»»åŠ¡å‡æŠ¥å‘Š **Precision, Recall, F1-score**ã€‚
- é‡‡ç”¨ä¸¤ç§åŒ¹é…æ ‡å‡†ï¼š
  - **Strict Match**ï¼šé¢„æµ‹çš„å®ä½“è·¨åº¦å’Œç±»å‹å¿…é¡»å®Œå…¨ä¸€è‡´ã€‚
  - **Lenient Match**ï¼šå…è®¸éƒ¨åˆ†é‡å çš„è·¨åº¦ï¼Œåªè¦ç±»å‹æ­£ç¡®å³è§†ä¸ºæ­£ç¡®ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | å…·ä½“åŸºçº¿ |
|------|--------|
| **LLM-based** | - Zero-shot LLaMA3 / DeepSeek-R1 / GPT-4o<br>- å•ç«™ç‚¹å¾®è°ƒï¼ˆSingle-site fine-tunedï¼‰LLaMA3 / DeepSeek-R1 |
| **BERT-based** | - å¾®è°ƒ Bio_ClinicalBERTï¼ˆåŒ»å­¦é¢†åŸŸæœ€å¼ºBERTä¹‹ä¸€ï¼‰ |
| **Federated Baseline** | - FedSA-LoRAï¼šä¸€ç§é€šç”¨é¢†åŸŸçš„LoRAè”é‚¦ç®—æ³•ï¼Œä»…ä¸Šä¼ ä½ç§©çŸ©é˜µAè¿›è¡Œèšåˆ |
| **ç†è®ºä¸Šé™** | - Centralized Trainingï¼šå°†æ‰€æœ‰ç«™ç‚¹æ•°æ®åˆå¹¶åé›†ä¸­è®­ç»ƒï¼Œä½œä¸ºç†æƒ³ä½†ä¸å¯è¡Œçš„æ€§èƒ½ä¸Šç•Œ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»
| æŒ‡æ ‡ | Fed-MedLoRA+ è¡¨ç° |
|------|------------------|
| **NER F1 (Strict)** | æœ€é«˜è¾¾ **86.6%**ï¼ˆMTSamplesï¼‰ï¼Œå¹³å‡æ¯”é›¶æ ·æœ¬æå‡è¶… **50%** |
| **RE F1 (Strict)** | æœ€é«˜è¾¾ **86.0%**ï¼ˆMIMIC-IIIï¼‰ï¼Œå¹³å‡æ¯”é›¶æ ·æœ¬æå‡çº¦ **65%** |
| **å¤–éƒ¨éªŒè¯å¢ç›Š** | ç›¸æ¯”åŸºçº¿F1æå‡ **10â€“70%** |
| **æ–°ç«™ç‚¹é€‚åº”** | åœ¨YNHHä½èµ„æºåœºæ™¯ä¸‹å–å¾— **73%ä¸¥æ ¼F1 / 85%å®½æ¾F1** |
| **é€šä¿¡æˆæœ¬é™ä½** | è¾ƒå…¨å‚æ•°æ›´æ–°å‡å°‘ **98.5%**ï¼ˆå¦‚ä»239GBâ†’1.25GBï¼‰ |
| **è®­ç»ƒç¡¬ä»¶éœ€æ±‚** | 8Bæ¨¡å‹å¯åœ¨ **å•å—RTX 4090 (16GB)** ä¸Šè®­ç»ƒï¼›1Bæ¨¡å‹å¯è·‘åœ¨ **RTX 3060 Ti (8GB)** ä¸Š |
| **æ¨ç†è®¾å¤‡æ”¯æŒ** | æ”¯æŒåœ¨ **æ ‡å‡†ç¬”è®°æœ¬ç”µè„‘**ï¼ˆå¦‚Apple M3 Proï¼‰éƒ¨ç½²æ¨ç† |

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
#### âœ… è¶…è¶Šæ‰€æœ‰åŸºçº¿
- **vs é›¶æ ·æœ¬LLM**ï¼šFed-MedLoRA+ä½¿LLMçš„F1æå‡é«˜è¾¾ **65ä¸ªç™¾åˆ†ç‚¹**ï¼ˆå¦‚LLaMA3åœ¨MIMIC-IIIä¸Šä»0.203â†’0.860ï¼‰ã€‚
- **vs å•ç«™ç‚¹å¾®è°ƒLLM**ï¼šå¹³å‡F1æå‡çº¦ **25%**ï¼Œå°¤å…¶åœ¨å¤æ‚REä»»åŠ¡ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚
- **vs å¾®è°ƒBERTæ¨¡å‹**ï¼šåœ¨REä»»åŠ¡ä¸ŠF1é¢†å…ˆè¶…è¿‡ **40%**ï¼ˆå¦‚0.860 vs 0.434ï¼‰ã€‚
- **vs FedSA-LoRA**ï¼šåœ¨ä¸‰ç«™ç‚¹å®éªŒä¸­ï¼ŒFed-MedLoRA+åœ¨NERå’ŒREä¸Šåˆ†åˆ«é«˜å‡º **~40%** å’Œ **~70%** F1ï¼Œè¯æ˜å…¶è‡ªé€‚åº”èšåˆçš„æœ‰æ•ˆæ€§ã€‚

#### âœ… æ¥è¿‘é›†ä¸­å¼è®­ç»ƒâ€œä¸Šç•Œâ€
- åœ¨å¤šæ•°ä»»åŠ¡ä¸Šï¼ŒFed-MedLoRA+æ€§èƒ½ä¸Centralized Trainingå·®è·æå°ï¼ˆ<2% F1ï¼‰ï¼Œè¡¨æ˜å…¶èƒ½æœ‰æ•ˆèåˆå¤šæºä¿¡æ¯è€Œä¸ç‰ºç‰²ç²¾åº¦ã€‚

#### âœ… å¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›
- åœ¨ç‹¬ç«‹å¤–éƒ¨æ•°æ®é›†ï¼ˆI2B2, UTPï¼‰ä¸Šï¼ŒFed-MedLoRA+ä»ä¿æŒå¼ºåŠ²è¡¨ç°ï¼ŒF1ç›¸æ¯”å•ç«™ç‚¹å¾®è°ƒæå‡ **9â€“10%**ï¼Œè¯´æ˜è”é‚¦è®­ç»ƒå¢å¼ºäº†æ¨¡å‹é²æ£’æ€§ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **Fed-MedLoRA vs Fed-MedLoRA+**ï¼š
  - Fed-MedLoRA+åœ¨ç»å¤§å¤šæ•°æµ‹è¯•é›†ä¸Šè¡¨ç°æ›´å¥½ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®å¼‚æ„æ€§å¼ºçš„ä¸‰ç«™ç‚¹è®¾ç½®ä¸­ã€‚
  - è¯æ˜äº†**å½±å“æ„ŸçŸ¥èšåˆï¼ˆinfluence-aware aggregationï¼‰** å¯¹ç¼“è§£å¼‚æ„æ€§è‡³å…³é‡è¦ã€‚
- **ä¸åŒbackboneæ¯”è¾ƒ**ï¼š
  - LLaMA3 å’Œ DeepSeek-R1 è¡¨ç°ç›¸è¿‘ï¼ŒLLaMA3ç•¥ä¼˜ã€‚
- **ä¸åŒæ¨¡å‹å¤§å°çš„å½±å“ï¼ˆ1B vs 8Bï¼‰**ï¼š
  - ä½¿ç”¨LLaMA3-1Bæ—¶ï¼ŒNERæ€§èƒ½ä¸‹é™çº¦3%ï¼ŒREä¸‹é™æœ€å¤š7%ï¼Œä½†è®­ç»ƒå†…å­˜ä»14GBé™è‡³2.15GBï¼Œæ¨ç†å¯è·‘åœ¨æ™®é€šç¬”è®°æœ¬ã€‚
  - æä¾›äº†**æ•ˆç‡-ç²¾åº¦æƒè¡¡é€‰é¡¹**ï¼Œé€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **è”é‚¦LLMåœ¨åŒ»å­¦ä¸­æ˜¯å¯è¡Œä¸”æœ‰æ•ˆçš„**ï¼šé€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆå¦‚LoRAï¼‰ï¼Œå¯ä»¥å®ç°è·¨æœºæ„åä½œè®­ç»ƒï¼ŒåŒæ—¶ä¿æŠ¤æ‚£è€…éšç§ã€‚
2. **Fed-MedLoRA+æ˜¾è‘—æå‡äº†è”é‚¦è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½**ï¼šå…¶æå‡ºçš„**å½±å“æ„ŸçŸ¥èšåˆæœºåˆ¶**æœ‰æ•ˆåº”å¯¹äº†åŒ»ç–—æ•°æ®çš„é«˜åº¦å¼‚è´¨æ€§é—®é¢˜ã€‚
3. **è”é‚¦è®­ç»ƒæå¤§å¢å¼ºäº†æ¨¡å‹æ³›åŒ–èƒ½åŠ›**ï¼šç›¸æ¯”å•æœºæ„è®­ç»ƒï¼Œè”é‚¦æ¨¡å‹åœ¨å¤–éƒ¨æ•°æ®å’Œæ–°ç«™ç‚¹ä¸Šè¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§ã€‚
4. **å®ç”¨æ€§å¼º**ï¼šè¯¥æ¡†æ¶å¯åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œæ”¯æŒä¸å®Œæ•´æ ‡æ³¨ï¼Œå…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼ˆæ¨¡æ‹Ÿè‡³10ä¸ªç«™ç‚¹æ€§èƒ½ä»…è½»å¾®ä¸‹é™ï¼‰ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **éšç§ä¿æŠ¤å°šæœªå®Œå…¨é—­ç¯**ï¼šè™½ç„¶ä¸ä¼ è¾“åŸå§‹æ•°æ®å’Œå®Œæ•´æ¨¡å‹ï¼Œä½†ä»éœ€ç ”ç©¶æ˜¯å¦å¯èƒ½é€šè¿‡LoRAå‚æ•°åæ¨æ•æ„Ÿä¿¡æ¯ï¼Œå»ºè®®æœªæ¥ç»“åˆSecure Aggregationæˆ–Differential Privacyã€‚
2. **ä»»åŠ¡èŒƒå›´æœ‰é™**ï¼šæœ¬ç ”ç©¶èšç„¦äº**ä¸´åºŠä¿¡æ¯æå–ï¼ˆIEï¼‰**ï¼Œå°šéœ€æ¢ç´¢å…¶ä»–åŒ»å­¦ä»»åŠ¡ï¼ˆå¦‚è¯Šæ–­è¾…åŠ©ã€ç”Ÿæˆæ‘˜è¦ï¼‰ä¸­çš„é€‚ç”¨æ€§ã€‚
3. **ä¾èµ–å°‘é‡å…¬å…±éªŒè¯é›†**ï¼šæœåŠ¡å™¨éœ€æŒæœ‰å°å‹éªŒè¯é›†æ¥è®¡ç®—å½±å“åˆ†æ•°ï¼Œè™½åˆç†ä½†åœ¨æç«¯éšç§è¦æ±‚ä¸‹å¯èƒ½å—é™ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **åŠ å¼ºéšç§ä¿éšœæœºåˆ¶**ï¼šé›†æˆå·®åˆ†éšç§ï¼ˆDPï¼‰ã€å®‰å…¨èšåˆï¼ˆSecure Aggregationï¼‰ç­‰æŠ€æœ¯ï¼Œæ„å»ºç«¯åˆ°ç«¯éšç§ä¿æŠ¤ç®¡é“ã€‚
2. **æ‹“å±•è‡³æ›´å¤šåŒ»å­¦ä»»åŠ¡**ï¼šéªŒè¯Fed-MedLoRAåœ¨é—®ç­”ã€è¯Šæ–­ã€æ²»ç–—æ¨èç­‰å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚
3. **æ¨åŠ¨çœŸå®ä¸´åºŠéƒ¨ç½²**ï¼šå¼€å±•å‰ç»æ€§å¤šä¸­å¿ƒè¯•éªŒï¼Œè§£å†³å®é™…éƒ¨ç½²ä¸­çš„ç½‘ç»œåè°ƒã€ç¡¬ä»¶å¼‚æ„ã€æ²»ç†åˆè§„ç­‰é—®é¢˜ã€‚
4. **æ¢ç´¢æ›´è½»é‡åŒ–æ¶æ„**ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–å°æ¨¡å‹ï¼ˆå¦‚1Bä»¥ä¸‹ï¼‰çš„æ€§èƒ½ï¼Œä½¿å…¶èƒ½åœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚åŒ»é™¢ç»ˆç«¯æœºï¼‰è¿è¡Œã€‚

> ğŸ”— **ä»£ç å¼€æº**ï¼šæ‰€æœ‰å®ç°å·²å…¬å¼€äº GitHub â†’ [https://github.com/Yale-BIDS-Chen-Lab/FLLLMMed](https://github.com/Yale-BIDS-Chen-Lab/FLLLMMed)  
> ğŸ“š **èµ„åŠ©æ¥æº**ï¼šæœ¬ç ”ç©¶ç”±ç¾å›½å›½ç«‹å«ç”Ÿç ”ç©¶é™¢ï¼ˆNIHï¼‰é¡¹ç›® 1R01LM014604 æ”¯æŒã€‚

</details>

---

### 16. [Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21316)

**Authors**: Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li, Chung Shue Chen, Man-On Pun  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.21316v1  

#### Abstract
Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with...

---

### 17. [Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting](https://arxiv.org/abs/2601.21899)

**Authors**: Zhiqing Cui, Siru Zhong, Ming Jin, Shirui Pan, Qingsong Wen, Yuxuan Liang  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.21899v1  

#### Abstract
Global air quality forecasting grapples with extreme spatial heterogeneity and the poor generalization of existing transductive models to unseen regions. To tackle this, we propose OmniAir, a semantic topology learning framework tailored for global station-level prediction. By encoding invariant phy...

---

### 18. [OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution](https://arxiv.org/abs/2601.20380)

**Authors**: Le Zhang, Yixiong Xiao, Xinjiang Lu, Jingjia Cao, Yusai Zhao, Jingbo Zhou, Lang An, Zikan Feng, Wanxiang Sha, Yu Shi, Congxi Xiao, Jian Xiong, Yankai Zhang, Hua Wu, Haifeng Wang  
**Category**: cs.AI  
**Published**: 2026-01-30  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.20380v1  

#### Abstract
Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on...

---

### 19. [ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas](https://arxiv.org/abs/2601.21558)

**Authors**: Xiaoyu Tian, Haotian Wang, Shuaiting Chen, Hao Zhou, Kaichi Yu, Yudian Zhang, Jade Ouyang, Junxi Yin, Jiong Chen, Baoyan Guo, Lei Zhang, Junjie Tao, Yuansheng Song, Ming Cui, Chengwei Liu  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.21558v1  

#### Abstract
Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either super...

---

### 20. [DynaWeb: Model-Based Reinforcement Learning of Web Agents](https://arxiv.org/abs/2601.22149)

**Authors**: Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.22149v1  

#### Abstract
The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which i...

---

### 21. [Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies](https://arxiv.org/abs/2601.21090)

**Authors**: Mohammad Walid Charrwi, Zaid Hussain  
**Category**: cs.DC  
**Published**: 2026-01-30  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.21090v1  

#### Abstract
The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics und...

---

### 22. [EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference](https://arxiv.org/abs/2601.21758)

**Authors**: Bronislav Sidik, Chaya Levi, Joseph Kampeas  
**Category**: cs.DC  
**Published**: 2026-01-30  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.21758v1  

#### Abstract
Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading t...

---

### 23. [Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21312)

**Authors**: Xiaozhuang Li, Xindi Tang, Fang He  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.21312v1  

#### Abstract
With the rapid expansion of electric vehicles (EVs) and charging infrastructure, the effective management of Autonomous Electric Taxi (AET) fleets faces a critical challenge in environments with dynamic and uncertain charging availability. While most existing research assumes a static charging netwo...

---

### 24. [Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving](https://arxiv.org/abs/2601.21351)

**Authors**: Chendong Song, Meixuan Wang, Hang Zhou, Hong Liang, Yuan Lyu, Zixi Chen, Yuwei Fan, Zijie Zhou  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.21351v1  

#### Abstract
Attention-FFN disaggregation (AFD) is an emerging architecture for LLM decoding that separates state-heavy, KV-cache-dominated Attention computation from stateless, compute-intensive FFN computation, connected by per-step communication. While AFD enables independent scaling of memory and compute res...

---

### 25. [Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks](https://arxiv.org/abs/2601.21572)

**Authors**: Jinhao Li, Yuhao Sun, Zhiyuan Ma, Hao He, Xinche Zhang, Xing Chen, Jin Li, Sen Song  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.21572v1  

#### Abstract
Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentia...

---

### 26. [Uncertainty-Aware Data-Based Method for Fast and Reliable Shape Optimization](https://arxiv.org/abs/2601.21956)

**Authors**: Yunjia Yang, Runze Li, Yufei Zhang, Haixin Chen  
**Category**: cs.LG  
**Published**: 2026-01-30  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.21956v1  

#### Abstract
Data-based optimization (DBO) offers a promising approach for efficiently optimizing shape for better aerodynamic performance by leveraging a pretrained surrogate model for offline evaluations during iterations. However, DBO heavily relies on the quality of the training database. Samples outside the...

---

### 27. [ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference](https://arxiv.org/abs/2601.21109)

**Authors**: Ketan Thakkar, Maitreyi Chatterjee, Ramasubramanian Balasubramanian, Achyuthan Jootoo, Rajendra Ugrani  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.21109v1  

#### Abstract
Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computati...

---

### 28. [Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space](https://arxiv.org/abs/2601.21169)

**Authors**: Tobias Materzok  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.21169v1  

#### Abstract
We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standa...

---

### 29. [Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis](https://arxiv.org/abs/2601.21709)

**Authors**: Qingyue Yang, Jie Wang, Xing Li, Yinqi Bai, Xialiang Tong, Huiling Zhen, Jianye Hao, Mingxuan Yuan, Bin Li  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.21709v1  

#### Abstract
Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this ga...

---

### 30. [VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning](https://arxiv.org/abs/2601.22069)

**Authors**: Yibo Wang, Yongcheng Jing, Shunyu Liu, Hao Guan, Rong-cheng Tu, Chengyu Wang, Jun Huang, Dacheng Tao  
**Category**: cs.CL  
**Published**: 2026-01-30  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.22069v1  

#### Abstract
Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, wh...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
