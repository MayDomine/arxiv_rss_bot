# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-25 12:55:56 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [DeCoRL: Decoupling Reasoning Chains via Parallel Sub-Step Generation and Cascaded Reinforcement for Interpretable and Scalable RLHF](https://arxiv.org/abs/2511.19097)

**Authors**: Ziyuan Gao, Di Liang, Xianjie Wu, Philippe Morel, Minlong Peng  
**Category**: cs.CL  
**Published**: 2025-11-25  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2511.19097v1  

Existing reinforcement learning methods for Chain-of-Thought reasoning suffer from two critical limitations. First, they operate as monolithic black boxes that provide undifferentiated reward signals, obscuring individual step contributions and hindering error diagnosis. Second, sequential decoding ...

---

### 2. [Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch](https://arxiv.org/abs/2511.17826)

**Authors**: Ziyang Zhang, Xinheng Ding, Jiayi Yuan, Rixin Liu, Huizi Mao, Jiarong Xing, Zirui Liu  
**Category**: cs.CL  
**Published**: 2025-11-25  
**Score**: 11.5  
**Type**: cross  
**ArXiv ID**: 2511.17826v1  

Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outp...

---

### 3. [Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2511.15015)

**Authors**: Kexin Chu, Dawei Xiang, Zixu Shen, Yiwei Yang, Zecheng Liu, Wei Zhang  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 10.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.15015v2  

Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressi...

---

### 4. [Comprehensive Design Space Exploration for Tensorized Neural Network Hardware Accelerators](https://arxiv.org/abs/2511.17971)

**Authors**: Jinsong Zhang, Minghe Li, Jiayi Tian, Jinming Lu, Zheng Zhang  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 10.0  
**Type**: cross  
**ArXiv ID**: 2511.17971v1  

High-order tensor decomposition has been widely adopted to obtain compact deep neural networks for edge deployment. However, existing studies focus primarily on its algorithmic advantages such as accuracy and compression ratio-while overlooking the hardware deployment efficiency. Such hardware-unawa...

---

### 5. [KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit](https://arxiv.org/abs/2511.18868)

**Authors**: Dezhi Ran, Shuxiao Xie, Mingfang Ji, Ziyue Hua, Mengzhou Wu, Yuan Cao, Yuzhe Guo, Yu Hao, Linyi Li, Yitao Hu, Tao Xie  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 10.0  
**Type**: cross  
**ArXiv ID**: 2511.18868v1  

High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimizati...

---

### 6. [N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory](https://arxiv.org/abs/2511.18723)

**Authors**: Longfei Wang, Junyan Liu, Fan Zhang, Jiangwen Wei, Yuanhua Tang, Jie Sun, Xiaodong Luo  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.18723v1  

Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&amp;B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (...

---

### 7. [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)

**Authors**: Maxime Heuillet, Yufei Cui, Boxing Chen, Audrey Durand, Prasanna Parthasarathi  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2508.10123v2  

Advanced reasoning in LLMs on challenging domains like mathematical reasoning can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In standard ReFT frameworks, a behavior model generates multiple completions with answers per problem, for the answer to be then scored by a rewa...

---

### 8. [GANGR: GAN-Assisted Scalable and Efficient Global Routing Parallelization](https://arxiv.org/abs/2511.17665)

**Authors**: Hadi Khodaei Jooshin, Inna Partin-Vaisband  
**Category**: cs.LG  
**Published**: 2025-11-25  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.17665v1  

Global routing is a critical stage in electronic design automation (EDA) that enables early estimation and optimization of the routability of modern integrated circuits with respect to congestion, power dissipation, and design complexity. Batching is a primary concern in top-performing global router...

---

### 9. [Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2511.17801)

**Authors**: Cuong Pham, Hoang Anh Dung, Cuong C. Nguyen, Trung Le, Gustavo Carneiro, Thanh-Toan Do  
**Category**: cs.LG  
**Published**: 2025-11-25  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.17801v1  

Large language models (LLMs) have significantly advanced natural language processing, but their massive parameter counts create substantial computational and memory challenges during deployment. Post-training quantization (PTQ) has emerged as a promising approach to mitigate these challenges with mi...

---

### 10. [QuantKAN: A Unified Quantization Framework for Kolmogorov Arnold Networks](https://arxiv.org/abs/2511.18689)

**Authors**: Kazi Ahmed Asif Fuad, Lizhong Chen  
**Category**: cs.LG  
**Published**: 2025-11-25  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.18689v1  

Kolmogorov Arnold Networks (KANs) represent a new class of neural architectures that replace conventional linear transformations and node-based nonlinearities with spline-based function approximations distributed along network edges. Although KANs offer strong expressivity and interpretability, thei...

---

### 11. [Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost](https://arxiv.org/abs/2511.18643)

**Authors**: Haojun Xia, Xiaoxia Wu, Jisen Li, Robert Wu, Junxiong Wang, Jue Wang, Chenxi Li, Aman Singhal, Alay Dilipbhai Shah, Alpay Ariyak, Donglin Zhuang, Zhongzhu Zhou, Ben Athiwaratkun, Zhen Zheng, Shuaiwen Leon Song  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2511.18643v1  

The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive e...

---

### 12. [Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning](https://arxiv.org/abs/2511.14617)

**Authors**: Ruoyu Qin, Weiran He, Weixiao Huang, Yangkun Zhang, Yikai Zhao, Bo Pang, Xinran Xu, Yingdi Shan, Yongwei Wu, Mingxing Zhang  
**Category**: cs.DC  
**Published**: 2025-11-25  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2511.14617v2  

Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utili...

---

### 13. [FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning](https://arxiv.org/abs/2511.17885)

**Authors**: Guoyang Xia, Yifeng Ding, Fengfa Li, Lei Ren, Wei Chen, Fangxiang Feng, Xiaojie Wang  
**Category**: cs.LG  
**Published**: 2025-11-25  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2511.17885v1  

Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance...

---

### 14. [Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction](https://arxiv.org/abs/2505.11254)

**Authors**: Jeffrey Willette, Heejun Lee, Sung Ju Hwang  
**Category**: cs.LG  
**Published**: 2025-11-25  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2505.11254v2  

The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference meth...

---

### 15. [SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression](https://arxiv.org/abs/2511.18936)

**Authors**: Santhosh G S, Saurav Prakash, Balaraman Ravindran  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.18936v1  

Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or ...

---

### 16. [FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning](https://arxiv.org/abs/2511.18977)

**Authors**: Xin Yuan, Siqi Li, Jiateng Wei, Chengrui Zhu, Yanming Wu, Qingpeng Li, Jiajun Lv, Xiaoke Lan, Jun Chen, Yong Liu  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.18977v1  

Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are o...

---

### 17. [Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration](https://arxiv.org/abs/2511.14730)

**Authors**: Parya Dolatyabi, Mahdi Khodayar  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2511.14730v2  

Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges m...

---

### 18. [Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics](https://arxiv.org/abs/2501.10100)

**Authors**: Chenhao Li, Andreas Krause, Marco Hutter  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2501.10100v4  

Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The propose...

---

### 19. [Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone](https://arxiv.org/abs/2511.15927)

**Authors**: Vaibhav Singh, Oleksiy Ostapenko, Pierre-Andr\'e No\"el, Torsten Scholak  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.15927v2  

Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language...

---

### 20. [MTGenRec: An Efficient Distributed Training System for Generative Recommendation Models in Meituan](https://arxiv.org/abs/2505.12663)

**Authors**: Yuxiang Wang, Xiao Yan, Chi Ma, Mincong Huang, Xiaoguang Li, Lei Yu, Chuan Liu, Ruidong Han, He Jiang, Bin Yin, Shangyu Chen, Fei Jiang, Xiang Li, Wei Lin, Haowei Han, Bo Du, Jiawei Jiang  
**Category**: cs.DC  
**Published**: 2025-11-25  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2505.12663v2  

Recommendation is crucial for both user experience and company revenue in Meituan as a leading lifestyle company, and generative recommendation models (GRMs) are shown to produce quality recommendations recently. However, existing systems are limited by insufficient functionality support and ineffic...

---

### 21. [MIST: Mutual Information Via Supervised Training](https://arxiv.org/abs/2511.18945)

**Authors**: German Gritsai, Megan Richards, Maxime M\'eloux, Kyunghyun Cho, Maxime Peyrard  
**Category**: cs.LG  
**Published**: 2025-11-25  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.18945v1  

We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is perform...

---

### 22. [Towards Efficient LLM-aware Heterogeneous Graph Learning](https://arxiv.org/abs/2511.17923)

**Authors**: Wenda Li, Tongya Zheng, Shunyu Liu, Yu Wang, Kaixuan Chen, Hanyang Yuan, Bingde Hu, Zujie Ren, Mingli Song, Gang Chen  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.17923v1  

Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies an...

---

### 23. [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)

**Authors**: Yunxin Li, Xinyu Chen, Shenyuan Jiang, Haoyuan Shi, Zhenyu Liu, Xuanyu Zhang, Nanhao Deng, Zhenran Xu, Yicheng Ma, Meishan Zhang, Baotian Hu, Min Zhang  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2511.12609v2  

We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build Uni-MoE-2.0-Omni from scratch through three c...

---

### 24. [Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets](https://arxiv.org/abs/2511.18054)

**Authors**: Gowtham, Sai Rupesh, Sanjay Kumar,  Saravanan, Venkata Chaithanya  
**Category**: cs.CL  
**Published**: 2025-11-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.18054v1  

High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize...

---

### 25. [Accelerating Time Series Foundation Models with Speculative Decoding](https://arxiv.org/abs/2511.18191)

**Authors**: Pranav Subbaraman, Fang Sun, Yue Yao, Huacong Tang, Xiao Luo, Yizhou Sun  
**Category**: cs.LG  
**Published**: 2025-11-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.18191v1  

Modern web applications--from real-time content recommendation and dynamic pricing to CDN optimization--increasingly rely on time-series forecasting to deliver personalized experiences to billions of users. Large-scale Transformer-based models have achieved state-of-the-art performance in time-serie...

---

### 26. [Edge-Based Predictive Data Reduction for Smart Agriculture: A Lightweight Approach to Efficient IoT Communication](https://arxiv.org/abs/2511.19103)

**Authors**: Dora Krekovic, Mario Kusek, Ivana Podnar Zarko, Danh Le-Phuoc  
**Category**: cs.LG  
**Published**: 2025-11-25  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.19103v1  

The rapid growth of IoT devices has led to an enormous amount of sensor data that requires transmission to cloud servers for processing, resulting in excessive network congestion, increased latency and high energy consumption. This is particularly problematic in resource-constrained and remote envir...

---

### 27. [NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations](https://arxiv.org/abs/2511.18793)

**Authors**: Yejing Wang, Shengyu Zhou, Jinyu Lu, Ziwei Liu, Langming Liu, Maolin Wang, Wenlin Zhang, Feng Li, Wenbo Su, Pengjie Wang, Jian Xu, Xiangyu Zhao  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.18793v1  

Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services an...

---

### 28. [AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks](https://arxiv.org/abs/2511.17506)

**Authors**: Narjes Nourzad, Mingyu Zong, Bhaskar Krishnamachari  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.17506v1  

Next-generation (NextG) cellular networks are expected to manage dynamic traffic while sustaining high performance. Large language models (LLMs) provide strategic reasoning for 6G planning, but their computational cost and latency limit real-time use. Multi-agent reinforcement learning (MARL) suppor...

---

### 29. [Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling](https://arxiv.org/abs/2511.19024)

**Authors**: Long Tang, Guoquan Zhen, Jie Hao, Jianbo Zhang, Huiyu Duan, Liang Yuan, Guangtao Zhai  
**Category**: cs.AI  
**Published**: 2025-11-25  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.19024v1  

Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision ...

---

### 30. [Pier: Efficient Large Language Model pretraining with Relaxed Global Communication](https://arxiv.org/abs/2511.17849)

**Authors**: Shuyuan Fan, Zhao Zhang  
**Category**: cs.DC  
**Published**: 2025-11-25  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.17849v1  

Global communication, such as all-reduce and allgather, is the prominent performance bottleneck in large language model (LLM) pretraining. To address this issue, we present Pier, an efficient and scalable optimizer with relaxed global communication. Pier is built upon DiLoCo, which leverages an inne...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
