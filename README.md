# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2025-12-29 06:01:44 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Accelerate Speculative Decoding with Sparse Computation in Verification](https://arxiv.org/abs/2512.21911)

**Authors**: Jikai Wang, Jianchao Tan, Yuxuan Hu, Jiayu Qin, Yerui Sun, Yuchen Xie, Xunliang Cai, Juntao Li, Min Zhang  
**Category**: cs.CL  
**Published**: 2025-12-29  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2512.21911v1  

#### Abstract
Speculative decoding accelerates autoregressive language model inference by verifying multiple draft tokens in parallel. However, the verification stage often becomes the dominant computational bottleneck, especially for long-context inputs and mixture-of-experts (MoE) models. Existing sparsificatio...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šAccelerate Speculative Decoding with Sparse Computation in Verification

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

è¯¥è®ºæ–‡é’ˆå¯¹ **Speculative Decoding** ä¸­çš„**éªŒè¯é˜¶æ®µï¼ˆverification stageï¼‰æˆä¸ºè®¡ç®—ç“¶é¢ˆ**çš„é—®é¢˜ã€‚å°½ç®¡ Speculative Decoding èƒ½é€šè¿‡å¹¶è¡ŒéªŒè¯å¤šä¸ª draft token åŠ é€Ÿç”Ÿæˆï¼Œä½†å½“è¾“å…¥ä¸Šä¸‹æ–‡å˜é•¿æˆ–ä½¿ç”¨ MoE æ¨¡å‹æ—¶ï¼ŒéªŒè¯è¿‡ç¨‹ä¸­çš„å…¨æ³¨æ„åŠ›ã€å¯†é›† FFN å’Œå¤šä¸“å®¶è®¡ç®—å¼€é”€å·¨å¤§ï¼Œå¯¼è‡´æ•ˆç‡æå‡å—é™ã€‚

æ­¤å¤–ï¼Œç°æœ‰çš„ç¨€ç–åŒ–æ–¹æ³•ï¼ˆå¦‚ Sparse Attentionã€Sparse FFNã€MoE Skippingï¼‰ä¸»è¦é¢å‘æ ‡å‡†è‡ªå›å½’è§£ç è®¾è®¡ï¼Œæœªè€ƒè™‘å¤š token å¹¶è¡ŒéªŒè¯çš„ç‰¹æ€§ï¼Œç›´æ¥åº”ç”¨å¯èƒ½å¯¼è‡´ç²¾åº¦ä¸‹é™æˆ–å†—ä½™æœªè¢«å……åˆ†æ¶ˆé™¤ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡ºäº†ä¸€å¥—**ç»Ÿä¸€çš„ç¨€ç–éªŒè¯æ¡†æ¶ï¼ˆsparse verification frameworkï¼‰**ï¼Œåœ¨éªŒè¯é˜¶æ®µå¯¹ä¸‰ä¸ªå…³é”®æ¨¡å—è¿›è¡Œè”åˆç¨€ç–åŒ–ï¼Œå¹¶å¼•å…¥è·¨å±‚é‡ç”¨æœºåˆ¶ä»¥è¿›ä¸€æ­¥å‡å°‘å†—ä½™ï¼š

1. **Sparse Attention**  
   - åŸºäºç¬¬ä¸€ä¸ª draft token çš„ query è¿›è¡Œé‡è¦æ€§è¯„åˆ†ï¼Œé€‰æ‹© top-N KV blocks å‚ä¸ attentionã€‚
   - å¼•å…¥ **piecewise budget control**ï¼šçŸ­åºåˆ—ä¸å‰ªæï¼Œé•¿åºåˆ—æŒ‰æ¯”ä¾‹æ§åˆ¶ä¿ç•™å—æ•°ï¼Œå¹³è¡¡æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚
   - ä¿ç•™é¦–å°¾ blocks ä»¥é˜²æ­¢ä¸¢å¤±å…³é”®ä¸Šä¸‹æ–‡ï¼ˆattention sink & locality biasï¼‰ã€‚

2. **Inter-layer Retrieval Reuse**  
   - è§‚å¯Ÿåˆ°ä¸­æ·±å±‚ Transformer å±‚é—´ block æ£€ç´¢é«˜åº¦ç›¸ä¼¼ã€‚
   - è®¾è®¡ **anchor layers**ï¼Œä»…åœ¨è¿™äº›å±‚æ‰§è¡Œæ£€ç´¢ï¼Œå…¶ä½™å±‚å¤ç”¨æœ€è¿‘å‰é©± anchor çš„ç»“æœï¼Œæ˜¾è‘—å‡å°‘é‡å¤æ£€ç´¢å¼€é”€ã€‚

3. **Sparse Feedforward Network (SFFN)**  
   - åœ¨éªŒè¯æ—¶è·³è¿‡æ¿€æ´»å€¼ä½äºé˜ˆå€¼ $T$ çš„ FFN é€šé“ã€‚
   - ä»…å¯¹æ´»è·ƒé€šé“æ‰§è¡Œ up-projection å’Œ down-projectionï¼Œé™ä½çŸ©é˜µä¹˜æ³•é‡ã€‚

4. **Sparse Mixture-of-Experts (SMoE)**  
   - æ‰©å±•åŠ¨æ€ä¸“å®¶è·³è¿‡æœºåˆ¶è‡³ä»»æ„æ¿€æ´»ä¸“å®¶æ•° $k > 2$ã€‚
   - å®šä¹‰æ¯å±‚å¯è·³è¿‡çš„æœ€å¤§ä¸“å®¶æ•° $m$ï¼ŒåŸºäºè·¯ç”±æƒé‡æ¯”å€¼è®¾å®šè·³è¿‡é˜ˆå€¼ $\beta_m$ï¼Œå®ç°è‡ªé€‚åº”è·³è¿‡ä½è´¡çŒ®ä¸“å®¶ã€‚

5. **Hybrid Sparse Method**  
   - å°†ä¸Šè¿°ä¸‰ç§ç¨€ç–ç­–ç•¥ç»„åˆï¼Œåœ¨ attentionã€FFNã€MoE ä¸‰ä¸ªæ­£äº¤ç»´åº¦åŒæ—¶ç¨€ç–åŒ–ï¼Œå½¢æˆç»¼åˆåŠ é€Ÿæ–¹æ¡ˆã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **é€‚ç”¨æ€§** | ä¸éœ€é‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œçº¯æ¨ç†æ—¶ä¼˜åŒ–ï¼Œé€‚ç”¨äº off-the-shelf LLMs |
| **é’ˆå¯¹æ€§** | é’ˆå¯¹ speculative decoding å¤š token éªŒè¯åœºæ™¯å®šåˆ¶ï¼Œè€Œéç®€å•ç§»æ¤æ ‡å‡†ç¨€ç–æ–¹æ³• |
| **ç³»ç»Ÿæ€§** | åŒæ—¶è¦†ç›– attentionã€FFNã€MoE ä¸‰å¤§è®¡ç®—ç“¶é¢ˆï¼Œæä¾›ç»Ÿä¸€æ¡†æ¶ |
| **é«˜æ•ˆæ€§** | å¼•å…¥ inter-layer reuse æ˜¾è‘—å‡å°‘å†—ä½™æ£€ç´¢ï¼Œæ— éœ€é¢å¤–å‚æ•° |
| **çµæ´»æ€§** | æ”¯æŒä¸åŒç¨€ç–å¼ºåº¦è°ƒèŠ‚ï¼ˆå¦‚ `Lo`, `T`, `m`ï¼‰ï¼Œå¯åœ¨ç²¾åº¦ä¸æ•ˆç‡é—´çµæ´»æƒè¡¡ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

å®éªŒåŸºäº **LongBench** åŸºå‡†ï¼Œæ¶µç›–å¤šç§ä»»åŠ¡ç±»å‹ï¼š

- **Summarization**: GovReport
- **Question Answering**: 2WikiMQA, HotpotQA
- **Code Editing**: LCC, RepoBench-P
- **Mathematical Reasoning**: GSM8K, Math, CollegeMath

---

### **å®éªŒè®¾ç½®**

| é¡¹ç›® | è®¾ç½®è¯´æ˜ |
|------|---------|
| **Target Models** | Llama3.1-8B-Instructï¼ˆSAï¼‰ã€Qwen3-30B-A3Bï¼ˆSFFNï¼‰ã€Deepseek-R1ï¼ˆSMoE & Hybridï¼‰ |
| **Draft Model** | EAGLE-3 æˆ– MTP heads |
| **Draft Structure** | Tree-structuredï¼Œå…± 60 ä¸ªå€™é€‰ token |
| **ç¡¬ä»¶å¹³å°** | 8 Ã— NVIDIA H800-80G GPUs |
| **ç¨€ç–å‚æ•°æœç´¢èŒƒå›´** | <br>- SA: `Lo âˆˆ {1K, 2K, 4K}`<br>- SFFN: `T âˆˆ {0.01, 0.05, 0.1}`<br>- SMoE: `m âˆˆ {2,3,4}` |

---

### **è¯„ä¼°æŒ‡æ ‡**

| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **ROUGE / F1 / Acc.** | ä»»åŠ¡æ€§èƒ½æŒ‡æ ‡ï¼ˆsummarization, QA, mathï¼‰ |
| **Sparsity (sa/sf/se)** | æ³¨æ„åŠ›/FFN/MoE çš„å¹³å‡ç¨€ç–åº¦ |
| **Mean Acceptance Length ($\bar{o}$)** | è¡¡é‡ draft ä¸ target å¯¹é½ç¨‹åº¦çš„å…³é”®æ•ˆç‡æŒ‡æ ‡ |
| **FLOPs Reduction** | ç†è®ºè®¡ç®—é‡ä¸‹é™ï¼ˆè§ Table 1ï¼‰ |

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

- **Strict**: æ ‡å‡† speculative decodingï¼Œæ— ä»»ä½•ç¨€ç–åŒ–ï¼ˆfull attention + dense FFN + full MoEï¼‰
- **SA / SFFN / SMoE**: å•ç‹¬å¯ç”¨å¯¹åº”ç¨€ç–æ¨¡å—
- **SA***: SA + inter-layer retrieval reuse
- **Hybrid**: ä¸‰è€…è”åˆç¨€ç–ï¼ˆSA + SFFN + SMoEï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»**

#### âœ… **Sparse Attention (Table 2)**

| æ–¹æ³• | Sparsity (sa) | GovReport (ROUGE) | 2WikiMQA (F1) | HotpotQA (F1) |
|------|-------------|------------------|--------------|--------------|
| Strict | 0 | 34.18 | 39.20 | 47.66 |
| SA (Lo=4K) | 0.34 | 33.50 (-0.68) | 39.32 (+0.12) | 47.43 (-0.23) |
| SA* (Lo=4K) | 0.34 | 33.28 (-0.90) | 35.78 (-3.42) | 44.78 (-2.88) |

> ğŸ” å‘ç°ï¼šé€‚åº¦ç¨€ç–ä¸‹æ€§èƒ½å‡ ä¹ä¸å˜ï¼›åŠ å…¥ inter-layer reuse å QA ç±»ä»»åŠ¡ç•¥æœ‰ä¸‹é™ï¼Œä½†é•¿æ–‡æœ¬æ‘˜è¦æ›´ç¨³å®šã€‚

---

#### âœ… **Sparse FFN (Table 3)**

| æ–¹æ³• | Sparsity (sf) | GovReport (ROUGE) | 2WikiMQA (F1) | GSM8K (Acc.) |
|------|-------------|------------------|--------------|-------------|
| Strict | 0 | 32.67 | 43.93 | 90.0 |
| SFFN (T=0.1) | 0.64 | **33.51 (+0.84)** | 41.96 (-1.97) | 91.0 (+1.0) |

> ğŸ” å‘ç°ï¼šå³ä½¿ç¨€ç–ç‡è¾¾ 64%ï¼Œæ€§èƒ½ä»ä¿æŒç¨³å¥ï¼Œéƒ¨åˆ†ä»»åŠ¡ç”šè‡³è½»å¾®æå‡ï¼Œè¡¨æ˜éªŒè¯é˜¶æ®µå­˜åœ¨å¤§é‡å†—ä½™æ¿€æ´»ã€‚

---

#### âœ… **Sparse MoE (Table 4)**

| æ–¹æ³• | Sparsity (se) | 2WikiMQA (F1) | Math (Acc.) | CollegeMath (Acc.) |
|------|-------------|--------------|------------|-------------------|
| Strict | 0 | 77.39 | 82.0 | 58.0 |
| SMoE (m=3) | 0.16 | **78.52 (+1.13)** | **87.0 (+5.0)** | 57.0 (-1.0) |
| SMoE (m=4) | 0.22 | 78.88 (+1.49) | 75.0 (-7.0) | 52.0 (-6.0) |

> ğŸ” å‘ç°ï¼šé€‚åº¦è·³è¿‡ä¸“å®¶ï¼ˆm=3ï¼‰å¯å»å™ªææ•ˆï¼›è¿‡åº¦è·³è¿‡ï¼ˆm=4ï¼‰æŸå®³å¤æ‚æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚

---

#### âœ… **Hybrid æ–¹æ³• (Table 5)**

| æ–¹æ³• | ROUGE (GovReport) | F1 (2WikiMQA) | Acc. (GSM8K) | $\bar{o}$ (å‡æ¥å—é•¿åº¦) |
|------|------------------|--------------|-------------|------------------------|
| Strict | 26.90 | 77.39 | 98.0 | 2.44 |
| Hybrid | **27.40 (+0.5)** | **79.82 (+2.43)** | 96.0 (-2.0) | 2.42 (-0.02) |

> ğŸ” å‘ç°ï¼šä¸‰é‡ç¨€ç–å åŠ åï¼Œå¤šæ•°ä»»åŠ¡æ€§èƒ½æŒå¹³æˆ–æå‡ï¼Œæ¥å—é•¿åº¦ä»…å¾®é™ï¼Œè¯´æ˜åˆ†å¸ƒåç§»å¯æ§ã€‚

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆéšå«åˆ†æï¼‰**

- **Inter-layer reuse**ï¼šå‡å°‘çº¦ 50% æ£€ç´¢æ“ä½œï¼ˆ30 anchor layers / 61 totalï¼‰ï¼ŒQA æ€§èƒ½ç•¥æŸï¼Œä½†é•¿æ–‡æœ¬æ›´ä¼˜ã€‚
- **Piecewise Budget Control**ï¼šé¿å…çŸ­åºåˆ—å› è¿‡åº¦å‰ªæå¯¼è‡´éšæœºæ€§ä¸Šå‡ï¼Œä¿éšœç¨³å®šæ€§ã€‚
- **Threshold Selection (T)**ï¼šT=0.05 æ˜¯ SFFN æœ€ä½³æŠ˜ä¸­ç‚¹ï¼Œå…¼é¡¾ç¨€ç–åº¦ä¸ç²¾åº¦ã€‚
- **Expert Skipping Limit (m)**ï¼šm=3 åœ¨ MoE ä¸­è¡¨ç°æœ€ä½³ï¼Œm=4 å¯¼è‡´ CollegeMath æ˜¾è‘—é€€åŒ–ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **éªŒè¯é˜¶æ®µå­˜åœ¨æ˜¾è‘—ç»“æ„æ€§å†—ä½™**  
   åœ¨ attentionã€FFN æ¿€æ´»ã€expert åˆ©ç”¨ç­‰å¤šä¸ªç»´åº¦å‡å¯å®‰å…¨ç¨€ç–åŒ–ï¼Œä¸”ä¸å½±å“æœ€ç»ˆè¾“å‡ºåˆ†å¸ƒä¸€è‡´æ€§ã€‚

2. **å¤š token éªŒè¯å…·æœ‰é«˜åº¦å…±äº«æ€§**  
   åŒä¸€æ­¥éª¤å†…å¤šä¸ª draft tokens æ£€ç´¢çš„ KV blocks é«˜åº¦é‡å ï¼ˆ>80%ï¼‰ï¼Œæ”¯æŒä»¥é¦–ä¸ª token ä¸ºä»£è¡¨è¿›è¡Œç»Ÿä¸€æ£€ç´¢ã€‚

3. **è·¨å±‚æ£€ç´¢æ¨¡å¼ç›¸ä¼¼æ€§å¼º**  
   ä¸­æ·±å±‚ Transformer å±‚é—´ block é€‰æ‹©é«˜åº¦ä¸€è‡´ï¼Œæ”¯æŒ inter-layer reuseï¼Œå¤§å¹…å‰Šå‡å†—ä½™è®¡ç®—ã€‚

4. **ç¨€ç–éªŒè¯å¯åœ¨é«˜ç¨€ç–åº¦ä¸‹ç»´æŒæ€§èƒ½**  
   - FFN ç¨€ç–åº¦è¾¾ 64% æ—¶æ€§èƒ½æ— æ˜æ˜¾ä¸‹é™
   - MoE è·³è¿‡ 16% ä¸“å®¶åè€Œæå‡éƒ¨åˆ†ä»»åŠ¡è¡¨ç°ï¼ˆå»å™ªæ•ˆåº”ï¼‰
   - Hybrid æ–¹æ³•å®ç°ä¸‰é‡å‹ç¼©ï¼Œæ€»ä½“æ€§èƒ½ç¨³å®š

5. **æ¥å—é•¿åº¦å½±å“æå°**  
   æ‰€æœ‰ç¨€ç–æ–¹æ³•å¼•èµ·çš„ mean acceptance length ä¸‹é™å‡å°äº 0.05ï¼Œå¯¹æ•´ä½“ååå½±å“å¯å¿½ç•¥ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

| å±€é™ | è¯´æ˜ |
|------|------|
| **å¯¹ç²¾ç»†ç¬¦å·æ¨ç†æ•æ„Ÿ** | å¦‚ CollegeMath åœ¨é«˜ç¨€ç–ä¸‹æ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼Œè¡¨æ˜å¤æ‚é€»è¾‘é“¾æ˜“å—å¹²æ‰° |
| **ä¾èµ– KV ç¼“å­˜ç»„ç»‡æ–¹å¼** | block-wise åˆ†å—å‡è®¾å¯èƒ½ä¸é€‚ç”¨äºæ‰€æœ‰æ¶æ„ï¼ˆå¦‚ sliding window attentionï¼‰ |
| **ç¼ºä¹ç«¯åˆ°ç«¯è®­ç»ƒé€‚é…** | å½“å‰ä¸º inference-only æ–¹æ³•ï¼Œè‹¥ç»“åˆè®­ç»ƒå¯èƒ½è¿›ä¸€æ­¥é‡Šæ”¾æ½œåŠ› |
| **inter-layer reuse å¯èƒ½å¼•å…¥è¯¯å·®ä¼ æ’­** | é”™è¯¯çš„ anchor layer æ£€ç´¢ä¼šå½±å“åç»­æ‰€æœ‰é anchor å±‚ |

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **åŠ¨æ€ç¨€ç–åº¦è°ƒæ§æœºåˆ¶**  
   æ ¹æ®è¾“å…¥é•¿åº¦ã€ä»»åŠ¡ç±»å‹è‡ªåŠ¨è°ƒæ•´ `Lo`, `T`, `m` å‚æ•°ï¼Œå®ç°è‡ªé€‚åº”ç¨€ç–ã€‚

2. **ç»“åˆè®­ç»ƒé˜¶æ®µç¨€ç–åŒ–**  
   æ¢ç´¢ joint training + sparse verification ååŒä¼˜åŒ–ï¼Œç¼“è§£ draft-target åˆ†å¸ƒåç§»ã€‚

3. **æ‰©å±•è‡³æ›´å¤šæ¨¡å‹ç»“æ„**  
   åº”ç”¨äº vision-language models æˆ– decoder-only æ¶æ„ä¸­çš„ cross-attention éªŒè¯ã€‚

4. **ç¡¬ä»¶å‹å¥½ç¨€ç–å®ç°**  
   ç»“åˆ NSAã€SnapKV ç­‰ç¡¬ä»¶å¯¹é½ç¨€ç–æ¨¡å¼ï¼Œæå‡å®é™…éƒ¨ç½²æ•ˆç‡ã€‚

5. **ç†è®ºåˆ†æç¨€ç–è¾¹ç•Œ**  
   å»ºç«‹ç¨€ç–ç¨‹åº¦ä¸ acceptance rateã€éªŒè¯å‡†ç¡®ç‡ä¹‹é—´çš„ç†è®ºå…³ç³»æ¨¡å‹ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼šæœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°å°†ç¨€ç–è®¡ç®—å¼•å…¥ speculative decoding çš„éªŒè¯é˜¶æ®µï¼Œæå‡ºä¸€ä¸ªæ— éœ€è®­ç»ƒã€å¤šç»´åº¦ååŒçš„ç¨€ç–æ¡†æ¶ï¼Œåœ¨æ˜¾è‘—é™ä½ FLOPs çš„åŒæ—¶ä¿æŒé«˜æ€§èƒ½ä¸ç¨³å®šæ¥å—é•¿åº¦ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é«˜æ•ˆæ¨ç†æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 2. [LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices](https://arxiv.org/abs/2512.21835)

**Authors**: Mingyu Sun, Xiao Zhang, Shen Qu, Yan Li, Mengbai Xiao, Yuan Yuan, Dongxiao Yu  
**Category**: cs.DC  
**Published**: 2025-12-29  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2512.21835v1  

#### Abstract
Large language models (LLMs) have emerged as a powerful foundation for intelligent reasoning and decision-making, demonstrating substantial impact across a wide range of domains and applications. However, their massive parameter scales and substantial resource demands pose critical challenges for ef...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šLIME: Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²é¢ä¸´**å†…å­˜å—é™ã€è®¡ç®—èƒ½åŠ›å¼±ã€ç½‘ç»œå¸¦å®½ä½**ç­‰æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚é‡åŒ–ï¼ˆquantizationï¼‰ã€è’¸é¦ï¼ˆdistillationï¼‰å’Œå‰ªæï¼ˆpruningï¼‰è™½ç„¶èƒ½é™ä½èµ„æºæ¶ˆè€—ï¼Œä½†ä¼šå¸¦æ¥**æ¨¡å‹ç²¾åº¦æŸå¤±**ï¼Œè¿™åœ¨é‡‘èé£æ§ã€åŒ»ç–—è¯Šæ–­ç­‰é«˜ç²¾åº¦è¦æ±‚åœºæ™¯ä¸­ä¸å¯æ¥å—ã€‚

æ­¤å¤–ï¼Œç°æœ‰çš„åˆ†å¸ƒå¼æ¨ç†æ¡†æ¶ï¼ˆå¦‚ PipeEdgeã€Galaxyã€TPI-LLMï¼‰é€šå¸¸å‡è®¾æ¯ä¸ªè®¾å¤‡æœ‰è¶³å¤Ÿçš„å†…å­˜æ¥å­˜å‚¨å…¶åˆ†é…çš„æ¨¡å‹åˆ†ç‰‡å’Œæ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆçš„ KV cacheï¼Œä½†åœ¨çœŸå®è¾¹ç¼˜ç¯å¢ƒä¸­ï¼Œè¿™ä¸€å‡è®¾å¾€å¾€ä¸æˆç«‹ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ ¸å¿ƒæ€è·¯
ä½œè€…æå‡º **LIME** â€”â€” ä¸€ç§é¢å‘å†…å­˜å—é™è¾¹ç¼˜è®¾å¤‡çš„**åä½œå¼æ— æŸå¤§æ¨¡å‹æ¨ç†åŠ é€Ÿç³»ç»Ÿ**ï¼Œå…¶æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬ï¼š

- **Interleaved Pipeline Parallelismï¼ˆäº¤é”™å¼æµæ°´çº¿å¹¶è¡Œï¼‰**  
  å°†ä¼ ç»Ÿæµæ°´çº¿ä¸åŠ¨æ€å¸è½½ï¼ˆoffloadingï¼‰æ·±åº¦èåˆï¼Œé€šè¿‡å¤šé˜¶æ®µäº¤é”™æ‰§è¡Œï¼Œå®ç°**å¸è½½æ—¶é—´ä¸è®¡ç®—å’Œé€šä¿¡æ—¶é—´çš„é«˜åº¦é‡å **ï¼Œä»è€Œå‡å°‘æ•´ä½“å»¶è¿Ÿã€‚

- **Fine-grained Offline Allocation Schedulerï¼ˆç»†ç²’åº¦ç¦»çº¿è°ƒåº¦å™¨ï¼‰**  
  åŸºäºå¼‚æ„è®¾å¤‡çš„è®¡ç®—ã€å†…å­˜å’ŒåŠ è½½èƒ½åŠ›ï¼Œæ„å»ºä¸€ä¸ª**ä»¥å»¶è¿Ÿæœ€å°åŒ–ä¸ºç›®æ ‡çš„æˆæœ¬æ¨¡å‹**ï¼Œé‡‡ç”¨åŠ¨æ€è§„åˆ’ç®—æ³•è¿›è¡Œé«˜æ•ˆå±‚åˆ†é…ï¼Œæœ€å¤§åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚

- **Online Memory Adaptation Strategyï¼ˆåœ¨çº¿å†…å­˜è‡ªé€‚åº”ç­–ç•¥ï¼‰**  
  åŒ…å«ä¸¤ä¸ªå­æ¨¡å—ï¼š
  - **Memory-aware Planner**ï¼šæ ¹æ® KV cache å¢é•¿åŠ¨æ€è§¦å‘ MHA æˆ– MLP æ¨¡å—çº§å¸è½½ï¼Œé¿å…å†…å­˜é¥±å’Œã€‚
  - **Bandwidth-sensitive KV Cache Transfer Protocol**ï¼šåˆ©ç”¨é«˜å†…å­˜ä½™é‡è®¾å¤‡ä½œä¸ºç›®æ ‡èŠ‚ç‚¹ï¼ˆdtargetï¼‰ï¼Œè·¨è®¾å¤‡è½¬ç§» KV cacheï¼Œç¼“è§£ç“¶é¢ˆè®¾å¤‡å‹åŠ›ï¼Œå¹¶é€‚åº”ç½‘ç»œæ³¢åŠ¨ã€‚

- **Block-level Fine-grained Offloadingï¼ˆå—çº§ç»†ç²’åº¦å¸è½½ï¼‰**  
  ä¸å†ä»¥æ•´å±‚ä¸ºå•ä½å¸è½½ï¼Œè€Œæ˜¯å°†æ¯å±‚æ‹†åˆ†ä¸º MHA å’Œ MLP æ¨¡å—åˆ†åˆ«ç®¡ç†ï¼Œæ˜¾è‘—å‡å°‘ä¸å¿…è¦çš„å‚æ•°ä¼ è¾“å¼€é”€ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | LIME çš„ä¼˜åŠ¿ |
|------|-------------|
| **å‡†ç¡®æ€§** | å®ç° **lossless inference**ï¼Œæ— ç²¾åº¦æŸå¤± |
| **å†…å­˜æ•ˆç‡** | æ”¯æŒè®¾å¤‡å†…å­˜ä¸è¶³ä»¥å®¹çº³å•ä¸ªæ¨¡å‹åˆ†ç‰‡ + KV cache çš„æç«¯åœºæ™¯ |
| **å»¶è¿Ÿä¼˜åŒ–** | é€šè¿‡äº¤é”™æµæ°´çº¿å’Œç»†ç²’åº¦è°ƒåº¦ï¼Œæ˜¾è‘—æå‡è®¡ç®—/é€šä¿¡/å¸è½½çš„é‡å ç‡ |
| **é²æ£’æ€§** | åœ¨ç½‘ç»œå¸¦å®½æ³¢åŠ¨å’Œ KV cache åŠ¨æ€å¢é•¿ä¸‹ä»ä¿æŒé«˜æ€§èƒ½ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ¨¡å‹
å®éªŒåŸºäºä¸‰ä¸ªä¸»æµ LLMsï¼Œè¦†ç›–ä» 13B åˆ° 70B å‚æ•°è§„æ¨¡ï¼š
- **Llama2-13B-Instruct**
- **Qwen3-32B**
- **Llama3.3-70B-Instruct**

è¯¦ç»†é…ç½®è§ Table IIIã€‚

### å®éªŒå¹³å°ä¸ç¡¬ä»¶ç¯å¢ƒ
ä½¿ç”¨ä¸‰ç§ NVIDIA Jetson è¾¹ç¼˜è®¾å¤‡æ„å»ºå¼‚æ„æµ‹è¯•åºŠï¼š
- **Jetson Xavier NX (16GB)**
- **Jetson AGX Orin (32GB)**
- **Jetson AGX Orin (64GB)**

å…±æ­å»ºä¸‰ä¸ªå®éªŒç¯å¢ƒï¼ˆE1â€“E3ï¼‰ï¼Œè®¾å¤‡ç»„åˆè¯¦è§ Table IVã€‚æ‰€æœ‰è®¾å¤‡é€šè¿‡äº¤æ¢æœºè¿æ¥ï¼Œæ€»å¸¦å®½ 1000 Mbpsï¼Œä½¿ç”¨ Linux TC å·¥å…·æ¨¡æ‹Ÿ **100 Mbps å’Œ 200 Mbps** çš„è¾¹å¸¦å®½æ¡ä»¶ã€‚

### è¯·æ±‚æ¨¡å¼
è€ƒè™‘ä¸¤ç§å…¸å‹è¾¹ç¼˜è¯·æ±‚æ¨¡å¼ï¼š
- **Sporadic Request Pattern**ï¼šå¾®æ‰¹æ¬¡å¤§å°ä¸º 1ï¼Œæ¨¡æ‹Ÿé›¶æ˜Ÿè¯·æ±‚ï¼ˆå¦‚è¯­éŸ³åŠ©æ‰‹ï¼‰
- **Bursty Request Pattern**ï¼šå¤šä¸ªè¯·æ±‚åŒæ—¶æäº¤ï¼Œæ¨¡æ‹Ÿæ‰¹é‡å¤„ç†

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
å…±æ¯”è¾ƒ 6 ç§åŸºçº¿ï¼š
1. **Pipeline Parallelism**
2. **Pipeline + Offloading**
3. **EdgeShard**ï¼ˆåŸºäº PP çš„é«˜æ•ˆè¾¹ç¼˜æ¨ç†æ¡†æ¶ï¼‰
4. **Galaxy**ï¼ˆåŸºäº TP çš„åˆ†å¸ƒå¼æ¨ç†ï¼‰
5. **TPI-LLM**ï¼ˆä¸“ä¸ºå†…å­˜å—é™è®¾è®¡çš„ TP æ¡†æ¶ï¼‰
6. **TPI-LLM + Offloading**

éƒ¨åˆ†åŸºçº¿æ— æ³•è¿è¡Œæ—¶æ ‡è®°ä¸º **OOMï¼ˆOut-of-Memoryï¼‰** æˆ– **OOTï¼ˆOut-of-Timeï¼Œå»¶è¿Ÿè¶…é˜ˆå€¼ï¼‰**ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **Inference Latency (ms/token)**ï¼šä¸»è¦æ€§èƒ½æŒ‡æ ‡
- **Speedup**ï¼šç›¸å¯¹äºåŸºçº¿çš„åŠ é€Ÿæ¯”
- **Ablation Study**ï¼šéªŒè¯å„ç»„ä»¶æœ‰æ•ˆæ€§

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆLlama3.3-70B-Instructï¼ŒE3 ç¯å¢ƒï¼‰
åœ¨å››å°å¼‚æ„ Jetson è®¾å¤‡ä¸Šè¿è¡Œ LLaMA3.3-70B-Instruct æ¨¡å‹ï¼š

| è¯·æ±‚æ¨¡å¼ | ç½‘ç»œå¸¦å®½ | LIME æ¨ç†å»¶è¿Ÿ | ç›¸å¯¹ SOTA åŠ é€Ÿæ¯” |
|----------|-----------|----------------|--------------------|
| Sporadic | 100/200 Mbps | **1501.9 ms/token** | **1.7Ã—** |
| Bursty   | 100/200 Mbps | **423.2 ms/token**  | **3.7Ã—** |

> æ³¨ï¼šSOTA åŸºçº¿æŒ‡ Pipeline+Offloading å’Œ EdgeShardã€‚

### ä¸å…¶ä»–æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
#### åœ¨å¸¸è§„å†…å­˜æ¡ä»¶ä¸‹ï¼ˆFig. 12â€“14ï¼‰ï¼š
- **ç›¸æ¯” TP-based æ–¹æ³•ï¼ˆGalaxy, TPI-LLMï¼‰**ï¼š
  - Burst åœºæ™¯ä¸‹åŠ é€Ÿ **3.4Ã— ~ 28Ã—**
  - Sporadic åœºæ™¯ä¸‹åŠ é€Ÿ **2.5Ã— ~ 10.8Ã—**
  - åŸå› ï¼šTP å¯¹é€šä¿¡ä¾èµ–å¼ºï¼Œåœ¨ä½å¸¦å®½è¾¹ç¼˜ç¯å¢ƒä¸‹é€šä¿¡å¼€é”€å·¨å¤§ã€‚

- **ç›¸æ¯” PP-based æ–¹æ³•ï¼ˆPipeline+Offloading, EdgeShardï¼‰**ï¼š
  - Burst åœºæ™¯ä¸‹åŠ é€Ÿ **1.9Ã— ~ 10.2Ã—**
  - Sporadic åœºæ™¯ä¸‹åŠ é€Ÿ **2.5Ã— ~ 7.6Ã—**
  - åŸå› ï¼šLIME çš„äº¤é”™æµæ°´çº¿å’Œç»†ç²’åº¦è°ƒåº¦æ›´ä¼˜åœ°æ©ç›–äº†å¸è½½å»¶è¿Ÿã€‚

#### åœ¨æä½å†…å­˜åœºæ™¯ï¼ˆSetting 1â€“3ï¼ŒFig. 15â€“17ï¼‰ï¼š
- å¤šæ•°åŸºçº¿å‡ºç° **OOM æˆ– OOT**ï¼ˆå»¶è¿Ÿ >40s/token æˆ– >15s/tokenï¼‰
- LIME ä»å¯ç¨³å®šè¿è¡Œï¼Œä¸”å»¶è¿Ÿè¿œä½äºå¯è¿è¡ŒåŸºçº¿ï¼ˆå¦‚ TPI-LLM åœ¨ sporadic ä¸‹è¾¾ OOTï¼‰
- è¡¨æ˜ LIME å…·å¤‡æ›´å¼ºçš„**å†…å­˜å¼¹æ€§ä¸é²æ£’æ€§**

### æ¶ˆèå®éªŒç»“æœï¼ˆTable Vï¼‰
éªŒè¯äº† LIME å„ç»„ä»¶çš„é‡è¦æ€§ï¼š

| æ–¹æ³• | Sporadic å»¶è¿Ÿ | Speedup | Bursty å»¶è¿Ÿ | Speedup |
|------|----------------|---------|--------------|---------|
| LIMEï¼ˆå®Œæ•´ï¼‰ | **1501.9 ms** | 1.00Ã— | **423.2 ms** | 1.00Ã— |
| ç¼ºå°‘ KV Cache Transfer | 1748.2 ms | 0.86Ã— | 489.0 ms | 0.87Ã— |
| ç¼ºå°‘ Memory-aware Planner | 2251.7 ms | 0.67Ã— | 614.8 ms | 0.69Ã— |

> ç»“è®ºï¼šä¸¤ä¸ªåœ¨çº¿ç­–ç•¥å‡æ˜¾è‘—å½±å“æ€§èƒ½ï¼Œå°¤å…¶æ˜¯ memory-aware planner è´¡çŒ®æœ€å¤§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **äº¤é”™å¼æµæ°´çº¿ï¼ˆInterleaved Pipelineï¼‰æ˜¯å®ç°é«˜æ•ˆå¸è½½é‡å çš„å…³é”®æœºåˆ¶**ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæµæ°´çº¿ä¸­â€œå¸è½½å»¶è¿Ÿæ— æ³•è¢«æ©ç›–â€çš„é—®é¢˜ã€‚
2. **å—çº§ç»†ç²’åº¦å¸è½½ï¼ˆMHA/MLP åˆ†ç¦»ï¼‰æ˜¾è‘—é™ä½äº†åŠ è½½å¼€é”€**ï¼Œä¼˜äºä¼ ç»Ÿçš„æ•´å±‚å¸è½½ã€‚
3. **åœ¨çº¿å†…å­˜è‡ªé€‚åº”ç­–ç•¥ä½¿ç³»ç»Ÿå…·å¤‡åº”å¯¹ KV cache å¢é•¿å’Œç½‘ç»œæ³¢åŠ¨çš„èƒ½åŠ›**ï¼Œæå‡äº†å®é™…éƒ¨ç½²ä¸­çš„ç¨³å®šæ€§ã€‚
4. LIME æˆåŠŸå®ç°äº†åœ¨**å†…å­˜ä¸¥é‡ä¸è¶³çš„è¾¹ç¼˜è®¾å¤‡é›†ç¾¤ä¸Šè¿è¡Œ 70B çº§åˆ« LLM çš„æ— æŸæ¨ç†**ï¼Œä¸”æ€§èƒ½ä¼˜äºæ‰€æœ‰ç°æœ‰æ–¹æ¡ˆã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ– SSD å­˜å‚¨æ€§èƒ½**ï¼šé¢‘ç¹çš„ offloading å¯¹æœ¬åœ° SSD çš„è¯»å†™é€Ÿåº¦æœ‰è¾ƒé«˜è¦æ±‚ï¼Œåœ¨ä½ç«¯è®¾å¤‡ä¸Šå¯èƒ½æˆä¸ºç“¶é¢ˆã€‚
- **è°ƒåº¦å¤æ‚åº¦å¢åŠ **ï¼šç¦»çº¿è°ƒåº¦å™¨è™½é«˜æ•ˆï¼Œä½†ä»éœ€é¢„ä¼°åºåˆ—é•¿åº¦ï¼›åœ¨çº¿ç­–ç•¥å¢åŠ äº†æ§åˆ¶é€»è¾‘å¤æ‚æ€§ã€‚
- **æœªæ”¯æŒè®­ç»ƒ**ï¼šå½“å‰ä»…é’ˆå¯¹æ¨ç†åœºæ™¯ï¼Œå°šæœªæ‰©å±•åˆ°ååŒè®­ç»ƒã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³æ›´å¤šç±»å‹çš„å¹¶è¡ŒèŒƒå¼ï¼ˆå¦‚ SP + Offloadingï¼‰ã€‚
- æ”¯æŒåŠ¨æ€è¾“å…¥é•¿åº¦çš„æ›´ç²¾å‡†è°ƒåº¦é¢„æµ‹ã€‚
- æ¢ç´¢åœ¨è”é‚¦å­¦ä¹ æˆ–ç§»åŠ¨è®¾å¤‡ç¾¤ä¸Šçš„è½»é‡åŒ–éƒ¨ç½²ç‰ˆæœ¬ã€‚
- é›†æˆæ›´å…ˆè¿›çš„ KV cache å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚ KVQuantï¼‰è¿›ä¸€æ­¥é™ä½å­˜å‚¨å‹åŠ›ã€‚

---

> âœ… æ€»ç»“ï¼šLIME æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§è§£å†³â€œ**å†…å­˜æåº¦å—é™ + ç½‘ç»œå¸¦å®½æœ‰é™ + æ— æŸæ¨ç†**â€ä¸‰éš¾å›°å¢ƒçš„åä½œæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡**äº¤é”™æµæ°´çº¿ + ç»†ç²’åº¦è°ƒåº¦ + åœ¨çº¿è‡ªé€‚åº”**ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯ï¼Œåœ¨çœŸå®è¾¹ç¼˜å¹³å°ä¸Šå®ç°äº†é«˜è¾¾ **3.7Ã— çš„ç«¯åˆ°ç«¯åŠ é€Ÿ**ï¼Œä¸ºå¤§æ¨¡å‹è½åœ°è¾¹ç¼˜æä¾›äº†å¯è¡Œè·¯å¾„ã€‚

</details>

---

### 3. [FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion](https://arxiv.org/abs/2512.22036)

**Authors**: Zhuoran Zhu, Chunyang Zhu, Hao Lin, Xu Fu, Yiming Zhou, Quanlu Zhang, Zhenhua Li, Feng Qian, Chao Yu, Boxun Li, Guohao Dai, Yu Wang  
**Category**: cs.DC  
**Published**: 2025-12-29  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.22036v1  

#### Abstract
Large-scale Mixture-of-Experts (MoE) models rely on \emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuff...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šFUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¤§å‹ **Mixture-of-Experts (MoE)** æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ä¾èµ– **expert parallelism**ï¼Œå³å°†ä¸“å®¶ï¼ˆexpertsï¼‰åˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šã€‚è¿™å¯¼è‡´æ¯ä¸ª token å¿…é¡»è¢«è·¯ç”±åˆ°å…¶å¯¹åº”çš„ä¸“å®¶æ‰€åœ¨çš„è®¾å¤‡ï¼Œä»è€Œå¼•å‘å¤§è§„æ¨¡çš„åˆ†å¸ƒå¼æ•°æ®é‡æ’ï¼ˆdata shufflingï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é€šä¿¡åº“ï¼ˆå¦‚ NCCLã€DeepEPï¼‰å°†æ•°æ®å˜æ¢ï¼ˆtransformationï¼‰ä¸é€šä¿¡ï¼ˆcommunicationï¼‰åˆ†ç¦»å¤„ç†ï¼Œé€ æˆå¤§é‡å†—ä½™çš„æ•°æ®å¤åˆ¶å’Œå†…å­˜é‡æ’æ“ä½œï¼Œä½¿å¾—æ•°æ® shuffling å¼€é”€å ç«¯åˆ°ç«¯è¿è¡Œæ—¶é—´çš„ **22%-61%**ï¼Œæˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ ¸å¿ƒæ€æƒ³
æå‡º **FUSCO** â€”â€” ä¸€ç§é¢å‘ MoE çš„é«˜æ•ˆé€šä¿¡åº“ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**èåˆæ•°æ®å˜æ¢ä¸é€šä¿¡ï¼ˆTransformation-Communication Fusionï¼‰**ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š
- **ç»Ÿä¸€çš„æ•°æ®æŠ½è±¡ï¼šSegment Descriptor**
  - å°†å¾…ä¼ è¾“çš„æ•°æ®ï¼ˆå¦‚ tokensï¼‰å»ºæ¨¡ä¸ºä¸€ç³»åˆ—é€»è¾‘ä¸Šçš„â€œæ®µâ€ï¼ˆsegmentsï¼‰ï¼Œå¹¶é€šè¿‡ `Segment Descriptor` æè¿°æ¯ä¸ªæ®µçš„æºåœ°å€ã€ç›®æ ‡åœ°å€å’Œå¤§å°ã€‚
  - è¯¥æè¿°ç¬¦ç›´æ¥ç¼–ç äº†æ•°æ®å¸ƒå±€è½¬æ¢ä¿¡æ¯ï¼Œä½¿é€šä¿¡è¿‡ç¨‹æœ¬èº«å°±èƒ½å®Œæˆæ•°æ®é‡æ’ï¼Œæ— éœ€é¢å¤–çš„é¢„/åå¤„ç†æ­¥éª¤ã€‚

- **æ•°æ®èåˆé€šä¿¡å¼•æ“ï¼ˆData-Fused Communication Engine, dCommï¼‰**
  - åŸºäº descriptor é©±åŠ¨çš„æµæ°´çº¿è®¾è®¡ï¼Œåœ¨æ•°æ®ä» GPU å†…å­˜æ‹·è´åˆ° NIC å‘é€ç¼“å†²åŒºçš„è¿‡ç¨‹ä¸­ï¼Œ**å†…è”æ‰§è¡Œæ•°æ®é‡æ’**ã€‚
  - åˆ©ç”¨ GPUDirect P2P å’Œ RDMA æŠ€æœ¯å®ç°é«˜æ•ˆçš„ intra-node å’Œ inter-node æ•°æ®ç§»åŠ¨ï¼Œå¹¶é€šè¿‡åˆ‡ç‰‡ï¼ˆsliceï¼‰æœºåˆ¶éšè— descriptor å¤„ç†å¼€é”€ã€‚

- **ä¸¤å±‚é€šä¿¡è§„åˆ’å™¨ï¼ˆCommunication Plannerï¼‰**
  - æ„å»º **node-level forwarder + expert-level distribution** çš„ä¸¤çº§ descriptor ç»“æ„ï¼Œæ”¯æŒè·¨èŠ‚ç‚¹å»é‡ï¼ˆdeduplicationï¼‰ï¼šåŒä¸€ token è¢«å¤šä¸ªåŒèŠ‚ç‚¹å†…çš„ä¸“å®¶ä½¿ç”¨æ—¶ï¼Œä»…éœ€è·¨èŠ‚ç‚¹å‘é€ä¸€æ¬¡ã€‚

- **åœ¨çº¿è´Ÿè½½å‡è¡¡å™¨ï¼ˆOnline Load Balancerï¼‰**
  - åŠ¨æ€åˆ†é… forwarder GPUï¼Œé¿å…çƒ­ç‚¹ï¼Œæå‡å¸¦å®½åˆ©ç”¨ç‡ã€‚
  - ä½¿ç”¨è½»é‡çº§è´ªå¿ƒç®—æ³•è¿›è¡Œåˆ†ç»„è°ƒåº¦ï¼Œå¤æ‚åº¦ä½ï¼Œé€‚åˆæ¯«ç§’çº§é€šä¿¡åœºæ™¯ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | NCCL / DeepEP | FUSCO |
|------|----------------|--------|
| æ•°æ®é‡æ’ | æ˜¾å¼å¤šæ¬¡å†…å­˜æ‹·è´ï¼ˆpre/post-rearrangementï¼‰ | å®Œå…¨æ¶ˆé™¤ï¼Œèåˆè¿›é€šä¿¡è·¯å¾„ |
| è·¨èŠ‚ç‚¹é€šä¿¡ | ä¸æ„ŸçŸ¥ token é‡å¤ï¼Œå¯èƒ½å¤šæ¬¡å‘é€ç›¸åŒæ•°æ® | æ”¯æŒè·¨èŠ‚ç‚¹å»é‡ï¼Œå‡å°‘å†—ä½™æµé‡ |
| é€šä¿¡æ¨¡å¼ | å›ºå®šæ‹“æ‰‘æ— å…³çš„ all-to-all | æ‹“æ‰‘æ„ŸçŸ¥ã€å±‚çº§è·¯ç”±ä¼˜åŒ– |
| æ€§èƒ½å¯æ‰©å±•æ€§ | éš expert parallelism å¢åŠ ï¼Œshuffle å æ¯”ä¸Šå‡ | æ˜¾è‘—é™ä½ shuffle æ—¶é—´å æ¯” |
| æ˜“é›†æˆæ€§ | éœ€æ¡†æ¶æ‰‹åŠ¨ç®¡ç†é‡æ’æµç¨‹ | æä¾›å¯¹ NCCL çš„ drop-in æ›¿ä»£æ¥å£ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### å®éªŒå¹³å°
- **ç¡¬ä»¶é…ç½®**ï¼š8 èŠ‚ç‚¹é›†ç¾¤ï¼Œæ¯èŠ‚ç‚¹é…å¤‡ï¼š
  - 8 Ã— NVIDIA H100 GPUï¼ˆ80GB HBM3ï¼‰
  - NVLinkï¼ˆintra-nodeï¼Œ~480 GB/sï¼‰
  - 10 Ã— 400Gbps Mellanox ConnectX-7 NICsï¼ˆinter-nodeï¼ŒRoCEï¼‰
- **è½¯ä»¶æ ˆ**ï¼šUbuntu 24.04, CUDA 12.9, NCCL 2.26.3, PyTorch 2.7.0

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **NCCL** | é€šç”¨é€šä¿¡åº“ | å¹¿æ³›ä½¿ç”¨çš„ all-to-all å®ç°ï¼Œæ—  MoE ä¸“ç”¨ä¼˜åŒ– |
| **DeepEP** | MoE ä¸“ç”¨é€šä¿¡åº“ | åŸºäº NVSHMEMï¼Œæ”¯æŒéƒ¨åˆ†å»é‡å’Œæµæ°´çº¿ï¼Œå½“å‰ SOTA |

### è¯„ä¼°ä»»åŠ¡ä¸æ•°æ®é›†
- **é€šä¿¡åŸºå‡†æµ‹è¯•ï¼ˆCommunication Benchmarksï¼‰**
  - ä½¿ç”¨çœŸå® MoE æ¨ç†æµé‡ï¼ˆæ¥è‡ª DeepSeek-V3 åœ¨ ShareGPT æ•°æ®é›†ä¸Šçš„ traceï¼‰
  - æ§åˆ¶å˜é‡æ„é€ ä¸¤ç§æç«¯æƒ…å†µï¼š
    1. **Single-node routed**ï¼šä¸€ä¸ª token çš„æ‰€æœ‰ top-k ä¸“å®¶å‡ä½äºåŒä¸€èŠ‚ç‚¹ â†’ æµ‹è¯•å»é‡èƒ½åŠ›
    2. **Load-imbalanced**ï¼šäººä¸ºåˆ¶é€  GPU é—´é€šä¿¡è´Ÿè½½ä¸å‡ â†’ æµ‹è¯•è´Ÿè½½å‡è¡¡æœ‰æ•ˆæ€§
  - åºåˆ—é•¿åº¦ï¼š4k, 8k, 16k, 32k
  - MoE å‚æ•°è§ Table 2ï¼šEP=64, Hidden Dim=7168, Top-k=8, Num Experts=256

- **ç«¯åˆ°ç«¯ä»»åŠ¡**
  - **è®­ç»ƒ**ï¼šé›†æˆè‡³ Megatron-LMï¼Œä½¿ç”¨ Qwen3 å’Œ DeepSeek-V3 æ¨¡å‹
  - **æ¨ç†**ï¼šé›†æˆè‡³ SGLangï¼Œæµ‹é‡ **Time-to-First-Token (TTFT)**

### è¯„ä¼°æŒ‡æ ‡
- é€šä¿¡å»¶è¿Ÿï¼ˆlatencyï¼‰åˆ†è§£ï¼špreprocessing, rearrangement, communication
- ç«¯åˆ°ç«¯æ€§èƒ½ï¼š
  - è®­ç»ƒï¼šæ¯ iteration æ—¶é—´ï¼ˆspeedupï¼‰
  - æ¨ç†ï¼šTTFT åŠ é€Ÿæ¯”
- æ¶ˆèå®éªŒï¼šåˆ†åˆ«å…³é—­ dCommã€Plannerã€Balancer æ¨¡å—

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### é€šä¿¡åŸºå‡†æµ‹è¯•ç»“æœ
| åœºæ™¯ | å¯¹æ¯”å¯¹è±¡ | åŠ é€Ÿæ¯”ï¼ˆSpeedupï¼‰ |
|------|----------|------------------|
| Real-world traffic | vs NCCL | **1.60Ã— ~ 1.66Ã—** |
| | vs DeepEP | **1.13Ã— ~ 1.34Ã—** |
| Single-node routed | vs NCCL | **3.47Ã— ~ 3.84Ã—** |
| | vs DeepEP | **1.95Ã— ~ 2.01Ã—** |
| Load-imbalanced | vs NCCL | **1.99Ã— ~ 2.24Ã—** |
| | vs DeepEP | **1.29Ã— ~ 1.42Ã—** |

> ğŸ” **å…³é”®è§‚å¯Ÿ**ï¼š
> - åœ¨ single-node routed åœºæ™¯ä¸‹ï¼ŒFUSCO è¡¨ç°å‡ºæœ€å¤§ä¼˜åŠ¿ï¼Œè¯´æ˜å…¶ **å»é‡æœºåˆ¶éå¸¸æœ‰æ•ˆ**ã€‚
> - DeepEP ä¹Ÿæœ‰ä¸€å®šå»é‡èƒ½åŠ›ï¼Œä½†ä¸å¦‚ FUSCO çš„ä¸¤çº§ descriptor è®¾è®¡çµæ´»é«˜æ•ˆã€‚
> - å°åºåˆ—é•¿åº¦ï¼ˆå¦‚ 4kï¼‰ä¸‹ FUSCO ä¼˜åŠ¿è¾ƒå°ï¼Œå› å…¶æœ‰å›ºå®š preprocessing å¼€é”€ã€‚

### ç«¯åˆ°ç«¯æ€§èƒ½
| ä»»åŠ¡ | å¯¹æ¯”å¯¹è±¡ | åŠ é€Ÿæ¯” |
|------|----------|--------|
| **Training**ï¼ˆQwen & DeepSeek-V3ï¼‰ | vs NCCL | **1.17Ã— ~ 1.39Ã—** |
| | vs DeepEP | **1.10Ã— ~ 1.19Ã—** |
| **Inference TTFT** | vs NCCL | **1.09Ã— ~ 1.25Ã—** |
| | vs DeepEP | **1.06Ã— ~ 1.16Ã—** |

> âœ… **è¶‹åŠ¿åˆ†æ**ï¼šæ¨¡å‹è§„æ¨¡è¶Šå¤§ï¼Œé€šä¿¡å æ¯”è¶Šé«˜ï¼ŒFUSCO çš„æ”¶ç›Šè¶Šæ˜æ˜¾ã€‚

### æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰
åœ¨ sequence length = 16k ä¸‹å…³é—­å„æ¨¡å—çš„ç»“æœï¼ˆå•ä½ï¼šmsï¼‰ï¼š

| åœºæ™¯ | FUSCOï¼ˆå®Œæ•´ï¼‰ | dComm-off | Planner-off | Balancer-off |
|------|---------------|-----------|-------------|--------------|
| Real-world | 86.8 | 119.5 (-37.7%) | 124.4 (-43.2%) | 95.1 (-8.7%) |
| Single-node routed | 40.99 | 61.47 (-33.3%) | 125.42 (-67.3%) | 42.36 (-3.2%) |
| Imbalanced | 151.3 | 219.8 (-31.2%) | 207.5 (-27.1%) | 181.5 (-16.6%) |

> ğŸ“Œ **ç»“è®º**ï¼š
> - **dComm æ˜¯æœ€æ ¸å¿ƒç»„ä»¶**ï¼Œå¹³å‡å¸¦æ¥ ~30% æ€§èƒ½æå‡ï¼Œä¸»è¦æºäºæ¶ˆé™¤æ•°æ®é‡æ’ã€‚
> - **Planner è‡³å…³é‡è¦**ï¼Œå°¤å…¶åœ¨ single-node åœºæ™¯ä¸‹ï¼Œç¼ºå°‘å®ƒä¼šå¯¼è‡´è·¨èŠ‚ç‚¹é€šä¿¡æš´å¢ã€‚
> - **Balancer åœ¨è´Ÿè½½ä¸å‡æ—¶ä½œç”¨æ˜¾è‘—**ï¼ˆæœ€é«˜æ”¹å–„ 16.6%ï¼‰ï¼Œä½†åœ¨å‡è¡¡åœºæ™¯ä¸­å½±å“å°ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **MoE ä¸­çš„æ•°æ® shuffling å¼€é”€ä¸»è¦æ¥è‡ªâ€œéé€šä¿¡â€éƒ¨åˆ†**ï¼šæœ¬åœ°æ•°æ®é‡æ’ï¼ˆrearrangementï¼‰å¯å æ€»å»¶è¿Ÿçš„ **25%-69%**ï¼Œè¿œè¶…ä¼ ç»Ÿè®¤çŸ¥ã€‚
2. **é€šä¿¡ä¸åº”åªæ˜¯â€œæ¬è¿å·¥â€**ï¼šFUSCO è¯æ˜ï¼Œé€šè¿‡å°†æ•°æ®å¸ƒå±€è¯­ä¹‰åµŒå…¥é€šä¿¡åŸè¯­ï¼Œå¯ä»¥åœ¨é€šä¿¡è¿‡ç¨‹ä¸­è‡ªç„¶å®Œæˆæ•°æ®é‡æ’ï¼Œå¤§å¹…å‡å°‘å†—ä½™æ“ä½œã€‚
3. **èåˆä¼˜äºåˆ†æ²»**ï¼šå°† transformation ä¸ communication èåˆï¼Œä¸ä»…èƒ½å‡å°‘å†…å­˜æ‹·è´ï¼Œè¿˜èƒ½æ›´å¥½åœ°åˆ©ç”¨ç°ä»£ GPU-NIC æ¶æ„ä¸­çš„å¸¦å®½å·®å¼‚å’Œå¹¶è¡Œæ½œåŠ›ã€‚
4. **ç³»ç»Ÿçº§ååŒè®¾è®¡æ›´æœ‰æ•ˆ**ï¼šFUSCO çš„ success æ¥è‡ªäº descriptor æŠ½è±¡ã€pipelined engineã€topology-aware planner å’Œ online balancer çš„ååŒå·¥ä½œã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–åº•å±‚é€šä¿¡åº“ï¼ˆNCCLï¼‰**ï¼šè™½ç„¶æ„å»ºåœ¨ NCCL ä¸Šæå‡äº†å¯ç§»æ¤æ€§ï¼Œä½†ä¹Ÿé™åˆ¶äº†å¯¹æ›´åº•å±‚åè®®çš„æ·±åº¦å®šåˆ¶ç©ºé—´ã€‚
- **descriptor ç®¡ç†æœ‰ä¸€å®šå…ƒæ•°æ®å¼€é”€**ï¼šå¯¹äºæå°ç²’åº¦æˆ–åŠ¨æ€å˜åŒ–å‰§çƒˆçš„ routing patternï¼Œdescriptor æ„é€ æˆæœ¬å¯èƒ½ä¸å¯å¿½ç•¥ã€‚
- **ç›®å‰ä¸»è¦é’ˆå¯¹ MoE åœºæ™¯ä¼˜åŒ–**ï¼šå°½ç®¡ä½œè€…è®¤ä¸ºå¯æ¨å¹¿è‡³ Vision Transformer ç­‰ç»“æ„åŒ–æ•°æ®é‡æ’åœºæ™¯ï¼Œä½†å°šæœªéªŒè¯å…¶ä»– workloadã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³æ›´å¤šåˆ†å¸ƒå¼æ¨¡å¼ï¼ˆå¦‚ tensor parallelism + expert parallelism æ··åˆï¼‰
- æ”¯æŒåŠ¨æ€ routingï¼ˆä¾‹å¦‚åŸºäº content-aware routing çš„å®æ—¶è°ƒæ•´ï¼‰
- è¿›ä¸€æ­¥ä¼˜åŒ– descriptor ç¼–ç æ–¹å¼ä»¥é™ä½å…ƒæ•°æ®å¼€é”€
- æ¢ç´¢åœ¨ CPU-GPU å¼‚æ„ç³»ç»Ÿæˆ–è¾¹ç¼˜è®¾å¤‡ä¸Šçš„è½»é‡åŒ–éƒ¨ç½²ç‰ˆæœ¬

---

> ğŸ’¡ **æ€»ä½“è¯„ä»·**ï¼š  
> FUSCO æå‡ºäº†ä¸€ç§å…¨æ–°çš„â€œé€šä¿¡å³è®¡ç®—â€çš„è§†è§’ï¼Œé‡æ–°å®šä¹‰äº†åˆ†å¸ƒå¼æ•°æ®é‡æ’çš„è®¾è®¡èŒƒå¼ã€‚å…¶å®éªŒå……åˆ†ã€å·¥ç¨‹æ‰å®ï¼Œä¸ä»…åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Š NCCL å’Œ DeepEPï¼Œæ›´é‡è¦çš„æ˜¯æå‡ºäº†ä¸€ä¸ª**é€šç”¨çš„èåˆé€šä¿¡æ¶æ„æ€æƒ³**ï¼Œæœ‰æœ›å¯å‘åç»­åœ¨åˆ†å¸ƒå¼ DNN ç³»ç»Ÿä¸­çš„æ›´å¤šâ€œfused primitiveâ€è®¾è®¡ã€‚

</details>

---

### 4. [MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations](https://arxiv.org/abs/2512.21633)

**Authors**: Qiuqi Li, Yiting Liu, Jin Zhao, Wencan Zhu  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.21633v1  

#### Abstract
Parametric partial differential equations (PDEs) are fundamental for modeling a wide range of physical and engineering systems influenced by uncertain or varying parameters. Traditional neural network-based solvers, such as Physics-Informed Neural Networks (PINNs) and Deep Galerkin Methods, often fa...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡é’ˆå¯¹**å‚æ•°åŒ–åå¾®åˆ†æ–¹ç¨‹**ï¼ˆparametric PDEsï¼‰åœ¨é«˜ç»´ã€é•¿æ—¶é—´æ¼”åŒ–åœºæ™¯ä¸‹çš„æ±‚è§£éš¾é¢˜ï¼Œå°¤å…¶æ˜¯ä¼ ç»ŸåŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼ˆå¦‚ PINNs å’Œ Deep Galerkin Methodsï¼‰å­˜åœ¨çš„ä»¥ä¸‹é—®é¢˜ï¼š
- **æ³›åŒ–èƒ½åŠ›å·®**ï¼šå¯¹æ–°å‚æ•°é…ç½®éœ€è¦é‡æ–°è®­ç»ƒï¼›
- **æ—¶é—´å› æœæ€§ä¸¢å¤±**ï¼šå…¨å±€æ—¶ç©ºé€¼è¿‘å¯¼è‡´é•¿æœŸé¢„æµ‹è¯¯å·®ç´¯ç§¯ï¼›
- **è®¡ç®—æˆæœ¬é«˜**ï¼šå…¨å‚æ•°æ›´æ–°å¸¦æ¥é«˜æ˜‚çš„è®­ç»ƒå¼€é”€ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶â€”â€”**Meta-Auto-Decoder Neural Galerkin Method (MAD-NGM)**ï¼Œå¹¶è¿›ä¸€æ­¥æ‰©å±•ä¸º **MAD-RSNGS**ï¼ˆRandomized Sparse ç‰ˆæœ¬ï¼‰ï¼Œå…¶æ ¸å¿ƒæ€æƒ³åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰ä¸¤é˜¶æ®µæ±‚è§£æ¡†æ¶ï¼ˆTwo-stage MAD-NGMï¼‰
- **åˆå§‹è¿‘ä¼¼é˜¶æ®µ**ï¼šé‡‡ç”¨ **Meta-Auto-Decoder (MAD)** èŒƒå¼ï¼Œåˆ©ç”¨å¤šä¸ªæ ·æœ¬æ„å»ºéçº¿æ€§è¯•å‡½æ•°æµå½¢ï¼ˆnonlinear trial manifoldï¼‰ï¼Œé€šè¿‡å°‘é‡è¿­ä»£å³å¯å¿«é€Ÿæ¨æ–­æ–°å‚æ•°ä¸‹çš„åˆå§‹ç½‘ç»œå‚æ•°ã€‚
- **æ—¶é—´æ¼”åŒ–é˜¶æ®µ**ï¼šåŸºäº NGM æ¡†æ¶ï¼Œä»å·²è·å¾—çš„åˆå§‹å‚æ•°å‡ºå‘ï¼Œé€æ­¥æ¨è¿›æ—¶é—´æ¼”åŒ–ï¼Œå®ç°ä»»æ„æ—¶åˆ»çš„çŠ¶æ€é¢„æµ‹ã€‚

#### ï¼ˆ2ï¼‰éšæœºç¨€ç–æ›´æ–°ç­–ç•¥ï¼ˆRandomized Sparse Updatesï¼‰
- åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä»…éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†ç½‘ç»œå‚æ•°è¿›è¡Œæ›´æ–°ï¼ˆsparse subsetï¼‰ï¼Œæ˜¾è‘—é™ä½æ¯æ­¥è®¡ç®—é‡ã€‚
- ä¿ç•™äº†ç‰©ç†ä¸€è‡´æ€§çš„åŒæ—¶æå‡äº†æ•ˆç‡ï¼Œé¿å…äº†å…¨å‚æ•°æ›´æ–°å¸¦æ¥çš„å†—ä½™è®¡ç®—å’Œè¿‡æ‹Ÿåˆé£é™©ã€‚

#### ï¼ˆ3ï¼‰ç©ºé—´-æ—¶é—´è§£è€¦æœºåˆ¶
- åˆ©ç”¨ **Neural Galerkin Method** ä¸­çš„æ—¶é—´å˜åˆ†åŸç†ï¼ˆDirac-Frenkel variational principleï¼‰ï¼Œå°†ç©ºé—´é€¼è¿‘ä¸æ—¶é—´ç§¯åˆ†åˆ†ç¦»ï¼Œå¢å¼ºäº†æ—¶é—´æ¼”åŒ–çš„ç¨³å®šæ€§å’Œç‰©ç†ä¸€è‡´æ€§ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | ç¼ºé™· | MAD-NGM/MAD-RSNGS æ”¹è¿› |
|------|------|------------------------|
| **PINNs / MAD-PINN** | å…¨å±€æ—¶ç©ºè®­ç»ƒï¼Œæ˜“ç§¯ç´¯æ—¶é—´è¯¯å·®ï¼›éœ€å¯†é›†é‡‡æ · | æ—¶é—´è§£è€¦ï¼Œé€æ—¶æ¨è¿›ï¼Œä¿æŒå› æœæ€§ï¼›å‡å°‘é‡‡æ ·éœ€æ±‚ |
| **ä¼ ç»Ÿ NGM** | æ¯ä¸ªæ–°å‚æ•°éœ€é‡è®­ç»ƒåˆå§‹å‚æ•° | å¼•å…¥ MADï¼Œé¢„è®­ç»ƒåä»…å¾®è°ƒ latent vector å³å¯é€‚åº”æ–°å‚æ•° |
| **Operator Learning (e.g., FNO, DeepONet)** | éš¾ä»¥å¤„ç†å¤æ‚å‡ ä½•æˆ–åŠ¨æ€è¾¹ç•Œ | å¯çµæ´»åµŒå…¥ positional encoding å¤„ç†å‘¨æœŸ/å˜åŒ–åŸŸ |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šå…¼å…·**é«˜æ•ˆæ€§**ï¼ˆä½è®¡ç®—å¼€é”€ï¼‰ã€**å¼ºæ³›åŒ–èƒ½åŠ›**ï¼ˆå¿«é€Ÿé€‚åº”æ–°å‚æ•°ï¼‰ã€**é•¿æ—¶ç¨³å®šæ€§**ï¼ˆç‰©ç†ä¸€è‡´çš„åºåˆ—é¢„æµ‹ï¼‰ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†ä¸PDEæ¨¡å‹
å®éªŒæ¶µç›–å››ç±»å…¸å‹å‚æ•°åŒ– PDEsï¼š
1. **Korteweg-de Vries (KdV) æ–¹ç¨‹**ï¼ˆ1Dï¼Œéçº¿æ€§è‰²æ•£æ³¢ï¼‰
2. **Burgers æ–¹ç¨‹**ï¼ˆ1Dï¼Œå«æ¿€æ³¢å½¢æˆï¼‰
3. **Allen-Cahn æ–¹ç¨‹**ï¼ˆ1D & 2Dï¼Œç›¸åˆ†ç¦»è¿‡ç¨‹ï¼‰
   - åŒ…æ‹¬**éšæ—¶é—´/ç©ºé—´å˜åŒ–çš„å‚æ•°é¡¹**
   - æ¶‰åŠ**éšæœºå®šä¹‰åŸŸ**ï¼ˆstochastic domainï¼‰

æ‰€æœ‰é—®é¢˜å‡å¼•å…¥å‚æ•°ä¸ç¡®å®šæ€§ï¼ˆå¦‚åˆå§‹æ¡ä»¶æœä»éšæœºåˆ†å¸ƒã€åŒºåŸŸè¾¹ç•Œæ‰°åŠ¨ç­‰ï¼‰ã€‚

### å®éªŒè®¾ç½®
- **è®­ç»ƒæ ·æœ¬æ•°**ï¼š80â€“200 ä¸ªä¸åŒå‚æ•°é…ç½®çš„æ ·æœ¬ç”¨äºé¢„è®­ç»ƒã€‚
- **æµ‹è¯•æ–¹å¼**ï¼šé€‰å–æœªè§å‚æ•°é…ç½®çš„æ–°æ ·æœ¬ï¼Œæ‰§è¡Œ fine-tuning åè¿›è¡Œæ—¶é—´æ¼”åŒ–é¢„æµ‹ã€‚
- **ç½‘ç»œæ¶æ„**ï¼š
  - ä½¿ç”¨å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFCNï¼‰ï¼Œå±‚æ•° 1â€“8 ä¸ç­‰ï¼Œå®½åº¦ 20â€“30ã€‚
  - Latent ç»´åº¦ï¼š5â€“80ï¼ˆä¾ä»»åŠ¡è€Œå®šï¼‰ã€‚
- **ä¼˜åŒ–å™¨**ï¼š
  - é¢„è®­ç»ƒé˜¶æ®µï¼šL-BFGSï¼ˆç²¾ç¡®æ”¶æ•›ï¼‰
  - å¾®è°ƒé˜¶æ®µï¼šAdam æˆ– L-BFGS
- **æ—¶é—´ç¦»æ•£åŒ–**ï¼šForward Euler æˆ– 4é˜¶ Runge-Kutta
- **ç¨€ç–æ›´æ–°è®¾ç½®**ï¼ˆMAD-RSNGSï¼‰ï¼š
  - æ¯æ­¥éšæœºæ›´æ–° 450â€“1500 å‚æ•°ï¼ˆæ€»å‚æ•°è¿œå¤§äºæ­¤ï¼‰

### è¯„ä¼°æŒ‡æ ‡
- **MSEï¼ˆMean Squared Errorï¼‰**ï¼š
  $$
  \text{MSE} = \frac{1}{N_{\text{test}} N} \sum_{i=1}^{N_{\text{test}}} \sum_{j=1}^{N} (\hat{u}_i(x_j,t_k) - u_i^{\text{true}}(x_j,t_k))^2
  $$
- å¯¹æ¯”ä¸åŒ fine-tuning è¿­ä»£æ¬¡æ•°ï¼ˆ100, 400, 4000ï¼‰ä¸‹çš„ç²¾åº¦æå‡ã€‚
- æŠ¥å‘Šå„æ—¶é—´ç‚¹ï¼ˆå¦‚ t=0.2, 0.5, 1.0ï¼‰çš„ MSEã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **MAD-PINN**ï¼šåŸºäºç‰©ç†ä¿¡æ¯çš„å…ƒå­¦ä¹  PINNï¼Œä½¿ç”¨å…¨æ—¶ç©ºæ®‹å·®æŸå¤±ã€‚
- **MAD-NGM**ï¼šæœ¬æ–‡å®Œæ•´ç‰ˆæœ¬ï¼ˆå…¨å‚æ•°æ›´æ–°ï¼‰ã€‚
- **MAD-RSNGS**ï¼šæœ¬æ–‡ç¨€ç–æ›´æ–°å˜ä½“ï¼ˆä¸åŒ s å€¼ï¼‰ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… KdV æ–¹ç¨‹ï¼ˆTable 1ï¼‰
| æ•°æ®é›† | é˜¶æ®µ | t=0.2 | t=0.5 | t=1.0 |
|-------|------|--------|--------|--------|
| **è®­ç»ƒé›†** | é¢„è®­ç»ƒ | ~6.5e-7 | ~1.5e-6 | ~1.0e-5 |
|        | å¾®è°ƒ4000æ¬¡ | **5.7e-7** | **1.2e-6** | **3.7e-5** |
| **æµ‹è¯•é›†** | é¢„è®­ç»ƒ | 4.95e-4 | â€” | â€” |
|        | å¾®è°ƒ4000æ¬¡ | **1.2e-6** | **1.7e-6** | **9.0e-6** |

> âœ”ï¸ æµ‹è¯•é›†è¯¯å·®ä¸‹é™çº¦ 4 ä¸ªæ•°é‡çº§ï¼Œè¡¨æ˜å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚

#### âœ… Burgers æ–¹ç¨‹ï¼ˆTable 2ï¼‰
| æ–¹æ³• | t=0.2 | t=0.5 | t=1.0 | t=1.1 |
|------|--------|--------|--------|--------|
| **MAD-NGM** | 1.18e-6 | 3.91e-6 | 4.11e-6 | 4.33e-6 |
| **MAD-RSNGS (s=3000)** | 1.51e-6 | 6.34e-6 | 1.21e-5 | 1.41e-5 |
| **MAD-PINN** | 2.75e-4 | 5.76e-4 | 5.89e-4 | 5.94e-4 |

> ğŸ”º MAD-PINN è¯¯å·®é«˜å‡º **ä¸¤ä¸ªæ•°é‡çº§ä»¥ä¸Š**ï¼Œä¸”éšæ—¶é—´å¢é•¿æ˜æ˜¾ï¼ˆè§ Figure 9ï¼‰ã€‚

#### âœ… Allen-Cahn æ–¹ç¨‹ï¼ˆTable 4ï¼‰
| å‚æ•°ç±»å‹ | t=0.4 | t=1.0 | t=2.0 |
|---------|--------|--------|--------|
| æ—¶é—´æ— å…³ | 2.68e-5 | 5.18e-5 | 7.35e-5 |
| æ—¶é—´ç›¸å…³ | 5.35e-5 | 6.98e-5 | 6.78e-5 |

> âœ”ï¸ å³ä½¿å‚æ•°éšæ—¶é—´å’Œç©ºé—´å˜åŒ–ï¼Œä»èƒ½ä¿æŒè‰¯å¥½ç²¾åº¦ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **ç²¾åº¦æ–¹é¢**ï¼š
  - MAD-NGM > MAD-RSNGS >> MAD-PINN
  - MAD-PINN å› å¿½ç•¥æ—¶é—´å› æœæ€§ï¼Œåœ¨é•¿æ—¶é¢„æµ‹ä¸­è¯¯å·®è¿…é€Ÿç´¯ç§¯ï¼ˆ~10â»â´ï¼‰ï¼Œè€Œ MAD-NGM ç»´æŒåœ¨ ~10â»â¶ã€‚
- **æ•ˆç‡æ–¹é¢**ï¼ˆTable 3ï¼‰ï¼š
  - **MAD-RSNGS æ—¶é—´æ¼”åŒ–è€—æ—¶ä»…ä¸º MAD-NGM çš„ ~10%**ï¼ˆä¾‹å¦‚ 3.5 min vs 46.47 minï¼‰
  - é¢„è®­ç»ƒå’Œå¾®è°ƒæ—¶é—´åŸºæœ¬ç›¸åŒï¼Œè¯´æ˜åŠ é€Ÿé›†ä¸­åœ¨æ—¶é—´æ¨è¿›é˜¶æ®µã€‚
- **å†…å­˜ä¸é‡‡æ ·éœ€æ±‚**ï¼š
  - MAD-PINN éœ€è¦åœ¨æ•´ä¸ªæ—¶ç©ºåŸŸé‡‡æ ·ä¸Šä¸‡ç‚¹ï¼ˆ13,029ï¼‰ï¼ŒGPU å†…å­˜å‹åŠ›å¤§ï¼›
  - MAD-RSNGS æ¯æ­¥åªéœ€å±€éƒ¨é‡‡æ ·ï¼Œæ›´é€‚ç”¨äºå¤§è§„æ¨¡é—®é¢˜ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆéšå«åˆ†æï¼‰
- **ç¨€ç–æ›´æ–°è§„æ¨¡å½±å“**ï¼ˆTable 2ï¼‰ï¼š
  - å½“ `s` ä» 600 å¢åŠ åˆ° 3000ï¼ŒMAD-RSNGS ç²¾åº¦æŒç»­æé«˜ï¼›
  - è¡¨æ˜æ›´å¤šå‚æ•°å‚ä¸æ›´æ–°æœ‰åŠ©äºé€¼è¿‘å®Œæ•´ NGM æ€§èƒ½ã€‚
- **fine-tuning è¿­ä»£çš„å½±å“**ï¼ˆTable 1ï¼‰ï¼š
  - å°‘é‡è¿­ä»£ï¼ˆ100æ¬¡ï¼‰å³å¯å¤§å¹…é™ä½è¯¯å·®ï¼›
  - è¾¾åˆ°ä¸€å®šè¿­ä»£åè¶‹äºé¥±å’Œï¼Œä½“ç°â€œå¿«é€Ÿé€‚åº”â€ç‰¹æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **MAD-NGM æˆåŠŸå®ç°äº†å‚æ•°åŒ– PDE çš„å¿«é€Ÿæ³›åŒ–æ±‚è§£**ï¼š
   - é€šè¿‡ MAD æ„å»ºå…±äº«æ½œæµå½¢ï¼Œæ–°æ ·æœ¬ä»…éœ€å¾®è°ƒ latent vector å³å¯å¯åŠ¨ã€‚
2. **æ—¶é—´è§£è€¦ + ç¨€ç–æ›´æ–° = é«˜æ•ˆé•¿æ—¶é¢„æµ‹**ï¼š
   - NGM ç»“æ„ä¿éšœäº†ç‰©ç†ä¸€è‡´æ€§ï¼›
   - éšæœºç¨€ç–æ›´æ–°å¤§å¹…é™ä½è®¡ç®—è´Ÿæ‹…è€Œä¸ç‰ºç‰²å¤ªå¤šç²¾åº¦ã€‚
3. **ä¼˜äºç«¯åˆ°ç«¯ PINN ç±»æ–¹æ³•**ï¼š
   - ç‰¹åˆ«æ˜¯åœ¨é•¿æ—¶ã€å¼ºéçº¿æ€§é—®é¢˜ä¸­ï¼ŒMAD-NGM æ˜¾è‘—æŠ‘åˆ¶è¯¯å·®ä¼ æ’­ã€‚
4. **å…·å¤‡å¤„ç†å¤æ‚å‡ ä½•ä¸éšæœºåŸŸçš„èƒ½åŠ›**ï¼š
   - é€šè¿‡ positional embedding å’Œ master domain è®¾è®¡ï¼Œæ”¯æŒæ‹“æ‰‘å˜åŒ–çš„æ±‚è§£åŸŸã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®**ï¼šè‹¥è®­ç»ƒæ ·æœ¬è¦†ç›–ä¸è¶³ï¼Œæ³›åŒ–å¯èƒ½å—é™ã€‚
- **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼šå¦‚ latent ç»´åº¦ã€ç¨€ç–æ¯”ä¾‹ `s`ã€å­¦ä¹ ç‡ç­‰éœ€ä»”ç»†è°ƒæ•´ã€‚
- **ç†è®ºä¿è¯æœ‰é™**ï¼šå½“å‰ç¼ºä¹å…³äºç¨€ç–æ›´æ–°ä¸‹è¯¯å·®ä¼ æ’­çš„ä¸¥æ ¼æ•°å­¦åˆ†æã€‚
- **å°šæœªéªŒè¯æç«¯é«˜ç»´é—®é¢˜**ï¼ˆå¦‚ >10 ç»´ï¼‰ï¼Œå®é™…æ‰©å±•æ€§æœ‰å¾…æ£€éªŒã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **è‡ªé€‚åº”ç½‘ç»œè®¾è®¡**ï¼šæ ¹æ®é—®é¢˜å¤æ‚åº¦è‡ªåŠ¨è°ƒèŠ‚ç½‘ç»œå®½åº¦/æ·±åº¦ã€‚
2. **å¤šå±‚çº§ä¼˜åŒ–ç­–ç•¥**ï¼šç»“åˆ coarse-to-fine æ—¶é—´æ­¥é•¿ä¸å‚æ•°æ›´æ–°æœºåˆ¶ã€‚
3. **å®æ—¶ä»¿çœŸåº”ç”¨**ï¼šéƒ¨ç½²äºæ•°å­—å­ªç”Ÿã€æ§åˆ¶åé¦ˆç³»ç»Ÿç­‰å·¥ä¸šåœºæ™¯ã€‚
4. **ä¸å…¶ä»–ç®—å­å­¦ä¹ æ–¹æ³•èåˆ**ï¼šæ¢ç´¢ MAD ä¸ FNO/CNO çš„ç»“åˆæ½œåŠ›ã€‚
5. **ä¸ç¡®å®šæ€§é‡åŒ–å¢å¼º**ï¼šé›†æˆè´å¶æ–¯æ¨ç†æˆ–ç”Ÿæˆæ¨¡å‹ä»¥è¾“å‡ºç½®ä¿¡åŒºé—´ã€‚

---

> ğŸ“Œ **æ€»ä½“è¯„ä»·**ï¼š  
> æœ¬æ–‡æå‡ºçš„ **MAD-NGM** æ˜¯ä¸€ç§æå…·å‰æ™¯çš„å‚æ•°åŒ– PDE æ±‚è§£èŒƒå¼ï¼Œå®ƒå·§å¦™åœ°èåˆäº† **meta-learningã€space-time decoupling ä¸ randomized sparse update**ï¼Œåœ¨ç²¾åº¦ã€æ•ˆç‡ä¸æ³›åŒ–ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ï¼Œä¸ºç§‘å­¦æœºå™¨å­¦ä¹ ä¸­çš„åŠ¨æ€ç³»ç»Ÿå»ºæ¨¡æä¾›äº†æ–°å·¥å…·ã€‚

</details>

---

### 5. [nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures](https://arxiv.org/abs/2512.21571)

**Authors**: Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang  
**Category**: cs.DC  
**Published**: 2025-12-29  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.21571v1  

#### Abstract
The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šnncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼‚æ„å­˜å‚¨æ¶æ„ä¸Šçš„é«˜æ•ˆéƒ¨ç½²é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š
- **å†…å­˜å¢™**ï¼šè®¡ç®—èƒ½åŠ›å¹´å¢é•¿100%ï¼Œè€Œå†…å­˜å¸¦å®½ä»…å¢é•¿20%ï¼Œæ•°æ®ç§»åŠ¨æˆä¸ºç“¶é¢ˆã€‚
- **å­˜å‚¨å±‚æ¬¡å¤æ‚**ï¼šä»SRAMåˆ°HBM/DRAMçš„å¤šçº§å†…å­˜ä½“ç³»è¦æ±‚ç²¾ç»†çš„æ•°æ®å¤ç”¨ç­–ç•¥ã€‚
- **è®¡ç®—å•å…ƒå¼‚æ„**ï¼šæ ‡é‡ã€å‘é‡ï¼ˆå¦‚AVX-512ï¼‰ã€çŸ©é˜µå•å…ƒï¼ˆå¦‚Intel AMXï¼‰å…±å­˜ï¼Œä¼ ç»Ÿç¼–è¯‘å™¨éš¾ä»¥ç»Ÿä¸€ä¼˜åŒ–ã€‚

ç°æœ‰ç¼–è¯‘æ¡†æ¶ï¼ˆå¦‚MLC LLMã€TVMï¼‰é€šå¸¸ä¸ºç‰¹å®šæ¶æ„è®¾è®¡ç‹¬ç«‹ä¼˜åŒ–æµç¨‹ï¼Œå¯¼è‡´**ç¢ç‰‡åŒ–å·¥ä½œæµ**å’Œ**é«˜æ˜‚é€‚é…æˆæœ¬**ï¼Œä¸”éš¾ä»¥è¿›è¡Œå…¨å±€ä¼˜åŒ–ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯**

nncase æ˜¯ä¸€ä¸ªå¼€æºçš„ç«¯åˆ°ç«¯ç¼–è¯‘æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯â€œ**ç»Ÿä¸€åˆ†å¸ƒå¼ç¼–è¯‘èŒƒå¼**â€ï¼ˆunified distributed compilation paradigmï¼‰ï¼Œé€šè¿‡ä»¥ä¸‹ä¸‰å¤§æ¨¡å—å®ç°è·¨å¼‚æ„æ¶æ„çš„é«˜æ•ˆä¼˜åŒ–ï¼š

#### âœ… åˆ›æ–°ç‚¹ä¸€ï¼šåŸºäº e-graph çš„ Term Rewriting å¼•æ“
- é‡‡ç”¨ **equality saturation** æŠ€æœ¯æ„å»ºéç ´åæ€§é‡å†™å¼•æ“ï¼Œé¿å…ä¼ ç»Ÿç¼–è¯‘å™¨çš„ **phase ordering problem**ã€‚
- åœ¨ e-graph ä¸­åŒæ—¶æ¢ç´¢å¤šç§ç­‰ä»·ç¨‹åºè¡¨ç¤ºï¼Œæœ€ç»ˆé€šè¿‡ **Weighted Partial MaxSAT (WPMAXSAT)** æ±‚è§£å™¨æå–æœ€ä¼˜æ–¹æ¡ˆã€‚
- é›†æˆ **Roofline æ¨¡å‹** è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–ï¼ˆå»¶è¿Ÿã€å†…å­˜å ç”¨ã€é€šä¿¡å¼€é”€ï¼‰ã€‚

#### âœ… åˆ›æ–°ç‚¹äºŒï¼šä¸‰å¤§è‡ªåŠ¨åŒ–ä¼˜åŒ–æ¨¡å—
1. **Auto Vectorize**
   - æå‡º `MetaPackOperation` å’Œ `FoldNopPack` è§„åˆ™ï¼Œåœ¨ e-graph å†…ç”Ÿæˆå¹¶èåˆå¤šç§å¼ é‡å¸ƒå±€å€™é€‰ã€‚
   - åŠ¨æ€è°ƒæ•´æ‰“åŒ…å› å­ï¼Œæ¶ˆé™¤å†—ä½™ layout è½¬æ¢ï¼Œå¹³è¡¡æ•°æ®æ ¼å¼è½¬æ¢ä¸è®¡ç®—å•å…ƒé¥±å’Œåº¦ã€‚

2. **Auto Distribution**
   - åŸºäº **SBP (Split, Broadcast, Partial)** æŠ½è±¡å»ºæ¨¡åˆ†å¸ƒå¼ç­–ç•¥ã€‚
   - å°†åˆ†å¸ƒç­–ç•¥æœç´¢ç©ºé—´åµŒå…¥ e-graphï¼Œæ”¯æŒæ‹“æ‰‘æ— å…³çš„å¹¶è¡ŒåŒ–ã€‚
   - æå–æ—¶ä»¥è®¾å¤‡å†…å­˜å®¹é‡ä¸ºç¡¬çº¦æŸï¼Œç¡®ä¿ç­–ç•¥å¯è¡Œæ€§ã€‚

3. **Auto Schedule**
   - å¼•å…¥ **nncase Tensor Template (NTT) Library**ï¼Œå°è£…é«˜æ€§èƒ½å¾®å†…æ ¸ï¼ˆukernelsï¼‰ä½œä¸ºè°ƒåº¦åŸå­å•å…ƒã€‚
   - åˆ†å±‚ä¼˜åŒ–ï¼š
     - ç»“æ„ä¼˜åŒ–ï¼ˆloop fusion/orderï¼‰ â†’ ä½¿ç”¨ **Monte Carlo Tree Search (MCTS)**
     - å‚æ•°ä¼˜åŒ–ï¼ˆtiling size / buffer placementï¼‰ â†’ ä½¿ç”¨ **Mixed-Integer Nonlinear Programming (MINLP)**

#### âœ… åˆ›æ–°ç‚¹ä¸‰ï¼šç¼“å†²æ„ŸçŸ¥ä»£ç ç”Ÿæˆï¼ˆBuffer-aware Codegenï¼‰
- æ”¯æŒåˆ«ååˆ†æï¼ˆAlias Analysisï¼‰å‡å°‘å†…å­˜æ‹·è´ã€‚
- ä½¿ç”¨ SAT æ±‚è§£å™¨è§£å†³ bin packing é—®é¢˜ï¼Œæœ€å¤§åŒ–ä¸­é—´å˜é‡å†…å­˜å¤ç”¨ã€‚
- å¸¸é‡é¢„åˆ†ç‰‡å¹¶ç»‘å®šè‡³æœ¬åœ°å­˜å‚¨ï¼Œæå‡è®¿é—®æ•ˆç‡ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | nncase | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚MLC LLM, IPEXï¼‰ |
|------|--------|-----------------------------|
| æ¶æ„é€‚åº”æ€§ | ç»Ÿä¸€å¤„ç†å‡åŒ€/éå‡åŒ€å†…å­˜æ¶æ„ï¼ˆNUMAæŠ½è±¡ï¼‰ | éœ€ä¸ºä¸åŒæ¶æ„å®šåˆ¶æµæ°´çº¿ |
| å…¨å±€ä¼˜åŒ–èƒ½åŠ› | e-graph æ”¯æŒè”åˆä¼˜åŒ– layoutã€distributionã€schedule | å„é˜¶æ®µå‰²è£‚ï¼Œæ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ |
| åˆ†å¸ƒå¼ç­–ç•¥ | è‡ªåŠ¨æœç´¢ + å†…å­˜çº¦æŸä¿éšœå¯è¡Œæ€§ | ä¾èµ–äººå·¥æ ‡æ³¨æˆ–å¯å‘å¼ |
| ç¼–è¯‘æ•ˆç‡ | MCTS + MINLP æ˜¾è‘—é™ä½æœç´¢æ—¶é—´ | å­¦ä¹ å‹æ–¹æ³•ï¼ˆå¦‚Ansorï¼‰ç¼–è¯‘è€—æ—¶é•¿ |
| æ€§èƒ½ä¸Šé™ | æ¥è¿‘æ‰‹è°ƒåº“ï¼ˆå¦‚llama.cppï¼‰ | é€šå¸¸ä½äºæ‰‹å·¥ä¼˜åŒ– |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ¨¡å‹ä¸å¹³å°**

- **æ¨¡å‹**ï¼šQwen3 ç³»åˆ—
  - Qwen3-0.6Bï¼ˆF32/F16ï¼‰
  - Qwen3-1.7Bï¼ˆF16ï¼‰
- **ç¡¬ä»¶å¹³å°**ï¼šAMD Ryzen 9 5900Xï¼ˆ12æ ¸ï¼‰ï¼Œ128GB DDR4-3600
- **æ“ä½œç³»ç»Ÿ**ï¼šUbuntu 24.04 LTSï¼ŒGCC 14.2
- **æŒ‡ä»¤é›†æ”¯æŒ**ï¼šAVX2

---

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**

| è®¾ç½®é¡¹ | æè¿° |
|-------|------|
| æ‰¹å¤§å°ï¼ˆbatch sizeï¼‰ | 1 |
| è¾“å…¥é•¿åº¦ | 8-token prompt |
| å¹¶å‘é…ç½® | å•æ ¸ï¼ˆ1Tï¼‰ã€4çº¿ç¨‹ï¼ˆ4Tï¼‰ã€8çº¿ç¨‹ï¼ˆ8Tï¼‰ |
| é‡å¤æ¬¡æ•° | æ¯ä¸ªæµ‹è¯•è¿è¡Œ100æ¬¡å–å¹³å‡å€¼ |
| ä¸»è¦æŒ‡æ ‡ | **Token ååé‡ï¼ˆtokens/sï¼‰**ï¼Œè¡¡é‡è§£ç é˜¶æ®µæ€»è€—æ—¶ |

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| æ¡†æ¶ | ç±»å‹ | ç‰ˆæœ¬ |
|------|------|------|
| **llama.cpp** | æ‰‹å·¥ä¼˜åŒ–åº“ | Tag: b5753 |
| **Intel IPEX** | å·¥ä¸šçº§PyTorchæ‰©å±• | v28.0 |
| **MLC LLM** | ä¸»æµDLç¼–è¯‘æ¡†æ¶ | Commit: 862a7311 |

> æ³¨ï¼šæ‰€æœ‰æ¡†æ¶å‡å¯ç”¨é»˜è®¤ä¼˜åŒ–åŠæœ€é«˜å¯ç”¨æŒ‡ä»¤é›†ï¼ˆAVX2ï¼‰ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å•æ ¸åœºæ™¯ï¼ˆ1Tï¼‰æ€§èƒ½å¯¹æ¯”**

| æ¨¡å‹ | nncase | llama.cpp | IPEX | MLC LLM |
|------|--------|-----------|------|---------|
| Qwen3-0.6B (F32) | **8.7 tokens/s** | 10.61 tokens/s | 7.58 tokens/s | 0.2 tokens/s |
| Qwen3-0.6B (F16) | **13.87 tokens/s** | 17.21 tokens/s | 10.22 tokens/s | â€” |
| Qwen3-1.7B (F16) | **5.09 tokens/s** | 6.3 tokens/s | 4.2 tokens/s | â€” |

- **ç»“è®º**ï¼šnncase åœ¨å•æ ¸ä¸‹æ˜¾è‘—ä¼˜äº IPEX å’Œ MLC LLMï¼Œè™½ç•¥é€Šäºæ‰‹å·¥ä¼˜åŒ–çš„ llama.cppï¼ˆå·®è·çº¦18%-19%ï¼‰ï¼Œä½†å·²æ¥è¿‘å…¶æ€§èƒ½è¾¹ç•Œã€‚

---

### **å¤šæ ¸åœºæ™¯ï¼ˆ4T / 8Tï¼‰æ€§èƒ½å¯¹æ¯”**

| æ¨¡å‹ | é…ç½® | nncase | llama.cpp | IPEX |
|------|------|--------|-----------|------|
| Qwen3-0.6B-F16 | 4T | **23.5 tokens/s** | 23.2 tokens/s | 15.52 tokens/s |
| Qwen3-0.6B-F16 | 8T | **23.98 tokens/s** | ~23.5 tokens/s | ~16 tokens/s |
| Qwen3-1.7B-F16 | 4T | **8.85 tokens/s** | 8.34 tokens/s | 6.93 tokens/s |

- **æ‰©å±•æ•ˆç‡**ï¼š
  - Qwen3-1.7B ä¸‹ï¼Œnncase ä» 1T åˆ° 4T æå‡ **74%**ï¼Œè€Œ llama.cpp ä»…æå‡ **32%**ã€‚
- **ä¼˜åŠ¿æ¥æº**ï¼š
  - nncase ä½¿ç”¨ **é™æ€ä»»åŠ¡åˆ’åˆ† + ç¼–è¯‘æœŸæ ¸å¿ƒæ˜ å°„**ï¼Œé¿å… OpenMP åŠ¨æ€è°ƒåº¦å¼€é”€ã€‚
  - å°†å¤šæ ¸è§†ä¸ºâ€œå¤šèŠ‚ç‚¹â€ï¼Œé€šè¿‡ Auto Distribution ä¼˜åŒ–ç²’åº¦ï¼Œå‡å°‘åŒæ­¥ä¸é€šä¿¡ã€‚

---

### **æ¶ˆèå®éªŒï¼ˆéšå«åˆ†æï¼‰**

å°½ç®¡æœªæ˜ç¡®åˆ—å‡ºæ¶ˆèè¡¨ï¼Œæ–‡ä¸­é€šè¿‡æ¨¡å—è®¾è®¡æ­ç¤ºäº†å„ç»„ä»¶ä½œç”¨ï¼š

- **Auto Vectorize**ï¼šåœ¨ Attention å­å›¾ä¸­æˆåŠŸæ¶ˆé™¤ä¸­é—´ Pack/Unpack æ“ä½œï¼Œå®ç°â€œpass-throughâ€å¸ƒå±€ï¼Œé™ä½å†…å­˜è®¿é—®ã€‚
- **Auto Distribution**ï¼šå¼•å…¥ Resharding Boxing å’Œ SBP signature çº¦æŸï¼Œä¿è¯ç­–ç•¥åˆæ³•æ€§ã€‚
- **Auto Schedule + NTT**ï¼šMINLP ç²¾ç¡®å»ºæ¨¡ç¼“å­˜å±‚çº§ä¸å¯„å­˜å™¨å¤ç”¨ï¼Œé¿å… spill/fill å¼€é”€ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **ç»Ÿä¸€ç¼–è¯‘èŒƒå¼å¯è¡Œä¸”é«˜æ•ˆ**  
   nncase æˆåŠŸå°† NUMA æŠ½è±¡åº”ç”¨äºå•æœºå¤šæ ¸ä¸åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œå®ç°äº†â€œ**compile once, adapt everywhere**â€ã€‚

2. **è‡ªåŠ¨åŒ–å¯é€¼è¿‘æ‰‹å·¥ä¼˜åŒ–æ€§èƒ½**  
   åœ¨å•æ ¸åœºæ™¯ä¸‹ï¼Œnncase è¾¾åˆ° llama.cpp çš„ **~80%-85%** æ€§èƒ½ï¼Œè¯æ˜è‡ªåŠ¨ç¼–è¯‘åœ¨ LLM éƒ¨ç½²ä¸­çš„å®ç”¨æ€§ã€‚

3. **å¤šæ ¸æ‰©å±•æ€§è¶…è¶Šæ‰‹å·¥åº“**  
   å¾—ç›Šäºé™æ€è°ƒåº¦ä¸åˆ†å¸ƒå¼æ€ç»´ï¼Œnncase åœ¨å¤šæ ¸ç¯å¢ƒä¸‹åè¶… llama.cppï¼ŒéªŒè¯äº†å…¶å¯¹ç°ä»£CPUæ¶æ„çš„æ›´å¥½é€‚é…ã€‚

4. **e-graph æ”¯æŒè”åˆä¼˜åŒ–**  
   å°† layoutã€distributionã€schedule ç»Ÿä¸€åœ¨ e-graph ä¸­è¡¨è¾¾ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•å‰²è£‚ä¼˜åŒ–çš„é—®é¢˜ã€‚

---

### **å±€é™æ€§**

1. **å½“å‰ä¸»è¦é¢å‘ CPU æ¶æ„**  
   å°šæœªæ”¯æŒ GPU/SIMT æ¶æ„ï¼Œæ— æ³•ç›´æ¥ç”¨äºå¤§è§„æ¨¡é›†ç¾¤è®­ç»ƒã€‚

2. **ç¼ºä¹å¯¹ç¨€ç–æ€§ä¸é‡åŒ–è”åˆä¼˜åŒ–çš„æ”¯æŒ**  
   å®éªŒé›†ä¸­åœ¨ F32/F16ï¼Œæœªæ¶‰åŠ INT4/GGUF ç­‰ä¸»æµæ¨ç†æ ¼å¼ã€‚

3. **ç¼–è¯‘æ—¶é—´æœªè¯¦ç»†æŠ¥å‘Š**  
   è™½å£°ç§°æ¯”å­¦ä¹ å‹æ–¹æ³•å¿«ï¼Œä½†ç¼ºå°‘ä¸ Ansor/TVM MetaSchedule çš„ç›´æ¥å¯¹æ¯”ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **å…¨æ ˆæ€§èƒ½è¡¨å¾**  
   æ‰©å±•è¯„ä¼°èŒƒå›´è‡³æ›´å¤šæ¨¡å‹è§„æ¨¡ã€ç¡¬ä»¶å¹³å°ï¼Œåˆ†æç¼–è¯‘å¼€é”€ä¸å†…å­˜å ç”¨ã€‚

2. **è·¨æ¶æ„é€‚é…ï¼ˆSIMT æ”¯æŒï¼‰**  
   æ‰©å±•åç«¯ä»¥æ”¯æŒ GPUï¼Œå®ç°çœŸæ­£çš„è·¨å¹³å°å¯ç§»æ¤æ€§ã€‚

3. **è®¡ç®—-é€šä¿¡é‡å ä¼˜åŒ–**  
   è‡ªåŠ¨èåˆé€šä¿¡ä¸è®¡ç®—å†…æ ¸ï¼Œé€šè¿‡æµæ°´çº¿éšè—å»¶è¿Ÿï¼Œæå‡åˆ†å¸ƒå¼ååã€‚

---

> **æºç åœ°å€**ï¼šhttps://github.com/kendryte/nncase

</details>

---

### 6. [Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2512.21651)

**Authors**: Dung Anh Hoang, Cuong Pham, Cuong Nguyen, Trung le, Jianfei Cai, Thanh-Toan Do  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.21651v1  

#### Abstract
Large Language Models (LLMs) deliver strong performance across a wide range of NLP tasks, but their massive sizes hinder deployment on resource-constrained devices. To reduce their computational and memory burden, various compression techniques have been proposed, including quantization, pruning, an...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šRethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡èšç„¦äº **1-bit Post-Training Quantization (PTQ)** åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨æŒ‘æˆ˜ã€‚å°½ç®¡ sub-4-bit PTQ å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½† **1-bit é‡åŒ–**ï¼ˆå°†æµ®ç‚¹æƒé‡å‹ç¼©ä¸º Â±1ï¼‰ä»é¢ä¸´ä¸¥é‡æ€§èƒ½é€€åŒ–é—®é¢˜ã€‚

ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨ **weight alignment**ï¼ˆæœ€å°åŒ– $||W - \hat{W}||$ï¼‰ï¼Œè€Œæ›´ç›´è§‚çš„ **output alignment**ï¼ˆæœ€å°åŒ– $||XW - X\hat{W}||$ï¼‰åœ¨å®è·µä¸­æ•ˆæœä¸ä½³ã€‚ä½œè€…ç³»ç»Ÿåˆ†æäº†ä¸ºä½• naive output alignment å¤±æ•ˆï¼Œå¹¶æå‡ºæ”¹è¿›æ–¹æ¡ˆã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ä¸ª **data-aware çš„ 1-bit PTQ æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

1. **Selective Layer-wise Output Matching**  
   - ä¸å¯¹æ‰€æœ‰å±‚è¿›è¡Œè¾“å‡ºå¯¹é½ï¼Œè€Œæ˜¯**ä»…åœ¨æ¯ä¸ª Transformer block çš„æœ€åä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆFinal FCï¼‰åº”ç”¨ output alignment**ï¼Œå…¶ä½™å±‚ä½¿ç”¨ weight alignmentã€‚
   - åŠ¨æœºï¼šé¿å…å› é€å±‚è¯¯å·®ç´¯ç§¯å¯¼è‡´ block-level æ€§èƒ½ä¸‹é™ã€‚

2. **True Target Output Objective**  
   - å°†ä¼˜åŒ–ç›®æ ‡ä» $||\hat{X}W - \hat{X}\hat{W}||$ï¼ˆpseudo targetï¼‰æ”¹ä¸º $||XW - \hat{X}\hat{W}||$ï¼ˆtrue targetï¼‰ï¼Œå…¶ä¸­ $X$ æ˜¯ full-precision è¾“å…¥ã€‚
   - æ˜¾å¼å»ºæ¨¡å¹¶ç¼“è§£ **activation error accumulation** é—®é¢˜ã€‚

3. **Attention Matrix Preservation (AMP)**  
   - å¼•å…¥ä¸€ç§æ³¨æ„åŠ›æ„ŸçŸ¥æœºåˆ¶ï¼Œé€šè¿‡æ¢¯åº¦ç¬¦å·æ„é€ æ©ç ï¼ˆmaskï¼‰ï¼Œä¿æŠ¤ token-level çš„ç›¸ä¼¼æ€§ç»“æ„ã€‚
   - é˜²æ­¢ output alignment ç ´å attention patternï¼Œå°¤å…¶åœ¨ LLaMA ç­‰æ¶æ„ä¸­è‡³å…³é‡è¦ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- æ›´è´´è¿‘é‡åŒ–æœ€ç»ˆç›®æ ‡ï¼ˆä¿æŒè¾“å‡ºä¸€è‡´æ€§ï¼‰
- æ˜¾å¼è€ƒè™‘è·¨å±‚æ¿€æ´»è¯¯å·®ä¼ æ’­
- ä¿ç•™å…³é”®æ³¨æ„åŠ›è¡Œä¸ºï¼Œé˜²æ­¢è¯­ä¹‰ç»“æ„é€€åŒ–
- åœ¨å¤šä¸ª LLM æ¶æ„ä¸Šå®ç°ä¸€è‡´ä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
| ç±»å‹ | æ•°æ®é›† |
|------|--------|
| **Calibration Set** | C4ï¼ˆç”¨äºé‡åŒ–æ ¡å‡†ï¼‰ |
| **Perplexity Evaluation** | C4, WikiText2, PTB |
| **Zero-shot QA Evaluation** | ARC-Easy, ARC-Challenge, PIQA, BoolQ, HellaSwag, WinoGrande, OBQA, LAMBADA |

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
| é¡¹ç›® | è®¾ç½® |
|------|------|
| **æ¨¡å‹èŒƒå›´** | OPT ç³»åˆ—ï¼ˆ1.3Bâ€“30Bï¼‰ã€LLaMA-2ï¼ˆ7B/13Bï¼‰ã€LLaMA-3ï¼ˆ8Bï¼‰ |
| **é‡åŒ–æ–¹å¼** | 1-bit PTQï¼Œblock size = 128 |
| **è¯„ä¼°æŒ‡æ ‡** | 
| - **Perplexity (PPL)** â†“ | è¡¡é‡è¯­è¨€å»ºæ¨¡èƒ½åŠ› |
| - **Zero-shot Accuracy** â†‘ | è¡¡é‡ä¸‹æ¸¸ä»»åŠ¡æ³›åŒ–èƒ½åŠ› |
| - **Ablation Studies** | åˆ†æå„ç»„ä»¶æœ‰æ•ˆæ€§ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **Full Precision** | FP16 baseline | æ€§èƒ½ä¸Šé™å‚è€ƒ |
| **PB-LLM** | 1-bit PTQ | Partial binarizationï¼Œéƒ¨åˆ†å‚æ•°ä¿ç•™é«˜ç²¾åº¦ |
| **BiLLM** | 1-bit PTQ | Hessian-guided é‡è¦æ€§ç­›é€‰ |
| **ARB-RC** | 1-bit PTQ | Weight alignment ä»£è¡¨æ–¹æ³• |
| **ARB-X** | 1-bit PTQ | Output alignment ä»£è¡¨æ–¹æ³• |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆä»¥ OPT å’Œ LLaMA ä¸ºä¾‹ï¼‰

#### âœ… Table 1 & 2: Perplexity å¯¹æ¯”ï¼ˆè¶Šä½è¶Šå¥½ï¼‰

| Model | Method | C4 (PPL) | WikiText2 (PPL) | PTB (PPL) |
|-------|--------|----------|----------------|-----------|
| OPT-13B | Full Precision | 12.06 | 10.13 | 19.09 |
| OPT-13B | ARB-RC | 15.07 | 13.10 | 19.09 |
| OPT-13B | ARB-X | 17.71 | 15.47 | 23.46 |
| OPT-13B | **Ours** | **14.71** | **12.84** | **18.85** |
| LLaMA-2-7B | Full Precision | 7.26 | 5.47 | 37.91 |
| LLaMA-2-7B | ARB-RC | 14.77 | 12.47 | 197.70 |
| LLaMA-2-7B | ARB-X | 19.82 | 14.86 | 182.10 |
| LLaMA-2-7B | **Ours** | **13.80** | **11.50** | **196.64** |

> ğŸ“Œ **ç»“è®º**ï¼šåœ¨å‡ ä¹æ‰€æœ‰é…ç½®ä¸‹ï¼Œæœ¬æ–‡æ–¹æ³•å‡ä¼˜äºæ‰€æœ‰ baselineï¼Œå°¤å…¶åœ¨å°æ¨¡å‹ï¼ˆå¦‚ OPT-1.3Bï¼‰ä¸Šä¼˜åŠ¿æ›´æ˜æ˜¾ã€‚

#### âœ… Zero-shot QA å‡†ç¡®ç‡ï¼ˆå¹³å‡å‡†ç¡®ç‡ â†‘ï¼‰

| Model | Method | Avg Accuracy (%) |
|-------|--------|------------------|
| OPT-30B | ARB-RC | 57.11 |
| OPT-30B | **Ours** | **57.70** (+0.59%) |
| LLaMA-3-8B | ARB-RC | 47.11 |
| LLaMA-3-8B | **Ours** | **47.71** (+0.60%) |

> ğŸ“Œ è™½ç„¶æå‡å¹…åº¦ä¸å¤§ï¼Œä½†åœ¨æç«¯å‹ç¼©æ¡ä»¶ä¸‹å·²å±æ˜¾è‘—ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ğŸ” Table 3: AMP æ©ç çš„å½±å“ï¼ˆLLaMA-2-7Bï¼‰

| è®¾ç½® | C4 PPL | WikiText2 PPL |
|------|--------|--------------|
| Without AMP | 29.12 | 26.24 |
| With AMP | **19.25** | **15.42** |

> âš ï¸ **è¯´æ˜**ï¼šAMP å¯¹ LLaMA è‡³å…³é‡è¦ï¼Œç§»é™¤åæ€§èƒ½å¤§å¹…ä¸‹é™ï¼ˆ+10 PPLï¼‰ï¼Œå› å…¶ä¾èµ– RMSNorm å¯¼è‡´æ–¹å‘æ•æ„Ÿã€‚

#### ğŸ” Table 4: è¾“å‡ºè¯¯å·® vs æ¿€æ´»æ¡ä»¶è¯¯å·®

| Objective | C4 PPL (LLaMA-2-7B) |
|----------|---------------------|
| Activation-conditioned Error | 19.97 |
| **True Output Error (ours)** | **19.25** |

> âœ… æ˜¾å¼å»ºæ¨¡çœŸå®è¾“å‡ºè¯¯å·®å¯å¸¦æ¥çº¦ 0.7 PPL æå‡ã€‚

#### ğŸ” Table 5: ä¸åŒå±‚åº”ç”¨ output alignment çš„æ•ˆæœ

| Layer | C4 PPL (LLaMA-2-7B) |
|-------|---------------------|
| Query | 20.08 |
| Key | 20.80 |
| Value | 21.44 |
| Attn Out | 21.02 |
| **Final FC** | **19.25** âœ… |

> ğŸ’¡ ç»“è®ºï¼š**ä»…åœ¨ block çš„ final FC å±‚åº”ç”¨ output alignment æ•ˆæœæœ€ä½³**ï¼ŒéªŒè¯ selective strategy çš„åˆç†æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Naive output alignment å¹¶ä¸èƒ½ä¿è¯ block-level æ€§èƒ½æå‡**  
   - å³ä½¿æŸå±‚è¾“å‡ºé‡å»ºè¯¯å·®é™ä½ï¼Œä¹Ÿå¯èƒ½å› è¯¯å·®ä¼ æ’­å¯¼è‡´æ•´ä½“æ€§èƒ½æ¶åŒ–ã€‚

2. **Activation error accumulation æ˜¯ output alignment å¤±æ•ˆçš„å…³é”®åŸå› **  
   - ä½¿ç”¨ $\hat{X}$ï¼ˆé‡åŒ–åè¾“å…¥ï¼‰ä½œä¸ºæ¡ä»¶ä¼šå¼•å…¥åå·®ï¼Œåº”ä½¿ç”¨åŸå§‹ $X$ æ„é€  true targetã€‚

3. **Output alignment å¯èƒ½ç ´å attention mask**  
   - å°¤å…¶åœ¨ LLaMA ä¸­ï¼Œç”±äº RMSNorm å¯¹è¡¨ç¤ºæ–¹å‘æ•æ„Ÿï¼Œtoken ç›¸ä¼¼æ€§æ˜“è¢«ç ´åã€‚

4. **Selective + True-target + AMP ç»„åˆç­–ç•¥æœ€æœ‰æ•ˆ**  
   - ä¸‰è€…ååŒä½œç”¨ï¼Œåœ¨ä¸å¢åŠ æ¨ç†å¼€é”€çš„å‰æä¸‹æ˜¾è‘—æå‡æ€§èƒ½ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ–¹æ³•ä»åŸºäº ARB æ¡†æ¶æ‰©å±•ï¼Œæœªè§£å†³ 1-bit æœ¬èº«çš„ä¿¡æ¯ç“¶é¢ˆã€‚
- AMP æœºåˆ¶ä¾èµ–æ¢¯åº¦ç¬¦å·ï¼Œå¯èƒ½å¯¹å™ªå£°æ•æ„Ÿã€‚
- æ‰€æœ‰å®éªŒåŸºäº A100 GPUï¼Œæœªæµ‹è¯•è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æ•ˆç‡ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ›´ç²¾ç»†çš„ block-level æˆ– sequence-level output alignment ç­–ç•¥
- ç»“åˆè½»é‡çº§å¾®è°ƒï¼ˆfew-step QATï¼‰è¿›ä¸€æ­¥ç¼©å° gap
- å°† AMP æ€æƒ³æ¨å¹¿åˆ°å…¶ä»–ç»“æ„ï¼ˆå¦‚ MoEã€vision encoderï¼‰
- ç ”ç©¶ 1-bit + sparse æ¶æ„è”åˆä¼˜åŒ–

---

> âœ… **æ€»ä½“è¯„ä»·**ï¼šè¯¥è®ºæ–‡ä¸ä»…æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ 1-bit PTQ æ–¹æ³•ï¼Œæ›´é‡è¦çš„æ˜¯æ­ç¤ºäº† output alignment åœ¨ LLM é‡åŒ–ä¸­çš„å†…åœ¨æœºåˆ¶ä¸é™·é˜±ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ·±åˆ»æ´è§ã€‚

</details>

---

### 7. [BLEST: Blazingly Efficient BFS using Tensor Cores](https://arxiv.org/abs/2512.21967)

**Authors**: Deniz Elbek, Kamer Kaya  
**Category**: cs.DC  
**Published**: 2025-12-29  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.21967v1  

#### Abstract
Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šBLEST: Blazingly Efficient BFS using Tensor Cores

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç°ä»£ GPU è™½ç„¶é…å¤‡äº†é«˜ååé‡çš„ **Tensor Cores (TC)**ï¼Œä½†è¿™äº›ç¡¬ä»¶å•å…ƒä¸“ä¸ºå¯†é›†çŸ©é˜µè¿ç®—è®¾è®¡ï¼Œéš¾ä»¥é«˜æ•ˆæ”¯æŒå›¾ç®—æ³•ä¸­å…¸å‹çš„ç¨€ç–ã€ä¸è§„åˆ™è®¡ç®—æ¨¡å¼ã€‚ä¼ ç»Ÿçš„ **Breadth-First Search (BFS)** åœ¨ GPU ä¸Šæ‰§è¡Œæ—¶é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- **è´Ÿè½½ä¸å‡è¡¡**ï¼šå›¾çš„é«˜åº¦ä¸è§„åˆ™æ€§å¯¼è‡´ä¸åŒçº¿ç¨‹å—ï¼ˆwarpï¼‰å¤„ç†çš„å·¥ä½œé‡å·®å¼‚å¤§ã€‚
- **å†—ä½™è®¡ç®—**ï¼šä¼ ç»Ÿ slice-set ç»“æ„é‡‡ç”¨â€œfrontier-obliviousâ€è°ƒåº¦ï¼Œå³ä½¿å½“å‰ frontier ä¸ºç©ºï¼Œä»ä¼šåˆ†é…ä»»åŠ¡ã€‚
- **åŸå­æ“ä½œå¼€é”€å¤§**ï¼šé¡¶ç‚¹æ›´æ–°æ¶‰åŠå¤§é‡åˆ†æ•£çš„åŸå­å†™æ“ä½œï¼Œå¯¼è‡´å†…å­˜è®¿é—®ä¸åˆå¹¶ï¼ˆuncoalescedï¼‰ï¼Œç¼“å­˜å‘½ä¸­ç‡ä½ã€‚
- **æ•°æ®å¸ƒå±€æ•ˆç‡ä½**ï¼šç¼ºä¹å¯¹å›¾ç»“æ„æ„ŸçŸ¥çš„é‡æ’åºç­–ç•¥ï¼Œå½±å“å‹ç¼©ç‡å’Œå±€éƒ¨æ€§ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°
è®ºæ–‡æå‡ºäº† **BLEST**ï¼ˆBlazingly Efficient BFS using Tensor Coresï¼‰ï¼Œä¸€ä¸ªåŸºäº Tensor Cores åŠ é€Ÿçš„ BFS æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰Binarised Virtual Slice Sets (BVSS)
- å°†ä¼ ç»Ÿå›ºå®šçš„ slice set ç»†åŒ–ä¸ºæ›´å°çš„ **Virtual Slice Set (VSS)** å•ä½ã€‚
- å¼•å…¥ round-robin åˆ†é…æœºåˆ¶ï¼Œç¡®ä¿æ¯ä¸ª warp å¤„ç†çš„ VSS æ•°é‡å‡ ä¹ç›¸ç­‰ï¼Œå®ç°è¿‘ä¹å®Œç¾çš„ **warp-level load balancing**ã€‚
- åªæœ‰å½“å¯¹åº” frontier éé›¶æ—¶æ‰å°† VSS åŠ å…¥é˜Ÿåˆ—ï¼Œé¿å…äº†æ— æ•ˆå·¥ä½œã€‚

#### ï¼ˆ2ï¼‰å›¾é‡æ’åºç­–ç•¥ï¼ˆGraph Reorderingï¼‰
- å¯¹äº **social-like graphs**ï¼ˆå¦‚ç¤¾äº¤ç½‘ç»œï¼‰ï¼Œæå‡º **JACCARDWITHWINDOWS** ç®—æ³•ï¼Œåˆ©ç”¨çª—å£å†… Jaccard ç›¸ä¼¼æ€§èšç±»æå‡ slice å†…è¾¹çš„å¯†åº¦ï¼Œä»è€Œæé«˜ **compression ratio**ã€‚
- å¯¹äº **non-social graphs**ï¼ˆå¦‚é“è·¯ç½‘ç»œï¼‰ï¼Œé‡‡ç”¨ **RCM (Reverse Cuthill-McKee)** é™ä½å¸¦å®½ï¼Œå‡å°‘ cache line fetchesï¼Œæ”¹å–„æ›´æ–°å±€éƒ¨æ€§ã€‚

#### ï¼ˆ3ï¼‰æœ€ä¼˜çš„ TC è®¡ç®—æ¨¡å¼ï¼ˆOptimal TC Layoutï¼‰
- è®¾è®¡äº†ä¸€ç§æ–°çš„ **bitwise SpMSpV â†’ batched SpMM** æ˜ å°„æ–¹å¼ã€‚
- åˆ©ç”¨ m8n8k128 TC tileï¼Œé€šè¿‡ç²¾å¿ƒæ„é€  `fragA` å’Œ `fragB` è¾“å…¥ï¼Œä½¿å¾—æ¯æ¬¡ TC è°ƒç”¨éƒ½èƒ½å……åˆ†åˆ©ç”¨å…¨éƒ¨ 64 ä¸ªè¾“å‡ºæ¡ç›®ï¼Œæ— æµªè´¹ã€‚
- ç›¸æ¯” SOTA çš„ 16 æ¬¡ TC è°ƒç”¨å¤„ç† 128 ä¸ªèŠ‚ç‚¹ï¼ŒBLEST ä»…éœ€ **2 æ¬¡ TC è°ƒç”¨**ï¼Œå‡å°‘äº† 8 å€çš„ MMA å¼€é”€ã€‚

#### ï¼ˆ4ï¼‰å»¶è¿Ÿé¡¶ç‚¹æ›´æ–°ï¼ˆLazy Vertex Updateï¼‰
- å°†é¡¶ç‚¹æ›´æ–°å»¶è¿Ÿåˆ°æ¯å±‚ BFS ç»“æŸåç»Ÿä¸€å¤„ç†ã€‚
- ä½¿ç”¨è½»é‡çº§å¼‚æ­¥ `REDG` åŸå­æ“ä½œè¿›è¡Œæ ‡è®°ï¼Œåœ¨æœ€ç»ˆé˜¶æ®µä»¥å®Œå…¨åˆå¹¶çš„æ–¹å¼æ‰¹é‡åº”ç”¨æ›´æ–°ï¼Œæ˜¾è‘—é™ä½åŸå­å†²çªå’Œ cache missã€‚

#### ï¼ˆ5ï¼‰æ ¸å‡½æ•°èåˆï¼ˆKernel Fusionï¼‰
- åˆ©ç”¨ CUDA Cooperative Groups å®ç°è®¾å¤‡ç«¯åŒæ­¥ï¼Œå°†å¤šå±‚ BFS å¾ªç¯èåˆè¿›å•ä¸ªæŒä¹…åŒ– kernelã€‚
- æ¶ˆé™¤äº†ä¸»æœºä¾§ï¼ˆhost-sideï¼‰æ¯å±‚ä¹‹é—´çš„åŒæ­¥å¼€é”€ï¼Œç‰¹åˆ«æœ‰åˆ©äºéœ€è¦æ•°åƒå±‚éå†çš„é“è·¯ç½‘ç»œã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´é«˜çš„ç¡¬ä»¶åˆ©ç”¨ç‡**ï¼šå……åˆ†æŒ–æ˜ Tensor Cores çš„æ½œåŠ›ï¼Œé€‚ç”¨äºç¨€ç–å›¾è®¡ç®—ã€‚
- **æ›´ä½çš„å†—ä½™ä¸æ›´é«˜å¹¶è¡Œæ•ˆç‡**ï¼šBVSS + lazy update å‡å°‘æ— æ•ˆè®¡ç®—å’ŒåŸå­äº‰ç”¨ã€‚
- **æ›´å¼ºçš„é€‚åº”æ€§**ï¼šé€šè¿‡å›¾ç±»å‹è‡ªé€‚åº”é€‰æ‹©é‡æ’åºç­–ç•¥ï¼Œå…¼é¡¾å‹ç¼©ä¸å±€éƒ¨æ€§ã€‚
- **æ›´å¥½çš„å¯æ‰©å±•æ€§**ï¼šæ ¸èåˆå‡å°‘ kernel launch å¼€é”€ï¼Œé€‚åˆé•¿è·¯å¾„ BFSã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
å®éªŒåœ¨ä¸¤ä¸ªåŸºå‡†å¥—ä»¶ä¸Šè¿›è¡Œï¼š
- **GAP Benchmark Suite**ï¼šåŒ…å«å¤šç§çœŸå®ä¸–ç•Œå›¾ï¼Œå¦‚ `GAP-road`, `GAP-twitter`, `GAP-web`, `GAP-kron` ç­‰ã€‚
- **Custom Benchmark**ï¼šä» **SuiteSparse** ä¸­é€‰å–çš„å¤§è§„æ¨¡å›¾ï¼ˆ|V| â‰¥ 2Â³â°ï¼‰ï¼Œæ’é™¤é‡å¤é¡¹åå…± 14 ä¸ªå›¾ï¼Œæ¶µç›–ç¤¾äº¤ã€ç½‘é¡µã€ç§‘å­¦è®¡ç®—ç­‰é¢†åŸŸã€‚

ä»£è¡¨æ€§å›¾ç¤ºä¾‹ï¼š
- `GAP-twitter`: 61M èŠ‚ç‚¹ï¼Œ1.4B è¾¹ï¼ˆsocial-likeï¼‰
- `GAP-road`: 23M èŠ‚ç‚¹ï¼Œ57M è¾¹ï¼ˆroad networkï¼‰
- `webbase-2001`: 118M èŠ‚ç‚¹ï¼Œ1.0B è¾¹ï¼ˆweb graphï¼‰

### å®éªŒè®¾ç½®
- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA H200 GPUï¼ˆæ”¯æŒ compute capability 8.0+ åŠ m8n8k128 æŒ‡ä»¤ï¼‰ï¼Œé…å¤‡ 141GB æ˜¾å­˜ã€‚
- **è½¯ä»¶ç¯å¢ƒ**ï¼šCUDA 13.0ï¼Œg++ 12.3.0ã€‚
- **è¿è¡Œæ–¹å¼**ï¼šæ¯å¼ å›¾æ‰§è¡Œ 64 æ¬¡éšæœºæºç‚¹ BFSï¼Œå–å¹³å‡æ—¶é—´ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - æ€»è¿è¡Œæ—¶é—´ï¼ˆmillisecondsï¼‰
  - ç›¸å¯¹äºåŸºçº¿çš„é€Ÿåº¦æå‡ï¼ˆspeedupï¼‰
  - æ¶ˆèå®éªŒåˆ†æå„ä¼˜åŒ–æ¨¡å—è´¡çŒ®
  - å†…å­˜å ç”¨ã€compression ratioã€update divergence ç­‰è¾…åŠ©æŒ‡æ ‡

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **GAP CPU BFS**ï¼šå¹¶è¡Œ CPU ç‰ˆæœ¬ BFS
- **Gunrock**ï¼šä¸»æµ GPU å›¾åˆ†ææ¡†æ¶ï¼Œæ”¯æŒ push/pull åˆ‡æ¢ä¸è´Ÿè½½å‡è¡¡
- **GSWITCH**ï¼šåŠ¨æ€é€‰æ‹©ä¼˜åŒ–æ¨¡å¼çš„é«˜æ€§èƒ½ GPU BFS
- **BerryBees**ï¼šé¦–ä¸ªä½¿ç”¨ Tensor Cores çš„ BFS æ¡†æ¶ï¼Œä¹Ÿæ˜¯æœ¬æ–‡æœ€ç›´æ¥çš„æ¯”è¾ƒå¯¹è±¡

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 2ï¼‰
| æ–¹æ³• | å¹³å‡é€Ÿåº¦æå‡ï¼ˆvs. BerryBeesï¼‰ | æœ€å¤§é€Ÿåº¦æå‡ |
|------|-------------------------------|-------------|
| **BLEST (full)** | **3.58Ã—** | 8.90Ã— (`Spielman_k600`) |

ä¸å…¶ä»–æ¡†æ¶ç›¸æ¯”ï¼š
- vs. **GAP CPU**: **13.25Ã—**
- vs. **Gunrock**: **4.64Ã—**
- vs. **GSWITCH**: **4.90Ã—**

> æ³¨ï¼šåœ¨æ‰€æœ‰æµ‹è¯•å›¾ä¸Šï¼ŒBLEST å‡ä¼˜äºå…¶ä»–æ¡†æ¶ï¼Œå¹³å‡æé€Ÿè¶…è¿‡ 3.5 å€ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 2 ä¸­ä¸åŒå˜ä½“ï¼‰
| å˜ä½“ | æè¿° | ç›¸æ¯” BerryBees å¹³å‡åŠ é€Ÿ |
|------|------|------------------------|
| **BLEST (a)** | åŸºç¡€ç‰ˆï¼ˆBVSS + æœ€ä¼˜ TC layoutï¼‰ | 1.89Ã— |
| **BLEST (ac)** | (a) + lazy update | 2.71Ã— |
| **BLEST (ab)** | (a) + é‡æ’åºï¼ˆw=2Â¹â¶ï¼‰ | 1.98Ã— |
| **BLEST (full)** | å®Œæ•´ç‰ˆæœ¬ï¼ˆa + b + cï¼‰ | **3.58Ã—** |

âœ… æ‰€æœ‰ä¼˜åŒ–å‡å¸¦æ¥æ­£å‘æ”¶ç›Šï¼Œç»„åˆåæ•ˆæœæ˜¾è‘—æ”¾å¤§ã€‚

### å…¶ä»–é‡è¦è§‚å¯Ÿ
- **é‡æ’åºæœ‰æ•ˆæ€§éªŒè¯**ï¼ˆTable 3ï¼‰ï¼š
  - åœ¨è‡ªç„¶é¡ºåºä¸åˆ©çš„å›¾ä¸Šï¼ˆå¦‚ `GAP-web`ï¼‰ï¼Œéšæœºæ‰“ä¹±å BLEST(ab) æ¯” BLEST(a) å¿« **2.60Ã—**ï¼Œè¯æ˜æ‰€æé‡æ’åºç®—æ³•æœ‰æ•ˆã€‚
- **window size å½±å“**ï¼ˆFigure 3ï¼‰ï¼š
  - å¢å¤§ `w` å¯æŒç»­æå‡ compression ratioï¼ˆä» 0.34 åˆ° 0.61ï¼‰ï¼Œå¸¦æ¥ **2.01Ã— BFS åŠ é€Ÿ**ï¼Œä½†å­˜åœ¨è¾¹é™…é€’å‡æ•ˆåº”ã€‚
- **lazy update å¯å‘å¼é˜ˆå€¼**ï¼š
  - å½“ `update divergence` > 25,000 æ—¶å¯ç”¨ lazy update æ›´æœ‰åˆ©ï¼›å¦åˆ™å¯èƒ½å› é¢å¤– O(n) æ‰«æè€Œå—æŸï¼ˆå¦‚ `mawi`, `Spielman_k600`ï¼‰ã€‚

### å†…å­˜å¼€é”€ï¼ˆTable 4ï¼‰
- **BVSS ç»“æ„ç´§å‡‘**ï¼šé™æ€ç»“æ„å†…å­˜å ç”¨åˆç†ï¼Œæ€»æ˜¾å­˜æ¶ˆè€—å¯æ§ï¼ˆå¤šæ•°å›¾åœ¨ 2â€“20 GB èŒƒå›´ï¼‰ï¼Œå¯åœ¨ä¸»æµ GPU ä¸Šéƒ¨ç½²ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Tensor Cores å¯ç”¨äºé«˜æ•ˆ BFS**ï¼šé€šè¿‡ BVSS å’Œ bitwise TC ç¼–ç¨‹æ¨¡å‹ï¼ŒæˆåŠŸå°†ç¨€ç–å›¾éå†æ˜ å°„åˆ°å¯†é›† MMA å•å…ƒï¼Œçªç ´ä¼ ç»Ÿé™åˆ¶ã€‚
2. **ç»†ç²’åº¦ä»»åŠ¡åˆ’åˆ† + frontier-aware è°ƒåº¦æ˜¯å…³é”®**ï¼šBVSS å®ç°äº†è¿‘ä¹å®Œç¾çš„è´Ÿè½½å‡è¡¡ï¼Œå¹¶æ¶ˆé™¤äº†æ— æ•ˆå·¥ä½œã€‚
3. **å›¾ç»“æ„æ„ŸçŸ¥çš„é¢„å¤„ç†è‡³å…³é‡è¦**ï¼šé’ˆå¯¹ä¸åŒç±»å‹å›¾é‡‡ç”¨ä¸åŒé‡æ’åºç­–ç•¥ï¼ˆJaccard-based vs RCMï¼‰ï¼Œèƒ½æ˜¾è‘—æå‡å‹ç¼©ç‡å’Œç¼“å­˜å±€éƒ¨æ€§ã€‚
4. **å»¶è¿Ÿæ›´æ–° + æ ¸èåˆå¤§å¹…æå‡æ•ˆç‡**ï¼šlazy update å‡å°‘åŸå­å¼€é”€ï¼Œkernel fusion æ¶ˆé™¤åŒæ­¥ç“¶é¢ˆï¼Œå°¤å…¶åˆ©äºæ·±å±‚ BFSã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **é™æ€æ‰§è¡Œç­–ç•¥**ï¼šç›®å‰æœªåœ¨è¿è¡Œæ—¶åŠ¨æ€åˆ‡æ¢ä¼˜åŒ–ç­–ç•¥ï¼ˆå¦‚æ˜¯å¦å¯ç”¨ lazy updateï¼‰ï¼Œå¯¹æŸäº›å›¾ï¼ˆå¦‚é«˜å¸¦å®½ä½†ä½ divergence çš„å›¾ï¼‰å¯èƒ½ä¸æ˜¯æœ€ä¼˜ã€‚
- **é‡æ’åºæˆæœ¬**ï¼šè™½ç„¶å¹¶è¡ŒåŒ–ï¼Œä½†å¯¹äºè¶…å¤§è§„æ¨¡å›¾ï¼ŒJACCARDWITHWINDOWS çš„é¢„å¤„ç†æ—¶é—´ä»ä¸å¯å¿½ç•¥ã€‚
- **ä¾èµ–ç‰¹å®šç¡¬ä»¶**ï¼šå¿…é¡»è¿è¡Œåœ¨æ”¯æŒ m8n8k128 çš„ Ampere æˆ–æ›´æ–°æ¶æ„ GPU ä¸Šï¼ˆå¦‚ A100/H100/H200ï¼‰ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ„å»ºæ™ºèƒ½å†³ç­–ç®¡é“**ï¼šåŸºäºå›¾ç‰¹å¾å’Œè¿è¡Œæ—¶çŠ¶æ€è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®ï¼ˆå¦‚æ˜¯å¦ lazy updateã€window size ç­‰ï¼‰ã€‚
2. **æ‰©å±•è‡³ Multi-source BFS**ï¼šåº”ç”¨äº closeness centralityã€diameter è®¡ç®—ç­‰éœ€è¦å¤šæº BFS çš„åœºæ™¯ã€‚
3. **è·¨å¹³å°ç§»æ¤**ï¼šé€‚é… AMD GPU çš„ Matrix Core æˆ–å…¶ä»–æ”¯æŒ MMA çš„åŠ é€Ÿå™¨ã€‚
4. **åŠ¨æ€è°ƒæ•´æ‰§è¡Œæµ**ï¼šæ ¹æ®å½“å‰ frontier å¯†åº¦ã€å±‚çº§æ·±åº¦ç­‰åŠ¨æ€è°ƒæ•´è°ƒåº¦ä¸æ›´æ–°ç­–ç•¥ã€‚

---

> âœ… **æ€»ç»“**ï¼šBLEST æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§åœ°å°† Tensor Cores é«˜æ•ˆåº”ç”¨äº BFS çš„å·¥ä½œï¼Œé€šè¿‡ **BVSS æ•°æ®ç»“æ„ã€å›¾æ„ŸçŸ¥é‡æ’åºã€æœ€ä¼˜ TC ç¼–ç¨‹ã€lazy update å’Œ kernel fusion** äº”å¤§æŠ€æœ¯åˆ›æ–°ï¼Œåœ¨å¹¿æ³›çš„çœŸå®å›¾ä¸Šå®ç°äº†å¹³å‡ **3.58Ã—** äº SOTA TC-based æ–¹æ³•ï¼ˆBerryBeesï¼‰çš„æ€§èƒ½æå‡ï¼Œç¡®ç«‹äº†åˆ©ç”¨ä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿå›¾ç®—æ³•çš„æ–°èŒƒå¼ã€‚

</details>

---

### 8. [RLLaVA: An RL-central Framework for Language and Vision Assistants](https://arxiv.org/abs/2512.21450)

**Authors**: Lei Zhao, Zihao Ma, Boyu Lin, Yuhe Liu, Wenjun Wu, Lei Huang  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.21450v1  

#### Abstract
We present an RL-central framework for Language and Vision Assistants (RLLaVA) with its formulation of Markov decision process (MDP). RLLaVA decouples RL algorithmic logic from model architecture and distributed execution, supporting researchers in implementing new RL algorithms with minimal code, a...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# RLLaVA: An RL-central Framework for Language and Vision Assistants è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Models, VLMsï¼‰ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†ç°æœ‰çš„ RL æ¡†æ¶ï¼ˆå¦‚ veRLã€OpenRLHFï¼‰ä¸»è¦é¢å‘å¤§è§„æ¨¡é›†ç¾¤å’Œçº¯æ–‡æœ¬å¤§æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- **ç¼ºä¹é’ˆå¯¹å¤šæ¨¡æ€åœºæ™¯çš„ä¸“ç”¨æ¡†æ¶**ï¼šæœªè€ƒè™‘ VLM ä¸­å›¾åƒä¸æ–‡æœ¬è”åˆå¤„ç†ã€ç¯å¢ƒåé¦ˆä¸­è§†è§‰å“åº”ç­‰ç‰¹æ€§ï¼›
- **ç®—æ³•é€»è¾‘ä¸åˆ†å¸ƒå¼æ‰§è¡Œè€¦åˆç´§å¯†**ï¼šå¯¼è‡´ç ”ç©¶äººå‘˜éš¾ä»¥å¿«é€Ÿå®ç°å’Œæµ‹è¯•æ–°çš„ RL ç®—æ³•ï¼›
- **èµ„æºé—¨æ§›é«˜**ï¼šä¾èµ–å¤§å‹ GPU é›†ç¾¤ï¼Œé™åˆ¶äº†ä¸­å°ç ”ç©¶å›¢é˜Ÿçš„å‚ä¸ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
æœ¬æ–‡æå‡º **RLLaVA** â€”â€” ä¸€ä¸ªä»¥ RL ä¸ºä¸­å¿ƒçš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œä¸“ä¸ºè¯­è¨€ä¸è§†è§‰åŠ©æ‰‹è®¾è®¡ã€‚å…¶æ ¸å¿ƒæ€æƒ³åŒ…æ‹¬ï¼š

- **ç»Ÿä¸€çš„ MDP å»ºæ¨¡**ï¼šå°† VLM çš„å›¾æ–‡åºåˆ—å†³ç­–è¿‡ç¨‹å½¢å¼åŒ–ä¸ºç»Ÿä¸€çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMarkov Decision Process, MDPï¼‰ï¼Œæ¶µç›–çŠ¶æ€ç©ºé—´ $S = (V \cup Z)^*$ï¼ˆå«å›¾åƒå’Œæ–‡æœ¬ï¼‰ã€åŠ¨ä½œç©ºé—´ $A = V$ï¼ˆç”Ÿæˆ tokenï¼‰ã€è½¬ç§»åˆ†å¸ƒä¸å¥–åŠ±å‡½æ•°ã€‚
- **ä¸‰é‡è§£è€¦æ¶æ„è®¾è®¡**ï¼š
  - **ç®—æ³•é€»è¾‘**ï¼ˆå¦‚ PPOã€GRPOï¼‰
  - **æ¨¡å‹æ¶æ„**ï¼ˆæ”¯æŒ Qwenã€Phiã€Gemma ç­‰ LLM å’Œ CLIPã€SigLIP ç­‰è§†è§‰ç¼–ç å™¨ï¼‰
  - **åˆ†å¸ƒå¼æ‰§è¡Œå¼•æ“**ï¼ˆå…¼å®¹ FSDPã€DeepSpeedã€vLLMã€SGLangï¼‰
- **è§’è‰²åŒ–ç»„ä»¶æŠ½è±¡**ï¼šå®šä¹‰å››ä¸ªæ ¸å¿ƒè§’è‰²ï¼š
  - `Actor`ï¼šç­–ç•¥ç½‘ç»œç®¡ç†ä¸æ›´æ–°
  - `Reward`ï¼šè½¨è¿¹è¯„åˆ†ï¼ˆè§„åˆ™å¥–åŠ± / å¥–åŠ±æ¨¡å‹ / LLM-as-Judgeï¼‰
  - `Critic`ï¼šä»·å€¼å‡½æ•°ä¼°è®¡ï¼ˆå¯é€‰ï¼‰
  - `Reference`ï¼šå‚è€ƒç­–ç•¥ç”¨äº KL æ­£åˆ™åŒ–

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | RLLaVA | å…¶ä»–æ¡†æ¶ï¼ˆå¦‚ veRLã€OpenRLHFï¼‰ |
|------|--------|-------------------------------|
| **å¤šæ¨¡æ€åŸç”Ÿæ”¯æŒ** | âœ… æ”¯æŒå›¾åƒè¾“å…¥ä¸è§†è§‰ç¯å¢ƒäº¤äº’ | âŒ ä¸»è¦é¢å‘æ–‡æœ¬ä»»åŠ¡ |
| **æ¨¡å—åŒ–ç¨‹åº¦** | âœ… å®Œå…¨è§£è€¦ï¼Œæ”¯æŒæ’ä»¶å¼æ›¿æ¢ | âš ï¸ è€¦åˆè¾ƒå¼ºï¼Œä¿®æ”¹æˆæœ¬é«˜ |
| **èµ„æºæ•ˆç‡** | âœ… å•å¼  24GB GPU å¯è®­ç»ƒ 4B è§„æ¨¡æ¨¡å‹ | âŒ é€šå¸¸éœ€å¤šå¡æˆ–å¤šèŠ‚ç‚¹ |
| **æ˜“ç”¨æ€§** | âœ… YAML é…ç½®é©±åŠ¨ï¼Œè½»é‡ä»£ç æ”¹åŠ¨å³å¯åˆ‡æ¢ç®—æ³• | âŒ éœ€æ·±å…¥ç³»ç»Ÿçº§å¼€å‘ |
| **çµæ´»æ€§** | âœ… æ”¯æŒæ··åˆç®—æ³•ç»„åˆï¼ˆå¦‚ OPO + CLIP-COVï¼‰ | âš ï¸ å›ºå®šæµç¨‹ä¸ºä¸» |

æ­¤å¤–ï¼ŒRLLaVA å€Ÿé‰´äº† TinyLLaVA Factory çš„è½»é‡åŒ–è®¾è®¡ç†å¿µï¼Œåœ¨å°è§„æ¨¡è®¾å¤‡ä¸Šå®ç°äº†é«˜æ•ˆè®­ç»ƒã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
å®éªŒè¦†ç›–äº”ç±»å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

| ä»»åŠ¡ç±»åˆ« | æ•°æ®é›† | æè¿° |
|--------|-------|------|
| **Mathematical Reasoning** | Geometry3K [24] | å›¾åƒç›¸å…³çš„å‡ ä½•æ¨ç†é¢˜ |
| **Counting** | CLEVR-Count-70k [13] | å¤åˆå¯¹è±¡è®¡æ•°ä»»åŠ¡ |
| **Grounding** | RefCOCO/+/g [14,44,25] | æŒ‡ä»£è¡¨è¾¾ç†è§£ï¼ˆå®šä½å›¾åƒä¸­è¢«æè¿°çš„å¯¹è±¡ï¼‰<br>LISA [16]ï¼šè·¨åŸŸæ³›åŒ–æµ‹è¯•é›† |
| **Agentic-Search** | MAT-Search [22] | å¤šè½®è§†è§‰ä¿¡æ¯æ£€ç´¢ï¼ˆç»“åˆç½‘é¡µæœç´¢å·¥å…·ï¼‰ |
| **Agentic-Coding** | MAT-Coding [22] | å›¾åƒæ“ä½œç›¸å…³çš„ä»£ç ç”Ÿæˆä»»åŠ¡ |

### å®éªŒè®¾ç½®
- **æ¨¡å‹**ï¼š
  - Qwen2-VL-2Bã€Qwen2.5-VL-3Bã€Qwen2.5-VL-7B
  - æ”¯æŒ full-parameter fine-tuning ä¸ LoRA å¾®è°ƒ
- **è®­ç»ƒé…ç½®**ï¼š
  - ä¸»è¦ä½¿ç”¨ **GRPO**ï¼ˆGroup Relative Policy Optimizationï¼‰ä½œä¸º RL ç®—æ³•
  - æ¯ä¸ª prompt é‡‡æ · 4â€“8 æ¡å“åº”è½¨è¿¹ï¼ˆrolloutï¼‰
  - åˆ†å¸ƒå¼è®­ç»ƒä½¿ç”¨ **FSDP**
  - æ¨ç†é˜¶æ®µä½¿ç”¨ **vLLM** åŠ é€Ÿ rollout
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - Math / Countingï¼šAccuracy
  - Groundingï¼šIoUï¼ˆIntersection over Unionï¼‰
  - Agentic Tasksï¼ˆSearch & Codingï¼‰ï¼šF1 Score

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Base Model**ï¼šæœªç» RL å¾®è°ƒçš„åŸå§‹ VLMï¼ˆå¦‚ Qwen2.5-VL-instructï¼‰
- **Visual-ARFT [22]**ï¼šä¸“é—¨é’ˆå¯¹å¤šæ¨¡æ€ä»£ç†ä»»åŠ¡è®¾è®¡çš„ RL æ¡†æ¶ï¼Œä½¿ç”¨ 7B æ¨¡å‹è¿›è¡Œå¯¹æ¯”

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1ï¼‰

| Task | Model | Dataset | Metric | Base | GRPO (RLLaVA) |
|------|-------|--------|--------|------|----------------|
| Math | Qwen2.5-VL-3B | Geometry3K | Accuracy | 35.1 | **39.0** (+3.9) |
| Counting | Qwen2.5-VL-3B | CLEVR-Count | Accuracy | 52.0 | **57.5** (+5.5) |
| Grounding | Qwen2-VL-2B | RefCOCO/+/g | IoU | 51.3 | **63.3** (+12.0) |
| Search | Qwen2.5-VL-3B | MAT-Search | F1 | 4.4 | **27.1** (+22.7) |
| Coding | Qwen2.5-VL-3B | MAT-Coding | F1 | 16.9 | **30.6** (+13.7) |

> æ³¨ï¼šè¡¨ä¸­â€œCodingâ€è¡ŒåŸæ–‡è¯¯å†™ä¸ºâ€œOwenâ€ï¼Œåº”ä¸ºâ€œQwenâ€ã€‚

#### ç»“æœåˆ†æï¼š
- æ‰€æœ‰ä»»åŠ¡ä¸Šï¼Œç»è¿‡ RLLaVA è®­ç»ƒçš„æ¨¡å‹å‡æ˜¾è‘—ä¼˜äº base modelï¼›
- **ä»£ç†ç±»ä»»åŠ¡æå‡æœ€å¤§**ï¼ˆSearch +22.7 F1ï¼ŒCoding +13.7 F1ï¼‰ï¼Œè¡¨æ˜ RL å¯¹å¤æ‚æ¨ç†ä¸å·¥å…·è°ƒç”¨èƒ½åŠ›æœ‰å¼ºå¢å¼ºä½œç”¨ï¼›
- æ„ŸçŸ¥ç±»ä»»åŠ¡ï¼ˆCountingã€Groundingï¼‰ä¹Ÿæœ‰æ˜æ˜¾æ”¹è¿›ï¼Œè¯´æ˜æ¨¡å‹è¯­ä¹‰ç†è§£æ›´ç²¾å‡†ã€‚

### æ³›åŒ–èƒ½åŠ›éªŒè¯ï¼ˆTable 2ï¼‰

| Model | RefCOCO | RefCOCO+ | RefCOCOg | LISA (OOD) |
|-------|---------|----------|-----------|------------|
| Base (Qwen2-VL-2B) | 54.79 | 51.48 | 56.75 | 20.78 |
| GRPO (300 steps) | 67.14 | 60.43 | 61.43 | **31.88** |
| Improvement | +12.35 | +8.95 | +4.68 | **+11.10** |

- åœ¨ out-of-domain çš„ **LISA** æ•°æ®é›†ä¸Šæå‡è¾¾ **+11.10 IoU**ï¼Œè¿œè¶…éƒ¨åˆ† in-domain æå‡ï¼›
- è¡¨æ˜ RL è®­ç»ƒå¢å¼ºäº†æ¨¡å‹çš„**è¯­ä¹‰æ¨ç†èƒ½åŠ›**è€Œéç®€å•è®°å¿†æˆ–å¯¹é½ã€‚

### ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”ï¼ˆFigure 2ï¼‰
- åœ¨ **Agentic-Search** ä»»åŠ¡ä¸­ï¼ŒQwen2.5-VL-3B + RLLaVA æ€§èƒ½è¶…è¿‡ Visual-ARFT-7Bï¼ˆ+0.32 All F1ï¼‰ï¼›
- åœ¨ **Agentic-Coding** ä¸Šç•¥é€Šäº Visual-ARFT-7Bï¼Œä½†ä»å¤§å¹…è¶…è¶Š base modelï¼›
- è¡¨æ˜ RLLaVA çš„é€šç”¨æ¡†æ¶å·²èƒ½è¾¾åˆ°ç”šè‡³è¶…è¶Šä¸“ç”¨ç³»ç»Ÿçš„æ€§èƒ½ã€‚

### æ¶ˆèå®éªŒï¼ˆæ–‡ä¸­è™½æœªå•ç‹¬åˆ—å‡ºè¡¨æ ¼ï¼Œä½†æœ‰éšå«åˆ†æï¼‰
- **èµ„æºæ•ˆç‡æœºåˆ¶æœ‰æ•ˆæ€§**ï¼š
  - å¯ç”¨ **co-located execution**ï¼ˆrollout ä¸ä¼˜åŒ–å…±äº« GPU å†…å­˜ï¼‰åï¼Œå•å¡ 24GB GPU å¯å®Œæˆç«¯åˆ°ç«¯è®­ç»ƒï¼›
  - ä½¿ç”¨ **FSDP2 offload** å’Œ **CPU offloading** æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ï¼ˆå³°å€¼çº¦ 21â€“22GBï¼‰ï¼›
- **ç®—æ³•ç»„ä»¶å¯ç»„åˆæ€§**ï¼š
  - æ”¯æŒå¤šç§ advantage estimatorï¼ˆGAEã€GRPOã€RLOOã€OPOï¼‰ä¸ policy lossï¼ˆPPO clipã€GSPOã€DAPOï¼‰è‡ªç”±ç»„åˆï¼›
  - ç¤ºä¾‹ï¼šOPO baseline + CLIP-COV æ›´æ–°ç­–ç•¥ï¼Œä½“ç°é«˜åº¦çµæ´»ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **RLLaVA æˆåŠŸæ„å»ºäº†ä¸€ä¸ªé€‚ç”¨äºå¤šæ¨¡æ€ RL çš„é€šç”¨ã€æ¨¡å—åŒ–æ¡†æ¶**ï¼Œé€šè¿‡ MDP å½¢å¼åŒ–å»ºæ¨¡å’Œä¸‰é‡è§£è€¦è®¾è®¡ï¼Œæå¤§é™ä½äº†ç®—æ³•ç ”å‘é—¨æ§›ã€‚
2. **åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸Šï¼ŒåŸºäº RLLaVA è®­ç»ƒçš„æ¨¡å‹æ˜¾è‘—ä¼˜äº base model**ï¼Œå°¤å…¶åœ¨éœ€è¦å¤šè½®æ¨ç†ä¸å·¥å…·ä½¿ç”¨çš„ agentic åœºæ™¯ä¸­è¡¨ç°çªå‡ºã€‚
3. **å…·å¤‡ä¼˜å¼‚çš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›**ï¼Œåœ¨ LISA ç­‰ out-of-domain æ•°æ®é›†ä¸Šä¹Ÿå–å¾—å¤§å¹…æå‡ï¼Œè¯æ˜ RL æå‡çš„æ˜¯æ·±å±‚è¯­ä¹‰ç†è§£è€Œéè¿‡æ‹Ÿåˆã€‚
4. **èµ„æºæ•ˆç‡æé«˜**ï¼šæ”¯æŒåœ¨å•å¼  24GB GPU ä¸Šå¯¹ 4B çº§åˆ«æ¨¡å‹è¿›è¡Œ full-parameter RL å¾®è°ƒï¼Œæå¤§ä¿ƒè¿›ä¸­å°å›¢é˜Ÿå‚ä¸å¤šæ¨¡æ€ RL ç ”ç©¶ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰å®éªŒé›†ä¸­åœ¨ **é™æ€å›¾åƒ + æ–‡æœ¬è¾“å‡º** åœºæ™¯ï¼Œå°šæœªæ¶‰åŠè§†é¢‘ã€åŠ¨æ€ GUI æˆ–å…·èº«æ™ºèƒ½ä½“ï¼ˆembodied agentsï¼‰ï¼›
- å°½ç®¡æ”¯æŒå¤šç§ç®—æ³•ï¼Œä½†æŸäº›é«˜çº§åŠŸèƒ½ï¼ˆå¦‚è‡ªåŠ¨ reward shapingã€multi-agent RLï¼‰ä»éœ€æ‰‹åŠ¨å®ç°ï¼›
- å¯¹ extremely large models (>7B) çš„æ‰©å±•æ€§æœªå……åˆ†éªŒè¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. å¼€å‘ **tool-call ä¸ multi-turn interaction æ¨¡å—**ï¼Œé›†æˆåŸºç¡€å·¥å…·ä¸ç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼›
2. æ‰©å±•è‡³æ›´å¤š agentic åœºæ™¯ï¼špixel-level reasoningã€GUI agentã€embodied AIï¼›
3. æŒç»­é›†æˆæ–°å…´ RL ç®—æ³•ï¼ˆå¦‚ REMAXã€OPOï¼‰ä¸å¤šæ¨¡æ€æ¶æ„ï¼›
4. æ„å»ºç¤¾åŒºç”Ÿæ€ï¼Œé¼“åŠ±å¼€æºè´¡çŒ®ä¸æ’ä»¶å¼€å‘ã€‚

---

> ğŸ”— **ä»£ç åœ°å€**ï¼šhttps://github.com/TinyLoopX/RLLaVA  
> ğŸ“„ **è®ºæ–‡å‡ºå¤„**ï¼šarXiv:2512.21450

</details>

---

### 9. [TimeBill: Time-Budgeted Inference for Large Language Models](https://arxiv.org/abs/2512.21859)

**Authors**: Qi Fan, An Zou, Yehan Ma  
**Category**: cs.CL  
**Published**: 2025-12-29  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.21859v1  

#### Abstract
Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. H...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šTimeBill: Time-Budgeted Inference for Large Language Models**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€å…·èº«æ™ºèƒ½å’Œå·¥ä¸šè‡ªåŠ¨åŒ–ç­‰**æ—¶é—´æ•æ„Ÿç³»ç»Ÿ**ä¸­åº”ç”¨æ—¥ç›Šå¹¿æ³›ã€‚è¿™äº›åœºæ™¯é€šå¸¸å…·æœ‰ç¡¬å®æ—¶çº¦æŸï¼ˆhard deadlinesï¼‰ï¼Œè¦æ±‚åœ¨ç»™å®šçš„æ—¶é—´é¢„ç®— $ T $ å†…å®Œæˆæ¨ç†å¹¶ç”Ÿæˆé«˜è´¨é‡å“åº”ã€‚

ç„¶è€Œï¼ŒLLMs çš„è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹å¯¼è‡´å…¶ç«¯åˆ°ç«¯æ‰§è¡Œæ—¶é—´å…·æœ‰é«˜åº¦ä¸ç¡®å®šæ€§ï¼Œéš¾ä»¥å»ºæ¨¡å’Œé¢„æµ‹ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„é«˜æ•ˆæ¨ç†æ–¹æ³•ï¼ˆå¦‚åŸºäºå›ºå®šæ¯”ä¾‹çš„ KV Cache è’¸é¦æˆ–é‡åŒ–ï¼‰ç¼ºä¹å¯¹åŠ¨æ€ä»»åŠ¡æ—¶é—´å’Œå“åº”é•¿åº¦å˜åŒ–çš„é€‚åº”èƒ½åŠ›ï¼Œå®¹æ˜“å‡ºç°è¶…æ—¶ï¼ˆoverrunï¼‰æˆ–æ€§èƒ½ä¸‹é™ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ï¼šTimeBill æ¡†æ¶**
æœ¬æ–‡æå‡ºäº† **TimeBill** â€”â€” ä¸€ç§é¢å‘æ—¶é—´é¢„ç®—çš„ LLM æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡**æ¨ç†æ•ˆç‡**ä¸**å“åº”è´¨é‡**ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **ç»†ç²’åº¦å“åº”é•¿åº¦é¢„æµ‹å™¨ï¼ˆFine-grained Response Length Predictor, RLPï¼‰**  
  åŸºäºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ„å»ºåˆ†ç±»å‹é¢„æµ‹å™¨ï¼Œå°†å“åº”é•¿åº¦åˆ’åˆ†ä¸ºå¤šä¸ªâ€œæ¡¶â€ï¼ˆbucketsï¼‰ï¼Œå®ç°å¯¹ç›®æ ‡ LLM è¾“å‡ºé•¿åº¦çš„é«˜ç²¾åº¦é¢„æµ‹ï¼Œä¼˜äºåŸºäº BERT çš„ç²—ç²’åº¦åˆ†ç±»å™¨ã€‚

- **å·¥ä½œè´Ÿè½½å¼•å¯¼çš„æ‰§è¡Œæ—¶é—´ä¼°è®¡å™¨ï¼ˆWorkload-guided Execution Time Estimator, ETEï¼‰**  
  ç»“åˆ **FLOPs åˆ†æå»ºæ¨¡** ä¸ **å®é™…æ€§èƒ½å‰–æï¼ˆprofilingï¼‰**ï¼Œå»ºç«‹é¢„å¡«å……é˜¶æ®µï¼ˆprefill phaseï¼‰å’Œè§£ç æ­¥éª¤ï¼ˆdecoding stepï¼‰çš„æ‰§è¡Œæ—¶é—´æ¨¡å‹ï¼Œå¹¶é›†æˆ KV Cache è’¸å‘æ¯”ä¾‹ $ \alpha $ å¯¹è§£ç å»¶è¿Ÿçš„å½±å“ã€‚

- **æ—¶é—´é¢„ç®—é©±åŠ¨çš„åŠ¨æ€ KV Cache è’¸å‘æœºåˆ¶**  
  æ ¹æ®é¢„æµ‹çš„æœ€åæƒ…å†µæ‰§è¡Œæ—¶é—´ï¼ˆWCETï¼‰å’Œç»™å®šæ—¶é—´é¢„ç®— $ T $ï¼Œåœ¨çº¿è®¡ç®—æœ€ä¼˜çš„ KV Cache è’¸å‘æ¯”ä¾‹ $ \alpha^* $ï¼Œä»¥æœ€å¤§åŒ–å“åº”æ€§èƒ½çš„åŒæ—¶ç¡®ä¿ä¸è¶…æ—¶ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| æ–¹æ³• | æ˜¯å¦æ”¯æŒæ—¶é—´é¢„ç®—æ§åˆ¶ | æ˜¯å¦åŠ¨æ€è°ƒæ•´ | å¯¹å“åº”æ€§èƒ½å½±å“ |
|------|------------------------|---------------|------------------|
| Vanilla Inference | âŒ | âŒ | é«˜è´¨é‡ä½†æ˜“è¶…æ—¶ |
| Fixed-$\alpha$ KV Eviction | âŒ | âŒ | å›ºå®šæŸå¤±ï¼Œæ— æ³•æƒè¡¡ |
| AWQ / Quantization | âŒ | âŒ | å‡å°‘å†…å­˜ä½†ä¸è§£å†³æ—¶é—´ä¸ç¡®å®šæ€§ |
| **TimeBill (æœ¬æ–‡)** | âœ… | âœ… | åŠ¨æ€ä¼˜åŒ–ï¼Œä¿æŒé«˜æ€§èƒ½ä¸”æ»¡è¶³æ—¶é™ |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šTimeBill æ˜¯é¦–ä¸ªæ˜¾å¼å»ºæ¨¡ LLM ç«¯åˆ°ç«¯æ‰§è¡Œæ—¶é—´å¹¶æ”¯æŒ**è¿è¡Œæ—¶è‡ªé€‚åº”é…ç½®**çš„æ¨ç†æ¡†æ¶ï¼Œåœ¨å¤šç§æ—¶é—´é¢„ç®—ä¸‹å‡èƒ½æ˜¾è‘—æå‡ä»»åŠ¡å®Œæˆç‡å’Œå¹³å‡å“åº”è´¨é‡ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **ä¸»æµ‹è¯•é›†**ï¼š`LongBench` â€”â€” ä¸€ä¸ªåŒè¯­ã€å¤šä»»åŠ¡çš„é•¿ä¸Šä¸‹æ–‡ç†è§£åŸºå‡†ï¼Œæ¶µç›–é—®ç­”ã€æ‘˜è¦ã€æ•°å­¦ç­‰å¤šç§ä»»åŠ¡ã€‚
- **è®­ç»ƒé›†ï¼ˆç”¨äº RLPï¼‰**ï¼š`Arena-Human-Preference-100k` æ•°æ®é›†ä¸­çš„æç¤ºï¼ˆpromptï¼‰ï¼Œé¿å…åœ¨æµ‹è¯•é›†ä¸Šè®­ç»ƒã€‚

### **å®éªŒè®¾ç½®**
- **ç›®æ ‡ LLM**ï¼š`Qwen2.5-7B-Instruct`ï¼ˆcontext length: 32,768ï¼›max generation: 8,192ï¼‰
- **SLM æ„å»º RLP**ï¼š`Qwen2.5-0.5B-Instruct`
- **ç¡¬ä»¶å¹³å°**ï¼šIntel Xeon Platinum 8350C + NVIDIA A40 GPU
- **KV Cache è’¸å‘å®ç°**ï¼šé‡‡ç”¨ `SnapKV` æŠ€æœ¯
- **æ—¶é—´é¢„ç®—èŒƒå›´**ï¼š5s ~ 10sï¼ˆæ¯ç§’é€’å¢ï¼‰
- **æœ€å¤§è’¸å‘è¾¾åº¦ $ \alpha_{\text{max}} $**ï¼š95%
- **æ‚²è§‚å› å­ $ k $**ï¼šé»˜è®¤è®¾ä¸º 5ï¼ˆç”¨äº WCET ä¿å®ˆä¼°è®¡ï¼‰

### **è¯„ä¼°æŒ‡æ ‡**
- **å“åº”æ€§èƒ½è¯„åˆ†**ï¼š
  - F1 Score
  - ROUGE-L
  - Levenshtein Distance
- **ä»»åŠ¡å®Œæˆç‡ï¼ˆCompletion Rateï¼‰**ï¼šæŒ‰æ—¶å®Œæˆçš„ä»»åŠ¡å æ¯”
- **å¹³å‡å“åº”æ€§èƒ½å¾—åˆ†ï¼ˆAverage Scoreï¼‰**ï¼šæ‰€æœ‰ä»»åŠ¡çš„åŠ æƒå¹³å‡å¾—åˆ†
- **è¶…æ—¶ç­–ç•¥ï¼ˆOverrun Strategiesï¼‰**ï¼š
  - **Kill**ï¼šè¶…æ—¶åˆ™ç»ˆæ­¢ï¼Œè¾“å‡ºä¸ºç©º
  - **Skip-Next**ï¼šè·³è¿‡åç»­è‹¥å¹²ä»»åŠ¡ç›´åˆ°å½“å‰å®Œæˆ

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| åŸºçº¿æ–¹æ³• | æè¿° |
|---------|------|
| **Vanilla** | ä¸åšä»»ä½•ä¼˜åŒ–ï¼Œç›´æ¥æ¨ç† |
| **Fixed-$\alpha$ KV Eviction** | è®¾ç½®å›ºå®šçš„ KV Cache è’¸å‘æ¯”ä¾‹ï¼ˆ25%, 50%, 75%, 95%ï¼‰ |
| **AWQ** | æƒé‡ 4-bit é‡åŒ–åŠ é€Ÿ |
| **TimeBill (Ours)** | æœ¬æ–‡æå‡ºçš„åŠ¨æ€æ—¶é—´é¢„ç®—æ„ŸçŸ¥æ¨ç†æ¡†æ¶ |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **(1) å“åº”é•¿åº¦é¢„æµ‹å™¨ï¼ˆRLPï¼‰æ•ˆæœï¼ˆTable 1ï¼‰**
| æ–¹æ³• | #Buckets | MAE â†“ | RMSE â†“ | RÂ² â†‘ |
|------|----------|-------|--------|------|
| ProxyModel (BERT-based) | 5 | 105.72 | 136.79 | 0.152 |
| S3 (DistilBERT-based) | 10 | 108.96 | 148.91 | -0.004 |
| **Ours (Regression)** | â€“ | 64.21 | 103.30 | 0.516 |
| **Ours (Classification, B=64)** | 128 | 48.95 | 87.57 | 0.652 |
| **Ours (B=32)** | 256 | 44.15 | 78.63 | 0.719 |
| **Ours (B=16)** | **512** | **42.71** | **78.13** | **0.723** |

> âœ… **ç»“è®º**ï¼šæ‰€æ RLP æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”éšç€ bucket æ•°å¢åŠ ï¼ˆå³ç²’åº¦æ›´ç»†ï¼‰ï¼Œæ€§èƒ½æŒç»­æå‡ã€‚

#### **(2) æ‰§è¡Œæ—¶é—´ä¼°è®¡è¯¯å·®**
- **Prefill Phase MAPE**: 1.22%
- **Decoding Step MAPE**: 1.69%
- å›¾6æ˜¾ç¤ºï¼Œé¢„æµ‹çš„ $ t_{e2e} $ å’Œä¿å®ˆä¼°è®¡çš„ $ t_{WCET} $ å‡ä¸å®æµ‹å€¼é«˜åº¦å»åˆï¼Œä¸” $ t_{WCET} $ èƒ½æœ‰æ•ˆè¦†ç›–çœŸå®æ‰§è¡Œæ—¶é—´ã€‚

#### **(3) å¹³å‡å“åº”å¾—åˆ† vs å®Œæˆç‡ï¼ˆFig. 7ï¼‰**
åœ¨ä¸åŒæ—¶é—´é¢„ç®—ï¼ˆ5~10sï¼‰å’Œä¸¤ç§è¶…æ—¶ç­–ç•¥ä¸‹ï¼š

| æ–¹æ³• | Avg Score â†‘ | Completion Rate â†‘ |
|------|-------------|--------------------|
| Vanilla | å¾ˆä½ | < 40% ï¼ˆå¸¸è¶…æ—¶ï¼‰ |
| Fixed-$\alpha=25\%$ | ä¸­ç­‰åä½ | è¾ƒä½ |
| Fixed-$\alpha=95\%$ | ä½ï¼ˆæ€§èƒ½å·®ï¼‰ | é«˜ï¼ˆæ¥è¿‘ TimeBillï¼‰ |
| **TimeBill** | **æœ€é«˜** | **ä¸ $\alpha=95\%$ ç›¸å½“ç”šè‡³æ›´é«˜** |

> â­ **æ ¸å¿ƒå‘ç°**ï¼šTimeBill åœ¨å‡ ä¹ä¸ç‰ºç‰²å®Œæˆç‡çš„å‰æä¸‹ï¼Œå®ç°äº†æœ€é«˜çš„å¹³å‡å“åº”è´¨é‡ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰ baselineã€‚

#### **(4) æ¶ˆèå®éªŒï¼šæ‚²è§‚å› å­ $ k $ çš„å½±å“ï¼ˆFig. 8ï¼‰**
- å½“ $ k=1\sim5 $ï¼šå¢å¤§ $ k $ å¯æé«˜å®Œæˆç‡å’Œå¹³å‡åˆ†ï¼ˆå› æ›´ä¿å®ˆè°ƒåº¦ï¼‰
- å½“ $ k>5 $ï¼šè¿‡åº¦ä¿å®ˆå¯¼è‡´ $ \alpha^* $ è¿‡å¤§ï¼Œå“åº”è´¨é‡æ€¥å‰§ä¸‹é™
- **æœ€ä½³ $ k \approx 5 $**ï¼Œç¬¦åˆç¡¬å®æ—¶ç³»ç»Ÿè®¾è®¡æƒ¯ä¾‹

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦ç»“è®º**
1. **LLM æ¨ç†å¿…é¡»è€ƒè™‘æ—¶é—´é¢„ç®—çº¦æŸ**ï¼šä¼ ç»Ÿâ€œè¶Šå¿«è¶Šå¥½â€ï¼ˆAFAPï¼‰æˆ–å›ºå®šä¼˜åŒ–ç­–ç•¥æ— æ³•å…¼é¡¾æ—¶æ•ˆæ€§ä¸å“åº”è´¨é‡ã€‚
2. **ç»†ç²’åº¦å“åº”é•¿åº¦é¢„æµ‹è‡³å…³é‡è¦**ï¼šåŸºäº SLM çš„åˆ†ç±»å¼ RLP æ¯” BERT æˆ–å›å½’æ–¹æ³•æ›´å‡†ç¡®ï¼Œå°¤å…¶é€‚åˆé•¿è¾“å…¥åœºæ™¯ã€‚
3. **åˆ†æå»ºæ¨¡ + å®æµ‹æ‹Ÿåˆçš„ ETE æ›´å¯é **ï¼šç»“åˆ FLOPs åˆ†æä¸ profiling çš„æ··åˆå»ºæ¨¡æ–¹å¼ï¼Œèƒ½ç²¾å‡†ä¼°è®¡å« KV Cache è’¸å‘çš„æ‰§è¡Œæ—¶é—´ã€‚
4. **åŠ¨æ€è°ƒæ•´ $ \alpha $ æ˜¯å…³é”®**ï¼šTimeBill èƒ½æ ¹æ®æ¯ä¸ªè¯·æ±‚çš„è¾“å…¥é•¿åº¦ã€æ—¶é—´é¢„ç®—è‡ªåŠ¨è®¡ç®—æœ€ä¼˜è’¸å‘è¾¾åº¦ï¼Œåœ¨ä¿è¯ä¸è¶…æ—¶çš„å‰æä¸‹æœ€å¤§åŒ–æ€§èƒ½ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–ç¦»çº¿å»ºæ¨¡**ï¼šFLOPs ç³»æ•°å’Œ ETE å‚æ•°éœ€æå‰åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Š profilingï¼Œè¿ç§»è‡³æ–°è®¾å¤‡éœ€é‡æ–°æ ¡å‡†ã€‚
- **é¢„æµ‹è¯¯å·®ç´¯ç§¯é£é™©**ï¼šè‹¥ RLP æˆ– ETE å­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œå¯èƒ½å¯¼è‡´ $ t_{WCET} $ ä½ä¼°ï¼Œå¼•å‘è¶…æ—¶ã€‚
- **ä»…æ”¯æŒ KV Cache è’¸å‘**ï¼šæœªæ•´åˆå…¶ä»–åœ¨çº¿ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚ early exitingï¼‰ï¼Œä»æœ‰æ‰©å±•ç©ºé—´ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- å°† TimeBill ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚ AWQã€prompt compressionï¼‰è”åˆä½¿ç”¨ï¼Œè¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚
- æ”¯æŒå¤šæ¨¡æ€ LLM çš„æ—¶é—´é¢„ç®—æ¨ç†ã€‚
- å¼•å…¥åé¦ˆæœºåˆ¶ï¼Œæ ¹æ®å†å²æ‰§è¡Œç»“æœåŠ¨æ€è°ƒæ•´æ‚²è§‚å› å­ $ k $ã€‚
- æ‰©å±•è‡³åˆ†å¸ƒå¼æˆ– disaggregated serving æ¶æ„ä¸‹çš„èµ„æºååŒè°ƒåº¦ã€‚

---

> ğŸ”š **æ€»ç»“**ï¼šTimeBill æ˜¯é¦–ä¸ªç³»ç»ŸåŒ–è§£å†³ LLM æ—¶é—´é¢„ç®—æ¨ç†é—®é¢˜çš„æ¡†æ¶ï¼Œé€šè¿‡â€œé¢„æµ‹ â†’ ä¼°æ—¶ â†’ è‡ªé€‚åº”é…ç½®â€çš„é—­ç¯æœºåˆ¶ï¼Œåœ¨çœŸå®æ—¶é—´çº¦æŸä¸‹å®ç°äº†æ¨ç†æ•ˆç‡ä¸å“åº”è´¨é‡çš„æœ€ä½³å¹³è¡¡ï¼Œä¸º LLM åœ¨å®æ—¶ç³»ç»Ÿä¸­çš„éƒ¨ç½²æä¾›äº†é‡è¦åŸºç¡€ã€‚

</details>

---

### 10. [Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View](https://arxiv.org/abs/2512.22035)

**Authors**: Yanmeng Wang, Zhiwen Dai, Shuai Wang, Jian Zhou, Fu Xiao, Tony Q. S. Quek, Tsung-Hui Chang  
**Category**: cs.DC  
**Published**: 2025-12-29  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.22035v1  

#### Abstract
Federated Fine-Tuning (FFT) has attracted growing interest as it leverages both server- and client-side data to enhance global model generalization while preserving privacy, and significantly reduces the computational burden on edge devices by avoiding training from scratch. Despite these advantages...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šRobust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹ **Federated Fine-Tuning (FFT)** åœ¨ç°å®å¼‚æ„ç½‘ç»œä¸­é¢ä¸´çš„ä¸¤å¤§æŒ‘æˆ˜ï¼š
- **æ•°æ®å¼‚è´¨æ€§**ï¼ˆNon-i.i.d. æ•°æ®åˆ†å¸ƒï¼‰ï¼šå®¢æˆ·ç«¯ç§æœ‰æ•°æ®ä¸æœåŠ¡å™¨å…¬å…±æ•°æ®ä¹‹é—´ã€ä»¥åŠä¸åŒå®¢æˆ·ç«¯ä¹‹é—´çš„æ•°æ®åˆ†å¸ƒå·®å¼‚æ˜¾è‘—ã€‚
- **é€šä¿¡ä¸å¯é æ€§**ï¼šç”±äºè®¾å¤‡æ•…éšœã€ä¿¡å·å¼±è¦†ç›–ã€ç”µæ± è€—å°½ç­‰åŸå› ï¼Œå®¢æˆ·ç«¯ä¸æœåŠ¡å™¨ä¹‹é—´çš„è¿æ¥å­˜åœ¨é¢‘ç¹ä¸”æ¨¡å¼å„å¼‚çš„ä¸­æ–­ï¼ˆå¦‚ç¬æ—¶ã€é—´æ­‡æ€§æˆ–æ··åˆå‹å¤±è´¥ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹èšåˆåå·®ã€‚

ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾åŒè´¨ç½‘ç»œç¯å¢ƒæˆ–éœ€è¦å…ˆéªŒçš„è¿æ¥å¤±è´¥æ¦‚ç‡ï¼Œè¿™åœ¨çœŸå®å•†ä¸šç½‘ç»œï¼ˆåŒ…å«æœ‰çº¿ã€Wi-Fiã€4Gã€5Gç­‰å¤šç§æ ‡å‡†ï¼‰ä¸­ä¸åˆ‡å®é™…ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ï¼šFedAuto
ä½œè€…æå‡ºäº†ä¸€ç§åä¸º **FedAuto** çš„æ–°å‹ FFT æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯é€šè¿‡**è‡ªé€‚åº”èšåˆç­–ç•¥**æ¥ç¼“è§£ä¸Šè¿°åŒé‡æŒ‘æˆ˜çš„å½±å“ã€‚

#### åˆ›æ–°æ€è·¯
1. **æ— éœ€ä¿®æ”¹åŸºç¡€è®¾æ–½æˆ–ä¾èµ–å…ˆéªŒçŸ¥è¯†**  
   FedAuto å®Œå…¨åœ¨æœåŠ¡å™¨ç«¯è¿è¡Œï¼Œä»…é€šè¿‡è°ƒæ•´èšåˆæƒé‡å®ç°é²æ£’æ€§ï¼Œæ— éœ€å¯¹å®¢æˆ·ç«¯è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼Œä¹Ÿæ— éœ€é¢„çŸ¥è¿æ¥å¤±è´¥çš„æ¦‚ç‡æˆ–æ¨¡å¼ï¼Œå…·å¤‡â€œå³æ’å³ç”¨â€ç‰¹æ€§ã€‚

2. **åŒæ¨¡å—è‡ªé€‚åº”èšåˆæœºåˆ¶**
   - **Module 1: Server-Side Compensatory Trainingï¼ˆæœåŠ¡å™¨ä¾§è¡¥å¿è®­ç»ƒï¼‰**  
     å½“æŸäº›ç±»åˆ«çš„æœ¬åœ°æ›´æ–°å› è¿æ¥å¤±è´¥è€Œç¼ºå¤±æ—¶ï¼ŒæœåŠ¡å™¨åˆ©ç”¨å…¶æ‹¥æœ‰çš„å…¬å…±æ•°æ®å­é›† $D_{\text{miss}}$ å¯¹è¿™äº›ç¼ºå¤±ç±»åˆ«è¿›è¡Œé¢å¤–è®­ç»ƒï¼Œç”Ÿæˆä¸€ä¸ªè¡¥å¿æ¨¡å‹ $w_{\text{miss}}$ï¼Œä»¥å¼¥è¡¥çŸ¥è¯†ç©ºç¼ºã€‚
   - **Module 2: Aggregation Weight Optimizationï¼ˆèšåˆæƒé‡ä¼˜åŒ–ï¼‰**  
     å°†æœåŠ¡å™¨æœ¬åœ°æ¨¡å‹ã€è¡¥å¿æ¨¡å‹åŠæˆåŠŸæ¥æ”¶åˆ°çš„å®¢æˆ·ç«¯æ¨¡å‹çš„èšåˆæƒé‡è®¾ä¸ºå¯è°ƒå˜é‡ï¼Œå¹¶é€šè¿‡æ±‚è§£ä¸€ä¸ªå‡¸çš„åŠ æƒæœ€å°äºŒä¹˜é—®é¢˜ï¼Œä½¿æ¯ä¸ªç±»çš„æœ‰æ•ˆè´¡çŒ®æ¯”ä¾‹å°½å¯èƒ½æ¥è¿‘å…¨å±€æ•°æ®åˆ†å¸ƒä¸­çš„çœŸå®æ¯”ä¾‹ $\alpha_{g,c}$ï¼Œä»è€Œæ¶ˆé™¤åç½®ã€‚

3. **æ–°é¢–çš„ per-round aggregation åˆ†æè§†è§’**
   - æ”¾å¼ƒä¼ ç»ŸåŸºäºæœŸæœ›çš„æ”¶æ•›åˆ†æï¼ˆéœ€å‡è®¾å¤±è´¥æ¦‚ç‡å·²çŸ¥ï¼‰ï¼Œè½¬è€Œé‡‡ç”¨æ¯è½®å®é™…èšåˆç»“æœçš„ç¡®å®šæ€§è§†è§’ã€‚
   - å°†è¿æ¥å¤±è´¥å’Œå®¢æˆ·ç«¯é€‰æ‹©çš„å½±å“ç»Ÿä¸€åµŒå…¥åˆ°èšåˆæƒé‡ä¸­ï¼Œå®ç°äº†å¯¹**æ¯ä¸€æ¬¡å…·ä½“æ‰§è¡Œè¿‡ç¨‹**ï¼ˆeach individual realizationï¼‰çš„æ”¶æ•›ä¿è¯ï¼Œç†è®ºæ›´å¼ºã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ç°æœ‰æ–¹æ³•å±€é™ | FedAuto ä¼˜åŠ¿ |
|------|---------------|-------------|
| **é€šä¿¡å¯é æ€§å»ºæ¨¡** | éœ€è¦ä¼°è®¡å¤±è´¥æ¦‚ç‡ï¼ˆå¦‚ TF-Aggregation, FedAWEï¼‰æˆ–ä¼˜åŒ–ç‰©ç†èµ„æºï¼ˆå¦‚ ResourceOptï¼‰ | ä¸éœ€è¦ä»»ä½•å…ˆéªŒçŸ¥è¯†ï¼Œé€‚ç”¨äºå¤æ‚å¤šå˜çš„çœŸå®ç½‘ç»œ |
| **éƒ¨ç½²çµæ´»æ€§** | ç‰©ç†å±‚ä¼˜åŒ–éš¾ä»¥è·¨æ ‡å‡†å®æ–½ï¼›å®¢æˆ·ç«¯éœ€é…åˆï¼ˆå¦‚ SCAFFOLD åŒå‘ä¼ è¾“ï¼‰ | ä»…æœåŠ¡å™¨ç«¯æ“ä½œï¼Œå…¼å®¹ç°æœ‰ç³»ç»Ÿï¼Œæ˜“äºéƒ¨ç½² |
| **ç†è®ºä¿éšœ** | å¤šæ•°ä¸ºæœŸæœ›æ„ä¹‰ä¸‹çš„æ”¶æ•› | æä¾›æ¯æ¬¡è¿è¡Œéƒ½æˆç«‹çš„å¼ºæ”¶æ•›ä¿è¯ |
| **æ€§èƒ½ç¨³å®šæ€§** | åœ¨é i.i.d. + ä¸å¯é è¿æ¥ä¸‹è¡¨ç°ä¸ç¨³å®šç”šè‡³å´©æºƒï¼ˆè§ Fig. 2ï¼‰ | åœ¨å„ç§å¤±è´¥æ¨¡å¼å’Œæ•°æ®åˆ†å¸ƒä¸‹å‡ä¿æŒç¨³å®šé«˜æ€§èƒ½ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **MNIST**: 10 ç±»æ‰‹å†™æ•°å­—å›¾åƒ
- **CIFAR-10**: 10 ç±»è‡ªç„¶å›¾åƒ
- **CIFAR-100**: 100 ç±»ç»†ç²’åº¦è‡ªç„¶å›¾åƒ

æ¯ç§æ•°æ®é›†å‡æµ‹è¯•äº†ä¸¤ç§è§„æ¨¡çš„æ¨¡å‹ï¼š
- å°æ¨¡å‹ï¼šCNN / ResNet
- å¤§æ¨¡å‹ï¼šViTï¼ˆç”¨äº LoRA å¾®è°ƒï¼‰

---

### å®éªŒè®¾ç½®
- **å®¢æˆ·ç«¯æ•°é‡**ï¼š$N = 20$
- **ç½‘ç»œå¼‚æ„æ€§æ¨¡æ‹Ÿ**ï¼šå®¢æˆ·ç«¯é€šè¿‡ä¸åŒé€šä¿¡æ–¹å¼æ¥å…¥æœåŠ¡å™¨ï¼š
  - æœ‰çº¿ï¼ˆEthernetï¼‰
  - Wi-Fiï¼ˆ2.4GHz å’Œ 5GHzï¼‰
  - 4G
  - 5G
- **è¿æ¥å¤±è´¥æ¨¡å¼**ï¼š
  - **Transient**ï¼šç”±ä¿¡é“æ³¢åŠ¨å¼•èµ·çš„çŸ­æ—¶ä¸­æ–­
  - **Intermittent**ï¼šç”±è®¾å¤‡æ–­ç”µç­‰å¼•èµ·çš„æ—¶é—´è¾ƒé•¿çš„æŒç»­ä¸­æ–­
  - **Mixed**ï¼šä¸¤è€…ç»“åˆ
- **å‚ä¸ç‡**ï¼š
  - Full Participation: $K=20$
  - Partial Participation: $K=10$

---

### è¯„ä¼°æŒ‡æ ‡
- **ä¸»æŒ‡æ ‡**ï¼šæµ‹è¯•å‡†ç¡®ç‡ï¼ˆTesting Accuracy %ï¼‰
- **æ”¶æ•›è¡Œä¸º**ï¼šç»˜åˆ¶å¤šä¸ªé€šä¿¡è½®æ¬¡ä¸‹çš„å‡†ç¡®ç‡æ›²çº¿
- **æ¶ˆèå®éªŒ**ï¼šéªŒè¯ä¸¤ä¸ªæ¨¡å—ï¼ˆè¡¥å¿è®­ç»ƒ vs æƒé‡ä¼˜åŒ–ï¼‰çš„ç‹¬ç«‹ä½œç”¨

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **ä¸­å¿ƒåŒ–è®­ç»ƒ** | Centralized(Public) |
| **ç»å…¸ FL** | FedAvg, FedProx, SCAFFOLD |
| **æ”¹è¿›èšåˆ** | FedLAW, TF-Aggregation |
| **åº”å¯¹è¿æ¥å¤±è´¥** | FedAWE |
| **FFTä¸“ç”¨** | FedEx-LoRA |
| **èµ„æºä¼˜åŒ–** | ResourceOpt-1ï¼ˆè”åˆä¼˜åŒ–ï¼‰ã€ResourceOpt-2ï¼ˆåˆ†æ ‡å‡†ä¼˜åŒ–ï¼‰ |
| **ç†æƒ³æƒ…å†µ** | FedAvg(Ideal) â€”â€” æ— è¿æ¥å¤±è´¥çš„ç†æƒ³ç‰ˆæœ¬ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ‘˜å½•è‡ª Tables 1â€“4ï¼‰

#### è¡¨æ ¼æ±‡æ€»ï¼ˆMixed Failures, Non-i.i.d., Full Participation, Full-Parameter Fine-Tuningï¼‰

| æ–¹æ³• | MNIST (%) | CIFAR-10 (%) | CIFAR-100 (%) |
|------|-----------|--------------|----------------|
| **FedAuto (Ours)** | **97.96 Â± 0.25** | **77.00 Â± 0.55** | **51.43 Â± 1.49** |
| FedAvg | 93.28 Â± 1.18 | 68.35 Â± 0.34 | 47.85 Â± 1.06 |
| FedProx | 93.45 Â± 1.39 | 68.27 Â± 0.38 | 47.54 Â± 1.01 |
| SCAFFOLD | 97.05 Â± 0.48 | 74.58 Â± 0.46 | 43.87 Â± 0.88 |
| FedLAW | 94.46 Â± 1.46 | 10.00 Â± 0.00 | 51.40 Â± 1.49 |
| FedAWE | 67.97 Â± 9.00 | 68.05 Â± 1.54 | 34.60 Â± 0.59 |
| **FedAvg(Ideal)** | 98.35 Â± 0.16 | 77.19 Â± 0.77 | 50.57 Â± 1.33 |

> âœ… **è§‚å¯Ÿ**ï¼šFedAuto åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿ï¼Œå°¤å…¶åœ¨ CIFAR-10 ä¸Šæå‡æ˜æ˜¾ï¼ˆ+8.65%ï¼‰ï¼Œä¸”æ¥è¿‘ç†æƒ³ä¸Šé™ã€‚

---

#### LoRA å¾®è°ƒç»“æœï¼ˆNon-i.i.d., Mixed Failuresï¼‰

| æ–¹æ³• | CIFAR-10 (%) | CIFAR-100 (%) |
|------|--------------|----------------|
| **FedAuto (Ours)** | **95.69 Â± 0.13** | **85.15 Â± 0.29** |
| FedAvg | 93.96 Â± 0.45 | 81.82 Â± 0.66 |
| FedEx-LoRA | 93.86 Â± 0.55 | 80.13 Â± 1.76 |
| **FedAvg(Ideal)** | 95.76 Â± 0.13 | 86.14 Â± 0.21 |

> âœ… **è§‚å¯Ÿ**ï¼šFedAuto åœ¨ LoRA åœºæ™¯ä¸‹ä¾ç„¶é¢†å…ˆï¼Œä¸”åœ¨æ›´å¤æ‚çš„ CIFAR-100 ä¸Šä¼˜åŠ¿æ›´å¤§ï¼ˆ+3.33%ï¼‰ã€‚

---

### ä¸èµ„æºä¼˜åŒ–æ–¹æ³•å¯¹æ¯”ï¼ˆFig. 5ï¼‰
- **ResourceOpt-1**ï¼šè¯•å›¾å‡è¡¡æ‰€æœ‰å®¢æˆ·ç«¯å¤±è´¥æ¦‚ç‡ï¼Œä½†å—é™äºæœ€å·®é“¾è·¯ï¼Œå¯¼è‡´æœ‰æ•ˆè¿æ¥æ•°ä¸‹é™ï¼Œæ€§èƒ½æ³¢åŠ¨å¤§ã€‚
- **ResourceOpt-2**ï¼šæŒ‰é€šä¿¡æ ‡å‡†åˆ†åˆ«ä¼˜åŒ–ï¼Œä»æ— æ³•è§£å†³è·¨æ ‡å‡†é—´çš„ä¸å¹³è¡¡é—®é¢˜ã€‚
- **FedAuto**ï¼šæ— éœ€å¹²é¢„åº•å±‚èµ„æºï¼Œåœ¨å„ç§å¤±è´¥æ¨¡å¼ä¸‹å‡è¡¨ç°å‡º**æ›´ç¨³å®šã€æ›´é«˜ç²¾åº¦**çš„æ”¶æ•›æ€§èƒ½ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 5ï¼‰

| è®¾ç½® | MNIST | CIFAR-10 | CIFAR-100 |
|------|--------|----------|------------|
| **å®Œæ•´ FedAuto**ï¼ˆè¡¥å¿ + æƒé‡ä¼˜åŒ–ï¼‰ | **97.96** | **77.00** | **51.43** |
| ä»…è¡¥å¿è®­ç»ƒï¼ˆæ— æƒé‡ä¼˜åŒ–ï¼‰ | 96.68 | 73.83 | 49.75 |
| ä»…æƒé‡ä¼˜åŒ–ï¼ˆæ— è¡¥å¿è®­ç»ƒï¼‰ | 94.06 | 72.21 | 47.88 |
| æ— ä»»ä½•å¢å¼ºï¼ˆåŸå§‹ FedAvgï¼‰ | 93.28 | 68.35 | 47.85 |

> ğŸ” **ç»“è®º**ï¼š
- ä¸¤ä¸ªæ¨¡å—å‡æœ‰æ˜¾è‘—è´¡çŒ®ï¼›
- **è¡¥å¿è®­ç»ƒ**ä¸»è¦ç”¨äºå¡«è¡¥ç¼ºå¤±ç±»åˆ«ï¼›
- **æƒé‡ä¼˜åŒ–**è¿›ä¸€æ­¥ç²¾ç»†åŒ–å¹³è¡¡å„ç±»åˆ«è´¡çŒ®ï¼›
- äºŒè€…ååŒä½œç”¨å¸¦æ¥æœ€å¤§å¢ç›Šã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **è¿æ¥ä¸å¯é æ€§ä¼šåŠ å‰§æ•°æ®å¼‚è´¨æ€§çš„è´Ÿé¢å½±å“**  
   åœ¨é i.i.d. æ•°æ®ä¸‹ï¼Œè¿æ¥é¢‘ç¹ä¸­æ–­ä¼šå¯¼è‡´éƒ¨åˆ†å®¢æˆ·ç«¯é•¿æœŸæ— æ³•å‚ä¸èšåˆï¼Œé€ æˆæ¨¡å‹åå‘æ´»è·ƒå®¢æˆ·ç«¯çš„æ•°æ®åˆ†å¸ƒï¼Œä¸¥é‡æŸå®³æ³›åŒ–èƒ½åŠ›ï¼ˆè§ Theorem 1 ä¸­ term (14b)ï¼‰ã€‚

2. **æ ‡ç­¾ç›¸å…³å¼‚è´¨æ€§ï¼ˆlabel-related heterogeneityï¼‰æ˜¯ä¸»å¯¼è¯¯å·®æº**  
   å®éªŒå’Œç†è®ºè¡¨æ˜ï¼Œåœ¨é i.i.d. åœºæ™¯ä¸­ï¼Œç±»åˆ«åˆ†å¸ƒåå·®å¸¦æ¥çš„æ¢¯åº¦åç§»è¿œå¤§äºç‰¹å¾å†…éƒ¨å·®å¼‚ï¼Œå› æ­¤åº”ä¼˜å…ˆæ ¡æ­£ç±»åˆ«è´¡çŒ®æ¯”ä¾‹ã€‚

3. **FedAuto èƒ½æœ‰æ•ˆæŠ‘åˆ¶èšåˆåå·®**  
   é€šè¿‡è¡¥å¿è®­ç»ƒå’ŒåŠ¨æ€æƒé‡è°ƒæ•´ï¼ŒFedAuto æˆåŠŸå°†æœ‰æ•ˆç±»åˆ«åˆ†å¸ƒé€¼è¿‘å…¨å±€åˆ†å¸ƒï¼ˆ$\hat{\alpha}_c \approx \alpha_{g,c}$ï¼‰ï¼Œä»è€Œä½¿æ”¶æ•›åå·®è¶‹è¿‘äºé›¶ã€‚

4. **ä¼˜äºé€šä¿¡èµ„æºä¼˜åŒ–ç­–ç•¥**  
   å³ä½¿ä¸ä¸“é—¨è®¾è®¡çš„ç‰©ç†å±‚èµ„æºåˆ†é…æ–¹æ³•ç›¸æ¯”ï¼ŒFedAuto ä¹Ÿèƒ½å–å¾—æ›´å¥½æˆ–ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”æ— éœ€æ”¹å˜ç½‘ç»œé…ç½®ï¼Œæ›´å…·å®ç”¨æ€§ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–æœåŠ¡å™¨æ‹¥æœ‰å®Œæ•´çš„ç±»åˆ«è¦†ç›–**  
  å½“å‰å‡è®¾æœåŠ¡å™¨çš„å…¬å…±æ•°æ®åŒ…å«æ‰€æœ‰å®¢æˆ·ç«¯å¯èƒ½å‡ºç°çš„ç±»åˆ«ã€‚è‹¥å­˜åœ¨â€œå…¨æ–°ç±»åˆ«â€ï¼Œåˆ™è¡¥å¿è®­ç»ƒæ— æ³•ç”Ÿæ•ˆã€‚æœªæ¥å¯é€šè¿‡ **class-incremental learning** æˆ– **diffusion-based data generation** æ‰©å±•ã€‚
- **è®¡ç®—å¼€é”€é›†ä¸­åœ¨æœåŠ¡å™¨**  
  è™½ç„¶è¾¹ç¼˜è®¾å¤‡è´Ÿæ‹…è½»ï¼Œä½†æœåŠ¡å™¨éœ€æ‰§è¡Œé¢å¤–è®­ç»ƒå’Œä¼˜åŒ–ï¼Œå¯¹å¤§è§„æ¨¡æ¨¡å‹å¯èƒ½æ„æˆå‹åŠ›ã€‚
- **æœªè€ƒè™‘éšç§æ³„éœ²é£é™©**  
  å®¢æˆ·ç«¯éœ€å…±äº«æœ¬åœ°ç±»åˆ«åˆ†å¸ƒ $\{\alpha_{i,c}\}$ï¼Œè™½ä¿¡æ¯é‡å°ï¼Œä½†ä»å­˜åœ¨æ½œåœ¨éšç§æ³„éœ²é£é™©ï¼Œå¯ç»“åˆ **secure multi-party computation** è¿›ä¸€æ­¥åŠ å›ºã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. æ‰©å±•è‡³ **Personalized FL** åœºæ™¯ï¼Œæ”¯æŒä¸ªæ€§åŒ–æ¨¡å‹è¾“å‡ºã€‚
2. æ¢ç´¢ **semi-supervised** å’Œ **time-varying data distributions** ä¸‹çš„é²æ£’ FFTã€‚
3. ç»“åˆ **class-incremental learning** ä¸ **generative models**ï¼Œå¤„ç†æœåŠ¡å™¨æœªçŸ¥ç±»åˆ«çš„åœºæ™¯ã€‚
4. è®¾è®¡æ›´é«˜æ•ˆçš„èšåˆæƒé‡æ±‚è§£ç®—æ³•ï¼Œé™ä½æœåŠ¡å™¨è®¡ç®—è´Ÿæ‹…ã€‚

--- 

> âœ… **æ€»ä½“è¯„ä»·**ï¼š  
> æœ¬æ–‡æå‡ºçš„ **FedAuto** æ˜¯ä¸€ç§ç®€æ´è€Œå¼ºå¤§çš„ FFT æ¡†æ¶ï¼Œä»**èšåˆè§†è§’**å‡ºå‘ï¼Œå·§å¦™åœ°è§£å†³äº†å¼‚æ„ç½‘ç»œä¸­è¿æ¥ä¸å¯é ä¸æ•°æ®éç‹¬ç«‹åŒåˆ†å¸ƒçš„è€¦åˆéš¾é¢˜ã€‚å…¶å®éªŒå……åˆ†ã€ç†è®ºä¸¥è°¨ï¼Œå…¼å…·å®ç”¨æ€§å’Œå…ˆè¿›æ€§ï¼Œä¸ºè”é‚¦å­¦ä¹ åœ¨çœŸå®å·¥ä¸šåœºæ™¯ä¸­çš„è½åœ°æä¾›äº†é‡è¦å‚è€ƒã€‚

</details>

---

### 11. [DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction](https://arxiv.org/abs/2512.22007)

**Authors**: Aicha Boutorh, Soumia Bouyahiaoui, Sara Belhadj, Nour El Yakine Guendouz, Manel Kara Laouar  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.22007v1  

#### Abstract
Predicting the binding affinity between antigens and antibodies is fundamental to drug discovery and vaccine development. Traditional computational approaches often rely on experimentally determined 3D structures, which are scarce and computationally expensive to obtain. This paper introduces DuaDee...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»ŸæŠ—åŸ-æŠ—ä½“äº²å’ŒåŠ›é¢„æµ‹æ–¹æ³•ä¸¥é‡ä¾èµ–å®éªŒæµ‹å®šçš„ä¸‰ç»´ï¼ˆ3Dï¼‰ç»“æ„ï¼Œè€Œè¿™ç±»ç»“æ„è·å–æˆæœ¬é«˜ã€è€—æ—¶é•¿ï¼Œä¸”åœ¨æŠ—ä½“-æŠ—åŸå¤åˆç‰©ä¸­ï¼ˆå°¤å…¶æ˜¯CDR-H3ç¯ï¼‰å­˜åœ¨é«˜åº¦æ„è±¡çµæ´»æ€§ï¼Œå¯¼è‡´ç»“æ„å»ºæ¨¡å›°éš¾ã€‚è¿™æ„æˆäº†â€œ**ç»“æ„æ€§ç“¶é¢ˆ**â€ï¼ˆstructural bottleneckï¼‰ï¼Œé™åˆ¶äº†é«˜é€šé‡è¯ç‰©ç­›é€‰å’Œç–«è‹—è®¾è®¡ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡º **DuaDeep-SeqAffinity** â€”â€”ä¸€ç§**ä»…åŸºäºæ°¨åŸºé…¸åºåˆ—**ï¼ˆsequence-onlyï¼‰çš„åŒæµæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæŠ—åŸ-æŠ—ä½“äº²å’ŒåŠ›é¢„æµ‹ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **Dual-Stream Hybrid Architecture**ï¼šé‡‡ç”¨å¹¶è¡Œçš„åŒåˆ†æ”¯ç»“æ„ï¼š
  - **Transformer åˆ†æ”¯**ï¼šæ•æ‰åºåˆ—ä¸­çš„**å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»**ï¼ˆlong-range global dependenciesï¼‰ï¼Œç†è§£è›‹ç™½è´¨æŠ˜å æ¨¡å¼å’Œç¨³å®šæ€§ã€‚
  - **1D CNN åˆ†æ”¯**ï¼šæå–**å±€éƒ¨åºåˆ—æ¨¡ä½“**ï¼ˆlocal motifsï¼‰ï¼Œè¯†åˆ«ç»“åˆçƒ­ç‚¹ç­‰ç²¾ç»†ç‰¹å¾ã€‚
- **å¤šå°ºåº¦ç‰¹å¾èåˆæœºåˆ¶**ï¼šé€šè¿‡ä¸“é—¨è®¾è®¡çš„èåˆæ¨¡å—æ•´åˆæ¥è‡ªTransformerå’ŒCNNçš„äº’è¡¥ç‰¹å¾ï¼Œå®ç°å¯¹ç»“åˆç•Œé¢ç‰©ç†åŒ–å­¦ç‰¹æ€§çš„å…¨é¢å»ºæ¨¡ã€‚
- **å®Œå…¨è„±ç¦»3Dç»“æ„è¾“å…¥**ï¼šä»…ä½¿ç”¨åŸå§‹æ°¨åŸºé…¸åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ **ESM-2 è›‹ç™½è´¨è¯­è¨€æ¨¡å‹**ï¼ˆProtein Language Model, PLMï¼‰ç”Ÿæˆé«˜è´¨é‡åµŒå…¥è¡¨ç¤ºã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **ä¼˜äºçº¯åºåˆ—æ¨¡å‹**ï¼šç›¸æ¯”å•ä¸€æ¶æ„ï¼ˆå¦‚ä»…ç”¨Transformeræˆ–CNNï¼‰æˆ–å…¶ä»–åºåˆ—æ¨¡å‹ï¼ˆå¦‚DG-Affinityã€MVSF-ABï¼‰ï¼ŒDuaDeepåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—æ›´ä¼˜æ€§èƒ½ã€‚
- **è¶…è¶Šç»“æ„-åºåˆ—æ··åˆæ¨¡å‹**ï¼šåœ¨AUCæŒ‡æ ‡ä¸Šç”šè‡³è¶…è¿‡äº†éœ€è¦3Dç»“æ„è¾“å…¥çš„ **WALLE-Affinity** æ¨¡å‹ï¼Œè¯æ˜é«˜è´¨é‡åºåˆ—åµŒå…¥è¶³ä»¥æ›¿ä»£éƒ¨åˆ†ç»“æ„ä¿¡æ¯ã€‚
- **é«˜æ•ˆå¯æ‰©å±•**ï¼šæ— éœ€ç»“æ„é¢„æµ‹æ­¥éª¤ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æŠ—ä½“åº“çš„å¿«é€Ÿç­›é€‰ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- ä½¿ç”¨å…¬å¼€æ•°æ®é›† **AbRank**ï¼ŒåŒ…å«ç»å®éªŒéªŒè¯çš„æŠ—åŸ-æŠ—ä½“é…å¯¹åŠå…¶ç»“åˆäº²å’ŒåŠ›ï¼ˆKdå€¼ï¼‰ã€‚
- è¾“å…¥ä¸ºæŠ—åŸåŠæŠ—ä½“é‡é“¾/è½»é“¾çš„æ°¨åŸºé…¸åºåˆ—ã€‚
- ç»è¿‡æ¸…æ´—åï¼Œå°†Kdè½¬æ¢ä¸ºè´Ÿå¯¹æ•°å½¢å¼ `pKd = 9 - log10(Kd)` å¹¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚

### å®éªŒè®¾ç½®
- **æ•°æ®åˆ’åˆ†**ï¼šéšæœºåˆ†ä¸º 80% è®­ç»ƒé›†ã€10% éªŒè¯é›†ã€10% æµ‹è¯•é›†ï¼Œç¡®ä¿æŠ—åŸå’ŒæŠ—ä½“åºåˆ—åœ¨å„é›†åˆé—´æ— é‡å ï¼Œä»¥è¯„ä¼°æ³›åŒ–èƒ½åŠ›ã€‚
- **åºåˆ—ç¼–ç **ï¼šä½¿ç”¨ **ESM-2**ï¼ˆ`facebook/esm2_t12_35M_UR50D`ï¼‰ç”Ÿæˆå›ºå®šç»´åº¦çš„ä¸Šä¸‹æ–‡åŒ–åµŒå…¥å‘é‡ã€‚
- **æ¨¡å‹å†»ç»“ç­–ç•¥**ï¼šESM-2 ç¼–ç å™¨æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒå†»ç»“ï¼ˆfrozenï¼‰ï¼Œä»…å¾®è°ƒåç»­ç½‘ç»œå‚æ•°ã€‚

### è¯„ä¼°æŒ‡æ ‡
- å›å½’ä»»åŠ¡æŒ‡æ ‡ï¼š
  - **RMSE**ï¼ˆRoot Mean Square Errorï¼‰
  - **MAE**ï¼ˆMean Absolute Errorï¼‰
  - **RÂ²**ï¼ˆCoefficient of Determinationï¼‰
  - **Pearson ç›¸å…³ç³»æ•°**
  - **Spearman ç›¸å…³ç³»æ•°**
- åˆ†ç±»/æ’åºä»»åŠ¡æŒ‡æ ‡ï¼š
  - **AUC**ï¼ˆArea Under the ROC Curveï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **å†…éƒ¨åŸºçº¿**ï¼š
  - **ESM-T**ï¼ˆESM-2 + Transformer-onlyï¼‰
  - **ESM-C**ï¼ˆESM-2 + CNN-onlyï¼‰
- **å¤–éƒ¨SOTAæ¨¡å‹**ï¼š
  - **DG-Affinity**ï¼ˆsequence-only, ConvNeXt æ¶æ„ï¼‰
  - **MVSF-AB**ï¼ˆsequence-only, å¤šè§†å›¾CNN+MLPï¼‰
  - **WALLE-Affinity**ï¼ˆstructure + sequence, GNN-basedï¼‰
  - **MINT** å’Œ **ESM-2 + AntiBERTy**ï¼ˆå‡ä¸ºsequence-only ranking modelsï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæµ‹è¯•é›†ï¼‰

| æ¨¡å‹ | RMSE | RÂ² | Pearson | AUC |
|------|------|-----|---------|-----|
| **DuaDeep (Ours)** | **0.737** | **0.460** | **0.688** | **0.890** |
| ESM-C (CNN-only) | 0.773 | 0.397 | 0.636 | 0.854 |
| ESM-T (Transformer-only) | 0.799 | 0.357 | 0.625 | 0.852 |
| DG-Affinity [44] | â€“ | â€“ | 0.656 | â€“ |
| MVSF-AB [46] | 1.447 | 0.467 | â€“ | â€“ |
| WALLE-Affinity (Ranking) [50] | â€“ | â€“ | â€“ | 0.866 |

> æ³¨ï¼šéƒ¨åˆ†å¤–éƒ¨æ¨¡å‹æœªæŠ¥å‘Šæ‰€æœ‰æŒ‡æ ‡ï¼Œè¡¨æ ¼ä¾æ®åŸæ–‡å¼•ç”¨è¡¥å……ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- DuaDeep åœ¨ **æ‰€æœ‰å›å½’æŒ‡æ ‡ä¸Šå‡ä¼˜äºå•åˆ†æ”¯æ¨¡å‹**ï¼ˆESM-C å’Œ ESM-Tï¼‰ï¼š
  - RMSE ä¸‹é™ > 4.6%
  - Pearson æå‡çº¦ 5â€“6%
- ç›¸æ¯”å½“å‰æœ€ä¼˜åºåˆ—æ¨¡å‹ **DG-Affinity**ï¼ˆPearson=0.656ï¼‰ï¼ŒDuaDeep è¾¾åˆ° **0.688**ï¼Œæ˜¾è‘—æå‡ã€‚
- å°½ç®¡ **MVSF-AB** çš„ RÂ² æ¥è¿‘ï¼ˆ0.467 vs 0.460ï¼‰ï¼Œä½†å…¶ RMSE é«˜è¾¾ 1.447ï¼Œè¯´æ˜é¢„æµ‹è¯¯å·®æ›´å¤§ã€‚
- åœ¨ **AUC ä¸Šè¾¾åˆ° 0.890**ï¼Œä¸ä»…è¶…è¿‡æ‰€æœ‰çº¯åºåˆ—æ¨¡å‹ï¼ˆMINT: 0.775, ESM-2+AntiBERTy: 0.761ï¼‰ï¼Œè¿˜**è¶…è¶Šäº†ä¾èµ–3Dç»“æ„çš„ WALLE-Affinity (0.866)**ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **åŒæµç»“æ„æœ‰æ•ˆæ€§éªŒè¯**ï¼š
  - å•ç‹¬ä½¿ç”¨ CNN æˆ– Transformer å‡è¡¨ç°æ¬¡ä¼˜ï¼Œè¡¨æ˜ä¸¤è€…æå–çš„ä¿¡æ¯å…·æœ‰äº’è¡¥æ€§ã€‚
  - CNN åˆ†æ”¯åœ¨å±€éƒ¨ç‰¹å¾ï¼ˆå¦‚ç»“åˆä½ç‚¹ï¼‰å»ºæ¨¡ä¸Šç•¥å¼ºäº Transformerï¼›è€Œ Transformer æ›´æ“…é•¿æ•æ‰é•¿ç¨‹ä¾èµ–ã€‚
- **èåˆç­–ç•¥é‡è¦æ€§**ï¼š
  - ç‰¹å¾æ‹¼æ¥ï¼ˆconcatenationï¼‰åçš„è”åˆè¡¨ç¤ºèƒ½æœ‰æ•ˆå¢å¼ºæœ€ç»ˆé¢„æµ‹å¤´çš„è¡¨ç°ã€‚
  - å­¦ä¹ æ›²çº¿æ˜¾ç¤ºï¼ŒåŒæµæ¨¡å‹æ”¶æ•›æ›´å¿«ã€éªŒè¯æŸå¤±æ›´ä½ï¼ˆè§ Figure 7ï¼‰ï¼Œæ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
- **é«˜è´¨é‡åºåˆ—åµŒå…¥è¶³ä»¥æ”¯æ’‘é«˜ç²¾åº¦äº²å’ŒåŠ›é¢„æµ‹**ï¼šé€šè¿‡ ESM-2 æä¾›çš„ä¸Šä¸‹æ–‡åŒ–è¡¨ç¤ºï¼Œç»“åˆå¤šå°ºåº¦ç‰¹å¾æå–ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–ä»»ä½•3Dç»“æ„çš„æƒ…å†µä¸‹è¾¾åˆ°ç”šè‡³è¶…è¶Šç»“æ„è¾…åŠ©æ¨¡å‹çš„æ€§èƒ½ã€‚
- **å¤šå°ºåº¦å»ºæ¨¡è‡³å…³é‡è¦**ï¼šåŒæ—¶æ•æ‰**å…¨å±€ä¸Šä¸‹æ–‡**ï¼ˆvia Transformerï¼‰å’Œ**å±€éƒ¨æ¨¡ä½“**ï¼ˆvia CNNï¼‰æ˜¯æå‡é¢„æµ‹å‡†ç¡®ç‡çš„å…³é”®ã€‚
- **åŒæµæ¶æ„ä¼˜äºå•ä¸€æµæ¶æ„**ï¼šå®éªŒè¯æ˜ï¼ŒTransformer ä¸ CNN çš„ååŒä½œç”¨å¸¦æ¥äº†æ˜¾è‘—å¢ç›Šï¼ŒéªŒè¯äº†â€œ**global + local = better binding signature**â€çš„è®¾è®¡ç†å¿µã€‚
- **åºåˆ—ä¼˜å…ˆèŒƒå¼æ­£åœ¨é€¼è¿‘ç»“æ„æ¨¡å‹æ€§èƒ½**ï¼šæœ¬ç ”ç©¶è¡¨æ˜ï¼Œéšç€ PLM å’Œæ·±åº¦å­¦ä¹ æ¶æ„çš„å‘å±•ï¼Œ**sequence-only æ–¹æ³•å·²æˆä¸ºç»“æ„æ–¹æ³•çš„å¼ºå¤§æ›¿ä»£æ–¹æ¡ˆ**ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ¨¡å‹æœªæ˜¾å¼å»ºæ¨¡æŠ—åŸä¸æŠ—ä½“ä¹‹é—´çš„æ®‹åŸºçº§äº¤äº’ï¼ˆå¦‚cross-attentionæœºåˆ¶ï¼‰ï¼Œå¯èƒ½é—æ¼ç»†ç²’åº¦ç›¸äº’ä½œç”¨ä¿¡å·ã€‚
- æ‰€æœ‰å®éªŒåŸºäº pKd å›å½’ä»»åŠ¡ï¼Œå¯¹äºçªå˜æ•ˆåº”æˆ–ç›¸å¯¹äº²å’ŒåŠ›æ’åºä»»åŠ¡å°šæœªå……åˆ†éªŒè¯ã€‚
- æ¨¡å‹è§£é‡Šæ€§æœ‰é™ï¼Œç¼ºä¹å¯è§†åŒ–å·¥å…·æ­ç¤ºå…³é”®ç»“åˆæ®‹åŸºæˆ–é©±åŠ¨å› ç´ ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼•å…¥ **Cross-Attention æœºåˆ¶**ï¼Œå¢å¼ºæŠ—åŸ-æŠ—ä½“é—´çš„äº¤äº’å»ºæ¨¡ã€‚
- é‡‡ç”¨ **pairwise ranking loss** æ›¿ä»£å›å½’æŸå¤±ï¼Œæå‡å¯¹å®éªŒå™ªå£°çš„é²æ£’æ€§ã€‚
- å¼€å‘ **å¯è§£é‡Šæ€§æ¨¡å—**ï¼Œç”Ÿæˆæ®‹åŸºçº§åˆ«çš„é‡è¦æ€§çƒ­å›¾ï¼ˆinteraction mapsï¼‰ï¼Œè¾…åŠ©æŠ—ä½“å·¥ç¨‹ä¼˜åŒ–ã€‚
- æ‰©å±•è‡³ **neutralization potency prediction** æˆ– **variant escape prediction** ç­‰ä¸´åºŠç›¸å…³ä»»åŠ¡ã€‚

---

> âœ… æ€»ç»“ï¼š  
> **DuaDeep-SeqAffinity æˆåŠŸè¯æ˜äº†â€œä»åºåˆ—åˆ°åŠŸèƒ½â€çš„å¯è¡Œæ€§ï¼Œåœ¨æ— éœ€3Dç»“æ„çš„å‰æä¸‹å®ç°äº†SOTAçº§åˆ«çš„æŠ—åŸ-æŠ—ä½“äº²å’ŒåŠ›é¢„æµ‹ã€‚è¯¥å·¥ä½œä¸ºé«˜é€šé‡æŠ—ä½“å‘ç°ã€ä¸ªæ€§åŒ–å…ç–«æ²»ç–—å’Œå¿«é€Ÿå“åº”ç–«æƒ…æä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•çš„AIè§£å†³æ–¹æ¡ˆã€‚**

</details>

---

### 12. [Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco](https://arxiv.org/abs/2512.21837)

**Authors**: Siyu Li, Chenwei Song, Wan Zhou, Xinyi Liu  
**Category**: cs.CL  
**Published**: 2025-12-29  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.21837v1  

#### Abstract
This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
è¯¥ç ”ç©¶é’ˆå¯¹çƒŸè‰ç—…è™«å®³é˜²æ²»é¢†åŸŸä¸­ï¼Œ**å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿›è¡ŒçŸ¥è¯†æ¨ç†æ—¶éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ç»“æ„åŒ–å…³ç³»ä¿¡æ¯**çš„é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚ Prompt Engineering æˆ–ç®€å•çš„çŸ¥è¯†æ³¨å…¥å¾€å¾€å¿½ç•¥äº†å®ä½“ä¹‹é—´çš„å¤æ‚å…³è”ï¼ˆå¦‚â€œç–¾ç—…-ç—‡çŠ¶-é˜²æ²»æ–¹æ³•â€ä¹‹é—´çš„å¤šè·³å…³ç³»ï¼‰ï¼Œå¯¼è‡´æ¨ç†èƒ½åŠ›å—é™ï¼Œå°¤å…¶åœ¨å¤šè·³æ¨ç†ï¼ˆmulti-hop reasoningï¼‰å’Œæ¯”è¾ƒæ¨ç†ï¼ˆcomparative reasoningï¼‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•/æ–°æ€è·¯
æå‡ºäº†ä¸€ç§åŸºäº **GraphRAG æ¡†æ¶**çš„å›¾å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œå°†å›¾ç»“æ„ä¿¡æ¯æ·±åº¦èåˆåˆ° LLM çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå…·ä½“åŒ…æ‹¬ï¼š

- åˆ©ç”¨ LLM è¾…åŠ©æ„å»ºä¸€ä¸ª**çƒŸè‰ç—…è™«å®³é¢†åŸŸçš„çŸ¥è¯†å›¾è°±ï¼ˆKnowledge Graphï¼‰**ï¼Œæ¶µç›–ç–¾ç—…ã€ç—‡çŠ¶ã€é˜²æ²»æ–¹æ³•ç­‰å®ä½“åŠå…¶å…³ç³»ã€‚
- å¼•å…¥ **TransE è¿›è¡Œå›¾åµŒå…¥å­¦ä¹ ï¼ˆGraph Embeddingï¼‰**ï¼Œæ•æ‰å®ä½“ä¸å…³ç³»çš„è¯­ä¹‰è¡¨ç¤ºã€‚
- ä½¿ç”¨ **å›¾ç¥ç»ç½‘ç»œï¼ˆGCNï¼‰** å¯¹èŠ‚ç‚¹è¿›è¡Œé«˜é˜¶é‚»åŸŸèšåˆï¼Œç”Ÿæˆæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„èŠ‚ç‚¹è¡¨ç¤ºã€‚
- å°† GCN å­¦ä¹ åˆ°çš„å›¾è¡¨ç¤ºä¸ **ChatGLM** çš„æ–‡æœ¬è¾“å…¥è¿›è¡Œèåˆï¼Œå½¢æˆå¢å¼ºçš„ä¸Šä¸‹æ–‡è¾“å…¥ï¼ŒæŒ‡å¯¼ LLM æ›´å‡†ç¡®åœ°ç”Ÿæˆç­”æ¡ˆã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | å±€é™æ€§ | æœ¬æ–‡ä¼˜åŠ¿ |
|------|--------|---------|
| çº¯ LLMï¼ˆå¦‚ ChatGLMï¼‰ | ç¼ºä¹å¤–éƒ¨çŸ¥è¯†æ”¯æŒï¼Œæ˜“äº§ç”Ÿå¹»è§‰ | æ˜¾å¼å¼•å…¥ç»“æ„åŒ–çŸ¥è¯†ï¼Œæå‡å‡†ç¡®æ€§ |
| KGE + LLM | ä»…ä½¿ç”¨æ‰å¹³åŒ–çš„å®ä½“åµŒå…¥ï¼Œå¿½ç•¥å›¾ç»“æ„ | åˆ©ç”¨ GCN æ•æ‰å±€éƒ¨ä¸å…¨å±€å›¾ç»“æ„ä¿¡æ¯ |
| RAGï¼ˆRetrieval-Augmented Generationï¼‰ | åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œéš¾ä»¥ç†è§£å¤æ‚å…³ç³» | åŸºäºçŸ¥è¯†å›¾è°±ç»“æ„è¿›è¡Œç²¾å‡†çš„çŸ¥è¯†æ£€ç´¢ä¸æ¨ç† |
| ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³• | éš¾ä»¥å¤„ç†è‡ªç„¶è¯­è¨€é—®ç­”ä¸å¤æ‚é€»è¾‘æ¨ç† | ç»“åˆ LLM çš„è¯­ä¹‰ç†è§£èƒ½åŠ›å’Œå›¾ç»“æ„çš„é€»è¾‘è¡¨è¾¾èƒ½åŠ› |

> **æ ¸å¿ƒåˆ›æ–°**ï¼šå®ç°äº† **ç»“æ„åŒ–çŸ¥è¯†ï¼ˆSymbolicï¼‰ä¸ç¥ç»è¯­è¨€æ¨¡å‹ï¼ˆNeuralï¼‰çš„æ·±åº¦ååŒ**ï¼Œé€šè¿‡ GraphRAG å®ç°ä»â€œå±€éƒ¨äº‹å®åŒ¹é…â€å‘â€œå…¨å±€å…³ç³»æ¨ç†â€çš„è·ƒè¿ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- è‡ªå»ºäº†ä¸€ä¸ª**çƒŸè‰ç—…è™«å®³çŸ¥è¯†å›¾è°±**ï¼ŒåŒ…å«è¶…è¿‡ 1,000 ä¸ªå®ä½“å’Œå…³ç³»ã€‚
  - å®ä½“ç±»å‹ï¼šç–¾ç—…ï¼ˆå¦‚ tobacco mosaic diseaseï¼‰ã€ç—‡çŠ¶ã€é˜²æ²»æ–¹æ³•ï¼ˆå¦‚ spraying antiviral agentsï¼‰ã€å†œè¯æˆåˆ†ã€è‡´ç—…å› ç´ ç­‰ã€‚
  - å…³ç³»ç±»å‹ï¼š`treated_by`, `prevented_by`, `caused_by` ç­‰ã€‚
- æ„å»ºäº†é…å¥—çš„**é—®ç­”æ•°æ®é›†**ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…å«ä¸‰ç±»ä»»åŠ¡ï¼š
  1. **ç›´æ¥é—®ç­”ï¼ˆDirect QAï¼‰**
  2. **å¤šè·³æ¨ç†ï¼ˆMulti-hop Reasoningï¼‰**
  3. **æ¯”è¾ƒæ¨ç†ï¼ˆComparative Reasoningï¼‰**

æ•°æ®æ¥æºï¼šå†œä¸šä¸“å®¶çŸ¥è¯†åº“ã€ç—…å®³ç®¡ç†æ‰‹å†Œã€å­¦æœ¯æ–‡çŒ®ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **åŸºç¡€ LLM**ï¼šé‡‡ç”¨ **ChatGLM** ä½œä¸ºä¸»å¹²æ¨¡å‹ã€‚
- **å‚æ•°é«˜æ•ˆå¾®è°ƒ**ï¼šä½¿ç”¨ **LoRA**ï¼ˆLow-Rank Adaptationï¼‰è¿›è¡Œå¾®è°ƒï¼ŒLoRA rank è®¾ä¸º 16ã€‚
- **å›¾è¡¨ç¤ºå­¦ä¹ **ï¼š
  - å›¾åµŒå…¥ï¼š**TransE**ï¼Œç»´åº¦ 100ï¼Œå­¦ä¹ ç‡ 0.01ã€‚
  - èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ ï¼šä¸¤å±‚ **GCN**ï¼ŒReLU æ¿€æ´»å‡½æ•°ã€‚
- **èåˆæœºåˆ¶**ï¼šåœ¨ GraphRAG æ¡†æ¶ä¸‹ï¼Œå°† GCN è¾“å‡ºçš„å›¾åµŒå…¥ä¸ Sentence-BERT ç”Ÿæˆçš„æŸ¥è¯¢åµŒå…¥æ‹¼æ¥åè¾“å…¥ ChatGLMã€‚

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
é‡‡ç”¨æ ‡å‡†åˆ†ç±»ä¸é—®ç­”è¯„ä»·æŒ‡æ ‡ï¼š
- **Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰**
- **Precisionï¼ˆç²¾ç¡®ç‡ï¼‰**
- **Recallï¼ˆå¬å›ç‡ï¼‰**
- **F1-scoreï¼ˆF1 åˆ†æ•°ï¼‰**

æµ‹è¯•é›†ä¸­åœ¨çŸ¥è¯†å¯†é›†å‹æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿æ¨¡å‹ | æè¿° |
|--------|------|
| **ChatGLM** | ä¸ä¾èµ–ä»»ä½•å¤–éƒ¨çŸ¥è¯†çš„åŸå§‹ LLM |
| **TransE + ChatGLM** | ä½¿ç”¨ TransE åµŒå…¥æ³¨å…¥çŸ¥è¯† |
| **KGE + ChatGLM** | åŸºäºçŸ¥è¯†å›¾è°±åµŒå…¥çš„æ–¹æ³• |
| **RAG + ChatGLM** | åŸºäºæ–‡æœ¬æ£€ç´¢çš„æ ‡å‡† RAG æ–¹æ³• |
| **GraphRAG + ChatGLM** | æœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼ˆGCN + TransE + LLM èåˆï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1ï¼‰
| Model | Accuracy (%) | Precision (%) | Recall (%) | F1-score (%) |
|-------|--------------|---------------|------------|---------------|
| ChatGLM | 75.2 | 78.5 | 72.1 | 75.2 |
| KGE + ChatGLM | 82.5 | 85.3 | 79.8 | 82.4 |
| RAG + ChatGLM | 85.7 | 87.9 | 83.2 | 85.5 |
| **GraphRAG + ChatGLM** | **90.1** | **92.3** | **88.2** | **90.2** |

> âœ… æ‰€æœ‰æŒ‡æ ‡å‡æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ **Accuracy å’Œ F1-score ä¸Šæå‡æ˜æ˜¾**ã€‚

### ğŸ” ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”åˆ†æ
- ç›¸æ¯”çº¯ **ChatGLM**ï¼Œå¼•å…¥ç»“æ„åŒ–çŸ¥è¯†ä½¿ Accuracy æå‡è¿‘ **15 ä¸ªç™¾åˆ†ç‚¹**ï¼Œè¯´æ˜å¤–éƒ¨çŸ¥è¯†å¯¹å†œä¸šé¢†åŸŸæ¨ç†è‡³å…³é‡è¦ã€‚
- **KGE + ChatGLM** è¡¨ç°ä¼˜äºçº¯ LLMï¼Œä½†å¼±äº RAG å’Œ GraphRAGï¼Œè¡¨æ˜ä»…é åµŒå…¥æ— æ³•å……åˆ†å»ºæ¨¡å¤æ‚å…³ç³»ã€‚
- **RAG + ChatGLM** è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ¶‰åŠå¤šè·³æˆ–æ¯”è¾ƒé€»è¾‘çš„é—®é¢˜ä¸Šå®¹æ˜“è¯¯æ£€æ— å…³æ–‡æœ¬ç‰‡æ®µã€‚
- **GraphRAG + ChatGLM** åœ¨ä»¥ä¸‹åœºæ™¯è¡¨ç°å‡ºæ›´å¼ºæ¨ç†èƒ½åŠ›ï¼š
  - å¤šè·³é—®é¢˜ï¼šâ€œå¦‚ä½•é€šè¿‡ç»¼åˆæªæ–½æ§åˆ¶ç”±èšœè™«ä¼ æ’­çš„çƒŸè‰èŠ±å¶ç—…ï¼Ÿâ€
  - æ¯”è¾ƒé—®é¢˜ï¼šâ€œç”Ÿç‰©é˜²æ²»ä¸åŒ–å­¦é˜²æ²»å“ªç§æ›´é€‚åˆé•¿æœŸé˜²æ§ï¼Ÿâ€

### ğŸ”§ æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰åˆ†æï¼ˆæ–‡ä¸­è™½æœªåˆ—è¯¦ç»†è¡¨æ ¼ï¼Œä½†æœ‰è®¨è®ºï¼‰
- ç§»é™¤ GCN åæ€§èƒ½ä¸‹é™æ˜æ˜¾ â†’ è¯æ˜ **GCN å¯¹æ•è·é«˜é˜¶é‚»å±…ä¿¡æ¯è‡³å…³é‡è¦**ã€‚
- è‹¥ä»…ä½¿ç”¨ TransE è€Œä¸æ¥å…¥ GCN â†’ æ¨ç†æ·±åº¦ä¸è¶³ï¼Œå°¤å…¶å½±å“ multi-hop ä»»åŠ¡ã€‚
- è‹¥ä¸èåˆå›¾åµŒå…¥ â†’ å›é€€è‡³æ™®é€š RAG æˆ– KGE æ°´å¹³ï¼ŒéªŒè¯äº†**å›¾-LLM èåˆæœºåˆ¶çš„æœ‰æ•ˆæ€§**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å›¾ç»“æ„ä¿¡æ¯èƒ½æ˜¾è‘—å¢å¼º LLM åœ¨å†œä¸šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›**ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é€»è¾‘é“¾æ¨å¯¼çš„ä»»åŠ¡ä¸­ã€‚
2. **GCN æ¯”å•çº¯çš„ KGE æ›´é€‚åˆå»ºæ¨¡å†œä¸šçŸ¥è¯†å›¾è°±ä¸­çš„å¤æ‚å…³ç³»ç½‘ç»œ**ï¼Œèƒ½å¤Ÿèšåˆå¤šå±‚æ¬¡ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
3. **GraphRAG æ¡†æ¶ä¸ºç»“æ„åŒ–çŸ¥è¯†ä¸ LLM çš„èåˆæä¾›äº†æœ‰æ•ˆè·¯å¾„**ï¼Œå®ç°äº†ä»â€œæ£€ç´¢â€åˆ°â€œæ¨ç†â€çš„å‡çº§ã€‚
4. è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äº**å¤šè·³æ¨ç†**å’Œ**æ¯”è¾ƒå‹å†³ç­–æ”¯æŒä»»åŠ¡**ï¼Œå…·å¤‡å®é™…åº”ç”¨æ½œåŠ›ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. å½“å‰æ„å»ºçš„çŸ¥è¯†å›¾è°±è§„æ¨¡è¾ƒå°ï¼ˆçº¦ 1,000 æ¡ä¸‰å…ƒç»„ï¼‰ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
2. ä½¿ç”¨å›ºå®šçš„å›¾åµŒå…¥å’Œç‰¹å®šçš„ GCN æ¶æ„ï¼Œç¼ºä¹åŠ¨æ€æ›´æ–°ä¸è‡ªé€‚åº”ä¼˜åŒ–æœºåˆ¶ã€‚
3. å®éªŒé›†ä¸­äºä¸­æ–‡çƒŸè‰é¢†åŸŸï¼Œè·¨ä½œç‰©ã€è·¨è¯­è¨€çš„å¯è¿ç§»æ€§æœ‰å¾…éªŒè¯ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. æ„å»ºæ›´å¤§è§„æ¨¡ã€è¦†ç›–æ›´å¤šä½œç‰©çš„å†œä¸šçŸ¥è¯†å›¾è°±ã€‚
2. æ¢ç´¢æ›´å…ˆè¿›çš„ GNN æ¶æ„ï¼ˆå¦‚ GATã€Temporal GNNï¼‰ä»¥å»ºæ¨¡åŠ¨æ€ç—…å®³æ¼”åŒ–è¿‡ç¨‹ã€‚
3. è®¾è®¡æ›´é«˜æ•ˆçš„å›¾-LLM èåˆç­–ç•¥ï¼ˆå¦‚ä¸­é—´å±‚æ³¨å…¥ã€æ³¨æ„åŠ›å¼•å¯¼ï¼‰ã€‚
4. å°†æ¡†æ¶æ‰©å±•è‡³å…¶ä»–çŸ¥è¯†å¯†é›†å‹é¢†åŸŸï¼Œå¦‚ **Healthcareã€Intelligent Decision-Making Systems**ã€‚

---

## æ€»ç»“
æœ¬è®ºæ–‡æˆåŠŸå°† **GraphRAG æ¡†æ¶åº”ç”¨äºçƒŸè‰ç—…è™«å®³é˜²æ²»é¢†åŸŸ**ï¼Œé€šè¿‡æ•´åˆ **TransEã€GCN ä¸ ChatGLM**ï¼Œå®ç°ç»“æ„åŒ–çŸ¥è¯†ä¸è¯­è¨€æ¨¡å‹çš„æ·±åº¦èåˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ Accuracyã€F1-score ç­‰å…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—è¶…è¶Šå¤šç§åŸºçº¿ï¼Œåœ¨å¤šè·³ä¸æ¯”è¾ƒæ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œä¸ºæ™ºèƒ½å†œä¸šå†³ç­–ç³»ç»Ÿæä¾›äº†ä¸€æ¡å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚

</details>

---

### 13. [Demystifying ARM SME to Optimize General Matrix Multiplications](https://arxiv.org/abs/2512.21473)

**Authors**: Chencheng Deng, Weiling Yang, Jianbin Fang, Dezun Dong  
**Category**: cs.DC  
**Published**: 2025-12-29  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.21473v1  

#### Abstract
General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Demystifying ARM SME to Optimize General Matrix Multiplications è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç°ä»£ ARM æ¶æ„å¼•å…¥äº† **Scalable Matrix Extension (SME)** æ¥åŠ é€ŸçŸ©é˜µè¿ç®—ï¼Œç‰¹åˆ«æ˜¯ **General Matrix Multiplication (GEMM)**ï¼Œè¿™æ˜¯é«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰å’Œæ·±åº¦å­¦ä¹ ä¸­çš„æ ¸å¿ƒæ“ä½œã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼€æºçº¿æ€§ä»£æ•°åº“ï¼ˆå¦‚ LIBXSMMã€OpenBLASã€KleidiAIï¼‰æœªèƒ½å……åˆ†æŒ–æ˜ SME åœ¨å¤„ç†å¤§è§„æ¨¡çŸ©é˜µæ—¶çš„æ½œåŠ›ï¼Œå­˜åœ¨ä»¥ä¸‹å…³é”®ç“¶é¢ˆï¼š

- **Cache åˆ©ç”¨æ•ˆç‡ä½**ï¼šç®€å•ä¸‰å±‚å¾ªç¯è®¾è®¡å¯¼è‡´å­å—å¤§å°ä¸å…±äº« L2 Cache ä¸åŒ¹é…ï¼Œé€ æˆå¤§é‡ Cache Miss å’Œ TLB Missã€‚
- **å†…å­˜å¸¦å®½æœªå……åˆ†åˆ©ç”¨**ï¼šç°æœ‰å®ç°å¤šé‡‡ç”¨å• Z å¯„å­˜å™¨åŠ è½½ï¼Œæ— æ³•å‘æŒ¥ SME2 æ”¯æŒçš„å›› Z å¯„å­˜å™¨å¹¶è¡ŒåŠ è½½é«˜è¾¾ 900 GB/s çš„å¸¦å®½ä¼˜åŠ¿ã€‚
- **ä»…æ‰“åŒ…ä¸€ä¸ªè¾“å…¥çŸ©é˜µ**ï¼šä¸ºå‡å°‘å¼€é”€ï¼Œå¤šæ•°åº“åªå¯¹ä¸€ä¸ªè¾“å…¥çŸ©é˜µè¿›è¡Œé¢„æ‰“åŒ…ï¼Œå½“å¦ä¸€ä¸ªå¤§çŸ©é˜µè¶…å‡º L2 Cache å®¹é‡æ—¶ï¼Œè®¿é—®å˜å¾—ä¸è¿ç»­ä¸”ä½æ•ˆã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹
æœ¬æ–‡æå‡ºäº† **MPGEMM** â€”â€” ä¸€ä¸ªé¢å‘ ARM SME æ¶æ„çš„å¼€æºé«˜æ€§èƒ½å¤šç²¾åº¦ GEMM åº“ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### âœ… 1. **Cache-Aware åˆ†å—ç®—æ³•è®¾è®¡**
- å°†ä¼ ç»Ÿçš„ä¸‰é‡å¾ªç¯é‡æ„ä¸º **å…­çº§åˆ†å—ç®—æ³•**ï¼ˆsix-level blocked algorithmï¼‰ï¼Œé€šè¿‡è§£ææ¨¡å‹æŒ‡å¯¼ `mc`, `nc`, `kc` ç­‰åˆ†å—å‚æ•°çš„é€‰æ‹©ã€‚
- è€ƒè™‘ L2 Cache å¤§å°ï¼ˆ8MBï¼‰ã€TLB å®¹é‡å’Œå†…å­˜å¸¦å®½é™åˆ¶ï¼Œæœ€å¤§åŒ–ç¼“å­˜å±€éƒ¨æ€§å’Œè®¡ç®—è®¿å­˜æ¯”ï¼ˆCMRï¼‰ã€‚
- æ˜¾è‘—æå‡æ•°æ®åœ¨ L2 Cache ä¸­çš„é©»ç•™æ—¶é—´ï¼Œå‡å°‘ Cache/TLB Missã€‚

#### âœ… 2. **é«˜æ•ˆçš„åŒçŸ©é˜µæ‰“åŒ…ç­–ç•¥**
- **On-the-fly Transposition for Matrix A**ï¼šåˆ©ç”¨ SME çš„ ZA tile å­˜å‚¨ç»“æ„ï¼Œåœ¨åŠ è½½è¿‡ç¨‹ä¸­é€šè¿‡æ°´å¹³å†™å…¥ã€å‚ç›´åˆ‡ç‰‡çš„æ–¹å¼å®ç°é›¶å¼€é”€è½¬ç½®ï¼Œé¿å… gather æ“ä½œã€‚
- **First-Round Online Packing for Matrix B**ï¼šåœ¨ç¬¬ä¸€è½®è®¡ç®—ä¸­è¾¹æ‰§è¡Œ FMOPA è¾¹æ‰“åŒ… B çŸ©é˜µï¼Œå°†æ‰“åŒ…å»¶è¿Ÿéšè—åœ¨è®¡ç®—æµæ°´çº¿ä¸­ï¼Œé™ä½æ€»å†…å­˜å¼€é”€ã€‚

#### âœ… 3. **é«˜ååå¾®å†…æ ¸è®¾è®¡**
- å¾®å†…æ ¸å§‹ç»ˆä½¿ç”¨ **å››ä¸ª Z å¯„å­˜å™¨ç»„ï¼ˆfour-Z-register loadsï¼‰** è¿›è¡Œæ•°æ®åŠ è½½ï¼Œä»¥è¾¾åˆ°æ¥è¿‘ 900 GB/s çš„å³°å€¼å†…å­˜å¸¦å®½ã€‚
- å……åˆ†åˆ©ç”¨æ‰€æœ‰å¯ç”¨çš„ **ZA Tiles**ï¼ˆä¾‹å¦‚ FP32 ä¸‹ä½¿ç”¨å…¨éƒ¨ 4 ä¸ª ZA.S tilesï¼‰ï¼Œç¡®ä¿è®¡ç®—å•å…ƒé¥±å’Œã€‚
- è®¾è®¡ä¸»å¾®å†…æ ¸ï¼ˆmain kernelï¼‰ä¸º 16Ã—64 å½¢çŠ¶ï¼Œå¹¶é…åˆè¾¹ç¼˜å¾®å†…æ ¸ï¼ˆedge kernelï¼‰å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œä¿æŒé«˜æ€§èƒ½ã€‚

#### âœ… 4. **æ”¯æŒæ··åˆç²¾åº¦ GEMM**
- æ‰©å±•è‡³ **FP16â†’FP32** å’Œ **INT8â†’INT32** ç­‰æ··åˆç²¾åº¦æ¨¡å¼ï¼Œåˆ©ç”¨ SME åŸç”Ÿæ”¯æŒçš„ä½ç²¾åº¦è¾“å…¥ã€é«˜ç²¾åº¦ç´¯åŠ ç‰¹æ€§ã€‚
- é’ˆå¯¹ FP16 æ•°æ®è®¾è®¡äº†åŸºäº ZIP æŒ‡ä»¤çš„å‚ç›´äº¤é”™æ‰“åŒ…æ–¹æ³•ï¼Œä¿è¯é«˜æ•ˆå‘é‡åŒ–è®¿é—®ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | MPGEMM | LIBXSMM / OpenBLAS / KleidiAI |
|------|--------|-------------------------------|
| åˆ†å—ç­–ç•¥ | å…­çº§åˆ†å—ï¼ŒCache-aware | ä¸‰çº§å¾ªç¯ï¼Œå›ºå®šåˆ†å— |
| æ•°æ®æ‰“åŒ… | åŒçŸ©é˜µæ‰“åŒ… + åœ¨çº¿æ‰“åŒ… | å•çŸ©é˜µæ‰“åŒ…æˆ–å…¨é¢„æ‰“åŒ… |
| å†…å­˜åŠ è½½ç²’åº¦ | å›› Z å¯„å­˜å™¨ç»„åŠ è½½ï¼ˆ~900 GB/sï¼‰ | å•/åŒå¯„å­˜å™¨åŠ è½½ï¼ˆâ‰¤460 GB/sï¼‰ |
| ZA Tile åˆ©ç”¨ç‡ | å…¨éƒ¨ä½¿ç”¨ï¼ˆ4 tiles for FP32ï¼‰ | è¾¹ç•Œå¸¸é™ä¸º 2 tiles |
| å¹¶è¡ŒåŒ–æ”¯æŒ | å¤š SME å•å…ƒä»»åŠ¡åŠ¨æ€åˆ†é… | å¤šæ•°æ— å¹¶è¡Œæ”¯æŒ |
| å¼€æºé€æ˜æ€§ | å®Œå…¨å¼€æº | Accelerate é—­æº |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### å®éªŒå¹³å°
- **ç¡¬ä»¶**ï¼šApple M4 Pro èŠ¯ç‰‡
  - P-core é›†ç¾¤ï¼š10 æ ¸ï¼Œ4.4GHzï¼Œæ¯é›†ç¾¤å…±äº« 16MB L2 Cache å’Œä¸€ä¸ª SME å•å…ƒ
  - E-core é›†ç¾¤ï¼š4 æ ¸ï¼Œ2.85GHzï¼Œå…±äº« 4MB L2 Cache å’Œä¸€ä¸ª SME å•å…ƒ
  - Streaming SVL = 512 bits â†’ Z å¯„å­˜å™¨ = 64 å­—èŠ‚ï¼ŒZA å­˜å‚¨ = 64Ã—64 bytes
- **æ“ä½œç³»ç»Ÿ**ï¼šmacOS 15.1
- **ç¼–è¯‘å™¨**ï¼šAppleClang 16

### å·¥ä½œè´Ÿè½½ï¼ˆWorkloadsï¼‰
æ¥è‡ªçœŸå®å¤§æ¨¡å‹çš„ GEMM é…ç½®ï¼š
- **DeepSeek æ¨¡å‹**ï¼ˆID 1â€“18ï¼‰ï¼šæ¶µç›–ä¸åŒè§„æ¨¡çš„å¤§çŸ©é˜µä¹˜æ³•
- **LLaMA æ¨¡å‹**ï¼ˆID 19â€“24ï¼‰ï¼šåŒ…å«â€œç˜¦é•¿â€ï¼ˆskinnyï¼‰çŸ©é˜µåœºæ™¯
- åŒ…æ‹¬è§„åˆ™å°ºå¯¸å’Œéæ•´é™¤å¾®å†…æ ¸å°ºå¯¸çš„ **ä¸è§„åˆ™çŸ©é˜µ** æµ‹è¯•ï¼ˆM/N âˆˆ {80,110,140,170,200}, K=25600ï¼‰

### è¯„ä¼°æŒ‡æ ‡
- **æ€§èƒ½æŒ‡æ ‡**ï¼šGFLOPSï¼ˆå•ç²¾åº¦ï¼‰ã€GOPSï¼ˆæ•´å‹ï¼‰ã€å®æµ‹å†…å­˜å¸¦å®½
- **å¯¹æ¯”æ–¹å¼**ï¼šå‡ ä½•å¹³å‡é€Ÿåº¦æå‡ï¼ˆGeometric Mean Speedupï¼‰
- **é‡å¤æ¬¡æ•°**ï¼šæ¯ä¸ªå®éªŒè¿è¡Œ 5 æ¬¡å–ç®—æœ¯å‡å€¼ï¼Œå«é¢„çƒ­è½®æ¬¡

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åº“å | æ˜¯å¦å¼€æº | SME æ”¯æŒ | ç²¾åº¦æ”¯æŒ | å¹¶è¡Œèƒ½åŠ› |
|------|----------|-----------|------------|-------------|
| **Apple Accelerate** | âŒ é—­æº | âœ… SME2 | FP32/FP64 | âœ… å¤š SME |
| **LIBXSMM** | âœ… å¼€æº | âœ… SME2 | FP32/small GEMM | âŒ |
| **OpenBLAS** | âœ… å¼€æº | âœ… SMEï¼ˆé SME2 å‘é‡æŒ‡ä»¤ï¼‰ | FP32 | âŒ |
| **KleidiAI** | âœ… å¼€æº | âœ… SME2 | FP32/BF16/INT8 | âŒ |

> æ³¨ï¼šBLIS ç­‰å…¶ä»–åº“å› ç¼ºä¹ SME æ”¯æŒè¢«æ’é™¤ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å• SME å•å…ƒæ€§èƒ½ï¼ˆFP32ï¼‰
- **ç›¸æ¯”å¼€æºåº“æ˜¾è‘—é¢†å…ˆ**ï¼š
  - æ¯” **LIBXSMM** å¿« **1.95Ã—**
  - æ¯” **KleidiAI** å¿« **2.34Ã—**
  - æ¯” **OpenBLAS** å¿« **2.85Ã—**
- **è¶…è¶Šå‚å•†ä¼˜åŒ–åº“**ï¼š
  - åœ¨è¡Œä¼˜å…ˆå­˜å‚¨ä¸‹æ¯” **Accelerate** å¿« **1.21Ã—**
  - åœ¨åˆ—ä¼˜å…ˆå­˜å‚¨ä¸‹å¿« **1.18Ã—**

### å¤š SME å•å…ƒå¹¶è¡Œæ€§èƒ½
- åŠ¨æ€è°ƒåº¦ä»»åŠ¡åˆ°å¤šä¸ª SME å•å…ƒï¼Œå®ç°è¿‘ä¸¤å€åŠ é€Ÿã€‚
- ç›¸æ¯” Accelerateï¼š
  - è¡Œä¼˜å…ˆï¼š**1.24Ã—**
  - åˆ—ä¼˜å…ˆï¼š**1.22Ã—**
- ç›¸æ¯”æ— å¹¶è¡Œèƒ½åŠ›çš„å¼€æºåº“ï¼š
  - æ¯” **LIBXSMM** å¿« **3.96Ã—**
  - æ¯” **KleidiAI** å¿« **4.69Ã—**
  - æ¯” **OpenBLAS** å¿« **5.7Ã—**

### åŒç²¾åº¦ GEMMï¼ˆFP64ï¼‰
- å°½ç®¡ SME ä¸Š FP64 ååä»…ä¸º FP32 çš„ 1/4ï¼ˆå³°å€¼ ~501 GFLOPSï¼‰ï¼ŒMPGEMM ä»èƒ½åœ¨å¤šæ ¸ä¸‹å®ç°æ¯” Accelerate **1.18Ã—** çš„åŠ é€Ÿã€‚

### ä¸è§„åˆ™çŸ©é˜µæ€§èƒ½
- ä½¿ç”¨ predication å¯„å­˜å™¨å¤„ç†éæ•´é™¤å°ºå¯¸ã€‚
- MPGEMM å‡­å€Ÿä¼˜åŒ–çš„è¾¹ç¼˜å¾®å†…æ ¸ï¼ˆ64Ã—16 / 16Ã—64ï¼‰å’Œé«˜æ•ˆæ‰“åŒ…ï¼Œåœ¨ M/N ä¸æ˜¯ tile æ•´æ•°å€çš„æƒ…å†µä¸‹ä¾ç„¶ç¨³å®šé¢†å…ˆã€‚

### æ··åˆç²¾åº¦ GEMM æ€§èƒ½
- **FP16â†’FP32 GEMM**ï¼š
  - æ¯” KleidiAIï¼ˆBF16ï¼‰å¿« **1.7Ã—**
  - æ›´é«˜çš„ compute-to-memory ratio å¸¦æ¥å®é™…æ€§èƒ½å¢ç›Š
- **INT8â†’INT32 GEMM**ï¼š
  - å³°å€¼å¯è¾¾ **4010 GOPS**ï¼ˆå•çº¿ç¨‹ï¼‰
  - å®æµ‹è¾¾åˆ° **7856 GOPS**ï¼ˆID 14ï¼‰ï¼Œçº¦ä¸ºç†è®ºå³°å€¼çš„ **94%**
  - æ¯” FP16 åŸºçº¿å¿«çº¦ **2Ã—**

### æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰
åœ¨å• SME å•å…ƒä¸Šä»¥ LIBXSMM ä¸ºåŸºçº¿é€æ­¥æ·»åŠ ä¼˜åŒ–ï¼š
| ä¼˜åŒ–é˜¶æ®µ | å¹³å‡æé€Ÿ |
|---------|----------|
| åŸºçº¿ï¼ˆLIBXSMMï¼‰ | 1.0Ã— |
| + Multi-vector loadï¼ˆå››å¯„å­˜å™¨åŠ è½½ï¼‰ | **1.17Ã—** |
| + Partitioning & Dual Packingï¼ˆåˆ†å—+åŒæ‰“åŒ…ï¼‰ | **1.62Ã—** |
| + First-round Online Packing | ç»¼åˆè¾¾ **1.95Ã—** |

> ç»“è®ºï¼š**åˆ†å—ä¸åŒæ‰“åŒ…** æ˜¯æœ€å¤§è´¡çŒ®å› ç´ ï¼›**åœ¨çº¿æ‰“åŒ…** æ•ˆæœæœ‰é™ï¼ˆå›  FMOPA å‘¨æœŸä¸è¶³ä»¥å®Œå…¨æ©ç›–å†™å›å»¶è¿Ÿï¼‰ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **SME æ€§èƒ½é«˜åº¦ä¾èµ– Cache å’Œå†…å­˜è®¿é—®æ¨¡å¼**ï¼š
   - å­çŸ©é˜µå¿…é¡»æ§åˆ¶åœ¨ **8MB ä»¥å†…** æ‰èƒ½ç»´æŒé«˜å¸¦å®½ï¼ˆ>900 GB/sï¼‰ã€‚
   - æ•°æ®å¸ƒå±€éœ€é€‚é… **multi-vector load** å’Œ **outer product instruction** çš„è®¿å­˜æ¨¡å¼ã€‚

2. **å›› Z å¯„å­˜å™¨åŠ è½½è‡³å…³é‡è¦**ï¼š
   - å•å¯„å­˜å™¨åŠ è½½ä»…æä¾› ~230 GB/sï¼Œè€Œå››å¯„å­˜å™¨ç»„å¯è¾¾åˆ° ~900 GB/sï¼Œå·®è·å·¨å¤§ã€‚
   - å¿…é¡»é‡æ„å¾®å†…æ ¸ä»¥æŒç»­ä½¿ç”¨ `LD1D/4Z` ç±»æŒ‡ä»¤ã€‚

3. **ZA Tile åˆ©ç”¨ç‡å†³å®šè®¡ç®—ååä¸Šé™**ï¼š
   - FP32 ä¸‹åº”åŒæ—¶æ¿€æ´»å…¨éƒ¨ 4 ä¸ª ZA.S tilesã€‚
   - è¾¹ç¼˜å¤„ç†ä¸å½“ä¼šå¯¼è‡´ tile æ•°ä¸‹é™ä¸€åŠï¼Œä¸¥é‡æ‹–ç´¯æ€§èƒ½ã€‚

4. **æ··åˆç²¾åº¦æå…·æ½œåŠ›**ï¼š
   - INT8â†’INT32 å¯è¾¾ **94% å³°å€¼æ€§èƒ½**ï¼Œé€‚åˆä½æ¯”ç‰¹æ¨ç†ã€‚
   - æ›´é«˜çš„è®¡ç®—å¯†åº¦ç¼“è§£å†…å­˜å‹åŠ›ï¼Œå°¤å…¶é€‚ç”¨äºå¤§æ¨¡å‹éƒ¨ç½²ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å¹³å°ä¾èµ–æ€§å¼º**ï¼šå½“å‰å‚æ•°è°ƒä¼˜åŸºäº Apple M4 Pro çš„ L2 Cacheï¼ˆ16MBï¼‰ã€SVLï¼ˆ512bitï¼‰ç­‰ç‰¹æ€§ï¼Œè¿ç§»åˆ°å…¶ä»– SME å¹³å°éœ€é‡æ–°å»ºæ¨¡ã€‚
- **å †å†…å­˜ç®¡ç†è¦æ±‚é«˜**ï¼šéœ€é¿å…æ ˆä¸Šåˆ†é…ç¼“å†²åŒºä»¥é˜² Core ä¸ SME å†…å­˜é¡ºåºå†²çªå¼•å‘ pipeline stallã€‚
- **å°šæœªæ”¯æŒæ‰€æœ‰æ•°æ®ç±»å‹ç»„åˆ**ï¼šå¦‚ BFloat16 æˆ–æ›´ä½æ¯”ç‰¹ï¼ˆINT4ï¼‰æš‚æœªè¦†ç›–ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¨å¹¿åˆ†ææ¨¡å‹è‡³æ›´å¤š SME æ¶æ„èŠ¯ç‰‡ï¼ˆå¦‚ AWS Gravitonã€Ampere Altraï¼‰ã€‚
- æ”¯æŒæ›´å¤šæ··åˆç²¾åº¦æ ¼å¼ï¼ˆå¦‚ FP8ã€INT4ï¼‰åŠç¨€ç–çŸ©é˜µè¿ç®—ã€‚
- ç»“åˆç¼–è¯‘å™¨è‡ªåŠ¨è°ƒä¼˜ï¼ˆAuto-tuningï¼‰è¿›ä¸€æ­¥æå‡è·¨å¹³å°å¯ç§»æ¤æ€§ã€‚
- æ‰©å±•è‡³ Convolutionã€Attention ç­‰æ›´å¤æ‚ç®—å­ä¼˜åŒ–ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> MPGEMM é€šè¿‡æ·±å…¥å‰–æ ARM SME æ¶æ„ç‰¹æ€§ï¼Œæå‡ºäº†ä¸€å¥—ç³»ç»Ÿæ€§çš„ GEMM ä¼˜åŒ–æ¡†æ¶â€”â€”ä» cache-aware åˆ†å—ã€åŒçŸ©é˜µé«˜æ•ˆæ‰“åŒ…åˆ° fully-utilized å¾®å†…æ ¸è®¾è®¡ï¼Œé¦–æ¬¡å®ç°äº†åœ¨å¼€æºåº“ä¸­å…¨é¢é‡Šæ”¾ SME çš„æ€§èƒ½æ½œåŠ›ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡å’Œæ··åˆç²¾åº¦åœºæ™¯ä¸‹å¤§å¹…è¶…è¶Šé—­æºæ–¹æ¡ˆã€‚

</details>

---

### 14. [A Reinforcement Learning Approach to Synthetic Data Generation](https://arxiv.org/abs/2512.21395)

**Authors**: Natalia Espinosa-Dice, Nicholas J. Jackson, Chao Yan, Aaron Lee, Bradley A. Malin  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.21395v1  

#### Abstract
Synthetic data generation (SDG) is a promising approach for enabling data sharing in biomedical studies while preserving patient privacy. Yet, state-of-the-art generative models often require large datasets and complex training procedures, limiting their applicability in small-sample settings. In th...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šA Reinforcement Learning Approach to Synthetic Data Generation

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰ä¸»æµçš„ **Synthetic Data Generation (SDG)** æ–¹æ³•ï¼ˆå¦‚ GANs å’Œ diffusion modelsï¼‰é€šå¸¸ä¾èµ–å¤§è§„æ¨¡è®­ç»ƒæ•°æ®å’Œå¤æ‚çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¯¼è‡´å…¶åœ¨**å°æ ·æœ¬ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†**ä¸Šçš„é€‚ç”¨æ€§å—é™ã€‚æ­¤å¤–ï¼š
- **GANs** å­˜åœ¨è®­ç»ƒä¸ç¨³å®šã€æ˜“å‘ç”Ÿ mode collapse å’Œæ¢¯åº¦æ¶ˆå¤±ç­‰é—®é¢˜ï¼›
- **Diffusion models** è™½ç„¶æ€§èƒ½ä¼˜è¶Šï¼Œä½†è®¡ç®—æˆæœ¬é«˜ã€éœ€è¦å¤§é‡æ•°æ®ï¼Œä¸”å­˜åœ¨**è®°å¿†åŒ–é£é™©**ï¼ˆmemorizationï¼‰ï¼Œå¯èƒ½æ³„éœ²éšç§ã€‚

è¿™äº›é—®é¢˜é™åˆ¶äº†å®ƒä»¬åœ¨çœŸå®ä¸–ç•Œä¸­å°å‹ä¸´åºŠç ”ç©¶ä¸­çš„åº”ç”¨ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šRLSyn
æœ¬æ–‡æå‡º **RLSyn** â€”â€”ä¸€ç§åŸºäº**å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰** çš„æ–°å‹åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œå°† SDG é‡æ–°å»ºæ¨¡ä¸ºä¸€ä¸ª RL ä»»åŠ¡ï¼š
- å°†ç”Ÿæˆå™¨è§†ä¸ºä¸€ä¸ª**éšæœºç­–ç•¥ï¼ˆstochastic policyï¼‰**ï¼Œè¾“å‡ºæ•´ä¸ªæ‚£è€…è®°å½•çš„æ¦‚ç‡åˆ†å¸ƒï¼›
- ä½¿ç”¨ **Proximal Policy Optimization (PPO)** ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼›
- åˆ¤åˆ«å™¨ä»…ä½œä¸º**å¥–åŠ±ä¿¡å·æä¾›è€…**ï¼ˆreward estimatorï¼‰ï¼Œä¸å‚ä¸ç”Ÿæˆå™¨çš„æ¢¯åº¦æ›´æ–°ï¼›
- å¼•å…¥è½»é‡çº§å‡å€¼åŒ¹é…æŸå¤±ï¼ˆmean-matching penaltyï¼‰ä»¥ç¨³å®šè®­ç»ƒã€‚

è¯¥æ–¹æ³•å±äº **inverse RL** èŒƒç•´ï¼Œé€šè¿‡æœ€å¤§åŒ–åˆ¤åˆ«å™¨æä¾›çš„â€œçœŸå®æ€§â€å¥–åŠ±æ¥ä¼˜åŒ–ç”Ÿæˆç­–ç•¥ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹**ï¼šPPO çš„ clipped surrogate objective é¿å…äº† GANs ä¸­å¸¸è§çš„å¯¹æŠ—å¤±è¡¡é—®é¢˜ï¼›
- **æ›´é«˜çš„æ•°æ®æ•ˆç‡**ï¼šåœ¨å°æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°ä¼˜äº GAN å’Œ diffusion æ¨¡å‹ï¼›
- **æ›´å¥½çš„éšç§ä¿æŠ¤æ½œåŠ›**ï¼šé¿å…ç›´æ¥å¤åˆ¶è®­ç»ƒæ•°æ®ï¼Œé™ä½ membership inference æ”»å‡»æˆåŠŸç‡ï¼›
- **æ›´å¼ºçš„å¤šæ ·æ€§ç”Ÿæˆèƒ½åŠ›**ï¼šæ˜¾å¼å»ºæ¨¡è¾“å‡ºåˆ†å¸ƒçš„æ–¹å·®ï¼Œé¼“åŠ±å¤šæ ·åŒ–é‡‡æ ·ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
| æ•°æ®é›† | æè¿° |
|-------|------|
| **AI-READI** | å°è§„æ¨¡ç³–å°¿ç—…é˜Ÿåˆ—æ•°æ®ï¼ŒåŒ…å« 1067 åæ‚£è€…ï¼Œå…± 4,537 ä¸ª person-day è®°å½•ï¼Œæ¯æ¡å« 108 ä¸ªç‰¹å¾ï¼ˆ72 è¿ç»­ + 36 åˆ†ç±»ï¼‰ã€‚ç”¨äºæµ‹è¯•å°æ ·æœ¬åœºæ™¯ã€‚ |
| **MIMIC-IV** | å¤§è§„æ¨¡é‡ç—‡ç›‘æŠ¤ç”µå­ç—…å†æ•°æ®ï¼ŒåŒ…å«çº¦ 18 ä¸‡ä½é™¢æ‚£è€…è®°å½•ï¼Œç”¨äºè¯„ä¼°å¤§æ ·æœ¬ä¸‹çš„æ€§èƒ½ã€‚ |

### å®éªŒè®¾ç½®
- æ•°æ®åˆ’åˆ†ï¼š
  - AI-READIï¼š90%/10% è®­ç»ƒ/æµ‹è¯•
  - MIMIC-IVï¼š70%/30% è®­ç»ƒ/æµ‹è¯•
- æ¯ä¸ªæ¨¡å‹åœ¨ 10 ç§ä¸åŒçš„ train-test split ä¸Šé‡å¤è®­ç»ƒå¹¶å–å¹³å‡ç»“æœï¼›
- è¶…å‚æ•°é€šè¿‡ **Optuna** è‡ªåŠ¨è°ƒä¼˜ï¼ˆ20 trialsï¼‰ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ– S2R åˆ†ç±»ä»»åŠ¡çš„è¯¯å·®ã€‚

### è¯„ä¼°æŒ‡æ ‡

#### ï¼ˆ1ï¼‰Privacyï¼ˆéšç§ï¼‰
- **Membership Inference Attack (MIA)**ï¼š
  - æ”»å‡»è€…åˆ¤æ–­æŸçœŸå®ä¸ªä½“æ˜¯å¦å‡ºç°åœ¨è®­ç»ƒé›†ä¸­ï¼›
  - è¡¡é‡æ ‡å‡†ï¼šAUROCï¼ˆè¶Šæ¥è¿‘ 0.5 è¡¨ç¤ºè¶Šå®‰å…¨ï¼‰ã€‚

#### ï¼ˆ2ï¼‰Utilityï¼ˆå®ç”¨æ€§ï¼‰
- **Synthetic-to-Real (S2R)** åˆ†ç±»ä»»åŠ¡ï¼š
  - åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒåˆ†ç±»å™¨ï¼Œåœ¨çœŸå®æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼›
  - AI-READIï¼šé¢„æµ‹ type 2 diabetes ç±»å‹ï¼›
  - MIMIC-IVï¼šé¢„æµ‹ä¸€å¹´å†…æ­»äº¡ç‡ï¼›
  - æŒ‡æ ‡ï¼šAUCã€‚

#### ï¼ˆ3ï¼‰Fidelityï¼ˆä¿çœŸåº¦ï¼‰
- **Real-to-Synthetic (R2S)**ï¼šåœ¨çœŸå®æ•°æ®ä¸Šè®­ç»ƒï¼Œåœ¨åˆæˆæ•°æ®ä¸Šæµ‹è¯•ï¼›
- **Column-wise Correlation Difference (CWC)**ï¼šè¡¡é‡ç‰¹å¾é—´ç›¸å…³æ€§ä¿ç•™ç¨‹åº¦ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ï¼›
- **Normalized Mutual Information (NMI)**ï¼šæ¯”è¾ƒçœŸå®ä¸åˆæˆæ•°æ®åœ¨ PCA + k-means èšç±»åçš„æ ‡ç­¾ä¸€è‡´æ€§ï¼ˆè¶Šä½è¡¨ç¤ºåˆ†å¸ƒè¶Šç›¸ä¼¼ï¼‰ï¼›
- **Dimension-Wise Difference (DWD)**ï¼š
  - å¯¹åˆ†ç±»å˜é‡ç”¨ **Absolute Prevalence Difference (APD)**ï¼›
  - å¯¹è¿ç»­å˜é‡ç”¨ **Average Wasserstein Distance (AWD)**ï¼›
  - ç»¼åˆå¾—åˆ†è¶Šä½è¶Šå¥½ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ¨¡å‹ | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **EMR-WGAN** | GAN-based | é’ˆå¯¹ EHR è®¾è®¡çš„ WGAN-GP æ¶æ„ï¼Œå¹¿æ³›ç”¨äºåŒ»ç–—æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ® |
| **EHRDiff** | Diffusion-based | ä¸“ä¸ºç»“æ„åŒ– EHR è®¾è®¡çš„æ‰©æ•£æ¨¡å‹ï¼Œå½“å‰ state-of-the-art |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### ï¼ˆ1ï¼‰Privacyï¼ˆè¡¨ 1ï¼‰
| æ¨¡å‹ | MIMIC-IV (AUROC) | AI-READI (AUROC) |
|------|------------------|------------------|
| Real Data | 0.998 | 1.000 |
| EMR-WGAN | 0.498 Â± 0.002 | **0.501 Â± 0.026** |
| EHRDiff | 0.499 Â± 0.002 | 0.601 Â± 0.036 |
| **RLSyn** | **0.499 Â± 0.002** | **0.544 Â± 0.030** |

> âœ… æ‰€æœ‰æ¨¡å‹åœ¨ MIMIC-IV ä¸Šå‡æ¥è¿‘éšæœºçŒœæµ‹ï¼ˆç†æƒ³æƒ…å†µï¼‰ï¼›  
> âš ï¸ åœ¨ AI-READI ä¸Šï¼ŒEHRDiff æ˜æ˜¾æ›´å®¹æ˜“è¢«æ”»å‡»ï¼ˆ>0.6ï¼‰ï¼Œè¡¨æ˜å…¶å­˜åœ¨è®°å¿†åŒ–å€¾å‘ã€‚

#### ï¼ˆ2ï¼‰Utilityï¼ˆå›¾ 2ï¼‰
| æ¨¡å‹ | MIMIC-IV (S2R AUC) | AI-READI (S2R AUC) |
|------|--------------------|---------------------|
| EMR-WGAN | 0.764 | 0.747 |
| EHRDiff | 0.906 | 0.871 |
| **RLSyn** | **0.902** | **0.873** |

> âœ… RLSyn åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äº GANï¼Œå¹¶ä¸ diffusion æ¨¡å‹ç›¸å½“ç”šè‡³ç•¥ä¼˜ã€‚

#### ï¼ˆ3ï¼‰Fidelityï¼ˆè¡¨ 2ï¼‰

| æ¨¡å‹ | MIMIC-IV CWCâ†“ | MIMIC-IV NMIâ†“ | MIMIC-IV DWDâ†“ |
|------|---------------|---------------|---------------|
| EMR-WGAN | 11.852 | 0.320 | 17.642 |
| EHRDiff | 7.848 | 0.003 | 2.797 |
| **RLSyn** | **8.877** | **0.001** | **2.073** |

| æ¨¡å‹ | AI-READI CWCâ†“ | AI-READI NMIâ†“ | AI-READI DWDâ†“ |
|------|---------------|---------------|---------------|
| EMR-WGAN | 1.352 | 0.809 | 77.056 |
| EHRDiff | 0.427 | 0.002 | 16.441 |
| **RLSyn** | **0.750** | **0.001** | **13.352** |

> âœ… RLSyn åœ¨æ‰€æœ‰ fidelity æŒ‡æ ‡ä¸Šå…¨é¢ä¼˜äº EMR-WGANï¼›
> âœ… åœ¨ AI-READI ä¸Šï¼ŒRLSyn åœ¨ NMI å’Œ DWD ä¸Šä¼˜äº EHRDiffï¼›
> ğŸ”º EHRDiff åœ¨ CWC ä¸Šç•¥ä¼˜ï¼Œä½†å…¶ R2S æ€§èƒ½è¶…è¿‡ R2R åŸºçº¿ï¼Œæç¤ºè¿‡æ‹Ÿåˆã€‚

#### ï¼ˆ4ï¼‰å¯è§†åŒ–åˆ†æï¼ˆå›¾ 3â€“4ï¼‰
- **PCA å¯è§†åŒ–** æ˜¾ç¤º RLSyn åˆæˆæ•°æ®åˆ†å¸ƒæœ€æ¥è¿‘çœŸå®æ•°æ®ï¼›
- **Bigram plots** æ˜¾ç¤º EMR-WGAN æ˜æ˜¾åå‘é«˜é¢‘è¯Šæ–­ï¼Œè€Œ RLSyn å’Œ EHRDiff æ›´å¥½åœ°è¿˜åŸäº†ç½•è§ç—…ç§ï¼›
- **ç›´æ–¹å›¾å¯¹æ¯”** æ˜¾ç¤º RLSyn æ›´å¥½åœ°è¦†ç›–äº†è¿ç»­å˜é‡ï¼ˆå¦‚ BMIã€è¡€ç³–ï¼‰çš„å®Œæ•´åˆ†å¸ƒèŒƒå›´ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **RLSyn åœ¨å°æ ·æœ¬åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•**ï¼š
   - åœ¨ AI-READI ä¸Šï¼ŒRLSyn åœ¨ utility å’Œ fidelity ä¸Šå‡ä¼˜äº EHRDiff å’Œ EMR-WGANï¼›
   - åŒæ—¶ä¿æŒè¾ƒä½çš„éšç§é£é™©ï¼ˆAUROC â‰ˆ 0.54ï¼‰ï¼Œè¿œä½äº EHRDiffï¼ˆ0.601ï¼‰ã€‚

2. **RL æä¾›äº†ä¸€ç§æ›´ç¨³å®šã€æ›´é«˜æ•ˆçš„æ•°æ®ç”ŸæˆèŒƒå¼**ï¼š
   - PPO çš„ clipped æ›´æ–°æœºåˆ¶æœ‰æ•ˆç¼“è§£äº† GAN çš„è®­ç»ƒä¸ç¨³å®šæ€§ï¼›
   - ä¸ä¾èµ–é•¿åºåˆ—å»å™ªè¿‡ç¨‹ï¼Œé€‚åˆå°æ•°æ®åœºæ™¯ã€‚

3. **EHRDiff å­˜åœ¨æ˜æ˜¾çš„è¿‡æ‹Ÿåˆä¸è®°å¿†åŒ–é£é™©**ï¼š
   - R2S æ€§èƒ½é«˜äº R2R åŸºçº¿ â†’ æš—ç¤ºæ¨¡å‹â€œè®°ä½äº†â€è®­ç»ƒæ•°æ®ï¼›
   - Membership inference æ”»å‡»æˆåŠŸç‡è¾ƒé«˜ â†’ å­˜åœ¨éšç§éšæ‚£ã€‚

4. **EMR-WGAN è¡¨ç°æœ€å·®**ï¼š
   - è™½ç„¶éšç§æ€§æœ€å¥½ï¼ˆå¯èƒ½æ˜¯å› æœªèƒ½å­¦å¥½æ•°æ®åˆ†å¸ƒï¼‰ï¼›
   - ä½†åœ¨ utility å’Œ fidelity ä¸Šå¤§å¹…è½åï¼Œè¯´æ˜å…¶ç”Ÿæˆè´¨é‡ä¸è¶³ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. å½“å‰ä»…é€‚ç”¨äº**æ¨ªæ–­é¢è¡¨æ ¼å‹æ•°æ®**ï¼ˆcross-sectional tabular dataï¼‰ï¼Œå°šæœªæ‰©å±•åˆ°æ—¶é—´åºåˆ—ã€æ–‡æœ¬æˆ–å›¾åƒç­‰å¤æ‚æ¨¡æ€ï¼›
2. RLSyn çš„ä¼˜åŠ¿æœºåˆ¶å°šä¸å®Œå…¨æ˜ç¡®ï¼ˆæ˜¯ PPOï¼Ÿè¿˜æ˜¯ policy parameterizationï¼Ÿï¼‰ï¼›
3. ç†è®ºåŸºç¡€æœ‰å¾…æ·±åŒ–ï¼Œç‰¹åˆ«æ˜¯ä¸ inverse RLã€energy-based models çš„è”ç³»éœ€è¿›ä¸€æ­¥æ¢ç´¢ï¼›
4. å¥–åŠ±å‡½æ•°ç›®å‰ä»ä¾èµ–åˆ¤åˆ«å™¨ï¼Œæœªæ¥å¯å°è¯•æ— åˆ¤åˆ«å™¨çš„ç»Ÿè®¡æŒ‡æ ‡å¥–åŠ±ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ RLSyn åœ¨ **longitudinal EHRã€clinical notesã€medical imaging** ä¸Šçš„åº”ç”¨ï¼›
- è®¾è®¡æ›´çµæ´»çš„å¥–åŠ±å‡½æ•°ï¼ˆå¦‚åŸºäºéšç§é¢„ç®—ã€å…¬å¹³æ€§ã€å› æœç»“æ„ç­‰ï¼‰ï¼›
- å¼€å±•ç†è®ºåˆ†æï¼Œæ­ç¤º RL-based SDG æˆåŠŸçš„å…³é”®å› ç´ ï¼›
- ç»“åˆ differential privacy æˆ– federated learningï¼Œæ„å»ºç«¯åˆ°ç«¯éšç§å¢å¼ºæ¡†æ¶ã€‚

---

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬è®ºæ–‡é¦–æ¬¡ç³»ç»ŸéªŒè¯äº† **Reinforcement Learning å¯ä½œä¸ºåˆæˆåŒ»ç–—æ•°æ®ç”Ÿæˆçš„æœ‰æ•ˆæ›¿ä»£èŒƒå¼**ï¼Œæå‡ºçš„ **RLSyn æ¡†æ¶åœ¨å°æ ·æœ¬æ¡ä»¶ä¸‹å®ç°äº†æ¯” GAN å’Œ diffusion æ›´ä¼˜çš„éšç§-æ•ˆç”¨æƒè¡¡**ï¼Œä¸ºèµ„æºæœ‰é™çš„ç”Ÿç‰©åŒ»å­¦ç ”ç©¶æä¾›äº†æ–°çš„å·¥å…·è·¯å¾„ã€‚

</details>

---

### 15. [Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM](https://arxiv.org/abs/2512.21580)

**Authors**: Alexander Podolskiy, Semen Molokov, Timofey Gerasin, Maksim Titov, Alexey Rukhovich, Artem Khrapov, Kirill Morozov, Evgeny Tetin, Constantine Korikov, Pavel Efimov, Polina Lazukova, Yuliya Skripkar, Nikita Okhotnikov, Irina Piontkovskaya, Meng Xiaojun, Zou Xueyi, Zhang Zhenhe  
**Category**: cs.CL  
**Published**: 2025-12-29  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.21580v1  

#### Abstract
We present Gamayun, a 1.5B-parameter multilingual language model trained entirely from scratch on 2.5T tokens. Designed for efficiency and deployment in resource-constrained environments, Gamayun addresses the lack of research on small non-English-centric LLMs by adopting a novel two-stage pre-train...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šGamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³**å°è§„æ¨¡å¤šè¯­è¨€å¤§æ¨¡å‹ï¼ˆSLMï¼‰ç ”ç©¶ä¸è¶³**çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éè‹±è¯­ä¸­å¿ƒåŒ–çš„å°å‹æ¨¡å‹ã€‚ç°æœ‰å°å‹ LLM å¤šä¸ºè‹±è¯­ä¸»å¯¼ï¼ˆ>90% è‹±è¯­è®­ç»ƒæ•°æ®ï¼‰ï¼Œå¯¼è‡´åœ¨å…¶ä»–è¯­è¨€ä¸Šè¡¨ç°ä¸ä½³ã€‚åŒæ—¶ï¼Œä»é›¶å¼€å§‹è®­ç»ƒå¤šè¯­è¨€æ¨¡å‹é¢ä¸´â€œå¤šè¯­è¨€è¯…å’’â€ï¼ˆcurse of multilingualityï¼‰â€”â€”å³åŠ å…¥å¤šè¯­è¨€æ•°æ®ä¼šé™ä½ä¸»è¯­è¨€çš„æ•°æ®æ•ˆç‡ã€‚

æ­¤å¤–ï¼Œåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹ï¼Œå¦‚ä½•ä»¥è¾ƒä½æˆæœ¬è®­ç»ƒå‡ºé«˜æ€§èƒ½ã€å¯éƒ¨ç½²çš„å¤šè¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ä¸ª**ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥ï¼ˆtwo-stage pre-training strategyï¼‰**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ†é˜¶æ®µä¼˜åŒ–è¯­è¨€å¯¹é½ä¸æ€§èƒ½è¿ç§»ï¼š

1. **ç¬¬ä¸€é˜¶æ®µï¼šå¹³è¡¡å¤šè¯­è¨€è®­ç»ƒï¼ˆBalanced Multilingual Trainingï¼‰**
   - åˆå§‹é˜¶æ®µé‡‡ç”¨å‡è¡¡çš„è¯­è¨€æ··åˆæ¯”ä¾‹ï¼ˆçº¦ 37% è‹±è¯­ï¼‰ï¼Œç¡®ä¿ä¸åŒè¯­è¨€ä¹‹é—´çš„**è·¨è¯­è¨€å¯¹é½ï¼ˆcross-lingual alignmentï¼‰**ã€‚
   - æ­¤é˜¶æ®µæœ‰åŠ©äºå»ºç«‹é€šç”¨è¯­ä¹‰ç©ºé—´ï¼Œé¿å…ä½è´¨é‡è¯­è¨€æ•°æ®å¹²æ‰°é«˜èµ„æºè¯­è¨€å­¦ä¹ ã€‚

2. **ç¬¬äºŒé˜¶æ®µï¼šé«˜è´¨é‡è‹±è¯­å¢å¼ºï¼ˆHigh-Quality English Enrichmentï¼‰**
   - åœ¨å®Œæˆåˆæ­¥å¯¹é½åï¼Œæ˜¾è‘—æå‡é«˜è´¨é‡è‹±è¯­æ•°æ®çš„æ¯”ä¾‹ï¼ˆå¢è‡³ ~70%ï¼‰ï¼Œå¼•å…¥å¯Œå«çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›çš„æ•°æ®ï¼ˆå¦‚ STEMã€ä»£ç ã€ä¹¦ç±ç­‰ï¼‰ã€‚
   - è¿™äº›é«˜è´¨é‡æ•°æ®å¸¦æ¥çš„èƒ½åŠ›å¯ä»¥é€šè¿‡å·²å»ºç«‹çš„è·¨è¯­è¨€è¡¨ç¤ºè¿ç§»åˆ°æ‰€æœ‰æ”¯æŒçš„è¯­è¨€ä¸­ï¼Œä»è€Œå®ç°â€œæ€§èƒ½è½¬ç§»â€ã€‚

è¯¥ç­–ç•¥æ— éœ€ä¾èµ–å¤§è§„æ¨¡è’¸é¦æˆ–åˆæˆæ•°æ®ç”Ÿæˆï¼Œä»…ä½¿ç”¨å…¬å¼€å¯ç”¨æ•°æ®å³å¯é«˜æ•ˆè®­ç»ƒã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æˆæœ¬æ•ˆç›Šé«˜**ï¼šæ€»è®­ç»ƒé‡ä»…ä¸º **2.6T tokens**ï¼Œè¿œä½äºåŒç±»æ¨¡å‹ï¼ˆå¦‚ Qwen2.5-1.5B ä½¿ç”¨ 18Tï¼ŒQwen3 ä½¿ç”¨ 36Tï¼‰ã€‚
- **æ€§èƒ½æ›´å¼º**ï¼šå°½ç®¡è®­ç»ƒé¢„ç®—æ›´å°ï¼Œä½†åœ¨å¤šæ•°è‹±æ–‡å’Œå¤šè¯­è¨€ä»»åŠ¡ä¸Šä¼˜äº LLaMA3.2-1B å’Œ Qwen2.5-1.5Bã€‚
- **æ–‡åŒ–ç†è§£æ›´æ·±**ï¼šç›¸æ¯”åŸºäºå¾®è°ƒçš„ä¿„è¯­é€‚é…æ¨¡å‹ï¼ˆå¦‚ ruadaptã€Vikhrï¼‰ï¼ŒGamayun å› ç›´æ¥åœ¨ä¿„è¯­æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ–‡åŒ–å¸¸è¯†ç†è§£å’Œå¤æ‚å½¢æ€å¤„ç†èƒ½åŠ›ã€‚
- **æ¶æ„ç®€æ´æœ‰æ•ˆ**ï¼šæœªé‡‡ç”¨ MoE æˆ– RLHF ç­‰å¤æ‚æŠ€æœ¯ï¼Œè¯æ˜é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®è°ƒåº¦ä¹Ÿèƒ½å–å¾— SOTA è¡¨ç°ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **åŸºç¡€è¯­æ–™åº“**ï¼š`mC4`ï¼ˆå¤šè¯­è¨€ç‰ˆæœ¬ï¼‰ä½œä¸ºåˆå§‹æ•°æ®æºã€‚
- **è¡¥å……é«˜è´¨é‡æ•°æ®**ï¼š
  - å›¾ä¹¦ï¼š`Books`ï¼ˆProject Gutenbergï¼‰
  - ç§‘å­¦æ–‡çŒ®ï¼š`PubMed`, `arXiv`, `FineWeb-Edu`
  - ä»£ç ï¼š`Starcoder`, `StackExchange`
  - æ³•å¾‹æ–‡æœ¬ï¼š`FreeLaw`
  - å…¶ä»–é«˜è´¨é‡è¯­æ–™ï¼š`ProofPile`, `Pile`

#### æ•°æ®åˆ†å¸ƒå˜åŒ–ï¼ˆè§ Figure 1ï¼‰
| é˜¶æ®µ | è‹±è¯­å æ¯” | ä¸»è¦æ•°æ®æ¥æº |
|------|--------|-------------|
| ç¬¬ä¸€é˜¶æ®µ | ~37% | mC4 å ä¸»å¯¼ï¼ˆ64.37%ï¼‰ï¼Œå«å¤šç§è¯­è¨€ |
| ç¬¬äºŒé˜¶æ®µ | ~70% | é«˜è´¨é‡è‹±è¯­æ•°æ®ä¸ºä¸»ï¼ˆå¦‚ FineWeb-Edu å  29.3%ï¼‰ï¼ŒmC4 æ¯”ä¾‹é™è‡³ 16% |

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹æ¶æ„
- ç±»ä¼¼ LLaMA æ¶æ„ï¼š`SwiGLU`, `RoPE`, `RMSNorm`
- å‚æ•°é…ç½®ï¼š1.5B å‚æ•°ï¼ˆ24 å±‚ï¼Œéšè—å±‚å¤§å° 2048ï¼Œæ³¨æ„åŠ›å¤´æ•° 16ï¼‰
- åˆ†è¯å™¨ï¼šé‡‡ç”¨ `LLaMA 3` çš„ tokenizerï¼ˆå¹³è¡¡è¯æ±‡é‡ä¸ fertilityï¼‰

#### åè®­ç»ƒæµç¨‹ï¼ˆPost-trainingï¼‰
- **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼šä½¿ç”¨ `SlimOrca`, `OpenHermes`, `Synthetic MCF`, æ•°å­¦ä¸ä»£ç æ•°æ®ç­‰ã€‚
- **åå¥½å¯¹é½ï¼ˆDPOï¼‰**ï¼šåŸºäº `Tulu-3` åå¥½æ•°æ®é›†è¿›è¡Œ Direct Preference Optimizationã€‚

#### è¯„ä¼°åŸºå‡†
| ç±»åˆ« | è‹±æ–‡ä»»åŠ¡ | ä¿„æ–‡ä»»åŠ¡ | å…¶ä»–è¯­è¨€ |
|------|--------|--------|---------|
| ç»¼åˆçŸ¥è¯† | MMLU, MMLU-Pro | GlobalMMLU-ru | ArabicMMLU, FrenchBench, La Leaderboard |
| æ¨ç†èƒ½åŠ› | ARC-e/c, Hellaswag, Winogrande, Ifeval | XNLI-ru, XStoryCloze, XWinograd | Belebeleï¼ˆå¤šè¯­è¨€é˜…è¯»ç†è§£ï¼‰ |
| æ•°å­¦èƒ½åŠ› | GSM8K | GSM8K-ru | â€” |
| æ–‡åŒ–ç†è§£ | â€” | **RuBIN**ï¼ˆè‡ªå»ºæ–‡åŒ–å¸¸è¯†é¢˜ï¼‰ã€**MERA**ï¼ˆä¿„è¯­ç»¼åˆè¯„æµ‹ï¼‰ | â€” |
| å¤šè¯­è¨€ç»¼åˆ | â€” | â€” | **INCLUDE**ï¼ˆè·¨åŒºåŸŸçŸ¥è¯†ä¸æ¨ç†ï¼‰ |

#### åŸºçº¿æ¨¡å‹å¯¹æ¯”
é€‰å–å‚æ•°é‡ç›¸è¿‘ï¼ˆ1â€“2Bï¼‰çš„å…ˆè¿›å¼€æºæ¨¡å‹ï¼š
- `LLaMA3.2-1B`ï¼ˆ9T tokensï¼‰
- `Qwen2.5-1.5B`ï¼ˆ18T tokensï¼‰
- `Qwen3-1.7B`ï¼ˆ36T tokensï¼‰
- `Gemma3-1B`ï¼ˆ2T tokensï¼‰
- `EuroLM-1.7B`ï¼ˆæ¬§æ´²å¤šè¯­è¨€ï¼‰
- `SmolLM2-1.7B`ï¼ˆ11T tokensï¼‰
- ä¿„è¯­é€‚é…æ¨¡å‹ï¼š`ruadapt`, `QVikhr`, `VikhrLlama`

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Tables 3 & 4ï¼‰

#### âœ… è‹±æ–‡ä»»åŠ¡å¹³å‡å¾—åˆ†ï¼ˆTable 2ï¼‰
| Model | MMLU | GSM8K | Average |
|-------|------|-------|--------|
| LLaMA3.2-1B | 45.6 | 38.2 | 46.09 |
| Qwen2.5-1.5B | 59.7 | 57.9 | 51.84 |
| **Gamayun-1.5B** | **46.4** | **56.4** | **50.36** |

> å°½ç®¡ MMLU è½åäº Qwen2.5ï¼Œä½†åœ¨æ•°å­¦ä»»åŠ¡ä¸Šæ¥è¿‘ï¼Œå¹¶ä¸”æ€»ä½“å¹³å‡åˆ†éå¸¸æœ‰ç«äº‰åŠ›ã€‚

#### âœ… å¤šè¯­è¨€ä»»åŠ¡å¹³å‡å¾—åˆ†ï¼ˆTable 3ï¼‰
| Model | Russian | Arabic | Chinese | Average |
|-------|--------|--------|--------|--------|
| Qwen2.5-1.5B | 49.8 | 53.7 | 68.7 | â€” |
| **Gamayun-1.5B** | **50.7** | **57.2** | **55.6** | **æœ€é«˜æˆ–æ¬¡é«˜** |

> Gamayun åœ¨å¤§å¤šæ•°è¯­è¨€ä¸Šè¾¾åˆ° SOTA æˆ–æ¥è¿‘ SOTA æ°´å¹³ï¼Œå°¤å…¶åœ¨é˜¿æ‹‰ä¼¯è¯­å’Œä¿„è¯­ä¸Šå¤§å¹…é¢†å…ˆã€‚

#### âœ… ä¿„è¯­ä¸“é¡¹è¯„æµ‹ï¼ˆTable 4ï¼‰
| Benchmark | Gamayun | Qwen2.5 | ruadapt | Qwen3 |
|----------|--------|--------|--------|--------|
| **MERA**ï¼ˆä¿„è¯­ç»¼åˆï¼‰ | **38.1** | 35.7 | 33.7 | 36.4 |
| **RuBIN**ï¼ˆæ–‡åŒ–å¸¸è¯†ï¼‰ | **35.2** | 32.0 | 31.0 | 32.0 |

> Gamayun åœ¨ä¸¤ä¸ªä¿„è¯­ä¸“å±è¯„æµ‹ä¸­å‡æ’åç¬¬ä¸€ï¼Œè¡¨æ˜å…¶åœ¨æœ¬åœ°æ–‡åŒ–å’Œè¯­è¨€ç»†èŠ‚ä¸Šçš„ä¼˜åŠ¿ã€‚

#### âœ… æ•°å­¦èƒ½åŠ›åˆ†ç±»
- **æ•°å­¦ä¸“å®¶çº§**ï¼ˆ>65%ï¼‰ï¼šQwen3, Olmo2
- **å¼ºè¡¨ç°è€…**ï¼ˆ56â€“58%ï¼‰ï¼š**Gamayun**, Qwen2.5
- **è¾ƒå¼±æ¨¡å‹**ï¼ˆ<45%ï¼‰ï¼šLLaMA3.2, Gemma3

> Gamayun åœ¨æ•°å­¦æ–¹é¢å±äºâ€œå¼ºè¡¨ç°è€…â€ï¼Œè™½ä¸åŠé‡åº¦ä¼˜åŒ–çš„ Qwen3ï¼Œä½†æ˜¾è‘—ä¼˜äºåŒè§„æ¨¡å¤šæ•°æ¨¡å‹ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAppendix B & Figure 2ï¼‰

#### ğŸ” ä¸¤é˜¶æ®µè®­ç»ƒæœ‰æ•ˆæ€§éªŒè¯
- å¯¹æ¯”å®éªŒæ˜¾ç¤ºï¼šè‹¥ä¸€å¼€å§‹å°±ä½¿ç”¨é«˜æ¯”ä¾‹è‹±è¯­ï¼ˆbaselineï¼‰ï¼Œè™½ç„¶åˆæœŸè‹±æ–‡è¡¨ç°æ›´å¥½ï¼Œä½†æœ€ç»ˆå¤šè¯­è¨€èƒ½åŠ›è¾ƒå·®ã€‚
- é‡‡ç”¨â€œå…ˆå¹³è¡¡ â†’ å†èšç„¦â€çš„ä¸¤é˜¶æ®µç­–ç•¥åï¼Œæ¨¡å‹ä¸ä»…è¿½ä¸Šäº†è‹±æ–‡ baselineï¼Œè¿˜åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸Šä¿æŒé¢†å…ˆï¼ˆè§ Figure 2ï¼‰ã€‚

#### ğŸ” è¯­è¨€é‡‡æ ·ç­–ç•¥æ¯”è¾ƒï¼ˆTable 5ï¼‰
å°è¯•äº† `Temperature Scaling` å’Œ `UniMax` ç­–ç•¥ï¼Œå‘ç°å®ƒä»¬æå‡äº†ä½èµ„æºè¯­è¨€æ€§èƒ½ï¼Œä½†ä¸¥é‡æŸå®³äº†é«˜èµ„æºè¯­è¨€ï¼ˆå¦‚è‹±è¯­ã€ä¿„è¯­ï¼‰çš„è¡¨ç°ã€‚å› æ­¤æœ€ç»ˆé€‰æ‹©æŒ‰ token æ•°é‡æˆæ¯”ä¾‹é‡‡æ ·ï¼ˆproportional samplingï¼‰ã€‚

#### ğŸ” æ•°æ®è´¨é‡å½±å“åˆ†æï¼ˆFigure 4ï¼‰
- æ·»åŠ æ›´å¤šä¸­æ–‡ mC4 æ•°æ®å¯¹ä¸­æ–‡æŸå¤±æ”¹å–„æœ‰é™ï¼Œåè€ŒåŠ å‰§è‹±æ–‡é€€åŒ–ã€‚
- æ·»åŠ ä¿„è¯­æ•°æ®åˆ™èƒ½æŒç»­æ”¹å–„ä¿„è¯­æ€§èƒ½ä¸”ä¸æ˜æ˜¾ä¼¤å®³è‹±æ–‡ã€‚
> ç»“è®ºï¼š**mC4 ä¸­éè‹±è¯­è¯­è¨€çš„æ•°æ®è´¨é‡å‚å·®ä¸é½**ï¼Œä¿„è¯­ç›¸å¯¹è¾ƒå¥½ï¼Œè€Œä¸­æ–‡å­˜åœ¨è¾ƒå¤šå™ªå£°ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥æœ‰æ•ˆç¼“è§£â€œå¤šè¯­è¨€è¯…å’’â€**ï¼šé€šè¿‡å…ˆå¯¹é½è¯­è¨€è¡¨ç¤ºï¼Œå†æ³¨å…¥é«˜è´¨é‡è‹±è¯­çŸ¥è¯†ï¼Œå®ç°äº†è·¨è¯­è¨€çš„èƒ½åŠ›è¿ç§»ã€‚
2. **2.5T tokens è¶³ä»¥è®­ç»ƒé«˜æ€§èƒ½å¤šè¯­è¨€ SLM**ï¼šå³ä½¿è®­ç»ƒé‡ä»…ä¸º Qwen3 çš„ 1/14ï¼ŒGamayun ä»èƒ½åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šä¸å…¶åª²ç¾ã€‚
3. **æ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦**ï¼šä½è´¨é‡å¤šè¯­è¨€æ•°æ®ä¸ä»…è‡ªèº«æ— æ•ˆï¼Œè¿˜ä¼šæ‹–ç´¯é«˜è´¨é‡è¯­è¨€çš„å­¦ä¹ ã€‚
4. **ä»å¤´é¢„è®­ç»ƒä¼˜äºåæœŸé€‚é…**ï¼šåœ¨ä¿„è¯­æ–‡åŒ–å¸¸è¯†ï¼ˆRuBINï¼‰å’Œå½¢æ€å¤æ‚å¥ç”Ÿæˆä¸Šï¼ŒGamayun æ˜¾è‘—ä¼˜äºåŸºäº Qwen/Llama å¾®è°ƒçš„ä¿„è¯­æ¨¡å‹ï¼ˆå¦‚ ruadaptï¼‰ï¼Œè¯´æ˜æ·±å±‚æ–‡åŒ–çŸ¥è¯†éœ€åœ¨é¢„è®­ç»ƒé˜¶æ®µå†…åŒ–ã€‚
5. **LLaMA-3 tokenizer æ˜¯å¤šè¯­è¨€åœºæ™¯ä¸‹çš„è‰¯å¥½æŠ˜è¡·æ–¹æ¡ˆ**ï¼šåœ¨è¯æ±‡é‡ã€fertility å’Œå¤šè¯­è¨€æ”¯æŒä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **é«˜çº§ STEM ä»»åŠ¡è½å**ï¼šåœ¨ MMLU å’Œæ•°å­¦æ¨ç†ä¸Šä»è½åäºç»è¿‡ä¸“é—¨å¼ºåŒ–è®­ç»ƒçš„å¤§æ¨¡å‹ï¼ˆå¦‚ Qwen3ï¼‰ã€‚
- **é•¿æ–‡æœ¬ç”Ÿæˆä»ä¼šå¹»è§‰**ï¼šå°½ç®¡æŒ‡ä»¤å¾®è°ƒåæœ‰æ‰€æ”¹å–„ï¼Œä½†åœ¨è‡ªç”±ç”Ÿæˆä¸­ä»å¯èƒ½å‡ºç°äº‹å®é”™è¯¯ï¼ˆè§ Appendix L ç¤ºä¾‹ï¼‰ã€‚
- **æœªä½¿ç”¨ RLHF æˆ– MoE æŠ€æœ¯**ï¼šå¯èƒ½é™åˆ¶äº†æé™æ€§èƒ½ä¸Šé™ï¼Œä½†ä¹Ÿä¿è¯äº†ä½æˆæœ¬å¯å¤ç°æ€§ã€‚
- **ä»…æ”¯æŒ 12 ç§è¯­è¨€**ï¼šç‰ºç‰²äº†è¯­è¨€å¹¿åº¦ä»¥æ¢å–æ¯ç§è¯­è¨€çš„æ·±åº¦è®­ç»ƒã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼•å…¥æ›´å¤š STEM å’Œæ•°å­¦ä¸“é¡¹æ•°æ®ä»¥ç¼©å°ä¸é¡¶çº§æ¨¡å‹çš„å·®è·ã€‚
- æ¢ç´¢è½»é‡çº§ RLHF æˆ–è¿‡ç¨‹å¥–åŠ±æœºåˆ¶æ¥è¿›ä¸€æ­¥å‡å°‘å¹»è§‰ã€‚
- æ‰©å±•è‡³æ›´å¤šè¯­è¨€ï¼ŒåŒæ—¶ç ”ç©¶åŠ¨æ€è¯­è¨€æ‰©å±•æœºåˆ¶ã€‚
- å¼€å‘æ›´é²æ£’çš„å¤šè¯­è¨€æ•°æ®æ¸…æ´—ç®¡é“ï¼Œæå‡éè‹±è¯­è¯­æ–™è´¨é‡ã€‚

---

> **æ€»ç»“**ï¼šGamayun è¯æ˜äº†åœ¨æœ‰é™ç®—åŠ›ä¸‹ï¼Œé€šè¿‡**åˆç†çš„ä¸¤é˜¶æ®µæ•°æ®è°ƒåº¦ç­–ç•¥**å’Œ**é«˜è´¨é‡æ•°æ®ç­›é€‰**ï¼Œå¯ä»¥è®­ç»ƒå‡ºæ€§èƒ½è¶…è¶Šæ›´å¤§æ¨¡å‹çš„å°å‹å¤šè¯­è¨€ LLMã€‚å…¶æˆåŠŸå…³é”®åœ¨äºâ€œ**å…ˆå¯¹é½ï¼Œåææ•ˆ**â€çš„è®¾è®¡å“²å­¦ï¼Œä¸ºèµ„æºå—é™åœºæ™¯ä¸‹çš„å¤šè¯­è¨€æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦èŒƒå¼ã€‚

</details>

---

### 16. [HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness](https://arxiv.org/abs/2512.22014)

**Authors**: Chengyu Tian, Wenbin Pei  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.22014v1  

#### Abstract
Robustness in complex systems is of significant engineering and economic importance. However, conventional attack-based a posteriori robustness assessments incur prohibitive computational overhead. Recently, deep learning methods, such as Convolutional Neural Networks (CNNs) and Graph Neural Network...

---

### 17. [Scaling Adversarial Training via Data Selection](https://arxiv.org/abs/2512.22069)

**Authors**: Youran Ye, Dejin Wang, Ajinkya Bhandare  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.22069v1  

#### Abstract
Projected Gradient Descent (PGD) is a strong and widely used first-order adversarial attack, yet its computational cost scales poorly, as all training samples undergo identical iterative inner-loop optimization despite contributing unequally to robustness. Motivated by this inefficiency, we propose ...

---

### 18. [SWE-RM: Execution-free Feedback For Software Engineering Agents](https://arxiv.org/abs/2512.21919)

**Authors**: KaShun Shum, Binyuan Hui, Jiawei Chen, Lei Zhang, X. W., Jiaxi Yang, Yuzhen Huang, Junyang Lin, Junxian He  
**Category**: cs.CL  
**Published**: 2025-12-29  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.21919v1  

#### Abstract
Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often ...

---

### 19. [RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models](https://arxiv.org/abs/2512.21572)

**Authors**: Anthony Bolton, Wuyang Zhou, Zehua Chen, Giorgos Iacovides, Danilo Mandic  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.21572v1  

#### Abstract
Financial time series forecasting is particularly challenging for transformer-based time series foundation models (TSFMs) due to non-stationarity, heavy-tailed distributions, and high-frequency noise present in data. Low-rank adaptation (LoRA) has become a popular parameter-efficient method for adap...

---

### 20. [Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care](https://arxiv.org/abs/2512.21602)

**Authors**: Yusuf Brima, Marcellin Atemkeng  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.21602v1  

#### Abstract
Emergency and intensive care environments require predictive models that are both accurate and computationally efficient, yet clinical data in these settings are often severely imbalanced. Such skewness undermines model reliability, particularly for rare but clinically crucial outcomes, making robus...

---

### 21. [Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model](https://arxiv.org/abs/2512.21540)

**Authors**: Yanhao Li, Lu Ma, Jiaran Zhang, Lexiang Tang, Wentao Zhang, Guibo Luo  
**Category**: cs.AI  
**Published**: 2025-12-29  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.21540v1  

#### Abstract
Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and...

---

### 22. [MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles](https://arxiv.org/abs/2512.21708)

**Authors**: Jing Han, Binwei Yan, Tianyu Guo, Zheyuan Bai, Mengyu Zheng, Hanting Chen, Ying Nie  
**Category**: cs.CL  
**Published**: 2025-12-29  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.21708v1  

#### Abstract
Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly domin...

---

### 23. [Embedding Samples Dispatching for Recommendation Model Training in Edge Environments](https://arxiv.org/abs/2512.21615)

**Authors**: Guopeng Li, Haisheng Tan, Chi Zhang, Hongqiu Ni, Zilong Wang, Xinyue Zhang, Yang Xu, Han Tian  
**Category**: cs.DC  
**Published**: 2025-12-29  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.21615v1  

#### Abstract
Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to ...

---

### 24. [Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data](https://arxiv.org/abs/2512.21516)

**Authors**: Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.21516v1  

#### Abstract
Recently, contrastive learning (CL) plays an important role in exploring complementary information for multi-view clustering (MVC) and has attracted increasing attention. Nevertheless, real-world multi-view data suffer from data incompleteness or noise, resulting in rare-paired samples or mis-paired...

---

### 25. [AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging](https://arxiv.org/abs/2512.21569)

**Authors**: Xiaobin Ren, Kaiqi Zhao, Katerina Ta\v{s}kova, Patricia Riddle  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.21569v1  

#### Abstract
Spatio-temporal kriging is a fundamental problem in sensor networks, driven by the sparsity of deployed sensors and the resulting missing observations. Although recent approaches model spatial and temporal correlations, they often under-exploit two practical characteristics of real deployments: the ...

---

### 26. [Dictionary-Transform Generative Adversarial Networks](https://arxiv.org/abs/2512.21677)

**Authors**: Angshul Majumdar  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.21677v1  

#### Abstract
Generative adversarial networks (GANs) are widely used for distribution learning, yet their classical formulations remain theoretically fragile, with ill-posed objectives, unstable training dynamics, and limited interpretability. In this work, we introduce \emph{Dictionary-Transform Generative Adver...

---

### 27. [Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs](https://arxiv.org/abs/2512.21915)

**Authors**: Yafeng Tang, Xiaoou Ding, Jianzhuo Du, Zishuo Yan, Zhuang Ma, Zheng Liang, Zekai Qian, Hongzhi Wang  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.21915v1  

#### Abstract
Tabular data generation has become increasingly essential for enabling robust machine learning applications, which require large-scale, high-quality data. Existing solutions leverage generative models to learn original data distributions. However, real-world data are naturally heterogeneous with div...

---

### 28. [Context as a Tool: Context Management for Long-Horizon SWE-Agents](https://arxiv.org/abs/2512.22087)

**Authors**: Shukai Liu, Jian Yang, Bo Jiang, Yizhi Li, Jinyang Guo, Xianglong Liu, Bryan Dai  
**Category**: cs.CL  
**Published**: 2025-12-29  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.22087v1  

#### Abstract
Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression he...

---

### 29. [An Equivariance Toolbox for Learning Dynamics](https://arxiv.org/abs/2512.21447)

**Authors**: Yongyi Yang, Liu Ziyin  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.21447v1  

#### Abstract
Many theoretical results in deep learning can be traced to symmetry or equivariance of neural networks under parameter transformations. However, existing analyses are typically problem-specific and focus on first-order consequences such as conservation laws, while the implications for second-order s...

---

### 30. [Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US](https://arxiv.org/abs/2512.21456)

**Authors**: Sukanya Krishna, Marie-Laure Charpignon, Maimuna Majumder  
**Category**: cs.LG  
**Published**: 2025-12-29  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.21456v1  

#### Abstract
Substance overdose mortality in the United States claimed over 80,000 lives in 2023, with the COVID-19 pandemic exacerbating existing trends through healthcare disruptions and behavioral changes. Estimating excess mortality, defined as deaths beyond expected levels based on pre-pandemic patterns, is...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
