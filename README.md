# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-17 06:40:30 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [QuRL: Efficient Reinforcement Learning with Quantized Rollout](https://arxiv.org/abs/2602.13953)

**Authors**: Yuhang Li, Reena Elangovan, Xin Dong, Priyadarshini Panda, Brucek Khailany  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2602.13953v1  

#### Abstract
Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\% of the ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šQuRL: Efficient Reinforcement Learning with Quantized Rollout**

---

## 1. **è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
åœ¨åŸºäºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆReasoning LLMsï¼‰çš„è¿‡ç¨‹ä¸­ï¼Œ**rollout é˜¶æ®µæˆä¸ºè®­ç»ƒæ•ˆç‡çš„ç“¶é¢ˆ**ï¼Œå æ€»è®­ç»ƒæ—¶é—´é«˜è¾¾ 70%ã€‚è¿™æ˜¯ç”±äº LLM çš„è‡ªå›å½’è§£ç ç‰¹æ€§å¯¼è‡´çš„é«˜å»¶è¿Ÿã€‚å°½ç®¡é‡åŒ–ï¼ˆquantizationï¼‰å¯åŠ é€Ÿæ¨ç†ï¼Œä½†ç›´æ¥å°†é‡åŒ–ç”¨äº rollout ä¼šå¼•å‘ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š
1. **é•¿æœŸè®­ç»ƒå´©æºƒï¼ˆtraining collapseï¼‰**ï¼šé‡åŒ– actor ä¸å…¨ç²¾åº¦ actor ä¹‹é—´çš„ç­–ç•¥å·®å¼‚éšè®­ç»ƒæ‰©å¤§ï¼Œå¯¼è‡´é‡è¦æ€§é‡‡æ ·å¤±æ•ˆã€‚
2. **æƒé‡æ›´æ–°è¢«é‡åŒ–å™ªå£°æ©ç›–**ï¼šRL æ›´æ–°çš„æƒé‡å˜åŒ–æå°ï¼ˆ~1e-7ï¼‰ï¼Œè¿œå°äº INT8/FP8 é‡åŒ–çš„ç²’åº¦ï¼Œå¯¼è‡´é‡åŒ–æ¨¡å‹â€œå†»ç»“â€ï¼Œæ— æ³•æœ‰æ•ˆå­¦ä¹ ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•**
æœ¬æ–‡æå‡º **QuRL (Quantized Reinforcement Learning)**ï¼Œä¸€ç§é€šè¿‡é‡åŒ– rollout åŠ é€Ÿ RL è®­ç»ƒçš„é«˜æ•ˆç®—æ³•ï¼ŒåŒ…å«ä¸¤å¤§æ ¸å¿ƒæŠ€æœ¯ï¼š

#### âœ… **Adaptive Clipping Range (ACR)**
- åŠ¨æ€è°ƒæ•´ PPO ä¸­çš„è£å‰ªèŒƒå›´ï¼ˆclipping rangeï¼‰ï¼Œä¾æ® **å…¨ç²¾åº¦ actor ä¸é‡åŒ– actor çš„ç­–ç•¥æ¯”ç‡** è°ƒæ•´ä¿¡ä»»åŸŸè¾¹ç•Œã€‚
- å½“è¡Œä¸ºç­–ç•¥ï¼ˆbehavior policyï¼‰ä¸è¿‘ç«¯ç­–ç•¥ï¼ˆproximal policyï¼‰å·®å¼‚è¿‡å¤§æ—¶ï¼Œè‡ªåŠ¨æ”¾å®½è£å‰ªä¸Šé™ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å’Œè®­ç»ƒä¸ç¨³å®šã€‚

#### âœ… **Update-Aware Quantization (UAQ)**
- å¼•å…¥ **ä¸å˜ç¼©æ”¾ï¼ˆinvariant scalingï¼‰** æŠ€æœ¯ï¼Œåœ¨é‡åŒ–å‰å¯¹æƒé‡è¿›è¡Œæ”¾å¤§ï¼ˆscaling factor $ s > 1 $ï¼‰ã€‚
- åŒé‡æ•ˆæœï¼š
  - å‡å°‘é‡åŒ–è¯¯å·®ï¼ˆå› æƒé‡å˜å¤§ï¼‰
  - æ”¾å¤§æ¢¯åº¦æ›´æ–°ï¼ˆå› æ¿€æ´»ä¹Ÿç¼©æ”¾ï¼‰
- å®ç° $ s^2 $ å€çš„â€œæ›´æ–°ä¿¡å· vs é‡åŒ–å™ªå£°â€ä¿¡å™ªæ¯”æå‡ï¼Œä½¿é‡åŒ–æ¨¡å‹èƒ½æ„ŸçŸ¥å¾®å°æƒé‡å˜åŒ–ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| æ–¹é¢ | QuRL | FlashRL / Naive Quantized RL |
|------|------|-----------------------------|
| **è®­ç»ƒç¨³å®šæ€§** | âœ… é•¿æœŸç¨³å®šï¼ˆ>1000 æ­¥æ— å´©æºƒï¼‰ | âŒ åæœŸæ€§èƒ½ä¸‹é™æˆ–å´©æºƒ |
| **æ€§èƒ½ä¿ç•™** | âœ… æ¥è¿‘ BF16 åŸºçº¿ï¼ˆå·®è·ä»… ~1â€“2%ï¼‰ | âŒ æ˜æ˜¾æ€§èƒ½é€€åŒ–ï¼ˆå¦‚ INT8 ä¸‹é™ 4%ï¼‰ |
| **æ•ˆç‡æå‡** | âœ… 20%â€“80% æ›´å¿« rollout | âœ… ç±»ä¼¼åŠ é€Ÿï¼Œä½†æ€§èƒ½å·® |
| **å®ç°å¤æ‚åº¦** | âœ… æ— éœ€ QAT æˆ–æ¯æ­¥æ ¡å‡† | âŒ FlashRL ä¾èµ–å·¥ç¨‹é€‚é… |

---

## 2. **æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†ä¸ä»»åŠ¡**
- **GSM8K**ï¼šå°å­¦æ•°å­¦åº”ç”¨é¢˜ï¼Œæµ‹è¯•åŸºæœ¬æ¨ç†èƒ½åŠ›ã€‚
- **AIME 2024**ï¼šé«˜çº§æ•°å­¦ç«èµ›é¢˜ï¼Œè¯„ä¼°å¤æ‚æ¨ç†ã€‚
- **DeepScaleR Benchmark**ï¼šç»¼åˆæ•°å­¦æ¨ç†åŸºå‡†ï¼Œæ¶µç›–ï¼š
  - AIME
  - AMC
  - MATH
  - Minerva
  - Olympiad

### **æ¨¡å‹è§„æ¨¡**
- Qwen2.5-0.5B / 7B / 14B / 32B
- DeepSeek-Distill-Qwen ç³»åˆ—ï¼ˆ7B/14B/32Bï¼‰

### **å®éªŒè®¾ç½®**
- **é‡åŒ–æ ¼å¼**ï¼šINT8 å’Œ FP8
- **é‡åŒ–æ–¹å¼**ï¼š
  - æƒé‡ï¼šchannel-wise scaling
  - æ¿€æ´»ï¼štoken-wise scaling
- **è®­ç»ƒæ¡†æ¶**ï¼šVeRLï¼ˆæ··åˆå¼•æ“ RL æ¡†æ¶ï¼‰
- **ç¡¬ä»¶å¹³å°**ï¼šA6000, A100, H100ï¼ˆå•å¡æˆ–åŒå¡ TPï¼‰
- **rollout è®¾ç½®**ï¼šä¸åŒä»»åŠ¡ä¸‹ç”Ÿæˆ 8â€“16 æ¡å“åº”ï¼Œä¸Šä¸‹æ–‡é•¿åº¦è¾¾ 24k

### **è¯„ä¼°æŒ‡æ ‡**
- **å‡†ç¡®æ€§**ï¼š
  - `Accuracy`ï¼ˆGSM8Kï¼‰
  - `Avg@1`ï¼ˆgreedy decodingï¼‰
  - `Avg@32`ï¼ˆé‡‡æ · 32 æ¡è·¯å¾„å–å¹³å‡ï¼‰
- **ååé‡**ï¼šqueries per secondï¼ˆQPSï¼‰
- **è®­ç»ƒåŠ¨æ€ç›‘æ§**ï¼š
  - KL æ•£åº¦ï¼ˆ$ D_{KL}(\pi_{\text{behav}} \| \pi_{\text{prox}}) $ï¼‰
  - è£å‰ªæ¯”ä¾‹ï¼ˆclipped fractionï¼‰
  - æƒé‡æ›´æ–°å¹…åº¦ vs é‡åŒ–è¯¯å·®

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| æ–¹æ³• | æè¿° |
|------|------|
| **Full-precision RL (BF16)** | å…¨ç²¾åº¦è®­ç»ƒï¼Œæ€§èƒ½ä¸Šé™åŸºå‡† |
| **Naive INT8/FP8 RL** | ç›´æ¥é‡åŒ– rolloutï¼Œæ— ä¿®æ­£æœºåˆ¶ |
| **FlashRL (Liu et al., 2025)** | ä½¿ç”¨ Truncated Importance Sampling (TIS) ç¼“è§£åˆ†å¸ƒåç§» |
| **QuRL w/o ACR** | ç§»é™¤ ACR æ¨¡å— |
| **QuRL w/o UAQ** | ç§»é™¤ UAQ æ¨¡å— |

---

## 3. **ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»**

#### ğŸ”¹ **GSM8K ç»“æœï¼ˆTable 1ï¼‰**
| æ–¹æ³• | Bitwidth | Accuracy |
|------|----------|----------|
| RL (BF16) | BF16 | 55.35 |
| RL (INT8) | INT8 | 48.78 |
| FlashRL | INT8 | 51.40 |
| **QuRL (Ours)** | **INT8** | **53.55** |
| FlashRL | FP8 | 53.60 |
| **QuRL (Ours)** | **FP8** | **54.28** |

> âœ… QuRL å°† INT8 æ€§èƒ½å·®è·ä» 6.57%ï¼ˆnaiveï¼‰ç¼©å°è‡³ **1.8%**ï¼ŒFP8 æ¥è¿‘ BF16ã€‚

#### ğŸ”¹ **AIME 2024 ç»“æœï¼ˆTable 2ï¼‰**
| æ–¹æ³• | Bitwidth | Avg@1 | Avg@32 |
|------|----------|--------|--------|
| RL (BF16) | BF16 | 33.33 | 31.67 |
| FlashRL | INT8 | 26.66 | 30.29 |
| **QuRL w/ UAQ** | **INT8** | **33.33** | **31.25** |
| FlashRL | FP8 | 30.00 | 32.60 |
| **QuRL w/ UAQ** | **FP8** | **33.33** | **33.27** |

> âœ… QuRL åœ¨ INT8 ä¸‹æ¢å¤åˆ° BF16 æ°´å¹³ï¼›FP8 ç”šè‡³ç•¥è¶…åŸºçº¿ã€‚

#### ğŸ”¹ **DeepScaleR ç»¼åˆç»“æœï¼ˆTable 3ï¼‰**
| æ–¹æ³• | Bitwidth | Avg Accuracy |
|------|----------|--------------|
| RL (BF16) | BF16 | 56.40 |
| RL (INT8) | INT8 | 52.31 |
| FlashRL | INT8 | 53.80 |
| **QuRL w/ UAQ** | **INT8** | **55.48** |

> âœ… QuRL åœ¨ INT8 ä¸‹æ¯” naive æå‡ **3.17%**ï¼Œæ¯” FlashRL æå‡ **1.68%**ï¼Œæ¥è¿‘ BF16 è¡¨ç°ã€‚

### **ååé‡æå‡ï¼ˆFigure 8ï¼‰**
| æ¨¡å‹ | GPU | åŠ é€Ÿæ¯”ï¼ˆINT8 vs BF16ï¼‰ |
|------|-----|------------------------|
| 7B | A6000/H100 | 1.2â€“1.4x |
| 14B | A6000/H100 | 1.3â€“1.6x |
| 32B | A100/H100 (TP=2) | **1.7â€“1.8x** |

> ğŸš€ æœ€å¤§æ¨¡å‹è·å¾—æœ€é«˜åŠ é€Ÿï¼ˆ**80%+ throughput gain**ï¼‰ï¼Œå› è®¡ç®—æ›´å¯†é›†ï¼Œå—ç›Šäºé‡åŒ–çŸ©é˜µä¹˜æ³•ä¼˜åŒ–ã€‚

### **æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰**
#### ğŸ”¹ **UAQ ç¼©æ”¾å› å­é€‰æ‹©ï¼ˆTable 4ï¼‰**
| Scale $ s $ | LR | Avg@32 |
|-------------|-----|--------|
| 1.0 | 1e-6 | 30.63 |
| **1.5** | **1e-6** | **31.25** âœ… |
| 2.0 | 1e-6 | 29.15 âŒï¼ˆè¿‡æ‹Ÿåˆï¼‰ |
| 1.0 | 1.5e-5 | 29.06 âŒï¼ˆå­¦ä¹ ç‡è¿‡å¤§ä¸ç¨³å®šï¼‰ |

> âœ… $ s=1.5 $ æ˜¯æœ€ä¼˜å¹³è¡¡ç‚¹ï¼šæ—¢æ”¾å¤§æ›´æ–°ï¼Œåˆä¸ç ´åè®­ç»ƒç¨³å®šæ€§ã€‚

#### ğŸ”¹ **æ¨¡å—æœ‰æ•ˆæ€§éªŒè¯**
- **ç§»é™¤ ACR**ï¼šè®­ç»ƒåæœŸ KL æ•£åº¦æ¿€å¢ï¼Œæ€§èƒ½éª¤é™ï¼ˆFig. 3ï¼‰
- **ç§»é™¤ UAQ**ï¼šæƒé‡æ›´æ–°å‡ ä¹è¢«é‡åŒ–å™ªå£°æ·¹æ²¡ï¼ˆFig. 4, 9ï¼‰

---

## 4. **å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **é‡åŒ– rollout å¯æ˜¾è‘—åŠ é€Ÿ RL è®­ç»ƒ**ï¼ˆ20%â€“80% throughput æå‡ï¼‰ï¼Œå°¤å…¶å¯¹å¤§æ¨¡å‹æ›´æ˜æ˜¾ã€‚
2. **ç›´æ¥é‡åŒ– rollout ä¼šå¯¼è‡´è®­ç»ƒå´©æºƒ**ï¼Œä¸»å› æ˜¯ç­–ç•¥æ¼‚ç§»å’Œæƒé‡æ›´æ–°ä¸¢å¤±ã€‚
3. **ACR æœ‰æ•ˆç»´æŒé•¿æœŸè®­ç»ƒç¨³å®šæ€§**ï¼Œé€šè¿‡åŠ¨æ€è£å‰ªé˜²æ­¢é‡è¦æ€§é‡‡æ ·åå·®ç´¯ç§¯ã€‚
4. **UAQ æˆåŠŸæ¡¥æ¥â€œæ›´æ–° vs é‡åŒ–â€å°ºåº¦é¸¿æ²Ÿ**ï¼Œåˆ©ç”¨ invariant scaling å®ç°é«˜æ•ˆä½æ¯”ç‰¹è®­ç»ƒã€‚
5. **QuRL åœ¨å¤šä¸ª RL ç®—æ³•ï¼ˆPPO, GRPO, DAPOï¼‰å’Œæ¨¡å‹è§„æ¨¡ï¼ˆ7Bâ€“32Bï¼‰ä¸Šå‡è¡¨ç°ä¼˜å¼‚**ï¼Œå…·å¤‡é€šç”¨æ€§ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ– vLLM ç­‰æ¨ç†å¼•æ“æ”¯æŒé‡åŒ– kernel**ï¼šå½“å‰ FP8 KV cache æœªä¼˜åŒ–ï¼Œé™åˆ¶è¿›ä¸€æ­¥åŠ é€Ÿã€‚
- **scaling factor $ s $ éœ€æ‰‹åŠ¨è°ƒå‚**ï¼šè™½ $ s=1.5 $ é€šç”¨æ€§å¥½ï¼Œä½†ä»éå®Œå…¨è‡ªåŠ¨åŒ–ã€‚
- **æœªæ¢ç´¢ä½äº INT8 çš„é‡åŒ–ï¼ˆå¦‚ INT4/W4A4ï¼‰**ï¼šå¯èƒ½é¢ä¸´æ›´å¤§å™ªå£°æŒ‘æˆ˜ã€‚
- **å‡è®¾ rollout ä¸è®­ç»ƒå¼•æ“ä¸€è‡´æ€§è¾ƒé«˜**ï¼šè‹¥å·®å¼‚è¿‡å¤§ï¼Œä»éœ€é¢å¤–å¯¹é½æœºåˆ¶ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- è‡ªåŠ¨åŒ– $ s $ çš„é€‰æ‹©ï¼Œç»“åˆè®­ç»ƒåŠ¨æ€åœ¨çº¿è°ƒæ•´ã€‚
- æ‰©å±•è‡³ **QLoRA + Quantized Rollout** è”åˆå‹ç¼©æ–¹æ¡ˆã€‚
- æ¢ç´¢ **asymmetric quantization** æˆ– **outlier-aware quantization** è¿›ä¸€æ­¥æå‡ç²¾åº¦ã€‚
- åº”ç”¨äº **multi-agent RL** æˆ– **long-horizon planning** åœºæ™¯ï¼ŒéªŒè¯æ³›åŒ–èƒ½åŠ›ã€‚

---

> âœ… **æ€»ä½“è¯„ä»·**ï¼š  
> QuRL æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§è§£å†³â€œé‡åŒ– rollout å¯¼è‡´ RL è®­ç»ƒä¸ç¨³å®šâ€çš„å·¥ä½œï¼Œæå‡ºäº†ç®€æ´è€Œæœ‰æ•ˆçš„ ACR ä¸ UAQ æœºåˆ¶ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°æ˜¾è‘—åŠ é€Ÿï¼Œä¸ºå¤§è§„æ¨¡ LLM çš„é«˜æ•ˆ RL è®­ç»ƒæä¾›äº†å®ç”¨è·¯å¾„ã€‚

</details>

---

### 2. [QuaRK: A Quantum Reservoir Kernel for Time Series Learning](https://arxiv.org/abs/2602.13531)

**Authors**: Abdallah Aaraba, Soumaya Cherkaoui, Ola Ahmad, Shengrui Wang  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.13531v1  

#### Abstract
Quantum reservoir computing offers a promising route for time series learning by modelling sequential data via rich quantum dynamics while the only training required happens at the level of a lightweight classical readout. However, studies featuring efficient and implementable quantum reservoir arch...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# QuaRK: A Quantum Reservoir Kernel for Time Series Learning è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
é‡å­å‚¨å±‚è®¡ç®—ï¼ˆQuantum Reservoir Computing, QRCï¼‰åœ¨æ—¶é—´åºåˆ—å­¦ä¹ ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨ä¸¤å¤§ç“¶é¢ˆï¼š
- **ç¡¬ä»¶å®ç°å›°éš¾**ï¼šè®¸å¤šQRCæ¶æ„ä¾èµ–æ˜‚è´µæˆ–éš¾ä»¥åœ¨çœŸå®è®¾å¤‡ä¸Šå®ç°çš„æµ‹é‡æ–¹æ¡ˆï¼ˆå¦‚å…¨æ€å±‚æï¼‰ã€‚
- **ç¼ºä¹ç†è®ºä¿éšœ**ï¼šç°æœ‰ç ”ç©¶ç¼ºå°‘å°†å…·ä½“å¯å®ç°çš„QRCæ¶æ„ä¸æ˜ç¡®çš„å­¦ä¹ ç†è®ºæ³›åŒ–ä¿è¯ç›¸ç»“åˆçš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚

æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæå‡ºä¸€ä¸ª**æ—¢å®ç”¨åˆå…·å¤‡ç†è®ºæ”¯æŒ**çš„æ—¶é—´åºåˆ—é‡å­å­¦ä¹ æ¡†æ¶ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
ä½œè€…æå‡ºäº† **QUARK**ï¼ˆQuantum Reservoir Kernelï¼‰ï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„é‡å­-ç»å…¸æ··åˆå­¦ä¹ æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š

1. **ç¡¬ä»¶ç°å®çš„é‡å­å‚¨å±‚ç‰¹å¾æå–å™¨ï¼ˆQuantum Reservoir Featurizerï¼‰**
   - é‡‡ç”¨**Contracted-Encoding Quantum Channels (CEQC)** æ¶æ„ï¼Œç¡®ä¿æ»¡è¶³å›å£°çŠ¶æ€æ€§è´¨ï¼ˆESPï¼‰å’Œè¡°å‡è®°å¿†æ€§è´¨ï¼ˆFMPï¼‰ï¼Œä»è€Œç¨³å®šå¤„ç†æ—¶åºä¾èµ–ã€‚
   - å¼•å…¥ **Johnson-Lindenstrauss (JL) æŠ•å½±**ï¼Œå°†é«˜ç»´è¾“å…¥æ•°æ®æŠ•å½±åˆ°ä¸é‡å­æ¯”ç‰¹æ•° $n$ åŒ¹é…çš„ä½ç»´ç©ºé—´ï¼Œ**è§£è€¦é‡å­èµ„æºä¸åŸå§‹æ•°æ®ç»´åº¦**ï¼Œæå‡å¯æ‰©å±•æ€§ã€‚
   - åˆ©ç”¨**ç©ºé—´å¤ç”¨ï¼ˆSpatial Multiplexingï¼‰**ï¼Œå¹¶è¡Œè¿è¡Œå¤šä¸ªå…·æœ‰ä¸åŒå‚æ•°çš„å­å‚¨å±‚ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚

2. **é«˜æ•ˆçš„ç‰¹å¾è¯»å‡ºæœºåˆ¶**
   - ä½¿ç”¨ **Classical Shadows** æŠ€æœ¯å¯¹å‚¨å±‚æœ«æ€è¿›è¡Œé«˜æ•ˆæµ‹é‡ï¼Œä»…éœ€å°‘é‡ç”µè·¯è¿è¡Œå³å¯åŒæ—¶ä¼°è®¡æ‰€æœ‰ $k$-å±€éƒ¨å¯è§‚æµ‹é‡ï¼ˆå¦‚2-local Pauliï¼‰çš„æœŸæœ›å€¼ï¼Œç”Ÿæˆç´§å‡‘ç‰¹å¾å‘é‡ã€‚
   - é¿å…äº†ä¼ ç»Ÿé‡å­æœºå™¨å­¦ä¹ ä¸­é«˜æ˜‚çš„æµ‹é‡æˆæœ¬ã€‚

3. **æ ¸æ–¹æ³•é©±åŠ¨çš„ç»å…¸è¯»å‡ºï¼ˆKernel-Based Readoutï¼‰**
   - åœ¨ç”Ÿæˆçš„é‡å­ç‰¹å¾ä¸Šåº”ç”¨**ç»å…¸æ ¸æ–¹æ³•**ï¼ˆMatÃ©rn kernelï¼‰ï¼Œæ„å»ºå†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰ä¸­çš„è¯»å‡ºå‡½æ•°ã€‚
   - é‡‡ç”¨**æ ¸å²­å›å½’ï¼ˆKernel Ridge Regressionï¼‰** è¿›è¡Œé—­å¼è®­ç»ƒï¼Œä¼˜åŒ–å¿«é€Ÿä¸”æ­£åˆ™åŒ–æ§åˆ¶æ˜ç¡®ã€‚

4. **å­¦ä¹ ç†è®ºåˆ†æ**
   - æä¾›äº†é’ˆå¯¹**å¼±ä¾èµ–æ—¶é—´åºåˆ—æ•°æ®**ï¼ˆå¦‚ $\beta$-mixing è¿‡ç¨‹ï¼‰çš„**æœ‰é™æ ·æœ¬æ³›åŒ–ç•Œ**ï¼ˆgeneralization boundï¼‰ã€‚
   - å°†è®¾è®¡é€‰æ‹©ï¼ˆå¦‚æŠ•å½±ç»´åº¦ã€å‚¨å±‚å¤§å°ã€æµ‹é‡é¢„ç®—ã€æ­£åˆ™åŒ–å¼ºåº¦ï¼‰ä¸å®é™…æ€§èƒ½è”ç³»èµ·æ¥ï¼Œä¸ºç³»ç»Ÿè®¾è®¡æä¾›åŸåˆ™æ€§æŒ‡å¯¼ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | QUARK | ä¼ ç»ŸQRCæ–¹æ³• |
|------|-------|-------------|
| **å¯æ‰©å±•æ€§** | âœ… é€šè¿‡JLæŠ•å½±è§£è€¦æ•°æ®ç»´ä¸é‡å­èµ„æº | âŒ é€šå¸¸ç›´æ¥ç¼–ç ï¼Œå—é™äºæ¯”ç‰¹æ•° |
| **æµ‹é‡æ•ˆç‡** | âœ… Classical Shadowsï¼Œæµ‹é‡å¼€é”€å° | âŒ å¸¸éœ€å¤§é‡æµ‹é‡æˆ–å…¨æ€å±‚æ |
| **è®­ç»ƒæ•ˆç‡** | âœ… æ ¸å²­å›å½’ï¼Œé—­å¼è§£ï¼Œæ— éœ€åå‘ä¼ æ’­ | âŒ å¯èƒ½éœ€è¦å¤æ‚ä¼˜åŒ– |
| **ç†è®ºä¿éšœ** | âœ… æ˜ç¡®çš„æ³›åŒ–ç•Œé€‚ç”¨äºéi.i.d.æ—¶åºæ•°æ® | âŒ å¤šæ•°ç¼ºä¹ä¸¥æ ¼ç†è®ºåˆ†æ |
| **çµæ´»æ€§** | âœ… æ ¸æ–¹æ³•å¤©ç„¶æ”¯æŒéçº¿æ€§å»ºæ¨¡ | âš ï¸ è¯»å‡ºå±‚å¯èƒ½å—é™ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **åˆæˆæ•°æ®é›†**ï¼šåŸºäº**å‘é‡è‡ªå›å½’æ»‘åŠ¨å¹³å‡æ¨¡å‹ï¼ˆVARMA(p=3, q=3)ï¼‰** ç”Ÿæˆå¤šå…ƒæ—¶é—´åºåˆ—ã€‚
  - è¾“å…¥ç»´åº¦ $d=3$ï¼Œå®šä¹‰åŸŸ $I = (-1,1)^d$ã€‚
  - è¿‡ç¨‹ä¸º**å¹³ç¨³ä¸”$\beta$-mixing**ï¼Œç¬¦åˆç†è®ºå‡è®¾ã€‚
  - é€šè¿‡ `tanh` å‡½æ•°ç¡®ä¿æœ‰ç•Œè¾“å‡ºã€‚
- **æ ‡ç­¾ç”Ÿæˆ**ï¼šç”±ä¸‰ç§ä¸åŒå¤æ‚åº¦çš„çœŸå®â€œåœ°åŸºâ€ï¼ˆground-truthï¼‰è¡°å‡è®°å¿†å‡½æ•° $H^*$ ç”Ÿæˆæ ‡é‡æ ‡ç­¾ $Y_t$ï¼š
  1. **å•æ­¥é¢„æµ‹ï¼ˆOne-step Forecastingï¼‰**ï¼š$H^*(X) = u^\top X_{t+1}$ï¼ˆçº¿æ€§ï¼‰
  2. **æŒ‡æ•°è¡°å‡çº¿æ€§å‡½æ•°ï¼ˆExponential Fadingï¼‰**ï¼š$H^*(X) = \sum_{k=0}^{w-1} \alpha^k u^\top X_{t-k}$
  3. **äºŒé˜¶æˆªæ–­Volterraå‡½æ•°ï¼ˆVolterraï¼‰**ï¼šåŒ…å«äºŒæ¬¡äº¤å‰æ»åé¡¹ï¼Œå½¢å¼ä¸º $\sum_k \sum_l \alpha^{k+l} (v^\top X_{t-k})(v^\top X_{t-l})$

### å®éªŒè®¾ç½®
- **çª—å£é•¿åº¦**ï¼š$w = 25$
- **é‡‡æ ·æ­¥å¹…**ï¼š$s = 100$ï¼ˆå³é—´éš” $g=75$ ä»¥å‡å°‘çª—å£é—´ä¾èµ–ï¼‰
- **é‡å­è®¾ç½®**ï¼š
  - æ¯ä¸ªå­å‚¨å±‚ä½¿ç”¨ $n=5$ ä¸ªé‡å­æ¯”ç‰¹ã€‚
  - å­å‚¨å±‚æ•°é‡ $R=3$ã€‚
  - æµ‹é‡ **2-local Pauliå¯è§‚æµ‹é‡**ã€‚
  - æ¯æ¬¡ç”µè·¯è¿è¡Œä½¿ç”¨ **1000æ¬¡æµ‹é‡shot** æ„å»ºClassical Shadowã€‚
- **æ ¸æ–¹æ³•**ï¼š
  - ä½¿ç”¨ **MatÃ©rnæ ¸**ï¼Œè¶…å‚æ•°ï¼ˆé•¿åº¦å°ºåº¦ $\ell$ã€å¹³æ»‘åº¦ $\nu$ï¼‰é€šè¿‡è½»é‡çº§éªŒè¯é›†è°ƒä¼˜åå›ºå®šã€‚
  - è¯»å‡ºè®­ç»ƒé‡‡ç”¨ **æ ¸å²­å›å½’**ï¼Œé€šè¿‡è°ƒèŠ‚æ­£åˆ™åŒ–å‚æ•° $\lambda_{\text{reg}}$ æ§åˆ¶RKHSèŒƒæ•°çº¦æŸ $A$ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **è®­ç»ƒå‡æ–¹è¯¯å·®ï¼ˆTrain MSEï¼‰**ï¼šç”¨äºéªŒè¯æ’å€¼ç°è±¡ï¼ˆinterpolation regimeï¼‰ã€‚
- **æµ‹è¯•å‡æ–¹è¯¯å·®ï¼ˆTest MSEï¼‰**ï¼šç”¨äºè¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼Œéšè®­ç»ƒçª—å£æ•°é‡ $N$ çš„å˜åŒ–è¶‹åŠ¿æ˜¯å…³é”®è§‚å¯Ÿç‚¹ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
è®ºæ–‡**æœªç›´æ¥ä¸å…¶ä»–QRCæˆ–ç»å…¸æ—¶åºæ¨¡å‹è¿›è¡Œæ€§èƒ½æ•°å€¼å¯¹æ¯”**ã€‚å…¶é‡ç‚¹åœ¨äºï¼š
- éªŒè¯è‡ªèº«æå‡ºçš„**ç†è®ºé¢„æµ‹**ï¼ˆå¦‚æ’å€¼è¡Œä¸ºã€æ³›åŒ–è¯¯å·®éš $N$ ä¸‹é™ï¼‰æ˜¯å¦åœ¨å®éªŒä¸­æˆç«‹ã€‚
- å±•ç¤ºæ–¹æ³•åœ¨ä¸åŒä»»åŠ¡å¤æ‚åº¦ä¸‹çš„è¡¨ç°è¶‹åŠ¿ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å‘ç°
1. **æ’å€¼ç°è±¡éªŒè¯ï¼ˆEffective Learningï¼‰**
   - **å›¾3** æ˜¾ç¤ºï¼Œéšç€æ­£åˆ™åŒ–å‡å¼±ï¼ˆ$\lambda_{\text{reg}}$ å‡å°ï¼Œå³å…è®¸æ›´å¤§çš„RKHSèŒƒæ•° $A$ï¼‰ï¼Œ**è®­ç»ƒMSEæ€¥å‰§ä¸‹é™å¹¶åœ¨ $\lambda_{\text{reg}} \sim 10^{-1}$ é™„è¿‘è¾¾åˆ°æ•°å€¼é›¶**ã€‚
   - è¿™è¯å®äº†**æ’å€¼åŒºåŸŸçš„å­˜åœ¨**ï¼Œå³æ¨¡å‹æœ‰èƒ½åŠ›å®Œç¾æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä¸ **Theorem 1** çš„é¢„æµ‹ä¸€è‡´ã€‚
   - **å›¾4** å±•ç¤ºäº†åœ¨æ’å€¼åŒºåŸŸå†…çš„è®­ç»ƒé›†é¢„æµ‹ï¼Œå‡ ä¹å®Œå…¨è´´åˆçœŸå®æ ‡ç­¾ï¼ˆMSE ~ $10^{-14}$ è‡³ $10^{-12}$ï¼‰ï¼Œè¯æ˜æ‹Ÿåˆéå¶ç„¶ã€‚

2. **æ³›åŒ–èƒ½åŠ›éªŒè¯ï¼ˆGeneralizationï¼‰**
   - **å›¾5** æ˜¾ç¤ºï¼Œåœ¨å›ºå®šå…¶ä»–è®¾ç½®ã€ä»…å¢åŠ è®­ç»ƒçª—å£æ•° $N$ çš„æƒ…å†µä¸‹ï¼Œ**æµ‹è¯•MSEéš $N$ å¢åŠ è€Œå•è°ƒä¸‹é™**ã€‚
   - ä¸‹é™è¶‹åŠ¿å¤§è‡´éµå¾ª $1/\sqrt{N}$ è§„å¾‹ï¼Œä¸ **Theorem 2** ä¸­çš„æ³›åŒ–ç•Œé¢„æµ‹ç›¸ç¬¦ã€‚
   - ä¸åŒä»»åŠ¡çš„æµ‹è¯•è¯¯å·®æ’åºåæ˜ äº†å…¶å†…åœ¨å¤æ‚åº¦ï¼š`Forecasting` < `Exponential` < `Volterra`ï¼Œç¬¦åˆé¢„æœŸã€‚

### æ¶ˆèå®éªŒç»“æœ
è®ºæ–‡**æœªåŒ…å«æ˜¾å¼çš„æ¶ˆèå®éªŒ**ï¼ˆå¦‚ç§»é™¤JLæŠ•å½±ã€å…³é—­ç©ºé—´å¤ç”¨ç­‰ï¼‰ã€‚å…¶éªŒè¯ä¸»è¦å›´ç»•**ç†è®ºé©±åŠ¨çš„å˜é‡æ‰«æ**ï¼š
- æ‰«ææ­£åˆ™åŒ–å‚æ•° $\lambda_{\text{reg}}$ æ¥éªŒè¯å®¹é‡ä¸æ’å€¼ã€‚
- æ‰«æè®­ç»ƒæ ·æœ¬æ•° $N$ æ¥éªŒè¯æ³›åŒ–è¡Œä¸ºã€‚
è¿™äº›å®éªŒé—´æ¥éªŒè¯äº†å„ç»„ä»¶ï¼ˆå¦‚æ ¸è¯»å‡ºçš„è¡¨è¾¾èƒ½åŠ›ã€æµ‹é‡æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼‰çš„å¿…è¦æ€§ï¼Œä½†æœªé‡åŒ–å•ä¸ªæ¨¡å—çš„è´¡çŒ®ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ç†è®ºä¸å®è·µçš„ä¸€è‡´æ€§**ï¼šQUARKæ¡†æ¶æˆåŠŸåœ°å°†**ç¡¬ä»¶å¯è¡Œçš„é‡å­å‚¨å±‚è®¾è®¡**ä¸**ä¸¥æ ¼çš„ç»Ÿè®¡å­¦ä¹ ç†è®º**ï¼ˆç‰¹åˆ«æ˜¯é’ˆå¯¹ä¾èµ–æ•°æ®çš„æ³›åŒ–ç•Œï¼‰ç»“åˆèµ·æ¥ï¼Œä¸”å®éªŒç»“æœä¸ç†è®ºé¢„æµ‹é«˜åº¦å»åˆã€‚
2. **æ’å€¼æ˜¯å¯å®ç°çš„**ï¼šå³ä½¿ä½¿ç”¨ä»…æœ‰5ä¸ªé‡å­æ¯”ç‰¹çš„å°å‹å‚¨å±‚ï¼Œé€šè¿‡é€‚å½“çš„JLæŠ•å½±å’Œå¼ºå¤§çš„æ ¸è¯»å‡ºï¼Œæ¨¡å‹ä¹Ÿèƒ½è¿›å…¥æ’å€¼åŒºåŸŸï¼Œè¡¨æ˜å…¶å…·æœ‰å¾ˆå¼ºçš„æ‹Ÿåˆèƒ½åŠ›ã€‚
3. **æ³›åŒ–è¡Œä¸ºå¯æ§**ï¼šåœ¨$\beta$-mixingå‡è®¾ä¸‹ï¼Œå¢åŠ è®­ç»ƒæ•°æ®é‡èƒ½æœ‰æ•ˆæå‡æ³›åŒ–æ€§èƒ½ï¼ŒéªŒè¯äº†æ‰€ææ³›åŒ–ç•Œçš„å®ç”¨æ€§ã€‚
4. **å¯æ‰©å±•æ€§è®¾è®¡æœ‰æ•ˆ**ï¼šJLæŠ•å½±æˆåŠŸè§£è€¦äº†æ•°æ®ç»´åº¦ä¸é‡å­èµ„æºï¼Œä¸ºç©ºé—´å¤ç”¨ç­‰å¢å¼ºè¡¨è¾¾åŠ›çš„æŠ€æœ¯æä¾›äº†åŸºç¡€ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **åˆæˆæ•°æ®éªŒè¯**ï¼šæ‰€æœ‰å®éªŒå‡åœ¨åˆæˆçš„VARMAæ•°æ®ä¸Šè¿›è¡Œï¼Œå°šæœªåœ¨çœŸå®ä¸–ç•Œå¤æ‚æ—¶åºï¼ˆå¦‚é‡‘èã€åŒ»ç–—ï¼‰ä¸ŠéªŒè¯ã€‚
2. **å™ªå£°å¿½ç•¥**ï¼šå®éªŒåœ¨ç†æƒ³æ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œï¼Œæœªè€ƒè™‘çœŸå®é‡å­ç¡¬ä»¶ä¸­çš„å™ªå£°ã€é€€ç›¸å¹²å’Œæœ‰é™æµ‹é‡shotå¸¦æ¥çš„åå·®ã€‚
3. **æ— ç›´æ¥åŸºçº¿å¯¹æ¯”**ï¼šç¼ºä¹ä¸ç»å…¸æ—¶åºæ¨¡å‹ï¼ˆå¦‚LSTMã€TCNï¼‰æˆ–å…¶ä»–QRCå˜ä½“çš„æ€§èƒ½æ¯”è¾ƒï¼Œéš¾ä»¥è¯„ä¼°å…¶ç›¸å¯¹ä¼˜åŠ¿ã€‚
4. **è¶…å‚æ•°è°ƒä¼˜æˆæœ¬**ï¼šè™½ç„¶æ ¸è¶…å‚åªè°ƒä¸€æ¬¡ï¼Œä½†åœ¨é‡å­ç‰¹å¾ç”Ÿæˆä»£ä»·é«˜çš„åœºæ™¯ä¸‹ï¼Œæ•´ä½“è°ƒä¼˜ä»å¯èƒ½æ˜‚è´µã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **å™ªå£°é²æ£’æ€§åˆ†æ**ï¼šå°†ç†è®ºåˆ†ææ‰©å±•åˆ°å«å™ªåœºæ™¯ï¼Œç ”ç©¶å™ªå£°å¯¹æ³›åŒ–ç•Œçš„å½±å“ã€‚
2. **çœŸå®æ•°æ®åº”ç”¨**ï¼šåœ¨æ›´é•¿ã€æ›´é«˜ç»´ã€éå¹³ç¨³çš„çœŸå®æ—¶é—´åºåˆ—ä¸Šæµ‹è¯•QUARKã€‚
3. **èµ„æº-ç²¾åº¦æƒè¡¡**ï¼šç³»ç»Ÿç ”ç©¶æµ‹é‡é¢„ç®—ã€å¯è§‚æµ‹é‡å±€åŸŸæ€§ $k$ã€å­å‚¨å±‚æ•° $R$ ç­‰å› ç´ å¯¹å‡†ç¡®ç‡çš„å…·ä½“å½±å“ã€‚
4. **è‡ªåŠ¨åŒ–è°ƒä¼˜**ï¼šå¼€å‘é’ˆå¯¹QUARKå„å±‚çº§ï¼ˆæŠ•å½±ã€å‚¨å±‚ã€æ ¸ï¼‰çš„è”åˆä¼˜åŒ–ç­–ç•¥ã€‚
5. **æ¢ç´¢å…¶ä»–æ ¸å‡½æ•°**ï¼šå°è¯•é™¤MatÃ©rnå¤–çš„å…¶ä»–æ ¸å‡½æ•°ä»¥é€‚åº”ä¸åŒæ•°æ®ç‰¹æ€§ã€‚

</details>

---

### 3. [Synergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specialization](https://arxiv.org/abs/2602.14159)

**Authors**: Rizhen Hu, Yuan Cao, Boao Kong, Mou Sun, Kun Yuan  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.14159v1  

#### Abstract
Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap -- redundant representations across experts and routing ambiguity, resulting in severely underutilized model capacity. While architectural solutions like DeepSeekMoE promote specialization, they requ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠSynergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specializationã€‹æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜

ç¨€ç– **Mixture-of-Experts (MoE)** æ¨¡å‹é€šè¿‡æ‰©å±•å‚æ•°é‡åŒæ—¶ä¿æŒæ¯ token è®¡ç®—æˆæœ¬æ’å®šï¼Œå·²æˆä¸ºé«˜æ•ˆæ‰©å±• **Transformer** çš„ä¸»æµæ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒMoE æ¨¡å‹åœ¨è®­ç»ƒä¸­æ™®éå­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š

- **Expert Overlapï¼ˆä¸“å®¶é‡å ï¼‰**ï¼šä¸åŒä¸“å®¶å¯¹ç›¸åŒ token äº§ç”Ÿé«˜åº¦ç›¸ä¼¼çš„æ¿€æ´»ï¼Œå¯¼è‡´åŠŸèƒ½å†—ä½™ï¼Œæµªè´¹æ¨¡å‹å®¹é‡ã€‚
- **Routing Ambiguityï¼ˆè·¯ç”±æ¨¡ç³Šï¼‰**ï¼šç›¸ä¼¼è¾“å…¥è¢«ä¸ä¸€è‡´åœ°åˆ†é…ç»™ä¸åŒä¸“å®¶ï¼Œå¯¼è‡´ä¸“å®¶è§’è‰²æ¨¡ç³Šï¼Œè·¯ç”±ç†µé«˜ã€‚

è¿™ä¸¤ä¸ªé—®é¢˜å…±åŒå¯¼è‡´ **ä¸“å®¶ä¸“ä¸šåŒ–ï¼ˆexpert specializationï¼‰** é€€åŒ–ï¼Œå‰Šå¼±äº† MoE çš„æ ¸å¿ƒä¼˜åŠ¿â€”â€”é€šè¿‡åˆ†å·¥æå‡æ•ˆç‡ä¸æ€§èƒ½ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

æœ¬æ–‡æå‡ºäº†ä¸€ç§**æŸå¤±å‡½æ•°ä¸ºä¸­å¿ƒï¼ˆloss-centricï¼‰** çš„è§£å†³æ–¹æ¡ˆï¼Œå¼•å…¥ä¸¤ä¸ªå³æ’å³ç”¨ï¼ˆplug-and-playï¼‰çš„æ­£åˆ™åŒ–æŸå¤±ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–è·¯ç”±å™¨è®¾è®¡ï¼š

#### (1) **Intra-Layer Specialization Loss (Rsp)**
- **ç›®æ ‡**ï¼šè§£å†³ **Expert Overlap**ã€‚
- **æœºåˆ¶**ï¼šæƒ©ç½šåŒä¸€ token åœ¨åŒä¸€å±‚ä¸­è¢«æ¿€æ´»çš„ä¸åŒä¸“å®¶ä¹‹é—´çš„ **SwiGLU ä¸­é—´æ¿€æ´»ï¼ˆintermediate activationï¼‰** çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
- **ä½œç”¨**ï¼šé¼“åŠ±ä¸“å®¶å‘å±•å‡ºäº’è¡¥çš„çŸ¥è¯†ï¼Œæ¨åŠ¨å…¶æ¢¯åº¦æ›´æ–°æ–¹å‘è¶‹äºæ­£äº¤ï¼Œå¢å¼ºå±‚å†…åŠŸèƒ½åˆ†åŒ–ã€‚

#### (2) **Cross-Layer Coupling Loss (Rcp)**
- **ç›®æ ‡**ï¼šè§£å†³ **Routing Ambiguity**ã€‚
- **æœºåˆ¶**ï¼šæœ€å¤§åŒ–ç›¸é‚»å±‚ä¹‹é—´ Top-k ä¸“å®¶å¯¹çš„è”åˆè·¯ç”±æ¦‚ç‡ï¼Œé¼“åŠ± token æ²¿ç€ä¸€è‡´çš„â€œä¸“å®¶è·¯å¾„â€ï¼ˆexpert pathï¼‰ä¼ æ’­ã€‚
- **ä½œç”¨**ï¼šå»ºç«‹è·¨å±‚çš„è¿è´¯ä¸“å®¶è·¯å¾„ï¼Œé™ä½è·¯ç”±ç†µï¼Œç¨³å®šä¸“å®¶èº«ä»½ï¼Œå¹¶ä½œä¸ºâ€œè‡ªç›‘ç£ä¿¡å·â€å°†æ·±å±‚æ›´ç¨³å®šçš„è·¯ç”±å†³ç­–åå‘ä¼ é€’åˆ°æµ…å±‚ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | æœ¬æ–‡æ–¹æ³• | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ DeepSeekMoE, ReMoEï¼‰ |
|------|----------|-------------------------------|
| **ä¿®æ”¹èŒƒå›´** | ä»…æ·»åŠ æŸå¤±é¡¹ï¼Œ**æ— éœ€ä¿®æ”¹æ¶æ„æˆ–è·¯ç”±å™¨** | éœ€è¦ä¿®æ”¹ä¸“å®¶å¸ƒå±€ã€å…±äº«ä¸“å®¶ã€æˆ–è·¯ç”±æœºåˆ¶ |
| **å…¼å®¹æ€§** | ä¸ä»»ä½• MoE æ¶æ„å…¼å®¹ï¼ˆvanilla æˆ– DeepSeek-styleï¼‰ | é€šå¸¸ä¸ºç‰¹å®šæ¶æ„è®¾è®¡ |
| **ç†è®ºåŸºç¡€** | æ˜ç¡®å»ºæ¨¡äº† specialization ä¸ routing çš„**æ­£åé¦ˆå¾ªç¯** | å¤šä¾èµ–ç»éªŒè®¾è®¡ï¼Œç¼ºä¹ç³»ç»Ÿç†è®ºåˆ†æ |
| **å®ç°éš¾åº¦** | ä»¥ **drop-in Megatron-LM æ¨¡å—** å®ç°ï¼Œä»…éœ€é…ç½®æ ‡å¿—ä½ | éœ€æ·±åº¦é›†æˆï¼Œä»£ç ä¾µå…¥æ€§å¼º |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼š**å³æ’å³ç”¨ã€æ¶æ„æ— å…³ã€ç†è®ºå¯è§£é‡Šã€æ•ˆæœæ˜¾è‘—**ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†

- **é¢„è®­ç»ƒæ•°æ®**ï¼š`C4-en` æ•°æ®é›†ã€‚
- **ä¸‹æ¸¸ä»»åŠ¡**ï¼š
  - **Zero-shot è¯„ä¼°**ï¼šBoolQ, ARC-Easy, ARC-Challenge, TruthfulQA-MC2, PIQA, MMLU, HellaSwagã€‚
  - **SFT è¯„ä¼°**ï¼šåŸºäºå†…éƒ¨è¯­æ–™è¿›è¡Œ LoRA å¾®è°ƒã€‚
  - **å…¨å‚æ•°å¾®è°ƒ**ï¼šåœ¨ `Qwen3-30B-A3B-Instruct-2507` ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹æ¶æ„
- **Vanilla MoE** å’Œ **DeepSeek-style MoE** ä¸¤ç§å˜ä½“ã€‚
- è§„æ¨¡è¦†ç›–ï¼šSmall (0.4B), Medium (1.1B), Large (7.0B)ã€‚
- ä½¿ç”¨ **SwiGLU**, **RoPE**, **RMSNorm** ç­‰ç°ä»£ç»„ä»¶ã€‚

#### æ­£åˆ™åŒ–é…ç½®
- åŸºçº¿ï¼š`L1b`ï¼ˆæ ‡å‡† load-balancing lossï¼‰
- å¯¹æ¯”ç»„ï¼š`L1b,z`, `L1b,sp`, `L1b,cp`, `L1b,sp,cp`, `L1b,z,sp,cp`
- è¶…å‚æ•°ï¼š`Î»_sp = 2e-3`, `Î»_cp = 1e-3`

#### è¯„ä¼°æŒ‡æ ‡
- **é¢„è®­ç»ƒ**ï¼šéªŒè¯é›† **Perplexity (â†“)**
- **ä¸‹æ¸¸ä»»åŠ¡**ï¼šå‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ã€pass@1ï¼ˆå¦‚ HumanEvalï¼‰
- **ä¸“ä¸šåŒ–ç¨‹åº¦**ï¼šè·¯ç”±ç†µã€ä¸“å®¶è·¯å¾„ä¸€è‡´æ€§ã€æ¿€æ´»ç›¸ä¼¼åº¦
- **æ¨ç†æ•ˆç‡**ï¼šååé‡ï¼ˆsamples/sï¼‰

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

- **æ ‡å‡†åŸºçº¿**ï¼šä»…ä½¿ç”¨ load-balancing loss (`L1b`)
- **å…ˆè¿›åŸºçº¿**ï¼š
  - `L1b,z`ï¼šåŠ å…¥ router z-loss
  - `L1b,o,v`ï¼šGuo et al. (2025a) æå‡ºçš„æ­£äº¤æ€§ä¸ logit æ–¹å·®æ­£åˆ™åŒ–
- **æ¶æ„çº§æ–¹æ³•**ï¼šDeepSeekMoE çš„ shared-expert è®¾è®¡

> æœ¬æ–‡æ–¹æ³•åœ¨ **ç›¸åŒæ¶æ„ä¸‹** ä¸è¿™äº›åŸºçº¿è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### (1) é¢„è®­ç»ƒ Perplexityï¼ˆè¶Šä½è¶Šå¥½ï¼‰

| æ¨¡å‹ | `L1b` | `L1b,z` | `L1b,sp,cp` | `L1b,z,sp,cp` |
|------|-------|--------|-------------|----------------|
| Vanilla MoE (Large) | 9.68 | 9.52 | 9.48 | **9.42** |
| DeepSeek-style (Large) | 9.56 | 9.46 | 9.47 | **9.39** |

> âœ… **ç›¸å¯¹æ”¹è¿›è¾¾ ~2.7%**ï¼Œä¸”ä¸ z-loss å åŠ åæ•ˆæœè¿›ä¸€æ­¥æå‡ã€‚

#### (2) ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼ˆå¹³å‡æå‡ï¼‰

| æ¨¡å‹ | LoRA SFT å¹³å‡æå‡ |
|------|------------------|
| DeepSeek-MoE-16B | **+3.9 pts** |
| DeepSeek-V2-Lite | **+4.6 pts** |

#### (3) å…¨å‚æ•°å¾®è°ƒï¼ˆQwen3-30B-A3Bï¼‰

| æŒ‡æ ‡ | `L1b` | `L1b,sp,cp` | æå‡ |
|------|------|--------------|------|
| HumanEval pass@1 | 92.07 | **95.73** | **+3.66** |
| GSM8K accuracy | 93.33 | 94.16 | +0.83 |

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

- **ä¼˜äº `L1b,z`**ï¼šåœ¨æ‰€æœ‰è®¾ç½®ä¸‹å‡å–å¾—æ›´ä½çš„å›°æƒ‘åº¦å’Œæ›´é«˜çš„ä¸‹æ¸¸æ€§èƒ½ã€‚
- **æ˜¾è‘—ä¼˜äº `L1b,o,v`**ï¼šè¯¥æ–¹æ³•ç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹åŠ£äºåŸºçº¿ï¼Œè¡¨æ˜ç›´æ¥æ”¾å¤§ logit æ–¹å·®å¯èƒ½ä¸ç¨³å®šã€‚
- **åª²ç¾ç”šè‡³è¶…è¶Š DeepSeek-style æ¶æ„**ï¼šåœ¨ vanilla MoE ä¸Šåº”ç”¨æœ¬æ–¹æ³•ï¼Œæ€§èƒ½å¯è¾¾åˆ°æˆ–è¶…è¿‡æœªä½¿ç”¨é¢å¤–æŸå¤±çš„ DeepSeek-style MoEï¼Œè¯´æ˜**è®­ç»ƒç›®æ ‡å¯éƒ¨åˆ†æ›¿ä»£æ¶æ„ä¿®æ”¹**ã€‚

---

### æ¶ˆèå®éªŒç»“æœ

| æŸå¤±ç»„åˆ | Vanilla MoE (Medium) PPL |
|---------|------------------------|
| `L1b` | 12.50 |
| `L1b,sp` | 12.44 |
| `L1b,cp` | 12.33 |
| `L1b,sp,cp` | **12.27** |

> âœ… ä¸¤ä¸ªæŸå¤±**ç‹¬ç«‹æœ‰æ•ˆä¸”ååŒå¢ç›Š**ï¼Œè”åˆä½¿ç”¨æ•ˆæœæœ€ä½³ã€‚

#### è¶…å‚æ•°æ•æ„Ÿæ€§
- åœ¨ `Î»_cp âˆˆ [2e-4, 2e-3]`, `Î»_sp âˆˆ [5e-4, 3e-3]` èŒƒå›´å†…ï¼Œæ€§èƒ½å˜åŒ–å°äº 1%ï¼Œè¡¨æ˜æ–¹æ³•**é²æ£’æ€§å¼º**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°

1. **ä¸“å®¶ä¸“ä¸šåŒ–å¯ä»¥ä½œä¸ºé¦–è¦è®­ç»ƒç›®æ ‡**ï¼šé€šè¿‡è®¾è®¡é’ˆå¯¹æ€§æŸå¤±å‡½æ•°ï¼Œè€Œéä¾èµ–æ¶æ„ä¿®æ”¹ï¼Œå³å¯æ˜¾è‘—æå‡ MoE æ€§èƒ½ã€‚
2. **Rsp ä¸ Rcp å½¢æˆæ­£åé¦ˆå¾ªç¯**ï¼š
   - Rsp å‡å°‘æ¿€æ´»é‡å  â†’ æå‡ä¸“å®¶å·®å¼‚ â†’ è·¯ç”±æ›´æœæ–­ï¼ˆç†µé™ä½ï¼‰ã€‚
   - Rcp å»ºç«‹ç¨³å®šè·¯å¾„ â†’ é™ä½è·¯ç”±æ¨¡ç³Š â†’ ä¸“å®¶è®­ç»ƒæ•°æ®æ›´çº¯å‡€ â†’ è¿›ä¸€æ­¥ä¿ƒè¿›ä¸“ä¸šåŒ–ã€‚
3. **å³æ’å³ç”¨çš„æœ‰æ•ˆæ€§**ï¼šæ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œä»…é€šè¿‡æ·»åŠ ä¸¤ä¸ªè½»é‡æŸå¤±ï¼Œå³å¯åœ¨å¤šç§ MoE æ¶æ„å’Œè§„æ¨¡ä¸Šå–å¾—ä¸€è‡´æ”¶ç›Šã€‚
4. **æ¨ç†åŠ é€Ÿ**ï¼šç”±äºè·¯ç”±æ›´ç¨³å®šã€è·¯å¾„æ›´ä¸€è‡´ï¼Œåœ¨ä¸“å®¶å¹¶è¡Œï¼ˆexpert parallelismï¼‰ä¸‹å¯å‡å°‘ All-to-All é€šä¿¡ï¼Œ**æ¨ç†ååæå‡æœ€é«˜è¾¾ 7%**ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§

- **ä¾èµ– Top-k è·¯ç”±æœºåˆ¶**ï¼šæ–¹æ³•å‡è®¾å­˜åœ¨æ˜ç¡®çš„ä¸“å®¶é€‰æ‹©è¿‡ç¨‹ï¼Œå¯¹å®Œå…¨åŠ¨æ€æˆ–è½¯è·¯ç”±çš„ MoE å˜ä½“é€‚ç”¨æ€§å¾…éªŒè¯ã€‚
- **è¶…å‚æ•°éœ€è°ƒä¼˜**ï¼šè™½ç„¶é²æ£’ï¼Œä½†åœ¨æç«¯è®¾ç½®ä¸‹ä»å¯èƒ½å½±å“ç¨³å®šæ€§ã€‚
- **æœªè§£å†³è´Ÿè½½å‡è¡¡çš„æ ¹æœ¬æŒ‘æˆ˜**ï¼šä»éœ€ä¾èµ– `L1b` ç­‰æœºåˆ¶é˜²æ­¢ä¸“å®¶è¿‡è½½ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. **æ‰©å±•åˆ°å…¶ä»–ä¸“å®¶ç±»å‹**ï¼šå¦‚æ³¨æ„åŠ›ä¸“å®¶ï¼ˆattention expertsï¼‰æˆ–æ··åˆä¸“å®¶ï¼ˆMoAï¼‰ã€‚
2. **åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡**ï¼šæ ¹æ®è®­ç»ƒé˜¶æ®µè‡ªåŠ¨è°ƒèŠ‚ `Î»_sp` å’Œ `Î»_cp`ã€‚
3. **ç»“åˆç¡¬ä»¶ä¼˜åŒ–**ï¼šåˆ©ç”¨ç¨³å®šçš„ä¸“å®¶è·¯å¾„è¿›è¡Œæ›´æ¿€è¿›çš„ç¼“å­˜ã€é¢„å–å’Œè®¾å¤‡æ”¾ç½®ç­–ç•¥ã€‚
4. **ç†è®ºæ·±åŒ–**ï¼šå»ºç«‹æ›´ç²¾ç¡®çš„æ”¶æ•›æ€§åˆ†æï¼Œé‡åŒ– specialization ä¸æ¨¡å‹å®¹é‡åˆ©ç”¨ç‡çš„å…³ç³»ã€‚

---

> **æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€å¥—ç®€å•ã€é€šç”¨ã€é«˜æ•ˆçš„ MoE ä¸“å®¶ä¸“ä¸šåŒ–è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ **intra-layer** å’Œ **cross-layer** ä¸¤ä¸ªæ­£åˆ™åŒ–æŸå¤±ï¼Œå®ç°äº†**æ€§èƒ½ã€ä¸“ä¸šåŒ–ã€æ•ˆç‡**çš„ä¸‰é‡æå‡ï¼Œä¸º MoE æ¨¡å‹çš„è®­ç»ƒæä¾›äº†æ–°çš„èŒƒå¼ã€‚

</details>

---

### 4. [Query as Anchor: Scenario-Adaptive User Representation via Large Language Model](https://arxiv.org/abs/2602.14492)

**Authors**: Jiahao Yuan, Yike Xu, Jinyong Wen, Baokun Wang, Ziyi Gao, Xiaotong Lin, Yun Liu, Xing Fu, Yu Cheng, Yongchao Liu, Weiqiang Wang, Zhongle Xie  
**Category**: cs.CL  
**Published**: 2026-02-17  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.14492v1  

#### Abstract
Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. F...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šQuery as Anchor: Scenario-Adaptive User Representation via Large Language Model**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å·¥ä¸šçº§ç”¨æˆ·è¡¨å¾å­¦ä¹ é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š
1. **åœºæ™¯é€‚åº”æ€§å·®**ï¼šä¼ ç»Ÿé™æ€åµŒå…¥ï¼ˆstatic embeddingsï¼‰æ— æ³•çµæ´»é€‚é…ä¸åŒä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚æ¨èã€é£æ§ã€è¥é”€ï¼‰ï¼Œå¯¼è‡´éœ€è¦ç»´æŠ¤å¤šä¸ªä¸“ç”¨æ¨¡å‹ï¼Œç³»ç»Ÿå¤æ‚åº¦é«˜ã€‚
2. **æ¨¡æ€ä¸è¯­ä¹‰é¸¿æ²Ÿ**ï¼šçœŸå®ç”¨æˆ·è¡Œä¸ºæ•°æ®ç¨€ç–ã€ç¬¦å·åŒ–ã€å¤šæºå¼‚æ„ï¼Œä¸LLMé¢„è®­ç»ƒæ‰€ç”¨çš„å¯†é›†æ–‡æœ¬æ•°æ®å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œé™åˆ¶äº†LLMåœ¨ç”¨æˆ·å»ºæ¨¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. **å™ªå£°ä¸å¼‚æ„æ•°æ®èåˆå›°éš¾**ï¼šå¤šæºè¡Œä¸ºæ—¥å¿—ï¼ˆæ”¯ä»˜ã€æœç´¢ã€å°ç¨‹åºç­‰ï¼‰ä¸­å­˜åœ¨å¤§é‡æ— å…³ä¿¡å·å’Œå†²çªæ¨¡æ€ï¼Œéš¾ä»¥æœ‰æ•ˆèšåˆã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**
ä½œè€…æå‡º **Query-as-Anchor (Q-Anchor)** æ¡†æ¶ï¼Œå°†ç”¨æˆ·å»ºæ¨¡ä»â€œé™æ€ç¼–ç â€è½¬å˜ä¸ºâ€œåŠ¨æ€æŸ¥è¯¢æ„ŸçŸ¥åˆæˆâ€ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- **ä»¥è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä½œä¸ºé”šç‚¹ï¼ˆquery as anchorï¼‰**ï¼Œé€šè¿‡åŒä¸€ä¸ªç”¨æˆ·è¡Œä¸ºåºåˆ—ï¼Œåœ¨ä¸åŒqueryå¼•å¯¼ä¸‹ç”Ÿæˆä¸åŒçš„ã€åœºæ™¯è‡ªé€‚åº”çš„ç”¨æˆ·åµŒå…¥ã€‚
- æ„å»º **UserU** æ•°æ®é›†ï¼Œç»“åˆæœªæ¥è¡Œä¸ºé¢„æµ‹ä¸LLMç”Ÿæˆçš„é—®ç­”å¯¹ï¼Œä¸ºç”¨æˆ·ç†è§£æä¾›å¼ºç›‘ç£ä¿¡å·ã€‚
- è®¾è®¡ **Q-Anchor Embedding** æ¶æ„ï¼Œé‡‡ç”¨åŒå¡”ç»“æ„ + å±‚æ¬¡åŒ–ç²—åˆ°ç»†ç¼–ç å™¨ï¼Œå®ç°å¤šæ¨¡æ€è¡Œä¸ºä¸è¯­ä¹‰æŒ‡ä»¤çš„å¯¹é½ã€‚
- å¼•å…¥ **Cluster-based Soft Prompt Tuning**ï¼Œåœ¨ä¸å¾®è°ƒä¸»å¹²ç½‘ç»œçš„å‰æä¸‹ï¼Œé€šè¿‡å¯å­¦ä¹ soft promptå®ç°è½»é‡çº§åœºæ™¯ä¸“ä¸šåŒ–ã€‚
- åˆ©ç”¨ **KV-cache åŠ é€Ÿæ¨ç†**ï¼šå°†ç”¨æˆ·å‰ç¼€ä¸€æ¬¡æ€§ç¼–ç å¹¶ç¼“å­˜ï¼Œåç»­åªéœ€è®¡ç®—queryåç¼€ï¼Œå®ç°ä½å»¶è¿Ÿå¤šåœºæ™¯æœåŠ¡ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | Q-Anchor |
|------|--------|---------|
| **çµæ´»æ€§** | é™æ€åµŒå…¥ï¼Œå›ºå®šä¸å˜ | åŠ¨æ€å“åº”ä¸åŒqueryï¼Œä¸€æ¨¡å‹æ”¯æŒå¤šåœºæ™¯ |
| **æ³›åŒ–æ€§** | ä¾èµ–å¤§è§„æ¨¡å‚æ•°æˆ–ç‰¹å®šæ¶æ„ | é€šè¿‡é¢„è®­ç»ƒ+prompt tuningå®ç°é«˜æ•ˆè¿ç§» |
| **éƒ¨ç½²æ•ˆç‡** | å¤šæ¨¡å‹å¹¶è¡Œï¼Œèµ„æºæ¶ˆè€—å¤§ | å•æ¨¡å‹å…±äº«å‰ç¼€ï¼Œå¢é‡æˆæœ¬æä½ |
| **å¯è§£é‡Šæ€§** | é»‘ç®±ç‰¹å¾èåˆ | prompt tuningå¸¦æ¥æ³¨æ„åŠ›åç§»ï¼Œå¯è§†åŒ–éªŒè¯ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **UserU**ï¼šå·¥ä¸šçº§é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š
  - **Dfuture**ï¼šåŸºäºå†å²è¡Œä¸ºé¢„æµ‹æœªæ¥åŠ¨ä½œçš„ç›‘ç£æ ·æœ¬ï¼ˆè¡Œä¸ºâ†’è¡Œä¸ºï¼‰ã€‚
  - **Duqa**ï¼šç”±LLMç”Ÿæˆçš„â€œç”¨æˆ·è¡Œä¸ºâ†’è¯­ä¹‰ç†è§£â€é—®ç­”å¯¹ï¼Œè¦†ç›–72ä¸ªç”Ÿæ´»ä¸»é¢˜ï¼ˆå¦‚è´¢åŠ¡è§„åˆ’ã€å¥åº·ä¹ æƒ¯ï¼‰ã€‚
- **Alipay ç”Ÿäº§ç¯å¢ƒæµ‹è¯•åŸºå‡†**ï¼š10ä¸ªçœŸå®äºŒåˆ†ç±»ä»»åŠ¡ï¼Œæ¶µç›–ä¸‰å¤§é¢†åŸŸï¼š
  - **User Engagement**ï¼ˆæ´»è·ƒåº¦é¢„æµ‹ï¼‰
  - **Risk Control**ï¼ˆæ¬ºè¯ˆã€æ´—é’±æ£€æµ‹ï¼‰
  - **Marketing Sensitivity**ï¼ˆå¤–å–ã€å“ç‰Œåå¥½è¯†åˆ«ï¼‰

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**
- **æ¨¡å‹æ¶æ„**ï¼šåŸºäº `Qwen2.5-0.5B-Instruct` ä½œä¸ºLLM backboneï¼Œé…åˆ modality-specific encodersï¼ˆå¦‚ gte-baseï¼‰å¤„ç†å¤šæºè¡Œä¸ºã€‚
- **è®­ç»ƒé…ç½®**ï¼š
  - é¢„è®­ç»ƒæ­¥æ•°ï¼š50k steps
  - Batch sizeï¼š2048
  - ä½¿ç”¨ LoRAï¼ˆrank=64, Î±=32ï¼‰è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
  - ç”¨æˆ·åµŒå…¥ç»´åº¦ï¼š128
- **è¯„ä¼°æ–¹å¼**ï¼š
  - **Linear Probing**ï¼šå†»ç»“ç”¨æˆ·åµŒå…¥ï¼Œè®­ç»ƒçº¿æ€§åˆ†ç±»å™¨è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°ã€‚
  - **ä¸»è¦æŒ‡æ ‡**ï¼š
    - **AUC**ï¼ˆROCæ›²çº¿ä¸‹é¢ç§¯ï¼‰ï¼šè¡¡é‡åˆ†ç±»æ€§èƒ½
    - **KS**ï¼ˆKolmogorov-Smirnovï¼‰ï¼šè¡¡é‡æ­£è´Ÿæ ·æœ¬æ’åºåˆ†ç¦»åº¦ï¼Œå°¤å…¶é€‚ç”¨äºé£æ§åœºæ™¯

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
åˆ†ä¸ºä¸¤ç±»ï¼š
1. **é€šç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹**ï¼š
   - Qwen2.5-0.5B-Instructï¼ˆæ— å¾®è°ƒï¼‰
   - Qwen3-Embedding-8B
   - Llama-Embed-Nemotron-8B
   - KaLM-Embedding-Gemma3-12B
2. **ä¸“ç”¨ç”¨æˆ·è¡¨å¾æ¨¡å‹**ï¼š
   - MSDP, One4all, CPCï¼ˆå¯¹æ¯”å­¦ä¹ ç±»ï¼‰
   - FOUNDï¼ˆåŸºäºLLMçš„ç”¨æˆ·åŸºç¡€æ¨¡å‹ï¼‰

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
| æ–¹æ³• | å¹³å‡ AUC | å¹³å‡ KS |
|------|--------|-------|
| Llama-Embed-Nemotron-8B | 0.7488 | 0.3805 |
| FOUND | 0.7832 | 0.4529 |
| Q-Anchor (Base) | **0.8104** | **0.5044** |
| Q-Anchor (Prompt Tuned) | **0.8225** | **0.5267** |

> âœ… **ç›¸å¯¹æœ€å¼ºåŸºçº¿æå‡**ï¼š
> - AUC â†‘ +9.84%ï¼ˆvs Llama-Embedï¼‰
> - KS â†‘ +38.4%ï¼ˆvs Llama-Embedï¼‰

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- åœ¨æ‰€æœ‰10ä¸ªä»»åŠ¡ä¸Šå‡å–å¾— **SOTA è¡¨ç°**ï¼Œå°¤å…¶åœ¨ **Risk å’Œ Marketing** åœºæ™¯ä¼˜åŠ¿æ˜¾è‘—ã€‚
- **Money Laundering Detection**ï¼šAUC è¾¾ 0.9439ï¼Œè¿œè¶… FOUND (0.9235) å’Œ MSDP (0.8746)ï¼Œè¯´æ˜èƒ½æœ‰æ•ˆæŠ‘åˆ¶äº¤æ˜“å™ªå£°ï¼Œèšç„¦å†³ç­–ç›¸å…³æ¨¡å¼ã€‚
- **Brand Sensitivity**ï¼šAUC ä» Base çš„ 0.7979 æå‡è‡³ Prompt Tuned çš„ 0.8535ï¼Œè¡¨æ˜æ¨¡å‹èƒ½æ•æ‰ç»†å¾®åå¥½ä¿¡å·ï¼Œå¹¶é€šè¿‡prompt tuningç²¾å‡†è°ƒæ•´è¾¹ç•Œã€‚

### **æ¶ˆèå®éªŒç»“æœ**
#### ï¼ˆ1ï¼‰ç»„ä»¶æ¶ˆèï¼ˆQ-Anchor Baseï¼‰
| ç§»é™¤ç»„ä»¶ | Avg AUC â†“ | Avg KS â†“ | ç»“è®º |
|--------|----------|--------|------|
| User Token | 0.8086 | 0.5002 | æ˜¾å¼ç»“æ„æ ‡è®°æœ‰åŠ©äºè¯æ®æº¯æº |
| Modal Token | 0.8088 | 0.5008 | åŒä¸Š |
| Both Tokens | 0.8065 | 0.4966 | ç»“æ„ä¿¡æ¯é‡è¦ |
| Contrastive Loss | 0.7667 | 0.4215 | å¯¹æ¯”å­¦ä¹ æ˜¯ä¸»å¯¼ä¿¡å· |
| NTP Loss | 0.8061 | 0.4961 | è¾…åŠ©ä»»åŠ¡å¢å¼ºå±€éƒ¨å»ºæ¨¡ |
| Margin Filter | 0.8047 | 0.4877 | å‡å°‘å‡è´Ÿä¾‹å¹²æ‰° |

#### ï¼ˆ2ï¼‰é¢„è®­ç»ƒå¿…è¦æ€§
- **without pretrain**ï¼šAUC é™è‡³ 0.7782ï¼ŒKS ä¸‹é™ 11.2%ï¼Œè¯æ˜é¢„è®­ç»ƒæä¾›äº†å…³é”®çš„è¡Œä¸ºå…ˆéªŒï¼ˆbehavioral priorï¼‰ï¼Œä¸å¯æ›¿ä»£ã€‚

#### ï¼ˆ3ï¼‰Prompt Tuning å¯æ‰©å±•æ€§åˆ†æ**
- **Prompt Tokens æ•°é‡**ï¼šæ€§èƒ½åœ¨ **6ä¸ªtokenæ—¶é¥±å’Œ**ï¼Œæ›´å¤štokenæ”¶ç›Šé€’å‡ã€‚
- **Training Steps**ï¼š500æ­¥è¾¾åˆ°æœ€ä¼˜ï¼Œæ”¶æ•›ç¨³å®šã€‚
- ç»“è®ºï¼š**è½»é‡çº§promptå³å¯å®ç°é«˜æ•ˆä¸“ä¸šåŒ–**ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **Query-as-Anchor å®ç°â€œä¸€æ¨¡å‹å¤šç”¨â€èŒƒå¼**ï¼š
   - åŒä¸€å¥—ç¼–ç å™¨å¯åœ¨ä¸åŒqueryä¸‹ç”Ÿæˆé€‚é…åœºæ™¯çš„åµŒå…¥ï¼Œæ‰“ç ´â€œä¸€ä¸ªä»»åŠ¡ä¸€ä¸ªæ¨¡å‹â€çš„åƒµå±€ã€‚
2. **é¢„è®­ç»ƒè´¨é‡ > æ¨¡å‹è§„æ¨¡**ï¼š
   - åœ¨å›ºå®šé¢„ç®—ä¸‹ï¼Œ**å¢åŠ æ•°æ®é‡æ¯”æ‰©å¤§æ¨¡å‹æ›´æœ‰æ•ˆ**ï¼›0.5B æ¨¡å‹ä¼˜äº 1.5B/3Bã€‚
   - æ›´å¤§æ¨¡å‹æ¢¯åº¦è¡°å‡ä¸¥é‡ï¼ˆgradient attenuationï¼‰ï¼Œä¼˜åŒ–å›°éš¾ã€‚
3. **Soft Prompt Tuning æ˜¯é«˜æ•ˆçš„åœºæ™¯é€‚é…æœºåˆ¶**ï¼š
   - ä»…æ›´æ–°å°‘é‡promptå‘é‡å³å¯æ˜¾è‘—æå‡æ€§èƒ½ï¼Œä¸”å¯é€šè¿‡æ³¨æ„åŠ›å˜åŒ–è§£é‡Šå…¶ä½œç”¨ï¼ˆå¦‚Takeoutä»»åŠ¡ä¸­Billæ¨¡æ€å…³æ³¨åº¦â†‘26%ï¼‰ã€‚
4. **KV-cache å®ç°å·¥ä¸šçº§é«˜æ•ˆéƒ¨ç½²**ï¼š
   - ç”¨æˆ·å‰ç¼€åªç¼–ç ä¸€æ¬¡ï¼Œæ–°å¢åœºæ™¯å‡ ä¹é›¶é¢å¤–å¼€é”€ï¼Œé€‚åˆé«˜å¹¶å‘çº¿ä¸Šç³»ç»Ÿã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–é«˜è´¨é‡promptè®¾è®¡**ï¼šè™½ç„¶ä½¿ç”¨LLMè‡ªåŠ¨ç”ŸæˆQAå¯¹ï¼Œä½†åœ¨æç«¯é•¿å°¾åœºæ™¯å¯èƒ½ä»éœ€äººå·¥å¹²é¢„ã€‚
- **å¯¹queryè¯­ä¹‰æ•æ„Ÿ**ï¼šè‹¥queryè¡¨è¿°æ¨¡ç³Šæˆ–æ­§ä¹‰ï¼Œå¯èƒ½å¯¼è‡´åµŒå…¥æ¼‚ç§»ã€‚
- **å½“å‰æœªæ¢ç´¢å¤šqueryè”åˆæ¨ç†**ï¼šç›®å‰æ¯æ¬¡ä»…å¤„ç†å•ä¸€queryã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **çªç ´Scaling Paradox**ï¼š
   - æ¢ç´¢å¦‚ä½•æ¢å¤å¤§æ¨¡å‹ï¼ˆ>1Bï¼‰çš„è®­ç»ƒæ¢¯åº¦ï¼Œä½¿å…¶ä¹Ÿèƒ½å—ç›Šäºæ›´å¤§å®¹é‡ã€‚
2. **åŠ¨æ€Promptç”Ÿæˆ**ï¼š
   - è‡ªåŠ¨æ ¹æ®ä¸šåŠ¡ç›®æ ‡ç”Ÿæˆæœ€ä¼˜promptï¼Œå‡å°‘äººå·¥ä¾èµ–ã€‚
3. **è·¨å¹³å°è¿ç§»èƒ½åŠ›éªŒè¯**ï¼š
   - å½“å‰åŸºäºAlipayç”Ÿæ€æ„å»ºï¼Œæœªæ¥å¯åœ¨å…¶ä»–å¹³å°ï¼ˆå¦‚ç”µå•†ã€ç¤¾äº¤ï¼‰éªŒè¯æ³›åŒ–æ€§ã€‚
4. **å¼•å…¥æ—¶åºæ˜¾å¼å»ºæ¨¡**ï¼š
   - å½“å‰å±‚æ¬¡ç¼–ç éšå«æ—¶é—´ä¿¡æ¯ï¼Œæœªæ¥å¯æ˜¾å¼åŠ å…¥æ—¶é—´æˆ³æˆ–å‘¨æœŸæ€§ä¿¡å·ã€‚

---

> ğŸ”— **ä»£ç å·²å¼€æº**ï¼š[https://github.com/JhCircle/Q-Anchor](https://github.com/JhCircle/Q-Anchor)  
> ğŸ“Œ **å·¥ä¸šè½åœ°éªŒè¯**ï¼šåœ¨Alipayä¸¤å¤§åœºæ™¯A/Bæµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼š
> - **IVRç°é‡‘å¬å›**ï¼šææ¬¾ç‡â†‘12.5%ï¼Œä½™é¢â†‘5.3%
> - **ä¿¡è´·é€¾æœŸé£é™©è¯†åˆ«**ï¼šKS â†‘1.96%

</details>

---

### 5. [Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws](https://arxiv.org/abs/2602.14208)

**Authors**: Jinbo Wang, Binghui Li, Zhanpeng Zhou, Mingze Wang, Yuxuan Sun, Jiaqi Zhang, Xunliang Cai, Lei Wu  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.14208v1  

#### Abstract
Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šFast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
æœ¬æ–‡ç ”ç©¶äº†å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­çš„ **Batch Size Scheduling (BSS)** é—®é¢˜ï¼Œæ—¨åœ¨ä»ç†è®ºä¸Šè§£é‡Šä¸ºä½•åœ¨å®é™…å¤§æ¨¡å‹é¢„è®­ç»ƒï¼ˆå¦‚ GPT-3ã€PaLMã€LLaMA-3 ç­‰ï¼‰ä¸­å¹¿æ³›é‡‡ç”¨åŠ¨æ€è°ƒæ•´ batch size çš„ç­–ç•¥ï¼Œå¹¶æ­ç¤ºå…¶æœ€ä¼˜è°ƒåº¦ç»“æ„ã€‚

å°½ç®¡ BSS åœ¨å·¥ä¸šç•Œå¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ç†è®ºåŸºç¡€é•¿æœŸç¼ºä¹ç³»ç»Ÿåˆ†æï¼Œç°æœ‰å·¥ä½œå¤šä¾èµ–ç»éªŒè°ƒå‚æˆ–å¯å‘å¼è®¾è®¡ã€‚æœ¬æ–‡å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œé¦–æ¬¡åŸºäº **Functional Scaling Law (FSL)** æ¡†æ¶å¯¹ BSS è¿›è¡Œäº†åŸåˆ™æ€§çš„ç†è®ºå»ºæ¨¡ä¸ä¼˜åŒ–åˆ†æã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

#### ï¼ˆ1ï¼‰æå‡º **Functional Scaling Law (FSL)** æ¡†æ¶ä¸‹çš„ BSS æœ€ä¼˜è°ƒåº¦ç†è®º
- å°† BSS å»ºæ¨¡ä¸ºä¸€ä¸ªå—æ•°æ®é¢„ç®—çº¦æŸçš„å˜åˆ†ä¼˜åŒ–é—®é¢˜ã€‚
- æ¨å¯¼å‡ºåœ¨ä¸åŒä»»åŠ¡éš¾åº¦ä¸‹ï¼ˆç”± source exponent $s$ å†³å®šï¼‰ï¼Œæœ€ä¼˜ batch size schedule çš„è§£æå½¢å¼ï¼š
  - **Easy-task regime ($s > 1 - 1/\beta$)**ï¼šbatch size åº”åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å•è°ƒé€’å¢ã€‚
  - **Hard-task regime ($s \leq 1 - 1/\beta$)**ï¼šåº”ä¿æŒå° batch size è®­ç»ƒå¤§éƒ¨åˆ†æ—¶é—´ï¼Œä»…åœ¨åæœŸåˆ‡æ¢åˆ°å¤§ batch sizeï¼ˆç§°ä¸ºâ€œlate switchingâ€ï¼‰ã€‚

#### ï¼ˆ2ï¼‰å‘ç°å¹¶å‘½å **Fast Catch-Up Effect**
- å½“ä»å° batch åˆ‡æ¢åˆ°å¤§ batch åï¼Œloss ä¼šè¿…é€Ÿâ€œåç¼©â€è‡³ä¸€ç›´ä½¿ç”¨å¤§ batch è®­ç»ƒçš„è½¨è¿¹ä¸Šã€‚
- è¿™ç§ç°è±¡æºäºæ¨¡å‹å¯¹æ—©æœŸæ¢¯åº¦å™ªå£°çš„å¿«é€Ÿé—å¿˜ï¼ˆforgetting of accumulated gradient noiseï¼‰ï¼Œä¸”ä»»åŠ¡è¶Šéš¾ï¼ˆ$s$ è¶Šå°ï¼‰ï¼Œcatch-up è¶Šå¿«ã€‚
- è¯¥æœºåˆ¶è§£é‡Šäº†ä¸ºä½•å¯ä»¥å®‰å…¨åœ°å°†å¤§ batch æ¨è¿Ÿåˆ°åæœŸè€Œä¸æŸå¤±æ€§èƒ½ã€‚

#### ï¼ˆ3ï¼‰æå‡º **Late-Switching Principle**
- å¯¹äºå›°éš¾ä»»åŠ¡ï¼Œå»¶è¿Ÿå¢å¤§ batch size ä¸ä»…ä¸ä¼šæŸå®³æœ€ç»ˆæ€§èƒ½ï¼Œåè€Œèƒ½æ˜¾è‘—å‡å°‘ token æ¶ˆè€—ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚
- è¯¥åŸåˆ™å…·æœ‰å¼ºæ³›åŒ–æ€§ï¼Œåœ¨ Dense å’Œ MoE æ¶æ„ã€å¤šç§è§„æ¨¡ä¸‹å‡æˆç«‹ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹é¢ | ç°æœ‰æ–¹æ³• | æœ¬æ–‡æ–¹æ³• |
|------|--------|---------|
| **ç†è®ºåŸºç¡€** | ç¼ºä¹ç»Ÿä¸€æ¡†æ¶ï¼Œä¾èµ– heuristics æˆ– control theory | åŸºäº FSL æä¾›å¯è§£é‡Šã€å¯é¢„æµ‹çš„è§£æè§£ |
| **æŒ‡å¯¼æ„ä¹‰** | éœ€å¤§é‡ trial-and-error è°ƒå‚ | å¯é€šè¿‡å°è§„æ¨¡å®éªŒæ‹Ÿåˆ scaling law æ¥å¤–æ¨æœ€ä¼˜åˆ‡æ¢ç‚¹ |
| **æ•ˆç‡ä¼˜åŠ¿** | å›ºå®š batch size æµªè´¹è®¡ç®—èµ„æº | Late-switch å‡å°‘å‰æœŸé«˜æˆæœ¬é€šä¿¡å¼€é”€ï¼ŒèŠ‚çœ token |
| **é€‚ç”¨æ€§** | å¤šé’ˆå¯¹ constant batch size | æ˜ç¡®åŒºåˆ† easy/hard task å¹¶ç»™å‡ºå·®å¼‚åŒ–ç­–ç•¥ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†

| å®éªŒç±»å‹ | æ•°æ®é›† | æè¿° |
|--------|-------|------|
| **Small-scale LLM å®éªŒ** | C4 dataset | å…¬å…±æ–‡æœ¬è¯­æ–™ï¼Œç”¨äºé¢„è®­ç»ƒ 50Mâ€“492M å‚æ•°çš„ LLaMA æ¨¡å‹ |
| **Large-scale LLM å®éªŒ** | ç§æœ‰çœŸå®ä¸–ç•Œ LLM æ•°æ®é›† | æ›´è´´è¿‘å·¥ä¸šçº§éƒ¨ç½²åœºæ™¯ï¼Œè®­ç»ƒé«˜è¾¾ 1.1B å‚æ•°çš„ MoE æ¨¡å‹ |
| **ç†è®ºéªŒè¯å®éªŒ** | Feature-space linear regression | åˆæˆæ•°æ®ï¼Œç”¨äºéªŒè¯ FSL æ¡†æ¶ä¸‹çš„ç†è®ºé¢„æµ‹ |

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹æ¶æ„
- **Dense æ¨¡å‹**ï¼šLLaMA æ¶æ„ï¼ˆRoPE, SwiGLU, RMSNormï¼‰
- **Sparse æ¨¡å‹**ï¼šShortcut-connected MoEï¼ˆScMoEï¼‰ï¼Œæ”¯æŒé«˜æ•ˆä¸“å®¶å¹¶è¡Œ

#### è®­ç»ƒé…ç½®
- **Optimizer**ï¼šAdamWï¼ˆ$\beta_1=0.9$, $\beta_2=0.95$, weight decay=0.1ï¼‰
- **Sequence Length**ï¼š256ï¼ˆsmall-scaleï¼‰ã€8192ï¼ˆlarge-scaleï¼‰
- **Learning Rate**ï¼šå¸¸æ•°æˆ– cosine decayï¼ˆæ‰©å±•å®éªŒï¼‰
- **Batch Size Schedule**ï¼šä¸¤é˜¶æ®µæˆ–å¤šé˜¶æ®µåˆ‡æ¢ï¼ˆe.g., 640 â†’ 1280ï¼‰

#### è¯„ä¼°æŒ‡æ ‡
- **ä¸»æŒ‡æ ‡**ï¼šValidation lossï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- **è¾…åŠ©åˆ†æ**ï¼š
  - Loss æ›²çº¿å¯¹é½æƒ…å†µï¼ˆéªŒè¯ fast catch-upï¼‰
  - ä¸åŒ switching point ä¸‹çš„æœ€ç»ˆ loss
  - Scaling law æ‹Ÿåˆç¨‹åº¦ï¼ˆ$\log(D-P)$ vs $\log D$ï¼‰

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿æ–¹æ³• | æè¿° |
|--------|------|
| **Constant Small Batch** | å…¨ç¨‹ä½¿ç”¨æœ€å° batch sizeï¼Œæ ·æœ¬æ•ˆç‡é«˜ä½†ç¡¬ä»¶åˆ©ç”¨ç‡ä½ |
| **Constant Large Batch** | å…¨ç¨‹ä½¿ç”¨æœ€å¤§ batch sizeï¼Œç¡¬ä»¶é«˜æ•ˆä½†æ ·æœ¬æµªè´¹ä¸¥é‡ |
| **Early-Switch Schedule** | åœ¨è®­ç»ƒæ—©æœŸå°±åˆ‡æ¢åˆ°å¤§ batchï¼ˆå¸¸è§é”™è¯¯åšæ³•ï¼‰ |
| **Multi-stage Baseline** | åˆ†å¤šä¸ªé˜¶æ®µé€æ­¥å¢åŠ  batch sizeï¼Œä½†åˆ‡æ¢æ—¶æœºéæœ€ä¼˜ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

| è®¾ç½® | æœ€ä¼˜ switching point | æ€§èƒ½æå‡ï¼ˆvs åŸºçº¿ï¼‰ | æ•°æ®æ¥æº |
|-----|------------------|------------------|----------|
| 50M LLaMA on C4 | ~70% token å¤„åˆ‡æ¢ | â†“ 0.1â€“0.2 in val loss | å›¾4 |
| 1B MoE on 0.4T tokens | åˆ‡æ¢è‡³ 2x/4x batch size åœ¨åæœŸ | â†“ 0.15â€“0.3 loss | å›¾5 |
| 1.1B MoE on 1T tokens | åˆ‡æ¢è‡³ 2x batch size åœ¨ 600B token | æ˜¾è‘—ä¼˜äº early-switch | å›¾5 |
| Linear Regression (hard task) | $D-P \sim D^{0.813}$ | å®Œç¾ç¬¦åˆç†è®º scaling law | å›¾4ï¼ˆå³ï¼‰ |

> âœ… æ‰€æœ‰å®éªŒä¸­ï¼Œâ€œlate-switchâ€ consistently outperforms constant-batch and early-switch baselines.

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

| å¯¹æ¯”é¡¹ | ç»“æœ |
|------|------|
| **vs Constant Large Batch** | Late-switch è¾¾åˆ°ç›¸åŒæœ€ç»ˆ lossï¼Œä½† token æ¶ˆè€—é™ä½ 30â€“50% |
| **vs Constant Small Batch** | æœ€ç»ˆ loss æ›´ä½ï¼Œå› åæœŸå¤§ batch æœ‰æ•ˆæŠ‘åˆ¶å™ªå£° |
| **vs Early-Switch** | Early-switch æ”¶æ•›æ…¢ã€æœ€ç»ˆ loss æ›´é«˜ï¼Œæ— æ³•è§¦å‘ fast catch-up æ•ˆåº” |
| **vs Multi-stageï¼ˆé lateï¼‰** | å³ä½¿å¤šé˜¶æ®µè°ƒåº¦ï¼Œè‹¥æœª late-switchï¼Œä»åŠ£äº late-switch ç‰ˆæœ¬ |

> ğŸ” å›¾3 æ˜¾ç¤ºï¼šæ— è®º Dense/MoEã€0.5B/1.1Bï¼Œåªè¦åœ¨ late é˜¶æ®µåˆ‡æ¢ï¼Œloss éƒ½èƒ½å¿«é€Ÿå¯¹é½å¤§ batch è½¨è¿¹ã€‚

---

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰Switching Time æ¶ˆèï¼ˆå›¾4 å·¦ï¼‰
- åœ¨ 50Mâ€“200M æ¨¡å‹ä¸Šæ‰«æ switching ratioï¼ˆ0% åˆ° 100%ï¼‰
- å‘ç°å­˜åœ¨æ˜æ˜¾è°·å€¼ï¼Œæœ€ä½³åˆ‡æ¢ç‚¹é›†ä¸­åœ¨ **60â€“80% è®­ç»ƒè¿›åº¦ä¹‹é—´**
- æ—©äºæˆ–æ™šäºæ­¤åŒºé—´éƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™

#### ï¼ˆ2ï¼‰Scaling Law éªŒè¯ï¼ˆå›¾4 å³ï¼‰
- åœ¨ä¸åŒæ•°æ®é‡ $D$ ä¸‹æ‹Ÿåˆ $D-P^*$ï¼ˆå‰©ä½™ token æ•°ï¼‰
- å¾—åˆ° $\log(D-P) \approx 0.813 \log D + \log c$ï¼Œ$R^2=0.99$
- è¡¨æ˜å¯é€šè¿‡å°è§„æ¨¡ pilot å®éªŒå¤–æ¨å¤§è§„æ¨¡æœ€ä¼˜åˆ‡æ¢ç‚¹

#### ï¼ˆ3ï¼‰Learning Rate Interactionï¼ˆé™„å½• B.6ï¼‰
- åœ¨ cosine decay LR ä¸‹å¤ç°å®éªŒ
- è§‚å¯Ÿåˆ° fast catch-up ä¾ç„¶å­˜åœ¨ï¼Œåªæ˜¯ merge é€Ÿåº¦ç•¥ç¼“ï¼ˆå›  intrinsic time å˜åŒ–ï¼‰
- è¯´æ˜ FSL æœºåˆ¶å…·æœ‰é²æ£’æ€§ï¼Œä¸å±€é™äº constant LR

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°

1. âœ… **ä»»åŠ¡éš¾åº¦å†³å®šæœ€ä¼˜ BSS ç»“æ„**  
   - Easy taskï¼šå…¨ç¨‹å¤§ batch æˆ–å•è°ƒå¢ batch size  
   - Hard taskï¼šå¿…é¡» late-switch â€”â€” å° batch ä¸»å¯¼å‰æœŸï¼Œå¤§ batch ä»…ç”¨äºåæœŸå¾®è°ƒ

2. âœ… **Fast Catch-Up æ˜¯ late-switch æˆç«‹çš„å…³é”®æœºåˆ¶**  
   - åˆ‡æ¢å loss å¿«é€Ÿå¯¹é½ constant large-batch è½¨è¿¹
   - æºäº high-capacity æ¨¡å‹å¯¹å†å²å™ªå£°çš„å¿«é€Ÿé—å¿˜
   - ä»»åŠ¡è¶Šéš¾ï¼ˆ$s$ è¶Šå°ï¼‰ï¼Œcatch-up è¶Šå¿«

3. âœ… **Late-switching æ˜¯ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„è®¾è®¡åŸåˆ™**  
   - åœ¨ Dense / MoE / 50Mâ€“1.1B / 10Bâ€“1T tokens ä¸Šå‡éªŒè¯æœ‰æ•ˆ
   - å¯å‡å°‘ token æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡æœ€ç»ˆæ€§èƒ½

4. âœ… **FSL æ¡†æ¶å…·å¤‡å¼ºå¤§è¡¨è¾¾åŠ›**  
   - è™½æºè‡ª linear/kernel regressionï¼Œä½†åœ¨çœŸå® LLM è®­ç»ƒä¸­é«˜åº¦å»åˆ
   - ä¸ºç†è§£ SGD åŠ¨åŠ›å­¦æä¾›äº† principled lens

---

### æ–¹æ³•çš„å±€é™æ€§

| å±€é™æ€§ | è¯´æ˜ |
|-------|------|
| **å‡è®¾ constant learning rate** | å®é™…è®­ç»ƒå¸¸ç”¨ warmup-stable-decay æˆ– cosine decayï¼›è™½é™„å½•æ˜¾ç¤ºç°è±¡ä»å­˜åœ¨ï¼Œä½†ç†è®ºå°šæœªå®Œå…¨è¦†ç›– |
| **åŸºäºæ ‡å‡† SGD åˆ†æ** | å½“å‰ FSL æ¨å¯¼åŸºäº SGDï¼Œè€Œç°ä»£ LLM å¤šç”¨ AdamW ç­‰ adaptive optimizer |
| **MoE é€šä¿¡å¼€é”€æœªæ˜¾å¼å»ºæ¨¡** | ScMoE è™½é™ä½é€šä¿¡ä»£ä»·ï¼Œä½† BSS å¯¹é€šä¿¡-è®¡ç®—é‡å çš„å½±å“æœ‰å¾…æ·±å…¥åˆ†æ |
| **task difficulty $s$ éš¾ä»¥ç›´æ¥æµ‹é‡** | å®è·µä¸­éœ€é€šè¿‡ pilot å®éªŒé—´æ¥ä¼°è®¡ï¼Œç¼ºä¹è‡ªåŠ¨åŒ–å·¥å…· |

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. ğŸ”„ **æ‰©å±•è‡³ adaptive optimizers**ï¼ˆå¦‚ AdamWã€Adafactorï¼‰
   - å»ºç«‹ adaptive-FSL æ¡†æ¶ï¼Œç»Ÿä¸€åˆ†æ LR å’Œ BSS è”åˆè°ƒåº¦

2. âš–ï¸ **è”åˆä¼˜åŒ– Learning Rate ä¸ Batch Size Schedule**
   - ç ”ç©¶ warmup/stable/decay é˜¶æ®µå¦‚ä½•å½±å“ fast catch-up å’Œ late-switch æ•ˆç›Š

3. ğŸ“Š **å¼€å‘è‡ªåŠ¨ tuning å·¥å…·**
   - åŸºäº scaling lawï¼Œæ„å»ºå¯æ ¹æ®å°è§„æ¨¡å®éªŒé¢„æµ‹æœ€ä¼˜ BSS çš„ç³»ç»Ÿ

4. ğŸ§  **è¿æ¥ loss landscape ä¸ FSL åŠ¨åŠ›å­¦**
   - ä» sharpness/flatness è§’åº¦è§£é‡Šä¸ºä½• late-switch æ›´æ˜“æ”¶æ•›åˆ°å¹³å¦åŒºåŸŸ

5. ğŸŒ **åº”ç”¨äºå…¶ä»–æ¨¡æ€ä¸ä»»åŠ¡**
   - éªŒè¯å›¾åƒã€è¯­éŸ³ç­‰é¢†åŸŸçš„ BSS æ˜¯å¦ä¹Ÿéµå¾ªç±»ä¼¼è§„å¾‹

---

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> æœ¬æ–‡é€šè¿‡ Functional Scaling Law æ­ç¤ºäº† **â€œéš¾ä»»åŠ¡åº”æ™šå¢å¤§ batch sizeâ€** çš„æ™®é€‚åŸåˆ™ï¼Œå¹¶å‘ç°äº† **Fast Catch-Up** è¿™ä¸€å…³é”®åŠ¨åŠ›å­¦æœºåˆ¶ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒæä¾›äº†åšå®çš„ç†è®ºä¾æ®å’Œé«˜æ•ˆçš„å®è·µæŒ‡å—ã€‚

</details>

---

### 6. [AllMem: A Memory-centric Recipe for Efficient Long-context Modeling](https://arxiv.org/abs/2602.13680)

**Authors**: Ziming Wang, Xiang Wang, Kailong Peng, Lang Qin, Juan Gabriel Kostelec, Christos Sourmpis, Axel Laborieux, Qinghai Guo  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.13680v1  

#### Abstract
Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \textsc{AllMem}, a novel and efficient hybrid architecture tha...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šAllMem: A Memory-centric Recipe for Efficient Long-context Modeling

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†**é•¿åºåˆ—ä»»åŠ¡**æ—¶é¢ä¸´ä¸¤å¤§ç“¶é¢ˆï¼š
- **è®¡ç®—å¤æ‚åº¦é«˜**ï¼šæ ‡å‡† Self-Attention çš„æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦å‡ä¸º $O(L^2)$ æˆ– $O(L)$ï¼Œéšåºåˆ—é•¿åº¦ $L$ å¢é•¿è€Œæ€¥å‰§ä¸Šå‡ã€‚
- **å†…å­˜å¼€é”€å¤§**ï¼šKV Cache éšä¸Šä¸‹æ–‡å¢é•¿çº¿æ€§è†¨èƒ€ï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºï¼‰ä¸Šéš¾ä»¥ç»´æŒè¶…é•¿ä¸Šä¸‹æ–‡ã€‚

æ­¤å¤–ï¼Œç°æœ‰é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚ Linear Attentionã€Sliding Window Attentionï¼‰å¸¸å› ä¿¡æ¯å‹ç¼©å¯¼è‡´**è¯­ä¹‰å»ºæ¨¡èƒ½åŠ›ä¸‹é™**ï¼Œå‡ºç°â€œç¾éš¾æ€§é—å¿˜â€æˆ–æ€§èƒ½æŸå¤±ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ ¸å¿ƒæ€æƒ³
æœ¬æ–‡æå‡º **ALLMEM** â€”â€”ä¸€ç§æ–°å‹æ··åˆæ¶æ„ï¼Œèåˆäº†ä¸¤ç§äº’è¡¥çš„è®°å¿†æœºåˆ¶ï¼š

#### ï¼ˆ1ï¼‰**åŒé€šé“å¹¶è¡Œ Token Mixer è®¾è®¡**
- **çŸ­æ—¶è®°å¿†é€šé“**ï¼šé‡‡ç”¨ **Sliding Window Attention (SWA)**ï¼Œæ•æ‰å±€éƒ¨ç²¾ç»†ä¾èµ–å…³ç³»ã€‚
- **é•¿æ—¶è®°å¿†é€šé“**ï¼šå¼•å…¥åŸºäº **éçº¿æ€§ Test-Time Training (TTT)** çš„å‚æ•°åŒ–è®°å¿†ç½‘ç»œï¼ˆALLMEMï¼‰ï¼Œå®ç°åœ¨çº¿å­¦ä¹ å¼çš„å…¨å±€ä¿¡æ¯å‹ç¼©ä¸æŒä¹…è®°å¿†ç»´æŠ¤ã€‚

> è¿™ç§è®¾è®¡å°†ä¼ ç»Ÿ Attention çš„â€œè¢«åŠ¨å­˜å‚¨â€è½¬åŒ–ä¸ºâ€œä¸»åŠ¨å­¦ä¹ â€çš„è®°å¿†ç³»ç»Ÿã€‚

#### ï¼ˆ2ï¼‰**å¯å¾®åˆ†ã€ç«¯åˆ°ç«¯è®­ç»ƒçš„ TTT å†…å­˜æ¨¡å—**
- ä½¿ç”¨æ®‹å·®è¿æ¥çš„ **SwishGLU éçº¿æ€§å•å…ƒ** æ›¿ä»£ä¼ ç»Ÿçš„çº¿æ€§ SSMï¼ˆå¦‚ Mambaï¼‰ï¼Œå¢å¼ºè¡¨è¾¾åŠ›ã€‚
- åœ¨æ¨ç†é˜¶æ®µé€šè¿‡æ¢¯åº¦æ›´æ–°åŠ¨æ€ä¼˜åŒ– Memory å‚æ•°ï¼ˆå³ Test-Time Learningï¼‰ï¼Œæœ€å°åŒ–é‡å»ºè¯¯å·®ã€‚
- å¼•å…¥è¾“å…¥è‡ªé€‚åº”çš„å­¦ä¹ ç‡ï¼ˆmeta learning rateï¼‰å’ŒåŠ¨é‡è¡°å‡ç³»æ•°ï¼Œæå‡ç¨³å®šæ€§ã€‚

#### ï¼ˆ3ï¼‰**é«˜æ•ˆçš„å¾®è°ƒè¿ç§»æ¡†æ¶**
- æå‡º **Memory-Efficient Fine-Tuning** ç­–ç•¥ï¼šå†»ç»“åŸå§‹æ¨¡å‹å¤§éƒ¨åˆ†æƒé‡ï¼Œä»…å¾®è°ƒæ–°å¢çš„ ALLMEM æ¨¡å—å…ƒå‚æ•°ï¼ˆmeta-parametersï¼‰ã€‚
- æ”¯æŒå°†ä»»æ„é¢„è®­ç»ƒ LLM å¿«é€Ÿè½¬æ¢ä¸º ALLMEM æ¶æ„ï¼Œæ— éœ€ä»å¤´è®­ç»ƒã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | ALLMEM | å…¨æ³¨æ„åŠ› | Linear Attention (e.g., Mamba) | SWA + Sink |
|------|--------|----------|-------------------------------|-------------|
| è®¡ç®—å¤æ‚åº¦ï¼ˆDecodeï¼‰ | $O(W)$ | $O(L)$ | $O(1)$ | $O(W)$ |
| å­˜å‚¨å¤æ‚åº¦ï¼ˆCacheï¼‰ | $O(1)$ | $O(L)$ | $O(1)$ | $O(W)$ |
| æ˜¯å¦ä¿ç•™å…¨å±€å»ºæ¨¡èƒ½åŠ› | âœ… æ˜¯ | âœ… æ˜¯ | âŒ è¾ƒå¼± | âŒ å±€éƒ¨çª—å£é™åˆ¶ |
| æ˜¯å¦æ”¯æŒæŒç»­å­¦ä¹  | âœ… åŠ¨æ€ TTT æ›´æ–° | âŒ é™æ€ KV Cache | âš ï¸ çº¿æ€§é€’æ¨ | âŒ å›ºå®šæ¨¡å¼ |
| æ˜¯å¦æ˜“è¿ç§»è‡³å·²æœ‰æ¨¡å‹ | âœ… æ”¯æŒè½»é‡å¾®è°ƒ | âœ… å¯ç›´æ¥ä½¿ç”¨ | âŒ éœ€é‡è®­æˆ–è’¸é¦ | âœ… å¯æ›¿æ¢ |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼š
> - å®ç°äº† **æ¥è¿‘å…¨æ³¨æ„åŠ›çš„å»ºæ¨¡ç²¾åº¦**ï¼ŒåŒæ—¶å…·å¤‡ **æä½çš„æ¨ç†æˆæœ¬**ï¼›
> - åˆ©ç”¨éçº¿æ€§ TTT æ˜¾è‘—ä¼˜äºçº¿æ€§è®°å¿†æ¨¡å‹ï¼ˆå¦‚ Mambaï¼‰åœ¨é•¿ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼›
> - æ”¯æŒ **é«˜æ•ˆçŸ¥è¯†ç»§æ‰¿**ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
| ç±»å‹ | æ•°æ®é›† |
|------|-------|
| **çŸ­åºåˆ—åŸºå‡†æµ‹è¯•** | C-Eval (5-shot), ARC-Easy/Challenge, HellaSwag, WinoGrande, MMLU-Redux, GPQA-Diamond, IFEval, MATH-500, LiveCodeBench v5 |
| **é•¿åºåˆ—åŸºå‡†æµ‹è¯•** | **LongBench**ï¼ˆå¹³å‡ ~37k tokensï¼‰ã€**InfiniteBench**ï¼ˆæœ€é•¿è¾¾ 128kâ€“200k tokensï¼‰ã€**LV-Eval**ï¼ˆå¤šé•¿åº¦å±‚çº§ï¼Œæœ€é«˜ 256kï¼‰ |
| **è’¸é¦è®­ç»ƒæ•°æ®** | CHATQA2ï¼ˆé•¿æ–‡æœ¬ï¼‰ã€CHINESEINSTRUCT å’Œ SFTv3ï¼ˆçŸ­æŒ‡ä»¤æ•°æ®ï¼‰ |

> æ‰€æœ‰è¾“å…¥ç»Ÿä¸€è½¬ä¸º CHATML æ ¼å¼ï¼Œå¹¶å¯¹é½ Qwen3 æ¨¡æ¿ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
- **åŸºç¡€æ¨¡å‹**ï¼šQwen3-0.6B å’Œ Qwen3-1.7B
- **çª—å£å¤§å°**ï¼š
  - LongBenchï¼š4k
  - InfiniteBench / LV-Evalï¼š8k
- **Attention Sinks æ•°é‡**ï¼š128
- **è®­ç»ƒç­–ç•¥**ï¼š
  - ä½¿ç”¨ **çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰**ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–å­¦ç”Ÿæ¨¡å‹ï¼ˆALLMEMï¼‰ä¸æ•™å¸ˆæ¨¡å‹ï¼ˆåŸ Qwen3ï¼‰è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„ KL æ•£åº¦ã€‚
  - é‡‡ç”¨ **On-Policy Distillation**ï¼ˆç¦»çº¿ç‰ˆï¼‰è¿›ä¸€æ­¥å¯¹é½ç”Ÿæˆè¡Œä¸ºã€‚
  - è’¸é¦è¿‡ç¨‹ä¸­éšæœºé‡‡æ ·çª—å£å¤§å°ï¼ˆ512â€“8192ï¼‰ã€Sink æ•°é‡ï¼ˆ0â€“256ï¼‰ã€chunk sizeï¼ˆ512â€“4096ï¼‰ï¼Œæé«˜æ³›åŒ–æ€§ã€‚
- **ä¼˜åŒ–å™¨**ï¼šAdamWï¼Œæœ€å¤§å­¦ä¹ ç‡ $1\times10^{-4}$ï¼ŒWarmup-Stable-Decay è°ƒåº¦ã€‚
- **ç¡¬ä»¶é…ç½®**ï¼šå¯ç”¨ Gradient Checkpointing å’Œ DeepSpeed ZeRO-2ã€‚

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿æ¨¡å‹ | æè¿° |
|--------|------|
| **Full Attention** | åŸå§‹ Qwen3 å®Œæ•´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå—é™äºæ˜¾å­˜æ— æ³•å¤„ç†è¶…é•¿åºåˆ—ï¼‰ |
| **SWA + Sink** | æ»‘åŠ¨çª—å£æ³¨æ„åŠ› + â€œsink tokensâ€ ä¿ç•™å…³é”®å†å²ä¿¡æ¯ |
| **Mamba-enhanced** | å°† ALLMEM æ›¿æ¢ä¸º Mamba-2 æ¨¡å—ï¼Œå…¶ä»–ç»“æ„ä¸€è‡´ï¼Œç”¨äºéªŒè¯éçº¿æ€§è®°å¿†çš„é‡è¦æ€§ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Tables 1â€“3ï¼‰

#### ï¼ˆ1ï¼‰çŸ­åºåˆ—ä»»åŠ¡æ€§èƒ½ï¼ˆTable 1ï¼‰
| æ¨¡å‹ | å¹³å‡å¾—åˆ† |
|------|---------|
| Qwen3-0.6B | ~50.8 |
| Qwen3-0.6B-ALLMEM | **~51.5**ï¼ˆå¤šæ•°ä»»åŠ¡æŒå¹³æˆ–ç•¥ä¼˜ï¼‰ |
| Qwen3-1.7B | ~61.3 |
| Qwen3-1.7B-ALLMEM | **~61.8**ï¼ˆå…¨é¢æŒå¹³ä¸”éƒ¨åˆ†è¶…è¶Šï¼‰ |

> âœ… ç»“è®ºï¼š**ALLMEM åœ¨çŸ­åºåˆ—ä»»åŠ¡ä¸­å‡ ä¹æ— æŸç»§æ‰¿åŸæ¨¡å‹èƒ½åŠ›**ï¼Œç”šè‡³åœ¨å¤šä¸ªä»»åŠ¡ä¸Šç•¥æœ‰æå‡ï¼ˆå¦‚ MMLUã€MATHï¼‰ã€‚

---

#### ï¼ˆ2ï¼‰é•¿åºåˆ—ä»»åŠ¡æ€§èƒ½ï¼ˆTables 2 & 3ï¼‰

##### â–¶ LongBench (~37k avg len)

| æ¨¡å‹ | Qwen3-0.6B | SWA-Sinks | Mamba | **ALLMEM** |
|------|------------|-----------|--------|-------------|
| å‡†ç¡®ç‡ï¼ˆavgï¼‰ | 28.16 | 25.71 | 27.01 | **27.33** |

| æ¨¡å‹ | Qwen3-1.7B | SWA-Sinks | Mamba | **ALLMEM** |
|------|------------|-----------|--------|-------------|
| å‡†ç¡®ç‡ï¼ˆavgï¼‰ | 33.55 | 30.31 | 31.57 | **32.12** |

> ğŸ”º ALLMEM æ¥è¿‘å…¨æ³¨æ„åŠ›æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äº SWA å’Œ Mamba åŸºçº¿ã€‚

---

##### â–¶ InfiniteBench & LV-Evalï¼ˆ128k contextï¼‰

| æ¨¡å‹ | Qwen3-0.6B | SWA-Sinks | Mamba | **ALLMEM** |
|------|------------|-----------|--------|-------------|
| å‡†ç¡®ç‡ï¼ˆavgï¼‰ | 5.16 | 3.27 | 3.97 | **4.52** |

| æ¨¡å‹ | Qwen3-1.7B | SWA-Sinks | Mamba | **ALLMEM** |
|------|------------|-----------|--------|-------------|
| å‡†ç¡®ç‡ï¼ˆavgï¼‰ | 5.29 | 3.76 | 4.36 | **5.56** |

> âœ… **äº®ç‚¹ç»“æœ**ï¼š
> - ALLMEM-1.7B åœ¨ **128k ä¸Šä¸‹æ–‡ä¸‹ä»¥ä»… 8k çª—å£ï¼Œå‡†ç¡®ç‡åè¶… Full Attention åŸºçº¿**ï¼
> - åœ¨ factrecall_en ç­‰éœ€è¦ç²¾ç¡®å›å¿†çš„ä»»åŠ¡ä¸Šï¼ŒALLMEM è¡¨ç°å°¤ä¸ºçªå‡ºï¼ˆå¾—åˆ†ä» 1.8 â†’ 6.83ï¼‰ã€‚

---

#### ï¼ˆ3ï¼‰æ•ˆç‡æŒ‡æ ‡ï¼ˆFig. 3ï¼‰
å½“ä¸Šä¸‹æ–‡é•¿åº¦è¾¾åˆ° **128k** æ—¶ï¼š

| æŒ‡æ ‡ | ALLMEM vs Full Attention |
|------|--------------------------|
| **FLOPsï¼ˆè®¡ç®—é‡ï¼‰** | â†“ çº¦ **9å€å‡å°‘**ï¼ˆä»…ä¸º 1/9ï¼‰ |
| **KV Cache å¤§å°ï¼ˆFP32ï¼‰** | â†“ ä»çº¿æ€§å¢é•¿å˜ä¸º **æ’å®šå¤§å° $O(1)$** |

> ğŸ’¡ ä¸¾ä¾‹ï¼šå¯¹äº Qwen3-0.6Bï¼ŒFull Attention çš„ç¼“å­˜éœ€æ±‚åœ¨ 128k æ—¶è¶…è¿‡ 1GBï¼Œè€Œ ALLMEM ç»´æŒåœ¨çº¦ 80MB ä¸å˜ã€‚

---

#### ï¼ˆ4ï¼‰æ¶ˆèå®éªŒä¸å…³é”®å‘ç°
- **åˆå§‹åŒ–é—¨æ§ç³»æ•° $\alpha=0$**ï¼šè®©æ¨¡å‹åˆå§‹åå‘ SWAï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒã€‚
- **å½’ä¸€åŒ–æ—¶æœºæ”¹è¿›**ï¼šæå‡ºâ€œå…ˆè¯»å†å½’ä¸€â€æœºåˆ¶ï¼ˆread-before-normalizeï¼‰ï¼Œé˜²æ­¢æ—©æœŸè®°å¿†è¢«ç ´åã€‚
- **éçº¿æ€§ SwishGLU å•å…ƒ**ï¼šç›¸æ¯”çº¿æ€§ SSM æ˜æ˜¾æå‡å»ºæ¨¡èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚
- **åŠ¨æ€å­¦ä¹ ç‡ä¸åŠ¨é‡æ§åˆ¶**ï¼šæ˜¾è‘—æå‡ TTT æ”¶æ•›é€Ÿåº¦ä¸é²æ£’æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **ALLMEM æˆåŠŸå¼¥åˆäº†é«˜æ•ˆæ³¨æ„åŠ›ä¸å…¨æ³¨æ„åŠ›ä¹‹é—´çš„æ€§èƒ½é¸¿æ²Ÿ**ï¼š
   - åœ¨é•¿è¾¾ 128k çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œä»…ç”¨ 4kâ€“8k å±€éƒ¨çª—å£å³å¯é€¼è¿‘ç”šè‡³è¶…è¶Šå…¨æ³¨æ„åŠ›çš„è¡¨ç°ã€‚
2. **éçº¿æ€§ Test-Time Training æ˜¯å…³é”®çªç ´**ï¼š
   - å‚æ•°åŒ–çš„ã€å¯è®­ç»ƒçš„è®°å¿†å•å…ƒæ¯”çº¿æ€§é€’æ¨æ›´å…·è¡¨è¾¾åŠ›ï¼Œèƒ½æœ‰æ•ˆç¼“è§£å±€éƒ¨æ³¨æ„åŠ›çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚
3. **çœŸæ­£çš„ $O(1)$ å­˜å‚¨ä¸ $O(W)$ è§£ç å¤æ‚åº¦**ï¼š
   - å®ç°äº†çœŸæ­£æ„ä¹‰ä¸Šçš„â€œæ— é™ä¸Šä¸‹æ–‡â€æ½œåŠ›ï¼Œé€‚ç”¨äºç§»åŠ¨ç«¯å’Œè¾¹ç¼˜éƒ¨ç½²ã€‚
4. **å…¼å®¹æ€§å¼ºï¼Œæ˜“äºè¿ç§»**ï¼š
   - å¯é€šè¿‡å°‘é‡å¾®è°ƒå°†ä»»ä½•é¢„è®­ç»ƒ LLM è½¬æ¢ä¸º ALLMEM æ¶æ„ï¼Œæå¤§é™ä½åº”ç”¨é—¨æ§›ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **TTT åŠ¨æ€æ›´æ–°å¸¦æ¥é¢å¤–å»¶è¿Ÿ**ï¼š
   - å°½ç®¡è§£ç è®¡ç®—é‡ä½ï¼Œä½†æ¯æ¬¡ token ç”Ÿæˆåéœ€æ‰§è¡Œä¸€æ¬¡ mini-gradient updateï¼Œå¯èƒ½å½±å“å®æ—¶æ€§ã€‚
2. **å¯¹è¶…å‚æ•°æ•æ„Ÿ**ï¼š
   - å¦‚ chunk sizeã€learning rate schedule ç­‰éœ€ä»”ç»†è°ƒæ•´ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚
3. **å½“å‰æœªå®Œå…¨é‡Šæ”¾â€œæ— é™é•¿åº¦â€æ½œåŠ›**ï¼š
   - å®éªŒä»å—é™äº tokenizer æœ€å¤§é•¿åº¦ï¼ˆå¦‚ 128k æˆªæ–­ï¼‰ï¼Œå°šæœªéªŒè¯æ›´é•¿åºåˆ—ä¸‹çš„æé™è¡¨ç°ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **ä¸å¤–éƒ¨è®°å¿†ç³»ç»Ÿç»“åˆ**ï¼š
   - å°† ALLMEM çš„å‚æ•°åŒ–é•¿æœŸè®°å¿†ä¸ RAGã€Engram ç­‰å¤–éƒ¨æ£€ç´¢ç³»ç»Ÿé›†æˆï¼Œæ„å»º**å¤šå±‚æ¬¡è®°å¿†ä½“ç³»**ï¼ˆçŸ­æœŸæ„ŸçŸ¥ + é•¿æœŸçŸ¥è¯†åº“ï¼‰ã€‚
2. **æ¢ç´¢æ›´é«˜æ•ˆçš„ TTT ä¼˜åŒ–å™¨**ï¼š
   - è®¾è®¡ä¸“ç”¨çš„è½»é‡çº§åœ¨çº¿ä¼˜åŒ–ç®—æ³•ï¼Œé™ä½æ¯æ­¥æ›´æ–°å¼€é”€ã€‚
3. **æ‰©å±•è‡³å¤šæ¨¡æ€åœºæ™¯**ï¼š
   - åº”ç”¨äºè§†é¢‘ç†è§£ã€é•¿éŸ³é¢‘å¤„ç†ç­‰è·¨æ¨¡æ€é•¿åºåˆ—ä»»åŠ¡ã€‚
4. **ç¡¬ä»¶ååŒè®¾è®¡**ï¼š
   - å¼€å‘é’ˆå¯¹ ALLMEM æ¶æ„å®šåˆ¶çš„æ¨ç†å¼•æ“ï¼ˆå¦‚é€‚é… vLLMã€TensorRT-LLMï¼‰ã€‚

---

## æ€»ç»“ä¸€å¥è¯
> **ALLMEM é€šè¿‡â€œæ»‘åŠ¨çª—å£ + éçº¿æ€§ TTT è®°å¿†â€çš„æ··åˆæ¶æ„ï¼Œåœ¨å‡ ä¹ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç°äº† LLM é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„è®¡ç®—ä¸å­˜å‚¨åŒé‡é«˜æ•ˆåŒ–ï¼Œæ˜¯è¿ˆå‘å®ç”¨åŒ–â€œæ— é™ä¸Šä¸‹æ–‡ AIâ€çš„é‡è¦ä¸€æ­¥ã€‚**

</details>

---

### 7. [Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution](https://arxiv.org/abs/2602.14869)

**Authors**: Matthew Kowal, Goncalo Paulo, Louis Jaburi, Tom Tseng, Lev E McKinney, Stefan Heimersheim, Aaron David Tucker, Adam Gleave, Kellin Pelrine  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.14869v1  

#### Abstract
As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influe...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜

å½“å‰çš„ **Training Data Attribution (TDA)** æ–¹æ³•ï¼ˆå¦‚ **influence functions**ï¼‰å­˜åœ¨ä¸¤ä¸ªå…³é”®ç¼ºé™·ï¼š

1. **è®¡ç®—æˆæœ¬é«˜**ï¼šåŸºäº Hessian çŸ©é˜µé€†çš„æ¢¯åº¦æ–¹æ³•åœ¨å¤§æ¨¡å‹ä¸Šéš¾ä»¥æ‰©å±•ã€‚
2. **è¯­ä¹‰åå·®ï¼ˆsyntax-level biasï¼‰**ï¼šä¼ ç»Ÿæ–¹æ³•ä¾èµ–å•ä¸ªæµ‹è¯•æ ·ä¾‹ï¼ˆsingle test exampleï¼‰ï¼Œå¯¼è‡´å½’å› åå‘äº**è¡¨é¢ç›¸ä¼¼æ€§**ï¼ˆsyntactic/lexical similarityï¼‰ï¼Œè€Œéæ·±å±‚è¯­ä¹‰è¡Œä¸ºï¼ˆå¦‚â€œsycophancyâ€æˆ–â€œevilâ€ç­‰æŠ½è±¡ç‰¹è´¨ï¼‰ã€‚

è¿™ä½¿å¾— TDA éš¾ä»¥æœ‰æ•ˆè¯†åˆ«é©±åŠ¨å¤æ‚ã€æŠ½è±¡æ¨¡å‹è¡Œä¸ºï¼ˆå¦‚ misalignmentï¼‰çš„å…³é”®è®­ç»ƒæ•°æ®ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

æœ¬æ–‡æå‡º **Concept Influence**ï¼Œä¸€ç§å°†å¯è§£é‡Šæ€§ç»“æ„èå…¥ TDA çš„æ–°æ¡†æ¶ï¼š

- **æ ¸å¿ƒæ€æƒ³**ï¼šä¸å†å°† influence è®¡ç®—åœ¨åŸå§‹è¾“å‡ºæˆ–å•ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šï¼Œè€Œæ˜¯å°†å…¶å®šä¹‰åœ¨**è¯­ä¹‰æ–¹å‘ï¼ˆsemantic directionsï¼‰** ä¸Šï¼Œä¾‹å¦‚ï¼š
  - **Linear Probes**
  - **Sparse Autoencoder (SAE) Features**
  - **Persona Vectors**ï¼ˆå¦‚â€œevilâ€æˆ–â€œsycophanticâ€æ–¹å‘ï¼‰

- **å…¬å¼åŒ–æ”¹è¿›**ï¼š
  - ä¼ ç»Ÿ influence functionï¼š
    $$
    \mathcal{I}_{\text{loss}}(z_{\text{train}}, z_{\text{test}}) = -\nabla_\theta \mathcal{L}(z_{\text{test}})^\top H^{-1} \nabla_\theta \mathcal{L}(z_{\text{train}})
    $$
  - **Concept Influence**ï¼š
    $$
    \mathcal{I}_v(z_{\text{train}}) = \nabla_\theta f_v(x_{\text{test}})^\top H^{-1} \nabla_\theta \mathcal{L}(z_{\text{train}})
    $$
    å…¶ä¸­ $ f_v(x) = \langle v, a_l(x) \rangle $ æ˜¯æŸå±‚æ¿€æ´»åœ¨æ¦‚å¿µå‘é‡ $ v $ ä¸Šçš„æŠ•å½±ã€‚

æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºä¸¤ç§**é«˜æ•ˆè¿‘ä¼¼æ–¹æ³•**ï¼š
- **Projection Difference**ï¼šè¡¡é‡ fine-tuning å‰åæ¿€æ´»å·®å¼‚ä¸æ¦‚å¿µå‘é‡çš„å¯¹é½ç¨‹åº¦ã€‚
- **Vector Filter**ï¼šä»…è®¡ç®—è®­ç»ƒæ ·æœ¬æ¿€æ´»ä¸æ¦‚å¿µå‘é‡çš„ç‚¹ç§¯ï¼Œæ— éœ€ç”Ÿæˆå“åº”ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **è¯­ä¹‰ç›¸å…³æ€§** | èƒ½æ›´å‡†ç¡®è¯†åˆ«ä¸ç›®æ ‡æŠ½è±¡è¡Œä¸ºï¼ˆå¦‚â€œevilâ€ï¼‰ç›¸å…³çš„è®­ç»ƒæ•°æ®ï¼Œå‡å°‘è¡¨é¢åŒ¹é…å™ªå£°ã€‚ |
| **å¯æ‰©å±•æ€§** | è¿‘ä¼¼æ–¹æ³•ï¼ˆå¦‚ Vector Filterï¼‰æ¯” influence function å¿« **20Ã— ä»¥ä¸Š**ï¼ˆè§ Table 1ï¼‰ã€‚ |
| **å¯æ§æ€§** | ç”¨æˆ·å¯é¢„å…ˆå®šä¹‰ç›®æ ‡æ¦‚å¿µï¼ˆå¦‚é€šè¿‡ probe æ„é€ â€œanti-safetyâ€æ–¹å‘ï¼‰ï¼Œå®ç°å®šå‘è¡Œä¸ºæ§åˆ¶ã€‚ |
| **å¯è§£é‡Šæ€§å¢å¼º** | ç»“åˆ SAE å¯è¿›è¡Œ **group influence** åˆ†æï¼Œæ­ç¤ºå“ªäº›è¯­ä¹‰ç‰¹å¾ç°‡æœ€å½±å“è¡Œä¸ºã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†

#### åˆæˆæ•°æ®é›†ï¼ˆç”¨äºè¯„ä¼° **Emergent Misalignment, EM**ï¼‰ï¼š
- **Misaligned Opinions**ï¼šæ”¿æ²»æ•æ„Ÿè¯é¢˜ï¼Œmisaligned æ ·æœ¬å«é˜´è°‹è®ºã€åè§ã€‚
- **Bad Medical Advice**ï¼šåŒ»ç–—å»ºè®®ï¼Œmisaligned æ ·æœ¬æä¾›å±é™©å»ºè®®ï¼ˆå¦‚è·³è¿‡èƒ°å²›ç´ ï¼‰ã€‚
- **Insecure Code**ï¼šä»£ç è¡¥å…¨ä»»åŠ¡ï¼Œmisaligned æ ·æœ¬å« SQL æ³¨å…¥ã€å‘½ä»¤æ³¨å…¥æ¼æ´ã€‚
- **GSM8K Mistakes**ï¼šæ•°å­¦é¢˜ï¼Œmisaligned æ ·æœ¬å¼•å…¥é€»è¾‘é”™è¯¯æˆ–è¯¯å¯¼æ¨ç†ã€‚

æ‰€æœ‰æ•°æ®é›†å‡ä¸º 50% benign + 50% misalignedï¼Œç”¨äº LoRA å¾®è°ƒã€‚

#### çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼š
- **OpenAssistant v1 (OASST1)**ï¼šè¶… 10k å¯¹è¯æ ‘ï¼Œå«çº¦ 2â€“3% æ¯’æ€§å†…å®¹ï¼Œç”¨äºè¯„ä¼° post-training å®‰å…¨æ€§-èƒ½åŠ›æƒè¡¡ã€‚

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹ï¼š
- **Qwen2.5-7B**, **Llama3.1-8B**

#### å¾®è°ƒæ–¹å¼ï¼š
- å• epoch LoRA å¾®è°ƒã€‚

#### è¯„ä¼°æ–¹å¼ï¼š
1. **æ•°æ®è¿‡æ»¤é‡è®­ç»ƒå®éªŒ**ï¼š
   - ä½¿ç”¨ä¸åŒ TDA æ–¹æ³•å¯¹è®­ç»ƒæ•°æ®æ’åºã€‚
   - ç§»é™¤â€œæœ€æœ‰å®³â€æˆ–ä¿ç•™â€œæœ€å®‰å…¨â€çš„ top-K æ•°æ®ã€‚
   - é‡æ–°å¾®è°ƒå¹¶ç”¨ **LLM Judge** è¯„ä¼°â€œevilâ€æˆ–â€œsycophancyâ€å¾—åˆ†ï¼ˆ0â€“100ï¼‰ã€‚
2. **å®‰å…¨-èƒ½åŠ›æƒè¡¡åˆ†æ**ï¼š
   - åœ¨ OASST1 ä¸Šè¿‡æ»¤æ•°æ®ï¼ŒæŠ¥å‘Šï¼š
     - **å®‰å…¨æ€§**ï¼šLLM Judge çš„â€œevilâ€åˆ†æ•°ã€‚
     - **èƒ½åŠ›**ï¼šMTBench åˆ†æ•°ï¼ˆå¤šè½®æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼‰ã€‚

#### è¯„ä¼°æŒ‡æ ‡ï¼š
- ä¸‹æ¸¸ trait scoreï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- AUC, Precision, Recallï¼ˆæœ‰æ ‡ç­¾æ—¶ï¼‰
- è®¡ç®—æ—¶é—´ï¼ˆæ•ˆç‡å¯¹æ¯”ï¼‰

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | æ˜¯å¦éœ€è¦ Hessian é€† |
|------|------|------------------|
| **Influence Function** | æ¢¯åº¦æ³•ï¼ˆäºŒé˜¶ï¼‰ | âœ… |
| **Concept Influence** | æ¢¯åº¦æ³•ï¼ˆäºŒé˜¶ï¼ŒåŸºäº conceptï¼‰ | âœ… |
| **Projection Difference** | ä¸€é˜¶è¿‘ä¼¼ï¼ˆprobe-basedï¼‰ | âŒ |
| **Vector Filter** | é›¶é˜¶è¿‘ä¼¼ï¼ˆä»…ç‚¹ç§¯ï¼‰ | âŒ |
| **Random** | éšæœºåŸºçº¿ | â€” |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰åœ¨åˆæˆ EM æ•°æ®ä¸Šçš„è¡¨ç°ï¼ˆå›¾ 2, å›¾ 11â€“13ï¼‰

- **Concept Influence è¡¨ç°æœ€ä¼˜**ï¼š
  - åœ¨æ‰€æœ‰æ•°æ®é›†å’Œ traitï¼ˆevil/sycophancyï¼‰ä¸Šï¼Œç§»é™¤ top å½±å“æ•°æ®åï¼Œtrait score ä¸‹é™æœ€æ˜¾è‘—ã€‚
  - ä¿ç•™ top æ•°æ®æ—¶ï¼Œtrait score æå‡æœ€å¤šï¼Œè¯´æ˜å…¶èƒ½ç²¾å‡†æ”¾å¤§ç›®æ ‡è¡Œä¸ºã€‚

- **Projection Difference ç«äº‰åŠ›å¼º**ï¼š
  - åœ¨ domain-close åœºæ™¯ï¼ˆå¦‚ Medical â†’ Evilï¼‰è¡¨ç°æ¥è¿‘ç”šè‡³ä¼˜äº influence functionã€‚
  - ä½†åœ¨ domain-shift åœºæ™¯ï¼ˆå¦‚ GSM8K â†’ Evilï¼‰æ€§èƒ½ä¸‹é™æ˜æ˜¾ã€‚

- **Vector Filter æ•ˆç‡æé«˜ä½†é²æ£’æ€§å¼±**ï¼š
  - åœ¨éƒ¨åˆ†åœºæ™¯å¤±æ•ˆï¼ˆå¦‚ GSM8Kï¼‰ï¼Œå› å…¶å¿½ç•¥æ¨¡å‹ç”Ÿæˆå·®å¼‚ã€‚

#### ï¼ˆ2ï¼‰çœŸå®æ•°æ®é›† OASST1 ä¸Šçš„å®‰å…¨-èƒ½åŠ›æƒè¡¡ï¼ˆå›¾ 5ï¼‰

- **Vector Filter è¡¨ç°æœ€ä½³**ï¼š
  - ä»…ç”¨ **5% æœ€å®‰å…¨æ•°æ®**ï¼Œå³å¯è¾¾åˆ°å®Œæ•´æ•°æ®é›†çš„èƒ½åŠ›æ°´å¹³ï¼ˆMTBench â‰ˆ 67ï¼‰ï¼ŒåŒæ—¶å°†â€œevilâ€åˆ†æ•°ä» 2.3 é™è‡³ **0.8**ã€‚
  - æ˜¾è‘—æ”¹å–„äº† safety-capability Pareto frontierã€‚

- **Concept Influence ä¸ Influence Function æ¥è¿‘**ï¼š
  - åœ¨å°æ•°æ®é‡ä¸‹ç•¥é€Šäº Vector Filterï¼Œä½†åœ¨ >10% æ•°æ®æ—¶è¡¨ç°æ›´å¥½ã€‚

#### ï¼ˆ3ï¼‰æ•ˆç‡å¯¹æ¯”ï¼ˆè¡¨ 1ï¼‰

| æ–¹æ³• | æ—¶é—´ (s) | ç›¸å¯¹åŠ é€Ÿæ¯” |
|------|--------|----------|
| Influence Function | 1161 | 1.0Ã— |
| Concept Influence | 1170 | 1.0Ã— |
| Projection Difference | 142 | 8.2Ã— |
| **Vector Filter** | **57** | **20.4Ã—** |

> âš ï¸ æ³¨æ„ï¼šæœªè®¡å…¥ Hessian é€†çš„è®¡ç®—å¼€é”€ï¼Œå®é™…å·®è·æ›´å¤§ã€‚

#### ï¼ˆ4ï¼‰æ¶ˆèå®éªŒä¸ç›¸å…³æ€§åˆ†æï¼ˆå›¾ 3, å›¾ 15ï¼‰

- **æ–¹æ³•é—´ç›¸å…³æ€§ä½**ï¼š
  - Vector-based æ–¹æ³•ï¼ˆVector Filter, Projection Difference, Concept Influenceï¼‰ä¹‹é—´ç›¸å…³æ€§é«˜ã€‚
  - ä¸ gradient-based æ–¹æ³•ï¼ˆInfluence Functionï¼‰ç›¸å…³æ€§ä½ï¼Œè¡¨æ˜æ•æ‰çš„æ˜¯ä¸åŒç±»å‹çš„â€œinfluenceâ€ã€‚

- **Concept Influence æ›´èšç„¦è¯­ä¹‰**ï¼ˆå›¾ 4ï¼‰ï¼š
  - Influence Function å½’å› åˆ°â€œæ³•å¾‹æœ¯è¯­â€ã€â€œç¨æ”¶å‡å…â€ç­‰ä¸æŸ¥è¯¢è¯­æ³•ç›¸å…³ä½†æ— å…³â€œevilâ€çš„ç‰¹å¾ã€‚
  - Concept Influence æˆåŠŸå®šä½åˆ°â€œå†å²å‹è¿«â€ã€â€œé˜´è°‹è®ºâ€ã€â€œçŠ¯ç½ªæ€§â€ç­‰çœŸæ­£ä¸â€œevilâ€ç›¸å…³çš„ SAE ç‰¹å¾ã€‚

- **æ¦‚å¿µçº§è¿‡æ»¤æ›´é«˜æ•ˆ**ï¼ˆå›¾ 15ï¼‰ï¼š
  - Concept Influence å°†å½±å“é›†ä¸­åœ¨æ›´å°‘çš„ high-impact æ¦‚å¿µä¸Šï¼Œè¦†ç›– 70% æ•°æ®ä»…éœ€ top 20% æ¦‚å¿µã€‚
  - åœ¨ç§»é™¤ 25â€“80% æ•°æ®æ—¶ï¼Œprecision æ˜æ˜¾é«˜äº Influence Functionã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. âœ… **å°†å¯è§£é‡Šæ€§ç»“æ„ï¼ˆå¦‚ probe, SAEï¼‰èå…¥ TDA å¯æ˜¾è‘—æå‡è¯­ä¹‰å‡†ç¡®æ€§**ã€‚
2. âœ… **Concept Influence åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿ influence functions**ï¼Œå°¤å…¶åœ¨è¯†åˆ«æŠ½è±¡ misaligned è¡Œä¸ºæ–¹é¢ã€‚
3. âœ… **ç®€å•çš„ä¸€é˜¶/é›¶é˜¶è¿‘ä¼¼æ–¹æ³•ï¼ˆå¦‚ Vector Filterï¼‰åœ¨å¤šæ•°åœºæ™¯ä¸‹æ€§èƒ½æ¥è¿‘ç”šè‡³è¶…è¶Šæ¢¯åº¦æ³•ï¼Œä¸”å¿« 20Ã— ä»¥ä¸Š**ã€‚
4. âœ… **æ¦‚å¿µçº§å½’å› ï¼ˆconcept-based filteringï¼‰æ¯”æ•°æ®ç‚¹çº§æ›´é«˜æ•ˆã€æ›´å…·è§£é‡Šæ€§**ã€‚
5. âœ… **åœ¨çœŸå® post-training åœºæ™¯ä¸­ï¼ŒåŸºäº concept çš„è¿‡æ»¤å¯æ˜¾è‘—æ”¹å–„ safety-capability tradeoff**ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§

1. **ä¾èµ–é«˜è´¨é‡ concept å‘é‡**ï¼š
   - è‹¥ probe æˆ– SAE ç‰¹å¾æ— æ³•å‡†ç¡®è¡¨ç¤ºç›®æ ‡è¡Œä¸ºï¼ˆå¦‚ poorly trained probeï¼‰ï¼Œæ•ˆæœä¼šä¸‹é™ã€‚
2. **Vector Filter å¿½ç•¥ fine-tuning åŠ¨æ€**ï¼š
   - ä¸è€ƒè™‘ base model å·²ç”Ÿæˆçš„å†…å®¹ï¼Œå¯èƒ½è¯¯åˆ¤â€œçœ‹ä¼¼ç›¸å…³ä½†æ— å½±å“â€çš„æ•°æ®ã€‚
3. **ä»å—é™äº SAE çš„è¦†ç›–èŒƒå›´**ï¼š
   - å½“å‰ SAE ä»…èƒ½æ•æ‰éƒ¨åˆ†ç¥ç»å…ƒæ¿€æ´»æ¨¡å¼ï¼Œéæ‰€æœ‰è¯­ä¹‰éƒ½èƒ½è¢«ç¨€ç–è¡¨ç¤ºã€‚
4. **å¯¹ domain-shift åœºæ™¯æ•æ„Ÿ**ï¼š
   - Projection Difference å’Œ Vector Filter åœ¨è·¨åŸŸä»»åŠ¡ä¸­æ€§èƒ½ä¸ç¨³å®šã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. **æ‰©å±•è‡³æ›´ä¸°å¯Œçš„å¯è§£é‡Šç»“æ„**ï¼š
   - å¦‚ circuitsã€mechanistic interpretability ä¸­çš„å› æœè·¯å¾„ã€‚
2. **å¼€å‘ principled çš„ group-level attribution æ–¹æ³•**ï¼š
   - åŸºäº SAE æˆ–èšç±»è‡ªåŠ¨è¯†åˆ«è¯­ä¹‰ç°‡å¹¶å½’å› ã€‚
3. **ç³»ç»Ÿæ¯”è¾ƒ base vs. fine-tuned æ¨¡å‹çš„ representation geometry**ï¼š
   - åŒºåˆ†å½’å› æ˜¯æ¥è‡ªè®­ç»ƒåŠ¨æ€è¿˜æ˜¯è¡¨ç¤ºç»“æ„æœ¬èº«ã€‚
4. **æ‰©å±•è‡³å¤šæ¨¡æ€ä¸æ›´å¤§è§„æ¨¡æ¨¡å‹**ï¼š
   - éªŒè¯æ–¹æ³•åœ¨è§†è§‰ã€éŸ³é¢‘ç­‰æ¨¡æ€ä¸‹çš„æœ‰æ•ˆæ€§ã€‚
5. **ç»“åˆä¸»åŠ¨å­¦ä¹ æˆ–æ•°æ®ç¼–è¾‘**ï¼š
   - åˆ©ç”¨ concept influence æŒ‡å¯¼æ•°æ®æ¸…æ´—æˆ–åˆæˆåäº‹å®æ ·æœ¬ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡é€šè¿‡å°† **interpretable representations**ï¼ˆå¦‚ probeã€SAEï¼‰å¼•å…¥ **Training Data Attribution**ï¼Œæå‡ºäº† **Concept Influence** æ¡†æ¶ï¼Œåœ¨ä¿æŒç”šè‡³æå‡å½’å› å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾ **20Ã— çš„æ•ˆç‡æå‡**ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºæ§åˆ¶æä¾›äº†æ›´å¯æ‰©å±•ã€æ›´å¯è§£é‡Šçš„æ–°å·¥å…·ã€‚

</details>

---

### 8. [HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2602.13710)

**Authors**: Xin Yan, Zhenglin Wan, Feiyang Ye, Xingrui Yu, Hangyu Du, Yang You, Ivor Tsang  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.13710v1  

#### Abstract
Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existin...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šHBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜  
Vision-Language-Action (VLA) æ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶**å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜å¼€é”€**é™åˆ¶äº†åœ¨èµ„æºå—é™è®¾å¤‡ï¼ˆå¦‚è¾¹ç¼˜æœºå™¨äººï¼‰ä¸Šçš„éƒ¨ç½²ã€‚è™½ç„¶ Post-Training Quantization (PTQ) æ˜¯ä¸€ç§é«˜æ•ˆçš„å‹ç¼©æ‰‹æ®µï¼Œä½†ç°æœ‰çš„ 1-bit é‡åŒ–æ–¹æ³•åœ¨åº”ç”¨äº VLA æ¨¡å‹æ—¶é¢ä¸´ä¸¥é‡æŒ‘æˆ˜ï¼š

- **é‡åŒ–è¯¯å·®ç´¯ç§¯**ï¼šç”±äº VLA è¾“å‡ºçš„æ˜¯è¿ç»­åŠ¨ä½œï¼Œåœ¨é—­ç¯æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå¾®å°çš„åŠ¨ä½œåå·®ä¼šè¢«åŠ¨åŠ›å­¦æ”¾å¤§ï¼Œå¯¼è‡´ä»»åŠ¡å¤±è´¥ï¼ˆå¦‚æŠ“å–ä¸ç¨³å®šã€è½¨è¿¹æ¼‚ç§»ï¼‰ã€‚
- **æ¨¡æ€ä¸å¹³è¡¡ä¸å¼‚å¸¸å€¼å¹²æ‰°**ï¼šæ ‡å‡† Hessian æ–¹æ³•æ— æ³•å‡†ç¡®è¯†åˆ«å¯¹ç­–ç•¥å…³é”®çš„æƒé‡ï¼Œå› ä¸ºæ¿€æ´»å›¾å—èƒŒæ™¯å™ªå£°å’Œè§†è§‰ token æ•°é‡å¤±è¡¡çš„å½±å“ä¸¥é‡ï¼ˆå³â€œåŒé‡ä¸»å¯¼é—®é¢˜â€ï¼‰ã€‚
- **è·¨æ¨¡æ€æƒé‡æ··åˆç ´å Haar å˜æ¢æ•ˆæœ**ï¼šç›´æ¥åº”ç”¨ Haar å˜æ¢ä¼šå¯¼è‡´ä¸åŒæ¨¡æ€åˆ—ä¹‹é—´äº§ç”Ÿå‰§çƒˆè·³å˜ï¼Œå¼•å…¥é«˜é¢‘å™ªå£°ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šHBVLA æ¡†æ¶

ä½œè€…æå‡º **HBVLA** â€”â€” ä¸€ä¸ªä¸“ä¸º VLA å®šåˆ¶çš„ 1-bit PTQ æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒè®¾è®¡ï¼š

#### ï¼ˆ1ï¼‰**Policy-Aware Enhanced Hessian**
- å¼•å…¥åŸºäºæ¢¯åº¦çš„é‡è¦æ€§çŸ©é˜µ $ S_t $ï¼Œé‡æ–°åŠ æƒæ¯ä¸ª token å¯¹åŠ¨ä½œç”Ÿæˆçš„å½±å“ã€‚
- æ„å»ºä¿®æ­£åçš„ Hessianï¼š$ H = \sum_t S_t X_t X_t^T $
- æœ‰æ•ˆæŠ‘åˆ¶å†—ä½™è§†è§‰ token å’Œ outlier çš„å½±å“ï¼Œç²¾å‡†å®šä½å¯¹ç­–ç•¥æ•æ„Ÿçš„å…³é”®æƒé‡ã€‚

#### ï¼ˆ2ï¼‰**Sparse Orthogonal Transform for Non-Salient Weights**
- é’ˆå¯¹éæ˜¾è‘—æƒé‡ï¼Œå…ˆè¿›è¡Œç¨€ç–æ­£äº¤å˜æ¢ï¼ˆé€šè¿‡æ’åˆ—çŸ©é˜µ $ P $ï¼‰ï¼Œå°†ç›¸ä¼¼åˆ—èšç±»åœ¨ä¸€èµ·ã€‚
- å‡å°‘ Haar å˜æ¢ä¸­çš„è·¨æ¨¡æ€è·³è·ƒï¼Œé™ä½é«˜é¢‘èƒ½é‡ï¼Œæå‡èƒ½é‡ç´§è‡´æ€§ï¼ˆenergy compactionï¼‰ã€‚

#### ï¼ˆ3ï¼‰**Group-wise 1-bit Quantization in Haar Domain**
- åœ¨ Haar åŸŸä¸­å¯¹æ˜¾è‘—å’Œéæ˜¾è‘—æƒé‡åˆ†åˆ«å¤„ç†ï¼š
  - æ˜¾è‘—æƒé‡ï¼šé‡‡ç”¨æ®‹å·®æ„ŸçŸ¥çš„ Column-Haar Quantizationï¼Œè¡¥å¿è¿‘ä¼¼è¯¯å·®ã€‚
  - éæ˜¾è‘—æƒé‡ï¼šä½¿ç”¨ Row-Haar Quantization + **å…±äº«å‡å€¼ binarization**ï¼ˆshared-mean binarizationï¼‰ï¼Œå‡å°‘å…ƒæ•°æ®å¼€é”€ã€‚
- åˆ©ç”¨é¢‘ç‡æ„ŸçŸ¥åˆ†ç»„ç­–ç•¥ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–ç²¾åº¦ä¸æ•ˆç‡å¹³è¡¡ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | HBVLA vs. ç°æœ‰æ–¹æ³• |
|------|------------------|
| **é’ˆå¯¹æ€§** | ä¸“ä¸º VLA è®¾è®¡ï¼Œè€ƒè™‘åŠ¨ä½œè¾“å‡ºçš„è¿ç»­æ€§å’Œé—­ç¯ç¨³å®šæ€§ï¼›è€Œ BiLLM/HBLLM/BiVLM ä¸»è¦é¢å‘ LLM/VLMï¼Œä¸å…³æ³¨ç‰©ç†æ‰§è¡Œé²æ£’æ€§ |
| **å‡†ç¡®æ€§** | æ”¿ç­–æ„ŸçŸ¥ Hessian æ›´å¥½åœ°ä¿ç•™å…³é”®è¯­ä¹‰è·¯å¾„ï¼Œé¿å…æ³¨æ„åŠ›æ¼‚ç§» |
| **ç»“æ„é€‚åº”æ€§** | Sparse transform è§£å†³äº†å¤šæ¨¡æ€æƒé‡äº¤é”™å¸¦æ¥çš„ Haar æ€§èƒ½ä¸‹é™é—®é¢˜ |
| **å­˜å‚¨æ•ˆç‡** | Shared-mean + åˆ†ç»„é‡åŒ–æ˜¾è‘—é™ä½å¹³å‡ bit-width å’Œ metadata å¼€é”€ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†ä¸å¹³å°

| å¹³å° | æè¿° |
|------|------|
| **LIBERO** | åŒ…å« 100 ä¸ªä»»åŠ¡ã€5000 æ¡è½¨è¿¹çš„ä»¿çœŸåŸºå‡†ï¼Œæ¶µç›–ç©ºé—´ã€å¯¹è±¡ã€ç›®æ ‡ã€é•¿è§†é‡å››ç±»ä»»åŠ¡ï¼ˆLIBERO-Spatial/Object/Goal/Longï¼‰ï¼ŒåŸºäº MuJoCo ä¸­çš„ Franka æœºæ¢°è‡‚ |
| **SIMPLER** | é«˜ä¿çœŸæ¨¡æ‹Ÿç¯å¢ƒï¼Œå¤ç°çœŸå®ä¸–ç•Œåœºæ™¯ï¼Œæ”¯æŒ Google æœºå™¨äººæ‰‹è‡‚æµ‹è¯•ï¼ŒåŒ…å«ä¸¤ç§è®¾å®šï¼š<br>â€¢ Visual Matchingï¼ˆä½å¹²æ‰°ï¼‰<br>â€¢ Variant Aggregationï¼ˆé«˜éšæœºæ€§ï¼šå…‰ç…§ã€èƒŒæ™¯ã€å¹²æ‰°ç‰©ï¼‰ |
| **Real-world: Mobile ALOHA** | å®ä½“åŒè‡‚åä½œæœºå™¨äººå¹³å°ï¼Œç”¨äºéªŒè¯ç°å®éƒ¨ç½²èƒ½åŠ›ï¼Œæµ‹è¯•ä¸‰å¤§ä»»åŠ¡ï¼š<br>â€¢ Pick and Placeï¼ˆä¸è§„åˆ™ç‰©ä½“ï¼‰<br>â€¢ Sequenced Instructionï¼ˆæ±‰è¯ºå¡”ï¼‰<br>â€¢ Flexible Foldingï¼ˆæ¯›å·¾æŠ˜å ï¼‰ |

---

### ğŸ“Š å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

| é¡¹ç›® | è®¾ç½® |
|------|------|
| **æ¨¡å‹** | OpenVLA, OpenVLA-OFT, CogACT |
| **é‡åŒ–æ–¹å¼** | ä»…å¯¹ vision encoder å’Œ language model è¿›è¡Œ 1-bit é‡åŒ–ï¼Œå…¶ä½™ä¿æŒ full precisionï¼ˆBF16ï¼‰ |
| **æ ¡å‡†é›†** | ä»è®­ç»ƒé›†ä¸­éšæœºé‡‡æ · 256 æ¡è½¨è¿¹ |
| **å—å¤§å°** | 128 |
| **Haar å‚æ•°** | å›ºå®šæ ¸ $ h_{lo}=[0.5, 0.5], h_{hi}=[0.5, -0.5] $ï¼Œstride=2 |
| **è¯„ä¼°æŒ‡æ ‡** | **Success Rate (SR)** â€”â€” æˆåŠŸå®Œæˆä»»åŠ¡çš„æ¯”ä¾‹ |

---

### âš”ï¸ åŸºçº¿æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **BiLLM** | LLM-oriented 1-bit PTQ | åŸºäº GPTQ çš„ OBQ æ–¹æ³•ï¼Œæœªè€ƒè™‘å¤šæ¨¡æ€ç‰¹æ€§ |
| **HBLLM** | Haar-based 1-bit PTQ for LLM | ä½¿ç”¨ Haar å˜æ¢è¿›è¡Œé¢‘åŸŸé‡åŒ–ï¼Œä½†ç¼ºä¹ç­–ç•¥æ„ŸçŸ¥æœºåˆ¶ |
| **BiVLM** | Multimodal 1-bit PTQ | é‡‡ç”¨é«˜æ–¯åˆ†ä½æ•°åˆ’åˆ†æ˜¾è‘—æƒé‡ï¼Œä½†åœ¨ VLA ä¸Šæœªèƒ½æ•æ‰åŠ¨ä½œç›¸å…³åˆ— |

> æ‰€æœ‰ baseline å‡é€‚é…è‡³ VLA æ¶æ„ï¼Œå¹¶ç»Ÿä¸€é‡åŒ– vision/language backboneã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### âœ… åœ¨ **LIBERO** ä¸Šçš„è¡¨ç°ï¼ˆOpenVLA-OFTï¼‰

| æ–¹æ³• | Spatial | Object | Goal | Long | Avg SR (%) | Î” vs FP |
|------|--------|--------|------|------|------------|---------|
| OpenVLA-OFT (FP) | 97.6 | 98.4 | 97.9 | 94.5 | **97.1** | â€” |
| BiLLM | 59.2 | 61.4 | 65.8 | 44.3 | 57.7 | -39.4 |
| BiVLM | 67.8 | 69.7 | 68.4 | 48.9 | 64.0 | -33.1 |
| HBLLM | 87.2 | 76.0 | 89.6 | 62.0 | 79.2 | -17.9 |
| **HBVLA (Ours)** | **89.3** | **97.8** | **91.3** | **82.7** | **90.3** | **-6.8** |

> ğŸ’¡ **ç»“è®º**ï¼šHBVLA ä¿ç•™äº† **92.2%** çš„ full-precision æ€§èƒ½ï¼Œè¿œè¶… SOTA æ–¹æ³•ã€‚

---

#### âœ… åœ¨ **SIMPLER** ä¸Šçš„è¡¨ç°ï¼ˆCogACTï¼‰

| æ–¹æ³• | Visual Matching (Avg) | Variant Aggregation (Avg) | Overall Avg |
|------|------------------------|----------------------------|-------------|
| CogACT (FP) | 74.8 | 61.3 | â€” |
| BiLLM | 28.8 (-46.0) | 14.9 (-46.4) | â€” |
| BiVLM | 57.1 (-17.7) | 47.4 (-13.9) | â€” |
| HBLLM | 62.3 (-12.5) | 47.9 (-13.4) | â€” |
| **HBVLA (Ours)** | **70.0 (-4.8)** | **51.0 (-10.3)** | **60.5** |

> ğŸ’¡ **ç»“è®º**ï¼šHBVLA åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ Variant Aggregation ä¸‹ä»è¡¨ç°æœ€ä¼˜ï¼Œè¯´æ˜å…¶æ›´å¼ºçš„é²æ£’æ€§ã€‚

---

#### âœ… åœ¨ **Mobile ALOHA** ä¸Šçš„çœŸå®ä¸–ç•Œè¡¨ç°

| ä»»åŠ¡ | FP Model | HBVLA | BiLLM | HBLLM |
|------|--------|--------|--------|--------|
| Pick and Place | 95.8% | 93.3% | 50.0% | 83.3% |
| Sequenced Instruction | 86.7% | 83.3% | 60.0% | 80.0% |
| Flexible Folding | 80.0% | 76.7% | 40.0% | 66.7% |

> å›¾ 3 æ˜¾ç¤ºï¼šHBVLA åœ¨ä¸‰é¡¹ä»»åŠ¡ä¸Šå‡æ¥è¿‘ full-precision è¡¨ç°ï¼Œä¸”æ˜¾è‘—ä¼˜äºæ‰€æœ‰ baselineã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰**Permutation Criterion for Non-Salient Columns**

| å‡†åˆ™ | Visual Matching â†“ | Variant Aggregation â†“ |
|------|------------------|-----------------------|
| $ l_1 $-norm | 11.6% | 15.6% |
| $ l_2 $-norm | **8.8%** | **12.8%** |

> âœ… $ l_2 $-norm æ›´å¥½æ•è·åˆ—é—´èƒ½é‡åˆ†å¸ƒï¼Œæå‡é‡åŒ–è´¨é‡ã€‚

---

#### ï¼ˆ2ï¼‰**Policy-Aware Hessian çš„æœ‰æ•ˆæ€§**

| Hessian ç±»å‹ | Visual Matching â†“ | Variant Aggregation â†“ |
|--------------|------------------|------------------------|
| Standard | 12.5% | 13.4% |
| **Policy-Aware** | **10.3%** | **12.1%** |

> âœ… æ”¿ç­–æ„ŸçŸ¥ Hessian èƒ½æ›´å‡†ç¡®è¯†åˆ«å…³é”®æƒé‡ï¼Œå‡å°‘ä»»åŠ¡å¤±è´¥ç‡ã€‚

---

#### ï¼ˆ3ï¼‰**ç»„ä»¶æ•æ„Ÿæ€§åˆ†æï¼ˆFigure 4ï¼‰**

| ç»„ä»¶ | æ•æ„Ÿåº¦æ’åº |
|------|-----------|
| Vision Encoder | æœ€é²æ£’ï¼ˆé‡åŒ–åæ€§èƒ½ä¸‹é™æœ€å°ï¼‰ |
| Language Model | ä¸­ç­‰æ•æ„Ÿ |
| Projector / Action Head | **æœ€æ•æ„Ÿ**ï¼Œè½»å¾®ç²¾åº¦æŸå¤±å³å¯¼è‡´ä¸¥é‡é€€åŒ– |

> â— ç»“è®ºï¼šåº”ä¼˜å…ˆä¿æŠ¤ projector å’Œ action head ä¸­çš„å…³é”®æƒé‡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **VLA æ¨¡å‹å„ç»„ä»¶å¯¹é‡åŒ–æ•æ„Ÿåº¦å·®å¼‚å¤§**ï¼š
   - Vision encoder æœ€é²æ£’ï¼›
   - Projector å’Œ Action Head æå…¶æ•æ„Ÿï¼Œéœ€é‡ç‚¹ä¿æŠ¤ã€‚

2. **ä¼ ç»Ÿ Hessian ä¸é€‚ç”¨äº VLA**ï¼š
   - å—è§†è§‰ token å¤±è¡¡å’ŒèƒŒæ™¯ outlier å½±å“ï¼Œæ˜“è¯¯åˆ¤é‡è¦æƒé‡ï¼›
   - æ”¿ç­–æ„ŸçŸ¥ Hessian æ˜¾è‘—æ”¹å–„ saliency ä¼°è®¡å‡†ç¡®æ€§ã€‚

3. **è·¨æ¨¡æ€æƒé‡äº¤é”™ç ´å Haar å˜æ¢æ•ˆæœ**ï¼š
   - ç›´æ¥åº”ç”¨ Haar ä¼šå› åˆ—é—´å¼‚è´¨æ€§å¼•å…¥é«˜é¢‘å™ªå£°ï¼›
   - Sparse orthogonal transform å¯æœ‰æ•ˆç¼“è§£æ­¤é—®é¢˜ã€‚

4. **HBVLA å®ç°è¿‘ä¹æ— æŸçš„ 1-bit å‹ç¼©**ï¼š
   - åœ¨å¤šä¸ª benchmark ä¸Šä¿ç•™ >90% çš„ full-precision æ€§èƒ½ï¼›
   - åœ¨çœŸå®æœºå™¨äººä¸Šä»…é€ æˆè½»å¾® success rate ä¸‹é™ï¼Œå…·å¤‡å¼º deployabilityã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

| å±€é™ | è¯´æ˜ |
|------|------|
| **ä»…é€‚ç”¨äº PTQ** | ä¸æ¶‰åŠ QATï¼Œè‹¥å…è®¸å¾®è°ƒå¯èƒ½è·å¾—æ›´é«˜æ€§èƒ½ |
| **éƒ¨åˆ†æ¨¡å—ä»ä¸º full precision** | å¦‚ action headã€proprioceptive projector æœªå®Œå…¨é‡åŒ–ï¼Œæœªæ¥å¯æ¢ç´¢ç«¯åˆ°ç«¯å…¨æ¨¡å‹ 1-bit åŒ– |
| **Haar å˜æ¢å±‚çº§æœ‰é™** | å½“å‰ä»…ä½¿ç”¨ one-level Haarï¼Œæ›´æ·±åˆ†è§£å¯èƒ½å¸¦æ¥é¢å¤–å¢ç›Šä½†å¢åŠ å¤æ‚åº¦ |
| **ä¾èµ–å±€éƒ¨æ¢¯åº¦ä¼°è®¡** | è™½ç„¶é«˜æ•ˆï¼Œä½†ä»ä¸ºè¿‘ä¼¼æ–¹æ³•ï¼Œéå…¨å±€æœ€ä¼˜è§£ |

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **æ‰©å±•è‡³æ›´ä½æ¯”ç‰¹ï¼ˆå¦‚ ternary æˆ– sub-1-bitï¼‰é‡åŒ–**
2. **ç»“åˆ QAT è¿›è¡Œ fine-tuning + PTQ ååŒä¼˜åŒ–**
3. **æ¢ç´¢åŠ¨æ€é‡åŒ–ç­–ç•¥**ï¼šæ ¹æ®ä»»åŠ¡æŒ‡ä»¤è‡ªé€‚åº”è°ƒæ•´é‡åŒ–ç²’åº¦
4. **åº”ç”¨äºæ›´å¤§è§„æ¨¡ VLA æ¨¡å‹**ï¼ˆå¦‚ UniVLAã€RT-2-Xï¼‰
5. **ç¡¬ä»¶ååŒè®¾è®¡**ï¼šå¼€å‘æ”¯æŒ Haar åŸŸè¿ç®—çš„ä¸“ç”¨åŠ é€Ÿå™¨

---

## âœ… æ€»ç»“

HBVLA æ˜¯é¦–ä¸ªä¸“ä¸º **Vision-Language-Action (VLA)** æ¨¡å‹è®¾è®¡çš„ **1-bit Post-Training Quantization** æ¡†æ¶ï¼Œé€šè¿‡ **policy-aware Hessian + sparse orthogonal transform + Haar-domain group-wise quantization** çš„ç»„åˆï¼ŒæˆåŠŸè§£å†³äº†æç«¯é‡åŒ–ä¸‹åŠ¨ä½œé€€åŒ–çš„é—®é¢˜ã€‚å®éªŒè¯æ˜å…¶åœ¨ **LIBEROã€SIMPLER å’Œ Mobile ALOHA** ä¸Šå‡å¤§å¹…è¶…è¶Šç°æœ‰ SOTA æ–¹æ³•ï¼Œä¸ºå¤§è§„æ¨¡ VLA æ¨¡å‹åœ¨èµ„æºå—é™æœºå™¨äººå¹³å°ä¸Šçš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†å®ç”¨åŸºç¡€ã€‚

</details>

---

### 9. [Data-driven Bi-level Optimization of Thermal Power Systems with embedded Artificial Neural Networks](https://arxiv.org/abs/2602.13746)

**Authors**: Talha Ansar, Muhammad Mujtaba Abbas, Ramit Debnath, Vivek Dua, Waqar Muhammad Ashraf  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.13746v1  

#### Abstract
Industrial thermal power systems have coupled performance variables with hierarchical order of importance, making their simultaneous optimization computationally challenging or infeasible. This barrier limits the integrated and computationally scaleable operation optimization of industrial thermal p...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šData-driven Bi-level Optimization of Thermal Power Systems with embedded Artificial Neural Networks

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å·¥ä¸šçƒ­åŠ›å‘ç”µç³»ç»Ÿä¸­çš„æ€§èƒ½å˜é‡å…·æœ‰**å±‚çº§é‡è¦æ€§ç»“æ„**ï¼ˆå¦‚ç”µå‚çº§ç»æµç›®æ ‡ä¼˜å…ˆäºé”…ç‚‰ç‡ƒæ–™æ¶ˆè€—ä¼˜åŒ–ï¼‰ï¼Œä¼ ç»Ÿçš„å¤šç›®æ ‡ä¼˜åŒ–éš¾ä»¥æœ‰æ•ˆå¤„ç†è¿™ç§åµŒå¥—å†³ç­–å…³ç³»ã€‚ç›´æ¥æ±‚è§£åŒå±‚ä¼˜åŒ–ï¼ˆbi-level optimizationï¼‰é—®é¢˜åœ¨è®¡ç®—ä¸Šæå…·æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶åœ¨é¢å¯¹å¤§è§„æ¨¡ã€éå‡¸ã€è€¦åˆå˜é‡æ—¶ï¼Œå¸¸å¯¼è‡´è®¡ç®—æˆæœ¬è¿‡é«˜æˆ–æ— å¯è¡Œè§£ã€‚

æ­¤å¤–ï¼Œç°æœ‰ç ”ç©¶ä¸­ç¼ºä¹å°†**æœºå™¨å­¦ä¹ æ¨¡å‹å®Œæ•´åµŒå…¥åŒå±‚æ¡†æ¶**å¹¶å®ç°**é²æ£’ä¼˜åŒ–**çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®å·¥ä¸šåœºæ™¯ä¸‹çš„å¯æ‰©å±•æ€§å’Œå®æ—¶æ€§ä¸è¶³ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°
æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„ **ANN-KKT æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

- **ANN å®Œå…¨æ›¿ä»£åŒå±‚ç›®æ ‡å‡½æ•°**ï¼šé¦–æ¬¡å°†äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰ä½œä¸ºä»£ç†æ¨¡å‹ï¼ˆsurrogate modelï¼‰åŒæ—¶ç”¨äºä¸Šå±‚å’Œä¸‹å±‚çš„ç›®æ ‡å‡½æ•°å»ºæ¨¡ï¼Œå®ç°äº†ä»æ•°æ®é©±åŠ¨çš„è§’åº¦é€¼è¿‘å¤æ‚éçº¿æ€§å…³ç³»ã€‚
  
- **KKT æ¡ä»¶è§£æåµŒå…¥**ï¼šé€šè¿‡ Karush-Kuhn-Tucker (KKT) æœ€ä¼˜æ€§æ¡ä»¶å°†ä¸‹å±‚ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºçº¦æŸï¼Œä»è€Œå°†åŸåŒå±‚é—®é¢˜é‡æ„ä¸ºå•å±‚æ•°å­¦è§„åˆ’é—®é¢˜ï¼ˆMPECï¼‰ï¼Œä¾¿äºé«˜æ•ˆæ±‚è§£ã€‚

- **Fischer-Burmeister å‡½æ•°å¢å¼ºæ•°å€¼ç¨³å®šæ€§**ï¼šé’ˆå¯¹äº’è¡¥æ¾å¼›æ¡ä»¶ï¼ˆcomplementarity slacknessï¼‰å¼•èµ·çš„æ•°å€¼ä¸ç¨³å®šæ€§ï¼Œé‡‡ç”¨ Fischer-Burmeister (FB) å‡½æ•°è¿›è¡Œå¹³æ»‘é‡æ„ï¼Œæå‡ä¼˜åŒ–å™¨æ”¶æ•›èƒ½åŠ›ã€‚

- **Mahalanobis è·ç¦»çº¦æŸä¿è¯åŸŸä¸€è‡´æ€§**ï¼šå¼•å…¥åŸºäºå†å²æ“ä½œæ•°æ®åæ–¹å·®ç»“æ„çš„ Mahalanobis è·ç¦»çº¦æŸï¼Œç¡®ä¿æœ€ä¼˜è§£ä½äºå®é™…å¯è¡Œçš„æ“ä½œåŒ…ç»œå†…ï¼Œé¿å…ç‰©ç†ä¸å¯è¡Œè§£ã€‚

- **æ‰©å±•è‡³é²æ£’ä¼˜åŒ–åœºæ™¯**ï¼šè¿›ä¸€æ­¥å°†è¯¥æ¡†æ¶åº”ç”¨äº**ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„é²æ£’æ“ä½œç©ºé—´è¯†åˆ«**ï¼Œé€šè¿‡â€œå¯¹æŠ—å¼â€ä¸‹å±‚é—®é¢˜å¯»æ‰¾æœ€åæƒ…å†µæ‰°åŠ¨ï¼Œæœ€å¤§åŒ–ç¨³å®šåŠå¾„ $ \rho $ï¼Œä»¥ä¿éšœçƒ­æ•ˆç‡é«˜äºè®¾å®šé˜ˆå€¼ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **è®¡ç®—æ•ˆç‡** | æ±‚è§£æ—¶é—´ä»…éœ€ **0.22â€“0.88 ç§’**ï¼Œè¿œä½äºä¼ ç»Ÿè¿­ä»£æ³•æˆ–åˆ†æ”¯å®šç•Œç­‰å…¨å±€æœç´¢ç®—æ³•ï¼Œé€‚ç”¨äºåŠ¨æ€æ§åˆ¶å“åº”ã€‚ |
| **å¯æ‰©å±•æ€§** | æ”¯æŒå¤§è§„æ¨¡å·¥ç¨‹ç³»ç»Ÿï¼ˆå¦‚ 660MW ç…¤ç”µã€395MW ç‡ƒæœºï¼‰ï¼Œå…·å¤‡å‘ Industry 5.0 åœºæ™¯æ¨å¹¿çš„èƒ½åŠ›ã€‚ |
| **æ•°æ®é©±åŠ¨é€‚åº”æ€§** | åˆ©ç”¨çœŸå®è¿è¡Œæ•°æ®è®­ç»ƒ ANNï¼Œèƒ½æ•æ‰å™ªå£°ã€å¼‚æ­¥æ§åˆ¶ç­–ç•¥ä¸‹çš„å¤æ‚éçº¿æ€§æ˜ å°„ã€‚ |
| **é²æ£’æ€§æ”¯æŒ** | é¦–æ¬¡å®ç°åŸºäº ANN-KKT çš„**é²æ£’æ“ä½œåŒ…ç»œè¯†åˆ«**ï¼Œå¯å¯¹å†²è¿‡ç¨‹ä¸ç¡®å®šæ€§ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **åŸºå‡†æµ‹è¯•é—®é¢˜ï¼ˆBenchmark Problemsï¼‰**ï¼š
  - Convex & Convex (C&C)
  - Convex & Non-convex (C&NC)
  - Non-convex & Non-convex (NC&NC)
  - åˆæˆç”Ÿæˆ 10,000 ä¸ªæ ·æœ¬ï¼ŒæŒ‰ 70%/15%/15% åˆ†å‰²ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ã€‚

- **çœŸå®å·¥ä¸šç³»ç»Ÿæ•°æ®**ï¼š
  - **660 MW ç…¤ç”µæœºç»„**ï¼šæ¥è‡ªæ–‡çŒ® [55] çš„è¿è¡Œæ•°æ®ï¼Œå…± 1,279 ä¸ªæ ·æœ¬ï¼Œè¾“å…¥å˜é‡åŒ…æ‹¬ CFRã€AFRã€MSPã€MST ç­‰ 8 ä¸ªæ“ä½œå‚æ•°ã€‚
  - **395 MW ç‡ƒæ°”è½®æœºç³»ç»Ÿ**ï¼šåŒæ ·æ¥è‡ª [55]ï¼Œå…± 579 ä¸ªæ ·æœ¬ï¼Œæ¶‰åŠ CDPã€GFFRã€ATã€APã€AH ç­‰ 9 ä¸ªå˜é‡ã€‚

æ‰€æœ‰è¾“å…¥å‡ç» min-max å½’ä¸€åŒ–è‡³ [0,1] åŒºé—´ã€‚

### å®éªŒè®¾ç½®
- **ANN æ¶æ„**ï¼šä¸‰å±‚å‰é¦ˆç½‘ç»œï¼ˆæµ…å±‚ï¼‰ï¼Œéšè—å±‚æ¿€æ´»å‡½æ•°ä¸º SiLUï¼Œè¾“å‡ºå±‚ä¸ºçº¿æ€§å‡½æ•°ï¼Œä¿è¯æ¢¯åº¦è¿ç»­å¯å¾®ã€‚
- **è¶…å‚æ•°ä¼˜åŒ–**ï¼šä½¿ç”¨ Hyperopt åº“ä¸­çš„ Tree Parzen Estimator è¿›è¡Œè´å¶æ–¯è°ƒå‚ï¼Œä¼˜åŒ– learning rateã€hidden neuronsã€L1/L2 æ­£åˆ™é¡¹ã€‚
- **è®­ç»ƒç»†èŠ‚**ï¼šæœ€å¤§ 5,000 è½®è®­ç»ƒï¼Œæ—©åœæœºåˆ¶ï¼ˆpatience=200ï¼‰ï¼ŒæŸå¤±å‡½æ•°å« MSE å’Œ L1 æ­£åˆ™é¡¹ã€‚
- **æ±‚è§£å™¨**ï¼š
  - ä¸»è¦ä½¿ç”¨ **IPOPT**ï¼ˆå¼€æºã€é€‚åˆéçº¿æ€§ä¼˜åŒ–ï¼‰
  - å¯¹æ¯”ä½¿ç”¨ **BARON**ï¼ˆå…¨å±€ä¼˜åŒ–æ±‚è§£å™¨ï¼‰
- **å¹³å°é…ç½®**ï¼šIntel i7-8850H CPU, 32GB RAM, NVIDIA Quadro P1000 GPUï¼›Python + Pyomo + GAMS æ¥å£ã€‚

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **RÂ²** | å†³å®šç³»æ•°ï¼Œè¡¡é‡ ANN æ¨¡å‹æ‹Ÿåˆç²¾åº¦ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰ |
| **RMSE** | å‡æ–¹æ ¹è¯¯å·®ï¼Œåæ˜ é¢„æµ‹åå·®å¤§å° |
| **Objective Value** | åŒå±‚ä¼˜åŒ–æœ€ç»ˆç›®æ ‡å€¼ï¼ˆå¦‚æœ€å¤§åŠŸç‡ã€æœ€å° THRï¼‰ |
| **CPU Time (s)** | å•æ¬¡æ±‚è§£è€—æ—¶ï¼Œä½“ç°è®¡ç®—æ•ˆç‡ |
| **Feasibility** | æ˜¯å¦æ»¡è¶³æ‰€æœ‰çº¦æŸæ¡ä»¶ï¼ˆç‰¹åˆ«æ˜¯ KKT å’Œ Mahalanobis çº¦æŸï¼‰ |
| **Stability Radius ($\rho$)** | é²æ£’ä¼˜åŒ–ä¸­å…è®¸çš„æœ€å¤§æ‰°åŠ¨èŒƒå›´ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Bi-level-KKT**ï¼šä¼ ç»Ÿ KKT é‡æ„æ–¹æ³•ï¼ˆæ—  ANN æ›¿ä»£ï¼‰
- **åŸå§‹åŒå±‚è§£ï¼ˆBi-level solutionï¼‰**ï¼šæ¥è‡ªæ ‡å‡†æµ‹è¯•é›†çš„ç†è®ºæœ€ä¼˜è§£
- **IPOPT vs BARON**ï¼šå±€éƒ¨ vs å…¨å±€æ±‚è§£å™¨æ€§èƒ½æ¯”è¾ƒ

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

| ç³»ç»Ÿ | æœ€å¤§åŠŸç‡è¾“å‡º | å¯¹åº” THR | æ±‚è§£æ—¶é—´ | RÂ² (Power) | RÂ² (THR) |
|------|--------------|----------|---------|------------|-----------|
| 660MW ç…¤ç”µ | **583 MW** | **7337 kJ/kWh** | 0.334 s | 0.99 | 0.72 |
| 395MW ç‡ƒæœº | **402 MW** | **7542 kJ/kWh** | 0.409 s | 0.99 | 0.85 |

> æ³¨ï¼šTHRï¼ˆTurbine Heat Rateï¼‰è¶Šä½è¡¨ç¤ºçƒ­æ•ˆç‡è¶Šé«˜ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

#### åœ¨ Benchmark é—®é¢˜ä¸Šçš„è¡¨ç°ï¼ˆè§ Table 1ï¼‰

| é—®é¢˜ç±»å‹ | æ–¹æ³• | x | y | Objective (F) | æ—¶é—´ (s) |
|--------|------|----|----|-------------|---------|
| C&C | Bi-level-KKT | 1.0 | 3.0 | 5.0 | 0.14 |
| C&C | **ANN-KKT (BARON)** | 0.981 | 2.962 | 4.976 | **1.27** |
| C&NC | Bi-level-KKT | 1.0 | 0.0 | 0.0 | 0.12 |
| C&NC | **ANN-KKT (BARON)** | 1.014 | 0.0 | 0.0034 | **1.27** |
| NC&NC | Bi-level-KKT | -0.4191 | -1.0 | 0.1756 | 0.09 |
| NC&NC | **ANN-KKT (BARON)** | -0.4191 | -1.0 | 0.1750 | **0.39** |

- **ç»“è®º**ï¼šANN-KKT èƒ½å¤Ÿä»¥æå°è¯¯å·®é€¼è¿‘çœŸå® bi-level è§£ï¼Œå°½ç®¡è®¡ç®—æ—¶é—´ç•¥é•¿ï¼ˆå› éœ€è®¡ç®— ANN æ¢¯åº¦ï¼‰ï¼Œä½†åœ¨å·¥ç¨‹å¯æ¥å—èŒƒå›´å†…ã€‚

#### IPOPT æ±‚è§£å™¨è¡¨ç°ï¼ˆè§ Appendix B.4ï¼‰
- åœ¨ **NC&NC** é—®é¢˜ä¸ŠæˆåŠŸæ”¶æ•›åˆ°æœ€ä¼˜è§£ï¼›
- åœ¨ C&C å’Œéƒ¨åˆ† C&NC ä¸Šå‡ºç°ä¸æ”¶æ•›æˆ–ä¸å¯è¡Œè§£ï¼Œè¯´æ˜ ANN å¼•å…¥çš„éå‡¸æ€§ä¼šå½±å“å±€éƒ¨æ±‚è§£å™¨è¡¨ç°ï¼›
- è¡¨æ˜ **ANN-KKT æ›´é€‚åˆé…åˆå…¨å±€æˆ–å¼ºå¥çš„å±€éƒ¨æ±‚è§£å™¨ä½¿ç”¨**ã€‚

### æ¶ˆèå®éªŒä¸å…³é”®åˆ†æ

#### ä¸åŒ Mahalanobis å®¹å·® $ T $ çš„å½±å“ï¼ˆç…¤ç”µç³»ç»Ÿï¼‰
- å½“ $ T = 94\% $ æ—¶è¾¾åˆ°æœ€ä½³è§£ï¼š**583 MW @ 7337 kJ/kWh**
- è¾ƒä½ $ T $ï¼ˆå¦‚ 81%-82%ï¼‰é™åˆ¶æœç´¢ç©ºé—´ï¼Œå¯¼è‡´æ¬¡ä¼˜è§£ï¼›
- è¾ƒé«˜ $ T $ï¼ˆå¦‚ 95%ï¼‰å¯èƒ½å¯¼è‡´çº¦æŸè¿åï¼ˆinfeasibleï¼‰ï¼›
- å¤šæ•°å¯è¡Œè§£ CPU æ—¶é—´ < 1 ç§’ï¼ŒéªŒè¯äº†**å®æ—¶ä¼˜åŒ–æ½œåŠ›**ã€‚

#### é²æ£’ä¼˜åŒ–ç»“æœï¼ˆç‡ƒæ°”è½®æœºç³»ç»Ÿï¼‰
- æˆåŠŸè¯†åˆ«å‡ºåœ¨ä¸åŒ **Target Efficiency Floor (TEtarget)** ä¸‹çš„é²æ£’æ“ä½œåŒºé—´ï¼ˆè§ Table 2ï¼‰
- å½“ TEtarget ä» 38% æå‡è‡³ 43%ï¼Œç¨³å®šåŠå¾„ $ \rho $ æ˜¾è‘—å‡å° â†’ è¡¨æ˜é«˜æ•ˆç‡è¦æ±‚ç‰ºç‰²é²æ£’æ€§
- åœ¨ TEtarget = 42% æ—¶ä»å¯ç»´æŒå¹³å‡ TE > 42%ï¼Œè¯æ˜æ¡†æ¶å¯å®ç°**é«˜æ€§èƒ½ä¸”é²æ£’çš„æ“ä½œåŒ…ç»œè®¾è®¡**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ANN-KKT æ˜¯ä¸€ç§å¯æ‰©å±•ã€é«˜æ•ˆçš„åŒå±‚ä¼˜åŒ–æ¡†æ¶**ï¼Œèƒ½å¤Ÿåœ¨ç§’çº§æ—¶é—´å†…å®Œæˆå¤§å‹çƒ­åŠ›ç³»ç»Ÿçš„ä¼˜åŒ–å†³ç­–ã€‚
2. å°† ANN åŒæ—¶åµŒå…¥ä¸Šä¸‹å±‚ç›®æ ‡å‡½æ•°æ˜¯å¯è¡Œä¸”æœ‰æ•ˆçš„ï¼Œå°¤å…¶é€‚ç”¨äºç”±æ•°æ®é©±åŠ¨çš„å¤æ‚éçº¿æ€§ç³»ç»Ÿå»ºæ¨¡ã€‚
3. ç»“åˆ Mahalanobis è·ç¦»çº¦æŸå¯æ˜¾è‘—æé«˜è§£çš„**ç‰©ç†å¯å®æ–½æ€§**ï¼Œé˜²æ­¢è¿›å…¥å¼‚å¸¸å·¥å†µåŒºåŸŸã€‚
4. æ‰©å±•è‡³é²æ£’ä¼˜åŒ–åï¼Œèƒ½å¤Ÿé‡åŒ–ä¸ç¡®å®šæ€§ä¸‹çš„æ“ä½œå¼¹æ€§ï¼Œå¹¶ä¸ºæ“ä½œå‘˜æä¾›å®‰å…¨è¾¹ç•ŒæŒ‡å¯¼ã€‚
5. æœ¬æ–¹æ³•æ¨åŠ¨äº† AI ä¸å·¥ä¸šæ§åˆ¶ç³»ç»Ÿèåˆï¼ŒåŠ©åŠ›å®ç° **Industry 5.0** ä¸­çš„äººæœºååŒæ™ºèƒ½è¿ç»´ã€‚

### å±€é™æ€§
1. **Mahalanobis å‚æ•° $ T $ éš¾ä»¥è‡ªåŠ¨è°ƒèŠ‚**ï¼šå½“å‰ä¾èµ–äººå·¥è®¾å®šï¼Œå½±å“è‡ªåŠ¨åŒ–éƒ¨ç½²ã€‚
2. **ANN å¼•å…¥éå‡¸æ€§å¯èƒ½ç ´åæ”¶æ•›æ€§**ï¼šå³ä½¿åŸé—®é¢˜æ˜¯å‡¸çš„ï¼ŒANN çš„è¿‘ä¼¼ä¹Ÿå¯èƒ½äº§ç”Ÿå¤šä¸ªå±€éƒ¨æå°ç‚¹ï¼Œå½±å“ IPOPT ç­‰å±€éƒ¨æ±‚è§£å™¨è¡¨ç°ã€‚
3. **æœªå®Œå…¨è§£å†³å…¨å±€æœ€ä¼˜æ€§ä¿è¯**ï¼šè™½ç„¶ KKT æä¾›å¿…è¦æ¡ä»¶ï¼Œä½†å¯¹äºéå‡¸é—®é¢˜æ— æ³•ä¿è¯å…¨å±€æœ€ä¼˜ã€‚
4. **ä¾èµ–é«˜è´¨é‡å†å²æ•°æ®**ï¼šè‹¥è®­ç»ƒæ•°æ®è¦†ç›–ä¸è¶³æˆ–å­˜åœ¨åå€šï¼Œå¯èƒ½å¯¼è‡´ä»£ç†æ¨¡å‹å¤±çœŸã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **ANN æ¨¡å‹å‡¸åŒ–æŠ€æœ¯ç ”ç©¶**ï¼šæ¢ç´¢å¦‚ä½•æ„é€ å…·æœ‰å‡¸æ€§è´¨çš„ ANN æ¶æ„ï¼Œä»¥åŠ é€Ÿä¼˜åŒ–æ”¶æ•›å¹¶æå‡ç¨³å®šæ€§ã€‚
2. **è‡ªé€‚åº” $ T $ è°ƒæ•´æœºåˆ¶**ï¼šå¼€å‘åŸºäºå¼ºåŒ–å­¦ä¹ æˆ–åœ¨çº¿å­¦ä¹ çš„åŠ¨æ€å®¹å·®è°ƒæ•´ç­–ç•¥ã€‚
3. **é›†æˆæ›´å¤šä¸ç¡®å®šæ€§å»ºæ¨¡æ–¹æ³•**ï¼šå¦‚ç»“åˆéšæœºè§„åˆ’ï¼ˆstochastic programmingï¼‰æˆ–åˆ†å¸ƒé²æ£’ä¼˜åŒ–ï¼ˆDROï¼‰ã€‚
4. **éƒ¨ç½²äºå®æ—¶ DCS æ§åˆ¶ç³»ç»Ÿ**ï¼šå¼€å±•ç°åœºé—­ç¯æµ‹è¯•ï¼ŒéªŒè¯å…¶åœ¨çœŸå®ç”µå‚ä¸­çš„æ§åˆ¶æ•ˆæœã€‚
5. **æ‹“å±•è‡³å…¶ä»–èƒ½æºç³»ç»Ÿ**ï¼šå¦‚ç»¼åˆèƒ½æºç³»ç»Ÿï¼ˆIESï¼‰ã€ç¢³æ•é›†ç³»ç»Ÿï¼ˆCCUSï¼‰ã€æ°¢èƒ½ç”Ÿäº§ç­‰ã€‚

--- 

> âœ… æ€»ç»“ä¸€å¥è¯ï¼š  
> æœ¬æ–‡æå‡ºçš„ **ANN-KKT æ¡†æ¶**æˆåŠŸæ‰“é€šäº†â€œæ•°æ® â†’ æ¨¡å‹ â†’ ä¼˜åŒ– â†’ å†³ç­–â€çš„é—­ç¯è·¯å¾„ï¼Œä¸ºå·¥ä¸šçƒ­åŠ›ç³»ç»Ÿçš„æ™ºèƒ½åŒ–ã€é«˜æ•ˆåŒ–ã€é²æ£’åŒ–è¿è¡Œæä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·æ”¯æ’‘ï¼Œæ ‡å¿—ç€æ•°æ®é©±åŠ¨ä¼˜åŒ–åœ¨èƒ½æºé¢†åŸŸè¿ˆå‘å®ç”¨åŒ–çš„é‡è¦ä¸€æ­¥ã€‚

</details>

---

### 10. [REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents](https://arxiv.org/abs/2602.14234)

**Authors**: Zheng Chu, Xiao Wang, Jack Hong, Huiming Fan, Yuqi Huang, Yue Yang, Guohai Xu, Chenxiao Zhao, Cheng Xiang, Shengchao Hu, Dongdong Kuang, Ming Liu, Bing Qin, Xing Yu  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.14234v1  

#### Abstract
Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the diffi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šREDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»é€šç”¨çŸ¥è¯†å¼•æ“å‘ç°å®ä¸–ç•Œé—®é¢˜æ±‚è§£è€…è½¬å˜çš„è¿‡ç¨‹ä¸­ï¼Œé¢ä¸´**æ·±åº¦æœç´¢ä»»åŠ¡ä¼˜åŒ–å›°éš¾**çš„æ ¸å¿ƒç“¶é¢ˆã€‚å…·ä½“è¡¨ç°ä¸ºï¼š
- **é«˜è´¨é‡æœç´¢è½¨è¿¹ç¨€ç–**ï¼šå¤æ‚ã€é•¿å‘¨æœŸçš„äº¤äº’å¼ä»»åŠ¡éš¾ä»¥å¤§è§„æ¨¡æ„å»ºã€‚
- **è®­ç»ƒæˆæœ¬é«˜æ˜‚**ï¼šä¾èµ–å¤–éƒ¨å·¥å…·è°ƒç”¨çš„rolloutè¿‡ç¨‹è®¡ç®—å’Œæ—¶é—´å¼€é”€å·¨å¤§ã€‚

è¿™äº›é—®é¢˜å¯¼è‡´ç°æœ‰æ–¹æ³•åœ¨æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šä¸Šå—é™ï¼Œéš¾ä»¥æœ‰æ•ˆè®­ç»ƒå…·å¤‡é•¿æœŸæ¨ç†èƒ½åŠ›çš„**search agent**ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯**
ä¸ºè§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº† **REDSearcher**ï¼Œä¸€ä¸ªç»Ÿä¸€çš„ã€å¯æ‰©å±•ä¸”ä½æˆæœ¬çš„æ¡†æ¶ï¼Œé€šè¿‡ååŒè®¾è®¡ä»»åŠ¡åˆæˆã€ä¸­æœŸè®­ç»ƒï¼ˆmid-trainingï¼‰å’Œåè®­ç»ƒï¼ˆpost-trainingï¼‰æ¥ä¼˜åŒ–é•¿å‘¨æœŸæœç´¢ä»£ç†ã€‚

#### **å››å¤§æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶**ï¼š

1. **Dual-Constrained Task Synthesisï¼ˆåŒçº¦æŸä»»åŠ¡åˆæˆï¼‰**
   - å°†ä»»åŠ¡ç”Ÿæˆå»ºæ¨¡ä¸º**å›¾æ‹“æ‰‘ç»“æ„**ä¸**è¯æ®åˆ†æ•£åº¦**åŒé‡çº¦æŸä¸‹çš„ä¼˜åŒ–é—®é¢˜ã€‚
   - å¼•å…¥**treewidth**ä½œä¸ºè¡¡é‡é€»è¾‘å¤æ‚æ€§çš„æŒ‡æ ‡ï¼Œæ„é€ å…·æœ‰ç¯çŠ¶ã€äº¤ç»‡çº¦æŸçš„éçº¿æ€§æ¨ç†è·¯å¾„ã€‚
   - å®šä¹‰**Minimum Source Dispersion (MSD)**ï¼Œç¡®ä¿ç›¸å…³äº‹å®åˆ†å¸ƒåœ¨ä¸åŒæ–‡æ¡£ä¸­ï¼Œé˜²æ­¢â€œå•é¡µæ·å¾„â€è§£å†³æ–¹æ¡ˆï¼Œå¼ºåˆ¶è·¨æ–‡æ¡£ç»¼åˆã€‚

2. **Proactive Tool-Augmented Queriesï¼ˆä¸»åŠ¨å¼å·¥å…·å¢å¼ºæŸ¥è¯¢ï¼‰**
   - ä¸å†ä¾èµ–è¢«åŠ¨å¬å›ï¼Œè€Œæ˜¯å°†å…³é”®å®ä½“æ›¿æ¢ä¸ºéœ€å·¥å…·è§£æçš„åŠŸèƒ½æ€§çº¦æŸï¼ˆå¦‚åœ°å›¾è·ç¦»ã€å­¦æœ¯å¼•ç”¨æ•°ï¼‰ï¼Œä½¿å·¥å…·è°ƒç”¨æˆä¸ºå®Œæˆä»»åŠ¡çš„å¿…è¦æ¡ä»¶ã€‚
   - æ˜¾è‘—**ç¨ å¯†åŒ–å­¦ä¹ ä¿¡å·**ï¼Œæå‡å¯¹å·¥å…·ä½¿ç”¨çš„ç›‘ç£æ•ˆç‡ã€‚

3. **Cost-Efficient Mid-Trainingï¼ˆä½æˆæœ¬ä¸­æœŸè®­ç»ƒï¼‰**
   - åˆ†ä¸¤é˜¶æ®µè¿›è¡Œï¼š
     - **Stage I**ï¼šå¼ºåŒ–åŸå­èƒ½åŠ›ï¼ˆknowledge grounding å’Œ hierarchical planningï¼‰ï¼Œæ— éœ€ç¯å¢ƒäº¤äº’ã€‚
     - **Stage II**ï¼šå¼•å…¥æ¨¡æ‹Ÿå·¥å…·å¾ªç¯å’Œé•¿å‘¨æœŸè½¨è¿¹ï¼Œé€æ­¥è¿‡æ¸¡åˆ°çœŸå®äº¤äº’ã€‚
   - å¤§å¹…é™ä½é«˜è´¨é‡è½¨è¿¹é‡‡é›†çš„æˆæœ¬å’Œæ ·æœ¬å¤æ‚åº¦ã€‚

4. **Functionally Equivalent Simulation Environmentï¼ˆåŠŸèƒ½ç­‰ä»·ä»¿çœŸç¯å¢ƒï¼‰**
   - æ„å»ºè½»é‡çº§æœ¬åœ°æ¨¡æ‹Ÿç¯å¢ƒï¼Œå¤ç°ç½‘é¡µåŠ¨æ€ä½†é¿å…å®æ—¶APIå»¶è¿Ÿä¸è´¹ç”¨ã€‚
   - ä¿è¯æ‰€æœ‰è¯æ®å­˜åœ¨ä½†ç‰©ç†åˆ†æ•£äºå¤§é‡å¹²æ‰°é¡¹ä¸­ï¼Œå½¢æˆé«˜ååæ²™ç›’ï¼Œæ”¯æŒå¿«é€ŸRLè¿­ä»£ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | REDSearcherä¼˜åŠ¿ |
|------|----------------|
| **å¯æ‰©å±•æ€§** | æ”¯æŒå¤§è§„æ¨¡ç”Ÿæˆé«˜å¤æ‚åº¦ã€é«˜åˆ†æ•£æ€§ä»»åŠ¡ï¼Œçªç ´äººå·¥æ ‡æ³¨é™åˆ¶ |
| **æˆæœ¬æ•ˆç‡** | ä¸­æœŸè®­ç»ƒåˆ†ç¦»æŠ€èƒ½è·å–ä¸äº¤äº’ï¼Œä»¿çœŸç¯å¢ƒæ›¿ä»£çœŸå®è°ƒç”¨ï¼Œæ˜¾è‘—é™ä½æˆæœ¬ |
| **è®­ç»ƒæœ‰æ•ˆæ€§** | åŒçº¦æŸä»»åŠ¡ + ä¸»åŠ¨å·¥å…·è°ƒç”¨ â†’ æ›´å¼ºçš„é•¿å‘¨æœŸè§„åˆ’ä¸å·¥å…·ä½¿ç”¨èƒ½åŠ› |
| **æ³›åŒ–æ€§** | åŒæ—¶æ”¯æŒ text-only ä¸ multimodal æœç´¢ä»»åŠ¡ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **Text-only Benchmarks**:
  - `BrowseComp`ï¼ˆè‹±æ–‡ç½‘é¡µæµè§ˆï¼‰
  - `BrowseComp-zh`ï¼ˆä¸­æ–‡ç½‘é¡µæµè§ˆï¼‰
  - `GAIA`ï¼ˆé€šç”¨AIåŠ©æ‰‹åŸºå‡†ï¼‰
  - `Humanity's Last Exam (HLE)`ï¼ˆç»¼åˆæ€§è€ƒè¯•é£æ ¼ä»»åŠ¡ï¼‰

- **Multimodal Benchmarks**:
  - `MM-BrowseComp`, `BrowseComp-VL`, `MMSearch-Plus`, `LiveVQA`, `MMLiveSearch`

- **è‡ªç ”åˆæˆæ•°æ®é›†**ï¼ˆç”¨äºè®­ç»ƒä¸éªŒè¯ï¼‰ï¼š
  - 10K é«˜è´¨é‡æ–‡æœ¬æœç´¢è½¨è¿¹
  - 5K å¤šæ¨¡æ€è½¨è¿¹
  - 1K æ–‡æœ¬ RL æŸ¥è¯¢é›†

---

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**
- **Backboneæ¨¡å‹**ï¼šåŸºäº `Qwen3-30B-A3B` è¿›è¡Œå¾®è°ƒ
- **ä¸Šä¸‹æ–‡é•¿åº¦**ï¼šæœ€å¤§æ”¯æŒ 128K tokens
- **ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥**ï¼šé‡‡ç”¨ **Discard-all**ï¼Œå½“æ¥è¿‘çª—å£ä¸Šé™æ—¶æ¸…ç©ºå†å²äº¤äº’è®°å½•ï¼Œä¿ç•™åŸå§‹é—®é¢˜ä»¥ç»§ç»­æ¢ç´¢
- **è¯„ä¼°åè®®**ï¼šéµå¾ªå„benchmarkå®˜æ–¹è¯„æµ‹æ–¹å¼ï¼ŒæŠ¥å‘Šå‡†ç¡®ç‡ï¼ˆaccuracyï¼‰

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
#### **é—­æºä»£ç†ï¼ˆProprietary Agentsï¼‰**
- Seed1.8, Gemini-3-Pro, GPT-5-Thinking-high, Claude-4.5-sonnet, OpenAI-o3

#### **å¼€æºä»£ç†ï¼ˆOpen-source Agentsï¼‰**
- Kimi-K2.5, GLM-4.7, DeepSeek-V3.2, WebSailorV2-30B, Tongyi DeepResearch-30B

#### **å¤šæ¨¡æ€ä»£ç†**
- Qwen3-VL Thinking, Vision-DeepResearch, DeepEyesV2, Gemini-3-Pro

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 1 & Table 3ï¼‰**

| æ¨¡å‹ | BrowseComp | BrowseComp-zh | GAIA | HLE | Overall |
|------|------------|---------------|------|-----|---------|
| **REDSearcher (30B)** | **57.4*** | **58.2*** | **80.1** | 34.3 | **51.6** |
| GPT-5-Thinking-high | 54.9 | 63.0 | 76.7 | 41.7 | 59.1 |
| Tongyi DeepResearch-30B | 43.4 | 46.7 | 70.9 | 32.9 | 48.5 |
| WebSailorV2-30B | 35.3 | 44.1 | 74.1 | 30.6 | 46.0 |

> æ³¨ï¼š`*` è¡¨ç¤ºä½¿ç”¨ Discard-all ä¸Šä¸‹æ–‡ç®¡ç†æŠ€æœ¯ï¼›REDSearcher åœ¨ **GAIA** ä¸Šè¶…è¶Š GPT-5-Thinking-highï¼Œæ˜¯å”¯ä¸€è¾¾åˆ°æ­¤æ°´å¹³çš„ **30Bçº§åˆ«å¼€æºæ¨¡å‹**ã€‚

---

### **å¤šæ¨¡æ€æ€§èƒ½è¡¨ç°ï¼ˆTable 3ï¼‰**

| æ¨¡å‹ | MM-BrowseComp | BrowseComp-VL | MMSearch-Plus | LiveVQA |
|------|----------------|----------------|----------------|----------|
| **REDSearcher-MM-RL** | **26.6** | **57.2** | **26.6** | **79.3** |
| Gemini-3-Pro | 28.5 | 56.4 | 38.1 | 79.9 |
| Qwen3-VL-235B | 12.1 | 43.1 | 17.4 | 70.2 |

- åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šæ¥è¿‘ç”šè‡³è¶…è¿‡ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚ Gemini-3-Proï¼‰ã€‚
- æ˜¾è‘—ä¼˜äºåŒè§„æ¨¡å¼€æºå¤šæ¨¡æ€ä»£ç†ï¼ˆå¦‚ Vision-DeepResearchï¼‰ã€‚

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆTable 2ï¼‰**

| é˜¶æ®µ | BrowseComp | BrowseComp-zh | HLE | GAIA | å¹³å‡å¾—åˆ† |
|------|------------|----------------|-----|------|-----------|
| Base | 34.74 | 26.82 | 32.25 | 77.43 | 42.81 |
| + Stage I (Grounding & Planning) | 36.97 | 29.84 | 31.37 | 80.83 | 44.75 |
| + Stage II (Agentic Interaction) | 40.44 | 38.75 | 31.25 | 79.13 | **47.39** |

- **Stage I** æå‡ä¿¡æ¯æå–ä¸åˆ†å±‚è§„åˆ’èƒ½åŠ›ï¼Œå°¤å…¶æ”¹å–„ GAIA è¡¨ç°ï¼ˆ+4.13ï¼‰ã€‚
- **Stage II** å¼•å…¥ç¯å¢ƒåé¦ˆåï¼ŒBrowseComp-zh æå‡è¾¾ **+8.91**ï¼ŒéªŒè¯é•¿å‘¨æœŸäº¤äº’çš„é‡è¦æ€§ã€‚

---

### **å¼ºåŒ–å­¦ä¹ æ•ˆæœåˆ†æï¼ˆFigure 6ï¼‰**
- **RLè®­ç»ƒæŒç»­æå‡æ€§èƒ½**ï¼š
  - SFTå¹³å‡è¯„åˆ†ä¸º 47.4 â†’ RLåæå‡è‡³ **51.3**ï¼ˆ+3.9ï¼‰
  - BrowseComp ä» 39.4 â†’ **42.1**ï¼ˆ+6.8%ç›¸å¯¹å¢ç›Šï¼‰
- **æœç´¢æ•ˆç‡æé«˜**ï¼š
  - å¹³å‡å·¥å…·è°ƒç”¨æ¬¡æ•°ä» **100.6 â†’ 90.1**ï¼ˆâ†“10.4%ï¼‰
  - è½¨è¿¹å˜çŸ­ä½†æˆåŠŸç‡ç¨³å®šï¼Œè¡¨æ˜æ¨¡å‹å­¦ä¼šæ›´é«˜æ•ˆçš„æ¢ç´¢ç­–ç•¥ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **åŒçº¦æŸä»»åŠ¡åˆæˆèƒ½æœ‰æ•ˆæ¿€å‘æ·±å±‚æ¨ç†èƒ½åŠ›**ï¼šé€šè¿‡æ§åˆ¶ treewidth ä¸ MSDï¼Œå¯ç³»ç»Ÿæ€§æ„é€ æŠ—æ·å¾„ã€éœ€å¤šè·³åˆæˆçš„ä»»åŠ¡ã€‚
2. âœ… **å·¥å…·ä½¿ç”¨åº”è¢«æ˜¾å¼æ¿€åŠ±è€Œéä¾èµ–è¯•é”™**ï¼šProactive tool-augmented queries è®¾è®¡æ˜¾è‘—æå‡å·¥å…·è°ƒç”¨åˆç†æ€§ã€‚
3. âœ… **ä¸­æœŸè®­ç»ƒæ˜¯è¿æ¥é¢„è®­ç»ƒä¸ä»£ç†è¡Œä¸ºçš„å…³é”®æ¡¥æ¢**ï¼šåˆ†ç¦»åŸå­èƒ½åŠ›è®­ç»ƒä¸äº¤äº’è®­ç»ƒï¼Œå¤§å¹…é™ä½æˆæœ¬å¹¶æå‡ç¨³å®šæ€§ã€‚
4. âœ… **æœ¬åœ°ä»¿çœŸç¯å¢ƒå¯è¡Œä¸”é«˜æ•ˆ**ï¼šåŠŸèƒ½ç­‰ä»·ä½†æ— ç½‘ç»œä¾èµ–çš„æ¨¡æ‹Ÿå™¨æ”¯æŒé«˜é€šé‡RLå®éªŒã€‚
5. âœ… **REDSearcheråœ¨å‚æ•°æ•ˆç‡ä¸Šé¢†å…ˆ**ï¼šä»…ç”¨30Bå‚æ•°å³è¶…è¶Šå¤šæ•°æ›´å¤§é—­æºæ¨¡å‹ï¼Œåœ¨ GAIA ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–é«˜è´¨é‡åˆæˆæ•°æ®çš„è´¨é‡**ï¼šå°½ç®¡æœ‰éªŒè¯æµç¨‹ï¼Œä½†ä»å¯èƒ½å­˜åœ¨éšå«åå·®æˆ–é”™è¯¯æ ‡ç­¾ã€‚
- **ä»¿çœŸç¯å¢ƒä¸çœŸå®ä¸–ç•Œçš„å·®è·**ï¼šè™½ç„¶æ¥å£ä¸€è‡´ï¼Œä½†æ— æ³•å®Œå…¨å¤ç°æœç´¢å¼•æ“æ’åæ³¢åŠ¨ã€é¡µé¢æ›´æ–°ç­‰åŠ¨æ€ç‰¹æ€§ã€‚
- **å¤šæ¨¡æ€ä»»åŠ¡ä»å—é™äºè§†è§‰ç†è§£ç“¶é¢ˆ**ï¼šéƒ¨åˆ†å›¾åƒç»†èŠ‚è¯†åˆ«å¤±è´¥ä¼šå½±å“æœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§ã€‚
- **ç¡¬æ€§æˆªæ–­æœºåˆ¶å½±å“å†³ç­–å®Œæ•´æ€§**ï¼šDiscard-all ç­–ç•¥å¯èƒ½å¯¼è‡´å†å²çŠ¶æ€ä¸¢å¤±ï¼Œå½±å“æé•¿ä»»åŠ¡çš„ä¸€è‡´æ€§ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
- æ‰©å±•è‡³æ›´å¤šå·¥å…·ç±»å‹ï¼ˆå¦‚æ•°æ®åº“æŸ¥è¯¢ã€é‚®ä»¶äº¤äº’ç­‰ï¼‰ã€‚
- æ¢ç´¢è‡ªåŠ¨å‘ç°ä¸ä¿®å¤é”™è¯¯æ¨ç†è·¯å¾„çš„è‡ªæˆ‘çº æ­£æœºåˆ¶ã€‚
- æ„å»ºå¼€æ”¾ä¸–ç•Œåœ¨çº¿æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œå®ç°çœŸå®ç¯å¢ƒä¸­è‡ªä¸»è¿›åŒ–ã€‚
- å¼€æºå‘å¸ƒçš„ **10Kæ–‡æœ¬è½¨è¿¹ã€5Kå¤šæ¨¡æ€è½¨è¿¹ã€1K RLæŸ¥è¯¢é›†** å°†æ¨åŠ¨ç¤¾åŒºç ”ç©¶ã€‚

---

> ğŸ”— **é¡¹ç›®ä¸»é¡µ**ï¼š[redsearchagent.github.io](https://redsearchagent.github.io)  
> ğŸ“¦ **å·²å‘å¸ƒèµ„æº**ï¼šä»£ç ã€æ¨¡å‹checkpointã€é«˜è´¨é‡è½¨è¿¹æ•°æ®é›†ã€RL query set

</details>

---

### 11. [Efficient Multi-round LLM Inference over Disaggregated Serving](https://arxiv.org/abs/2602.14516)

**Authors**: Wenhao He, Youhe Jiang, Penghao Zhao, Quanqing Xu, Eiko Yoneki, Bin Cui, Fangcheng Fu  
**Category**: cs.DC  
**Published**: 2026-02-17  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.14516v1  

#### Abstract
With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šEfficient Multi-round LLM Inference over Disaggregated Serving**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
å½“å‰ä¸»æµçš„ **Prefill-Decoding (PD) disaggregation** æ¶æ„å°† LLM æ¨ç†åˆ†ä¸ºè®¡ç®—å¯†é›†å‹çš„ **prefill é˜¶æ®µ** å’Œå†…å­˜å¯†é›†å‹çš„ **decode é˜¶æ®µ**ï¼Œåˆ†åˆ«éƒ¨ç½²åœ¨ç‹¬ç«‹ç¡¬ä»¶ä¸Šä»¥æå‡èµ„æºåˆ©ç”¨ç‡ã€‚ç„¶è€Œï¼Œåœ¨ **multi-round LLM workflows**ï¼ˆå¦‚ autonomous agentsã€iterative RAGï¼‰ä¸­ï¼Œæ¨ç†è¿‡ç¨‹å‘ˆç° **interleaved prefill-decode æ¨¡å¼**â€”â€”æ¯è½®å¤–éƒ¨äº¤äº’åéƒ½éœ€è¦æ‰§è¡Œå¢é‡ prefillï¼ˆincremental prefillï¼‰ï¼Œè€Œç°æœ‰ç³»ç»Ÿå¯¹æ­¤ç¼ºä¹æœ‰æ•ˆæ”¯æŒã€‚

å…·ä½“å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š
1. **ç¼ºä¹è‡ªé€‚åº”è°ƒåº¦æœºåˆ¶**ï¼šæ— æ³•åŠ¨æ€å†³å®šå¢é‡ prefill æ˜¯åº”åœ¨ decode worker ä¸Šæœ¬åœ°æ‰§è¡Œï¼ˆé¿å… KV ä¼ è¾“å¼€é”€ï¼‰è¿˜æ˜¯è·¯ç”±åˆ°ä¸“ç”¨ prefill workerï¼ˆå‡è½»å¹²æ‰°ï¼‰ã€‚
2. **æ¨¡å‹éƒ¨ç½²é…ç½®ä¸åˆç†**ï¼šä¼ ç»Ÿéƒ¨ç½²ç­–ç•¥ä»…åŸºäºå•è½®æ¨ç†çš„è¾“å…¥/è¾“å‡ºé•¿åº¦è®¾è®¡ï¼Œæœªè€ƒè™‘å¤šè½®åœºæ™¯ä¸‹äº¤é”™è´Ÿè½½å¯¹èµ„æºåˆ†é…å’Œå¹¶è¡Œç­–ç•¥çš„å½±å“ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
æœ¬æ–‡æå‡º **AMPD**ï¼ˆAdaptive Multi-round inference with PD disaggregationï¼‰ï¼Œä¸€ä¸ªä¸“ä¸ºå¤šè½® LLM æ¨ç†ä¼˜åŒ–çš„æ–°å‹è§£è€¦æœåŠ¡æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### **(1) åœ¨çº¿è‡ªé€‚åº”è°ƒåº¦æœºåˆ¶ï¼ˆOnline Adaptive Schedulingï¼‰**
- **Adaptive Routingï¼ˆè‡ªé€‚åº”è·¯ç”±ï¼‰**  
  åŠ¨æ€åˆ¤æ–­æ¯ä¸ª prefill ä»»åŠ¡åº”æœ¬åœ°æ‰§è¡Œè¿˜æ˜¯è¿œç¨‹æ‰§è¡Œï¼Œä¾æ®å®æ—¶ç³»ç»Ÿçš„ **windowed TTFT / ITL ç»Ÿè®¡** æ¥è¯„ä¼°è´Ÿè½½å‹åŠ›ï¼Œå¹¶é€‰æ‹©æ›´å¯èƒ½æ»¡è¶³ SLO çš„è·¯å¾„ã€‚
- **Prefill Reorderingï¼ˆé¢„å¡«å……é‡æ’åºï¼‰**  
  åœ¨ prefill worker çš„ä»»åŠ¡é˜Ÿåˆ—ä¸­é‡‡ç”¨è½»é‡çº§é‡æ’åºç­–ç•¥ï¼Œä¼˜å…ˆæ‰§è¡Œæ¥è¿‘ SLO é˜ˆå€¼çš„ä»»åŠ¡ï¼Œæœ€å¤§åŒ–æ»¡è¶³ TTFT SLO çš„è¯·æ±‚æ•°é‡ã€‚

#### **(2) ç¦»çº¿éƒ¨ç½²è§„åˆ’å™¨ï¼ˆOffline Deployment Plannerï¼‰**
- å°†èµ„æºåˆ†é…ä¸å¹¶è¡Œç­–ç•¥å»ºæ¨¡ä¸º **Integer Linear Programming (ILP)** é—®é¢˜ï¼Œè”åˆä¼˜åŒ– prefill å’Œ decode worker çš„ **data/model parallelism é…ç½®**ï¼Œåœ¨å›ºå®š GPU èµ„æºä¸‹æœ€å°åŒ–ç“¶é¢ˆå»¶è¿Ÿï¼ˆP95 latencyï¼‰ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| æ–¹é¢ | ä¼ ç»Ÿæ–¹æ³•ç¼ºé™· | AMPD æ”¹è¿› |
|------|-------------|----------|
| **è°ƒåº¦çµæ´»æ€§** | å›ºå®šè·¯ç”±ï¼ˆå…¨ remote æˆ–å…¨ colocationï¼‰ | åŠ¨æ€å†³ç­–ï¼Œå¹³è¡¡ TTFT ä¸ ITL |
| **KV ç¼“å­˜ç®¡ç†** | å¿½è§†å¢é‡ prefill çš„ä¼ è¾“æˆæœ¬ | ä»…ä¼ è¾“å¢é‡ KVï¼Œæ”¯æŒæ‡’åŠ è½½ |
| **éƒ¨ç½²åˆç†æ€§** | åŸºäºå•è½® workload è®¾è®¡ | æ˜¾å¼å»ºæ¨¡å¤šè½®äº¤é”™è´Ÿè½½ç‰¹å¾ |
| **SLO è¾¾æˆç‡** | å®¹æ˜“å—æŸä¸€é˜¶æ®µç“¶é¢ˆé™åˆ¶ | ååŒä¼˜åŒ–ä¸¤ç«¯ï¼Œæ˜¾è‘—æå‡ SLO attainment |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†ä¸å·¥ä½œè´Ÿè½½**
å®éªŒåŸºäºå››ç§ä»£è¡¨æ€§çš„ **multi-round LLM å·¥ä½œæµ trace**ï¼Œæ¥æºäºä»¥ä¸‹å…¬å¼€æ•°æ®é›†ï¼š
- **ToolBench**ï¼šå·¥å…·è°ƒç”¨ç±» agent ä»»åŠ¡ï¼ˆå¹³å‡ 3.96 è½®ï¼‰
- **GAIA**ï¼šé€šç”¨ AI åŠ©æ‰‹åŸºå‡†ï¼ˆå¹³å‡ 11.32 è½®ï¼‰
- **HotpotQA**ï¼šå¤šè·³é—®ç­” + è¿­ä»£æ£€ç´¢ï¼ˆ3 è½®ï¼‰
- **DuReader**ï¼šä¸­æ–‡é˜…è¯»ç†è§£ + æ£€ç´¢å¢å¼ºï¼ˆ3 è½®ï¼‰

è¿™äº› trace æ•è·äº†çœŸå®ç¯å¢ƒä¸­çš„ **prefill-decode äº¤é”™æ¨¡å¼** å’Œ **å¤–éƒ¨äº¤äº’å»¶è¿Ÿ**ã€‚

---

### **å®éªŒè®¾ç½®**
- **æ¨¡å‹**ï¼šQwen3-32Bã€Llama-3.1-70Bã€Mixtral-8x7B
- **ç¡¬ä»¶ç¯å¢ƒ**ï¼š4 å°æœåŠ¡å™¨ï¼Œæ¯å° 8Ã—NVIDIA H20ï¼ˆ96GBï¼‰GPUï¼ŒNVLinkï¼ˆ900 GB/sï¼‰+ InfiniBandï¼ˆ200 GB/sï¼‰
- **è¯·æ±‚åˆ°è¾¾æ¨¡å¼**ï¼šPoisson processï¼Œä¸åŒ arrival rate ä¸‹æµ‹è¯•ç³»ç»Ÿè¡¨ç°
- **éƒ¨ç½²è§„æ¨¡åŒ¹é…**ï¼š
  - HotpotQA / ToolBenchï¼š1 server (8 GPUs)
  - DuReaderï¼š2 servers (16 GPUs)
  - GAIAï¼š4 servers (32 GPUs)

---

### **è¯„ä¼°æŒ‡æ ‡**
- **ä¸»æŒ‡æ ‡**ï¼š**SLO Attainment Rate**ï¼ˆæ»¡è¶³å»¶è¿Ÿçº¦æŸçš„è¯·æ±‚æ¯”ä¾‹ï¼‰
  - TTFTï¼ˆTime-to-First-Tokenï¼‰SLOï¼šæ ¹æ®ä¸åŒ workload è®¾ç½®ï¼ˆå¦‚ 350ms ~ 5000msï¼‰
  - ITLï¼ˆInter-Token Latencyï¼‰SLOï¼šé€šå¸¸ 20â€“50ms
- **è¾…åŠ©æŒ‡æ ‡**ï¼š
  - å¹³å‡ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆE2E Latencyï¼‰
  - TTFT / ITL åˆ†è§£æ€§èƒ½
  - æ¶ˆèå®éªŒä¸­ local vs. remote æ‰§è¡Œæ¯”ä¾‹

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| åŸºçº¿ | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **Dynamo** | PD disaggregation | NVIDIA å®˜æ–¹è§£è€¦æ¶æ„ï¼Œé»˜è®¤æ‰€æœ‰ prefill å‘é€åˆ° prefill worker |
| **vLLM** | Colocated serving | ä¸»æµå…±ç½®æ¶æ„ï¼Œprefill ä¸ decode å…±äº« GPU |
| **vLLM-Continuum** | Colocated + å¤šè½®ä¼˜åŒ– | æ”¯æŒ TTL-based KV ç¼“å­˜ä¿ç•™ï¼Œé’ˆå¯¹å¤šè½®ä¼˜åŒ– |

> æ³¨ï¼šæ‰€æœ‰åŸºçº¿å‡è¿›è¡Œéƒ¨ç½²å‚æ•°è°ƒä¼˜ï¼ŒæŠ¥å‘Šæœ€ä½³ç»“æœä»¥ç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
- **SLO Attainment æå‡æ˜¾è‘—**ï¼š
  - ç›¸æ¯” **Dynamo**ï¼ˆdisaggregated baselineï¼‰ï¼š**å¹³å‡æå‡ 67.29%**ï¼Œæœ€é«˜è¾¾ **967.54%**
  - ç›¸æ¯” **vLLM**ï¼ˆcolocated baselineï¼‰ï¼š**å¹³å‡æå‡ 339.74%**ï¼Œæœ€é«˜è¾¾ **3435.1%**
  - ç›¸æ¯” **vLLM-Continuum**ï¼šä»å–å¾—æ˜æ˜¾ä¼˜åŠ¿ï¼Œè¯´æ˜å³ä½¿æœ‰ KV ç¼“å­˜ä¼˜åŒ–ï¼Œå…±ç½®æ¶æ„éš¾ä»¥æ ¹æœ¬è§£å†³ PD å¹²æ‰°é—®é¢˜

- **å…¸å‹åœºæ™¯ç¤ºä¾‹ï¼ˆLlama-3.1-70B, GAIA, 2 req/sï¼‰**ï¼š
  - AMPD SLO attainment â‰ˆ **92.4%**
  - Dynamo â‰ˆ 68.7%ï¼ŒvLLM â‰ˆ 31.3%ï¼ŒvLLM-Continuum â‰ˆ 31.7%

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
| å¯¹æ¯”ç»´åº¦ | è§‚å¯Ÿç»“æœ |
|---------|--------|
| **vs. Dynamo** | AMPD æ˜¾è‘—é™ä½ TTFTï¼ˆå›  adaptive routing å‡è½» prefill worker è´Ÿè½½ï¼‰ï¼ŒåŒæ—¶ä¿æŒä½ ITLï¼ˆé¿å… decode ä¸­æ–­ï¼‰ |
| **vs. vLLM** | vLLM é€šè¿‡ä»»åŠ¡ä¼˜å…ˆçº§ä¿éšœ TTFTï¼Œä½†é¢‘ç¹ä¸­æ–­ decode å¯¼è‡´ ITL æ¶åŒ–ï¼›AMPD åˆ©ç”¨è§£è€¦ç»“æ„ç»´æŒç¨³å®š ITL |
| **vs. vLLM-Continuum** | è™½ç„¶ Continuum é€šè¿‡ session å†…ä»»åŠ¡ä¼˜å…ˆè°ƒåº¦æ”¹å–„ TTFTï¼Œä½†åœ¨é«˜å¹¶å‘ä¸‹ä»å—é™äºèµ„æºäº‰ç”¨ï¼ŒSLO æå‡æœ‰é™ |

---

### **æ¶ˆèå®éªŒç»“æœ**
- **ç§»é™¤ Adaptive Routing (w/o AR)**ï¼š
  - SLO attainment ä¸‹é™ **27.37% â€“ 350%**
  - æœ¬åœ°æ‰§è¡Œå æ¯”ä» 13.9%~31.7% é™è‡³ 0%ï¼Œprefill worker æˆä¸ºç“¶é¢ˆ
- **ç§»é™¤ Prefill Reordering (w/o PR)**ï¼š
  - SLO attainment ä¸‹é™ **13.42% â€“ 14.81%**
  - è¡¨æ˜é˜Ÿåˆ—å†…éƒ¨è°ƒåº¦å¯¹å°¾éƒ¨å»¶è¿Ÿæ§åˆ¶è‡³å…³é‡è¦
- **ç»„åˆæ•ˆæœ**ï¼šä¸¤é¡¹æŠ€æœ¯ååŒå¸¦æ¥ **44.47% â€“ 402%** çš„æ•´ä½“æå‡

> å›¾ 5 æ˜¾ç¤ºï¼Œåœ¨ DuReader ä¸Šï¼Œå¯ç”¨å®Œæ•´ AMPD å SLO attainment ä» ~55% æå‡è‡³ ~95%

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **å¤šè½®æ¨ç†å¿…é¡»é‡æ–°æ€è€ƒè°ƒåº¦ä¸éƒ¨ç½²**  
   ä¼ ç»Ÿçš„ â€œä¸€åˆ€åˆ‡â€ è·¯ç”±ç­–ç•¥ï¼ˆå…¨éƒ¨è¿œç¨‹æˆ–å…¨éƒ¨æœ¬åœ°ï¼‰æ— æ³•é€‚åº”åŠ¨æ€äº¤é”™è´Ÿè½½ï¼Œ**è¿è¡Œæ—¶æ„ŸçŸ¥çš„ adaptive routing æ˜¯å…³é”®**ã€‚

2. **è§£è€¦æ¶æ„ + è‡ªé€‚åº”è°ƒåº¦ > å…±ç½®æ¶æ„ + KV ä¼˜åŒ–**  
   å³ä½¿ vLLM-Continuum å¼•å…¥äº† TTL-based KV ç¼“å­˜ä¿ç•™æœºåˆ¶ï¼Œå…¶æ€§èƒ½ä»è¿œä½äº AMPDï¼Œè¯´æ˜ **ä»æ ¹æœ¬ä¸Šåˆ†ç¦» prefill ä¸ decode æ›´æœ‰åˆ©äºå¤æ‚ workflow çš„é«˜æ•ˆæœåŠ¡**ã€‚

3. **ç¦»çº¿è§„åˆ’èƒ½æœ‰æ•ˆæŒ‡å¯¼éƒ¨ç½²å†³ç­–**  
   é€šè¿‡ ILP å»ºæ¨¡ï¼ŒAMPD çš„ planner èƒ½å‡†ç¡®é¢„æµ‹æœ€ä¼˜èµ„æºé…ç½®ï¼Œå®éªŒè¯æ˜å…¶æ¨èçš„ top-1 éƒ¨ç½²é…ç½®ä¸å®é™…æœ€ä¼˜ä¸€è‡´ï¼ˆè§ Appendix A.3ï¼‰ã€‚

4. **å°çª—å£å³å¯å®ç°é«˜æ•ˆè°ƒåº¦**  
   Prefill reordering ä½¿ç”¨æå°çš„ lookahead windowï¼ˆw=3ï¼‰å³èƒ½è¾¾åˆ°è¿‘ä¼¼æœ€ä¼˜æ•ˆæœï¼Œè¯´æ˜è¯¥ç­–ç•¥å…·æœ‰ **ä½å¼€é”€ã€é«˜å®ç”¨æ€§**ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–å‡†ç¡®çš„æ€§èƒ½å»ºæ¨¡**ï¼šTTFT/ITL é¢„æµ‹ç²¾åº¦å½±å“ routing å†³ç­–è´¨é‡ï¼Œè‹¥ workload åç¦»è®­ç»ƒåˆ†å¸ƒå¯èƒ½å¯¼è‡´æ¬¡ä¼˜å†³ç­–ã€‚
- **ç½‘ç»œå¸¦å®½æ•æ„Ÿ**ï¼šKV cache ä¼ è¾“å¼€é”€éš context length å¢é•¿è€Œä¸Šå‡ï¼Œåœ¨è¶…é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹å¯èƒ½å‰Šå¼±è¿œç¨‹æ‰§è¡Œä¼˜åŠ¿ã€‚
- **æœªå¤„ç†å¼‚æ„ç¡¬ä»¶æ··åˆè°ƒåº¦**ï¼šç›®å‰å‡è®¾é›†ç¾¤å†… GPU åŒæ„ï¼Œæ‰©å±•è‡³ heterogeneous environment éœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **å¼•å…¥ ML-based é¢„æµ‹æ¨¡å‹**ï¼šç”¨äºæ›´ç²¾å‡†åœ°ä¼°è®¡ç¯å¢ƒäº¤äº’æ—¶é—´ã€è¾“å‡ºé•¿åº¦ç­‰ä¸ç¡®å®šæ€§å› ç´ ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–è°ƒåº¦ã€‚
2. **æ”¯æŒåŠ¨æ€å¼¹æ€§æ‰©ç¼©å®¹**ï¼šç»“åˆ Tokenscale ç­‰ autoscaling æŠ€æœ¯ï¼Œå®ç° AMPD åœ¨äº‘ç¯å¢ƒä¸‹çš„æŒ‰éœ€ä¼¸ç¼©ã€‚
3. **æ‰©å±•è‡³å¤šæ¨¡æ€ LLM**ï¼šå°† AMPD æ€è·¯åº”ç”¨äº video/audio è¾“å…¥åœºæ™¯ä¸‹çš„è·¨æ¨¡æ€ prefill-decode è°ƒåº¦ã€‚
4. **æ¢ç´¢ hybrid æ‰§è¡Œæ¨¡å¼**ï¼šå…è®¸éƒ¨åˆ† layer-level disaggregationï¼Œè€Œéä»… phase-levelï¼Œè¿›ä¸€æ­¥ç»†ç²’åº¦è§£è€¦ã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **AMPD é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°† adaptive runtime scheduling ä¸ optimized offline deployment planning ç»“åˆï¼Œå®ç°äº†åœ¨ PD disaggregation æ¶æ„ä¸‹å¯¹ multi-round LLM inference çš„é«˜æ•ˆæ”¯æŒï¼Œå¤§å¹…æå‡äº† SLO attainmentï¼Œä¸ºä¸‹ä¸€ä»£ LLM serving system æä¾›äº†é‡è¦èŒƒå¼å‚è€ƒã€‚**

</details>

---

### 12. [RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch](https://arxiv.org/abs/2602.14578)

**Authors**: Isam Vrce, Andreas Kassler, G\"ok\c{c}e Aydos  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.14578v1  

#### Abstract
Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. Ho...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
- **æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ä¸­çš„æ¨¡å‹å‹ç¼©ä¸ç¡¬ä»¶åŠ é€ŸçŸ›ç›¾**ï¼š
  - ç°æœ‰ç¨€ç–è®­ç»ƒæ–¹æ³•å¤šé‡‡ç”¨ *unstructured sparsity*ï¼Œè™½èƒ½æœ‰æ•ˆå‹ç¼©æ¨¡å‹ï¼Œä½†ç”±äºä¸è§„åˆ™çš„æƒé‡åˆ†å¸ƒï¼Œéš¾ä»¥åœ¨é€šç”¨ç¡¬ä»¶ï¼ˆå¦‚GPUï¼‰ä¸Šå®ç°å®é™…åŠ é€Ÿã€‚
  - ç»“æ„åŒ–ç¨€ç–ï¼ˆå¦‚ channel pruningï¼‰å¯æ”¯æŒç¡¬ä»¶åŠ é€Ÿï¼Œä½†é€šå¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™ä¸”å‰ªæå¤æ‚åº¦é«˜ã€‚
  - æ­¤å¤–ï¼Œå¤§å¤šæ•°ç¨€ç–DRLæ–¹æ³•ä¾èµ–äºé¢„è®­ç»ƒåå‰ªææˆ–åŠ¨æ€æ‹“æ‰‘æ¼”åŒ–ç­–ç•¥ï¼Œæ— æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨ç¨€ç–æ€§è¿›è¡ŒåŠ é€Ÿã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
- é¦–æ¬¡å°† **N:M semi-structured sparsity** å¼•å…¥åˆ°ç«¯åˆ°ç«¯çš„ DRL è®­ç»ƒä¸­ï¼Œæå‡º **RNM-TD3** æ¡†æ¶ï¼š
  - åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å¼ºåˆ¶ä¿æŒ **row-wise N:M sparsity**ï¼ˆä¾‹å¦‚ 2:4ã€1:4ï¼‰ï¼Œå³æ¯è¿ç»­ M ä¸ªæƒé‡ä¸­æœ€å¤šä¿ç•™ N ä¸ªéé›¶å€¼ã€‚
  - æ‰€æœ‰ç½‘ç»œï¼ˆactorã€critic åŠå…¶ target ç½‘ç»œï¼‰å‡ä»ç¨€ç–åˆå§‹åŒ–å¼€å§‹è®­ç»ƒï¼Œæ— éœ€å…ˆè®­ç»ƒå¯†é›†æ¨¡å‹ã€‚
  - åˆ©ç”¨å‘¨æœŸæ€§çš„ mask æ›´æ–°æœºåˆ¶ç»´æŒç¨€ç–ç»“æ„ï¼Œå¹¶å…¼å®¹æ”¯æŒ N:M ç¨€ç–çŸ©é˜µè¿ç®—çš„ç¡¬ä»¶ï¼ˆå¦‚ NVIDIA Ampere Tensor Coresï¼‰ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **ç¡¬ä»¶å…¼å®¹æ€§** | æ”¯æŒç¡¬ä»¶åŠ é€Ÿï¼ˆå°¤å…¶æ˜¯å‰å‘ä¼ æ’­ï¼‰ï¼Œè§£å†³äº† unstructured æ–¹æ³•â€œç†è®ºFLOPsä½ä½†æ— å®é™…åŠ é€Ÿâ€çš„é—®é¢˜ |
| **è®­ç»ƒæ•ˆç‡æ½œåŠ›** | ç”±äºå§‹ç»ˆè¿è¡Œåœ¨ç¨€ç–æ¨¡å¼ä¸‹ï¼Œå…·å¤‡ç«¯åˆ°ç«¯è®­ç»ƒåŠ é€Ÿæ½œåŠ› |
| **æ€§èƒ½è¡¨ç°** | åœ¨å¤šä¸ªç¯å¢ƒä¸­ä¼˜äº dense baseline å’Œéƒ¨åˆ† unstructured åŠ¨æ€ç¨€ç–æ–¹æ³• |
| **ç»“æ„çµæ´»æ€§** | åŠ¨æ€æ›´æ–° maskï¼Œç›¸æ¯”é™æ€ç¨€ç–æ–¹æ³•æ›´å…·é€‚åº”æ€§ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›† / ç¯å¢ƒ
- ä½¿ç”¨ **Gymnasium MuJoCo** è¿ç»­æ§åˆ¶åŸºå‡†ä»»åŠ¡ï¼š
  - `Ant-v5`
  - `HalfCheetah-v5`
  - `Humanoid-v5`
  - `Walker2d-v5`

### âš™ï¸ å®éªŒè®¾ç½®
- **ç®—æ³•åŸºç¡€**ï¼šåŸºäº **TD3**ï¼ˆTwin Delayed Deep Deterministic Policy Gradientï¼‰
- **ç½‘ç»œæ¶æ„**ï¼š
  - Actor ä¸ Critic å‡ä¸ºä¸¤å±‚éšè—å±‚ï¼Œæ¯å±‚ 256 neurons
- **è®­ç»ƒé…ç½®**ï¼š
  - æ€»å…±è®­ç»ƒ 1M environment steps
  - ä½¿ç”¨æ ‡å‡†é™æ€ replay buffer å’Œ 1-step returns
  - æ‰€æœ‰æ–¹æ³•å…±äº«ç›¸åŒè¶…å‚æ•°ä»¥ä¿è¯å…¬å¹³æ¯”è¾ƒ
- **ç¨€ç–æ¨¡å¼**ï¼š
  - æµ‹è¯•å¤šç§ N:M ratioï¼š2:4 (50% sparsity), 1:4 (75%), 1:8 (87.5%)
  - æœ€åä¸€å±‚ä¿æŒ denseï¼ˆä¸å…¶ä»–æ–¹æ³•ä¸€è‡´ï¼‰

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
- **å¹³å‡å›æŠ¥ï¼ˆAverage Returnï¼‰**ï¼šåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç´¯è®¡å¥–åŠ±å‡å€¼
- **æ ‡å‡†åŒ–å¾—åˆ†**ï¼šç›¸å¯¹äº dense baseline çš„å½’ä¸€åŒ–æ€§èƒ½ï¼ˆdense = 1.0ï¼‰
- **Sparse Architecture Divergence (SAD)**ï¼šè¡¡é‡è¿ç»­ mask ä¹‹é—´çš„æ±‰æ˜è·ç¦»ï¼Œåæ˜ æ‹“æ‰‘å˜åŒ–ç¨‹åº¦
- **æ–¹å·®ï¼ˆStd. Deviationï¼‰**ï¼šè¯„ä¼°ç¨³å®šæ€§

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **Dense TD3** | å¯†é›†æ¨¡å‹ | æ€§èƒ½åŸºå‡† |
| **SSN-N:M** | Static Sparse Network | å›ºå®š maskï¼Œåˆå§‹åŒ–æ—¶é‡‡æ ·ä¸€æ¬¡ |
| **DS-TD3** (Sokar et al., 2021) | Dynamic Sparse Training | éšæœº regrowth ç­–ç•¥ |
| **RLx2** (Tan et al., 2022) | Dynamic Pruning + Regrowth | åŸºäºæ¢¯åº¦å¤§å°è°ƒæ•´è¿æ¥ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

| ç¯å¢ƒ | Sparsity | RNM-TD3 (ç›¸å¯¹ dense) | Dense Baseline Score |
|-------|----------|------------------------|------------------------|
| Ant-v5 | 50% (2:4) | **1.14Â±0.05** âœ… | 4453Â±1320 |
| HalfCheetah-v5 | 50% (2:4) | 1.01Â±0.08 | 10211Â±1110 |
| Humanoid-v5 | 50% (2:4) | 1.05Â±0.03 | 4868Â±659 |
| Walker2d-v5 | 50% (2:4) | 1.04Â±0.13 | 3767Â±833 |

> âœ… **äº®ç‚¹**ï¼šåœ¨ **Ant ç¯å¢ƒä¸‹ 2:4 ç¨€ç–åº¦æ—¶æ€§èƒ½æå‡è¾¾ 14%**

#### æ›´é«˜ç¨€ç–åº¦ä¸‹çš„è¡¨ç°ï¼š
- åœ¨ **75% sparsity (1:4)** ä¸‹ä»ä¿æŒç«äº‰åŠ›ï¼Œå¤šæ•°ç¯å¢ƒæ¥è¿‘æˆ–è¶…è¿‡ dense baseline
- å³ä½¿åœ¨ **87.5% sparsity (1:8)** ä¸‹ï¼Œé™¤ Humanoid å¤–åŸºæœ¬ä¿æŒå¯ç”¨æ€§èƒ½
  - Humanoid åœ¨ 1:8 ä¸‹å‡ºç°å´©æºƒï¼ˆ`0.28Â±0.20`ï¼‰ï¼Œéœ€ soft reset æœºåˆ¶æ¢å¤

### ğŸ” ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”
| å¯¹æ¯”ç»´åº¦ | ç»“æœ |
|---------|------|
| vs **SSN-N:M**ï¼ˆé™æ€ç¨€ç–ï¼‰ | RNM-TD3 æ˜¾è‘—æ›´ä¼˜ï¼Œå°¤å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­ï¼›è¯æ˜åŠ¨æ€ mask æ›´æ–°çš„é‡è¦æ€§ |
| vs **DS-TD3 / RLx2**ï¼ˆéç»“æ„åŒ–åŠ¨æ€ç¨€ç–ï¼‰ | RNM-TD3 å¹³å‡æ€§èƒ½æ›´é«˜ï¼ˆå¦‚ Ant ä¸Š +25%ï¼‰ï¼Œä¸”å…·å¤‡ç¡¬ä»¶åŠ é€Ÿæ½œåŠ› |
| vs **Dense TD3** | åœ¨ 50%~75% ç¨€ç–åº¦ä¸‹æ™®éæŒå¹³ç”šè‡³è¶…è¶Šï¼Œæ‰“ç ´â€œç¨€ç–å¿…é™æ€§èƒ½â€è®¤çŸ¥ |

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰**Mask Update Period å½±å“ï¼ˆTable 3 & Figure 3ï¼‰**
- åœ¨ HalfCheetah ä¸Šæµ‹è¯•ä¸åŒæ›´æ–°å‘¨æœŸ Kï¼š
  - æœ€ä½³æ€§èƒ½å‡ºç°åœ¨ **K=4000 environment steps**
  - è¿‡çŸ­ï¼ˆK=10ï¼‰â†’ mask flickering â†’ SAD æä½ â†’ æ€§èƒ½å·®
  - è¿‡é•¿ï¼ˆK=20000ï¼‰â†’ æ‹“æ‰‘æ›´æ–°æ»å â†’ å­¦ä¹ ç¼“æ…¢
- **å‘ç°**ï¼šé€‚åº¦é¢‘ç‡çš„ mask æ›´æ–°å¸¦æ¥ä¸­ç­‰éé›¶ SADï¼Œå¯¹åº”æœ€ä½³æ€§èƒ½

#### ï¼ˆ2ï¼‰**Soft Reset æœºåˆ¶æœ‰æ•ˆæ€§**
- åœ¨ Humanoid ç­‰ä¸ç¨³å®šç¯å¢ƒä¸­ï¼Œå½“ SAD < 10 æ—¶è§¦å‘ soft resetï¼ˆé‡æ–°é‡‡æ · maskï¼‰
- æˆåŠŸé¿å…æ—©æœŸè®­ç»ƒå´©æºƒï¼ˆåŸå¤±è´¥ç‡çº¦ 80%ï¼‰
- æ¢å¤å SAD ç»´æŒå¥åº·æ°´å¹³ï¼Œè®­ç»ƒæ¢å¤æ­£å¸¸

#### ï¼ˆ3ï¼‰**Initialization Ablationï¼ˆTable 2ï¼‰**
- ä½¿ç”¨é€‚é… N:M fan-in çš„ **Kaiming åˆå§‹åŒ–**ï¼ˆscale by âˆš(M/N)ï¼‰ï¼š
  - æ˜¾è‘—é™ä½è®­ç»ƒæ–¹å·®
  - åœ¨é«˜ç¨€ç–åº¦ä¸‹é˜²æ­¢è¾“å‡ºé¥±å’Œï¼ˆé¿å… tanh æ¢¯åº¦æ¶ˆå¤±ï¼‰
- æ ‡å‡† Kaiming åˆå§‹åŒ–åœ¨ 1:8 åœºæ™¯ä¸‹å¯èƒ½å¯¼è‡´ç¾éš¾æ€§å¤±è´¥ï¼ˆå¦‚ Humanoid ä¸Š std é«˜è¾¾ 1.10ï¼‰

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **N:M structured sparsity å¯ç”¨äºç«¯åˆ°ç«¯ DRL è®­ç»ƒ**ï¼š
   - ä¸ä»…å¯è¡Œï¼Œè¿˜èƒ½åœ¨æŸäº›ç¯å¢ƒä¸‹**è¶…è¶Š dense æ¨¡å‹æ€§èƒ½**ã€‚
2. **åŠ¨æ€ mask æ›´æ–°ä¼˜äºé™æ€åˆå§‹åŒ–**ï¼š
   - åŠ¨æ€æ‹“æ‰‘æ¼”åŒ–å¯¹å¤æ‚ç­–ç•¥å­¦ä¹ è‡³å…³é‡è¦ã€‚
3. **ä¸­ç­‰éé›¶ SAD æ˜¯è‰¯å¥½è®­ç»ƒçš„æ ‡å¿—**ï¼š
   - ä¸ç›‘ç£å­¦ä¹ è¿½æ±‚æœ€å° SAD ä¸åŒï¼ŒDRL ä¸­**é€‚åº¦æ‹“æ‰‘å˜åŒ–æœ‰åŠ©äºé€‚åº”éå¹³ç¨³æ•°æ®åˆ†å¸ƒ**ã€‚
4. **mask æ›´æ–°é¢‘ç‡æ˜¯å…³é”®è¶…å‚**ï¼š
   - è¿‡é¢‘ â†’ ä¸ç¨³å®šï¼›è¿‡æ…¢ â†’ å“åº”è¿Ÿé’ï¼›å­˜åœ¨æœ€ä¼˜ä¸­é—´åŒºé—´ï¼ˆ~4000â€“6000 stepsï¼‰ã€‚
5. **ç¡¬ä»¶åŠ é€Ÿæ½œåŠ›æ˜ç¡®**ï¼š
   - å°½ç®¡å½“å‰å®ç°æœªå¯ç”¨åŒå‘ N:Mï¼ˆcolumn-wiseï¼‰ï¼Œä»…é å‰å‘ pass åŠ é€Ÿå³å¯å¸¦æ¥æ˜¾è‘— end-to-end speedupã€‚

### âš ï¸ æ–¹æ³•å±€é™æ€§
- **æç«¯ç¨€ç–ä¸‹ç¨³å®šæ€§æŒ‘æˆ˜**ï¼š
  - å¦‚ Humanoid åœ¨ 1:8 ä¸‹å®¹æ˜“å´©æºƒï¼Œéœ€é¢å¤–æœºåˆ¶å¹²é¢„ã€‚
- **å°šæœªé›†æˆå®Œæ•´ç¡¬ä»¶åŠ é€Ÿæ ˆ**ï¼š
  - å½“å‰å®éªŒåŸºäºæ¨¡æ‹Ÿç¨€ç–è®¡ç®—ï¼Œæœªè°ƒç”¨çœŸå® N:M kernelï¼ˆå¦‚ CUDA 2:4 kernelsï¼‰ã€‚
- **åˆå§‹åŒ–æ•æ„Ÿ**ï¼š
  - é«˜ç¨€ç–åº¦ä¸‹åˆå§‹æƒé‡è¿‡å¤§æ˜“å¼•å‘é¥±å’Œï¼Œéœ€ç²¾ç»†åˆå§‹åŒ–è®¾è®¡ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **å¼€å‘è‡ªé€‚åº” mask update schedule**ï¼š
   - æ ¹æ® SAD æˆ–ç¯å¢ƒåé¦ˆåŠ¨æ€è°ƒèŠ‚æ›´æ–°å‘¨æœŸã€‚
2. **æ¢ç´¢åˆ—æ–¹å‘ N:M sparsityï¼ˆBi-directional N:Mï¼‰**ï¼š
   - åŒæ—¶ä¼˜åŒ–å‰å‘ä¸åå‘ä¼ æ’­åŠ é€Ÿã€‚
3. **ä¸“ç”¨é«˜ç¨€ç–åˆå§‹åŒ–æ–¹æ¡ˆ**ï¼š
   - è®¾è®¡é’ˆå¯¹ N:M ç»“æ„çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œæå‡é²æ£’æ€§ã€‚
4. **æ‰©å±•è‡³å…¶ä»– DRL ç®—æ³•**ï¼š
   - å¦‚ SACã€PPO ç­‰ï¼ŒéªŒè¯é€šç”¨æ€§ã€‚
5. **éƒ¨ç½²åˆ°çœŸå®ç¨€ç–åŠ é€Ÿç¡¬ä»¶å¹³å°**ï¼š
   - åœ¨æ”¯æŒ 2:4 æˆ– 1:4 çš„ GPU/FPGA ä¸Šå®æµ‹æ¨ç†ä¸è®­ç»ƒåŠ é€Ÿæ¯”ã€‚

---

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **RNM-TD3 é¦–æ¬¡å®ç°äº†ä»å¤´å¼€å§‹çš„ N:M ç»“æ„åŒ–ç¨€ç–å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨ä¿æŒç¡¬ä»¶å‹å¥½æ€§çš„åŒæ—¶ï¼Œä¸ä»…æ²¡æœ‰ç‰ºç‰²æ€§èƒ½ï¼Œåè€Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†è¶…è¶Šå¯†é›†æ¨¡å‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº†â€œç¨€ç–å³é«˜æ•ˆä¸”æ›´å¼ºâ€çš„æ–°å¯èƒ½ã€‚**

</details>

---

### 13. [When to Think Fast and Slow? AMOR: Entropy-Based Metacognitive Gate for Dynamic SSM-Attention Switching](https://arxiv.org/abs/2602.13215)

**Authors**: Haoran Zheng  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.13215v1  

#### Abstract
Transformers allocate uniform computation to every position, regardless of difficulty. State Space Models (SSMs) offer efficient alternatives but struggle with precise information retrieval over a long horizon. Inspired by dual-process theories of cognition (Kahneman, 2011), we propose AMOR (Adaptiv...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*When to Think Fast and Slow? AMOR: Entropy-Based Metacognitive Gate for Dynamic SSM-Attention Switching*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
ä¼ ç»Ÿ **Transformer** æ¶æ„å¯¹åºåˆ—ä¸­æ¯ä¸ªä½ç½®éƒ½åˆ†é…ç›¸åŒçš„è®¡ç®—é‡ï¼ˆuniform computationï¼‰ï¼Œæ— è®ºè¯¥ä½ç½®æ˜¯ç®€å•é¢„æµ‹è¿˜æ˜¯éœ€è¦é•¿è·ç¦»ä¿¡æ¯æ£€ç´¢ã€‚è¿™ç§â€œä¸€åˆ€åˆ‡â€çš„ç­–ç•¥åœ¨æ•ˆç‡ä¸Šå­˜åœ¨æµªè´¹ã€‚å¦ä¸€æ–¹é¢ï¼Œ**State Space Models (SSMs)** è™½ç„¶å…·æœ‰çº¿æ€§å¤æ‚åº¦ã€é«˜æ•ˆå¤„ç†å±€éƒ¨æ¨¡å¼ï¼Œä½†ç”±äºå…¶å‹ç¼©å†å²çŠ¶æ€çš„æœºåˆ¶ï¼Œåœ¨ç²¾ç¡®æ£€ç´¢è¿œè·ç¦»ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ã€‚

æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€çŸ›ç›¾ï¼šå¦‚ä½•åœ¨ä¿æŒé«˜æ•ˆçš„åŒæ—¶ï¼Œä»…åœ¨å¿…è¦æ—¶è°ƒç”¨é«˜æˆæœ¬çš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ä»¥å®ç°ç²¾å‡†æ£€ç´¢ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡º **AMOR (Adaptive Metacognitive Output Router)**ï¼Œä¸€ç§å—è®¤çŸ¥ç§‘å­¦å¯å‘çš„æ··åˆæ¶æ„ï¼Œç»“åˆäº† **System 1 (å¿«æ€è€ƒ)** å’Œ **System 2 (æ…¢æ€è€ƒ)** çš„åŒè¿‡ç¨‹ç†è®ºï¼š

- **System 1**: ç”± SSMï¼ˆå¦‚ GRU æˆ– Mambaï¼‰å®ç°ï¼Œè´Ÿè´£å¿«é€Ÿã€è‡ªåŠ¨åœ°å¤„ç†æ‰€æœ‰ä½ç½®ï¼Œå»ºæ¨¡å±€éƒ¨ä¾èµ–ã€‚
- **System 2**: ç”±ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰å®ç°ï¼Œä»…åœ¨æ¨¡å‹â€œä¸ç¡®å®šâ€æ—¶è¢«æ¿€æ´»ï¼Œç”¨äºç²¾ç¡®æ£€ç´¢ã€‚
- **Metacognitive Gate**: ä½¿ç”¨ **prediction entropy**ï¼ˆé¢„æµ‹ç†µï¼‰ä½œä¸ºè·¯ç”±ä¿¡å·ã€‚å½“ SSM è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒç†µè¾ƒé«˜ï¼ˆå³æ¨¡å‹ä¸ç¡®å®šï¼‰æ—¶ï¼Œè§¦å‘æ³¨æ„åŠ›æœºåˆ¶ï¼›å¦åˆ™ç›´æ¥ä½¿ç”¨ SSM è¾“å‡ºã€‚

æ­¤å¤–ï¼Œå¼•å…¥ **Ghost KV Cache**ï¼šå°† SSM çš„éšè—çŠ¶æ€æŠ•å½±ä¸º Key å’Œ Valueï¼Œè€Œéä»åŸå§‹åµŒå…¥é‡æ–°è®¡ç®—ã€‚è¿™å¤ç”¨äº† SSM å·²æœ‰çš„ä¸Šä¸‹æ–‡åŒ–è¡¨ç¤ºï¼Œé¿å…äº†é¢å¤–çš„ $O(n^2)$ æ³¨æ„åŠ›å¼€é”€ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| å¯¹æ¯”ç»´åº¦ | AMOR | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ Jamba, Griffinï¼‰ |
|--------|------|-----------------------------|
| **åŠ¨æ€æ€§** | åŠ¨æ€æŒ‰éœ€è§¦å‘æ³¨æ„åŠ› | å›ºå®šæ¨¡å¼äº¤æ›¿ä½¿ç”¨ SSM å’Œ Attention |
| **å¯è§£é‡Šæ€§** | è·¯ç”±å†³ç­–åŸºäº entropyï¼Œè¯­ä¹‰æ˜ç¡®ï¼ˆâ€œæˆ‘ä¸çŸ¥é“â€æ‰æŸ¥ï¼‰ | è·¯ç”±å™¨ä¸ºé»‘ç®±å­¦ä¹ ï¼Œéš¾ä»¥è§£é‡Šä¸ºä½•æŸä½ç½®è¢«é€‰ä¸­ |
| **æ•ˆç‡æœºåˆ¶** | Ghost KV å¤ç”¨ SSM éšè—çŠ¶æ€ï¼ŒèŠ‚çœè®¡ç®— | æ¯æ¬¡ Attention éƒ½éœ€ç‹¬ç«‹è®¡ç®— KV |
| **è®¡ç®—ç±»å‹å˜åŒ–** | æ˜ç¡®åŒºåˆ†ä¸¤ç§å¤„ç†æ¨¡å¼ï¼ˆSSM vs Attentionï¼‰ | é€šå¸¸æ˜¯æ¨¡å—å †å ï¼Œæ— æœ¬è´¨å¤„ç†æ¨¡å¼åˆ‡æ¢ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
è®ºæ–‡è®¾è®¡äº†ä¸¤ä¸ªåˆæˆä»»åŠ¡æ¥éªŒè¯æ ¸å¿ƒå‡è®¾ï¼š

1. **Simple Retrieval Task**
   - åºåˆ—é•¿åº¦ï¼š128
   - åŒ…å«å±€éƒ¨é‡å¤æ¨¡å¼ï¼ˆå¦‚ A B A Bï¼‰å’Œè¿œè·ç¦»å¤åˆ¶éœ€æ±‚ï¼ˆ`M X ... R ?` â†’ `? = X`ï¼‰
   - çº¦ 3% çš„ä½ç½®éœ€è¦æ£€ç´¢
   - ç›®æ ‡ï¼šéªŒè¯ entropy æ˜¯å¦èƒ½æœ‰æ•ˆæŒ‡ç¤º retrieval éœ€æ±‚

2. **NeedleHaystack Task**
   - æ›´å…·æŒ‘æˆ˜æ€§çš„å…³è”è®°å¿†ä»»åŠ¡
   - ä¸‰é˜¶æ®µç»“æ„ï¼š
     - **Store**: å­˜å‚¨è‹¥å¹² key-value å¯¹ï¼ˆ`STORE k v`ï¼‰
     - **Noise**: æ’å…¥ 50â€“150 ä¸ªéšæœº tokenï¼Œè¿«ä½¿ SSM çŠ¶æ€é—å¿˜
     - **Query**: æŸ¥è¯¢æŸä¸ª key çš„å€¼ï¼ˆ`QUERY k ?`ï¼‰
   - æµ‹è¯•æ¨¡å‹åœ¨ SSM çŠ¶æ€è¡°å‡åæ˜¯å¦ä»èƒ½é€šè¿‡æ³¨æ„åŠ›æ£€ç´¢ä¿¡æ¯

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹é…ç½®
- **SSM Only**: GRU æ¨¡å‹ï¼ˆ~51K å‚æ•°ï¼‰
- **Full Attention**: æ ‡å‡† Transformerï¼ˆ~167K å‚æ•°ï¼‰
- **AMOR Oracle**: ä½¿ç”¨çœŸå®æ ‡ç­¾æ§åˆ¶é—¨æ§ï¼ˆç†æƒ³ä¸Šé™ï¼‰
- **AMOR Entropy**: ä½¿ç”¨ entropy gate æ§åˆ¶

#### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ |
|-----|------|
| **Overall Accuracy** | æ‰€æœ‰ä½ç½®çš„å¹³å‡é¢„æµ‹å‡†ç¡®ç‡ |
| **Retrieval Accuracy** | ä»…åœ¨éœ€è¦æ£€ç´¢çš„ä½ç½®ä¸Šçš„å‡†ç¡®ç‡ |
| **Gate Fires (%)** | è§¦å‘æ³¨æ„åŠ›çš„æ¯”ä¾‹ï¼ˆåæ˜ æ•ˆç‡ï¼‰ |
| **Gate F1** | é—¨æ§å†³ç­–ä¸çœŸå®æ£€ç´¢éœ€æ±‚ä¹‹é—´çš„ F1 åˆ†æ•° |
| **Entropy Gap** | æ£€ç´¢ä½ç½®ä¸å±€éƒ¨ä½ç½®ä¹‹é—´çš„å¹³å‡ç†µå·®ï¼ˆå•ä½ï¼šnatsï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸åŸºçº¿å¯¹æ¯”

#### Simple Retrieval Task ç»“æœï¼ˆè¡¨ 3ï¼‰

| Model | Overall Acc | **Retrieval Acc** | Gate Fires | Params |
|-------|-------------|--------------------|------------|--------|
| SSM Only | 89.76% | 68.35% | â€” | 51K |
| Full Attention | 89.37% | 87.30% | 100% | 167K |
| AMOR Oracle | 90.71% | 99.63% | 2.90% | 77K |
| **AMOR Entropy** | **90.88%** | **100%** | **22.32%** | **77K** |

- âœ… **AMOR åœ¨ retrieval å‡†ç¡®ç‡ä¸Šè¾¾åˆ° 100%**ï¼Œæ˜¾è‘—ä¼˜äº SSM Onlyï¼ˆ68%ï¼‰å’Œ Full Attentionï¼ˆ87%ï¼‰
- â±ï¸ ä»…åœ¨ **22.32% çš„ä½ç½®** è§¦å‘æ³¨æ„åŠ›ï¼Œç›¸æ¯” Transformer èŠ‚çœçº¦ **78% çš„æ³¨æ„åŠ›è®¡ç®—**
- ğŸ§  å°½ç®¡å‚æ•°æ›´å°‘ï¼Œæ€§èƒ½åè€Œæ›´é«˜ï¼Œè¯´æ˜ **Ghost KV + entropy gate çš„ç»„åˆéå¸¸æœ‰æ•ˆ**

#### Entropy Gap éªŒè¯ï¼ˆè¡¨ 2ï¼‰
| é¡¹ç›® | æ•°å€¼ |
|------|------|
| æ£€ç´¢ä½ç½®å¹³å‡ç†µ | 1.98 nats |
| å±€éƒ¨ä½ç½®å¹³å‡ç†µ | 0.89 nats |
| **Entropy Gap** | **1.09 nats**ï¼ˆæ¥è¿‘å½’ä¸€åŒ–èŒƒå›´çš„ä¸€åŠï¼‰ |

ğŸ‘‰ è¡¨æ˜ SSM ç¡®å®èƒ½åœ¨ä¿¡æ¯ä¸è¶³æ—¶äº§ç”Ÿé«˜ç†µè¾“å‡ºï¼Œâ€œçŸ¥é“è‡ªå·±ä¸çŸ¥é“â€ï¼Œä¸ºé—¨æ§æä¾›äº†å¯é ä¾æ®ã€‚

#### NeedleHaystack Task ç»“æœï¼ˆè¡¨ 4ï¼‰
| Model | Retrieval Acc |
|-------|----------------|
| SSM Only | 8.02% |
| Full Attention | 4.40% |
| AMOR Entropy | 9.93% |
| **AMOR Oracle** | **37.08%** |

- å³ä½¿åœ¨æ­¤æç«¯å›°éš¾çš„ä»»åŠ¡ä¸Šï¼Œ**AMOR Entropy ä»ä¼˜äº Full Attention**
- **Oracle æ€§èƒ½è¿œè¶…å…¶ä»–æ¨¡å‹ï¼ˆ37% vs 4.4%ï¼‰**ï¼Œè¡¨æ˜è‹¥èƒ½ç²¾å‡†è¯†åˆ«ä½•æ—¶éœ€è¦æ£€ç´¢ï¼Œè¯¥æ¶æ„æ½œåŠ›å·¨å¤§
- å½“å‰ entropy gate åœ¨å…¨å±€ä¸ç¡®å®šæ€§é«˜æ—¶ï¼ˆgate fire rate è¾¾ 80.97%ï¼‰å¤±å»åˆ¤åˆ«èƒ½åŠ›

### æ¶ˆèå®éªŒç»“æœï¼ˆé™„å½• C.6ï¼‰

| å˜ä½“ | Retrieval Acc | Gate F1 | Gate Fires |
|------|---------------|---------|----------|
| **Entropy gate (default)** | **96.16%** | 22.11% | 23.36% |
| Learned STE gate | 100% | 0.00% | 1.19% |
| Target rate=0.1 | 97.73% | 17.52% | 30.28% |
| Target rate=0.2 | 99.79% | 17.43% | 30.46% |

- å­¦ä¹ å‹é—¨æ§è™½èƒ½è¾¾åˆ°å®Œç¾å‡†ç¡®ç‡ï¼Œä½† **å®Œå…¨ä¸å¯è§£é‡Šï¼ˆF1=0ï¼‰**
- entropy gate åœ¨ **å¯è§£é‡Šæ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡**

### å…¶ä»–é‡è¦å‘ç°
- **Ghost KV vs Raw Embedding KV**ï¼š
  - Ghost KVï¼ˆæ¥è‡ª SSM éšè—çŠ¶æ€ï¼‰ï¼š**36.28%** retrieval acc
  - Raw Embedding KVï¼šä»… **6.08%**
  - ğŸ‘‰ è¯æ˜ SSM æä¾›äº†æœ‰ä»·å€¼çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- **Attention Sparsity**ï¼š
  - Top-k=3 æ•ˆæœæœ€å¥½ï¼ˆ81.39%ï¼‰ï¼Œk=16 åè€Œä¸‹é™è‡³ 6.26%
  - ğŸ‘‰ ç¨€ç–æ³¨æ„åŠ›æœ‰åŠ©äºèšç„¦å…³é”®ä¿¡æ¯ï¼Œé˜²æ­¢æ³¨æ„åŠ›åˆ†æ•£

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **Prediction entropy æ˜¯ä¸€ä¸ªå¯é ä¸”å¯è§£é‡Šçš„å…ƒè®¤çŸ¥ä¿¡å·**ï¼šSSM åœ¨æ— æ³•ä»…å‡­å±€éƒ¨ä¿¡æ¯åšå‡ºåˆ¤æ–­æ—¶ä¼šè‡ªç„¶äº§ç”Ÿé«˜ç†µè¾“å‡ºï¼Œå¯ç”¨äºè§¦å‘æ³¨æ„åŠ›ã€‚
2. âœ… **Selective attention is sufficient**ï¼šæ— éœ€åœ¨æ¯ä¸ªä½ç½®éƒ½è¿è¡Œæ³¨æ„åŠ›ï¼Œä»…åœ¨çº¦ 22% çš„ä½ç½®æ¿€æ´»å³å¯å®ç°å®Œç¾ retrievalã€‚
3. âœ… **Ghost KV å®ç°é«˜æ•ˆå¤ç”¨**ï¼šåˆ©ç”¨ SSM éšè—çŠ¶æ€ç”Ÿæˆ KVï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œå¸¦æ¥æ˜¾è‘—æ•ˆç‡ä¼˜åŠ¿ã€‚
4. âœ… **AMOR å®ç°äº†çœŸæ­£çš„â€œåŒè¿‡ç¨‹â€æ¶æ„**ï¼šSSM ä¸ Attention åœ¨åŠŸèƒ½å’Œç»“æ„ä¸Šåˆ†ç¦»ï¼Œç¬¦åˆè®¤çŸ¥ç§‘å­¦ç›´è§‰ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **SSM State Decay Limitation**ï¼šå½“å™ªå£°è¿‡é•¿ï¼ˆ>50 tokensï¼‰ï¼ŒSSM çŠ¶æ€ä¸¥é‡è¡°å‡ï¼ŒGhost KV ä¸­çš„ä¿¡æ¯ä¹Ÿéšä¹‹ä¸¢å¤±ï¼Œå¯¼è‡´å³ä½¿è§¦å‘æ³¨æ„åŠ›ä¹Ÿæ— æ³•æ¢å¤ã€‚
2. **Reactive Gating ç¼ºä¹å‰ç»æ€§**ï¼šå½“å‰é—¨æ§æ˜¯ååº”å¼çš„ï¼ˆreactiveï¼‰ï¼Œåªèƒ½åœ¨ä¸ç¡®å®šæ€§å‡ºç°åæ‰è§¦å‘ attentionï¼Œæ— æ³•æå‰ç¼“å­˜é‡è¦ä¿¡æ¯ã€‚
3. **æç«¯ä»»åŠ¡ä¸‹åˆ¤åˆ«åŠ›ä¸‹é™**ï¼šåœ¨å…¨å±€é«˜åº¦ä¸ç¡®å®šçš„ä»»åŠ¡ä¸­ï¼ˆå¦‚ NeedleHaystackï¼‰ï¼Œentropy å¤±å»åŒºåˆ†èƒ½åŠ›ï¼Œgate fire rate è¿‡é«˜ï¼ˆ80%+ï¼‰ï¼Œå‰Šå¼±æ•ˆç‡ä¼˜åŠ¿ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **Proactive Storage Mechanism**ï¼š
   - å¼•å…¥æŒä¹… KV å­˜å‚¨ï¼ˆPersistent KV Cacheï¼‰ï¼Œåœ¨ç¼–ç é˜¶æ®µä¸»åŠ¨ç¼“å­˜å…³é”®ä½ç½®çš„è¡¨ç¤º
   - è®¾è®¡â€œå­˜å‚¨é—¨â€ï¼ˆstorage gateï¼‰ï¼ŒåŸºäºä¿¡æ¯é‡è¦æ€§è€Œéå½“å‰ä¸ç¡®å®šæ€§è¿›è¡Œç¼“å­˜
2. **State Feedback**ï¼š
   - æ¢ç´¢å°† attention è¾“å‡ºåé¦ˆå› SSM çŠ¶æ€æ›´æ–°ä¸­ï¼ˆ$h_{t+1} = \text{SSM}(e_t, f(h_t, \text{combined}))$ï¼‰ï¼Œæ”¯æŒè¿­ä»£æ¨ç†
3. **Predictive Gating**ï¼š
   - ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•è®­ç»ƒé—¨æ§é¢„æµ‹æœªæ¥çš„ retrieval éœ€æ±‚ï¼Œå®ç°â€œé¢„åˆ¤å¼â€è®¡ç®—åˆ†é…
4. **æ‰©å±•åˆ°çœŸå®åœºæ™¯**ï¼š
   - åœ¨å¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡ã€é•¿æ–‡æœ¬ç†è§£ç­‰å®é™…ä»»åŠ¡ä¸­éªŒè¯ AMOR çš„æœ‰æ•ˆæ€§ä¸æ³›åŒ–èƒ½åŠ›

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> AMOR æå‡ºäº†ä¸€ç§**åŸºäº prediction entropy çš„å¯è§£é‡Šå…ƒè®¤çŸ¥é—¨æ§æœºåˆ¶**ï¼Œå®ç°äº† SSM ä¸ Attention çš„åŠ¨æ€åˆ‡æ¢ï¼Œåœ¨åˆæˆä»»åŠ¡ä¸­ä»¥ **ä»… 22% çš„æ³¨æ„åŠ›è°ƒç”¨ç‡è¾¾åˆ°äº† 100% çš„æ£€ç´¢å‡†ç¡®ç‡**ï¼Œå±•ç¤ºäº†â€œçŸ¥é“ä½•æ—¶è¯¥æ·±å…¥æ€è€ƒâ€çš„æ™ºèƒ½è®¡ç®—èŒƒå¼ã€‚

</details>

---

### 14. [Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking](https://arxiv.org/abs/2602.13852)

**Authors**: Zhengmian Hu, Lei Shi, Ritwik Sinha, Justin Grover, David Arbour  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.13852v1  

#### Abstract
Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could gui...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ç°ä»£åœ¨çº¿A/Bæµ‹è¯•é¢ä¸´ä¸¤å¤§ç“¶é¢ˆï¼š
- **æµé‡ç¨€ç¼º**ï¼šå˜ä½“ï¼ˆvariantsï¼‰æ•°é‡æ¿€å¢ï¼ˆå°¤å…¶å—GenAIæ¨åŠ¨ï¼‰ï¼Œå¯¼è‡´ç»Ÿè®¡åŠŸæ•ˆç¨€é‡Šã€æ£€æµ‹æ•ˆåº”é—¨æ§›æé«˜ã€å®éªŒå‘¨æœŸå»¶é•¿ï¼Œéš¾ä»¥å†³å®šä¼˜å…ˆæµ‹è¯•å“ªäº›å˜ä½“ã€‚
- **ä¸‹æ¸¸åˆ†æä½æ•ˆ**ï¼šå®éªŒåçš„**insight extraction**ï¼ˆæ´å¯Ÿæå–ï¼‰å’Œ**opportunity generation**ï¼ˆæœºä¼šç”Ÿæˆï¼‰é€šå¸¸ä¾èµ–äººå·¥ã€ä¸»è§‚ä¸”å†…å®¹æ— å…³ï¼ˆcontent-agnosticï¼‰ï¼Œç¼ºä¹ç³»ç»Ÿæ€§å’Œå¯æ‰©å±•æ€§ã€‚

æ­¤å¤–ï¼Œä¼ä¸šæ™®éæœªå……åˆ†åˆ©ç”¨å†å²A/Bæµ‹è¯•ç»“æœå’Œä¸°å¯Œçš„åˆ›æ„å†…å®¹åµŒå…¥ï¼ˆembeddingsï¼‰æ¥æŒ‡å¯¼æ–°å®éªŒçš„è®¾è®¡ä¸ä¼˜åŒ–ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ¡†æ¶
ä½œè€…æå‡ºä¸€ä¸ªç»Ÿä¸€çš„ **Experimentation Accelerator** æ¡†æ¶ï¼Œé›†æˆä¸‰å¤§æ¨¡å—ï¼š

#### ï¼ˆ1ï¼‰**Rankingï¼ˆæ’åºï¼‰æ¨¡å—**
- åˆ©ç”¨å†å²A/Bæµ‹è¯•æ•°æ®ä¸­çš„ treatment embeddings å’Œ CTR ç»“æœï¼Œè®­ç»ƒä¸€ä¸ªåŸºäº **Mixed-Effects Regression (MER)** çš„ CTR ranking modelã€‚
- è¯¥æ¨¡å‹é€šè¿‡ PCA é™ç»´å¤„ç†åµŒå…¥ï¼Œå¹¶å¼•å…¥å®éªŒçº§ fixed effects æ¥æ ¡æ­£ä¸åŒå®éªŒé—´çš„å…¨å±€å·®å¼‚ã€‚
- åœ¨æ¨ç†é˜¶æ®µï¼Œå¯¹æ–°å®éªŒçš„å€™é€‰å˜ä½“è¿›è¡Œç›¸å¯¹ CTR æ’åºï¼Œè¾…åŠ©ä¼˜å…ˆçº§å†³ç­–ã€‚

#### ï¼ˆ2ï¼‰**Insightsï¼ˆæ´å¯Ÿï¼‰æ¨¡å—**
- å°† treatment æ˜ å°„åˆ°ä¸€ç»„é¢„å®šä¹‰çš„ **semantic marketing attributes**ï¼ˆå¦‚ urgency, FOMO, action_oriented ç­‰å…±122ä¸ªï¼‰ã€‚
- ä½¿ç”¨ **constrained Lasso** å°†åŸå§‹ ranker çš„æƒé‡æŠ•å½±åˆ° attribute spaceï¼Œå¾—åˆ°å¸¦ç¬¦å·çš„ per-attribute è´¡çŒ®ç³»æ•°ï¼ˆsigned contributionsï¼‰ã€‚
- æ”¯æŒå¯è§†åŒ–è§£é‡Šã€top-k é©±åŠ¨å› ç´ è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€æ´å¯Ÿç”Ÿæˆï¼ˆvia LLMï¼‰ã€‚

#### ï¼ˆ3ï¼‰**Opportunityï¼ˆæœºä¼šï¼‰æ¨¡å—**
- æ„å»º **Opportunity Index**ï¼šç»“åˆå±æ€§é‡è¦æ€§ï¼ˆæ¥è‡ª ranker ç³»æ•°ï¼‰ä¸å½“å‰å®éªŒä¸­è¯¥å±æ€§çš„â€œè¡¨è¾¾ä¸è¶³â€ç¨‹åº¦ï¼ˆunder-expressionï¼‰ã€‚
- åˆ©ç”¨ LLM å°†é«˜æ½œåŠ›ç¼ºå¤±å±æ€§è½¬åŒ–ä¸ºå…·ä½“çš„åˆ›æ„æ”¹è¿›å»ºè®®ï¼ˆcreative suggestionsï¼‰ï¼Œå¹¶ä¼°è®¡å…¶ conversion potential ä¸ learning potentialã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | æœ¬æ–‡æ–¹æ³• | ç°æœ‰æ–¹æ³• |
|------|----------|--------|
| **Interpretability** | æ˜¾å¼æ˜ å°„è‡³è¯­ä¹‰å±æ€§ç©ºé—´ï¼Œæä¾›å¯è§£é‡Šçš„é©±åŠ¨å› å­ | å¤šåœ¨ embedding space æ“ä½œï¼Œé»‘ç®±æ€§å¼º |
| **Actionability** | è¾“å‡ºå…·ä½“æ–‡æœ¬å»ºè®®ï¼Œç›´æ¥æŒ‡å¯¼åˆ›æ„è¿­ä»£ | ä»…é¢„æµ‹æ•ˆæœï¼Œæ— åç»­è¡ŒåŠ¨æŒ‡å¼• |
| **Knowledge Transfer** | åˆ©ç”¨è·¨åŸŸå†å²æ•°æ®å®ç°è¿ç§»å­¦ä¹ ï¼ˆUpworthy â†’ å®¢æˆ·æ•°æ®ï¼‰ | å¤šä¸ºå•ä»»åŠ¡å»ºæ¨¡æˆ–é›¶æ ·æœ¬æç¤º |
| **ç³»ç»Ÿæ•´åˆæ€§** | ä¸‰é˜¶æ®µé—­ç¯ï¼šRank â†’ Explain â†’ Improve | åŠŸèƒ½å‰²è£‚ï¼Œç¼ºä¹ç«¯åˆ°ç«¯æ”¯æŒ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†ä½¿ç”¨
| ç±»å‹ | åç§° | æè¿° |
|------|------|------|
| **è®­ç»ƒæ•°æ®** | **Upworthy Dataset** [9] | åŒ…å«2013â€“2015å¹´é—´32,487ä¸ªæ–°é—»æ ‡é¢˜A/Bæµ‹è¯•çš„çœŸå®CTRæ•°æ®ï¼Œç”¨äºè®­ç»ƒ ranking modelã€‚ |
| **æµ‹è¯•æ•°æ®** | **Adobe Customer Experiments** | æ¥è‡ªçœŸå®å®¢æˆ·çš„65ä¸ªå·²å®Œæˆä¸”å…·æœ‰æ˜¾è‘—CTRå·®å¼‚çš„è¥é”€æ–‡æ¡ˆå®éªŒï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚ |

> âš ï¸ æ³¨æ„ï¼šå‡ºäºéšç§åˆè§„è¦æ±‚ï¼Œå®¢æˆ·æ•°æ®ä¸èƒ½ç”¨äºè®­ç»ƒï¼Œä»…ä½œæµ‹è¯•ï¼Œä½“ç°B2Bå¹³å°çš„å®é™…éƒ¨ç½²çº¦æŸã€‚

### ğŸ§ª å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### ï¼ˆ1ï¼‰Ranking æ€§èƒ½è¯„ä¼°
- **ç›®æ ‡**ï¼šéªŒè¯ ranking model æ˜¯å¦èƒ½å‡†ç¡®é¢„æµ‹æ–°å®éªŒä¸­å„å˜ä½“çš„ç›¸å¯¹æ’åã€‚
- **æŒ‡æ ‡**ï¼š
  - **Spearmanâ€™s Rank Correlation (Ï)**ï¼šè¡¡é‡é¢„æµ‹æ’åä¸çœŸå®æ’åçš„ç›¸å…³æ€§ã€‚
  - **Top-1 Accuracy**ï¼šé¢„æµ‹æœ€ä¼˜å˜ä½“æ˜¯å¦ä¸å®é™…æœ€ä½³ä¸€è‡´çš„æ¯”ä¾‹ã€‚

#### ï¼ˆ2ï¼‰Embedding æ¶ˆèæ¯”è¾ƒ
- å¯¹æ¯”ä¸‰ç§ embedding æ–¹æ¡ˆåœ¨ leave-one-out è®¾ç½®ä¸‹çš„è¡¨ç°ï¼š
  1. **MiniLM**ï¼šè½»é‡çº§ sentence transformer
  2. **Llama**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ç¼–ç å™¨ï¼ˆæ›´å¼ºè¯­ä¹‰æ•æ‰ï¼‰
  3. **Attribute Score**ï¼šLLM-as-a-judge ç›´æ¥æ‰“åˆ†ï¼ˆ1â€“5åˆ†ï¼‰

#### ï¼ˆ3ï¼‰Insights & Opportunities è´¨é‡è¯„ä¼°
- **äººç±»è¯„ä¼°**ï¼šç”±å¸‚åœºä¸“å®¶åˆ¤æ–­ç”Ÿæˆçš„ insight/opportunity æ˜¯å¦ç»“æ„è‰¯å¥½ã€å‡†ç¡®åˆç†ã€‚
- **LLM-as-a-Judge**ï¼šä½¿ç”¨ GPT-4o ä½œä¸ºè‡ªåŠ¨è£åˆ¤ï¼Œä¾æ®é¢„è®¾ rubric æ‰“å‡º accept/reject å†³ç­–ã€‚
- **è¾“å‡ºç»´åº¦**ï¼šæ¥å—ç‡ï¼ˆAcceptance Rateï¼‰ã€é«˜è´¨é‡é¡¹ç›®æ•°ã€è¦†ç›–å®éªŒæ•°ç­‰ã€‚

#### ï¼ˆ4ï¼‰Baseline å¯¹æ¯”
- **Random Guess**ï¼šéšæœºçŒœæµ‹æ’åï¼ŒæœŸæœ› Ï = 0ï¼ŒTop-1 Acc â‰ˆ 1/Kï¼ˆKä¸ºå¹³å‡å˜ä½“æ•°ï¼‰ã€‚
- **Attribute Score Only**ï¼šä¸ç»è¿‡è®­ç»ƒæ¨¡å‹ï¼Œç›´æ¥ç”¨LLMå¯¹å±æ€§æ‰“åˆ†æ’åºã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### ï¼ˆ1ï¼‰Ranking æ¨¡å—æ€§èƒ½ï¼ˆè¡¨2ï¼‰
| æ–¹æ³• | Spearman Ï | Top-1 Accuracy |
|------|------------|----------------|
| **Transfer Learning (Ours)** | **0.514 Â± 0.166** | **70.2% Â± 11.8%** |
| Random Guess Baseline | ~0 | ~42.9% |

> âœ… ç»“è®ºï¼šæ¨¡å‹æ˜¾è‘—ä¼˜äºéšæœºçŒœæµ‹ï¼Œåœ¨çœŸå®å®¢æˆ·æ•°æ®ä¸Šå®ç°äº†è¶…è¿‡70%çš„é¦–åå‘½ä¸­ç‡ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒ Embedding çš„ Leave-One-Out è¡¨ç°ï¼ˆè¡¨3ï¼‰
| Embedding | Spearman Ï |
|-----------|------------|
| **Llama** | **0.727 Â± 0.116** âœ… |
| Attribute Score | 0.576 Â± 0.142 |
| MiniLM | 0.454 Â± 0.155 |
| Random Guess | 0 |

> âœ… ç»“è®ºï¼š**Llama embeddings** è¡¨ç°æœ€ä½³ï¼Œè¯´æ˜æ›´æ·±å±‚æ¬¡çš„è¯­è¨€ç†è§£æœ‰åŠ©äºæ•æ‰å½±å“CTRçš„å…³é”®è¯­ä¹‰ä¿¡å·ã€‚

#### ï¼ˆ3ï¼‰Insight Generation äººç±»è¯„ä¼°ç»“æœï¼ˆè¡¨4ï¼‰
| LM æ¨¡å‹ | ç”Ÿæˆæ•° | é«˜è´¨é‡æ•° | æ¥å—ç‡ |
|--------|-------|---------|--------|
| **GPT-4o** | 52 | 46 | **88.46%** âœ… |
| LLaMA-70B | 26 | 23 | 88.46% |
| GPT-4o-mini | 157 | 66 | 42.04% âŒ |

> âœ… GPT-4o åœ¨è´¨é‡å’Œæ•°é‡é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼›mini ç‰ˆæœ¬è™½å¤šäº§ä½†å™ªå£°å¤§ã€‚

#### ï¼ˆ4ï¼‰Opportunity Generation äººç±»è¯„ä¼°ç»“æœï¼ˆè¡¨5ï¼‰
| LM æ¨¡å‹ | ç”Ÿæˆæ•° | é«˜è´¨é‡æ•° | æ¥å—ç‡ |
|--------|-------|---------|--------|
| **GPT-4o** | 195 | 168 | **86.15%** âœ… |
| GPT-4o-mini | 195 | 164 | 84.10% |
| LLaMA-70B | 123 | 101 | 82.11% |

> âœ… æ‰€æœ‰ä¸»æµLLMå‡èƒ½ç”Ÿæˆé«˜æ¥å—åº¦çš„æœºä¼šå»ºè®®ï¼ŒGPT-4oç»¼åˆè¡¨ç°æœ€ä¼˜ã€‚

#### ï¼ˆ5ï¼‰LLM è‡ªåŠ¨è¯„ä¼°ç»“æœï¼ˆé™„å½•Bï¼‰
- **Insight Evaluation (GPT-4o Judge)**ï¼šGPT-4oç”Ÿæˆçš„insightä»è·æœ€é«˜æ¥å—ç‡ï¼ˆ53.87%ï¼‰ï¼Œè¿œé«˜äº mini ç‰ˆæœ¬ï¼ˆ16.87%ï¼‰ã€‚
- **Opportunity Evaluation**ï¼šæ‰€æœ‰æ¨¡å‹æ¥å—ç‡ >75%ï¼ŒGPT-4o è¾¾ **81.15%**ï¼ŒéªŒè¯å»ºè®®ä¸€è‡´æ€§é«˜ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å†å²A/Bæ•°æ®å¯ç”¨äºæœ‰æ•ˆè¿ç§»å­¦ä¹ **ï¼šå°½ç®¡è®­ç»ƒæ•°æ®ï¼ˆUpworthyï¼‰ä¸å®¢æˆ·åœºæ™¯å­˜åœ¨é¢†åŸŸåç§»ï¼Œranking model ä¾ç„¶èƒ½åœ¨çœŸå®ä¸šåŠ¡ä¸­å®ç°æ˜¾è‘—é«˜äºéšæœºçš„é¢„æµ‹æ€§èƒ½ã€‚
2. **attribute-level æŠ•å½±æå¤§æå‡å¯è§£é‡Šæ€§**ï¼šé€šè¿‡ constrained Lasso å°†é»‘ç®± embedding weights æ˜ å°„åˆ°è¥é”€è¯­ä¹‰å±æ€§ï¼Œä½¿æ¨¡å‹è¾“å‡ºå¯è¢«ä¸šåŠ¡äººå‘˜ç†è§£å’Œä¿¡ä»»ã€‚
3. **Opportunity Index å¯ç³»ç»Ÿå‘ç°é«˜ä»·å€¼æ”¹è¿›ç‚¹**ï¼šç»“åˆâ€œé‡è¦æ€§â€ä¸â€œè¡¨è¾¾ä¸è¶³â€ï¼Œèƒ½ç²¾å‡†å®šä½è¢«å¿½ç•¥ä½†æ½œåœ¨æ”¶ç›Šé«˜çš„åˆ›æ„æ–¹å‘ã€‚
4. **LLM æ˜¯å¼ºå¤§çš„è§£é‡Šä¸å»ºè®®å¼•æ“**ï¼šé…åˆç»“æ„åŒ–è¾“å…¥ä¸ prompt engineeringï¼ŒLLM å¯ç¨³å®šè¾“å‡ºé«˜è´¨é‡ã€æ–‡æœ¬é”šå®šï¼ˆtext-groundedï¼‰çš„è‡ªç„¶è¯­è¨€æ´å¯Ÿä¸å»ºè®®ã€‚

### âš ï¸ å±€é™æ€§
1. **é¢†åŸŸè¿ç§»é£é™©**ï¼štransfer learning æˆåŠŸä¾èµ–äºæºåŸŸä¸ç›®æ ‡åŸŸä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚è‹¥å®¢æˆ·è¡Œä¸šä¸ Upworthy å·®å¼‚è¿‡å¤§ï¼ˆå¦‚é‡‘è vs æ–°é—»ï¼‰ï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™ã€‚
2. **è¯­è¨€é™åˆ¶**ï¼šç›®å‰ä»…æ”¯æŒè‹±æ–‡å†…å®¹ï¼Œå…¶ä»–è¯­è¨€éœ€é‡æ–°æ„å»º attribute lexicon ä¸ embedding pipelineã€‚
3. **embedding è´¨é‡æ•æ„Ÿ**ï¼šä¸åŒ embedderï¼ˆå¦‚ MiniLM vs Llamaï¼‰æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œé€‰æ‹©ä¸å½“ä¼šå½±å“æ•´ä¸ª pipeline æ•ˆæœã€‚
4. **attribute set å›ºå®šæ€§**ï¼šè™½ç„¶å…è®¸å®¢æˆ·å®šåˆ¶ï¼Œä½†åˆå§‹122ä¸ªå±æ€§å¯èƒ½æ— æ³•è¦†ç›–æ‰€æœ‰å‚ç›´é¢†åŸŸç‰¹æœ‰æ¦‚å¿µã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **å¤šæ¨¡æ€æ‰©å±•**ï¼šå°†æ¡†æ¶æ¨å¹¿è‡³å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘åŠé¡µé¢å¸ƒå±€ç­‰éæ–‡æœ¬å†…å®¹ã€‚
2. **æ›´å¤šæ ·åŒ–çš„ Outcome Modeling**ï¼šä» CTR æ‰©å±•åˆ° conversion rateã€revenueã€LTV ç­‰å•†ä¸šæŒ‡æ ‡ã€‚
3. **åŠ¨æ€ attribute discovery**ï¼šåˆ©ç”¨ LLM è‡ªåŠ¨ç”Ÿæˆæ–°çš„è¯­ä¹‰å±æ€§ï¼Œè€Œéä¾èµ–äººå·¥é¢„å®šä¹‰ã€‚
4. **ä¸ªæ€§åŒ– ranking models**ï¼šä¸ºä¸åŒå®¢æˆ·è®­ç»ƒä¸“å± rankerï¼Œç¼“è§£ domain shift é—®é¢˜ã€‚
5. **é—­ç¯è‡ªåŠ¨åŒ–å®éªŒè®¾è®¡**ï¼šå°† opportunity suggestion ç›´æ¥æ¥å…¥ GenAI å†…å®¹ç”Ÿæˆ pipelineï¼Œå®ç°â€œå»ºè®®â†’ç”Ÿæˆâ†’æµ‹è¯•â†’åé¦ˆâ€çš„å…¨è‡ªåŠ¨åŒ–å¾ªç¯ã€‚

---

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> *Experimentation Accelerator* é€šè¿‡èåˆ historical A/B dataã€content-aware embeddingsã€interpretable modeling ä¸ GenAIï¼Œæ„å»ºäº†ä¸€ä¸ª**å¯è§£é‡Šã€å¯è¡ŒåŠ¨ã€å¯è§„æ¨¡åŒ–**çš„A/Bæµ‹è¯•å¢å¼ºç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†æ•°å­—è¥é”€å®éªŒçš„æ•ˆç‡ä¸æ™ºèƒ½æ°´å¹³ã€‚

</details>

---

### 15. [GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses](https://arxiv.org/abs/2602.14676)

**Authors**: Attila Lischka, Bal\'azs Kulcs\'ar  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.14676v1  

#### Abstract
Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of g...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šGREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡æå‡ºå¹¶ç ”ç©¶äº†**Bus Evacuation Orienteering Problem (BEOP)**ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ç´§æ€¥ç–æ•£åœºæ™¯ä¸‹ã€ä»¥å…¬äº¤è½¦ä¸ºå·¥å…·çš„ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚å…¶ç›®æ ‡æ˜¯åœ¨æœ‰é™æ—¶é—´å†…ï¼Œåˆ©ç”¨æœ‰é™æ•°é‡å’Œå®¹é‡çš„å…¬äº¤è½¦ï¼Œä»åŸå¸‚åŒºåŸŸç–æ•£å°½å¯èƒ½å¤šçš„äººå£ã€‚

è¯¥é—®é¢˜æºäºç°å®ä¸­çš„ç´§æ€¥æƒ…å†µï¼ˆå¦‚è‡ªç„¶ç¾å®³ã€å·¥ä¸šäº‹æ•…ç­‰ï¼‰ï¼Œæ—¨åœ¨ç¼“è§£çº¯ç§å®¶è½¦ç–æ•£å¯¼è‡´çš„äº¤é€šæ‹¥å µå’Œæ··ä¹±ã€‚ä¸ä¼ ç»Ÿçš„ Bus Evacuation Problem (BEP) ä¸åŒï¼ŒBEOP å…è®¸æ— æ³•è¦†ç›–æ‰€æœ‰ç–æ•£ç‚¹ï¼Œè½¬è€Œè¿½æ±‚æœ€å¤§åŒ–ç–æ•£äººæ•°ï¼Œè¿™ä½¿å…¶æ›´è´´è¿‘å®é™…èµ„æºå—é™çš„æƒ…å†µã€‚

### æå‡ºçš„æ–°æ–¹æ³•å’Œæ–°æ€è·¯
- **æå‡ºäº† BEOP æ¨¡å‹**ï¼šé¦–æ¬¡å°†å¤šè¶Ÿæ¬¡ã€å¸¦æ—¶é—´çª—çš„ capacitated team orienteering problem åº”ç”¨äºå…¬äº¤åº”æ€¥ç–æ•£ï¼Œå¹¶å½¢å¼åŒ–ä¸ºä¸€ä¸ª NP-hard é—®é¢˜ã€‚
- **è®¾è®¡äº†æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ (MILP) å’Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP) æ¡†æ¶**ï¼šä¸º BEOP æä¾›äº†ç²¾ç¡®å’Œåºåˆ—å†³ç­–ä¸¤ç§å»ºæ¨¡æ–¹å¼ã€‚
- **å¼€å‘äº†åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ GREAT-EER**ï¼š
  - **Encoder**: é‡‡ç”¨ **Graph Edge Attention Network (GREAT)**ï¼Œä¸“é—¨å¤„ç†éæ¬§å‡ é‡Œå¾—ã€ä¸å¯¹ç§°çš„å›¾ç»“æ„ï¼ˆå¦‚çœŸå®é“è·¯ç½‘ç»œä¸­çš„å•è¡Œé“ã€æ‹¥å µï¼‰ã€‚
  - **Decoder**: ä½¿ç”¨ multi-pointer network è¿›è¡Œé€æ­¥è§£ç ã€‚
  - **è®­ç»ƒæœºåˆ¶**: åŸºäº POMO (Policy Optimization with Multiple Optima) æ¡†æ¶è¿›è¡Œæ— ç›‘ç£å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ— éœ€ä¾èµ–æ±‚è§£å™¨ç”Ÿæˆæ ‡ç­¾ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **é«˜æ•ˆæ¨ç†é€Ÿåº¦**ï¼šè®­ç»ƒå®Œæˆåï¼ŒGREAT-EER å¯åœ¨**ç§’çº§ç”šè‡³æ¯«ç§’çº§**å†…ç”Ÿæˆé«˜è´¨é‡ç–æ•£æ–¹æ¡ˆï¼Œè¿œå¿«äºä¼ ç»Ÿ MILP æ±‚è§£å™¨ï¼ˆå¯èƒ½éœ€è¦æ•°å°æ—¶ï¼‰ã€‚
- **é«˜æ³›åŒ–èƒ½åŠ›**ï¼šæ¨¡å‹èƒ½åœ¨æœªè§è¿‡çš„æ›´å¤§è§„æ¨¡å®ä¾‹ï¼ˆå¦‚ 200 èŠ‚ç‚¹ï¼‰ã€å«â€œå±é™©åŒºâ€ï¼ˆhazard zonesï¼‰çš„é“è·¯ä¸­æ–­åœºæ™¯ä¸­ä¿æŒè‰¯å¥½è¡¨ç°ã€‚
- **æ”¯æŒåŠ¨æ€åœ¨çº¿å†³ç­–**ï¼šå¯æ‰©å±•è‡³éšæœºç¯å¢ƒï¼ˆå¦‚ä¸ç¡®å®šçš„å‡ºè¡Œæ—¶é—´å’Œéœ€æ±‚ï¼‰ï¼Œå®æ—¶è°ƒæ•´è·¯çº¿ï¼Œé¿å…è¿åç¡¬çº¦æŸï¼ˆå¦‚è¶…æ—¶ï¼‰ã€‚
- **æä¾›æˆ˜ç•¥è§„åˆ’æŒ‡å¯¼**ï¼šå¯ç”¨äºäº‹å‰åˆ†æä¸åŒå‚æ•°ï¼ˆè½¦è¾†æ•°ã€å®¹é‡ã€æ—¶é—´ï¼‰å¯¹ç–æ•£é…é¢çš„å½±å“ï¼Œè¾…åŠ©åº”æ€¥èµ„æºè°ƒé…ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **åœ°ç†æ•°æ®æ¥æº**ï¼šä½¿ç”¨ **OpenStreetMap** è·å–æ—§é‡‘å±±å¸‚çš„çœŸå®é“è·¯ç½‘ç»œï¼ˆçº¦ 10,000 ä¸ªèŠ‚ç‚¹ï¼‰ã€‚
- **äººå£å¯†åº¦ä»£ç†æ•°æ®**ï¼šä½¿ç”¨å…¬å¼€çš„ **Uber GPS dataset** æ¨æ–­åŸå¸‚ä¸­äººæµå¯†é›†çš„ POIï¼ˆå…´è¶£ç‚¹ï¼‰ï¼Œä½œä¸ºæ½œåœ¨çš„ç–æ•£ç‚¹ã€‚
- **ç”Ÿæˆ BEOP å®ä¾‹**ï¼š
  - éšæœºé‡‡æ · 100 æˆ– 200 ä¸ªèŠ‚ç‚¹ä½œä¸ºç–æ•£ç‚¹ï¼Œå¦é€‰ä¸€ä¸ªä½œä¸ºå®‰å…¨ä¸­å¿ƒï¼ˆdepotï¼‰ã€‚
  - æ¯ä¸ªèŠ‚ç‚¹åˆ†é…éšæœºéœ€æ±‚ï¼ˆ1â€“5 äººï¼‰å’Œå¯é€‰çš„æ—¶é—´çª—ï¼ˆæœ€å¤š 30% èŠ‚ç‚¹éœ€åœ¨æŒ‡å®šæ—¶é—´å†…è¢«æ¥èµ°ï¼‰ã€‚
  - è½¦è¾†å‚æ•°ï¼ˆæ•°é‡ã€å®¹é‡ï¼‰ã€æœ€å¤§ç–æ•£æ—¶é—´éšæœºé‡‡æ ·ç”¨äºè®­ç»ƒã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **è®­ç»ƒè®¾ç½®**ï¼š
  - æ¨¡å‹è§„æ¨¡ï¼šGREAT Encoder å« 5 å±‚ï¼Œéšè—ç»´åº¦ 128ï¼Œ8 ä¸ªæ³¨æ„åŠ›å¤´ã€‚
  - ä¼˜åŒ–å™¨ï¼šADAMï¼Œå­¦ä¹ ç‡ 0.0001ã€‚
  - è®­ç»ƒæ•°æ®ï¼š25,000 ä¸ª BEOP å®ä¾‹ï¼Œbatch size 50ï¼Œè®­ç»ƒ 50 è½®ã€‚
  - ç¡¬ä»¶ï¼šNVIDIA A40 GPUï¼Œæ€»è®­ç»ƒæ—¶é—´çº¦ 8 å°æ—¶ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Evacuation Quota (%)**ï¼šæˆåŠŸé€šè¿‡å…¬äº¤è½¦ç–æ•£çš„äººæ•°å æ€»éœ€æ±‚çš„æ¯”ä¾‹ã€‚
  - **è¿è¡Œæ—¶é—´ (Runtime)**ï¼šç”Ÿæˆè§£å†³æ–¹æ¡ˆæ‰€éœ€æ—¶é—´ã€‚
  - **Optimality Gap (%)**ï¼šä¸ Gurobi æ±‚è§£å™¨åœ¨æ—¶é—´é™åˆ¶ä¸‹æ‰€å¾—ä¸Šç•Œä¹‹é—´çš„å·®è·ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Gurobi (MILP)**ï¼šä½¿ç”¨å•†ä¸šæ±‚è§£å™¨æ±‚è§£æå‡ºçš„ MILP æ¨¡å‹ï¼Œè®¾ 30 åˆ†é’Ÿæˆ– 24 å°æ—¶æ—¶é—´é™åˆ¶ï¼Œä½œä¸ºè¿‘ä¼¼æœ€ä¼˜å‚è€ƒã€‚
- **Greedy Heuristic**ï¼šä¸€ç§ç®€å•çš„è´ªå¿ƒç®—æ³•ï¼Œæ¯æ¬¡é€‰æ‹©å•ä½æ—¶é—´æ”¶ç›Šæœ€é«˜çš„å¯è¡ŒèŠ‚ç‚¹ã€‚
- **Warm-start Gurobi**ï¼šä½¿ç”¨ GREAT-EER çš„è§£ä½œä¸º Gurobi çš„åˆå§‹è§£ï¼Œæµ‹è¯•å…¶æ”¹è¿›æ½œåŠ›ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
- åœ¨ **100 èŠ‚ç‚¹**çš„ BEOP å®ä¾‹ä¸Šï¼š
  - GREAT-EER å¹³å‡å¯åœ¨ **~0.4 ç§’** å†…å®Œæˆæ±‚è§£ã€‚
  - åœ¨å¤šä¸ªé…ç½®ä¸‹ï¼Œå…¶ç–æ•£é…é¢è¾¾åˆ° **85%â€“100%**ï¼Œæ¥è¿‘ Gurobi åœ¨é•¿æ—¶é—´è¿è¡Œåçš„ç»“æœã€‚
- åœ¨ **20 èŠ‚ç‚¹å°è§„æ¨¡å®ä¾‹**ä¸Šçš„è¯¦ç»†å¯¹æ¯”ï¼ˆè§ Table 2ï¼‰ï¼š
  - GREAT-EER çš„å¹³å‡æœ€ä¼˜æ€§å·®è·ä»…ä¸º **0.0â€“2.18%**ï¼Œæ˜¾è‘—ä¼˜äº Greedyï¼ˆ1.21â€“14.44%ï¼‰ã€‚
  - è¿è¡Œæ—¶é—´ï¼šGREAT-EER ~0.38s vs. Gurobi >5000s vs. Greedy ~0.004sã€‚
- åœ¨ **200 èŠ‚ç‚¹å¤§è§„æ¨¡å®ä¾‹**ä¸Šçš„æ³›åŒ–æµ‹è¯•ï¼ˆè§ Table 9ï¼‰ï¼š
  - å³ä½¿è¶…å‡ºè®­ç»ƒåˆ†å¸ƒï¼ŒGREAT-EER ä»èƒ½å®ç° **87.4%â€“100%** çš„ç–æ•£é…é¢ï¼Œå§‹ç»ˆä¼˜äº Greedyï¼ˆ78.82%â€“99.98%ï¼‰ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
| æ–¹æ³• | å¹³å‡ç–æ•£é…é¢ | å¹³å‡è¿è¡Œæ—¶é—´ | æœ€ä¼˜æ€§å·®è· |
|------|---------------|----------------|-------------|
| **GREAT-EER** | é«˜ (â‰ˆ Gurobi) | **æå¿« (~0.4s)** | æä½ |
| **Gurobi (30min)** | æœ€é«˜ | ææ…¢ (>10000s) | 0 (åŸºå‡†) |
| **Gurobi + GREAT-EER init** | **æ›´é«˜** | å¿« (å‡ åˆ†é’Ÿ) | è´Ÿå€¼ï¼ˆæ”¹è¿›ï¼‰ |
| **Greedy** | æ˜æ˜¾è¾ƒä½ | æœ€å¿« (~0.004s) | é«˜ |

- **å…³é”®å‘ç°**ï¼šGREAT-EER çš„è§£å¯ä½œä¸º Gurobi çš„ä¼˜ç§€ warm-startï¼Œä½¿å…¶åœ¨ **10 åˆ†é’Ÿå†…è¶…è¶Š GREAT-EER è‡ªèº«æ€§èƒ½**ï¼ˆä¾‹å¦‚ä» 94.41% æå‡åˆ° 95.07%ï¼Œè§ Figure 5ï¼‰ã€‚

### æ¶ˆèå®éªŒä¸é¢å¤–éªŒè¯
- **Out-of-Distribution æµ‹è¯•**ï¼š
  - åœ¨å« **hazard zones**ï¼ˆé“è·¯å°é”ï¼‰çš„åœºæ™¯ä¸­ï¼ŒGREAT-EER ä»èƒ½æœ‰æ•ˆè§„åˆ’ï¼Œç–æ•£é…é¢è¾¾ 70â€“100%ï¼Œè¿œé«˜äº Greedyã€‚
  - åœ¨ **200 èŠ‚ç‚¹**çš„å¤§è§„æ¨¡å®ä¾‹ä¸Šï¼Œæ€§èƒ½ä¸‹é™æœ‰é™ï¼Œè¯æ˜å¼ºæ³›åŒ–æ€§ã€‚
- **Stochastic Online Setting**ï¼š
  - åœ¨æ¨¡æ‹Ÿéšæœºå‡ºè¡Œæ—¶é—´å’Œéœ€æ±‚å˜åŒ–çš„ç¯å¢ƒä¸­ï¼Œ**stochastic GREAT-EER æ¨¡å‹**èƒ½å¤ŸåŠ¨æ€å“åº”ï¼Œ**é›¶è¿è§„**ï¼ˆæ— è¶…æ—¶æ–¹æ¡ˆï¼‰ã€‚
  - ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¡®å®šæ€§è®¡åˆ’å³ä½¿ç»è¿‡å¢å¼ºï¼ˆPOMO=100ï¼‰ï¼Œä»æœ‰ **9â€“15% çš„å®ä¾‹å› è¶…æ—¶è€Œå¤±æ•ˆ**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
- **BEOP æ˜¯ä¸€ä¸ªå®ç”¨ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°é—®é¢˜**ï¼Œèƒ½æ›´å¥½åæ˜ ç°å®åº”æ€¥ç–æ•£ä¸­çš„èµ„æºçº¦æŸã€‚
- **GREAT-EER èƒ½å¤Ÿåœ¨æçŸ­æ—¶é—´å†…ç”Ÿæˆæ¥è¿‘æœ€ä¼˜çš„ç–æ•£æ–¹æ¡ˆ**ï¼Œé€‚ç”¨äºäº‹å‰è§„åˆ’å’Œå®æ—¶å“åº”ã€‚
- **GREAT-EER çš„è¾“å‡ºæ˜¯å¼ºå¤§çš„ warm-start è§£**ï¼Œå¯æå¤§åŠ é€Ÿç²¾ç¡®æ±‚è§£å™¨çš„æ”¶æ•›ã€‚
- **æ¨¡å‹å…·å¤‡å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›**ï¼Œèƒ½é€‚åº”æ›´å¤§è§„æ¨¡ã€é“è·¯ä¸­æ–­ç­‰å¤æ‚åœºæ™¯ã€‚
- **åœ¨éšæœºåŠ¨æ€ç¯å¢ƒä¸­ï¼ŒGREAT-EER å¯å®ç°å®æ—¶è‡ªé€‚åº”è°ƒåº¦**ï¼Œç¡®ä¿æ–¹æ¡ˆå¯è¡Œæ€§å¹¶æå‡èµ„æºåˆ©ç”¨ç‡ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰ MDP æ¡†æ¶åœ¨éšæœºè®¾ç½®ä¸‹ä»…æ”¯æŒå•è¾†è½¦çš„å®æ—¶è°ƒåº¦ï¼Œå¤šè½¦ååŒçš„åœ¨çº¿ä¼˜åŒ–å°šæœªè§£å†³ã€‚
- æ¨¡å‹å‡è®¾èŠ‚ç‚¹éœ€æ±‚å’Œå‡ºè¡Œæ—¶é—´çš„ä¸ç¡®å®šæ€§æ˜¯å¤–ç”Ÿçš„ï¼Œæœªè€ƒè™‘ä¸ªä½“é€‰æ‹©è¡Œä¸ºï¼ˆå¦‚æ›´å¤šäººæ”¹ç”¨ç§å®¶è½¦ï¼‰å¯¹äº¤é€šæµçš„åé¦ˆå½±å“ã€‚
- æœªè€ƒè™‘ä¸Šä¸‹è½¦æ—¶é—´ã€è½¦è¾†å¼‚æ„æ€§ï¼ˆå¦‚è½®æ¤…ä¸“ç”¨å·´å£«ï¼‰ã€ä»¥åŠå¯¹å¼±åŠ¿ç¾¤ä½“çš„ä¼˜å…ˆçº§è°ƒåº¦ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³ **å¤šè½¦ååŒçš„éšæœºåœ¨çº¿è°ƒåº¦**ã€‚
- å¼•å…¥ **æ›´å¤æ‚çš„åŠ¨æ€æ¨¡å‹**ï¼Œå¦‚æ‹¥å µéšç–æ•£è¡Œä¸ºæ¼”åŒ–çš„åé¦ˆæœºåˆ¶ã€‚
- æ”¯æŒ **split-delivery**ï¼ˆåŒä¸€åœ°ç‚¹å¤šæ¬¡è®¿é—®ï¼‰ã€**loading time** å’Œ **heterogeneous fleets**ã€‚
- æ¢ç´¢ **multi-objective ä¼˜åŒ–**ï¼Œå¦‚å…¼é¡¾å…¬å¹³æ€§ï¼ˆequityï¼‰å’Œæ•ˆç‡ã€‚
- è¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨ **è¶…å¤§è§„æ¨¡åŸå¸‚ç½‘ç»œ** ä¸Šçš„å¯æ‰©å±•æ€§ã€‚

</details>

---

### 16. [WebWorld: A Large-Scale World Model for Web Agent Training](https://arxiv.org/abs/2602.14721)

**Authors**: Zikai Xiao, Jianhong Tu, Chuhang Zou, Yuxin Zuo, Zhi Li, Peng Wang, Bowen Yu, Fei Huang, Junyang Lin, Zuozhu Liu  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.14721v1  

#### Abstract
Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with ...

---

### 17. [Speculative Decoding with a Speculative Vocabulary](https://arxiv.org/abs/2602.13836)

**Authors**: Miles Williams, Young D. Kwon, Rui Li, Alexandros Kouris, Stylianos I. Venieris  
**Category**: cs.CL  
**Published**: 2026-02-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.13836v1  

#### Abstract
Speculative decoding has rapidly emerged as a leading approach for accelerating language model (LM) inference, as it offers substantial speedups while yielding identical outputs. This relies upon a small draft model, tasked with predicting the outputs of the target model. State-of-the-art speculativ...

---

### 18. [Scenario-Adaptive MU-MIMO OFDM Semantic Communication With Asymmetric Neural Network](https://arxiv.org/abs/2602.13557)

**Authors**: Chongyang Li, Tianqian Zhang, Shouyin Liu  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.13557v1  

#### Abstract
Semantic Communication (SemCom) has emerged as a promising paradigm for 6G networks, aiming to extract and transmit task-relevant information rather than minimizing bit errors. However, applying SemCom to realistic downlink Multi-User Multi-Input Multi-Output (MU-MIMO) Orthogonal Frequency Division ...

---

### 19. [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)

**Authors**: Vashista Nobaub  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.13804v1  

#### Abstract
Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors ...

---

### 20. [Small Reward Models via Backward Inference](https://arxiv.org/abs/2602.13551)

**Authors**: Yike Wang, Faeze Brahman, Shangbin Feng, Teng Xiao, Hannaneh Hajishirzi, Yulia Tsvetkov  
**Category**: cs.CL  
**Published**: 2026-02-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.13551v1  

#### Abstract
Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit...

---

### 21. [LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2602.14428)

**Authors**: Wang Xing, Wei Song, Siyu Lin, Chen Wu, Man Wang  
**Category**: cs.CL  
**Published**: 2026-02-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.14428v1  

#### Abstract
Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may over...

---

### 22. [Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability](https://arxiv.org/abs/2602.13485)

**Authors**: Ayse Tursucular, Ayush Mohanty, Nazal Mohamed, Nagi Gebraeel  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.13485v1  

#### Abstract
Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem r...

---

### 23. [Pseudo-differential-enhanced physics-informed neural networks](https://arxiv.org/abs/2602.14663)

**Authors**: Andrew Gracyk  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.14663v1  

#### Abstract
We present pseudo-differential enhanced physics-informed neural networks (PINNs), an extension of gradient enhancement but in Fourier space. Gradient enhancement of PINNs dictates that the PDE residual is taken to a higher differential order than prescribed by the PDE, added to the objective as an a...

---

### 24. [Extending Multi-Source Bayesian Optimization With Causality Principles](https://arxiv.org/abs/2602.14791)

**Authors**: Luuk Jacobs, Mohammad Ali Javidian  
**Category**: cs.LG  
**Published**: 2026-02-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.14791v1  

#### Abstract
Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments....

---

### 25. [Hippocampus: An Efficient and Scalable Memory Module for Agentic AI](https://arxiv.org/abs/2602.13594)

**Authors**: Yi Li, Lianjie Cao, Faraz Ahmed, Puneet Sharma, Bingzhe Li  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.13594v1  

#### Abstract
Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an ag...

---

### 26. [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)

**Authors**: Guojie Liu, Yiqi Wang, Yanfeng Yang, Wenqi Fan, Songlei Jian, Jianfeng Zhang, Jie Yu  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.13980v1  

#### Abstract
Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, contex...

---

### 27. [Prompt-Driven Low-Altitude Edge Intelligence: Modular Agents and Generative Reasoning](https://arxiv.org/abs/2602.14003)

**Authors**: Jiahao You, Ziye Jia, Chao Dong, Qihui Wu  
**Category**: cs.AI  
**Published**: 2026-02-17  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.14003v1  

#### Abstract
The large artificial intelligence models (LAMs) show strong capabilities in perception, reasoning, and multi-modal understanding, and can enable advanced capabilities in low-altitude edge intelligence. However, the deployment of LAMs at the edge remains constrained by some fundamental limitations. F...

---

### 28. [LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation](https://arxiv.org/abs/2602.14054)

**Authors**: Jizheng Chen, Weiming Zhang, Xinyi Dai, Weiwen Liu, Kounianhua Du, Yasheng Wang, Ruiming Tang, Yong Yu, Weinan Zhang  
**Category**: cs.CL  
**Published**: 2026-02-17  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.14054v1  

#### Abstract
Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges: (1) underthinking, where reasoning chains tend t...

---

### 29. [Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation](https://arxiv.org/abs/2602.15013)

**Authors**: Ruoxi Liu, Philipp Koehn  
**Category**: cs.CL  
**Published**: 2026-02-17  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.15013v1  

#### Abstract
This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingua...

---

### 30. [Floe: Federated Specialization for Real-Time LLM-SLM Inference](https://arxiv.org/abs/2602.14302)

**Authors**: Chunlin Tian, Kahou Tam, Yebo Wu, Shuaihang Zhong, Li Li, Nicholas D. Lane, Chengzhong Xu  
**Category**: cs.DC  
**Published**: 2026-02-17  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.14302v1  

#### Abstract
Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based b...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
