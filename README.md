# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-09-30 12:53:55 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM](https://arxiv.org/abs/2509.22832)

**Authors**: Biyao Zhang, Mingkai Zheng, Debargha Ganguly, Xuecen Zhang, Vikash Singh, Vipin Chaudhary, Zhao Zhang  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 13.5

arXiv:2509.22832v1 Announce Type: cross 
Abstract: Training Large Language Models(LLMs) is one of the most compute-intensive tasks in high-performance computing. Predicting end-to-end training time for multi-billion parameter models distributed across hundreds of GPUs remains challenging due to comp...

---

### 2. [GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference](https://arxiv.org/abs/2509.25041)

**Authors**: Yu Han, Lehan Pan, Jie Peng, Ziyang Tao, Wuyang Zhang, Yanyong Zhang  
**Category**: cs.DC  
**Published**: 2025-09-30  
**Score**: 13.0

arXiv:2509.25041v1 Announce Type: new 
Abstract: Sparse Mixture of Experts (SMoE) performs conditional computation by selectively activating a subset of experts, thereby enabling scalable parameter growth in large language models (LLMs). However, the expanded parameter scale exceeds the memory capac...

---

### 3. [READER: Retrieval-Assisted Drafter for Efficient LLM Inference](https://arxiv.org/abs/2508.09072)

**Authors**: Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Stanislav Ilyushin, Sultan Isali, Vasily Kalugin, Nuriza Aitassova, Fei Yi, Weidi Zeng  
**Category**: cs.CL  
**Published**: 2025-09-30  
**Score**: 12.5

arXiv:2508.09072v2 Announce Type: replace 
Abstract: Autoregressive Language Models instantiate a factorized likelihood over token sequences, yet their strictly sequential decoding process imposes an intrinsic lower bound on inference latency. This bottleneck has emerged as a central obstacle to the...

---

### 4. [SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving](https://arxiv.org/abs/2509.24626)

**Authors**: Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng  
**Category**: cs.DC  
**Published**: 2025-09-30  
**Score**: 12.0

arXiv:2509.24626v1 Announce Type: new 
Abstract: Serving long-context LLMs is costly because attention computation grows linearly with context length. Dynamic sparse attention algorithms (DSAs) mitigate this by attending only to the key-value (KV) cache of critical tokens. However, with DSAs, the ma...

---

### 5. [Sparsity Forcing: Reinforcing Token Sparsity of MLLMs](https://arxiv.org/abs/2504.18579)

**Authors**: Feng Chen, Yefei He, Lequan Lin, Chenhui Gou, Jing Liu, Bohan Zhuang, Qi Wu  
**Category**: cs.LG  
**Published**: 2025-09-30  
**Score**: 11.5

arXiv:2504.18579v3 Announce Type: replace 
Abstract: Sparse attention mechanisms aim to reduce computational overhead with minimal accuracy loss by selectively processing salient tokens. Despite their effectiveness, most methods merely exploit a model's inherent sparsity and thus plateau at moderate...

---

### 6. [HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.23928)

**Authors**: Zhinan Xie, Peisong Wang, Jian Cheng  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 11.0

arXiv:2509.23928v1 Announce Type: cross 
Abstract: Speculative decoding is an effective approach for accelerating inference in Large Language models (LLMs), but its adaptation to Vision-Language models (VLMs) remains challenging for additional visual tokens in multimodal inputs. First, owing to the ...

---

### 7. [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)

**Authors**: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 11.0

arXiv:2505.20776v3 Announce Type: replace-cross 
Abstract: Speculative decoding is a widely used technique for accelerating inference in large language models (LLMs), but its performance degrades as input length grows, with significant drops even at moderate lengths. Yet, this early degradation has ...

---

### 8. [Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training](https://arxiv.org/abs/2509.21841)

**Authors**: Chang Chen, Tiancheng Chen, Jiangfei Duan, Qianchao Zhu, Zerui Wang, Qinghao Hu, Peng Sun, Xiuhong Li, Chao Yang, Torsten Hoefler  
**Category**: cs.DC  
**Published**: 2025-09-30  
**Score**: 11.0

arXiv:2509.21841v2 Announce Type: replace 
Abstract: Training large language models (LLMs) with increasingly long and varying sequence lengths introduces severe load imbalance challenges in large-scale data-parallel training. Recent frameworks attempt to mitigate these issues through data reorganiza...

---

### 9. [Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization](https://arxiv.org/abs/2509.23202)

**Authors**: Vage Egiazarian, Roberto L. Castro, Denis Kuznedelev, Andrei Panferov, Eldar Kurtic, Shubhra Pandit, Alexandre Marques, Mark Kurtz, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh  
**Category**: cs.LG  
**Published**: 2025-09-30  
**Score**: 11.0

arXiv:2509.23202v1 Announce Type: new 
Abstract: The recent hardware-accelerated microscaling 4-bit floating-point formats such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to revolutionize large language model (LLM) inference. Yet, their practical benefits remain unproven. We prese...

---

### 10. [Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation](https://arxiv.org/abs/2509.23866)

**Authors**: Pengxiang Li, Zechen Hu, Zirui Shang, Jingrong Wu, Yang Liu, Hui Liu, Zhi Gao, Chenrui Shi, Bofei Zhang, Zihao Zhang, Xiaochuan Shi, Zedong YU, Yuwei Wu, Xinxiao Wu, Yunde Jia, Liuyu Xiang, Zhaofeng He, Qing Li  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 10.5

arXiv:2509.23866v1 Announce Type: cross 
Abstract: Vision-language model (VLM) based GUI agents show promise for automating complex desktop and mobile tasks, but face significant challenges in applying reinforcement learning (RL): (1) slow multi-turn interactions with GUI environments for policy rol...

---

### 11. [Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime](https://arxiv.org/abs/2506.24120)

**Authors**: Yuqing Wang, Shangding Gu  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 10.5

arXiv:2506.24120v2 Announce Type: replace-cross 
Abstract: Data selection plays a crucial role in data-driven decision-making, including in large language models (LLMs), and is typically task-dependent. Properties such as data quality and diversity have been extensively studied and are known to enha...

---

### 12. [HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous Clusters](https://arxiv.org/abs/2509.24859)

**Authors**: Antian Liang, Zhigang Zhao, Kai Zhang, Xuri Shi, Chuantao Li, Chunxiao Wang, Zhenying He, Yinan Jing, X. Sean Wang  
**Category**: cs.DC  
**Published**: 2025-09-30  
**Score**: 10.5

arXiv:2509.24859v1 Announce Type: new 
Abstract: With the rapid evolution of GPU architectures, the heterogeneity of model training infrastructures is steadily increasing. In such environments, effectively utilizing all available heterogeneous accelerators becomes critical for distributed model trai...

---

### 13. [A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration](https://arxiv.org/abs/2509.25044)

**Authors**: Rohit Jena, Vedant Zope, Pratik Chaudhari, James C. Gee  
**Category**: cs.DC  
**Published**: 2025-09-30  
**Score**: 10.5

arXiv:2509.25044v1 Announce Type: cross 
Abstract: In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels supplemented with a distributed framework for image registration at unprecedented scales. Image registration is an inverse problem fundamental to biomedical and life sciences, b...

---

### 14. [Disaggregated Prefill and Decoding Inference System for Large Language Model Serving on Multi-Vendor GPUs](https://arxiv.org/abs/2509.17542)

**Authors**: Xing Chen, Rong Shi, Lu Zhao, Lingbin Wang, Xiao Jin, Yueqiang Chen, Hongfeng Sun  
**Category**: cs.DC  
**Published**: 2025-09-30  
**Score**: 10.5

arXiv:2509.17542v2 Announce Type: replace 
Abstract: LLM-based applications have been widely used in various industries, but with the increasing of models size, an efficient large language model (LLM) inference system is an urgent problem to be solved for service providers. Since the inference syste...

---

### 15. [Muon: Training and Trade-offs with Latent Attention and MoE](https://arxiv.org/abs/2509.24406)

**Authors**: Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat  
**Category**: cs.LG  
**Published**: 2025-09-30  
**Score**: 10.5

arXiv:2509.24406v1 Announce Type: new 
Abstract: We present a comprehensive theoretical and empirical study of the Muon optimizer for training transformers only with a small to medium decoder (30M - 200M parameters), with an emphasis on its mathematical foundations, convergence properties and synerg...

---

### 16. [BiHDTrans: binary hyperdimensional transformer for efficient multivariate time series classification](https://arxiv.org/abs/2509.24425)

**Authors**: Jingtao Zhang, Yi Liu, Qi Shen, Changhong Wang  
**Category**: cs.LG  
**Published**: 2025-09-30  
**Score**: 10.5

arXiv:2509.24425v1 Announce Type: new 
Abstract: The proliferation of Internet-of-Things (IoT) devices has led to an unprecedented volume of multivariate time series (MTS) data, requiring efficient and accurate processing for timely decision-making in resource-constrained edge environments. Hyperdim...

---

### 17. [AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models](https://arxiv.org/abs/2509.23722)

**Authors**: Jihu Guo, Tenghui Ma, Wei Gao, Peng Sun, Jiaxing Li, Xun Chen, Yuyang Jin, Dahua Lin  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 10.0

arXiv:2509.23722v1 Announce Type: cross 
Abstract: Pipeline parallelism is widely used to train large language models (LLMs). However, increasing heterogeneity in model architectures exacerbates pipeline bubbles, thereby reducing training efficiency. Existing approaches overlook the co-optimization ...

---

### 18. [EOE: Evolutionary Optimization of Experts for Training Language Models](https://arxiv.org/abs/2509.24436)

**Authors**: Yingshi Chen  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 10.0

arXiv:2509.24436v1 Announce Type: cross 
Abstract: This paper presents an evolutionary framework for the training of large language models(LLM). The models are divided into several experts(sub-networks), which have the same structure but different parameter values. Only one expert is trained at each...

---

### 19. [RServe: Overlapping Encoding and Prefill for Efficient LMM Inference](https://arxiv.org/abs/2509.24381)

**Authors**: Tianyu Guo, Tianming Xu, Xianjie Chen, Junru Chen, Nong Xiao, Xianwei Zhang  
**Category**: cs.DC  
**Published**: 2025-09-30  
**Score**: 10.0

arXiv:2509.24381v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) typically employ an encoding module to transform multimodal data inputs into embeddings, which are then fed to language models for further processing. However, efficiently serving LMMs remains highly challenging due to t...

---

### 20. [Hybrid Layer-Wise ANN-SNN With Surrogate Spike Encoding-Decoding Structure](https://arxiv.org/abs/2509.24411)

**Authors**: Nhan T. Luu, Duong T. Luu, Pham Ngoc Nam, Truong Cong Thang  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 9.5

arXiv:2509.24411v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) have gained significant traction in both computational neuroscience and artificial intelligence for their potential in energy-efficient computing. In contrast, artificial neural networks (ANNs) excel at gradient-based ...

---

### 21. [BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation](https://arxiv.org/abs/2508.10774)

**Authors**: Youping Gu, Xiaolong Li, Yuhao Hu, Minqi Chen, Bohan Zhuang  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 9.5

arXiv:2508.10774v2 Announce Type: replace-cross 
Abstract: Diffusion Transformers currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both st...

---

### 22. [Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed](https://arxiv.org/abs/2509.23241)

**Authors**: Ankita Dutta, Nabendu Chaki, Rajat K. De  
**Category**: cs.DC  
**Published**: 2025-09-30  
**Score**: 9.5

arXiv:2509.23241v1 Announce Type: new 
Abstract: High resource requirement for Deep Neural Network (DNN) training across multiple GPUs necessitates development of various parallelism techniques. In this paper, we introduce two interconnected DNN training frameworks, namely, V-TiMePReSt and I-TiMePRe...

---

### 23. [StarTrail: Concentric Ring Sequence Parallelism for Efficient Near-Infinite-Context Transformer Model Training](https://arxiv.org/abs/2407.00611)

**Authors**: Ziming Liu, Shaoyu Wang, Shenggan Cheng, Zhongkai Zhao, Kai Wang, Xuanlei Zhao, James Demmel, Yang You  
**Category**: cs.DC  
**Published**: 2025-09-30  
**Score**: 9.5

arXiv:2407.00611v4 Announce Type: replace 
Abstract: Training Transformer models on long sequences in a distributed setting poses significant challenges in terms of efficiency and scalability. Current methods are either constrained by the number of attention heads or excessive communication overhead...

---

### 24. [JAX-MPM: A Learning-Augmented Differentiable Meshfree Framework for GPU-Accelerated Lagrangian Simulation and Geophysical Inverse Modeling](https://arxiv.org/abs/2507.04192)

**Authors**: Honghui Du, QiZhi He  
**Category**: cs.LG  
**Published**: 2025-09-30  
**Score**: 9.5

arXiv:2507.04192v2 Announce Type: replace 
Abstract: Differentiable programming has emerged as a powerful paradigm in scientific computing, enabling automatic differentiation through simulation pipelines and naturally supporting both forward and inverse modeling. We present JAX-MPM, a general-purpos...

---

### 25. [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generation](https://arxiv.org/abs/2509.19128)

**Authors**: Alexandre Pich\'e, Ehsan Kamalloo, Rafael Pardinas, Xiaoyin Chen, Dzmitry Bahdanau  
**Category**: cs.LG  
**Published**: 2025-09-30  
**Score**: 9.5

arXiv:2509.19128v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs). However, effectively scaling these RL methods presents significant challenges, primarily due to the difficulty in maintaini...

---

### 26. [Enhancing Polyp Segmentation via Encoder Attention and Dynamic Kernel Update](https://arxiv.org/abs/2509.23502)

**Authors**: Fatemeh Salahi Chashmi, Roya Sotoudeh  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 9.0

arXiv:2509.23502v1 Announce Type: cross 
Abstract: Polyp segmentation is a critical step in colorectal cancer detection, yet it remains challenging due to the diverse shapes, sizes, and low contrast boundaries of polyps in medical imaging. In this work, we propose a novel framework that improves seg...

---

### 27. [PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM](https://arxiv.org/abs/2509.24085)

**Authors**: Ju-Hyung Lee, Yanqing Lu, Klaus Doppler  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 9.0

arXiv:2509.24085v1 Announce Type: cross 
Abstract: We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a framework for cooperative cross-layer optimization in device-to-device (D2D) communication. Building on our previous work on single-device on-device LLMs, PEARL extends the paradig...

---

### 28. [Intelligent Optimization of Wireless Access Point Deployment for Communication-Based Train Control Systems Using Deep Reinforcement Learning](https://arxiv.org/abs/2509.24819)

**Authors**: Kunyu Wu, Qiushi Zhao, Zihan Feng, Yunxi Mu, Hao Qin, Xinyu Zhang, Xingqi Zhang  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 9.0

arXiv:2509.24819v1 Announce Type: cross 
Abstract: Urban railway systems increasingly rely on communication based train control (CBTC) systems, where optimal deployment of access points (APs) in tunnels is critical for robust wireless coverage. Traditional methods, such as empirical model-based opti...

---

### 29. [End-to-End On-Device Quantization-Aware Training for LLMs at Inference Cost](https://arxiv.org/abs/2509.00031)

**Authors**: Qitao Tan, Xiaoying Song, Jin Lu, Guoming Li, Jun Liu, Lingzi Hong, Caiwen Ding, Jundong Li, Xiaoming Zhai, Shaoyi Huang, Wei Niu, Geng Yuan  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 9.0

arXiv:2509.00031v2 Announce Type: replace-cross 
Abstract: Quantization is an effective technique to reduce the deployment cost of large language models (LLMs), and post-training quantization (PTQ) has been widely studied due to its efficiency. However, existing PTQ methods are limited by their inab...

---

### 30. [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)

**Authors**: Feilong Chen, Yijiang Liu, Yi Huang, Hao Wang, Miren Tian, Ya-Qi Yu, Minghui Liao, Jihao Wu  
**Category**: cs.AI  
**Published**: 2025-09-30  
**Score**: 9.0

arXiv:2509.11662v2 Announce Type: replace-cross 
Abstract: We propose MindVL, a multimodal large language model (MLLMs) trained on Ascend NPUs. The training of state-of-the-art MLLMs is often confined to a limited set of hardware platforms and relies heavily on massive, undisclosed data recipes, whi...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
