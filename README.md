# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-10-22 12:56:07 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism](https://arxiv.org/abs/2510.17896)

**Authors**: Tao Bu, Qiangang Wang, Bowen Zeng, Hanwen Sun, Yunpeng Huang, Chun Cao, Jingwei Xu  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 13.5  
**Type**: cross  
**ArXiv ID**: 2510.17896v1  

Transformer-based large language models (LLMs) have achieved remarkable success, yet their standard attention mechanism incurs quadratic computation and memory costs with respect to sequence length, posing a major bottleneck for long-context training. Prior work tackles this challenge along two dire...

---

### 2. [MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training](https://arxiv.org/abs/2510.18830)

**Authors**: Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu  
**Category**: cs.CL  
**Published**: 2025-10-22  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2510.18830v1  

The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios. Dynamic sparse attention is a promising approach for reducing ...

---

### 3. [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)

**Authors**: Ye Qiao, Zhiheng Chen, Yifan Zhang, Yian Wang, Sitao Huang  
**Category**: cs.LG  
**Published**: 2025-10-22  
**Score**: 12.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.15926v2  

With the emergence of wearable devices and other embedded systems, deploying large language models (LLMs) on edge platforms has become an urgent need. However, this is challenging because of their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, Deep...

---

### 4. [Efficient Long-context Language Model Training by Core Attention Disaggregation](https://arxiv.org/abs/2510.18121)

**Authors**: Yonghao Zhuang, Junda Chen, Bo Pang, Yi Gu, Yibo Zhu, Yimin Jiang, Ion Stoica, Eric Xing, Hao Zhang  
**Category**: cs.DC  
**Published**: 2025-10-22  
**Score**: 11.5  
**Type**: cross  
**ArXiv ID**: 2510.18121v1  

We present core attention disaggregation (CAD), a technique that improves long-context large language model training by decoupling the core attention computation, softmax(QK^T)V, from the rest of the model and executing it on a separate pool of devices. In existing systems, core attention is colocat...

---

### 5. [L-MoE: End-to-End Training of a Lightweight Mixture of Low-Rank Adaptation Experts](https://arxiv.org/abs/2510.17898)

**Authors**: Shihao Ji, Zihui Song  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 11.0  
**Type**: cross  
**ArXiv ID**: 2510.17898v1  

The Mixture of Experts (MoE) architecture enables the scaling of Large Language Models (LLMs) to trillions of parameters by activating a sparse subset of weights for each input, maintaining constant computational cost during inference. Concurrently, Low-Rank Adaptation (LoRA) has emerged as a domina...

---

### 6. [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)

**Authors**: Yuze Sun, Wentao Luo, Yanfei Xiang, Jiancheng Pan, Jiahao Li, Quan Zhang, Xiaomeng Huang  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2510.17852v1  

With the growing role of artificial intelligence in climate and weather research, efficient model training and inference are in high demand. Current models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware independence, especially for Chinese domestic hardware and frameworks. To...

---

### 7. [Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs](https://arxiv.org/abs/2510.18245)

**Authors**: Song Bian, Tao Yu, Shivaram Venkataraman, Youngsuk Park  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2510.18245v1  

Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance...

---

### 8. [C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression](https://arxiv.org/abs/2510.18636)

**Authors**: Baptiste Bauvin, Lo\"ic Baret, Ola Ahmad  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2510.18636v1  

Neural network compression has gained increasing attention in recent years, particularly in computer vision applications, where the need for model reduction is crucial for overcoming deployment constraints. Pruning is a widely used technique that prompts sparsity in model structures, e.g. weights, n...

---

### 9. [HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs](https://arxiv.org/abs/2509.23967)

**Authors**: Ken Deng, Zizheng Zhan, Wen Xiang, Wenqiang Zhu, Weihao Li, Jingxuan Xu, Tianhao Peng, Xinping Lei, Kun Wu, Yifan Yao, Haoyang Huang, Huaixi Tang, Kepeng Lei, Zhiyi Lai, Songwei Yu, Zongxian Feng, Zuchen Gao, Weihao Xie, Chenchen Zhang, Yanan Wu, Yuanxing Zhang, Lecheng Huang, Yuqun Zhang, Jie Liu, Zhaoxiang Zhang, Haotian Zhang, Bin Chen, Jiaheng Liu  
**Category**: cs.CL  
**Published**: 2025-10-22  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2509.23967v2  

Large Language Models (LLMs) increasingly rely on Chain-of-Thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimizati...

---

### 10. [Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation](https://arxiv.org/abs/2510.18152)

**Authors**: Zhuoyu Yao, Yue Wang, Songyang Zhang, Yingshu Li, Zhipeng Cai, Zhi Tian  
**Category**: cs.DC  
**Published**: 2025-10-22  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2510.18152v1  

Recent advances in distributed learning systems have introduced effective solutions for implementing collaborative artificial intelligence techniques in wireless communication networks. Federated learning approaches provide a model-aggregation mechanism among edge devices to achieve collaborative tr...

---

### 11. [T\'yr-the-Pruner: Structural Pruning LLMs via Global Sparsity Distribution Optimization](https://arxiv.org/abs/2503.09657)

**Authors**: Guanchen Li, Yixing Xu, Zeping Li, Ji Liu, Xuanwu Yin, Dong Li, Emad Barsoum  
**Category**: cs.LG  
**Published**: 2025-10-22  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2503.09657v4  

Structural pruning enhances hardware-agnostic inference efficiency for large language models (LLMs) yet often fails to maintain comparable performance. Local pruning performs efficient layer-by-layer compression but ignores global topology. Although global pruning aims to identify an optimal sparse ...

---

### 12. [FlexQuant: A Flexible and Efficient Dynamic Precision Switching Framework for LLM Quantization](https://arxiv.org/abs/2506.12024)

**Authors**: Fangxin Liu, Zongwu Wang, JinHong Xia, Junping Zhao, Shouren Zhao, Jinjin Li, Jian Liu, Li Jiang, Haibing Guan  
**Category**: cs.LG  
**Published**: 2025-10-22  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2506.12024v3  

The rapid advancement of large language models (LLMs) has exacerbated the memory bottleneck due to the widening gap between model parameter scaling and hardware capabilities. While post-training quantization techniques effectively reduce memory overhead, existing methods predominantly rely on static...

---

### 13. [Reasoning Language Model Inference Serving Unveiled: An Empirical Study](https://arxiv.org/abs/2510.18672)

**Authors**: Qi Li, Junpan Wu, Xiang Liu, Yuxin Wang, Zeyu Li, Zhenheng Tang, Yuhan Chen, Shaohuai Shi, Xiaowen Chu  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2510.18672v1  

The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-...

---

### 14. [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)

**Authors**: Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, Peixian Chen, Mengdan Zhang, Hang Shao, Jian Li, Jinlong Peng, Haoyu Cao, Ke Li, Rongrong Ji, Xing Sun  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.03739v2  

With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during stre...

---

### 15. [Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle](https://arxiv.org/abs/2508.05612)

**Authors**: Linghao Zhu, Yiran Guan, Dingkang Liang, Jianzhong Ju, Zhenbo Luo, Bin Qin, Jian Luan, Yuliang Liu, Xiang Bai  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2508.05612v3  

Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where m...

---

### 16. [Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving](https://arxiv.org/abs/2509.02718)

**Authors**: Fangzhou Wu, Sandeep Silwal  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2509.02718v2  

Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. LLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features. However, existing works primarily focus on offline sc...

---

### 17. [DRL-Based Resource Allocation for Energy-Efficient IRS-Assisted UAV Spectrum Sharing Systems](https://arxiv.org/abs/2510.17877)

**Authors**: Yiheng Wang  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.17877v1  

Intelligent reflecting surface (IRS) assisted unmanned aerial vehicle (UAV) systems provide a new paradigm for reconfigurable and flexible wireless communications. To enable more energy efficient and spectrum efficient IRS assisted UAV wireless communications, this paper introduces a novel IRS-assis...

---

### 18. [POPI: Personalizing LLMs via Optimized Natural Language Preference Inference](https://arxiv.org/abs/2510.17881)

**Authors**: Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.17881v1  

Large language models (LLMs) achieve strong benchmark performance, yet user experiences remain inconsistent due to diverse preferences in style, tone, and reasoning mode. Nevertheless, existing alignment techniques such as reinforcement learning from human feedback (RLHF) or Direct Preference Optimi...

---

### 19. [Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2510.18413)

**Authors**: Siyuan Yan, Guo-Qing Jiang, Yuchen Zhang, Xiaoxing Ma, Ran Zhu, Chun Cao, Jingwei Xu  
**Category**: cs.CL  
**Published**: 2025-10-22  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2510.18413v1  

Large language models (LLMs) now support context windows of hundreds of thousands to millions of tokens, enabling applications such as long-document summarization, large-scale code synthesis, multi-document question answering and persistent multi-turn dialogue. However, such extended contexts exacer...

---

### 20. [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)

**Authors**: Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Ruihua Song  
**Category**: cs.CL  
**Published**: 2025-10-22  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2505.16552v5  

Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses rea...

---

### 21. [Learning under Quantization for High-Dimensional Linear Regression](https://arxiv.org/abs/2510.18259)

**Authors**: Dechen Zhang, Junwei Su, Difan Zou  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.18259v1  

The use of low-bit quantization has emerged as an indispensable technique for enabling the efficient training of large-scale models. Despite its widespread empirical success, a rigorous theoretical understanding of its impact on learning performance remains notably absent, even in the simplest linea...

---

### 22. [StreamingTOM: Streaming Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.18269)

**Authors**: Xueyi Chen, Keda Tao, Kele Shao, Huan Wang  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.18269v1  

Unlike offline processing, streaming video vision-language models face two fundamental constraints: causality and accumulation. Causality prevents access to future frames that offline methods exploit, while accumulation causes tokens to grow unbounded, creating efficiency bottlenecks. However, exist...

---

### 23. [ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/abs/2508.14040)

**Authors**: Hanyu Lai, Xiao Liu, Yanxiao Zhao, Han Xu, Hanchen Zhang, Bohao Jing, Yanyu Ren, Shuntian Yao, Yuxiao Dong, Jie Tang  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2508.14040v2  

We introduce ComputerRL, a framework for autonomous desktop intelligence that enables agents to operate complex digital workspaces skillfully. ComputerRL features the API-GUI paradigm, which unifies programmatic API calls and direct GUI interaction to address the inherent mismatch between machine ag...

---

### 24. [Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning](https://arxiv.org/abs/2509.06239)

**Authors**: Manvi Jha, Jiaxin Wan, Deming Chen  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2509.06239v2  

Large Language Models (LLMs) have demonstrated impressive capabilities in automated code generation but frequently produce code that fails formal verification, an essential requirement for hardware and safety-critical domains. To overcome this fundamental limitation, we previously proposed PREFACE, ...

---

### 25. [Changing Base Without Losing Pace: A GPU-Efficient Alternative to MatMul in DNNs](https://arxiv.org/abs/2503.12211)

**Authors**: Nir Ailon, Akhiad Bercovich, Yahel Uffenheimer, Omri Weinstein  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2503.12211v3  

Modern AI relies on huge matrix multiplications (MatMuls), whose computation poses a scalability problem for inference and training. We propose an alternative, GPU native bilinear operator to MatMuls in neural networks, which offers a three-way tradeoff between: speed, accuracy and parameter count. ...

---

### 26. [VLLFL: A Vision-Language Model Based Lightweight Federated Learning Framework for Smart Agriculture](https://arxiv.org/abs/2504.13365)

**Authors**: Long Li, Jiajia Li, Dong Chen, Lina Pu, Haibo Yao, Yanbo Huang  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2504.13365v2  

In modern smart agriculture, object detection plays a crucial role by enabling automation, precision farming, and monitoring of resources. From identifying crop health and pest infestations to optimizing harvesting processes, accurate object detection enhances both productivity and sustainability. H...

---

### 27. [MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization](https://arxiv.org/abs/2505.17745)

**Authors**: Zeyuan Ma, Yue-Jiao Gong, Hongshu Guo, Wenjie Qiu, Sijie Ma, Hongqiao Lian, Jiajun Zhan, Kaixu Chen, Chen Wang, Zhiyang Huang, Zechuan Huang, Guojun Peng, Ran Cheng, Yining Ma  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.17745v2  

Meta-Black-Box Optimization (MetaBBO) streamlines the automation of optimization algorithm design through meta-learning. It typically employs a bi-level structure: the meta-level policy undergoes meta-training to reduce the manual effort required in developing algorithms for low-level optimization t...

---

### 28. [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling](https://arxiv.org/abs/2505.19187)

**Authors**: Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, Pengfei Liu  
**Category**: cs.AI  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.19187v2  

Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose eleme...

---

### 29. [Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs](https://arxiv.org/abs/2510.18225)

**Authors**: Xueyao Zhang, Bo Yang, Zhiwen Yu, Xuelin Cao, Wei Xiang, Bin Guo, Liang Wang, Billy Pik Lik Lau, George C. Alexandropoulos, Jun Luo, M\'erouane Debbah, Zhu Han, Chau Yuen  
**Category**: cs.LG  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.18225v1  

This paper investigates underwater cooperative target detection using autonomous underwater vehicles (AUVs), with a focus on the critical trade-off between cooperation efficiency and communication covertness. To tackle this challenge, we first formulate a joint trajectory and power control optimizat...

---

### 30. [Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations](https://arxiv.org/abs/2510.18228)

**Authors**: Zhendong Mi, Qitao Tan, Grace Li Zhang, Zhaozhuo Xu, Geng Yuan, Shaoyi Huang  
**Category**: cs.LG  
**Published**: 2025-10-22  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.18228v1  

Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization has emerged as a promising alternative to traditional gradient-based methods due to its reduced memory footprint requirement. However, existing ZO methods suffer from high variance in gradient estimation, leading to slow c...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
