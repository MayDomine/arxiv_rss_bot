# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-09-29 12:53:44 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [Prima.cpp: Fast 30-70B LLM Inference on Heterogeneous and Low-Resource Home Clusters](https://arxiv.org/abs/2504.08791)

**Authors**: Zonghang Li, Tao Li, Wenjiao Feng, Rongxing Xiao, Jianshu She, Hong Huang, Mohsen Guizani, Hongfang Yu, Qirong Ho, Wei Xiang, Steve Liu  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 16.0

arXiv:2504.08791v2 Announce Type: replace-cross 
Abstract: On-device inference offers privacy, offline use, and instant response, but consumer hardware restricts large language models (LLMs) to low throughput and capability. To overcome this challenge, we present prima.cpp, a distributed on-device i...

---

### 2. [Sparsity Forcing: Reinforcing Token Sparsity of MLLMs](https://arxiv.org/abs/2504.18579)

**Authors**: Feng Chen, Yefei He, Lequan Lin, Chenhui Gou, Jing Liu, Bohan Zhuang, Qi Wu  
**Category**: cs.LG  
**Published**: 2025-09-29  
**Score**: 11.5

arXiv:2504.18579v2 Announce Type: replace 
Abstract: Sparse attention mechanisms aim to reduce computational overhead with minimal accuracy loss by selectively processing salient tokens. Despite their effectiveness, most methods merely exploit a model's inherent sparsity and thus plateau at moderate...

---

### 3. [Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training](https://arxiv.org/abs/2509.21841)

**Authors**: Chang Chen, Tiancheng Chen, Jiangfei Duan, Qianchao Zhu, Zerui Wang, Qinghao Hu, Peng Sun, Xiuhong Li, Chao Yang, Torsten Hoefler  
**Category**: cs.DC  
**Published**: 2025-09-29  
**Score**: 11.0

arXiv:2509.21841v1 Announce Type: new 
Abstract: Training large language models (LLMs) with increasingly long and varying sequence lengths introduces severe load imbalance challenges in large-scale data-parallel training. Recent frameworks attempt to mitigate these issues through data reorganization...

---

### 4. [SecP-Tuning: Efficient Privacy-Preserving Prompt Tuning for Large Language Models via MPC](https://arxiv.org/abs/2506.15307)

**Authors**: Jinglong Luo, Zhuo Zhang, Yehong Zhang, Shiyu Liu, Ye Dong, Hui Wang, Yue Yu, Xun Zhou, Zenglin Xu  
**Category**: cs.LG  
**Published**: 2025-09-29  
**Score**: 10.5

arXiv:2506.15307v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized numerous fields, yet their adaptation to specialized tasks in privacy-sensitive domains such as healthcare and finance remains constrained due to the scarcity of accessible training data caused by s...

---

### 5. [FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning](https://arxiv.org/abs/2509.21792)

**Authors**: Yizhou Zhang, Ning Lv, Teng Wang, Jisheng Dang  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 10.0

arXiv:2509.21792v1 Announce Type: cross 
Abstract: Group relative policy optimization (GRPO) has demonstrated significant potential in improving the reasoning capabilities of large language models (LLMs) via reinforcement learning. However, its practical deployment is impeded by an excessively slow ...

---

### 6. [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)

**Authors**: Shijing Hu, Jingyang Li, Zhihui Lu, Pan Zhou  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 9.5

arXiv:2509.22134v1 Announce Type: cross 
Abstract: Speculative decoding accelerates large language model (LLM) inference by letting a lightweight draft model propose multiple tokens that the target model verifies in parallel. Yet existing training objectives optimize only a single greedy draft path,...

---

### 7. [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)

**Authors**: Zhuokun Chen, Zeren Chen, Jiahao He, Lu Sheng, Mingkui Tan, Jianfei Cai, Bohan Zhuang  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 9.5

arXiv:2507.17307v4 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) enhances the problem-solving ability of large language models (LLMs) but incurs substantial inference cost due to long autoregressive trajectories. Existing acceleration strategies either shorten traces via early stopp...

---

### 8. [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)

**Authors**: Feng Hong, Geng Yu, Yushi Ye, Haicheng Huang, Huangjie Zheng, Ya Zhang, Yanfeng Wang, Jiangchao Yao  
**Category**: cs.CL  
**Published**: 2025-09-29  
**Score**: 9.5

arXiv:2507.18578v2 Announce Type: replace 
Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling alternative to Autoregressive models, designed for fast parallel generation. However, existing DLLMs are plagued by a severe quality-speed trade-off, where faster parallel decodi...

---

### 9. [SpecMER: Fast Protein Generation with K-mer Guided Speculative Decoding](https://arxiv.org/abs/2509.21689)

**Authors**: Thomas Walton, Darin Tsui, Aryan Musharaf, Amirali Aghazadeh  
**Category**: cs.LG  
**Published**: 2025-09-29  
**Score**: 9.5

arXiv:2509.21689v1 Announce Type: new 
Abstract: Autoregressive models have transformed protein engineering by enabling the generation of novel protein sequences beyond those found in nature. However, their sequential inference introduces significant latency, limiting their utility in high-throughpu...

---

### 10. [Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead](https://arxiv.org/abs/2509.22174)

**Authors**: Durgesh Kalwar, Mayank Baranwal, Harshad Khadilkar  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 9.0

arXiv:2509.22174v1 Announce Type: cross 
Abstract: In today's data-sensitive landscape, distributed learning emerges as a vital tool, not only fortifying privacy measures but also streamlining computational operations. This becomes especially crucial within fully decentralized infrastructures where ...

---

### 11. [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)

**Authors**: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 9.0

arXiv:2507.13681v2 Announce Type: replace-cross 
Abstract: Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and m...

---

### 12. [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)

**Authors**: Haotian Tan, Hiroki Ouchi, Sakriani Sakti  
**Category**: cs.CL  
**Published**: 2025-09-29  
**Score**: 8.5

arXiv:2509.21932v1 Announce Type: new 
Abstract: How to make human-interpreter-like read/write decisions for simultaneous speech translation (SimulST) systems? Current state-of-the-art systems formulate SimulST as a multi-turn dialogue task, requiring specialized interleaved training data and relyin...

---

### 13. [Boosting LLM Serving through Spatial-Temporal GPU Resource Sharing](https://arxiv.org/abs/2504.19516)

**Authors**: Zejia Lin, Hongxin Xu, Guanyi Chen, Zhiguang Chen, Yutong Lu, Xianwei Zhang  
**Category**: cs.DC  
**Published**: 2025-09-29  
**Score**: 8.5

arXiv:2504.19516v4 Announce Type: replace 
Abstract: Modern LLM serving systems confront inefficient GPU utilization due to the fundamental mismatch between compute-intensive prefill and memory-bound decode phases. While current practices attempt to address this by organizing these phases into hybri...

---

### 14. [GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments](https://arxiv.org/abs/2502.01778)

**Authors**: Stavros Orfanoudakis, Nanda Kishor Panda, Peter Palensky, Pedro P. Vergara  
**Category**: cs.LG  
**Published**: 2025-09-29  
**Score**: 8.5

arXiv:2502.01778v3 Announce Type: replace 
Abstract: Reinforcement Learning (RL) methods used for solving real-world optimization problems often involve dynamic state-action spaces, larger scale, and sparse rewards, leading to significant challenges in convergence, scalability, and efficient explora...

---

### 15. [CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2509.21981)

**Authors**: Zhimin Wang, Shaokang He, Duo Wu, Jinghe Wang, Linjia Kang, Jing Yu, Zhi Wang  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 8.0

arXiv:2509.21981v1 Announce Type: new 
Abstract: Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable en...

---

### 16. [Multi-Agent Path Finding via Offline RL and LLM Collaboration](https://arxiv.org/abs/2509.22130)

**Authors**: Merve Atasever, Matthew Hong, Mihir Nitin Kulkarni, Qingpei Li, Jyotirmoy V. Deshmukh  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 8.0

arXiv:2509.22130v1 Announce Type: cross 
Abstract: Multi-Agent Path Finding (MAPF) poses a significant and challenging problem critical for applications in robotics and logistics, particularly due to its combinatorial complexity and the partial observability inherent in realistic environments. Decen...

---

### 17. [Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs](https://arxiv.org/abs/2509.22166)

**Authors**: Shirin Alanova, Kristina Kazistova, Ekaterina Galaeva, Alina Kostromina, Vladimir Smirnov, Redko Dmitry, Alexey Dontsov, Maxim Zhelnin, Evgeny Burnaev, Egor Shvetsov  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 8.0

arXiv:2509.22166v1 Announce Type: cross 
Abstract: The demand for efficient large language model (LLM) inference has intensified the focus on sparsification techniques. While semi-structured (N:M) pruning is well-established for weights, its application to activation pruning remains underexplored de...

---

### 18. [Exploiting Block Coordinate Descent for Cost-Effective LLM Model Training](https://arxiv.org/abs/2506.12037)

**Authors**: Zeyu Liu, Yan Li, Yunquan Zhang, Boyang Zhang, Guoyong Jiang, Xin Zhang, Limin Xiao, Weifeng Zhang, Daning Cheng  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 8.0

arXiv:2506.12037v2 Announce Type: replace-cross 
Abstract: Training large language models typically demands extensive GPU memory and substantial financial investment, which poses a barrier for many small- to medium-sized teams. In this paper, we propose a full-parameter pre-training and fine-tuning ...

---

### 19. [DAMR: Efficient and Adaptive Context-Aware Knowledge Graph Question Answering with LLM-Guided MCTS](https://arxiv.org/abs/2508.00719)

**Authors**: Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siyang Gao, Chao Wang, Nan Yin  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 8.0

arXiv:2508.00719v4 Announce Type: replace-cross 
Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Existing methods...

---

### 20. [KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache](https://arxiv.org/abs/2509.21354)

**Authors**: Wanshun Xu, Long Zhuang  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 7.5

arXiv:2509.21354v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models promise unified robotic perception and control, yet their scalability is constrained by the quadratic cost of attention and the unbounded growth of key-value (KV) memory during long-horizon inference. While recent...

---

### 21. [Scalable In-Context Q-Learning](https://arxiv.org/abs/2506.01299)

**Authors**: Jinmei Liu, Fuhong Liu, Jianye Hao, Bo Wang, Huaxiong Li, Chunlin Chen, Zhi Wang  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 7.5

arXiv:2506.01299v2 Announce Type: replace 
Abstract: Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynam...

---

### 22. [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)

**Authors**: Gautam Siddharth Kashyap, Mark Dras, Usman Naseem  
**Category**: cs.CL  
**Published**: 2025-09-29  
**Score**: 7.5

arXiv:2509.22510v1 Announce Type: new 
Abstract: Alignment of Large Language Models (LLMs) along multiple objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe and reliable deployment. Prior work has used steering vector-small control signals injected into hidden states-to gui...

---

### 23. [LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via Momentum-Guided Perturbation Optimization](https://arxiv.org/abs/2502.14538)

**Authors**: Yupeng Chang, Chenlu Guo, Yi Chang, Yuan Wu  
**Category**: cs.CL  
**Published**: 2025-09-29  
**Score**: 7.5

arXiv:2502.14538v3 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT), particularly Low-Rank Adaptation (LoRA), adapts large language models (LLMs) by training only a small fraction of parameters. However, as the rank of the low-rank matrices used for adaptation increases, LoRA...

---

### 24. [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)

**Authors**: Giorgos Iacovides, Wuyang Zhou, Chao Li, Qibin Zhao, Danilo Mandic  
**Category**: cs.CL  
**Published**: 2025-09-29  
**Score**: 7.5

arXiv:2505.23537v2 Announce Type: replace-cross 
Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional data, yet identification of the optimal TN structures, the so called tensor network structure search (TN-SS) problem, remains a challenge. Current state-of-the-art (...

---

### 25. [Sailor: Automating Distributed Training over Dynamic, Heterogeneous, and Geo-distributed Clusters](https://arxiv.org/abs/2504.17096)

**Authors**: Foteini Strati, Zhendong Zhang, George Manos, Ixeia S\'anchez P\'eriz, Qinghao Hu, Tiancheng Chen, Berk Buzcu, Song Han, Pamela Delgado, Ana Klimovic  
**Category**: cs.DC  
**Published**: 2025-09-29  
**Score**: 7.5

arXiv:2504.17096v2 Announce Type: replace 
Abstract: The high GPU demand of ML training makes it hard to allocate large homogeneous clusters of high-end GPUs in a single availability zone. Leveraging heterogeneous GPUs available within and across zones can improve throughput at a reasonable cost. Ho...

---

### 26. [Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration](https://arxiv.org/abs/2505.17621)

**Authors**: Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, Xiangyu Zhao  
**Category**: cs.LG  
**Published**: 2025-09-29  
**Score**: 7.5

arXiv:2505.17621v4 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has emerged as a pivotal method for improving the reasoning capabilities of Large Language Models (LLMs). However, prevalent RL approaches such as Proximal Policy Optimization (PPO) and Group-Regularized Policy Optimiza...

---

### 27. [Self-Supervised Learning of Graph Representations for Network Intrusion Detection](https://arxiv.org/abs/2509.16625)

**Authors**: Lorenzo Guerra, Thomas Chapuis, Guillaume Duc, Pavlo Mozharovskyi, Van-Tam Nguyen  
**Category**: cs.LG  
**Published**: 2025-09-29  
**Score**: 7.5

arXiv:2509.16625v2 Announce Type: replace 
Abstract: Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often deco...

---

### 28. [Aligning Inductive Bias for Data-Efficient Generalization in State Space Models](https://arxiv.org/abs/2509.20789)

**Authors**: Qiyu Chen, Guozhang Chen  
**Category**: cs.LG  
**Published**: 2025-09-29  
**Score**: 7.5

arXiv:2509.20789v2 Announce Type: replace 
Abstract: The remarkable success of large-scale models is fundamentally tied to scaling laws, yet the finite nature of high-quality data presents a looming challenge. One of the next frontiers in modeling is data efficiency: the ability to learn more from l...

---

### 29. [StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](https://arxiv.org/abs/2509.22558)

**Authors**: Chenyu Zhou, Tianyi Xu, Jianghao Lin, Dongdong Ge  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 7.0

arXiv:2509.22558v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations...

---

### 30. [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)

**Authors**: Adit Jain, Brendan Rappazzo  
**Category**: cs.AI  
**Published**: 2025-09-29  
**Score**: 7.0

arXiv:2509.21482v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading approach for improving large language model (LLM) reasoning capabilities. Most current methods follow variants of Group Relative Policy Optimization, which samples multiple r...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
