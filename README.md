# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-10-20 12:53:42 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Accelerating Mobile Language Model Generation via Hybrid Context and Hardware Coordination](https://arxiv.org/abs/2510.15312)

**Authors**: Zhiyang Chen, Daliang Xu, Haiyang Shen, Mengwei Xu, Shangguang Wang, Yun Ma  
**Category**: cs.CL  
**Published**: 2025-10-20  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2510.15312v1  

Enhancing on-device large language models (LLMs) with contextual information from local data enables personalized and task-aware generation, powering use cases such as intelligent assistants and UI agents. While recent developments in neural processors have substantially improved the efficiency of p...

---

### 2. [SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware](https://arxiv.org/abs/2510.15542)

**Authors**: Ivan Kartashov, Mariia Pushkareva, Iakov Karandashev  
**Category**: cs.LG  
**Published**: 2025-10-20  
**Score**: 10.0  
**Type**: cross  
**ArXiv ID**: 2510.15542v1  

This paper introduces SpikeFit, a novel training method for Spiking Neural Networks (SNNs) that enables efficient inference on neuromorphic hardware, considering all its stringent requirements: the number of neurons and synapses that can fit on a single device, and lower bit-width representations (e...

---

### 3. [MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for Large-Scale Recommendation](https://arxiv.org/abs/2510.15286)

**Authors**: Xianyang Qi, Yuan Tian, Zhaoyu Hu, Zhirui Kuai, Chang Liu, Hongxiang Lin, Lei Wang  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2510.15286v1  

Industrial recommender systems critically depend on high-quality ranking models. However, traditional pipelines still rely on manual feature engineering and scenario-specific architectures, which hinder cross-scenario transfer and large-scale deployment. To address these challenges, we propose \text...

---

### 4. [AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent](https://arxiv.org/abs/2509.02444)

**Authors**: Jingru Fan, Yufan Dang, Jingyao Wu, Huatao Li, Runde Yang, Xiyuan Yang, Yuheng Wang, Chen Qian  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 9.5  
**Type**: replace  
**ArXiv ID**: 2509.02444v2  

With the raid evolution of large language models and multimodal models, the mobile-agent landscape has proliferated without converging on the fundamental challenges. This paper identifies four core problems that should be solved for mobile agents to deliver practical, scalable impact: (1) generaliza...

---

### 5. [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)

**Authors**: Filipe Laitenberger, Dawid Kopiczko, Cees G. M. Snoek, Yuki M. Asano  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.13876v2  

We introduce GateSkip, a simple residual-stream gating mechanism that enables token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is equipped with a sigmoid-linear gate that condenses the branch's output before it re-enters the residual stream. During inference we rank tokens by...

---

### 6. [MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production](https://arxiv.org/abs/2505.11432)

**Authors**: Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu  
**Category**: cs.DC  
**Published**: 2025-10-20  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.11432v3  

We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models. MoE emerges as a promising architecture to scale large language models (LLMs) to unprecedented sizes, thereby enhancing model performance. However, existing MoE training ...

---

### 7. [Tequila: Trapping-free Ternary Quantization for Large Language Models](https://arxiv.org/abs/2509.23809)

**Authors**: Hong Huang, Decheng Wu, Rui Cen, Guanghua Yu, Zonghang Li, Kai Liu, Jianchen Zhu, Peng Chen, Xue Liu, Dapeng Wu  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2509.23809v2  

Quantization techniques are essential for the deployment of Large Language Models (LLMs) on edge devices. However, prevailing methods often rely on mixed-precision multiplication that lacks efficient hardware support, making it not feasible. Ternary weight quantization addresses this by constraining...

---

### 8. [Deep Neural ODE Operator Networks for PDEs](https://arxiv.org/abs/2510.15651)

**Authors**: Ziqian Li, Kang Liu, Yongcun Song, Hangrui Yue, Enrique Zuazua  
**Category**: cs.LG  
**Published**: 2025-10-20  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2510.15651v1  

Operator learning has emerged as a promising paradigm for developing efficient surrogate models to solve partial differential equations (PDEs). However, existing approaches often overlook the domain knowledge inherent in the underlying PDEs and hence suffer from challenges in capturing temporal dyna...

---

### 9. [Fast and Compact Tsetlin Machine Inference on CPUs Using Instruction-Level Optimization](https://arxiv.org/abs/2510.15653)

**Authors**: Yefan Zeng, Shengyu Duan, Rishad Shafik, Alex Yakovlev  
**Category**: cs.LG  
**Published**: 2025-10-20  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2510.15653v1  

The Tsetlin Machine (TM) offers high-speed inference on resource-constrained devices such as CPUs. Its logic-driven operations naturally lend themselves to parallel execution on modern CPU architectures. Motivated by this, we propose an efficient software implementation of the TM by leveraging instr...

---

### 10. [Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles](https://arxiv.org/abs/2505.08222)

**Authors**: Matteo Gallici, Ivan Masmitja, Mario Mart\'in  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.08222v2  

Autonomous vehicles (AV) offer a cost-effective solution for scientific missions such as underwater tracking. Recently, reinforcement learning (RL) has emerged as a powerful method for controlling AVs in complex marine environments. However, scaling these techniques to a fleet--essential for multi-t...

---

### 11. [Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation](https://arxiv.org/abs/2510.11620)

**Authors**: Siheng Xiong, Ali Payani, Faramarz Fekri  
**Category**: cs.CL  
**Published**: 2025-10-20  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2510.11620v2  

Inference-time scaling enhances the reasoning ability of a language model (LM) by extending its chain-of-thought (CoT). However, existing approaches typically generate the entire reasoning chain in a single forward pass, which often leads to CoT derailment, i.e., the reasoning trajectory drifting of...

---

### 12. [Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.16606)

**Authors**: Wei Duan, Jie Lu, Junyu Xuan  
**Category**: cs.LG  
**Published**: 2025-10-20  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2509.16606v2  

In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. Whil...

---

### 13. [Changing Base Without Losing Pace: A GPU-Efficient Alternative to MatMul in DNNs](https://arxiv.org/abs/2503.12211)

**Authors**: Nir Ailon, Akhiad Bercovich, Omri Weinstein  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2503.12211v2  

Modern AI relies on huge matrix multiplications (MatMuls), whose computation poses a scalability problem for inference and training. We propose an alternative, GPU native bilinear operator to MatMuls in neural networks, which offers a three-way tradeoff between: speed, accuracy and parameter count. ...

---

### 14. [FPEdit: Robust LLM Fingerprinting through Localized Parameter Editing](https://arxiv.org/abs/2508.02092)

**Authors**: Shida Wang, Chaohu Liu, Yubo Wang, Linli Xu  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2508.02092v2  

Large language models represent significant investments in computation, data, and engineering expertise, making them extraordinarily valuable intellectual assets. Nevertheless, these AI assets remain vulnerable to unauthorized redistribution and commercial exploitation through fine-tuning or black-b...

---

### 15. [GOGH: Correlation-Guided Orchestration of GPUs in Heterogeneous Clusters](https://arxiv.org/abs/2510.15652)

**Authors**: Ahmad Raeisi, Mahdi Dolati, Sina Darabi, Sadegh Talebi, Patrick Eugster, Ahmad Khonsari  
**Category**: cs.DC  
**Published**: 2025-10-20  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.15652v1  

The growing demand for computational resources in machine learning has made efficient resource allocation a critical challenge, especially in heterogeneous hardware clusters where devices vary in capability, age, and energy efficiency. Upgrading to the latest hardware is often infeasible, making sus...

---

### 16. [Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks](https://arxiv.org/abs/2509.13053)

**Authors**: Lorenzo Pes, Bojian Yin, Sander Stuijk, Federico Corradi  
**Category**: cs.LG  
**Published**: 2025-10-20  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2509.13053v2  

Spiking Neural Networks (SNNs) provide an efficient framework for processing dynamic spatio-temporal signals and for investigating the learning principles underlying biological neural systems. A key challenge in training SNNs is to solve both spatial and temporal credit assignment. The dominant appr...

---

### 17. [TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs](https://arxiv.org/abs/2510.15545)

**Authors**: Sibo Xiao, Jinyuan Fu, Zhongle Xie, Lidan Shou  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.15545v1  

Accelerating the inference of large language models (LLMs) has been a critical challenge in generative AI. Speculative decoding (SD) substantially improves LLM inference efficiency. However, its utility is limited by a fundamental constraint: the draft and target models must share the same vocabular...

---

### 18. [SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients](https://arxiv.org/abs/2510.15830)

**Authors**: Dominik Kallusky, Vinay Rao, Vishal Nandavanam, Hao-Jun Michael Shi  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.15830v1  

The rapid development of large language models (LLMs) has driven the demand for more efficient optimization techniques. Among these, the Lookahead family of optimizers employs a two-loop framework, maintaining fast and slow sets of model weights. Multiple inner optimizer steps on the fast weights pr...

---

### 19. [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)

**Authors**: Wenjun Wang, Shuo Cai, Congkai Xie, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Hongxia Yang  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2509.22536v4  

The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation. While FP8 training offers a promising solution with significant theoretical efficiency gains, its widespread adoption has been hindered by the lack of a comprehensive, open-source training...

---

### 20. [Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive Retrieval Depth](https://arxiv.org/abs/2510.15719)

**Authors**: Helia Hashemi, Victor R\"uhle, Saravan Rajmohan  
**Category**: cs.CL  
**Published**: 2025-10-20  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2510.15719v1  

Reasoning models have gained significant attention due to their strong performance, particularly when enhanced with retrieval augmentation. However, these models often incur high computational costs, as both retrieval and reasoning tokens contribute substantially to the overall resource usage. In th...

---

### 21. [Scaling Physical Reasoning with the PHYSICS Dataset](https://arxiv.org/abs/2506.00022)

**Authors**: Shenghe Zheng, Qianjia Cheng, Junchi Yao, Mengsong Wu, Haonan He, Ning Ding, Yu Cheng, Shuyue Hu, Lei Bai, Dongzhan Zhou, Ganqu Cui, Peng Ye  
**Category**: cs.CL  
**Published**: 2025-10-20  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2506.00022v4  

Large Language Models (LLMs) have achieved remarkable progress on advanced reasoning tasks such as mathematics and coding competitions. Meanwhile, physics, despite being both reasoning-intensive and essential to real-world understanding, received limited academic and industrial attention. This paper...

---

### 22. [Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning](https://arxiv.org/abs/2510.15388)

**Authors**: Mingyang Sun, Pengxiang Ding, Weinan Zhang, Donglin Wang  
**Category**: cs.LG  
**Published**: 2025-10-20  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2510.15388v1  

While behavior cloning with flow/diffusion policies excels at learning complex skills from demonstrations, it remains vulnerable to distributional shift, and standard RL methods struggle to fine-tune these models due to their iterative inference process and the limitations of existing workarounds. I...

---

### 23. [Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model](https://arxiv.org/abs/2505.23606)

**Authors**: Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, Shuicheng Yan  
**Category**: cs.LG  
**Published**: 2025-10-20  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2505.23606v3  

Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-auto...

---

### 24. [Direct Preference Optimization with Unobserved Preference Heterogeneity: The Necessity of Ternary Preferences](https://arxiv.org/abs/2510.15716)

**Authors**: Keertana Chidambaram, Karthik Vinary Seetharaman, Vasilis Syrgkanis  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2510.15716v1  

Reinforcement Learning from Human Feedback (RLHF) has become central to aligning large language models with human values, typically by first learning a reward model from preference data which is then used to update the model with reinforcement learning. Recent alternatives such as Direct Preference ...

---

### 25. [Exemplar-Guided Planing: Enhanced LLM Agent for KGQA](https://arxiv.org/abs/2510.15283)

**Authors**: Jingao Xu, Shuoyoucheng Ma, Xin Song, Rong Jiang, Hongkui Tu, Bin Zhou  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2510.15283v1  

Large Language Models (LLMs) as interactive agents show significant promise in Knowledge Graph Question Answering (KGQA) but often struggle with the semantic gap between natural language queries and structured knowledge graph (KG) representations. This leads to suboptimal planning and inefficient ex...

---

### 26. [UNet with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning for Medical Image Segmentation](https://arxiv.org/abs/2505.15234)

**Authors**: Saqib Qamar, Mohd Fazil, Parvez Ahmad, Shakir Khan, Abu Taha Zamani  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.15234v2  

Medical image segmentation plays an important role in various clinical applications; however, existing deep learning models face trade-offs between efficiency and accuracy. Convolutional Neural Networks (CNNs) capture local details well but miss the global context, whereas transformers handle the gl...

---

### 27. [GradES: Significantly Faster Training in Transformers with Gradient-Based Early Stopping](https://arxiv.org/abs/2509.01842)

**Authors**: Qifu Wen, Xi Zeng, Zihan Zhou, Shuaijun Liu, Mehdi Hosseinzadeh, Ningxin Su, Reza Rawassizadeh  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2509.01842v3  

Early stopping monitors global validation loss and halts all parameter updates simultaneously, which is computationally costly for large transformers due to the extended time required for validation inference. We propose \textit{GradES}, a novel gradient-based early stopping approach that operates w...

---

### 28. [MSCloudCAM: Cross-Attention with Multi-Scale Context for Multispectral Cloud Segmentation](https://arxiv.org/abs/2510.10802)

**Authors**: Md Abdullah Al Mazid, Liangdong Deng, Naphtali Rishe  
**Category**: cs.AI  
**Published**: 2025-10-20  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.10802v2  

Clouds remain a critical challenge in optical satellite imagery, hindering reliable analysis for environmental monitoring, land cover mapping, and climate research. To overcome this, we propose MSCloudCAM, a Cross-Attention with Multi-Scale Context Network tailored for multispectral and multi-sensor...

---

### 29. [Antislop: A Comprehensive Framework for Identifying and Eliminating Repetitive Patterns in Language Models](https://arxiv.org/abs/2510.15061)

**Authors**: Samuel Paech, Allen Roush, Judah Goldfeder, Ravid Shwartz-Ziv  
**Category**: cs.CL  
**Published**: 2025-10-20  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2510.15061v1  

Widespread LLM adoption has introduced characteristic repetitive phraseology, termed ``slop,'' which degrades output quality and makes AI-generated text immediately recognizable. We present Antislop, a comprehensive framework providing tools to both detect and eliminate these overused patterns. Our ...

---

### 30. [Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization](https://arxiv.org/abs/2510.15165)

**Authors**: Xin Guo, Zijiu Lyu  
**Category**: cs.LG  
**Published**: 2025-10-20  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2510.15165v1  

Reinforcement Learning (RL) enables agents to learn optimal decision-making strategies through interaction with an environment, yet training from scratch on complex tasks can be highly inefficient. Transfer learning (TL), widely successful in large language models (LLMs), offers a promising directio...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
