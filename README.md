# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-09-23 12:51:02 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill](https://arxiv.org/abs/2509.17357)

**Authors**: Yunzhao Liu, Qiang Xu, Y. Charlie Hu  
**Category**: cs.DC  
**Published**: 2025-09-23  
**Score**: 12.5

arXiv:2509.17357v1 Announce Type: new 
Abstract: Efficient LLM inference is critical for real-world applications, especially within heterogeneous GPU clusters commonly found in organizations and on-premise datacenters as GPU architecture rapidly evolves. Current disaggregated prefill strategies, whi...

---

### 2. [70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float](https://arxiv.org/abs/2504.11651)

**Authors**: Tianyi Zhang, Mohsen Hariri, Shaochen Zhong, Vipin Chaudhary, Yang Sui, Xia Hu, Anshumali Shrivastava  
**Category**: cs.DC  
**Published**: 2025-09-23  
**Score**: 11.5

arXiv:2504.11651v2 Announce Type: replace-cross 
Abstract: Large-scale AI models, such as Large Language Models (LLMs) and Diffusion Models (DMs), have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynami...

---

### 3. [Shift Parallelism: Low-Latency, High-Throughput LLM Inference for Dynamic Workloads](https://arxiv.org/abs/2509.16495)

**Authors**: Mert Hidayetoglu, Aurick Qiao, Michael Wyatt, Jeff Rasley, Yuxiong He, Samyam Rajbhandari  
**Category**: cs.DC  
**Published**: 2025-09-23  
**Score**: 11.0

arXiv:2509.16495v1 Announce Type: new 
Abstract: Efficient parallelism is necessary for achieving low-latency, high-throughput inference with large language models (LLMs). Tensor parallelism (TP) is the state-of-the-art method for reducing LLM response latency, however GPU communications reduces com...

---

### 4. [Cross-Attention Speculative Decoding](https://arxiv.org/abs/2505.24544)

**Authors**: Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 10.5

arXiv:2505.24544v3 Announce Type: replace-cross 
Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly...

---

### 5. [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.11815)

**Authors**: Haiduo Huang, Fuwei Yang, Zhenhua Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 10.5

arXiv:2509.11815v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a powerful way to accelerate autoregressive large language models (LLMs), but directly porting it to vision-language models (VLMs) faces unique systems constraints: the prefill stage is dominated by visual tokens whos...

---

### 6. [Disaggregated Prefill and Decoding Inference System for Large Language Model Serving on Multi-Vendor GPUs](https://arxiv.org/abs/2509.17542)

**Authors**: Xing Chen, Rong Shi, Lu Zhao, Lingbin Wang, Xiao Jin, Yueqiang Chen, Hongfeng Sun  
**Category**: cs.DC  
**Published**: 2025-09-23  
**Score**: 10.5

arXiv:2509.17542v1 Announce Type: new 
Abstract: LLM-based applications have been widely used in various industries, but with the increasing of models size, an efficient large language model (LLM) inference system is an urgent problem to be solved for service providers. Since the inference system is...

---

### 7. [Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection](https://arxiv.org/abs/2509.13934)

**Authors**: Zhixion Chen, Jiangzhou Wang, Hyundong Shin, Arumugam Nallanathan  
**Category**: cs.LG  
**Published**: 2025-09-23  
**Score**: 10.0

arXiv:2509.13934v2 Announce Type: replace-cross 
Abstract: The deployment of unmanned aerial vehicles (UAVs) for reliable and energy-efficient data collection from spatially distributed devices holds great promise in supporting diverse Internet of Things (IoT) applications. Nevertheless, the limited...

---

### 8. [Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale MoE Serving](https://arxiv.org/abs/2509.17863)

**Authors**: Ziming Liu, Boyu Tian, Guoteng Wang, Zhen Jiang, Peng Sun, Zhenhua Han, Tian Tang, Xiaohe Hu, Yanmin Jia, Yan Zhang, He Liu, Mingjun Zhang, Yiqi Zhang, Qiaoling Chen, Shenggan Cheng, Mingyu Gao, Yang You, Siyuan Feng  
**Category**: cs.DC  
**Published**: 2025-09-23  
**Score**: 9.5

arXiv:2509.17863v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models challenge serving infrastructures with dynamic, sparse expert utilization, causing instability on conventional systems designed for dense architectures. We propose EaaS, a novel serving system to enable efficient, scala...

---

### 9. [TranSQL+: Serving Large Language Models with SQL on Low-Resource Hardware](https://arxiv.org/abs/2502.02818)

**Authors**: Wenbo Sun, Qiming Guo, Wenlu Wang, Rihan Hai  
**Category**: cs.LG  
**Published**: 2025-09-23  
**Score**: 9.5

arXiv:2502.02818v3 Announce Type: replace-cross 
Abstract: Deploying Large Language Models (LLMs) on resource-constrained devices remains challenging due to limited memory, lack of GPUs, and the complexity of existing runtimes. In this paper, we introduce TranSQL+, a template-based code generator th...

---

### 10. [LightCode: Compiling LLM Inference for Photonic-Electronic Systems](https://arxiv.org/abs/2509.16443)

**Authors**: Ryan Tomich, Zhizhen Zhong, Dirk Englund  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 9.0

arXiv:2509.16443v1 Announce Type: cross 
Abstract: The growing demand for low-latency, energy-efficient inference in large language models (LLMs) has catalyzed interest in heterogeneous architectures. While GPUs remain dominant, they are poorly suited for integration with emerging domain-specific ac...

---

### 11. [Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers](https://arxiv.org/abs/2509.17533)

**Authors**: Anastasios Fanariotis, Theofanis Orphanoudakis, Vasilis Fotopoulos  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 9.0

arXiv:2509.17533v1 Announce Type: cross 
Abstract: The deployment of machine learning (ML) models on microcontrollers (MCUs) is constrained by strict energy, latency, and memory requirements, particularly in battery-operated and real-time edge devices. While software-level optimizations such as quan...

---

### 12. [Confidence-gated training for efficient early-exit neural networks](https://arxiv.org/abs/2509.17885)

**Authors**: Saad Mokssit, Ouassim Karrakchou, Alejandro Mousist, Mounir Ghogho  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 9.0

arXiv:2509.17885v1 Announce Type: cross 
Abstract: Early-exit neural networks reduce inference cost by enabling confident predictions at intermediate layers. However, joint training often leads to gradient interference, with deeper classifiers dominating optimization. We propose Confidence-Gated Tra...

---

### 13. [An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation](https://arxiv.org/abs/2508.17007)

**Authors**: Riad Hassan, M. Rubaiyat Hossain Mondal, Sheikh Iqbal Ahamed, Fahad Mostafa, Md Mostafijur Rahman  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 9.0

arXiv:2508.17007v2 Announce Type: replace-cross 
Abstract: Proper segmentation of organs-at-risk is important for radiation therapy, surgical planning, and diagnostic decision-making in medical image analysis. While deep learning-based segmentation architectures have made significant progress, they ...

---

### 14. [Scaling Efficient LLMs](https://arxiv.org/abs/2402.14746)

**Authors**: B. N. Kausik  
**Category**: cs.CL  
**Published**: 2025-09-23  
**Score**: 9.0

arXiv:2402.14746v4 Announce Type: replace 
Abstract: Trained LLMs in the transformer architecture are typically sparse in that most of the parameters are negligible, raising questions on efficiency. Furthermore, the so called "AI scaling law" for transformers suggests that the number of parameters m...

---

### 15. [Odyssey: Adaptive Policy Selection for Resilient Distributed Training](https://arxiv.org/abs/2508.21613)

**Authors**: Yuhang Zhou, Zhibin Wang, Peng Jiang, Haoran Xia, Junhe Lu, Qianyu Jiang, Rong Gu, Hengxi Xu, Xinjing Huang, Guanghuan Fang, Zhiheng Hu, Jingyi Zhang, Yongjin Cai, Jian He, Chen Tian  
**Category**: cs.DC  
**Published**: 2025-09-23  
**Score**: 9.0

arXiv:2508.21613v3 Announce Type: replace 
Abstract: Training large language models faces frequent interruptions due to various faults, demanding robust fault-tolerance. Existing backup-free methods, such as redundant computation, dynamic parallelism, and data rerouting, each incur performance penal...

---

### 16. [Error Correction Code Transformer: From Non-Unified to Unified](https://arxiv.org/abs/2410.03364)

**Authors**: Yongli Yan, Jieao Zhu, Tianyue Zheng, Zhuo Xu, Chao Jiang, Jiaqi He, Linglong Dai  
**Category**: cs.LG  
**Published**: 2025-09-23  
**Score**: 9.0

arXiv:2410.03364v3 Announce Type: replace-cross 
Abstract: Channel coding is vital for reliable data transmission in modern wireless systems, and its significance will increase with the emergence of sixth-generation (6G) networks, which will need to support various error correction codes. However, t...

---

### 17. [MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE](https://arxiv.org/abs/2509.17238)

**Authors**: Soheil Zibakhsh, Mohammad Samragh, Kumari Nishu, Lauren Hannah, Arnav Kundu, Minsik Cho  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 8.5

arXiv:2509.17238v1 Announce Type: new 
Abstract: The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quali...

---

### 18. [Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding](https://arxiv.org/abs/2509.18085)

**Authors**: Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee, Christopher Lott, Fatih Porikli  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 8.5

arXiv:2509.18085v1 Announce Type: cross 
Abstract: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at mu...

---

### 19. [A Scalable Multi-Robot Framework for Decentralized and Asynchronous Perception-Action-Communication Loops](https://arxiv.org/abs/2309.10164)

**Authors**: Saurav Agarwal, Frederic Vatnsdal, Romina Garcia Camargo, Vijay Kumar, Alejandro Ribeiro  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 8.5

arXiv:2309.10164v2 Announce Type: replace-cross 
Abstract: Collaboration in large robot swarms to achieve a common global objective is a challenging problem in large environments due to limited sensing and communication capabilities. The robots must execute a Perception-Action-Communication (PAC) lo...

---

### 20. [PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference](https://arxiv.org/abs/2509.04467)

**Authors**: Hao Zhang, Mengsi Lyu, Zhuo Chen, Xingrun Xing, Yulong Ao, Yonghua Lin  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 8.5

arXiv:2509.04467v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However...

---

### 21. [MobiZO: Enabling Efficient LLM Fine-Tuning at the Edge via Inference Engines](https://arxiv.org/abs/2409.15520)

**Authors**: Lei Gao, Amir Ziashahabi, Yue Niu, Salman Avestimehr, Murali Annavaram  
**Category**: cs.DC  
**Published**: 2025-09-23  
**Score**: 8.5

arXiv:2409.15520v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are currently pre-trained and fine-tuned on large cloud servers. The next frontier is LLM personalization, where a foundation model can be fine-tuned with user/task-specific data. Given the sensitive nature of su...

---

### 22. [Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark](https://arxiv.org/abs/2509.17894)

**Authors**: Siu Hang Ho, Prasad Ganesan, Nguyen Duong, Daniel Schlabig  
**Category**: cs.LG  
**Published**: 2025-09-23  
**Score**: 8.5

arXiv:2509.17894v1 Announce Type: new 
Abstract: Efficient inference is a critical challenge in deep generative modeling, particularly as diffusion models grow in capacity and complexity. While increased complexity often improves accuracy, it raises compute costs, latency, and memory requirements. T...

---

### 23. [Knowledge Distillation for Variational Quantum Convolutional Neural Networks on Heterogeneous Data](https://arxiv.org/abs/2509.16699)

**Authors**: Kai Yu, Binbin Cai, Song Lin  
**Category**: cs.LG  
**Published**: 2025-09-23  
**Score**: 8.5

arXiv:2509.16699v1 Announce Type: cross 
Abstract: Distributed quantum machine learning faces significant challenges due to heterogeneous client data and variations in local model structures, which hinder global model aggregation. To address these challenges, we propose a knowledge distillation fram...

---

### 24. [PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models](https://arxiv.org/abs/2509.16989)

**Authors**: He Xiao, Runming Yang, Qingyao Yang, Wendong Xu, Zheng Li, Yupeng Su, Zhengwu Liu, Hongxia Yang, Ngai Wong  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 8.0

arXiv:2509.16989v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and model expressiveness. While existing ultra-low-bit PTQ methods rel...

---

### 25. [SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models](https://arxiv.org/abs/2509.14269)

**Authors**: Jianbin Zhang, Yulin Zhu, Wai Lun Lo, Richard Tai-Chiu Hsung, Harris Sik-Ho Tsang, Kai Zhou  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 8.0

arXiv:2509.14269v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved great success in medical question answering and clinical decision-making, promoting the efficiency and popularization of the personalized virtual doctor in society. However, the traditional fine-tun...

---

### 26. [TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation](https://arxiv.org/abs/2509.15666)

**Authors**: Yongsheng Feng, Yuetonghui Xu, Jiehui Luo, Hongjia Liu, Xiaobing Li, Feng Yu, Wei Li  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 8.0

arXiv:2509.15666v2 Announce Type: replace-cross 
Abstract: Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly ...

---

### 27. [EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs](https://arxiv.org/abs/2509.16686)

**Authors**: Zhengge Cai, Haowen Hou  
**Category**: cs.CL  
**Published**: 2025-09-23  
**Score**: 8.0

arXiv:2509.16686v1 Announce Type: new 
Abstract: Reducing the key-value (KV) cache size is a crucial step toward enabling efficient inference in large language models (LLMs), especially under latency and memory constraints. While Multi-Head Attention (MHA) offers strong representational power, it in...

---

### 28. [ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding](https://arxiv.org/abs/2509.15235)

**Authors**: Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen  
**Category**: cs.CL  
**Published**: 2025-09-23  
**Score**: 8.0

arXiv:2509.15235v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (...

---

### 29. [Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.16606)

**Authors**: Wei Duan, Jie Lu, Junyu Xuan  
**Category**: cs.LG  
**Published**: 2025-09-23  
**Score**: 8.0

arXiv:2509.16606v1 Announce Type: cross 
Abstract: In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptabi...

---

### 30. [MCTS-EP: Empowering Embodied Planning with Online Preference Optimization](https://arxiv.org/abs/2509.17116)

**Authors**: Hang Xu, Zang Yu, Yehui Tang, Pengbo Hu, Yuhao Tang, Hao Dong  
**Category**: cs.AI  
**Published**: 2025-09-23  
**Score**: 7.5

arXiv:2509.17116v1 Announce Type: new 
Abstract: This paper introduces MCTS-EP, an online learning framework that combines large language models (LLM) with Monte Carlo Tree Search (MCTS) for training embodied agents. MCTS-EP integrates three key components: MCTS-guided exploration for preference dat...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
