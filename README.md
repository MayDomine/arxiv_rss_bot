# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-20 12:54:00 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)

**Authors**: Jinwoo Park, Seunggeun Cho, Dongsu Han  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 12.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.17052v2  

Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge an...

---

### 2. [Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2511.15015)

**Authors**: Kexin Chu, Dawei Xiang, Zixu Shen, Yiwei Yang, Zecheng Liu, Wei Zhang  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 10.5  
**Type**: cross  
**ArXiv ID**: 2511.15015v1  

Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressi...

---

### 3. [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/abs/2502.13685)

**Authors**: Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2502.13685v4  

Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state...

---

### 4. [A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation](https://arxiv.org/abs/2511.15543)

**Authors**: Georgios Venianakis, Constantinos Theodoropoulos, Michail Kavousanakis  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2511.15543v1  

Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown para...

---

### 5. [MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping](https://arxiv.org/abs/2511.15690)

**Authors**: Yushi Huang, Zining Wang, Zhihang Yuan, Yifu Ding, Ruihao Gong, Jinyang Guo, Xianglong Liu, Jun Zhang  
**Category**: cs.CL  
**Published**: 2025-11-20  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2511.15690v1  

Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However,...

---

### 6. [PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants](https://arxiv.org/abs/2511.14852)

**Authors**: Mingkun Yu, Heming Zhong, Dan Huang, Yutong Lu, Jiazhi Jiang  
**Category**: cs.DC  
**Published**: 2025-11-20  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2511.14852v1  

Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this c...

---

### 7. [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)

**Authors**: Yuxuan Gu, Weimin Bai, Yifei Wang, Weijian Luo, He Sun  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2511.15190v1  

Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner...

---

### 8. [Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration](https://arxiv.org/abs/2511.14730)

**Authors**: Parya Dolatyabi, Mahdi Khodayar  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.14730v1  

Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges m...

---

### 9. [GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention](https://arxiv.org/abs/2511.14087)

**Authors**: Jun Ding, Shang Gao  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.14087v1  

Medical image segmentation underpins computer-aided diagnosis and therapy by supporting clinical diagnosis, preoperative planning, and disease monitoring. While U-Net style convolutional neural networks perform well due to their encoder-decoder structures with skip connections, they struggle to capt...

---

### 10. [A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving](https://arxiv.org/abs/2508.08343)

**Authors**: Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral  
**Category**: cs.CL  
**Published**: 2025-11-20  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2508.08343v3  

With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause req...

---

### 11. [Confidential Prompting: Privacy-preserving LLM Inference on Cloud](https://arxiv.org/abs/2409.19134)

**Authors**: Caihua Li, In Gim, Lin Zhong  
**Category**: cs.CL  
**Published**: 2025-11-20  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2409.19134v5  

This paper introduces a vision of confidential prompting: securing user prompts from an untrusted, cloud-hosted large language model (LLM) while preserving model confidentiality, output invariance, and compute efficiency. As a first step toward this vision, we present Petridish, a system built on to...

---

### 12. [Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning](https://arxiv.org/abs/2511.15175)

**Authors**: Le Tung Giang, Vu Hoang Viet, Nguyen Xuan Tung, Trinh Van Chien, Won-Joo Hwang  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.15175v1  

The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptro...

---

### 13. [DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models](https://arxiv.org/abs/2511.15669)

**Authors**: Cheng Yin, Yankai Lin, Wang Xu, Sikyuen Tam, Xiangrui Zeng, Zhiyuan Liu, Zhouping Yin  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.15669v1  

Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both...

---

### 14. [Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss](https://arxiv.org/abs/2511.15530)

**Authors**: Max Hirsch, Federico Pichi  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.15530v1  

In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively...

---

### 15. [Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts](https://arxiv.org/abs/2511.11743)

**Authors**: Sebasti\'an Andr\'es Cajas Ord\'o\~nez, Luis Fernando Torres Torres, Mackenzie J. Meni, Carlos Andr\'es Duran Paredes, Eric Arazo, Cristian Bosch, Ricardo Simon Carbajo, Yuan Lai, Leo Anthony Celi  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2511.11743v2  

Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epi...

---

### 16. [Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation](https://arxiv.org/abs/2511.14023)

**Authors**: Chiharu Hagiwara, Naoki Nonaka, Yuhta Hashimoto, Ryu Uchimido, Jun Seita  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.14023v1  

Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation requir...

---

### 17. [LSP-YOLO: A Lightweight Single-Stage Network for Sitting Posture Recognition on Embedded Devices](https://arxiv.org/abs/2511.14322)

**Authors**: Nanjun Li, Ziyue Hao, Quanqiang Wang, Xuanyin Wang  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.14322v1  

With the rise in sedentary behavior, health problems caused by poor sitting posture have drawn increasing attention. Most existing methods, whether using invasive sensors or computer vision, rely on two-stage pipelines, which result in high intrusiveness, intensive computation, and poor real-time pe...

---

### 18. [Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer](https://arxiv.org/abs/2511.14691)

**Authors**: Kallol Mondal (Department of Electronics and Communication Engineering, National Institute of Technology Allahabad, Prayagraj, Centre for Nanotechnology, Indian Institute of Technology Roorkee), Ankush Kumar (Centre for Nanotechnology, Indian Institute of Technology Roorkee)  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.14691v1  

Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training a...

---

### 19. [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)

**Authors**: Haoyang Hong, Jiajun Yin, Yuan Wang, Jingnan Liu, Zhe Chen, Ailing Yu, Ji Li, Zhiling Ye, Hansong Xiao, Yefei Chen, Hualei Zhou, Yun Yue, Minghui Yang, Chunxiao Guo, Junwei Liu, Peng Wei, Jinjie Gu  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2511.13288v2  

Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions und...

---

### 20. [Physics-Informed Neural Networks for Real-Time Gas Crossover Prediction in PEM Electrolyzers: First Application with Multi-Membrane Validation](https://arxiv.org/abs/2511.05879)

**Authors**: Yong-Woon Kim, Chulung Kang, Yung-Cheol Byun  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.05879v2  

Green hydrogen production via polymer electrolyte membrane (PEM) water electrolysis is pivotal for energy transition, yet hydrogen crossover through membranes threatens safety and economic viability-approaching explosive limits (4 mol% H$_2$ in O$_2$) while reducing Faradaic efficiency by 2.5%. Curr...

---

### 21. [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)

**Authors**: Jian-Ting Guo, Yu-Cheng Chen, Ping-Chun Hsieh, Kuo-Hao Ho, Po-Wei Huang, Ti-Rong Wu, I-Chen Wu  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.15055v1  

Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents o...

---

### 22. [Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s](https://arxiv.org/abs/2509.07928)

**Authors**: Mahmudul Islam Masum, Miad Islam  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2509.07928v2  

As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This p...

---

### 23. [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)

**Authors**: Minghu Wang (College of Computer and Cyber Security, Hebei Normal University, Hebei, China), Shuliang Zhao (College of Computer and Cyber Security, Hebei Normal University, Hebei, China), Yuanyuan Zhao (Hebei Provincial Engineering Research Center for Supply Chain Big Data Analytics and Data Security, Hebei, China), Hongxia Xu (College of Computer and Cyber Security, Hebei Normal University, Hebei, China)  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.14018v1  

The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex...

---

### 24. [HFL-FlowLLM: Large Language Models for Network Traffic Flow Classification in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.14199)

**Authors**: Jiazhuo Tian, Yachao Yuan  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.14199v1  

In modern communication networks driven by 5G and the Internet of Things (IoT), effective network traffic flow classification is crucial for Quality of Service (QoS) management and security. Traditional centralized machine learning struggles with the distributed data and privacy concerns in these he...

---

### 25. [From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)

**Authors**: Sparsh Gupta, Kamalavasan Kamalakkannan, Maxim Moraru, Galen Shipman, Patrick Diehl  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2509.12443v3  

Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to mode...

---

### 26. [IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention](https://arxiv.org/abs/2511.14515)

**Authors**: Xinxin Tang, Bin Qin, Yufang Li  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.14515v1  

Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi...

---

### 27. [Personalized Image Generation for Recommendations Beyond Catalogs](https://arxiv.org/abs/2502.18477)

**Authors**: Gabriel Patron, Zhiwei Xu, Ishan Kapnadak, Felipe Maia Polo  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2502.18477v2  

Personalization is central to human-AI interaction, yet current diffusion-based image generation systems remain largely insensitive to user diversity. Existing attempts to address this often rely on costly paired preference data or introduce latency through Large Language Models. In this work, we in...

---

### 28. [MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding](https://arxiv.org/abs/2510.25327)

**Authors**: Runxi Huang, Mingxuan Yu, Mingyu Tsoi, Xiaomin Ouyang  
**Category**: cs.AI  
**Published**: 2025-11-20  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.25327v5  

Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex int...

---

### 29. [RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling](https://arxiv.org/abs/2507.04416)

**Authors**: Xiuying Wei, Anunay Yadav, Razvan Pascanu, Caglar Gulcehre  
**Category**: cs.CL  
**Published**: 2025-11-20  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2507.04416v3  

Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representa...

---

### 30. [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)

**Authors**: Zia Badar  
**Category**: cs.LG  
**Published**: 2025-11-20  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.16088v3  

Quantization of neural networks provides benefits of inference in less compute and memory requirements. Previous work in quantization lack two important aspects which this work provides. First almost all previous work in quantization used a non-differentiable approach and for learning; the derivativ...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
