# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-14 12:53:31 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [TawPipe: Topology-Aware Weight Pipeline Parallelism for Accelerating Long-Context Large Models Training](https://arxiv.org/abs/2511.09741)

**Authors**: Houming Wu, Ling Chen  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 11.5  
**Type**: cross  
**ArXiv ID**: 2511.09741v1  

Training large language models (LLMs) is fundamentally constrained by limited device memory and costly inter-device communication. Although pipeline parallelism alleviates memory pressure by partitioning models across devices, it incurs activation communication overhead that scales linearly with seq...

---

### 2. [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)

**Authors**: Changhai Man, Joongun Park, Hanjiang Wu, Huan Xu, Srinivas Sridharan, Tushar Krishna  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 10.5  
**Type**: cross  
**ArXiv ID**: 2511.10480v1  

Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies...

---

### 3. [EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training](https://arxiv.org/abs/2511.10333)

**Authors**: Qingao Yi, Jiaang Duan, Hanwen Hu, Qin Hua, Haiyan Zhao, Shiyou Qian, Dingyu Yang, Jian Cao, Jinghua Tang, Yinghao Yu, Chenzhi Liao, Kangjin Wang, Liping Zhang  
**Category**: cs.LG  
**Published**: 2025-11-14  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2511.10333v1  

Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static grad...

---

### 4. [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)

**Authors**: Yesheng Liang, Haisheng Chen, Song Han, Zhijian Liu  
**Category**: cs.CL  
**Published**: 2025-11-14  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2511.10645v1  

Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe...

---

### 5. [LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication](https://arxiv.org/abs/2511.09557)

**Authors**: Prajwal Singhania, Siddharth Singh, Lannie Dalton Hough, Akarsh Srivastava, Harshitha Menon, Charles Fredrick Jekel, Abhinav Bhatele  
**Category**: cs.DC  
**Published**: 2025-11-14  
**Score**: 10.0  
**Type**: replace  
**ArXiv ID**: 2511.09557v2  

As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node di...

---

### 6. [Making Every Head Count: Sparse Attention Without the Speed-Performance Trade-off](https://arxiv.org/abs/2511.09596)

**Authors**: Mingkuan Zhao, Wentao Hu, Jiayin Wang, Xin Lai, Tianchen Huang, Yuheng Min, Rui Yan, Xiaoyan Zhu  
**Category**: cs.LG  
**Published**: 2025-11-14  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2511.09596v1  

The design of Large Language Models (LLMs) has long been hampered by a fundamental conflict within their core attention mechanism: its remarkable expressivity is built upon a computational complexity of $O(H \cdot N^2)$ that grows quadratically with the context size ($N$) and linearly with the numbe...

---

### 7. [Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access](https://arxiv.org/abs/2511.10291)

**Authors**: Aswin Arun, Christo Kurisummoottil Thomas, Rimalpudi Sarvendranath, Walid Saad  
**Category**: cs.LG  
**Published**: 2025-11-14  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2511.10291v1  

Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement lea...

---

### 8. [From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance](https://arxiv.org/abs/2511.09820)

**Authors**: Jeongho Min, Dongyoung Kim, Jaehyup Lee  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2511.09820v1  

Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panora...

---

### 9. [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)

**Authors**: Marco Kurzynski, Shaizeen Aga, Di Wu  
**Category**: cs.DC  
**Published**: 2025-11-14  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2511.09861v1  

GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, su...

---

### 10. [AgentEvolver: Towards Efficient Self-Evolving Agent System](https://arxiv.org/abs/2511.10395)

**Authors**: Yunpeng Zhai, Shuchang Tao, Cheng Chen, Anni Zou, Ziqian Chen, Qingxu Fu, Shinji Mai, Li Yu, Jiaji Deng, Zouying Cao, Zhaoyang Liu, Bolin Ding, Jingren Zhou  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.10395v1  

Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typica...

---

### 11. [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)

**Authors**: Lu Zhao, Rong Shi, Shaoqing Zhang, Shangchao Su, Ziqing Yin, Zhiyan Cui, Hongfeng Sun, Baoguo He, Yueqiang Chen, Liang Dong, Xiyuan Li, Lingbin Wang, Lijun Ma, Qiang Huang, Ting Liu, Chong Wang, Can Wei  
**Category**: cs.DC  
**Published**: 2025-11-14  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.09837v1  

The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strat...

---

### 12. [ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads](https://arxiv.org/abs/2511.04162)

**Authors**: Xiaokai Wang, Shaoyuan Huang, Yuting Li, Xiaofei Wang  
**Category**: cs.LG  
**Published**: 2025-11-14  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2511.04162v2  

Deep neural networks (DNNs) form the cornerstone of modern AI services, supporting a wide range of applications, including autonomous driving, chatbots, and recommendation systems. As models increase in size and complexity, DNN workloads such as training and inference tasks impose unprecedented dema...

---

### 13. [HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization](https://arxiv.org/abs/2511.09578)

**Authors**: Hadi Keramati, Morteza Sadeghi, Rajeev K. Jaiman  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.09578v1  

This study presents a generative optimization framework based on a guided denoising diffusion probabilistic model (DDPM) that leverages surrogate gradients to generate heat sink designs minimizing pressure drop while maintaining surface temperatures below a specified threshold. Geometries are repres...

---

### 14. [Fractional neural attention for efficient multiscale sequence processing](https://arxiv.org/abs/2511.10208)

**Authors**: Cheng Kevin Qu, Andrew Ly, Pulin Gong  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.10208v1  

Attention mechanisms underpin the computational power of Transformer models, which have achieved remarkable success across diverse domains. Yet understanding and extending the principles underlying self-attention remains a key challenge for advancing artificial intelligence. Drawing inspiration from...

---

### 15. [BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages](https://arxiv.org/abs/2511.10338)

**Authors**: Guduru Manoj, Neel Prabhanjan Rachamalla, Ashish Kulkarni, Gautam Rajeev, Jay Piplodiya, Arul Menezes, Shaharukh Khan, Souvik Rana, Manya Sah, Chandra Khatri, Shubham Agarwal  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.10338v1  

In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across...

---

### 16. [LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures](https://arxiv.org/abs/2511.08158)

**Authors**: Kelun Lei, Hailong Yang, Kaige Zhang, Kejie Ma, Yiqing Wang, Xin You, Yufan Xu, Enrique S. Quintana-Orti, Zhongzhi Luan, Yi Liu, Depei Qian  
**Category**: cs.DC  
**Published**: 2025-11-14  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2511.08158v2  

Sparse matrix-dense matrix multiplication (SpMM) is a critical kernel in both scientific computing and emerging graph learning workloads. The recent Armv9 architecture introduces Scalable Matrix Extension (SME), enabling tile-based matrix operations with high throughput. However, effectively exploit...

---

### 17. [URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding](https://arxiv.org/abs/2511.10552)

**Authors**: Yongxin Shi, Jiapeng Wang, Zeyu Shan, Dezhi Peng, Zening Lin, Lianwen Jin  
**Category**: cs.CL  
**Published**: 2025-11-14  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.10552v1  

Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall in...

---

### 18. [Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs](https://arxiv.org/abs/2508.02573)

**Authors**: J\'er\'emie Dentan, Davide Buscaldi, Sonia Vanier  
**Category**: cs.CL  
**Published**: 2025-11-14  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2508.02573v2  

Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on t...

---

### 19. [FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference](https://arxiv.org/abs/2511.05534)

**Authors**: Kunxi Li, Yufan Xiong, Zhonghua Jiang, Yiyun Zhou, Zhaode Wang, Chengfei Lv, Shengyu Zhang  
**Category**: cs.CL  
**Published**: 2025-11-14  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2511.05534v2  

Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in m...

---

### 20. [Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments](https://arxiv.org/abs/2411.17741)

**Authors**: Nikoleta Iliakopoulou, Jovan Stojkovic, Chloe Alverti, Tianyin Xu, Hubertus Franke, Josep Torrellas  
**Category**: cs.DC  
**Published**: 2025-11-14  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2411.17741v2  

The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods lik...

---

### 21. [Steering Pretrained Drafters during Speculative Decoding](https://arxiv.org/abs/2511.09844)

**Authors**: Fr\'ed\'eric Berdoz, Peer Rheinboldt, Roger Wattenhofer  
**Category**: cs.LG  
**Published**: 2025-11-14  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.09844v1  

Speculative decoding accelerates language model inference by separating generation into fast drafting and parallel verification. Its main limitation is drafter-verifier misalignment, which limits token acceptance and reduces overall effectiveness. While small drafting heads trained from scratch comp...

---

### 22. [Pretrained Joint Predictions for Scalable Batch Bayesian Optimization of Molecular Designs](https://arxiv.org/abs/2511.10590)

**Authors**: Miles Wang-Henderson, Ben Kaufman, Edward Williams, Ryan Pederson, Matteo Rossi, Owen Howell, Carl Underkoffler, Narbe Mardirossian, John Parkhill  
**Category**: cs.LG  
**Published**: 2025-11-14  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.10590v1  

Batched synthesis and testing of molecular designs is the key bottleneck of drug development. There has been great interest in leveraging biomolecular foundation models as surrogates to accelerate this process. In this work, we show how to obtain scalable probabilistic surrogates of binding affinity...

---

### 23. [Preconditioned Inexact Stochastic ADMM for Deep Model](https://arxiv.org/abs/2502.10784)

**Authors**: Shenglong Zhou, Ouya Wang, Ziyan Luo, Yongxu Zhu, Geoffrey Ye Li  
**Category**: cs.LG  
**Published**: 2025-11-14  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2502.10784v4  

The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent...

---

### 24. [Fine-grained Token Allocation Via Operation Pruning for Efficient MLLMs](https://arxiv.org/abs/2507.02909)

**Authors**: Aoming Liu, Reuben Tan, Boqing Gong, Bryan A. Plummer  
**Category**: cs.LG  
**Published**: 2025-11-14  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2507.02909v2  

Token reduction accelerates Multimodal Large Language Models (MLLMs) by reducing excessive tokens, but overlooks structural redundancy differences, where critical and redundant modules process identical token loads. For fine-grained computation control, we define an ``operation" as the computation f...

---

### 25. [Efficient Thought Space Exploration through Strategic Intervention](https://arxiv.org/abs/2511.10038)

**Authors**: Ziheng Li, Hengyi Cai, Xiaochi Wei, Yuchen Li, Shuaiqiang Wang, Zhi-Hong Deng, Dawei Yin  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.10038v1  

While large language models (LLMs) demonstrate emerging reasoning capabilities, current inference-time expansion methods incur prohibitive computational costs by exhaustive sampling. Through analyzing decoding trajectories, we observe that most next-token predictions align well with the golden outpu...

---

### 26. [SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning](https://arxiv.org/abs/2511.09681)

**Authors**: Tairan Huang, Yulin Jin, Junxu Liu, Qingqing Ye, Haibo Hu  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.09681v1  

Visual reinforcement learning has achieved remarkable progress in visual control and robotics, but its vulnerability to adversarial perturbations remains underexplored. Most existing black-box attacks focus on vector-based or discrete-action RL, and their effectiveness on image-based continuous cont...

---

### 27. [A General Anchor-Based Framework for Scalable Fair Clustering](https://arxiv.org/abs/2511.09889)

**Authors**: Shengfei Wei, Suyuan Liu, Jun Wang, Ke Liang, Miaomiao Li, Lei Luo  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.09889v1  

Fair clustering is crucial for mitigating bias in unsupervised learning, yet existing algorithms often suffer from quadratic or super-quadratic computational complexity, rendering them impractical for large-scale datasets. To bridge this gap, we introduce the Anchor-based Fair Clustering Framework (...

---

### 28. [MIRNet: Integrating Constrained Graph-Based Reasoning with Pre-training for Diagnostic Medical Imaging](https://arxiv.org/abs/2511.10013)

**Authors**: Shufeng Kong, Zijie Wang, Nuan Cui, Hao Tang, Yihan Meng, Yuanyuan Wei, Feifan Chen, Yingheng Wang, Zhuo Cai, Yaonan Wang, Yulong Zhang, Yuzheng Li, Zibin Zheng, Caihua Liu  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.10013v1  

Automated interpretation of medical images demands robust modeling of complex visual-semantic relationships while addressing annotation scarcity, label imbalance, and clinical plausibility constraints. We introduce MIRNet (Medical Image Reasoner Network), a novel framework that integrates self-super...

---

### 29. [Enhancing Kernel Power K-means: Scalable and Robust Clustering with Random Fourier Features and Possibilistic Method](https://arxiv.org/abs/2511.10392)

**Authors**: Yixi Chen, Weixuan Liang, Tianrui Liu, Jun-Jie Huang, Ao Li, Xueling Zhu, Xinwang Liu  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.10392v1  

Kernel power $k$-means (KPKM) leverages a family of means to mitigate local minima issues in kernel $k$-means. However, KPKM faces two key limitations: (1) the computational burden of the full kernel matrix restricts its use on extensive data, and (2) the lack of authentic centroid-sample assignment...

---

### 30. [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)

**Authors**: Sarat Chandra Bobbili, Ujwal Dinesha, Dheeraj Narasimha, Srinivas Shakkottai  
**Category**: cs.AI  
**Published**: 2025-11-14  
**Score**: 6.5  
**Type**: replace  
**ArXiv ID**: 2507.20067v2  

Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a rewar...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
