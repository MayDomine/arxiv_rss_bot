# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-21 12:55:13 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference](https://arxiv.org/abs/2509.25041)

**Authors**: Yu Han, Lehan Pan, Jie Peng, Ziyang Tao, Wuyang Zhang, Yanyong Zhang  
**Category**: cs.DC  
**Published**: 2025-10-21  
**Score**: 13.0  
**Type**: replace  
**ArXiv ID**: 2509.25041v2  

Sparse Mixture of Experts (SMoE) performs conditional computation by selectively activating a subset of experts, thereby enabling scalable parameter growth in large language models (LLMs). However, the expanded parameter scale exceeds the memory capacity of a single device, necessitating distributed...

---

### 2. [ESSA: Evolutionary Strategies for Scalable Alignment](https://arxiv.org/abs/2507.04453)

**Authors**: Daria Korotyshova, Boris Shaposhnikov, Alexey Malakhov, Alexey Khokhulin, Nikita Surnachev, Kirill Ovcharenko, George Bredis, Alexey Gorbatovski, Viacheslav Sinii, Daniil Gavrilov  
**Category**: cs.LG  
**Published**: 2025-10-21  
**Score**: 12.5  
**Type**: replace  
**ArXiv ID**: 2507.04453v2  

Alignment of Large Language Models (LLMs) typically relies on Reinforcement Learning from Human Feedback (RLHF) with gradient-based optimizers such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO). While effective, these methods require complex distributed training,...

---

### 3. [SpikingBrain: Spiking Brain-inspired Large Models](https://arxiv.org/abs/2509.05276)

**Authors**: Yuqi Pan, Yupeng Feng, Jinghao Zhuang, Siyu Ding, Han Xu, Zehao Liu, Bohan Sun, Yuhong Chou, Xuerui Qiu, Anlin Deng, Anjie Hu, Peng Zhou, Man Yao, Jibin Wu, Jian Yang, Guoliang Sun, Bo Xu, Guoqi Li  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 12.0  
**Type**: replace-cross  
**ArXiv ID**: 2509.05276v2  

Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable ...

---

### 4. [Accelerating Mobile Language Model via Speculative Decoding and NPU-Coordinated Execution](https://arxiv.org/abs/2510.15312)

**Authors**: Zhiyang Chen, Daliang Xu, Haiyang Shen, Mengwei Xu, Shangguang Wang, Yun Ma  
**Category**: cs.CL  
**Published**: 2025-10-21  
**Score**: 12.0  
**Type**: replace  
**ArXiv ID**: 2510.15312v2  

Enhancing on-device large language models (LLMs) with contextual information from local data enables personalized and task-aware generation, powering use cases such as intelligent assistants and UI agents. While recent developments in neural processors have substantially improved the efficiency of p...

---

### 5. [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)

**Authors**: Ye Qiao, Zhiheng Chen, Yifan Zhang, Yian Wang, Sitao Huang  
**Category**: cs.LG  
**Published**: 2025-10-21  
**Score**: 12.0  
**Type**: cross  
**ArXiv ID**: 2510.15926v1  

With the emergence of wearable devices and other embedded systems, deploying large language models (LLMs) on edge platforms has become an urgent need. However, this is challenging because of their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, Deep...

---

### 6. [MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)

**Authors**: Rizhen Hu, Yutong He, Ran Yan, Mou Sun, Binghang Yuan, Kun Yuan  
**Category**: cs.DC  
**Published**: 2025-10-21  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2510.16415v1  

As distributed optimization scales to meet the demands of Large Language Model (LLM) training, hardware failures become increasingly non-negligible. Existing fault-tolerant training methods often introduce significant computational or memory overhead, demanding additional resources. To address this ...

---

### 7. [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)

**Authors**: Jian Ma, Xinchen Lyu, Jun Jiang, Longhao Zou, Chenshan Ren, Qimei Cui, Xiaofeng Tao  
**Category**: cs.DC  
**Published**: 2025-10-21  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2510.16418v1  

Collaborative large language model (LLM) inference enables real-time, privacy-preserving AI services on resource-constrained edge devices by partitioning computational workloads between client devices and edge servers. However, this paradigm is severely hindered by communication bottlenecks caused b...

---

### 8. [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)

**Authors**: Mengtao Lv, Ruiqi Zhu, Xinyu Wang, Yun Li  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 10.5  
**Type**: cross  
**ArXiv ID**: 2510.16045v1  

Large language models (LLMs) have demonstrated remarkable capabilities in various kinds of tasks, while the billion or even trillion parameters bring storage and efficiency bottlenecks for inference. Quantization, particularly floating-point quantization, is known to be capable of speeding up LLM in...

---

### 9. [GRIFFIN: Effective Token Alignment for Faster Speculative Decoding](https://arxiv.org/abs/2502.11018)

**Authors**: Shijing Hu, Jingyang Li, Xingyu Xie, Zhihui Lu, Kim-Chuan Toh, Pan Zhou  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2502.11018v3  

Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a...

---

### 10. [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)

**Authors**: Shuang Cheng, Yihan Bian, Dawei Liu, Linfeng Zhang, Qian Yao, Zhongbo Tian, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, Bowen Zhou  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.06303v3  

We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies the training efficiency of autoregressive models with the parallel inference capability of diffusion. Instead of costly end-to-end diffusion training, SDAR performs a lightweight paradigm conversion that transforms a well-...

---

### 11. [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)

**Authors**: Mingyan Yang, Guanjie Wang, Manqi Luo, Yifei Liu, Chen Chen, Han Zhao, Yu Feng, Quan Chen, Minyi Guo  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2510.17015v1  

In the era of Large Language Models (LLMs), it has been popular to launch a series of LLM inferences -- we call an LLM application -- to better solve real-world problems. When serving those applications in shared GPU servers, the schedulers are expected to attain fast application completions with gu...

---

### 12. [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)

**Authors**: Lipeng Xie, Sen Huang, Zhuo Zhang, Anni Zou, Yunpeng Zhai, Dingchao Ren, Kezun Zhang, Haoyuan Hu, Boyin Liu, Haoran Chen, Zhaoyang Liu, Bolin Ding  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2510.17314v1  

Reward models are essential for aligning Large Language Models (LLMs) with human values, yet their development is hampered by costly preference datasets and poor interpretability. While recent rubric-based approaches offer transparency, they often lack systematic quality control and optimization, cr...

---

### 13. [Trainable Dynamic Mask Sparse Attention](https://arxiv.org/abs/2508.02124)

**Authors**: Jingze Shi, Yifan Wu, Yiran Peng, Bingheng Wu, Liangdong Wang, Guang Liu, Yuyu Luo  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2508.02124v5  

The increasing demand for long-context modeling in large language models (LLMs) is bottlenecked by the quadratic complexity of the standard self-attention mechanism. The community has proposed sparse attention to mitigate this issue. However, position-aware sparse attention methods rely on static sp...

---

### 14. [TemplateRL: Structured Template-Guided Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2505.15692)

**Authors**: Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Haoran Luo, Ling Yang, Huazhe Xu, Jianhua Tao  
**Category**: cs.CL  
**Published**: 2025-10-21  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2505.15692v4  

Reinforcement learning (RL) has emerged as an effective paradigm for enhancing model reasoning. However, existing RL methods like GRPO often rely on unstructured self-sampling to fit scalar rewards, often producing inefficient rollouts that fail to capture transferable problem-solving strategies. To...

---

### 15. [FlexQuant: A Flexible and Efficient Dynamic Precision Switching Framework for LLM Quantization](https://arxiv.org/abs/2506.12024)

**Authors**: Fangxin Liu, Zongwu Wang, JinHong Xia, Junping Zhao, Shouren Zhao, Jinjin Li, Jian Liu, Li Jiang, Haibing Guan  
**Category**: cs.LG  
**Published**: 2025-10-21  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2506.12024v2  

The rapid advancement of large language models (LLMs) has exacerbated the memory bottleneck due to the widening gap between model parameter scaling and hardware capabilities. While post-training quantization techniques effectively reduce memory overhead, existing methods predominantly rely on static...

---

### 16. [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)

**Authors**: Juncheng Dong, Yang Yang, Tao Liu, Yang Wang, Feng Qi, Vahid Tarokh, Kaushik Rangadurai, Shuang Yang  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2510.16996v1  

The efficiency of GPU kernels is central to the progress of modern AI, yet optimizing them remains a difficult and labor-intensive task due to complex interactions between memory hierarchies, thread scheduling, and hardware-specific characteristics. While recent advances in large language models (LL...

---

### 17. [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)

**Authors**: Melika Filvantorkaman, Maral Filvan Torkaman  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2510.16611v1  

Medical imaging plays a vital role in modern diagnostics; however, interpreting high-resolution radiological data remains time-consuming and susceptible to variability among clinicians. Traditional image processing techniques often lack the precision, robustness, and speed required for real-time cli...

---

### 18. [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)

**Authors**: Joachim Diederich  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2510.17358v1  

We present a novel framework for training large language models with continuously adjustable internal representations that span the full spectrum from localist (interpretable, rule-based) to distributed (generalizable, efficient) encodings. The key innovations are (1) a locality dial, a tunable para...

---

### 19. [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)

**Authors**: Yongshun Zhang, Zhongyi Fan, Yonghang Zhang, Zhangzikang Li, Weifeng Chen, Zhongwei Feng, Chaoyue Wang, Peng Hou, Anxiang Zeng  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2510.17519v1  

In recent years, large-scale generative models for visual content (\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable progress. However, training large-scale video generation models remains particularly challenging and resource-intensive due to cross-modal text-video alignmen...

---

### 20. [Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning](https://arxiv.org/abs/2502.03304)

**Authors**: Qitao Tan, Jun Liu, Zheng Zhan, Caiwei Ding, Yanzhi Wang, Xiaolong Ma, Jaewoo Lee, Jin Lu, Geng Yuan  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2502.03304v3  

Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes...

---

### 21. [Adaptive Policy Synchronization for Scalable Reinforcement Learning](https://arxiv.org/abs/2507.10990)

**Authors**: Rodney Lafuente-Mercado  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2507.10990v2  

Scaling reinforcement learning (RL) often requires running environments across many machines, but most frameworks tie simulation, training, and infrastructure into rigid systems. We introduce ClusterEnv, a lightweight interface for distributed environment execution that preserves the familiar Gymnas...

---

### 22. [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)

**Authors**: Yeh Keng Hao, Hsu Tzu Wei, Sun Min  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.16396v1  

With the increasing ubiquity of AR/VR devices, the deployment of deep learning models on edge devices has become a critical challenge. These devices require real-time inference, low power consumption, and minimal latency. Many framework designers face the conundrum of balancing efficiency and perfor...

---

### 23. [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)

**Authors**: Mingzheng Zhang, Jinfeng Gao, Dan Xu, Jiangrui Yu, Yuhan Qiao, Lan Chen, Jin Tang, Xiao Wang  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.16776v1  

X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens for clinicians and patient wait times. Existing MRG models predominantly rely on Large Language Models (LLMs) to improve report generation, with limited exp...

---

### 24. [TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart Sound Segmentation](https://arxiv.org/abs/2510.17346)

**Authors**: Peihong Zhang, Zhixin Li, Yuxuan Liu, Rui Sang, Yiqiang Cai, Yizhou Tan, Shengchen Li  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.17346v1  

Deep learning approaches for heart-sound (PCG) segmentation built on time--frequency features can be accurate but often rely on large expert-labeled datasets, limiting robustness and deployment. We present TopSeg, a topological representation-centric framework that encodes PCG dynamics with multi-sc...

---

### 25. [CorrSteer: Generation-Time LLM Steering via Correlated Sparse Autoencoder Features](https://arxiv.org/abs/2508.12535)

**Authors**: Seonglae Cho, Zekun Wu, Adriano Koshiyama  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2508.12535v2  

Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose ...

---

### 26. [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)

**Authors**: Yuxuan Lu, Jing Huang, Hui Liu, Jiri Gesi, Yan Han, Shihan Fu, Tianqi Zheng, Dakuo Wang  
**Category**: cs.CL  
**Published**: 2025-10-21  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.16252v1  

Training and evaluation of Reinforcement Learning (RL) web agents have gained increasing attention, yet a scalable and efficient environment that couples realistic and robust browser-side interaction with controllable server-side state at scale is still missing. Existing environments tend to have on...

---

### 27. [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)

**Authors**: U. V. B. L Udugama, George Vosselman, Francesco Nex  
**Category**: cs.LG  
**Published**: 2025-10-21  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.17363v1  

Deploying real-time spatial perception on edge devices requires efficient multi-task models that leverage complementary task information while minimizing computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel multi-task learning framework designed for semantic segmentation and...

---

### 28. [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)

**Authors**: Xiaozhe Li, Xinyu Fang, Shengyuan Ding, Linyang Li, Haodong Duan, Qingwen Liu, Kai Chen  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.16476v1  

Large Language Models (LLMs) have shown strong reasoning capabilities, with models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as mathematics, coding, logic, and puzzles through Reinforcement Learning with Verifiable Rewards (RLVR). However, their ability to solve more complex opt...

---

### 29. [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)

**Authors**: Lunchen Xie, Zehua He, Qingjiang Shi  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.16065v1  

Personalized Federated Learning (PFL) has emerged as a critical research frontier addressing data heterogeneity issue across distributed clients. Novel model architectures and collaboration mechanisms are engineered to accommodate statistical disparities while producing client-specific models. Param...

---

### 30. [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)

**Authors**: Ang Li, Yifei Wang, Zhihang Yuan, Stefanie Jegelka, Yisen Wang  
**Category**: cs.AI  
**Published**: 2025-10-21  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.16552v1  

Reinforcement learning in large language models (LLMs) often relies on scalar rewards, a practice that discards valuable textual rationale buried in the rollouts, forcing the model to explore \textit{de novo} with each attempt and hindering sample efficiency. While LLMs can uniquely learn from langu...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
