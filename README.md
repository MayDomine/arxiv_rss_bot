# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-09-18 12:50:43 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection](https://arxiv.org/abs/2509.13934)

**Authors**: Zhixion Chen, Jiangzhou Wang, and Hyundong Shin, Arumugam Nallanathan  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 10.0

arXiv:2509.13934v1 Announce Type: cross 
Abstract: The deployment of unmanned aerial vehicles (UAVs) for reliable and energy-efficient data collection from spatially distributed devices holds great promise in supporting diverse Internet of Things (IoT) applications. Nevertheless, the limited enduran...

---

### 2. [EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](https://arxiv.org/abs/2506.09061)

**Authors**: Alyssa Pinnock, Shakya Jayakody, Kawsher A Roxy, Md Rubel Ahmed  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 9.5

arXiv:2506.09061v3 Announce Type: replace-cross 
Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs offer remarkable capabilities in natural language understanding and generation, their...

---

### 3. [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)

**Authors**: Johnny R. Zhang (Independent Researcher), Xiaomei Mi (University of Manchester), Gaoyuan Du (Amazon), Qianyi Sun (Microsoft), Shiqi Wang (Meta), Jiaxuan Li (Amazon), Wenhua Zhou (Independent Researcher)  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 8.5

arXiv:2509.14216v1 Announce Type: cross 
Abstract: Stochastic optimization powers the scalability of modern artificial intelligence, spanning machine learning, deep learning, reinforcement learning, and large language model training. Yet, existing theory remains largely confined to Hilbert spaces, r...

---

### 4. [Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation](https://arxiv.org/abs/2508.02618)

**Authors**: Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang  
**Category**: cs.CL  
**Published**: 2025-09-18  
**Score**: 8.5

arXiv:2508.02618v2 Announce Type: replace 
Abstract: The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is i...

---

### 5. [Physics-based deep kernel learning for parameter estimation in high dimensional PDEs](https://arxiv.org/abs/2509.14054)

**Authors**: Weihao Yan, Christoph Brune, Mengwu Guo  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 8.5

arXiv:2509.14054v1 Announce Type: cross 
Abstract: Inferring parameters of high-dimensional partial differential equations (PDEs) poses significant computational and inferential challenges, primarily due to the curse of dimensionality and the inherent limitations of traditional numerical methods. Th...

---

### 6. [Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training](https://arxiv.org/abs/2504.18454)

**Authors**: Hiroki Naganuma, Xinzhi Zhang, Man-Chung Yue, Ioannis Mitliagkas, Philipp A. Witte, Russell J. Hewett, Yin Tat Lee  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 8.5

arXiv:2504.18454v2 Announce Type: replace 
Abstract: Following AI scaling trends, frontier models continue to grow in size and continue to be trained on larger datasets. Training these models requires huge investments in exascale computational resources, which has in turn driven developtment of dist...

---

### 7. [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)

**Authors**: Bingsheng Peng, Shutao Zhang, Xi Zheng, Ye Xue, Xinyu Qin, Tsung-Hui Chang  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 8.0

arXiv:2509.13686v1 Announce Type: new 
Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular network optimization, enabling reliable prediction of network performance during parameter tuning. Localized statistical channel modeling (LSCM) is the state-of-the-art channel ...

---

### 8. [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)

**Authors**: Zhipeng Bian, Jieming Zhu, Xuyang Xie, Quanyu Dai, Zhou Zhao, Zhenhua Dong  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 7.5

arXiv:2509.13773v1 Announce Type: new 
Abstract: The rapid advancement of generative AI technologies is driving the integration of diverse AI-powered services into smartphones, transforming how users interact with their devices. To simplify access to predefined AI services, this paper introduces MIR...

---

### 9. [CoPL: Collaborative Preference Learning for Personalizing LLMs](https://arxiv.org/abs/2503.01658)

**Authors**: Youngbin Choi, Seunghyuk Cho, Minjong Lee, MoonJeong Park, Yesong Ko, Jungseul Ok, Dongwoo Kim  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 7.5

arXiv:2503.01658v2 Announce Type: replace-cross 
Abstract: Personalizing large language models (LLMs) is important for aligning outputs with diverse user preferences, yet existing methods struggle with flexibility and generalization. We propose CoPL (Collaborative Preference Learning), a graph-based...

---

### 10. [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)

**Authors**: Xiaobo Yang, Xiaojin Gong  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 7.0

arXiv:2509.13676v1 Announce Type: cross 
Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM) have achieved impressive results. However, adapting MLLM to segmentation is computationally intensive,...

---

### 11. [Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices](https://arxiv.org/abs/2509.14049)

**Authors**: Jordi Grau-Haro, Ruben Ribes-Serrano, Javier Naranjo-Alcazar, Marta Garcia-Ballesteros, Pedro Zuccarello  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 7.0

arXiv:2509.14049v1 Announce Type: cross 
Abstract: Convolutional Neural Networks (CNNs) have demonstrated exceptional performance in audio tagging tasks. However, deploying these models on resource-constrained devices like the Raspberry Pi poses challenges related to computational efficiency and the...

---

### 12. [Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization](https://arxiv.org/abs/2507.20923)

**Authors**: Minh Hieu Ha, Hung Phan, Tung Duy Doan, Tung Dao, Dao Tran, Huynh Thi Thanh Binh  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 7.0

arXiv:2507.20923v2 Announce Type: replace-cross 
Abstract: Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they ...

---

### 13. [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)

**Authors**: Mengting Ai, Tianxin Wei, Sirui Chen, Jingrui He  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 7.0

arXiv:2509.14230v1 Announce Type: new 
Abstract: Structured pruning of large language models (LLMs) offers substantial efficiency improvements by removing entire hidden units, yet current approaches often suffer from significant performance degradation, particularly in zero-shot settings, and necess...

---

### 14. [Modernizing Facebook Scoped Search: Keyword and Embedding Hybrid Retrieval with LLM Evaluation](https://arxiv.org/abs/2509.13603)

**Authors**: Yongye Su, Zeya Zhang, Jane Kou, Cheng Ju, Shubhojeet Sarkar, Yamin Wang, Ji Liu, Shengbo Guo  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2509.13603v1 Announce Type: cross 
Abstract: Beyond general web-scale search, social network search uniquely enables users to retrieve information and discover potential connections within their social context. We introduce a framework of modernized Facebook Group Scoped Search by blending tra...

---

### 15. [Deep Lookup Network](https://arxiv.org/abs/2509.13662)

**Authors**: Yulan Guo, Longguang Wang, Wendong Mao, Xiaoyu Dong, Yingqian Wang, Li Liu, Wei An  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2509.13662v1 Announce Type: cross 
Abstract: Convolutional neural networks are constructed with massive operations with different types and are highly computationally intensive. Among these operations, multiplication operation is higher in computational complexity and usually requires {more} e...

---

### 16. [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)

**Authors**: Colin Hong, Xu Guo, Anand Chaanan Singh, Esha Choukse, Dmitrii Ustiugov  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2509.13990v1 Announce Type: cross 
Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for improving LLM reasoning performance at test time without retraining the model. A notable TTS technique is Self-Consistency (SC), which generates multiple reasoning chains in paral...

---

### 17. [Evolution Meets Diffusion: Efficient Neural Architecture Generation](https://arxiv.org/abs/2504.17827)

**Authors**: Bingye Zhou, Caiyang Yu  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2504.17827v4 Announce Type: replace-cross 
Abstract: Neural Architecture Search (NAS) has gained widespread attention for its transformative potential in deep learning model design. However, the vast and complex search space of NAS leads to significant computational and time costs. Neural Arch...

---

### 18. [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)

**Authors**: Yuvraj Dutta, Aaditya Sikder, Basabdatta Palit  
**Category**: cs.DC  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2509.13631v1 Announce Type: cross 
Abstract: Accurate identification of deforestation from satellite images is essential in order to understand the geographical situation of an area. This paper introduces a new distributed approach to identify as well as locate deforestation across different c...

---

### 19. [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)

**Authors**: Tom Almog  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2509.13516v1 Announce Type: new 
Abstract: As machine learning models grow increasingly complex and computationally demanding, understanding the environmental impact of training decisions becomes critical for sustainable AI development. This paper presents a comprehensive empirical study inves...

---

### 20. [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)

**Authors**: Varun Kumar, Jing Bi, Cyril Ngo Ngoc, Victor Oancea, George Em Karniadakis  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2509.13520v1 Announce Type: new 
Abstract: Neural surrogates and operator networks for solving partial differential equation (PDE) problems have attracted significant research interest in recent years. However, most existing approaches are limited in their ability to generalize solutions acros...

---

### 21. [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)

**Authors**: Qingqi Zhao, Heng Xiao  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2509.13841v1 Announce Type: new 
Abstract: Accurate prediction of permeability in porous media is essential for modeling subsurface flow. While pure data-driven models offer computational efficiency, they often lack generalization across scales and do not incorporate explicit physical constrai...

---

### 22. [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)

**Authors**: Robiul Islam, Dmitry I. Ignatov, Karl Kaberg, Roman Nabatchikov  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2509.14078v1 Announce Type: new 
Abstract: This study investigates classifier performance across EEG frequency bands using various optimizers and evaluates efficient class prediction for the left and right hemispheres. Three neural network architectures - a deep dense network, a shallow three-...

---

### 23. [DRDT3: Diffusion-Refined Decision Test-Time Training Model](https://arxiv.org/abs/2501.06718)

**Authors**: Xingshuai Huang, Di Wu, Benoit Boulet  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2501.06718v2 Announce Type: replace 
Abstract: Decision Transformer (DT), a trajectory modelling method, has shown competitive performance compared to traditional offline reinforcement learning (RL) approaches on various classic control tasks. However, it struggles to learn optimal policies fr...

---

### 24. [HAM: Hierarchical Adapter Merging for Scalable Continual Learning](https://arxiv.org/abs/2509.13211)

**Authors**: Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco  
**Category**: cs.LG  
**Published**: 2025-09-18  
**Score**: 6.5

arXiv:2509.13211v2 Announce Type: replace 
Abstract: Continual learning is an essential capability of human cognition, yet it poses significant challenges for current deep learning models. The primary issue is that new knowledge can interfere with previously learned information, causing the model to...

---

### 25. [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)

**Authors**: Qikai Chang, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Yicheng Pan, Jianshu Zhang, Jun Du, Quan Liu, Jianqing Gao  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 6.0

arXiv:2509.13761v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a pro...

---

### 26. [Explainable AI-Enhanced Supervisory Control for High-Precision Spacecraft Formation](https://arxiv.org/abs/2509.13331)

**Authors**: Reza Pirayeshshirazinezhad  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 6.0

arXiv:2509.13331v1 Announce Type: cross 
Abstract: We use artificial intelligence (AI) and supervisory adaptive control systems to plan and optimize the mission of precise spacecraft formation. Machine learning and robust control enhance the efficiency of spacecraft precision formation of the Virtua...

---

### 27. [A reduced-order derivative-informed neural operator for subsurface fluid-flow](https://arxiv.org/abs/2509.13620)

**Authors**: Jeongjin (Jayjay),  Park, Grant Bruer, Huseyin Tuna Erdinc, Abhinav Prakash Gahlot, Felix J. Herrmann  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 6.0

arXiv:2509.13620v1 Announce Type: cross 
Abstract: Neural operators have emerged as cost-effective surrogates for expensive fluid-flow simulators, particularly in computationally intensive tasks such as permeability inversion from time-lapse seismic data, and uncertainty quantification. In these app...

---

### 28. [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)

**Authors**: Michal Szczepanski, Martyna Poreba, Karim Haroun  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 6.0

arXiv:2509.14165v1 Announce Type: cross 
Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic segmentation but are hindered by high computational and memory costs. To address this, we propose STEP (SuperToken and Early-Pruning), a hybrid token-reduction framework tha...

---

### 29. [Direct Video-Based Spatiotemporal Deep Learning for Cattle Lameness Detection](https://arxiv.org/abs/2504.16404)

**Authors**: Md Fahimuzzman Sohan, Raid Alzubi, Hadeel Alzoubi, Eid Albalawi, A. H. Abdul Hafez  
**Category**: cs.AI  
**Published**: 2025-09-18  
**Score**: 6.0

arXiv:2504.16404v3 Announce Type: replace-cross 
Abstract: Cattle lameness is a prevalent health problem in livestock farming, often resulting from hoof injuries or infections, and severely impacts animal welfare and productivity. Early and accurate detection is critical for minimizing economic loss...

---

### 30. [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)

**Authors**: Yaxin Gao, Yao Lu, Zongfei Zhang, Jiaqi Nie, Shanqing Yu, Qi Xuan  
**Category**: cs.CL  
**Published**: 2025-09-18  
**Score**: 6.0

arXiv:2509.13723v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success in many natural language processing (NLP) tasks. To achieve more accurate output, the prompts used to drive LLMs have become increasingly longer, which incurs higher computational costs. To...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
