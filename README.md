# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-09-12 12:48:29 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)

**Authors**: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji  
**Category**: cs.CL  
**Published**: 2025-09-12  
**Score**: 12.5

arXiv:2508.19740v3 Announce Type: replace 
Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identi...

---

### 2. [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)

**Authors**: Wenhao Li, Bangcheng Sun, Weihao Ye, Tianyi Zhang, Daohai Yu, Fei Chao, Rongrong Ji  
**Category**: cs.CL  
**Published**: 2025-09-12  
**Score**: 11.0

arXiv:2509.09199v1 Announce Type: new 
Abstract: Scaling language models to longer contexts is essential for capturing rich dependencies across extended discourse. However, na\"ive context extension imposes significant computational and memory burdens, often resulting in inefficiencies during both t...

---

### 3. [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)

**Authors**: Omar Erak, Omar Alhussein, Hatem Abou-Zeid, Mehdi Bennis  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 8.5

arXiv:2509.09168v1 Announce Type: cross 
Abstract: Large-scale transformer models have emerged as a powerful tool for semantic communication systems, enabling edge devices to extract rich representations for robust inference across noisy wireless channels. However, their substantial computational de...

---

### 4. [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)

**Authors**: Sophia Maria  
**Category**: cs.CL  
**Published**: 2025-09-12  
**Score**: 8.5

arXiv:2509.09121v1 Announce Type: new 
Abstract: Large language models (LLMs) excel in general-domain applications, yet their performance often degrades in specialized tasks requiring domain-specific knowledge. E-commerce is particularly challenging, as its data are noisy, heterogeneous, multilingua...

---

### 5. [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)

**Authors**: Shuocheng Li, Yihao Liu, Silin Du, Wenxuan Zeng, Zhe Xu, Mengyu Zhou, Yeye He, Haoyu Dong, Shi Han, Dongmei Zhang  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 8.0

arXiv:2509.09245v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown great promise in automating data science workflows, but existing models still struggle with multi-step reasoning and tool use, which limits their effectiveness on complex data analysis tasks. To address this, we...

---

### 6. [Diffusion Graph Neural Networks for Robustness in Olfaction Sensors and Datasets](https://arxiv.org/abs/2506.00455)

**Authors**: Kordel K. France, Ovidiu Daescu  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 7.5

arXiv:2506.00455v3 Announce Type: replace-cross 
Abstract: Robotic odour source localization (OSL) is a critical capability for autonomous systems operating in complex environments. However, current OSL methods often suffer from ambiguities, particularly when robots misattribute odours to incorrect ...

---

### 7. [MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining](https://arxiv.org/abs/2509.06806)

**Authors**: Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 7.5

arXiv:2509.06806v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonst...

---

### 8. [Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings](https://arxiv.org/abs/2501.08219)

**Authors**: Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic  
**Category**: cs.LG  
**Published**: 2025-09-12  
**Score**: 7.5

arXiv:2501.08219v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing (NLP) tasks, leading to widespread adoption in both research and industry. However, their inference workloads are computationa...

---

### 9. [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)

**Authors**: Haowei Yang, Yushang Zhao, Sitao Min, Bo Su, Chao Yao, Wei Xu  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 7.0

arXiv:2509.09066v1 Announce Type: new 
Abstract: The cold-start user issue further compromises the effectiveness of recommender systems in limiting access to the historical behavioral information. It is an effective pipeline to optimize instructional prompts on a few-shot large language model (LLM) ...

---

### 10. [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)

**Authors**: Hengyu Fang, Yijiang Liu, Yuan Du, Li Du, Huanrui Yang  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 7.0

arXiv:2509.09090v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for embodied intelligence. However, their extensive computational and memory costs hinder their practical deployment. Existing VLA compression and acceleration approaches conduct...

---

### 11. [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)

**Authors**: Mohammed Tiouti, Mohamed Bal-Ghaoui  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 7.0

arXiv:2509.09387v1 Announce Type: cross 
Abstract: Effective model and hyperparameter selection remains a major challenge in deep learning, often requiring extensive expertise and computation. While AutoML and large language models (LLMs) promise automation, current LLM-based approaches rely on tria...

---

### 12. [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)

**Authors**: Zhiyu He, Maojiang Wang, Xinwen Gao, Yuchuan Luo, Lin Liu, Shaojing Fu  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 7.0

arXiv:2509.09424v1 Announce Type: cross 
Abstract: Secure inference enables privacy-preserving machine learning by leveraging cryptographic protocols that support computations on sensitive user data without exposing it. However, integrating cryptographic protocols with large language models (LLMs) p...

---

### 13. [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)

**Authors**: Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 7.0

arXiv:2509.09674v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges...

---

### 14. [Towards Generalized Routing: Model and Agent Orchestration for Adaptive and Efficient Inference](https://arxiv.org/abs/2509.07571)

**Authors**: Xiyu Guo, Shan Wang, Chunfang Ji, Xuefeng Zhao, Wenhao Xi, Yaoyao Liu, Qinglan Li, Chao Deng, Junlan Feng  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 7.0

arXiv:2509.07571v2 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models (LLMs) and domain-specific AI agents has greatly expanded the ecosystem of AI-powered services. User queries, however, are highly diverse and often span multiple domains and task types, resultin...

---

### 15. [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)

**Authors**: Mohsen Fayyaz, Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Ryan Rossi, Trung Bui, Hinrich Sch\"utze, Nanyun Peng  
**Category**: cs.CL  
**Published**: 2025-09-12  
**Score**: 7.0

arXiv:2509.09660v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token through a subset of specialized Feed-Forward Networks (FFN), known as experts. We present SteerMoE, a framework for steering MoE models by detecting and controlling behavior-li...

---

### 16. [LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning](https://arxiv.org/abs/2507.20999)

**Authors**: Yining Huang, Bin Li, Keke Tang, Meilian Chen  
**Category**: cs.CL  
**Published**: 2025-09-12  
**Score**: 7.0

arXiv:2507.20999v2 Announce Type: replace-cross 
Abstract: Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While pa...

---

### 17. [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)

**Authors**: Shulai Zhang, Ao Xu, Quan Chen, Han Zhao, Weihao Cui, Ningxin Zheng, Haibin Lin, Xin Liu, Minyi Guo  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2509.09560v1 Announce Type: new 
Abstract: Embodied AI systems operate in dynamic environments, requiring seamless integration of perception and generation modules to process high-frequency input and output demands. Traditional sequential computation patterns, while effective in ensuring accur...

---

### 18. [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)

**Authors**: Piyush Pant  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2509.09055v1 Announce Type: cross 
Abstract: This research investigates the effectiveness of alignment techniques, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a combined SFT+DPO approach on improving the safety and helpfulness of the OPT-350M language model. Utilizi...

---

### 19. [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)

**Authors**: Honglan Yu, Yibin Wang, Feifei Dai, Dong Liu, Haihui Fan, Xiaoyan Gu  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2509.09091v1 Announce Type: cross 
Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP) have gained wide applications for private inference. Due to high inference latency in TEEs, researchers use partition-based approaches that offload linear model components...

---

### 20. [Discovering physical laws with parallel symbolic enumeration](https://arxiv.org/abs/2407.04405)

**Authors**: Kai Ruan, Yilong Xu, Ze-Feng Gao, Yike Guo, Hao Sun, Ji-Rong Wen, Yang Liu  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2407.04405v4 Announce Type: replace-cross 
Abstract: Symbolic regression plays a crucial role in modern scientific research thanks to its capability of discovering concise and interpretable mathematical expressions from data. A key challenge lies in the search for parsimonious and generalizabl...

---

### 21. [Entropy-Gated Branching for Efficient Test-Time Reasoning](https://arxiv.org/abs/2503.21961)

**Authors**: Xianzhi Li, Ethan Callanan, Abdellah Ghassel, Xiaodan Zhu  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2503.21961v2 Announce Type: replace-cross 
Abstract: Test-time compute methods like beam search can significantly improve the reasoning capabilities and problem-solving accuracy of large language models. However, these approaches require substantially increased computational resources, with mo...

---

### 22. [Group Expectation Policy Optimization for Heterogeneous Reinforcement Learning](https://arxiv.org/abs/2508.17850)

**Authors**: Han Zhang, Ruibin Zheng, Zexuan Yi, Zhuo Zhang, Hanyang Peng, Hui Wang, Zike Yuan, Cai Ke, Shiwei Chen, Jiacheng Yang, Yangning Li, Xiang Li, Jiangyue Yan, Yaoqi Liu, Liwen Jing, Jiayin Qi, Ruifeng Xu, Binxing Fang, Yue Yu  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2508.17850v3 Announce Type: replace-cross 
Abstract: As single-center computing approaches power constraints, decentralized training is becoming essential. Reinforcement Learning (RL) post-training enhances Large Language Models (LLMs) but faces challenges in heterogeneous distributed environm...

---

### 23. [DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models](https://arxiv.org/abs/2408.04275)

**Authors**: Zili Zhang, Yinmin Zhong, Yimin Jiang, Hanpeng Hu, Jianjian Sun, Zheng Ge, Yibo Zhu, Daxin Jiang, Xin Jin  
**Category**: cs.DC  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2408.04275v3 Announce Type: replace 
Abstract: Multimodal large language models (LLMs) empower LLMs to ingest inputs and generate outputs in multiple forms, such as text, image, and audio. However, the integration of multiple modalities introduces heterogeneity in both the model and training d...

---

### 24. [Efficient Optimization Accelerator Framework for Multistate Ising Problems](https://arxiv.org/abs/2505.20250)

**Authors**: Chirag Garg, Sayeef Salahuddin  
**Category**: cs.DC  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2505.20250v2 Announce Type: replace-cross 
Abstract: Ising Machines are emerging hardware architectures that efficiently solve NP-Hard combinatorial optimization problems. Generally, combinatorial problems are transformed into quadratic unconstrained binary optimization (QUBO) form, but this t...

---

### 25. [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)

**Authors**: Jingwen Liu, Hantao Yu, Clayton Sanford, Alexandr Andoni, Daniel Hsu  
**Category**: cs.LG  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2509.09001v1 Announce Type: new 
Abstract: Transformers have the representational capacity to simulate Massively Parallel Computation (MPC) algorithms, but they suffer from quadratic time complexity, which severely limits their scalability. We introduce an efficient attention mechanism called ...

---

### 26. [Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination](https://arxiv.org/abs/2501.06058)

**Authors**: Kevin Fu, Shalin Anand Jain, Pierce Howell, Harish Ravichandar  
**Category**: cs.LG  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2501.06058v5 Announce Type: replace-cross 
Abstract: Recent advances have enabled heterogeneous multi-robot teams to learn complex and effective coordination skills. However, existing neural architectures that support heterogeneous teaming tend to force a trade-off between expressivity and eff...

---

### 27. [Self-Optimizing Machine Learning Potential Assisted Automated Workflow for Highly Efficient Complex Systems Material Design](https://arxiv.org/abs/2505.08159)

**Authors**: Jiaxiang Li, Junwei Feng, Jie Luo, Bowen Jiang, Xiangyu Zheng, Qigang Song, Jian Lv, Keith Butler, Hanyu Liu, Congwei Xie, Yu Xie, Yanming Ma  
**Category**: cs.LG  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2505.08159v2 Announce Type: replace-cross 
Abstract: Machine learning interatomic potentials have revolutionized complex materials design by enabling rapid exploration of material configurational spaces via crystal structure prediction with ab initio accuracy. However, critical challenges pers...

---

### 28. [MasconCube: Fast and Accurate Gravity Modeling with an Explicit Representation](https://arxiv.org/abs/2509.08607)

**Authors**: Pietro Fanti, Dario Izzo  
**Category**: cs.LG  
**Published**: 2025-09-12  
**Score**: 6.5

arXiv:2509.08607v2 Announce Type: replace-cross 
Abstract: The geodesy of irregularly shaped small bodies presents fundamental challenges for gravitational field modeling, particularly as deep space exploration missions increasingly target asteroids and comets. Traditional approaches suffer from cri...

---

### 29. [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)

**Authors**: Bingning Huang, Tu Nguyen, Matthieu Zimmer  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 6.0

arXiv:2509.09284v1 Announce Type: new 
Abstract: Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explor...

---

### 30. [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)

**Authors**: K. Pronk, Q. Zhao  
**Category**: cs.AI  
**Published**: 2025-09-12  
**Score**: 6.0

arXiv:2509.08867v1 Announce Type: cross 
Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on the climate due to the substantial energy required for their deployment and use. To create awareness for developers who are implementing LLMs in their products, there is a...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
