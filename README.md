# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-28 04:23:32 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference](https://arxiv.org/abs/2511.21513)

**Authors**: Wanli Zhong, Haibo Feng, Zirui Zhou, Hanyang Peng, Shiqi Yu  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2511.21513v1  

Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the dominant bottleneck. This stage incurs a costly dequantize-softmax-requantize detour, which can acc...

---

### 2. [Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios](https://arxiv.org/abs/2511.20340)

**Authors**: Luohe Shi, Zuchao Li, Lefei Zhang, Baoyuan Qi, Guoming Liu, Hai Zhao  
**Category**: cs.CL  
**Published**: 2025-11-27  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.20340v1  

Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a sma...

---

### 3. [Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases](https://arxiv.org/abs/2511.21612)

**Authors**: Shahir Abdullah, Syed Rohit Zaman  
**Category**: cs.DC  
**Published**: 2025-11-27  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.21612v1  

Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-...

---

### 4. [SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space](https://arxiv.org/abs/2511.20102)

**Authors**: Zhenyi Shen, Junru Lu, Lin Gui, Jiazheng Li, Yulan He, Di Yin, Xing Sun  
**Category**: cs.CL  
**Published**: 2025-11-27  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.20102v1  

The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation...

---

### 5. [BRIDGE: Building Representations In Domain Guided Program Verification](https://arxiv.org/abs/2511.21104)

**Authors**: Robert Joseph George, Carson Eisenach, Udaya Ghai, Dominique Perrault-Joncas, Anima Anandkumar, Dean Foster  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.21104v1  

Large language models (LLMs) have achieved impressive results in code generation, yet struggle with program verification, especially in interactive proof frameworks such as Lean4. A central challenge is scalability: verified synthesis requires not just code, but also precise specifications and corre...

---

### 6. [Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.21011)

**Authors**: Sid Bharthulwar, Stone Tao, Hao Su  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.21011v1  

Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the upda...

---

### 7. [Generative Early Stage Ranking](https://arxiv.org/abs/2511.21095)

**Authors**: Juhee Hong, Meng Liu, Shengzhi Wang, Xiaoheng Mao, Huihui Cheng, Leon Gao, Christopher Leung, Jin Zhou, Chandra Mouli Sekar, Zhao Zhu, Ruochen Liu, Tuan Trieu, Dawei Sun, Jeet Kanjani, Rui Li, Jing Qian, Xuan Cao, Minjie Fan, Mingze Gao  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.21095v1  

Large-scale recommendations commonly adopt a multi-stage cascading ranking system paradigm to balance effectiveness and efficiency. Early Stage Ranking (ESR) systems utilize the "user-item decoupling" approach, where independently learned user and item representations are only combined at the final ...

---

### 8. [Prune4Web: DOM Tree Pruning Programming for Web Agent](https://arxiv.org/abs/2511.21398)

**Authors**: Jiayuan Zhang, Kaiquan Chen, Zhihao Lu, Enshen Zhou, Qian Yu, Jing Zhang  
**Category**: cs.AI  
**Published**: 2025-11-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.21398v1  

Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohi...

---

### 9. [Latent Collaboration in Multi-Agent Systems](https://arxiv.org/abs/2511.20639)

**Authors**: Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, Gaotang Li, Katherine Tieu, Pan Lu, Ke Shen, Hanghang Tong, Yejin Choi, Jingrui He, James Zou, Mengdi Wang, Ling Yang  
**Category**: cs.CL  
**Published**: 2025-11-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.20639v1  

Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly...

---

### 10. [Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks](https://arxiv.org/abs/2511.20834)

**Authors**: Dionysios Adamopoulos, Anastasia Poulopoulou, Georgios Goumas, Christina Giannoula  
**Category**: cs.DC  
**Published**: 2025-11-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.20834v1  

Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work id...

---

### 11. [MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training](https://arxiv.org/abs/2511.21431)

**Authors**: Lu Zhao, Rong Shi, Shaoqing Zhang, Yueqiang Chen, Baoguo He, Hongfeng Sun, Ziqing Yin, Shangchao Su, Zhiyan Cui, Liang Dong, Xiyuan Li, Lingbin Wang, Jianwei He, Jiesong Ma, Weikang Huang, Jianglei Tong, Dongdong Gao, Jian Zhang, Hong Tian  
**Category**: cs.DC  
**Published**: 2025-11-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.21431v1  

The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which c...

---

### 12. [Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression](https://arxiv.org/abs/2511.21016)

**Authors**: Liangzu Peng, Aditya Chattopadhyay, Luca Zancato, Elvis Nunez, Wei Xia, Stefano Soatto  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.21016v1  

As efficient alternatives to softmax Attention, linear state-space models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall oriented tasks. We propose Gated KalmaNet (GKA), a layer that reduces th...

---

### 13. [OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection](https://arxiv.org/abs/2511.21064)

**Authors**: Chujie Wang, Jianyu Lu, Zhiyuan Luo, Xi Chen, Chu He  
**Category**: cs.AI  
**Published**: 2025-11-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.21064v1  

Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal...

---

### 14. [MTA: A Merge-then-Adapt Framework for Personalized Large Language Model](https://arxiv.org/abs/2511.20072)

**Authors**: Xiaopeng Li, Yuanjin Zheng, Wanyu Wang, wenlin zhang, Pengyue Jia, Yiqi Wang, Maolin Wang, Xuetao Wei, Xiangyu Zhao  
**Category**: cs.CL  
**Published**: 2025-11-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.20072v2  

Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly wit...

---

### 15. [An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids](https://arxiv.org/abs/2511.21590)

**Authors**: Muhammad Siddique, Sohaib Zafar  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.21590v1  

Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliabil...

---

### 16. [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)

**Authors**: Jinpeng Wang, Chao Li, Ting Ye, Mengyuan Zhang, Wei Liu, Jian Luan  
**Category**: cs.AI  
**Published**: 2025-11-27  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.21005v1  

Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, ...

---

### 17. [MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning](https://arxiv.org/abs/2511.21460)

**Authors**: Junjian Wang, Lidan Zhao, Xi Sheryl Zhang  
**Category**: cs.AI  
**Published**: 2025-11-27  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.21460v1  

Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or o...

---

### 18. [$\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers](https://arxiv.org/abs/2511.19987)

**Authors**: Xinyu Wang, Hanwei Wu, Qingchen Hu, Zhenghan Tai, Jingrui Tian, Lei Ding, Jijun Chi, Hailin He, Tung Sum Thomas Kwok, Yufei Cui, Sicheng Lyu, Muzhi Li, Mingze Li, Xinyue Yu, Ling Zhou, Peng Lu  
**Category**: cs.CL  
**Published**: 2025-11-27  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.19987v1  

Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R...

---

### 19. [A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems](https://arxiv.org/abs/2511.21032)

**Authors**: Yuxuan Zhu, Cong Fu, Yabo Ni, Anxiang Zeng, Yuan Fang  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.21032v1  

Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning of...

---

### 20. [How to Correctly Report LLM-as-a-Judge Evaluations](https://arxiv.org/abs/2511.21140)

**Authors**: Chungpa Lee, Thomas Zeng, Jongwon Jeong, Jy-yong Sohn, Kangwook Lee  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.21140v1  

Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and...

---

### 21. [A Physics-Informed U-net-LSTM Network for Data-Driven Seismic Response Modeling of Structures](https://arxiv.org/abs/2511.21276)

**Authors**: Sutirtha Biswas, Kshitij Kumar Yadav  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.21276v1  

Accurate and efficient seismic response prediction is essential for the design of resilient structures. While the Finite Element Method (FEM) remains the standard for nonlinear seismic analysis, its high computational demands limit its scalability and real time applicability. Recent developments in ...

---

### 22. [Controlling changes to attention logits](https://arxiv.org/abs/2511.21377)

**Authors**: Ben Anson, Laurence Aitchison  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.21377v1  

Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying normalization to queries and keys, known as `QK norm', fixes stability issues in practice, but is no...

---

### 23. [Lost in Time? A Meta-Learning Framework for Time-Shift-Tolerant Physiological Signal Transformation](https://arxiv.org/abs/2511.21500)

**Authors**: Qian Hong, Cheng Bian, Xiao Zhou, Xiaoyu Li, Yelei Li, Zijing Zeng  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.21500v1  

Translating non-invasive signals such as photoplethysmography (PPG) and ballistocardiography (BCG) into clinically meaningful signals like arterial blood pressure (ABP) is vital for continuous, low-cost healthcare monitoring. However, temporal misalignment in multimodal signal transformation impairs...

---

### 24. [Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO](https://arxiv.org/abs/2511.21638)

**Authors**: Daniel R. Jiang, Jalaj Bhandari, Yukai Yang, R\'emi Munos, Tyler Lu  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.21638v1  

Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the d...

---

### 25. [Representation Interventions Enable Lifelong Unstructured Knowledge Control](https://arxiv.org/abs/2511.20892)

**Authors**: Xuyuan Liu, Zhengzhang Chen, Xinshuai Dong, Yanchi Liu, Xujiang Zhao, Shengyu Chen, Haoyu Wang, Yujun Yan, Haifeng Chen  
**Category**: cs.AI  
**Published**: 2025-11-27  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2511.20892v1  

Large language models (LLMs) often produce incorrect or outdated content. Updating their knowledge efficiently and accurately without costly retraining is a major challenge. This problem is especially hard for complex, unstructured knowledge in a lifelong setting, where many edits must coexist witho...

---

### 26. [Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search](https://arxiv.org/abs/2511.19648)

**Authors**: Manil Shrestha, Edward Kim  
**Category**: cs.CL  
**Published**: 2025-11-27  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2511.19648v1  

Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deploymen...

---

### 27. [Online-PVLM: Advancing Personalized VLMs with Online Concept Learning](https://arxiv.org/abs/2511.20056)

**Authors**: Huiyu Bai, Runze Wang, Zhuoyun Du, Yiyang Zhao, Fengji Zhang, Haoyu Chen, Xiaoyong Zhu, Bo Zheng, Xuejiao Zhao  
**Category**: cs.CL  
**Published**: 2025-11-27  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2511.20056v1  

Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to suppo...

---

### 28. [REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance](https://arxiv.org/abs/2511.20233)

**Authors**: Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, Zhiyuan Fan  
**Category**: cs.CL  
**Published**: 2025-11-27  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2511.20233v1  

The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, int...

---

### 29. [A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving](https://arxiv.org/abs/2511.20982)

**Authors**: Junhan Liao, Minxian Xu, Wanyi Zheng, Yan Wang, Kejiang Ye, Rajkumar Buyya, Chengzhong Xu  
**Category**: cs.DC  
**Published**: 2025-11-27  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2511.20982v1  

To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance...

---

### 30. [A Unified Understanding of Offline Data Selection and Online Self-refining Generation for Post-training LLMs](https://arxiv.org/abs/2511.21056)

**Authors**: Quan Xiao, Tianyi Chen  
**Category**: cs.LG  
**Published**: 2025-11-27  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2511.21056v1  

Offline data selection and online self-refining generation, which enhance the data quality, are crucial steps in adapting large language models (LLMs) to specific downstream tasks. We tackle offline data selection and online self-refining generations through an optimization perspective. Specifically...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
