# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2026-01-16 05:52:06 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems](https://arxiv.org/abs/2601.09985)

**Authors**: Tianqi Zhang, Flavio Ponzina, Tajana Rosing  
**Category**: cs.LG  
**Published**: 2026-01-16  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.09985v1  

#### Abstract
Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed...

---

### 2. [CAFEDistill: Learning Personalized and Dynamic Models through Federated Early-Exit Network Distillation](https://arxiv.org/abs/2601.10015)

**Authors**: Boyi Liu, Zimu Zhou, Yongxin Tong  
**Category**: cs.LG  
**Published**: 2026-01-16  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.10015v1  

#### Abstract
Personalized Federated Learning (PFL) enables collaboratively model training on decentralized, heterogeneous data while tailoring them to each client's unique distribution. However, existing PFL methods produce static models with a fixed tradeoff between accuracy and efficiency, limiting their appli...

---

### 3. [Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection](https://arxiv.org/abs/2601.10067)

**Authors**: Hung Vinh Tran, Tong Chen, Hechuan Wen, Quoc Viet Hung Nguyen, Bin Cui, Hongzhi Yin  
**Category**: cs.LG  
**Published**: 2026-01-16  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.10067v1  

#### Abstract
Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommod...

---

### 4. [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)

**Authors**: Zerui Yang, Weichuan Wang, Yanwei Xu, Linqi Song, Yudai Matsuda, Wei Han, Bo Bai  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.10011v1  

#### Abstract
Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, ...

---

### 5. [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)

**Authors**: Tiesunlong Shen, Rui Mao, Jin Wang, Heming Sun, Jian Zhang, Xuejie Zhang, Erik Cambria  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.10416v1  

#### Abstract
Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sa...

---

### 6. [TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction](https://arxiv.org/abs/2601.10410)

**Authors**: Mihai Dan Nadas, Laura Diosan, Andreea Tomescu, Andrei Piscoran  
**Category**: cs.CL  
**Published**: 2026-01-16  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.10410v1  

#### Abstract
Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there i...

---

### 7. [Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text](https://arxiv.org/abs/2601.10355)

**Authors**: Zhihao Xu, Rumei Li, Jiahuan Li, Rongxiang Weng, Jingang Wang, Xunliang Cai, Xiting Wang  
**Category**: cs.CL  
**Published**: 2026-01-16  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.10355v1  

#### Abstract
Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. ...

---

### 8. [TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models](https://arxiv.org/abs/2601.09776)

**Authors**: Khalid Oublal, Quentin Bouniot, Qi Gan, Stephan Cl\'emen\c{c}on, Zeynep Akata  
**Category**: cs.LG  
**Published**: 2026-01-16  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.09776v1  

#### Abstract
As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-dist...

---

### 9. [Communication-Efficient Federated Learning by Exploiting Spatio-Temporal Correlations of Gradients](https://arxiv.org/abs/2601.10491)

**Authors**: Shenlong Zheng, Zhen Zhang, Yuhui Deng, Geyong Min, Lin Cui  
**Category**: cs.LG  
**Published**: 2026-01-16  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.10491v1  

#### Abstract
Communication overhead is a critical challenge in federated learning, particularly in bandwidth-constrained networks. Although many methods have been proposed to reduce communication overhead, most focus solely on compressing individual gradients, overlooking the temporal correlations among them. Pr...

---

### 10. [Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis](https://arxiv.org/abs/2601.10701)

**Authors**: Chun Hei Michael Shiu, Chih Wei Ling  
**Category**: cs.LG  
**Published**: 2026-01-16  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.10701v1  

#### Abstract
Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in ...

---

### 11. [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)

**Authors**: Nguyen Minh Phuong, Dang Huu Tien, Naoya Inoue  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.09805v1  

#### Abstract
Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While inte...

---

### 12. [SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation](https://arxiv.org/abs/2601.09974)

**Authors**: Seoyeon Kim, Jaehyung Kim  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.09974v1  

#### Abstract
Personalizing Large Language Models typically relies on static retrieval or one-time adaptation, assuming user preferences remain invariant over time. However, real-world interactions are dynamic, where user interests continuously evolve, posing a challenge for models to adapt to preference drift wi...

---

### 13. [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)

**Authors**: Ziming Dai, Dabiao Ma, Jinle Tong, Mengyuan Han, Jian Yang, Haojun Fei  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.10457v1  

#### Abstract
Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boo...

---

### 14. [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)

**Authors**: Jianheng Tang, Shilong Tao, Zhe Feng, Haonan Sun, Menglu Wang, Zhanxing Zhu, Yunhuai Liu  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.10031v1  

#### Abstract
The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typical...

---

### 15. [Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel](https://arxiv.org/abs/2601.10266)

**Authors**: Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira  
**Category**: cs.CL  
**Published**: 2026-01-16  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.10266v1  

#### Abstract
Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Pro...

---

### 16. [Discrete Feynman-Kac Correctors](https://arxiv.org/abs/2601.10403)

**Authors**: Mohsin Hasan, Viktor Ohanesian, Artem Gazizov, Yoshua Bengio, Al\'an Aspuru-Guzik, Roberto Bondesan, Marta Skreta, Kirill Neklyudov  
**Category**: cs.LG  
**Published**: 2026-01-16  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.10403v1  

#### Abstract
Discrete diffusion models have recently emerged as a promising alternative to the autoregressive approach for generating discrete sequences. Sample generation via gradual denoising or demasking processes allows them to capture hierarchical non-sequential interdependencies in the data. These custom p...

---

### 17. [CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning](https://arxiv.org/abs/2601.10407)

**Authors**: Yuanjie Zhao, Junnan Qiu, Yue Ding, Jie Li  
**Category**: cs.LG  
**Published**: 2026-01-16  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.10407v1  

#### Abstract
Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to backdoor attacks. Existing attack strategies typically struggle against safety-constrained algorithms (e.g., CQL) due to inefficient random poisoning and the use of easily detectable ...

---

### 18. [Single-Stage Huffman Encoder for ML Compression](https://arxiv.org/abs/2601.10673)

**Authors**: Aditya Agrawal, Albert Magyar, Hiteshwar Eswaraiah, Patrick Sheridan, Pradeep Janedula, Ravi Krishnan Venkatesan, Krishna Nair, Ravi Iyer  
**Category**: cs.LG  
**Published**: 2026-01-16  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.10673v1  

#### Abstract
Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design...

---

### 19. [PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization](https://arxiv.org/abs/2601.10029)

**Authors**: Tingyue Pan, Jie Ouyang, Mingyue Cheng, Qingchuan Li, Zirui Liu, Mingfan Pan, Shuo Yu, Qi Liu  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.10029v1  

#### Abstract
Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential ...

---

### 20. [MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning](https://arxiv.org/abs/2601.10157)

**Authors**: Yusong Wang, Jialun Shen, Zhihao Wu, Yicheng Xu, Shiyin Tan, Mingkun Xu, Changshuo Wang, Zixing Song, Prayag Tiwari  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.10157v1  

#### Abstract
Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties ...

---

### 21. [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)

**Authors**: Xin Guan, Zijian Li, Shen Huang, Pengjun Xie, Jingren Zhou, Jiuxin Cao  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.10306v1  

#### Abstract
While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. ...

---

### 22. [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)

**Authors**: Zixun Lan, Maochun Xu, Yifan Ren, Rui Wu, Jianghui Zhou, Xueyang Cheng, Jianan Ding Ding, Xinheng Wang, Mingmin Chi, Fei Ma  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.09972v1  

#### Abstract
Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, an...

---

### 23. [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)

**Authors**: Yizhan Li, Florence Cloutier, Sifan Wu, Ali Parviz, Boris Knyazev, Yan Zhang, Glen Berseth, Bang Liu  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.10131v1  

#### Abstract
Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. ...

---

### 24. [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)

**Authors**: Haochong Xia, Yao Long Teng, Regan Tan, Molei Qin, Xinrun Wang, Bo An  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.10143v1  

#### Abstract
In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in ...

---

### 25. [Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering](https://arxiv.org/abs/2601.10402)

**Authors**: Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.10402v1  

#### Abstract
The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have de...

---

### 26. [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)

**Authors**: Kimia Abedini, Farzad Shami, Gianmaria Silvello  
**Category**: cs.AI  
**Published**: 2026-01-16  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.10581v1  

#### Abstract
Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databa...

---

### 27. [Credit C-GPT: A Domain-Specialized Large Language Model for Conversational Understanding in Vietnamese Debt Collection](https://arxiv.org/abs/2601.10167)

**Authors**: Nhung Nguyen Thi Hong, Cuong Nguyen Dang, Tri Le Ngoc  
**Category**: cs.CL  
**Published**: 2026-01-16  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.10167v1  

#### Abstract
Debt collection is a critical function within the banking, financial services, and insurance (BFSI) sector, relying heavily on large-scale human-to-human conversational interactions conducted primarily in Vietnamese contact centers. These conversations involve informal spoken language, emotional var...

---

### 28. [An Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit](https://arxiv.org/abs/2601.10321)

**Authors**: Warren Jouanneau, Emma Jouffroy, Marc Palyart  
**Category**: cs.CL  
**Published**: 2026-01-16  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.10321v1  

#### Abstract
Finding the most relevant person for a job proposal in real time is challenging, especially when resumes are long, structured, and multilingual. In this paper, we propose a re-ranking model based on a new generation of late cross-attention architecture, that decomposes both resumes and project brief...

---

### 29. [Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure](https://arxiv.org/abs/2601.10566)

**Authors**: Syed Naveed Mahmood, Md. Rezaur Rahman Bhuiyan, Tasfia Zaman, Jareen Tasneem Khondaker, Md. Sameer Sakib, Nazia Tasnim, Farig Sadeque  
**Category**: cs.CL  
**Published**: 2026-01-16  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.10566v1  

#### Abstract
Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by intro...

---

### 30. [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)

**Authors**: Ziting Zhang, Kai Wan, Minquan Cheng, Shuo Shao, Giuseppe Caire  
**Category**: cs.DC  
**Published**: 2026-01-16  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.10177v1  

#### Abstract
Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem,...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
