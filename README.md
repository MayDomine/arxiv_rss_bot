# arXiv Papers Bot ðŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ðŸ“Š Statistics

- **Last Updated**: 2025-11-17 12:54:44 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ðŸ“š Recent Papers

### 1. [DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference](https://arxiv.org/abs/2511.11446)

**Authors**: Farhana Amin, Sabiha Afroz, Kanchon Gharami, Mona Moghadampanah, Dimitrios S. Nikolopoulos  
**Category**: cs.LG  
**Published**: 2025-11-17  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2511.11446v1  

Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precis...

---

### 2. [SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices](https://arxiv.org/abs/2511.11038)

**Authors**: Jiaming Huang, Yi Gao, Fuchang Pan, Renjie Li, Wei Dong  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 10.0  
**Type**: cross  
**ArXiv ID**: 2511.11038v1  

With the rapid growth of the Internet of Things (IoT), integrating artificial intelligence (AI) on extremely weak embedded devices has garnered significant attention, enabling improved real-time performance and enhanced data privacy. However, the resource limitations of such devices and unreliable n...

---

### 3. [EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence](https://arxiv.org/abs/2511.10834)

**Authors**: Ansel Kaplan Erol, Seungjun Lee, Divya Mahajan  
**Category**: cs.DC  
**Published**: 2025-11-17  
**Score**: 10.0  
**Type**: cross  
**ArXiv ID**: 2511.10834v1  

Low-latency delivery of satellite imagery is essential for time-critical applications such as disaster response, intelligence, and infrastructure monitoring. However, traditional pipelines rely on downlinking all captured images before analysis, introducing delays of hours to days due to restricted ...

---

### 4. [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)

**Authors**: Shulin Liu, Dong Du, Tao Yang, Yang Li, Boyu Qiu  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.11373v1  

Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offe...

---

### 5. [STAGE: A Symbolic Tensor grAph GEnerator for distributed AI system co-design](https://arxiv.org/abs/2511.10480)

**Authors**: Changhai Man, Joongun Park, Hanjiang Wu, Huan Xu, Srinivas Sridharan, Tushar Krishna  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.10480v2  

Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies...

---

### 6. [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)

**Authors**: Guangxuan Xiao, Junxian Guo, Kasra Mazaheri, Song Han  
**Category**: cs.CL  
**Published**: 2025-11-17  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2511.11571v1  

Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's pe...

---

### 7. [StochEP: Stochastic Equilibrium Propagation for Spiking Convergent Recurrent Neural Networks](https://arxiv.org/abs/2511.11320)

**Authors**: Jiaqi Lin, Yi Jiang, Abhronil Sengupta  
**Category**: cs.LG  
**Published**: 2025-11-17  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2511.11320v1  

Spiking Neural Networks (SNNs) promise energy-efficient, sparse, biologically inspired computation. Training them with Backpropagation Through Time (BPTT) and surrogate gradients achieves strong performance but remains biologically implausible. Equilibrium Propagation (EP) provides a more local and ...

---

### 8. [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)

**Authors**: Shien Zhu, Samuel Bohl, Robin Oester, Gustavo Alonso  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.10676v1  

Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, ex...

---

### 9. [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)

**Authors**: Omar Erak, Omar Alhussein, Hatem Abou-Zeid, Mehdi Bennis  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2509.09168v2  

Large-scale transformer models have emerged as a powerful tool for semantic communication systems, enabling edge devices to extract rich representations for robust inference across noisy wireless channels. However, their substantial computational demands remain a major barrier to practical deploymen...

---

### 10. [FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models](https://arxiv.org/abs/2511.11505)

**Authors**: Yonatan Dukler, Guihong Li, Deval Shah, Vikram Appia, Emad Barsoum  
**Category**: cs.LG  
**Published**: 2025-11-17  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.11505v1  

Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to...

---

### 11. [SGLP: A Similarity Guided Fast Layer Partition Pruning for Compressing Large Deep Models](https://arxiv.org/abs/2410.14720)

**Authors**: Yuqi Li, Yao Lu, Junhao Dong, Zeyu Dong, Chuanguang Yang, Xin Yin, Yihao Chen, Jianping Gou, Yingli Tian, Tingwen Huang  
**Category**: cs.LG  
**Published**: 2025-11-17  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2410.14720v2  

Layer pruning has emerged as a potent approach to remove redundant layers in the pre-trained network on the purpose of reducing network size and improve computational efficiency. However, existing layer pruning methods mostly overlook the intrinsic connections and inter-dependencies between differen...

---

### 12. [Representation Meets Optimization: Training PINNs and PIKANs for Gray-Box Discovery in Systems Pharmacology](https://arxiv.org/abs/2504.07379)

**Authors**: Nazanin Ahmadi Daryakenari, Khemraj Shukla, George Em Karniadakis  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2504.07379v2  

Physics-Informed Kolmogorov-Arnold Networks (PIKANs) are gaining attention as an effective counterpart to the original multilayer perceptron-based Physics-Informed Neural Networks (PINNs). Both representation models can address inverse problems and facilitate gray-box system identification. However,...

---

### 13. [W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search](https://arxiv.org/abs/2511.11518)

**Authors**: Zhenyu Ding, Yuhao Wang, Tengyue Xiao, Haoying Wang, Guojun Ma, Mingyang Wan, Caigui Jiang, Ning Ding  
**Category**: cs.CL  
**Published**: 2025-11-17  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.11518v1  

Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLH...

---

### 14. [DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition](https://arxiv.org/abs/2511.11000)

**Authors**: HongYu Liu, Junxin Li, Changxi Guo, Hao Chen, Yaqian Huang, Yifu Guo, Huan Yang, Lihua Cai  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.11000v1  

Recognizing speaker intent in long audio dialogues among speakers has a wide range of applications, but is a non-trivial AI task due to complex inter-dependencies in speaker utterances and scarce annotated data. To address these challenges, an end-to-end framework, namely DialogGraph-LLM, is propose...

---

### 15. [Virtual Width Networks](https://arxiv.org/abs/2511.11238)

**Authors**: Seed, Baisheng Li, Banggu Wu, Bole Ma, Bowen Xiao, Chaoyi Zhang, Cheng Li, Chengyi Wang, Chenyin Xu, Chi Zhang, Chong Hu, Daoguang Zan, Defa Zhu, Dongyu Xu, Du Li, Faming Wu, Fan Xia, Ge Zhang, Guang Shi, Haobin Chen, Hongyu Zhu, Hongzhi Huang, Huan Zhou, Huanzhang Dou, Jianhui Duan, Jianqiao Lu, Jianyu Jiang, Jiayi Xu, Jiecao Chen, Jin Chen, Jin Ma, Jing Su, Jingji Chen, Jun Wang, Jun Yuan, Juncai Liu, Jundong Zhou, Kai Hua, Kai Shen, Kai Xiang, Kaiyuan Chen, Kang Liu, Ke Shen, Liang Xiang, Lin Yan, Lishu Luo, Mengyao Zhang, Ming Ding, Mofan Zhang, Nianning Liang, Peng Li, Penghao Huang, Pengpeng Mu, Qi Huang, Qianli Ma, Qiyang Min, Qiying Yu, Renming Pang, Ru Zhang, Shen Yan, Shen Yan, Shixiong Zhao, Shuaishuai Cao, Shuang Wu, Siyan Chen, Siyu Li, Siyuan Qiao, Tao Sun, Tian Xin, Tiantian Fan, Ting Huang, Ting-Han Fan, Wei Jia, Wenqiang Zhang, Wenxuan Liu, Xiangzhong Wu, Xiaochen Zuo, Xiaoying Jia, Ximing Yang, Xin Liu, Xin Yu, Xingyan Bin, Xintong Hao, Xiongcai Luo, Xujing Li, Xun Zhou, Yanghua Peng, Yangrui Chen, Yi Lin, Yichong Leng, Yinghao Li, Yingshuan Song, Yiyuan Ma, Yong Shan, Yongan Xiang, Yonghui Wu, Yongtao Zhang, Yongzhen Yao, Yu Bao, Yuehang Yang, Yufeng Yuan, Yunshui Li, Yuqiao Xian, Yutao Zeng, Yuxuan Wang, Zehua Hong, Zehua Wang, Zengzhi Wang, Zeyu Yang, Zhengqiang Yin, Zhenyi Lu, Zhexi Zhang, Zhi Chen, Zhi Zhang, Zhiqi Lin, Zihao Huang, Zilin Xu, Ziyun Wei, Zuo Wang  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.11238v1  

We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly...

---

### 16. [Efficient Reasoning via Thought-Training and Thought-Free Inference](https://arxiv.org/abs/2511.03408)

**Authors**: Canhui Wu, Qiong Cao, Chao Xue, Wei Xi, Xiaodong He  
**Category**: cs.CL  
**Published**: 2025-11-17  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2511.03408v2  

Recent advances in large language models (LLMs) have leveraged explicit Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most existing methods primarily compress verbose reasoning outputs. These Long-to-Short transformations aim to improve efficiency, but still rely on explic...

---

### 17. [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)

**Authors**: Zhenghao Zhang, Jun Xie, Xingchen Chen, Tao Yu, Hongzhu Yi, Kaixin Xu, Yuanxiang Wang, Tianyu Zong, Xinming Wang, Jiahuan Chen, Guoqing Chao, Feng Chen, Zhepeng Wang, Jungang Xu  
**Category**: cs.LG  
**Published**: 2025-11-17  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.11181v1  

The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, so...

---

### 18. [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)

**Authors**: Jugal Gajjar, Kaustik Ranaware, Kamalasankari Subramaniakuppusamy, Vaibhav Gandhi  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.10842v1  

Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vect...

---

### 19. [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)

**Authors**: Andrew Kiruluta, Priscilla Burity  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.10655v1  

This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with...

---

### 20. [ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation](https://arxiv.org/abs/2511.11483)

**Authors**: Kaishen Wang, Ruibo Chen, Tong Zheng, Heng Huang  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.11483v1  

Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approac...

---

### 21. [Pretrained Joint Predictions for Scalable Batch Bayesian Optimization of Molecular Designs](https://arxiv.org/abs/2511.10590)

**Authors**: Miles Wang-Henderson, Benjamin Kaufman, Edward Williams, Ryan Pederson, Matteo Rossi, Owen Howell, Carl Underkoffler, Narbe Mardirossian, John Parkhill  
**Category**: cs.LG  
**Published**: 2025-11-17  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2511.10590v2  

Batched synthesis and testing of molecular designs is the key bottleneck of drug development. There has been great interest in leveraging biomolecular foundation models as surrogates to accelerate this process. In this work, we show how to obtain scalable probabilistic surrogates of binding affinity...

---

### 22. [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)

**Authors**: Rabimba Karanjai, Lei Xu, Weidong Shi  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.10860v1  

Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel mul...

---

### 23. [PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI](https://arxiv.org/abs/2511.11048)

**Authors**: Sun Jo, Seok Young Hong, JinHyun Kim, Seungmin Kang, Ahjin Choi, Don-Gwan An, Simon Song, Je Hyeong Hong  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.11048v1  

4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical cond...

---

### 24. [NetGent: Agent-Based Automation of Network Application Workflows](https://arxiv.org/abs/2509.00625)

**Authors**: Jaber Daneshamooz, Eugene Vuong, Laasya Koduru, Sanjay Chandrasekaran, Arpit Gupta  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 6.5  
**Type**: replace  
**ArXiv ID**: 2509.00625v2  

We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web a...

---

### 25. [Strada-LLM: Graph LLM for traffic prediction](https://arxiv.org/abs/2410.20856)

**Authors**: Seyed Mohamad Moghadas, Bruno Cornelis, Alexandre Alahi, Adrian Munteanu  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2410.20856v3  

Traffic forecasting is pivotal for intelligent transportation systems, where accurate and interpretable predictions can significantly enhance operational efficiency and safety. A key challenge stems from the heterogeneity of traffic conditions across diverse locations, leading to highly varied traff...

---

### 26. [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)

**Authors**: Qinghui Gong, Xue Yang, Xiaohu Tang  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2506.19891v2  

Efficient and controllable data unlearning in federated learning remains challenging, due to the trade-off between forgetting and retention performance. Especially under non-independent and identically distributed (non-IID) settings, where deep feature entanglement exacerbates this dilemma. To addre...

---

### 27. [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)

**Authors**: Jon Saad-Falcon, Avanika Narayan, Hakki Orhun Akengin, J. Wes Griffin, Herumb Shandilya, Adrian Gamarra Lafuente, Medhya Goel, Rebecca Joseph, Shlok Natarajan, Etash Kumar Guha, Shang Zhu, Ben Athiwaratkun, John Hennessy, Azalia Mirhoseini, Christopher R\'e  
**Category**: cs.AI  
**Published**: 2025-11-17  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.07885v2  

Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active ...

---

### 28. [Fast and Expressive Multi-Token Prediction with Probabilistic Circuits](https://arxiv.org/abs/2511.11346)

**Authors**: Andreas Grivas, Lorenzo Loconte, Emile van Krieken, Piotr Nawrot, Yu Zhao, Euan Wielewski, Pasquale Minervini, Edoardo Ponti, Antonio Vergari  
**Category**: cs.LG  
**Published**: 2025-11-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.11346v1  

Multi-token prediction (MTP) is a prominent strategy to significantly speed up generation in large language models (LLMs), including byte-level LLMs, which are tokeniser-free but prohibitively slow. However, existing MTP methods often sacrifice expressiveness by assuming independence between future ...

---

### 29. [Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning](https://arxiv.org/abs/2511.11402)

**Authors**: Amit Jain, Victor Rodriguez-Fernandez, Richard Linares  
**Category**: cs.LG  
**Published**: 2025-11-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.11402v1  

Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual ast...

---

### 30. [CAT-Net: A Cross-Attention Tone Network for Cross-Subject EEG-EMG Fusion Tone Decoding](https://arxiv.org/abs/2511.10935)

**Authors**: Yifan Zhuang, Calvin Huang, Zepeng Yu, Yongjie Zou, Jiawei Ju  
**Category**: cs.LG  
**Published**: 2025-11-17  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.10935v1  

Brain-computer interface (BCI) speech decoding has emerged as a promising tool for assisting individuals with speech impairments. In this context, the integration of electroencephalography (EEG) and electromyography (EMG) signals offers strong potential for enhancing decoding performance. Mandarin t...

---

## ðŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ðŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ðŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ðŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ðŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
