# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv-rss-equ1ow2e6-maydomines-projects.vercel.app/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-08-19 09:34:52 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [Accelerating Edge Inference for Distributed MoE Models with Latency-Optimized Expert Placement](https://arxiv.org/abs/2508.12851)

**Authors**: Tian Wu, Liming Wang, Zijian Wen, Xiaoxi Zhang, Jingpu Duan, Xianwei Zhang, Jinhang Zuo  
**Category**: cs.DC  
**Published**: 2025-08-19  
**Score**: 7.0

arXiv:2508.12851v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) have become a cornerstone for training and scaling large language models (LLMs), offering substantial gains in model capacity and efficiency through sparse expert activation. However, serving these models remains challenging i...

---

### 2. [Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding](https://arxiv.org/abs/2508.12590)

**Authors**: Jihoon Park, Seungeun Oh, Seong-Lyun Kim  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 6.5

arXiv:2508.12590v1 Announce Type: cross 
Abstract: To address the growing demand for on-device LLM inference in resource-constrained environments, hybrid language models (HLM) have emerged, combining lightweight local models with powerful cloud-based LLMs. Recent studies on HLM have primarily focuse...

---

### 3. [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)

**Authors**: Seonglae Cho, Zekun Wu, Adriano Koshiyama  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 6.0

arXiv:2508.12535v1 Announce Type: cross 
Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation...

---

### 4. [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)

**Authors**: Jinyi Han, Tingyun Li, Shisong Chen, Jie Shi, Xinyi Wang, Guanglei Yue, Jiaqing Liang, Xin Lin, Liqian Wen, Zulong Chen, Yanghua Xiao  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 5.5

arXiv:2508.12040v1 Announce Type: cross 
Abstract: While large language models (LLMs) have demonstrated remarkable performance across diverse tasks, they fundamentally lack self-awareness and frequently exhibit overconfidence, assigning high confidence scores to incorrect predictions. Accurate confi...

---

### 5. [PVChat: Personalized Video Chat with One-Shot Learning](https://arxiv.org/abs/2503.17069)

**Authors**: Yufei Shi, Weilong Yan, Gang Xu, Yumeng Li, Yucheng Chen, Zhenxi Li, Fei Richard Yu, Ming Li, Si Yong Yeo  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 5.0

arXiv:2503.17069v4 Announce Type: replace-cross 
Abstract: Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as "Wilson is receiving chemotherapy" or "Tom is discussing...

---

### 6. [RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards](https://arxiv.org/abs/2508.12165)

**Authors**: Rohit Krishnan, Jon Evans  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 4.5

arXiv:2508.12165v1 Announce Type: new 
Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified Rewards), a framework for training language models using noisy, real-world feedback signals without requiring explicit human verification. Traditional RLHF requires expensive, verif...

---

### 7. [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)

**Authors**: Ananya Singha, Harshita Sahijwani, Walt Williams, Emmanuel Aboah Boateng, Nick Hausman, Miguel Di Luca, Keegan Choudhury, Chaya Binet, Vu Le, Tianwei Chen, Oryan Rokeah Chen, Sulaiman Vesal, Sadid Hasan  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 4.5

arXiv:2508.11715v1 Announce Type: cross 
Abstract: Excel is a pervasive yet often complex tool, particularly for novice users, where runtime errors arising from logical mistakes or misinterpretations of functions pose a significant challenge. While large language models (LLMs) offer promising assist...

---

### 8. [Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges](https://arxiv.org/abs/2506.02048)

**Authors**: Lajos Muzsai, David Imolai, Andr\'as Luk\'acs  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 4.5

arXiv:2506.02048v2 Announce Type: replace-cross 
Abstract: We present 'Random-Crypto', a procedurally generated cryptographic Capture The Flag (CTF) dataset designed to unlock the potential of Reinforcement Learning (RL) for LLM-based agents in security-sensitive domains. Cryptographic reasoning off...

---

### 9. [WeChat-YATT: A Scalable, Simple, Efficient, and Production Ready Training Library](https://arxiv.org/abs/2508.07970)

**Authors**: Junyu Wu, Weiming Chang, Xiaotao Liu, Guanyou He, Tingfeng Xian, Haoqiang Hong, Boqi Chen, Hongtao Tian, Tao Yang, Yunsheng Shi, Feng Lin, Ting Yao, Jiatao Xu  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 4.5

arXiv:2508.07970v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent paradigm for training large language models and multimodal systems. Despite the notable advances enabled by existing RLHF training frameworks, significant challenge...

---

### 10. [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)

**Authors**: Ali Zolnour, Hossein Azadmaleki, Yasaman Haghbin, Fatemeh Taherinezhad, Mohamad Javad Momeni Nezhad, Sina Rashidi, Masoud Khani, AmirSajjad Taleban, Samin Mahdizadeh Sani, Maryam Dadkhah, James M. Noble, Suzanne Bakken, Yadollah Yaghoobzadeh, Abdol-Hossein Vahabie, Masoud Rouhizadeh, Maryam Zolnoori  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 4.5

arXiv:2508.10027v2 Announce Type: replace-cross 
Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five million older adults in the U.S., yet over half remain undiagnosed. Speech-based natural language processing (NLP) offers a promising, scalable approach to detect ear...

---

### 11. [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)

**Authors**: Weize Liu, Yongchi Zhao, Yijia Luo, Mingyu Xu, Jiaheng Liu, Yanan Li, Xiguo Hu, Yuchi Xu, Wenbo Su, Bo Zheng  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.5

arXiv:2508.12726v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often either lack disciplinary bre...

---

### 12. [iFairy: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$](https://arxiv.org/abs/2508.05571)

**Authors**: Feiyu Wang, Guoan Wang, Yihao Zhang, Shengfan Wang, Weitao Li, Bokai Huang, Shimao Chen, Zihan Jiang, Rui Xu, Tong Yang  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.5

arXiv:2508.05571v3 Announce Type: replace-cross 
Abstract: Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust low-bit representations, and is widely recognized as one of the most promising research directions. All current QAT research focu...

---

### 13. [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)

**Authors**: Reza Shirkavand, Shangqian Gao, Peiran Yu, Heng Huang  
**Category**: cs.LG  
**Published**: 2025-08-19  
**Score**: 4.5

arXiv:2508.12491v1 Announce Type: new 
Abstract: We study cost-aware routing for large language models across diverse and dynamic pools of models. Existing approaches often overlook prompt-specific context, rely on expensive model profiling, assume a fixed set of experts, or use inefficient trial-an...

---

### 14. [Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation](https://arxiv.org/abs/2505.16752)

**Authors**: Hao Guo, Erpeng Xue, Lei Huang, Shichao Wang, Xiaolei Wang, Lei Wang, Jinpeng Wang, Sheng Chen  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2505.16752v3 Announce Type: replace-cross 
Abstract: Deep Learning Recommendation Models (DLRMs) often rely on extensive manual feature engineering to improve accuracy and user experience, which increases system complexity and limits scalability of model performance with respect to computation...

---

### 15. [AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up](https://arxiv.org/abs/2505.24584)

**Authors**: Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2505.24584v3 Announce Type: replace-cross 
Abstract: Recent advances in generative AI have accelerated the discovery of novel chemicals and materials. However, scaling these discoveries to industrial production remains a major bottleneck due to the synthesis gap -- the need to develop entirely...

---

### 16. [OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference](https://arxiv.org/abs/2507.03865)

**Authors**: Seungjun Shin, Jaehoon Oh, Dokwan Oh  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2507.03865v2 Announce Type: replace-cross 
Abstract: Attention mechanisms are central to the success of large language models (LLMs), enabling them to capture intricate token dependencies and implicitly assign importance to each token. Recent studies have revealed the sink token, which receive...

---

### 17. [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)

**Authors**: Zhuorui Liu, Chen Zhang, Dawei Song  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2508.12407v1 Announce Type: new 
Abstract: With the rapid development of large language models (LLMs), handling long context has become one of the vital abilities in LLMs. Such long-context ability is accompanied by difficulties in deployment, especially due to the increased consumption of KV ...

---

### 18. [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)

**Authors**: Dayy\'an O'Brien, Bhavitvya Malik, Ona de Gibert, Pinzhen Chen, Barry Haddow, J\"org Tiedemann  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2508.13079v1 Announce Type: new 
Abstract: Existing document-level machine translation resources are only available for a handful of languages, mostly high-resourced ones. To facilitate the training and evaluation of document-level translation and, more broadly, long-context modeling for globa...

---

### 19. [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)

**Authors**: Ziyi Cao, Qingyi Si, Jingbin Zhang, Bingquan Liu  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2508.11661v1 Announce Type: cross 
Abstract: Large language models face significant cost challenges in long-sequence inference. To address this, reusing historical Key-Value (KV) Cache for improved inference efficiency has become a mainstream approach. Recent advances further enhance throughpu...

---

### 20. [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)

**Authors**: Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yinqian Min, Yanzipeng Gao, Wayne Xin Zhao, Ji-Rong Wen  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2508.07534v2 Announce Type: replace 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based feedback to guide LLMs i...

---

### 21. [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)

**Authors**: Haitong Luo, Weiyao Zhang, Suhang Wang, Wenji Zou, Chungang Lin, Xuying Meng, Yujun Zhang  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2508.11343v2 Announce Type: replace 
Abstract: The proliferation of high-quality text from Large Language Models (LLMs) demands reliable and efficient detection methods. While existing training-free approaches show promise, they often rely on surface-level statistics and overlook fundamental s...

---

### 22. [LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning](https://arxiv.org/abs/2406.05881)

**Authors**: Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2406.05881v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable abilities in logical reasoning, in-context learning, and code generation. However, translating natural language instructions into effective robotic control policies remains a significant cha...

---

### 23. [Generalizable LLM Learning of Graph Synthetic Data with Post-training Alignment](https://arxiv.org/abs/2506.00845)

**Authors**: Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xinyun Liu, Yulia Tsvetkov  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2506.00845v3 Announce Type: replace-cross 
Abstract: Previous research has sought to enhance the graph reasoning capabilities of LLMs by supervised fine-tuning on synthetic graph data. While these led to specialized LLMs better at solving graph algorithm problems, we don't need LLMs for shorte...

---

### 24. [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)

**Authors**: Zhihao Zhan, Jianan Zhao, Zhaocheng Zhu, Jian Tang  
**Category**: cs.CL  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2507.00449v2 Announce Type: replace-cross 
Abstract: Efficient long-context modeling remains a critical challenge for natural language processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (S...

---

### 25. [MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models](https://arxiv.org/abs/2508.13148)

**Authors**: Haoyu He, Katrin Renz, Yong Cao, Andreas Geiger  
**Category**: cs.LG  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2508.13148v1 Announce Type: new 
Abstract: Diffusion language models, as a promising alternative to traditional autoregressive (AR) models, enable faster generation and richer conditioning on bidirectional context. However, they suffer from a key discrepancy between training and inference: dur...

---

### 26. [CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based Token Eviction](https://arxiv.org/abs/2504.14051)

**Authors**: Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott  
**Category**: cs.LG  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2504.14051v5 Announce Type: replace 
Abstract: While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training meth...

---

### 27. [Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning](https://arxiv.org/abs/2507.19855)

**Authors**: Aditya Sharma, Ananya Gupta, Chengyu Wang, Chiamaka Adebayo, Jakub Kowalski  
**Category**: cs.LG  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2507.19855v3 Announce Type: replace 
Abstract: Large Language Models (LLMs), despite their advanced linguistic capabilities, fundamentally lack an intuitive understanding of physical dynamics, which limits their effectiveness in real-world scenarios that require causal reasoning. In this paper...

---

### 28. [From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation](https://arxiv.org/abs/2508.10118)

**Authors**: Ke Niu, Haiyang Yu, Zhuofan Chen, Mengyang Zhao, Teng Fu, Bin Li, Xiangyang Xue  
**Category**: cs.LG  
**Published**: 2025-08-19  
**Score**: 4.0

arXiv:2508.10118v2 Announce Type: replace 
Abstract: Computer-Aided Design (CAD) plays a vital role in engineering and manufacturing, yet current CAD workflows require extensive domain expertise and manual modeling effort. Recent advances in large language models (LLMs) have made it possible to gene...

---

### 29. [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)

**Authors**: Szymon Pawlonka, Miko{\l}aj Ma{\l}ki\'nski, Jacek Ma\'ndziuk  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 3.5

arXiv:2508.12026v1 Announce Type: new 
Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual reasoning (AVR), requiring models to identify visual concepts fromjust a few examples and describe them in natural language. Early BP benchmarks featured synthetic black-and-whit...

---

### 30. [AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions](https://arxiv.org/abs/2508.11867)

**Authors**: Mohammad Baqar, Saba Naqvi, Rajat Khanda  
**Category**: cs.AI  
**Published**: 2025-08-19  
**Score**: 3.5

arXiv:2508.11867v1 Announce Type: cross 
Abstract: Modern software delivery has accelerated from quarterly releases to multiple deployments per day. While CI/CD tooling has matured, human decision points interpreting flaky tests, choosing rollback strategies, tuning feature flags, and deciding when ...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative Decoding

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
