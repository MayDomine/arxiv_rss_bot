# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-01-12 06:01:00 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Double: Breaking the Acceleration Limit via Double Retrieval Speculative Parallelism](https://arxiv.org/abs/2601.05524)

**Authors**: Yuhao Shen, Tianyu Liu, Junyi Shen, Jinyang Wu, Quan Kong, Li Huan, Cong Wang  
**Category**: cs.CL  
**Published**: 2026-01-12  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.05524v1  

#### Abstract
Parallel Speculative Decoding (PSD) accelerates traditional Speculative Decoding (SD) by overlapping draft generation with verification. However, it remains hampered by two fundamental challenges: (1) a theoretical speedup ceiling dictated by the speed ratio between the draft and target models, and ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šDoUBLE: Breaking the Acceleration Limit via Double Retrieval Speculative Parallelism**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å½“å‰ä¸»æµçš„ **Parallel Speculative Decoding (PSD)** æ–¹æ³•è™½ç„¶é€šè¿‡å¹¶è¡ŒåŒ– draft ç”Ÿæˆä¸éªŒè¯æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§ç“¶é¢ˆï¼š
1. **ç†è®ºåŠ é€Ÿä¸Šé™é™åˆ¶**ï¼šPSD çš„æœ€å¤§åŠ é€Ÿæ¯”å—é™äº draft æ¨¡å‹ä¸ target æ¨¡å‹ä¹‹é—´çš„å»¶è¿Ÿæ¯” $ C = T_p / T_q $ï¼Œéš¾ä»¥çªç ´è¿™ä¸€â€œé€Ÿåº¦å¤©èŠ±æ¿â€ã€‚
2. **ä¸­æ®µ token è¢«æ‹’å¯¼è‡´çš„è®¡ç®—æµªè´¹**ï¼šå½“ draft åºåˆ—åœ¨ä¸­é—´ä½ç½®è¢«æ‹’ç»æ—¶ï¼Œåç»­æ‰€æœ‰å·²ç”Ÿæˆ token éƒ½éœ€å›æ»šï¼ˆrollbackï¼‰ï¼Œé€ æˆä¸¥é‡çš„ pipeline stall å’Œèµ„æºæµªè´¹ã€‚

æ­¤å¤–ï¼ŒåŸºäºæ£€ç´¢çš„ speculative decoding å­˜åœ¨ **Retrieval Precision-Efficiency Dilemma**ï¼š
- **Target-side retrieval**ï¼šç²¾åº¦é«˜ä½†æ•ˆç‡ä½ï¼Œå› ä»ä¾èµ–æ˜‚è´µçš„ target æ¨¡å‹å‰å‘ä¼ æ’­ï¼›
- **Draft-side retrieval**ï¼šæ•ˆç‡é«˜ä½†ç²¾åº¦å·®ï¼Œå› å°æ¨¡å‹è¯­ä¹‰èƒ½åŠ›å¼±ï¼Œæ£€ç´¢ç»“æœå¸¸è¢«æ‹’ç»ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯**
ä½œè€…æå‡º **DoUBLE (Double Retrieval Speculative Parallelism)**ï¼Œä¸€ç§å…¨æ–°çš„è®­ç»ƒå…è´¹ã€æ— æŸçš„å¹¶è¡Œæ¨æµ‹è§£ç æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº **åŒä¾§åŒæ­¥æ£€ç´¢æœºåˆ¶ï¼ˆdouble retrievalï¼‰**ï¼š

#### **(1) Draft Model ä¾§ï¼šè¿­ä»£å¼æ£€ç´¢ç”Ÿæˆï¼ˆIterative Retrieval Drafterï¼‰**
- ä¸å†ä½¿ç”¨è‡ªå›å½’æ–¹å¼é€ token ç”Ÿæˆ draftï¼Œè€Œæ˜¯è®© draft model æ‰§è¡Œå¤šè½® `Retrieval Forward`ï¼Œä» datastore ä¸­æ£€ç´¢è¿ç»­çŸ­è¯­ï¼ˆphraseï¼‰æ¥æ„å»ºé•¿å€™é€‰é“¾ã€‚
- åœ¨ä¸€ä¸ª target éªŒè¯å‘¨æœŸå†…å¯ç”Ÿæˆè¿œè¶… $ C $ ä¸ª tokenï¼Œä»è€Œæ‰“ç ´ä¼ ç»Ÿ PSD çš„ç†è®ºåŠ é€Ÿä¸Šé™ã€‚

#### **(2) Target Model ä¾§ï¼šä¸»åŠ¨å¼•å¯¼å¼éªŒè¯ï¼ˆTarget-Guided Verificationï¼‰**
- Target model åŒæ­¥æ‰§è¡Œä¸€æ¬¡æ£€ç´¢ï¼Œç”Ÿæˆä¸€ç»„é«˜è´¨é‡çš„å¤š token å¼•å¯¼åºåˆ— $ Y_p $ã€‚
- åˆ©ç”¨è¯¥åºåˆ—å®ç°ï¼š
  - **Multi-token Pre-verify**ï¼šæå‰è¿‡æ»¤ä¸ä¸€è‡´è·¯å¾„ï¼Œé¿å… pipeline stallï¼›
  - **Forward Guidance**ï¼šè‹¥ draft å‡ºé”™ï¼Œç›´æ¥ç”¨ target æ£€ç´¢å‡ºçš„æ­£ç¡® token æ›¿æ¢å¹¶æ‰©å±•è¾“å‡ºï¼Œæ— éœ€å®Œå…¨å›æ»šã€‚

#### **(3) åˆ†å±‚æ•°æ®å­˜å‚¨ï¼ˆHierarchical Datastoreï¼‰**
- ç»“åˆå…ˆéªŒçŸ¥è¯†ï¼ˆæ¥è‡ªé€šç”¨è¯­æ–™å¦‚ ShareGPTï¼‰ä¸åŠ¨æ€æ¨ç†å†å²ï¼Œæ„å»ºè½»é‡çº§ã€é«˜æ•ˆçš„æ··åˆ datastoreã€‚
- ç¼“è§£å†·å¯åŠ¨é—®é¢˜ï¼Œå¹¶æå‡è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | DoUBLE | å…¶ä»–æ–¹æ³•ï¼ˆå¦‚ PEARL, Ouroboros, EAGLE-3ï¼‰ |
|------|--------|------------------------------------------|
| æ˜¯å¦è®­ç»ƒ | âœ… å®Œå…¨æ— éœ€è®­ç»ƒ | âŒ EAGLE-3 ç­‰éœ€å¤§é‡è®­ç»ƒ |
| æ˜¯å¦æœ‰æŸ | âœ… ä¸¥æ ¼ lossless | âœ… å¤šæ•°ä¸º lossless |
| åŠ é€Ÿä¸Šé™ | âœ… å¯çªç ´ $ C $ ä¸Šé™ | âŒ è¢« $ C $ ä¸¥æ ¼é™åˆ¶ |
| æ‹’ç»å¤„ç† | âœ… æ”¯æŒçº æ­£ä¸æ‰©å±•ï¼Œå‡å°‘ rollback | âŒ å¤šæ•°ä¸ºç®€å• reject + rollback |
| æ£€ç´¢ç­–ç•¥ | âœ… åŒä¾§ååŒæ£€ç´¢ï¼Œå…¼é¡¾ç²¾åº¦ä¸æ•ˆç‡ | âŒ å•ä¾§æ£€ç´¢ï¼Œå­˜åœ¨ç²¾åº¦-æ•ˆç‡æƒè¡¡ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
æ¶µç›–ä»£ç ã€æ•°å­¦ã€æ‘˜è¦ã€å¯¹è¯ç­‰å¤šä¸ªä»»åŠ¡ï¼š
- **HumanEval**ï¼šä»£ç ç”Ÿæˆ
- **GSM8K**ï¼šæ•°å­¦æ¨ç†
- **CNN/DM**ï¼šæ–‡æœ¬æ‘˜è¦
- **Alpaca**ï¼šæŒ‡ä»¤éµå¾ª
- **MT-Bench**ï¼šç»¼åˆå¯¹è¯è´¨é‡è¯„ä¼°

---

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**

#### **æ¨¡å‹ç»„åˆ**
æµ‹è¯•äº†å¤šç§ä¸»æµå¤§æ¨¡å‹å¯¹ï¼š
- **LLaMA-2/3**ï¼ˆ7B/70Bï¼‰
- **Deepseek-Coder**ï¼ˆ1.3B/33Bï¼‰
- **Qwen3**ï¼ˆ0.6B/14B/32Bï¼‰

å…¶ä¸­è¾ƒå°æ¨¡å‹ä½œä¸º draft modelï¼Œè¾ƒå¤§æ¨¡å‹ä¸º target modelã€‚

#### **ç¡¬ä»¶ç¯å¢ƒ**
- 8 Ã— NVIDIA A100 (80GB) GPUs
- Greedy samplingï¼ˆä¸»å®éªŒï¼‰ï¼Œä¹Ÿæµ‹è¯•äº† $ T=1.0 $ çš„éšæœºé‡‡æ ·åœºæ™¯

#### **è¯„ä¼°æŒ‡æ ‡**
- **Wall-time Speedup**ï¼šç›¸å¯¹äº vanilla autoregressive æ¨ç†çš„å®é™…æ—¶é—´åŠ é€Ÿæ¯”
- **Mean Accepted Tokens (M)**ï¼šå¹³å‡æ¯è½®æ¥å—çš„ token æ•°é‡ï¼ˆè¡¡é‡ç¨³å®šæ€§ï¼‰
- **Average Matched Tokens (AMT)**ï¼šæ£€ç´¢åŒ¹é…é•¿åº¦æœŸæœ›å€¼ï¼ˆåæ˜ æ£€ç´¢è´¨é‡ï¼‰

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **æ ‡å‡† SD** | Standard SD |
| **Target-side Retrieval** | Lookahead, PLD, Token Recycling (TR) |
| **Draft-side Retrieval** | Ouroboros |
| **Parallel SD** | PEARL, Sps |
| **è®­ç»ƒå‹ SOTA** | EAGLE-3 |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
| æ¨¡å‹å¯¹ | æ–¹æ³• | æœ€é«˜åŠ é€Ÿæ¯”ï¼ˆSpeedupï¼‰ |
|--------|------|---------------------|
| **LLaMA3.3-70B** | **DoUBLE (ours)** | **5.33Ã—** |
| **Qwen3-32B** | **DoUBLE (ours)** | **2.84Ã—** |
| **Deepseek-33B** | **DoUBLE (ours)** | **4.45Ã—** |
| **Qwen3-14B** ($ C \sim 1.6 $) | **DoUBLE (ours)** | **1.89Ã—**ï¼ˆ**çªç ´ç†è®ºä¸Šé™**ï¼‰|

> æ³¨ï¼šå›¾2æ˜¾ç¤ºï¼Œåœ¨ HumanEval å’Œ CNN/DM ä¸Šï¼ŒDoUBLE æ˜¾è‘—ä¼˜äº EAGLE-3ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- åœ¨ **LLaMA3.3-70B** ä¸Šï¼š
  - ç›¸æ¯” **PEARL**ï¼ˆ4.39Ã—ï¼‰æå‡ **21.4%**
  - è¶…è¿‡ **EAGLE-3**ï¼ˆ4.56Ã—ï¼‰è¾¾ **5.33Ã—**
- åœ¨ **Qwen3-32B** ä¸Šï¼š
  - è¾¾åˆ° **2.84Ã—**ï¼Œè¶…è¿‡ EAGLE-3ï¼ˆ2.27Ã—ï¼‰çº¦ **25%**
- åœ¨ä½é€Ÿæ¯”åœºæ™¯ï¼ˆQwen3-14B, $ C=1.6 $ï¼‰ä¸‹ä»å®ç° **1.89Ã—** åŠ é€Ÿï¼Œ**é¦–æ¬¡çªç ´ç†è®ºä¸Šé™ $ C $**

| æ–¹æ³• | å¹³å‡åŠ é€Ÿæ¯”ï¼ˆAcross Benchmarksï¼‰ |
|------|-------------------------------|
| PEARL | ~2.7â€“3.0Ã— |
| EAGLE-3 | ~3.0â€“4.0Ã— |
| **DoUBLE** | **~4.0â€“4.3Ã—** |

---

### **æ¶ˆèå®éªŒç»“æœ**
#### **ç»„ä»¶æœ‰æ•ˆæ€§åˆ†æï¼ˆè§å›¾5ï¼‰**
- **ç§»é™¤ Draft-side Retrieval**ï¼ˆw/o Draftï¼‰ï¼š
  - åŠ é€Ÿæ¯”ä» **5.33Ã— â†’ 4.50Ã—**
  - è¡¨æ˜è¿­ä»£æ£€ç´¢æ˜¯çªç ´ $ C $ ä¸Šé™çš„å…³é”®
- **ç§»é™¤ Target-side Retrieval**ï¼ˆw/o Targetï¼‰ï¼š
  - Mean Accepted Tokens ä» **14.35 â†’ 11.5**
  - éªŒè¯äº† target å¼•å¯¼å¯¹æé«˜æ¥å—ç‡è‡³å…³é‡è¦

#### **å…¶ä»–æ•æ„Ÿæ€§åˆ†æ**
- **æ£€ç´¢æ·±åº¦ $ d $**ï¼š$ d=10 $ ä¸ºæœ€ä¼˜ï¼Œæ›´æ·±ä¼šå¯¼è‡´å‡†ç¡®ç‡ä¸‹é™ï¼ˆå›¾6ï¼‰
- **å…ˆéªŒçŸ¥è¯†åˆå§‹åŒ–è½®æ¬¡ $ K $**ï¼š$ K=10 $ï¼ˆçº¦ 9.5MBï¼‰å³å¯é¥±å’Œæ€§èƒ½å¢ç›Šï¼ˆè¡¨3ï¼‰
- **æ¸©åº¦é‡‡æ · $ T=1.0 $**ï¼šä»ä¿æŒ **3.77Ã—** åŠ é€Ÿï¼Œä¼˜äº EAGLE-3ï¼ˆ3.71Ã—ï¼‰å’Œ PEARLï¼ˆ3.34Ã—ï¼‰ï¼ˆè¡¨2ï¼‰

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **æˆåŠŸæ‰“ç ´ PSD çš„ç†è®ºåŠ é€Ÿä¸Šé™ $ C $**ï¼š
   - é€šè¿‡å°† draft ç”Ÿæˆè§£è€¦ä¸ºæ£€ç´¢æ“ä½œï¼Œæœ‰æ•ˆå°†ç­‰æ•ˆé€Ÿåº¦æ¯”æå‡è‡³ $ C' = C(1+\text{AMT}) $
2. âœ… **è§£å†³ Retrieval Precision-Efficiency Dilemma**ï¼š
   - åŒä¾§æ£€ç´¢ååŒå·¥ä½œï¼šdraft ä¾§è´Ÿè´£é«˜æ•ˆæ‰©å¢é•¿åº¦ï¼Œtarget ä¾§æä¾›ç²¾å‡†å¼•å¯¼
3. âœ… **æ— éœ€è®­ç»ƒå³è¶…è¶Š SOTA è®­ç»ƒæ–¹æ³•**ï¼š
   - åœ¨å¤šä¸ªæ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºéœ€è¦å¤æ‚è®­ç»ƒçš„ EAGLE-3
4. âœ… **å…·å¤‡è‰¯å¥½çš„å·¥ç¨‹å¯è¡Œæ€§**ï¼š
   - æ£€ç´¢å¼€é”€ä»…å æ€»è€—æ—¶ **1.9%**ï¼Œé€šä¿¡ä¸éªŒè¯å¼€é”€å¯æ§ï¼ˆå›¾7ï¼‰

---

### **æ–¹æ³•çš„å±€é™æ€§**
1. **ç»Ÿä¸€ datastore è®¾è®¡å¯èƒ½éæœ€ä¼˜**ï¼š
   - å½“å‰ä½¿ç”¨å…±äº« datastore å’Œå›ºå®šæ£€ç´¢æ·±åº¦ï¼Œæœªé’ˆå¯¹ draft/target æ¨¡å‹åˆ†åˆ«ä¼˜åŒ–
2. **ä¾èµ–å¤–éƒ¨ datastore æ„å»º**ï¼š
   - å°½ç®¡è½»é‡åŒ–ï¼Œä½†ä»éœ€é¢„æ„å»º n-gram ç´¢å¼•ï¼Œå¢åŠ éƒ¨ç½²å¤æ‚æ€§
3. **Python æ¡†æ¶å¸¦æ¥çš„å·¥ç¨‹ç“¶é¢ˆ**ï¼š
   - KV-cache rollbackã€è¿›ç¨‹é—´é€šä¿¡ç­‰åœ¨ transformers åº“ä¸­æ•ˆç‡è¾ƒä½ï¼Œé™åˆ¶äº†å®é™…åŠ é€Ÿæ½œåŠ›

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **æ¨¡å‹å®šåˆ¶åŒ–ä¼˜åŒ–**ï¼š
   - æ¢ç´¢ adaptive retrieval depthã€decoupled datastores
2. **ç³»ç»Ÿçº§é›†æˆ**ï¼š
   - ä¸ vLLMã€SGLangã€Nano-vLLM ç­‰é«˜æ€§èƒ½æ¨ç†å¼•æ“ç»“åˆï¼Œåˆ©ç”¨ CUDA kernel ä¼˜åŒ–é€šä¿¡ä¸ç¼“å­˜ç®¡ç†
3. **æ¢ç´¢ä¸è®­ç»ƒå‹æ–¹æ³•èåˆ**ï¼š
   - å°† DoUBLE ä¸ EAGLE-style è®­ç»ƒ draft model ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½
4. **å• GPU åœºæ™¯ä¸‹çš„é€»è¾‘å¹¶è¡Œä¼˜åŒ–**ï¼š
   - åˆ©ç”¨å¹¶å‘ CUDA stream å®ç°å•å¡å†…çš„ pipeline å¹¶è¡Œ

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **DoUBLE é€šè¿‡â€œåŒæ£€ç´¢â€æœºåˆ¶ï¼Œåœ¨æ— éœ€ä»»ä½•è®­ç»ƒçš„å‰æä¸‹ï¼Œé¦–æ¬¡å®ç°äº†å¯¹ Parallel Speculative Decoding ç†è®ºåŠ é€Ÿæé™çš„çªç ´ï¼Œå¹¶åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¾¾åˆ° SOTA æ€§èƒ½ï¼Œä¸ºé«˜æ•ˆã€æ— æŸçš„å¤§æ¨¡å‹æ¨ç†æä¾›äº†æ–°èŒƒå¼ã€‚**

</details>

---

### 2. [FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching](https://arxiv.org/abs/2601.05684)

**Authors**: Hongyaoxing Gul, Lijuan Hu, Shuzi Niu, Fangfang Liu  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.05684v1  

#### Abstract
Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šFLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿçš„ **Post-Training Quantization (PTQ)** è™½ç„¶èƒ½æœ‰æ•ˆå‹ç¼©å¤§æ¨¡å‹å¹¶åŠ é€Ÿæ¨ç†ï¼Œä½†åœ¨å¼•å…¥ **ä½ç§©è¡¥å¿ï¼ˆlow-rank compensationï¼‰** æ—¶å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- **å›ºå®šç§©ï¼ˆfixed rankï¼‰ç­–ç•¥ä¸çµæ´»**ï¼šä¸åŒå±‚å¯¹ä½ç§©è¿‘ä¼¼çš„æ•æ„Ÿåº¦ä¸åŒï¼Œç»Ÿä¸€ä½¿ç”¨å›ºå®šç§©ä¼šå¯¼è‡´éƒ¨åˆ†å±‚è¿‡åº¦è¡¥å¿ï¼ˆæµªè´¹å­˜å‚¨ï¼‰ï¼Œè€Œå¦ä¸€äº›å±‚è¡¥å¿ä¸è¶³ï¼ˆç²¾åº¦æŸå¤±ï¼‰ã€‚
- **è®¡ç®—å¼€é”€å¤§**ï¼šä¸»æµæ–¹æ³•ä¾èµ– **SVDï¼ˆSingular Value Decompositionï¼‰** è¿›è¡Œä½ç§©åˆ†è§£ï¼Œå…¶æ—¶é—´å¤æ‚åº¦é«˜ï¼Œæ˜¾è‘—æ‹–æ…¢é‡åŒ–è¿‡ç¨‹ã€‚
- **ç¼ºä¹é«˜æ•ˆè¿­ä»£ä¼˜åŒ–æœºåˆ¶**ï¼šç°æœ‰æ–¹æ³•å¤šä¸ºå•æ­¥åˆ†è§£ï¼Œéš¾ä»¥å……åˆ†å‹ç¼©æ®‹å·®è¯¯å·®ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šFLRQ
æœ¬æ–‡æå‡º **Flexible Low-Rank Quantization (FLRQ)**ï¼Œä¸€ç§å¿«é€Ÿã€çµæ´»ä¸”é«˜æ•ˆçš„ä½ç§©é‡åŒ–æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

#### ï¼ˆ1ï¼‰R1-FLRï¼ˆRank-1 Sketch-based Flexible Rank Selectionï¼‰
- åˆ©ç”¨ **R1-Sketch**ï¼ˆåŸºäºé«˜æ–¯æŠ•å½±çš„ç§©-1çŸ©é˜µè‰å›¾æŠ€æœ¯ï¼‰ï¼Œä»éšæœºåŒ– SVDï¼ˆRSVDï¼‰ä¸­æ¨å¯¼å‡ºé«˜æ•ˆçš„ç§©-1è¿‘ä¼¼ç®—æ³•ã€‚
- åŠ¨æ€é€å±‚é€‰æ‹©æœ€ä¼˜ç§©ï¼Œæ— éœ€é¢„å…ˆè®¾å®šæˆ–è°ƒå‚ã€‚
- é€šè¿‡ç›‘æ§æ®‹å·®çŸ©é˜µçš„ `amax`ï¼ˆæœ€å¤§ç»å¯¹å€¼ï¼‰å˜åŒ–è¶‹åŠ¿è‡ªåŠ¨ç»ˆæ­¢ï¼Œå®ç°â€œæŒ‰éœ€æå–â€ã€‚

#### ï¼ˆ2ï¼‰BLCï¼ˆBest Low-rank Approximation under Clippingï¼‰
- åœ¨ **ç¼©æ”¾ï¼ˆscalingï¼‰ä¸å‰ªè£ï¼ˆclippingï¼‰** ç­–ç•¥ä¸‹ï¼Œé€šè¿‡è¿­ä»£æ–¹å¼æœ€å°åŒ–ä½ç§©é‡åŒ–è¯¯å·®ã€‚
- äº¤æ›¿æ›´æ–°ä½ç§©çŸ©é˜µ $W_r$ å’Œé‡åŒ–æ®‹å·® $W_q$ï¼Œé€æ­¥å‹ç¼©æ®‹å·®ç©ºé—´ï¼Œæå‡æœ€ç»ˆç²¾åº¦ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | FLRQä¼˜åŠ¿ |
|------|---------|
| **çµæ´»æ€§** | æ”¯æŒæ¯å±‚åŠ¨æ€é€‰æ‹©æœ€ä¼˜ç§©ï¼Œé¿å…å›ºå®šç§©å¸¦æ¥çš„èµ„æºæµªè´¹æˆ–ç²¾åº¦ä¸è¶³ |
| **æ•ˆç‡** | R1-Sketch ä»…ä½¿ç”¨ BLAS Level-2 æ“ä½œï¼ˆå¦‚ GEMVï¼‰ï¼Œæ¯” SVD å¿«æ•°å€ï¼Œå°¤å…¶åœ¨ 2-bit åœºæ™¯ä¸‹æé€Ÿè¶… 5 å€ |
| **ç²¾åº¦** | åœ¨ 2-bit é‡åŒ–ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äº AWQã€OmniQuant ç­‰ä¸»æµ PTQ æ–¹æ³• |
| **å†…å­˜å¼€é”€ä½** | å¹³å‡é¢å¤–æ¯”ç‰¹ï¼ˆextra bitï¼‰ä»…çº¦ 0.2â€“0.3ï¼Œè¿œä½äº LQERï¼ˆ1.6+ï¼‰ç­‰å›ºå®šé«˜ç§©æ–¹æ³• |
| **æ— éœ€å¾®è°ƒ** | å®Œå…¨å±äº PTQ èŒƒç•´ï¼Œä¸ä¾èµ– LoRA å¾®è°ƒæˆ–å…¶ä»–æ˜‚è´µè®­ç»ƒè¿‡ç¨‹ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **è¯­è¨€å»ºæ¨¡ä»»åŠ¡**ï¼š
  - **WikiText2**ï¼šç”¨äº perplexityï¼ˆPPLï¼‰è¯„ä¼°
  - **C4**ï¼šå¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ï¼Œç”¨äº PPL æµ‹è¯•
- **æ ¡å‡†æ•°æ®é›†ï¼ˆCalibration Setï¼‰**ï¼š
  - ä½¿ç”¨ **WikiText2 ä¸­éšæœºæŠ½å–çš„ 128 ä¸ª 2048 é•¿åº¦ token æ®µ**è¿›è¡Œé‡åŒ–æ ¡å‡†
- **ä¸‹æ¸¸é›¶æ ·æœ¬ä»»åŠ¡ï¼ˆZero-Shot Evaluationï¼‰**ï¼š
  - ARC-Easy / ARC-Challenge
  - BOOLQ
  - OpenBookQA
  - PIQA
  - Winogrande

### å®éªŒè®¾ç½®
- **æ¨¡å‹ç³»åˆ—**ï¼š
  - OPT: 1.3B, 6.7B, 13B
  - LLaMA2: 7B, 13B
  - LLaMA3: 8Bï¼ˆéƒ¨åˆ†å®éªŒï¼‰
- **é‡åŒ–é…ç½®**ï¼š
  - æƒé‡é‡åŒ–ï¼šW2A16, W3A16, W4A16ï¼ˆå³æƒé‡ 2/3/4-bitï¼Œæ¿€æ´» 16-bitï¼‰
  - åˆ†ç»„å¤§å°ï¼ˆgroup sizeï¼‰ï¼š128
  - R1-Sketch è¿­ä»£æ¬¡æ•°ï¼ˆitï¼‰ï¼šé»˜è®¤è®¾ä¸º 2
- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA A100 40GB GPU

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Perplexity (PPLâ†“)** | ä¸»è¦è¯­è¨€æ¨¡å‹è´¨é‡æŒ‡æ ‡ï¼Œè¶Šä½è¶Šå¥½ |
| **Zero-Shot Accuracy (%)â†‘** | ä¸‹æ¸¸ä»»åŠ¡å¹³å‡å‡†ç¡®ç‡ |
| **Quantization Time** | å®Œæ•´æ¨¡å‹é‡åŒ–è€—æ—¶ |
| **Inference Latency / Throughput** | æ¨ç†å»¶è¿Ÿä¸ååé‡å¯¹æ¯” |
| **Extra Bits / Memory Overhead** | ä½ç§©ç»„ä»¶å¼•å…¥çš„é¢å¤–å­˜å‚¨æˆæœ¬ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **RTN (Round-To-Nearest)** | åŸºçº¿ | æœ€ç®€å•çš„å‡åŒ€é‡åŒ– |
| **AWQ** | PTQ | ä¿ç•™é‡è¦é€šé“ï¼Œæ¿€æ´»æ„ŸçŸ¥ç¼©æ”¾ |
| **OmniQuant** | PTQ | å¯å­¦ä¹ å‰ªè£è¾¹ç•Œ |
| **AffineQuant** | PTQ | ç­‰æ•ˆä»¿å°„å˜æ¢ä¼˜åŒ– |
| **LQER** | Low-Rank PTQ | å›ºå®šé«˜ç§©ï¼ˆå¦‚ 256ï¼‰ï¼ŒSVD åˆ†è§£ |
| **CALDERA / RILQ** | Fine-tuning-based | ç»“åˆ LoRA å¾®è°ƒçš„ä½ç§©è¡¥å¿æ–¹æ³• |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 2 & Table 4ï¼‰

#### âœ… è¯­è¨€æ¨¡å‹å›°æƒ‘åº¦ï¼ˆPPL â†“ï¼‰
| æ¨¡å‹ | æ–¹æ³• | WikiText2 (â†“) | C4 (â†“) |
|------|------|-------------|--------|
| OPT-1.3B W4A16 | FLRQ | **14.65** | **14.97** |
| OPT-1.3B W3A16 | FLRQ | **15.53** | **16.07** |
| OPT-1.3B W2A16 | FLRQ | **22.99** | **22.99** |
| LLaMA2-7B W2A16 | FLRQ | **9.14** | **12.10** |

> ğŸ’¡ **è¯´æ˜**ï¼šFLRQ åœ¨æ‰€æœ‰ bit-width ä¸‹å‡è¾¾åˆ°æˆ–è¶…è¶ŠåŸºçº¿ï¼Œå°¤å…¶åœ¨ **2-bit** è¡¨ç°çªå‡ºã€‚

#### âœ… ä¸ LQER å¯¹æ¯”ï¼ˆTable 4ï¼‰
| æ–¹æ³• | Rank | Extra Bit | Wiki2 PPL |
|------|------|-----------|----------|
| LQER (2-bit) | 256 | 1.60 | 10.33 |
| **FLRQ (2-bit)** | **~39** | **0.24** | **9.14** |

> ğŸ”¥ **ç»“è®º**ï¼šFLRQ ä»¥ **æä½çš„ç§©ï¼ˆ39 vs 256ï¼‰å’Œæ›´å°‘çš„é¢å¤–æ¯”ç‰¹ï¼ˆ0.24 vs 1.6ï¼‰** å®ç°äº† **æ›´ä½çš„ PPLï¼ˆ9.14 vs 10.33ï¼‰**ï¼Œè¯æ˜å…¶é«˜æ•ˆæ€§å’Œç²¾åº¦ä¼˜åŠ¿ã€‚

### ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•ç»¼åˆæ¯”è¾ƒï¼ˆTable 5ï¼‰
| æ–¹æ³• | Avg Rank | Extra Bit | LLaMA3-8B PPL (â†“) | Latency â†‘ |
|------|----------|-----------|------------------|-----------|
| Quip# | â€“ | â€“ | 12.74 | Baseline |
| Quip+CALDERA | 256 | 0.4 | 8.87 | +26.7% |
| **FLRQ** | **40** | **0.24** | **14.12** | **+4.8%** |
| FLRQ+RILQ | 56 | 0.36 | 9.78 | +6.5% |

> âš ï¸ å°½ç®¡ CALDERA æ›´ç²¾ç¡®ï¼Œä½†å¸¦æ¥å·¨å¤§å»¶è¿Ÿï¼›FLRQ åœ¨ç²¾åº¦ä¸æ•ˆç‡ä¹‹é—´å–å¾—æ›´å¥½å¹³è¡¡ã€‚

### é›¶æ ·æœ¬ä»»åŠ¡å‡†ç¡®ç‡ï¼ˆTable 6ï¼‰
| æ–¹æ³• | å¹³å‡å‡†ç¡®ç‡ï¼ˆ6é¡¹ä»»åŠ¡ avg â†‘ï¼‰ |
|------|----------------------------|
| FP16 | 63.4% |
| FLRQ (4-bit) | **65.4%** |
| FLRQ (3-bit) | **64.2%** |
| FLRQ (2-bit) | **60.4%** |

> âœ… FLRQ åœ¨ 4-bit å’Œ 3-bit ä¸‹æ¥è¿‘ç”šè‡³è¶…è¿‡ FP16ï¼Œ2-bit ä»ä¿æŒè¾ƒé«˜å¯ç”¨æ€§ã€‚

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰R1-FLR æ¶ˆèï¼ˆTable 9ï¼‰
| æ–¹æ³• | å›ºå®šç§©=64 | avg bit | PPL |
|------|----------|--------|-----|
| FLRQ (no BLC) | â€“ | **4.24** | **4.98** |
| å›ºå®šç§©=64 | âœ“ | 4.44 | 4.98 |

> âœ… FLRQ åœ¨ç›¸åŒ PPL ä¸‹å‡å°‘ **40% é¢å¤–å†…å­˜å¼€é”€**ã€‚

#### ï¼ˆ2ï¼‰BLC æ¶ˆèï¼ˆTable 10ï¼‰
| Bit | æ˜¯å¦å¯ç”¨ BLC | LLaMA2-13B PPL |
|-----|---------------|----------------|
| 2 | Ã— (æ— ) | 1.2e6 |
| 2 | âˆš (æœ‰) | **6.77** |

> â— BLC æå¤§ç¼“è§£äº† 2-bit é‡åŒ–å¤±çœŸé—®é¢˜ï¼Œæ˜¯ä¿è¯ä½æ¯”ç‰¹ç¨³å®šæ€§çš„å…³é”®ã€‚

### é‡åŒ–æ•ˆç‡ï¼ˆTable 8ï¼‰
| æ¨¡å‹ | æ–¹æ³• | é‡åŒ–æ—¶é—´ï¼ˆ2-bitï¼‰ |
|------|------|------------------|
| LLaMA2-13B | AffineQuant | 23.1h |
| LLaMA2-13B | **FLRQ** | **3.8h** (**å¿« 6 å€ä»¥ä¸Š**) |
| OPT-13B | LQER (SVD) | ~12.8h |
| OPT-13B | **FLRQ (R1-Sketch)** | **4.9h** (**å¿« 2.6 å€**) |

> ğŸš€ R1-Sketch æ˜¾è‘—é™ä½è®¡ç®—è´Ÿæ‹…ï¼Œä½¿å¤§è§„æ¨¡ä½ç§©é‡åŒ–çœŸæ­£å®ç”¨ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ä¸åŒå±‚çš„æœ€ä½³ä½ç§©ç§©å·®å¼‚æ˜¾è‘—**ï¼šæŸäº›å±‚åªéœ€å°ç§©å³å¯æ•æ‰ä¸»è¦ç‰¹å¾ï¼Œè€Œå…¶ä»–å±‚éœ€è¦æ›´é«˜ç§©ã€‚**å›ºå®šç§©ç­–ç•¥ä¸¥é‡ä½æ•ˆ**ã€‚
2. **R1-Sketch æ˜¯é«˜æ•ˆä½ç§©ä¼°è®¡çš„å…³é”®**ï¼šç›¸æ¯” SVD æˆ–æˆªæ–­ SVDï¼Œå®ƒèƒ½åœ¨ä¿è¯ç²¾åº¦çš„åŒæ—¶å¤§å¹…æå‡é€Ÿåº¦ï¼Œç‰¹åˆ«é€‚åˆ PTQ ä¸­é¢‘ç¹è°ƒç”¨çš„åœºæ™¯ã€‚
3. **BLC è¿­ä»£æœºåˆ¶æ˜¾è‘—æå‡ä½æ¯”ç‰¹é²æ£’æ€§**ï¼šå°¤å…¶æ˜¯åœ¨ 2-bit é‡åŒ–ä¸­ï¼ŒBLC èƒ½æœ‰æ•ˆæŠ‘åˆ¶é‡åŒ–å™ªå£°ç´¯ç§¯ï¼Œé˜²æ­¢å´©æºƒã€‚
4. **FLRQ å®ç°äº†ç²¾åº¦ä¸æ•ˆç‡çš„å¸•ç´¯æ‰˜å‰æ²¿**ï¼šåœ¨æä½å†…å­˜å¼€é”€ä¸‹ï¼ˆ+0.2â€“0.3 bitï¼‰ï¼Œè·å¾—æ¥è¿‘ç”šè‡³è¶…è¶Šç°æœ‰ SOTA PTQ çš„æ€§èƒ½ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–æ ¡å‡†æ•°æ®åˆ†å¸ƒ**ï¼šè™½ç„¶ä¸éœ€è¦å¾®è°ƒï¼Œä½†ä»éœ€ä»£è¡¨æ€§æ ¡å‡†é›†æ¥æŒ‡å¯¼ç¼©æ”¾å› å­è®¡ç®—ã€‚
- **GPU å®ç°å°šæœªå®Œå…¨ä¼˜åŒ–**ï¼šå°½ç®¡å·²è®¾è®¡èåˆå†…æ ¸ï¼Œä»æœ‰è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´ï¼ˆå¦‚æ”¯æŒæ›´å¤šç¡¬ä»¶åç«¯ï¼‰ã€‚
- **ç†è®ºæœ€ä¼˜ç§©é€‰æ‹©ä»æœ‰æ”¹è¿›ç©ºé—´**ï¼šå½“å‰åŸºäº `amax` å¯å‘å¼åˆ¤æ–­ï¼Œæœªæ¥å¯æ¢ç´¢æ›´ç²¾ç»†çš„è¯¯å·®é¢„æµ‹æ¨¡å‹ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å°† R1-Sketch æ‰©å±•åˆ° **MoE æ¨¡å‹** æˆ– **è§†è§‰ Transformer** ä¸­çš„å¤§çŸ©é˜µå¤„ç†ã€‚
- æ¢ç´¢ **æ··åˆç²¾åº¦ä½ç§©åˆ†é…**ï¼Œç»“åˆ mix-precision quantization å®ç°æ›´ç»†ç²’åº¦æ§åˆ¶ã€‚
- å¼€å‘ **è‡ªåŠ¨åŒ– rank predictor**ï¼ŒåŸºäºå±‚ç»“æ„æˆ–æ¢¯åº¦ä¿¡æ¯é¢„ä¼°æœ€ä½³ç§©ï¼Œå‡å°‘è¿è¡Œæ—¶å¼€é”€ã€‚
- ä¸ **rotation-based quantizationï¼ˆå¦‚ Quip#ï¼‰** ç»“åˆï¼Œæ„å»ºæ›´å¼ºçš„ hybrid quantization pipelineã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **FLRQ é€šè¿‡ R1-Sketch å®ç°å¿«é€Ÿçµæ´»çš„é€å±‚ä½ç§©é€‰æ‹©ï¼Œå¹¶ç»“åˆ BLC è¿­ä»£ä¼˜åŒ–ï¼Œåœ¨å‡ ä¹ä¸å¢åŠ å†…å­˜çš„å‰æä¸‹ï¼Œå¤§å¹…æå‡äº†ä½æ¯”ç‰¹ï¼ˆå°¤å…¶æ˜¯ 2-bitï¼‰PTQ çš„ç²¾åº¦ä¸æ•ˆç‡ï¼Œæ˜¯è¿ˆå‘æè‡´å‹ç¼© LLM çš„é‡è¦ä¸€æ­¥ã€‚**

</details>

---

### 3. [Self-Evolving Distributed Memory Architecture for Scalable AI Systems](https://arxiv.org/abs/2601.05569)

**Authors**: Zixuan Li, Chuanzhen Wang, Haotian Sun  
**Category**: cs.DC  
**Published**: 2026-01-12  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.05569v1  

#### Abstract
Distributed AI systems face critical memory management challenges across computation, communication, and deployment layers. RRAM based in memory computing suffers from scalability limitations due to device non idealities and fixed array sizes. Decentralized AI frameworks struggle with memory efficie...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Self-Evolving Distributed Memory Architecture for Scalable AI Systems*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

åˆ†å¸ƒå¼ AI ç³»ç»Ÿåœ¨ **è®¡ç®—ã€é€šä¿¡ã€éƒ¨ç½²** ä¸‰ä¸ªå±‚é¢é¢ä¸´ä¸¥é‡çš„å†…å­˜ç®¡ç†æŒ‘æˆ˜ï¼š

- **RRAM-based in-memory computing** å—é™äºè®¾å¤‡éç†æƒ³æ€§å’Œå›ºå®šé˜µåˆ—å°ºå¯¸ï¼Œç¼ºä¹å¯æ‰©å±•æ€§ï¼›
- **å»ä¸­å¿ƒåŒ– AI æ¡†æ¶** åœ¨ NAT é™åˆ¶ç½‘ç»œä¸­å› é™æ€è·¯ç”±å¿½ç•¥è®¡ç®—è´Ÿè½½ï¼Œå¯¼è‡´å†…å­˜æ•ˆç‡ä½ä¸‹ï¼›
- **å¤šæ™ºèƒ½ä½“éƒ¨ç½²ç³»ç»Ÿ** å°†åº”ç”¨é€»è¾‘ä¸æ‰§è¡Œç¯å¢ƒç´§å¯†è€¦åˆï¼Œæ— æ³•å®ç°è‡ªé€‚åº”å†…å­˜ä¼˜åŒ–ã€‚

è¿™äº›é—®é¢˜çš„æ ¹æœ¬åŸå› åœ¨äºï¼š**ç¼ºä¹è·¨æ¶æ„å±‚çš„ååŒå†…å­˜ç®¡ç†æœºåˆ¶**ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡º **SEDMA**ï¼ˆSelf-Evolving Distributed Memory Architectureï¼‰ï¼Œä¸€ä¸ªä¸‰å±‚ç»Ÿä¸€æ¡†æ¶ï¼Œå®ç°è·¨è®¡ç®—ã€é€šä¿¡ä¸éƒ¨ç½²å±‚çš„ååŒå†…å­˜ç®¡ç†ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

1. **Memory-Guided Matrix Processing**  
   åŸºäºè®¾å¤‡ç‰¹æ€§åŠ¨æ€åˆ’åˆ†çŸ©é˜µï¼Œåˆ©ç”¨åŒå†…å­˜ç³»ç»Ÿï¼ˆé•¿æœŸè®°å¿† + çŸ­æœŸç»Ÿè®¡ï¼‰æŒ‡å¯¼åˆ†åŒºç­–ç•¥ä¸è¯¯å·®æ ¡æ­£å‚æ•°è°ƒæ•´ã€‚

2. **Adaptive Distributed Communication**  
   æå‡º **memory-aware peer selection**ï¼Œç»¼åˆè€ƒè™‘ç½‘ç»œæ‹“æ‰‘ã€è®¡ç®—èƒ½åŠ›ä¸å†…å­˜å¯ç”¨æ€§ï¼Œå®ç°æ™ºèƒ½å¯¹ç­‰èŠ‚ç‚¹é€‰æ‹©ä¸ç¼“å­˜ç­–ç•¥ã€‚

3. **Dynamic Deployment Optimization**  
   æ”¯æŒè¿è¡Œæ—¶æŒç»­é‡é…ç½®ï¼Œé€šè¿‡åé¦ˆæœºåˆ¶è§¦å‘ç¼–è¯‘å™¨é‡æ–°ç”Ÿæˆéƒ¨ç½²é…ç½®ï¼ˆå¦‚ Kubernetes YAMLï¼‰ï¼Œä¼˜åŒ–èµ„æºåˆ†é…ä¸æ•°æ®å±€éƒ¨æ€§ã€‚

4. **Dual Memory Architecture**  
   ç»´æŠ¤ä¸¤ä¸ªå†…å­˜ç³»ç»Ÿï¼š
   - **Episodic Memory**ï¼šå­˜å‚¨é•¿æœŸæ€§èƒ½æ¨¡å¼ï¼ˆå†å²æˆåŠŸç­–ç•¥ï¼‰ï¼›
   - **Working Memory**ï¼šè·Ÿè¸ªçŸ­æœŸå·¥ä½œè´Ÿè½½ç»Ÿè®¡ï¼ˆå®æ—¶èµ„æºåˆ©ç”¨ç‡ï¼‰ï¼›
   å®ç°â€œè‡ªæˆ‘è¿›åŒ–â€â€”â€”ä»ç»éªŒä¸­å­¦ä¹ å¹¶å¿«é€Ÿå“åº”å³æ—¶å˜åŒ–ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| æ–¹é¢ | SEDMA ä¼˜åŠ¿ |
|------|------------|
| **å†…å­˜åˆ©ç”¨ç‡** | æ˜¾è‘—æå‡è‡³ 87.3%ï¼Œä¼˜äº Ray Distributedï¼ˆ72.1%ï¼‰ |
| **ååé‡** | è¾¾åˆ° 142.5 ops/secï¼Œè¿œè¶… Rayï¼ˆ98.7ï¼‰å’Œ PyTorch Distributedï¼ˆ91.2ï¼‰ |
| **é€šä¿¡å»¶è¿Ÿ** | é™ä½ 30.2%ï¼Œå¹³å‡å»¶è¿Ÿé™è‡³ 171.2ms |
| **ç³»ç»Ÿé€‚åº”æ€§** | éƒ¨ç½²é‡æ„æ—¶é—´ä»å°æ—¶çº§ç¼©çŸ­è‡³åˆ†é’Ÿçº§ï¼ˆ3.2 min vs 47.8 minï¼‰ |
| **é²æ£’æ€§** | èŠ‚ç‚¹æ•…éšœåä»èƒ½ç»´æŒ 89% ååï¼Œæ˜¾è‘—ä¼˜äºé™æ€éƒ¨ç½² |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- **COCO 2017**ï¼šç”¨äºå¤§è§„æ¨¡å›¾åƒå¤„ç†ä»»åŠ¡ï¼Œæµ‹è¯•çŸ©é˜µè¿ç®—ä¸æ¨ç†æ€§èƒ½ï¼›
- **ImageNet**ï¼šè¯„ä¼°å¼‚æ„èŠ‚ç‚¹ä¸Šçš„åˆ†å¸ƒå¼åˆ†ç±»ä»»åŠ¡ï¼›
- **SQuAD**ï¼šæµ‹è¯•åˆ†å¸ƒå¼ NLP æ¨ç†åœºæ™¯ä¸‹çš„å†…å­˜ä¸é€šä¿¡æ•ˆç‡ï¼›
- **åˆæˆå¼ é‡ä¼ è¾“åŸºå‡† & è”é‚¦å­¦ä¹ åœºæ™¯**ï¼šè¯„ä¼°é€šä¿¡å¼€é”€ä¸ç½‘ç»œä¼˜åŒ–èƒ½åŠ›ã€‚

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### å®éªŒå¹³å°
- å¼‚æ„é›†ç¾¤ï¼š6â€“10 ä¸ªèŠ‚ç‚¹ï¼Œé…ç½®ä¸º 8â€“32 CPU æ ¸å¿ƒï¼Œ16â€“128 GB RAMï¼›
- ä½¿ç”¨ **PyTorch 2.0.0**ã€**libp2p-rs 0.52.0** å’Œ **Kubernetes client-go v0.28.0** æ„å»ºï¼›
- å†…å­˜æ¨¡å¼å­˜å‚¨ä½¿ç”¨ **PostgreSQL 14.5**ï¼ˆæ”¯æŒ 10,000 æ¡é•¿æœŸæ¨¡å¼ + 100 æ¡çŸ­æœŸæ ·æœ¬ï¼‰ï¼›
- NAT ç¯å¢ƒæ¨¡æ‹ŸçœŸå®å¤æ‚ç½‘ç»œæ¡ä»¶ã€‚

#### è¯„ä¼°æŒ‡æ ‡
| ç±»åˆ« | æŒ‡æ ‡ |
|------|------|
| **å†…å­˜æ•ˆç‡** | Memory Efficiency (%) |
| **é€šä¿¡æ€§èƒ½** | Communication Latency (ms), Bandwidth Usage (GB/h), Peer Selection Accuracy (%) |
| **ç³»ç»Ÿæ€§èƒ½** | Throughput (ops/sec), Resource Utilization (%) |
| **é€‚åº”æ€§** | Adaptation Time (min), Resilience Score |
| **æ¨¡å‹è´¨é‡** | COCO AP, ImageNet Accuracy, SQuAD F1 |

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| åŸºçº¿æ–¹æ³• | æè¿° |
|--------|------|
| **Ray Distributed** | ä¸»æµåˆ†å¸ƒå¼æ¡†æ¶ï¼Œä»£è¡¨é™æ€è°ƒåº¦ä¸é€šä¿¡æœºåˆ¶ |
| **PyTorch Distributed** | å¹¿æ³›ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ |
| **Static RRAM (Chi et al.)** | å›ºå®šäº¤å‰é˜µåˆ—åˆ†åŒº + é™æ€è¯¯å·®æ ¡æ­£çš„ RRAM è®¡ç®—æ–¹æ¡ˆ |
| **Kubernetes Native Scheduling** | åŸç”Ÿè°ƒåº¦å™¨ï¼Œæ— è¿è¡Œæ—¶é‡é…ç½®èƒ½åŠ› |
| **libp2p Baseline** | åŸºäº DHT çš„æ ‡å‡†å»ä¸­å¿ƒåŒ–é€šä¿¡åè®® |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1ï¼‰**

| Method | Mem. Eff. (%) | Comm. Lat. (ms) | Thpt. (ops/s) | COCO Acc. (%) | ImageNet (%) | SQuAD F1 |
|--------|----------------|------------------|---------------|----------------|--------------|----------|
| Ray Distributed | 72.1 | 245.3 | 98.7 | 84.2 | 76.8 | 88.4 |
| PyTorch Distributed | 69.8 | 267.1 | 91.2 | 83.7 | 75.9 | 87.9 |
| Static RRAM | 70.6 | 298.4 | 98.9 | 84.0 | 76.2 | 88.1 |
| **Ours (SEDMA)** | **87.3** | **171.2** | **142.5** | **85.1** | **78.3** | **89.7** |

> âœ… **å†…å­˜åˆ©ç”¨ç‡æå‡ 21.1%**ï¼ˆ87.3% vs 72.1%ï¼‰  
> âœ… **é€šä¿¡å»¶è¿Ÿé™ä½ 30.2%**ï¼ˆ171.2ms vs 245.3msï¼‰  
> âœ… **ååé‡æé«˜ 44.3%**ï¼ˆ142.5 vs 98.7 ops/sï¼‰

---

### **ç³»ç»Ÿé€‚åº”æ€§è¡¨ç°ï¼ˆè§ Table 2ï¼‰**

| Method | Adapt. (min) | Res. Util. (%) | Resil. Score | Cache Hit (%) | BW (GB/h) | Peer Sel. (%) |
|--------|---------------|----------------|---------------|----------------|-----------|----------------|
| Kubernetes Native | 47.8 | 64.2 | 6.7 | 58.3 | 12.4 | 71.2 |
| Static Deployment | 52.1 | 61.9 | 6.1 | 55.7 | 13.8 | 68.9 |
| libp2p Baseline | 41.2 | 67.3 | 7.2 | 62.1 | 11.7 | 74.6 |
| **Ours** | **3.2** | **82.7** | **9.4** | **78.9** | **7.3** | **86.2** |

> âœ… **éƒ¨ç½²é€‚åº”æ—¶é—´å‡å°‘ 93.3%**ï¼ˆ3.2 min vs 47.8 minï¼‰  
> âœ… **èµ„æºåˆ©ç”¨ç‡æå‡è‡³ 82.7%**  
> âœ… **å¸¦å®½æ¶ˆè€—ä¸‹é™ 40%+**

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰**

#### è¡¨ 3ï¼šç§»é™¤å†…å­˜ç»„ä»¶çš„å½±å“

| Variant | Mem. Eff. (%) | Adapt. (min) | Thpt. (ops/s) |
|--------|----------------|---------------|----------------|
| Full Model | 87.3 | 3.2 | 142.5 |
| w/o Long-term Memory | 71.4 | 8.7 | 118.3 |
| w/o Short-term Memory | 79.2 | 5.1 | 128.9 |

> ğŸ” **é•¿æœŸè®°å¿†ç¼ºå¤±å¯¼è‡´æ•ˆç‡ä¸‹é™ 15.9%**ï¼Œè¯´æ˜å†å²ç»éªŒå¯¹ä¼˜åŒ–è‡³å…³é‡è¦ã€‚

#### è¡¨ 4ï¼šç§»é™¤é«˜å±‚æ¨¡å—çš„å½±å“

| Variant | Latency (ms) | Res. Util. (%) | Resil. Score |
|--------|---------------|----------------|---------------|
| Full Model | 171.2 | 82.7 | 9.4 |
| w/o Adaptive Peer Sel. | 234.8 | 68.9 | 7.1 |
| w/o Dynamic Recomp. | 189.3 | 71.2 | 6.8 |

> ğŸ” ç§»é™¤è‡ªé€‚åº”å¯¹ç­‰é€‰æ‹©ä½¿å»¶è¿Ÿå¢åŠ  **37.1%**ï¼ŒéªŒè¯å…¶åœ¨é€šä¿¡ä¼˜åŒ–ä¸­çš„å…³é”®ä½œç”¨ã€‚

#### è¡¨ 5ï¼šä½å±‚å®ç°ç»†èŠ‚å¯¹æ¯”

| Variant | Acc. (%) | Mem. Eff. (%) | Conv. (epochs) |
|--------|-----------|----------------|----------------|
| Full Model | 85.1 | 87.3 | 45.2 |
| Fixed Î»=10â»Â¹Â² | 83.7 | 82.1 | 52.8 |
| Static Learning Rate | 84.2 | 84.6 | 48.9 |

> ğŸ” è‡ªé€‚åº”æ­£åˆ™åŒ–å‚æ•°ï¼ˆÎ»ï¼‰å¸¦æ¥ **1.4% å‡†ç¡®ç‡æå‡** å’Œæ›´å¿«æ”¶æ•›ã€‚

#### è¡¨ 6ï¼šè·¯ç”±ç­–ç•¥å¯¹æ¯”

| Variant | Peer Sel. Acc. (%) | Cache Hit (%) | BW (GB/h) |
|--------|----------------------|----------------|------------|
| Full Model | 86.2 | 78.9 | 7.3 |
| DHT-only Routing | 74.6 | 62.1 | 11.7 |
| Random Peer Sel. | 68.3 | 55.4 | 13.2 |

> ğŸ” DHT-only è·¯ç”±å¸¦å®½å¤šæ¶ˆè€— **60%**ï¼Œè¯æ˜ memory-aware routing æ›´é«˜æ•ˆã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **è·¨å±‚ååŒå†…å­˜ç®¡ç†æ˜¯æå‡åˆ†å¸ƒå¼ AI æ€§èƒ½çš„å…³é”®**  
   å•ç‹¬ä¼˜åŒ–è®¡ç®—ã€é€šä¿¡æˆ–éƒ¨ç½²æ— æ³•è§£å†³æ ¹æœ¬ç“¶é¢ˆï¼Œå¿…é¡»æ‰“é€šä¸‰å±‚è¿›è¡Œè”åˆä¼˜åŒ–ã€‚

2. **åŒå†…å­˜æ¶æ„æœ‰æ•ˆå¹³è¡¡â€œå­¦ä¹ â€ä¸â€œå“åº”â€**  
   - é•¿æœŸè®°å¿†ï¼ˆepisodic memoryï¼‰ç§¯ç´¯ç»éªŒï¼Œæ”¯æŒé¢„æµ‹æ€§ä¼˜åŒ–ï¼›
   - çŸ­æœŸè®°å¿†ï¼ˆworking memoryï¼‰æ•æ‰ç¬æ—¶å˜åŒ–ï¼Œä¿éšœç³»ç»Ÿæ•æ·æ€§ã€‚

3. **åŠ¨æ€é‡é…ç½®æ˜¾è‘—æå‡ç³»ç»Ÿé€‚åº”æ€§ä¸é²æ£’æ€§**  
   åœ¨è´Ÿè½½çªå˜æˆ–èŠ‚ç‚¹å¤±æ•ˆæ—¶ï¼Œå¯åœ¨æ•°åˆ†é’Ÿå†…å®Œæˆéƒ¨ç½²é‡æ„ï¼Œç»´æŒé«˜ååè¿è¡Œã€‚

4. **memory-aware peer selection å¤§å¹…é™ä½é€šä¿¡å¼€é”€**  
   ç»“åˆè®¡ç®—èƒ½åŠ›ã€å†…å­˜çŠ¶æ€ä¸ç½‘ç»œå»¶è¿Ÿé€‰æ‹©ç›®æ ‡èŠ‚ç‚¹ï¼Œé¿å…çƒ­ç‚¹æ‹¥å¡ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

1. **ä¾èµ–å¤–éƒ¨æ•°æ®åº“ï¼ˆPostgreSQLï¼‰å­˜å‚¨é•¿æœŸè®°å¿†**ï¼Œå¯èƒ½æˆä¸ºå•ç‚¹ç“¶é¢ˆï¼›
2. **åˆå§‹åŒ–é˜¶æ®µéœ€ä¸€å®šè®­ç»ƒ/è§‚å¯Ÿæ—¶é—´** æ‰èƒ½ç§¯ç´¯æœ‰æ•ˆæ¨¡å¼ï¼Œå†·å¯åŠ¨æ€§èƒ½ç•¥ä½ï¼›
3. **å½“å‰å®ç°åœ¨è¶…å¤§è§„æ¨¡é›†ç¾¤ï¼ˆ>100 èŠ‚ç‚¹ï¼‰çš„æ‰©å±•æ€§å°šæœªéªŒè¯**ï¼›
4. **å¯¹æç«¯ç½‘ç»œåˆ†åŒºï¼ˆnetwork partitionï¼‰çš„æ”¯æŒä»æœ‰é™**ï¼Œè™½æœ‰å†—ä½™æœºåˆ¶ä½†ä»ä¼šé™çº§ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **è½»é‡åŒ–å†…å­˜å­˜å‚¨æœºåˆ¶**ï¼šæ¢ç´¢åŸºäºå‘é‡æ•°æ®åº“æˆ–åµŒå…¥ç´¢å¼•æ›¿ä»£ä¼ ç»Ÿ DB æŸ¥è¯¢ï¼›
2. **å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„æƒé‡è‡ªé€‚åº”æœºåˆ¶**ï¼šè‡ªåŠ¨å­¦ä¹  `w_j` æ›´æ–°è§„åˆ™ï¼Œè€Œéä¾èµ–æ¢¯åº¦ä¼°ç®—ï¼›
3. **æ”¯æŒæ›´å¤šç¡¬ä»¶åç«¯**ï¼šæ‰©å±•è‡³ SRAMã€PCMã€Flash-based CIM æ¶æ„ï¼›
4. **é›†æˆå®‰å…¨ä¸éšç§æœºåˆ¶**ï¼šåœ¨è”é‚¦å­¦ä¹ ç­‰åœºæ™¯ä¸­ç»“åˆ Differential Privacyï¼›
5. **è‡ªåŠ¨åŒ– workload åˆ†ç±»å™¨å‡çº§**ï¼šå¼•å…¥åœ¨çº¿å­¦ä¹ æœºåˆ¶æŒç»­æ›´æ–° `CLASSIFYWORKLOAD()`ã€‚

---

## æ€»ç»“

SEDMA æå‡ºäº†ä¸€ç§å…¨æ–°çš„ **self-evolving** åˆ†å¸ƒå¼å†…å­˜æ¶æ„ï¼Œé¦–æ¬¡å®ç°äº† **è®¡ç®—ã€é€šä¿¡ã€éƒ¨ç½²ä¸‰å±‚çš„ååŒå†…å­˜ç®¡ç†**ã€‚é€šè¿‡ **åŒå†…å­˜ç³»ç»Ÿ + åŠ¨æ€é‡é…ç½®æœºåˆ¶**ï¼Œåœ¨å¤šä¸ªçœŸå® benchmark ä¸Šå®ç°äº†æ˜¾è‘—æ€§èƒ½è¶…è¶Šï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„é€‚åº”æ€§ä¸é²æ£’æ€§ã€‚è¯¥å·¥ä½œä¸ºä¸‹ä¸€ä»£å¯æ‰©å±• AI ç³»ç»Ÿæä¾›äº†é‡è¦çš„æ¶æ„èŒƒå¼å‚è€ƒã€‚

</details>

---

### 4. [Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization](https://arxiv.org/abs/2601.05955)

**Authors**: Yuliang Chen, Xi Lin, Jun Wu, Xiangrui Cai, Qiaolun Zhang, Xichun Fan, Jiapeng Xu, Xiu Su  
**Category**: cs.DC  
**Published**: 2026-01-12  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.05955v1  

#### Abstract
Federated Domain Generalization (FDG) aims to collaboratively train a global model across distributed clients that can generalize well on unseen domains. However, existing FDG methods typically struggle with cross-client data heterogeneity and incur significant communication and computation overhead...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹ **Federated Domain Generalization (FDG)** ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼š
- **è·¨å®¢æˆ·ç«¯æ•°æ®å¼‚æ„æ€§**ï¼ˆcross-client data heterogeneityï¼‰å¯¼è‡´æ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°æœªè§åŸŸï¼ˆunseen domainsï¼‰ï¼›
- ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºåœ¨å®¢æˆ·ç«¯ä¹‹é—´å…±äº«é¢†åŸŸç›¸å…³ç‰¹å¾æˆ–é£æ ¼ä¿¡æ¯ï¼Œå¸¦æ¥**éšç§æ³„éœ²é£é™©**å’Œ**é«˜é€šä¿¡å¼€é”€**ï¼›
- å¤æ‚çš„é¢†åŸŸé€‚åº”æ¨¡å—å¯¼è‡´**è®¡ç®—æˆæœ¬é«˜æ˜‚**ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šFaST-PT
ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ FDG æ¡†æ¶â€”â€”**FaST-PT**ï¼ˆFederated Multi-Modal Style Transfer with Prompt Tuningï¼‰ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç»“åˆ **CLIP çš„å›¾æ–‡å¯¹é½èƒ½åŠ›** å’Œ **è½»é‡çº§ Prompt Tuning èŒƒå¼**ï¼Œå®ç°é«˜æ•ˆã€å®‰å…¨çš„è”é‚¦åŸŸæ³›åŒ–ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š
1. **Multi-Modal Style Transfer (MST)**  
   - åˆ©ç”¨ CLIP çš„æ–‡æœ¬ç›‘ç£ï¼Œåœ¨åµŒå…¥ç©ºé—´ä¸­è¿›è¡Œå›¾åƒåµŒå…¥çš„é£æ ¼è¿ç§»ã€‚
   - å®¢æˆ·ç«¯é€šè¿‡æ–‡æœ¬æè¿°ï¼ˆå¦‚ `"a sketch of a {}"`ï¼‰æŒ‡å¯¼æœ¬åœ°å›¾åƒå‘å¤–éƒ¨åŸŸé£æ ¼è½¬æ¢ï¼Œä»è€Œæ‰©å±•è®­ç»ƒåˆ†å¸ƒï¼Œç¼“è§£åŸŸåç§»ã€‚
   - ä¸éœ€å…±äº«åŸå§‹å›¾åƒæˆ–æ•æ„Ÿç‰¹å¾ï¼Œä»…ä¾èµ–éæ•æ„Ÿæ–‡æœ¬æè¿°ï¼Œå¢å¼ºéšç§ä¿æŠ¤ã€‚

2. **Dual-Prompt Moduleï¼ˆåŒæç¤ºæ¨¡å—ï¼‰**
   - å°† Prompt åˆ†è§£ä¸ºï¼š
     - **Global Prompts**ï¼šèšåˆæ¥è‡ªå„å®¢æˆ·ç«¯å¢å¼ºæ•°æ®çš„çŸ¥è¯†ï¼Œæ•æ‰é€šç”¨è¯­ä¹‰ã€‚
     - **Domain Prompts**ï¼šä¿ç•™æœ¬åœ°æ•°æ®ä¸­çš„é¢†åŸŸç‰¹å¼‚æ€§çŸ¥è¯†ã€‚
   - å®ç°ç‰¹å¾è§£è€¦ï¼ˆfeature disentanglementï¼‰ï¼Œç®€åŒ–ä¼ ç»Ÿ DG ä¸­å¤æ‚çš„æ­£åˆ™åŒ–è®¾è®¡ã€‚

3. **Domain-aware Prompt Generation (DPG)**
   - å¼•å…¥ä¸€ä¸ªè½»é‡çº§åŸŸåˆ†ç±»å™¨ `f(Â·)`ï¼Œé¢„æµ‹è¾“å…¥æ ·æœ¬çš„â€œä¼ªåŸŸæ ‡ç­¾â€ã€‚
   - åŠ¨æ€åŠ æƒå¤šä¸ª Domain Promptsï¼Œç”Ÿæˆé€‚åº”å½“å‰æ ·æœ¬çš„åŠ¨æ€ Promptï¼Œæå‡å¯¹æœªè§åŸŸçš„é€‚åº”èƒ½åŠ›ã€‚

4. **ç»Ÿä¸€åŸºäº CLIP çš„æ¶æ„**
   - æ‰€æœ‰æ“ä½œå‡æ„å»ºäºå†»ç»“çš„ CLIP æ¨¡å‹ä¹‹ä¸Šï¼Œæ— éœ€é¢å¤–éƒ¨ç½²å¤æ‚æ¨¡å‹ï¼Œé™ä½å†…å­˜ä¸è®¡ç®—è´Ÿæ‹…ã€‚
   - ä»…ä¼ è¾“ Prompt å‚æ•°ï¼ˆæå°è§„æ¨¡ï¼‰ï¼Œæ˜¾è‘—å‡å°‘é€šä¿¡å¼€é”€ã€‚

#### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | FaST-PT | ä¼ ç»Ÿ FDG æ–¹æ³•ï¼ˆå¦‚ ELCFS, CCSTï¼‰ |
|------|--------|-------------------------------|
| éšç§æ€§ | é«˜ï¼ˆä»…å…±äº«æ–‡æœ¬æè¿°ï¼‰ | ä½ï¼ˆéœ€å…±äº«é¢‘è°±/é£æ ¼ç‰¹å¾ï¼‰ |
| é€šä¿¡æ•ˆç‡ | æé«˜ï¼ˆä»…ä¼  Promptï¼‰ | ä½ï¼ˆä¼ å®Œæ•´æ¨¡å‹æˆ–å¤§ç‰¹å¾ï¼‰ |
| è®¡ç®—å¼€é”€ | ä½ï¼ˆå†»ç»“ä¸»å¹² + è½»é‡æ¨¡å—ï¼‰ | é«˜ï¼ˆé¢å¤–ç¼–ç å™¨/è§£ç å™¨ï¼‰ |
| æ³›åŒ–æ€§èƒ½ | SOTA | æœ‰é™ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
åœ¨å››ä¸ªä¸»æµè·¨åŸŸåŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼š
- **PACS**ï¼šPhoto, Art-painting, Cartoon, Sketchï¼ˆ4 domainsï¼‰
- **VLCS**ï¼šVOC2007, LabelMe, Caltech101, SUN09ï¼ˆ4 domainsï¼‰
- **OfficeHome**ï¼šArt, Clipart, Product, Real-Worldï¼ˆ4 domainsï¼‰
- **DomainNet**ï¼šClipart, Infograph, Painting, Quickdraw, Real, Sketchï¼ˆ6 domainsï¼‰

é‡‡ç”¨ **leave-one-domain-out** åè®®ï¼šæ¯æ¬¡å°†ä¸€ä¸ªåŸŸä½œä¸ºç›®æ ‡åŸŸï¼ˆunseenï¼‰ï¼Œå…¶ä½™ä½œä¸ºæºåŸŸç”¨äºè®­ç»ƒã€‚

### å®éªŒè®¾ç½®
- **æ¨¡å‹ä¸»å¹²**ï¼šOpenAI çš„ CLIP-ViT-L/14ï¼ˆå›¾åƒä¸æ–‡æœ¬ç¼–ç å™¨å‡ä¸º 768 ç»´ï¼‰
- **å®¢æˆ·ç«¯æ•°é‡**ï¼šK = 4ï¼ˆæ¯ä¸ªåŸŸè§†ä¸ºä¸€ä¸ªå®¢æˆ·ç«¯ï¼‰
- **è®­ç»ƒè½®æ•°**ï¼šR = 20ï¼›æœ¬åœ°è®­ç»ƒ epochï¼šEc = EL = 5
- **ä¼˜åŒ–å™¨**ï¼š
  - MST é˜¶æ®µï¼šAdamï¼Œlr=1e-3
  - Prompt è®­ç»ƒé˜¶æ®µï¼šSGDï¼Œlr=0.005ï¼ˆpromptï¼‰ï¼Œlr=0.01ï¼ˆdomain classifierï¼‰
- **Batch Size**ï¼š16ï¼›Prompt Lengthï¼š4
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå¹³å‡å‡†ç¡®ç‡ï¼ˆAverage Accuracy %ï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
åˆ†ä¸ºä¸‰ç±»ï¼š
1. **ä¼ ç»Ÿ FL æ–¹æ³•**ï¼š
   - FedAvg, FedProx
2. **ç»å…¸ FDG æ–¹æ³•**ï¼š
   - ELCFS, FedSR, FedADG, FedDG-GA
3. **åŸºäº Prompt çš„ FL æ–¹æ³•**ï¼š
   - PromptFL, FedAPT, DiPrompt

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 1 & Table 2ï¼‰

| æ–¹æ³• | PACS (%) | VLCS (%) | OfficeHome (%) | DomainNet (%) |
|------|----------|----------|----------------|---------------|
| FedAvg | 78.48 | 75.79 | 85.30 | 72.22 |
| FedDG-GA | 84.25 | 77.47 | 90.11 | 80.17 |
| DiPrompt (SOTA) | 94.86 | 84.08 | 95.94 | 86.84 |
| **FaST-PT (Ours)** | **98.16** | **85.52** | **97.48** | **89.17** |

> âœ… åœ¨æ‰€æœ‰å››ä¸ªæ•°æ®é›†ä¸­å‡è¾¾åˆ° SOTA æ€§èƒ½ï¼

#### è¯¦ç»†è¡¨ç°äº®ç‚¹ï¼š
- åœ¨ **PACS** ä¸Šè¶…è¶Š DiPrompt **+3.30%**
- åœ¨ **DomainNet** ä¸Šè¶…è¶Š DiPrompt **+2.33%**
- åœ¨æ€»å…± **18 ä¸ªè·¨åŸŸä»»åŠ¡**ä¸­ï¼ŒFaST-PT åœ¨ **17 é¡¹ä»»åŠ¡ä¸­å–å¾—æœ€ä½³æ€§èƒ½**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- ç›¸æ¯”ä¼ ç»Ÿ FLï¼ˆFedAvgï¼‰å’Œæ—©æœŸ FDG æ–¹æ³•ï¼ˆELCFS, FedSRï¼‰ï¼ŒFaST-PT æ˜¾è‘—æå‡äº†æ³›åŒ–èƒ½åŠ›ï¼ˆ+10%~20%ï¼‰ã€‚
- ç›¸æ¯”æœ€å…ˆè¿›çš„ Prompt-based æ–¹æ³•ï¼ˆDiPromptï¼‰ï¼Œä»ä¿æŒæ˜æ˜¾ä¼˜åŠ¿ï¼Œè¯´æ˜ MST å’Œ DPG çš„æœ‰æ•ˆæ€§ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 3ï¼‰

| å˜ä½“ | A | C | P | S | Avg |
|------|----|----|----|----|-----|
| â‘  Only Global Prompt | 91.21 | 90.94 | 94.54 | 93.61 | 92.58 |
| â‘¡ Only Domain Prompt | 85.48 | 83.16 | 90.79 | 91.25 | 87.67 |
| â‘¢ w/o DPG | 92.43 | 94.14 | 95.81 | 95.13 | 94.38 |
| â‘£ w/o MST | 96.03 | 97.44 | 98.98 | 95.77 | 97.06 |
| â‘¤ w/o L_con | 97.74 | 99.09 | 99.48 | 95.48 | 97.95 |
| â‘¥ +Target Text | 97.93 | 99.68 | 99.85 | 96.27 | 98.42 |
| **FaST-PT (Full)** | **97.32** | **99.31** | **99.93** | **96.09** | **98.16** |

#### æ¶ˆèåˆ†æç»“è®ºï¼š
- ç§»é™¤ MST å¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ â†’ **MST å¯¹ç¼“è§£åŸŸåç§»è‡³å…³é‡è¦**
- ç§»é™¤ DPG æˆ–å•ä¸€ Prompt ç±»å‹ä¼šå‰Šå¼±æ³›åŒ–èƒ½åŠ› â†’ **Dual-Prompt + DPG è®¾è®¡æœ‰æ•ˆèåˆçŸ¥è¯†**
- åŠ å…¥ç›®æ ‡åŸŸæ–‡æœ¬ä»…å¾®å¼±æå‡ï¼ˆ+0.24%ï¼‰â†’ è¡¨æ˜ FaST-PT å·²å…·å¤‡å¼ºæ¨ç†èƒ½åŠ›ï¼Œä¸ä¾èµ–çœŸå®ç›®æ ‡åŸŸä¿¡æ¯

### å…¶ä»–å®éªŒæ”¯æŒ
- **Few-shot å®éªŒ**ï¼ˆFigure 4ï¼‰ï¼šåœ¨ shots=1~32 ä¸‹ï¼ŒFaST-PT å§‹ç»ˆä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ˜¾ç¤ºå…¶åœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚
- **å¯è§†åŒ– MST æ•ˆæœ**ï¼ˆFigure 2ï¼‰ï¼šNearest Neighbor åˆ†æè¡¨æ˜ï¼Œé£æ ¼è¿ç§»åå›¾åƒè™½å¤–è§‚å˜åŒ–ï¼Œä½†è¯­ä¹‰ä¸€è‡´ï¼ŒéªŒè¯äº† MST çš„æœ‰æ•ˆæ€§ã€‚
- **Prompt Length å½±å“**ï¼ˆFigure 3ï¼‰ï¼šé•¿åº¦ä¸º 4 æ—¶æ€§èƒ½æœ€ä¼˜ï¼Œè¿‡é•¿åè€Œå¼•å…¥å™ªå£°ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **MST æ˜¯ä¸€ç§é«˜æ•ˆä¸”å®‰å…¨çš„æœ¬åœ°æ•°æ®å¢å¼ºæ‰‹æ®µ**ï¼šåˆ©ç”¨ CLIP çš„å›¾æ–‡å¯¹é½èƒ½åŠ›ï¼Œåœ¨åµŒå…¥å±‚é¢å®Œæˆé£æ ¼è¿ç§»ï¼Œé¿å…å…±äº«æ•æ„Ÿæ•°æ®ã€‚
2. **Prompt Partitioning å¯è‡ªç„¶å®ç°ç‰¹å¾è§£è€¦**ï¼šGlobal/Domain Prompts åˆ†åˆ«æ•è·å…±æ€§å’Œä¸ªæ€§çŸ¥è¯†ï¼Œç®€åŒ–äº†ä¼ ç»Ÿ DG çš„å¤æ‚å»ºæ¨¡ã€‚
3. **DPG å®ç°äº†åŸºäºç›¸ä¼¼æ€§çš„çŸ¥è¯†èåˆ**ï¼šé€šè¿‡åŸŸåˆ†ç±»å™¨åŠ¨æ€ç»„åˆ Domain Promptsï¼Œå¢å¼ºäº†å¯¹æœªçŸ¥åŸŸçš„é€‚åº”èƒ½åŠ›ã€‚
4. **FaST-PT åœ¨æ€§èƒ½ã€æ•ˆç‡ã€éšç§ä¸‰æ–¹é¢å–å¾—å¹³è¡¡**ï¼š
   - æ€§èƒ½ SOTA
   - é€šä¿¡é‡æä½ï¼ˆä»…ä¼  ~5K å‚æ•°ï¼‰
   - è®¡ç®—å¼€é”€å°ï¼ˆFLOPs æ¥è¿‘ FedAvgï¼‰

### æ–¹æ³•çš„å±€é™æ€§
- ä¾èµ–é¢„è®­ç»ƒ CLIP æ¨¡å‹çš„è´¨é‡å’Œé¢†åŸŸè¦†ç›–èŒƒå›´ï¼›
- MST çš„æ•ˆæœå—é™äºå¤–éƒ¨åŸŸæ–‡æœ¬æè¿°çš„å‡†ç¡®æ€§ä¸å¤šæ ·æ€§ï¼›
- å½“å‰å®éªŒé›†ä¸­åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå°šæœªæ‹“å±•è‡³åˆ†å‰²ã€æ£€æµ‹ç­‰æ›´å¤æ‚ä»»åŠ¡ï¼›
- å¯¹æç«¯ Non-IID åœºæ™¯ï¼ˆå¦‚ç±»åˆ«ç¼ºå¤±ï¼‰çš„æ”¯æŒæœ‰å¾…éªŒè¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢è‡ªåŠ¨ç”Ÿæˆå¤–éƒ¨åŸŸæ–‡æœ¬æè¿°çš„æ–¹æ³•ï¼ˆæ›¿ä»£äººå·¥ç¼–å†™ï¼‰ï¼›
- å°† FaST-PT æ‰©å±•è‡³å¤šæ¨¡æ€æˆ–å¤šä»»åŠ¡è”é‚¦å­¦ä¹ åœºæ™¯ï¼›
- ç»“åˆ Differential Privacy è¿›ä¸€æ­¥å¼ºåŒ–éšç§ä¿éšœï¼›
- åº”ç”¨äºåŒ»ç–—ã€é‡‘èç­‰å®é™…é«˜éšç§éœ€æ±‚é¢†åŸŸã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> FaST-PT é€šè¿‡ **Multi-Modal Style Transfer** å’Œ **Dual-Prompt Tuning** çš„ååŒè®¾è®¡ï¼Œåœ¨ä¿è¯éšç§ä¸æ•ˆç‡çš„å‰æä¸‹ï¼Œå®ç°äº†å½“å‰æœ€å¼ºå¤§çš„ Federated Domain Generalization æ€§èƒ½ï¼Œä¸ºåŸºäº Foundation Models çš„è½»é‡åŒ–è”é‚¦å­¦ä¹ æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 5. [Toward an Integrated Cross-Urban Accident Prevention System: A Multi-Task Spatial-Temporal Learning Framework for Urban Safety Management](https://arxiv.org/abs/2601.05521)

**Authors**: Jiayu Fang, Zhiqi Shao, Haoning Xi, Boris Choy, Junbin Gao  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.05521v1  

#### Abstract
The development of a cross-city accident prevention system is particularly challenging due to the heterogeneity, inconsistent reporting, and inherently clustered, sparse, cyclical, and noisy nature of urban accident data. These intrinsic data properties, combined with fragmented governance and incom...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³**è·¨åŸå¸‚äº¤é€šäº‹æ•…é¢„é˜²ç³»ç»Ÿä¸­çš„æ•°æ®å¼‚è´¨æ€§ã€ç¢ç‰‡åŒ–æ²»ç†å’Œé¢„æµ‹æ¨¡å‹å­¤ç«‹**ç­‰é—®é¢˜ã€‚ä¼ ç»Ÿäº‹æ•…é¢„æµ‹æ¨¡å‹å¤šä¸ºå•ä»»åŠ¡è®¾è®¡ï¼Œå—é™äºå„åŸå¸‚ä¸åŒçš„æŠ¥å‘Šæ ‡å‡†ã€åŸºç¡€è®¾æ–½æ¨¡å¼å’Œæ‰§æ³•èƒ½åŠ›ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥åœ¨å¤šä¸ªåŸå¸‚é—´è¿ç§»å’ŒååŒã€‚æ­¤å¤–ï¼Œäº‹æ•…æ•°æ®æœ¬èº«å…·æœ‰**ç©ºé—´èšé›†æ€§ã€æ—¶é—´ç¨€ç–æ€§ã€å‘¨æœŸæ€§å’Œå™ªå£°å¤§**ç­‰å†…åœ¨ç‰¹æ€§ï¼Œè¿›ä¸€æ­¥å¢åŠ äº†å»ºæ¨¡éš¾åº¦ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
ä½œè€…æå‡ºäº† **MLA-STNet (Mamba Local-Attention Spatial-Temporal Network)**ï¼Œä¸€ä¸ªç»Ÿä¸€çš„**è·¨åŸå¸‚äº‹æ•…é¢„é˜²ç³»ç»Ÿæ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†äº‹æ•…é£é™©é¢„æµ‹å»ºæ¨¡ä¸ºä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼š

- **Spatio-Temporal Geographical Mamba-Attention (STG-MA)**  
  è¯¥æ¨¡å—é€šè¿‡ç»“åˆåŸºäº **Mamba** çš„é•¿ç¨‹æ—¶åºå»ºæ¨¡ä¸å±€éƒ¨æ©ç æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆæŠ‘åˆ¶ä¸ç¨³å®šçš„ç©ºé—´-æ—¶é—´æ³¢åŠ¨ï¼Œå¢å¼ºé•¿æœŸä¾èµ–å…³ç³»ï¼Œä»è€Œåº”å¯¹äº‹æ•…æ•°æ®çš„ç¨€ç–æ€§ã€å‘¨æœŸæ€§å’Œå™ªå£°ã€‚

- **Spatio-Temporal Semantic Mamba-Attention (STS-MA)**  
  è¯¥æ¨¡å—é€šè¿‡å…±äº«å‚æ•°è®¾è®¡å¤„ç†è·¨åŸå¸‚å¼‚è´¨æ€§ï¼Œæ‰€æœ‰åŸå¸‚å…±ç”¨å…¨å±€å‚æ•°è¿›è¡Œè”åˆè®­ç»ƒï¼ŒåŒæ—¶ä¿ç•™å„è‡ªç‹¬ç«‹çš„è¯­ä¹‰è¡¨ç¤ºç©ºé—´ï¼ˆå¦‚é“è·¯æ‹“æ‰‘ã€å‡ºè¡Œéœ€æ±‚ã€æ°”è±¡å› ç´ ï¼‰ï¼Œå®ç°â€œ**å…±äº«ç»“æ„ï¼Œä¿ç•™ä¸ªæ€§**â€çš„ååŒä¼˜åŒ–ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **ç»Ÿä¸€æ€§ä¸å¯æ‰©å±•æ€§**ï¼šé¦–æ¬¡æ„å»ºäº†ä¸€ä¸ªå¯æ•´åˆå¼‚æ„åŸå¸‚æ•°æ®çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ”¯æŒè·¨åŸå¸‚ååŒé¢„æµ‹ã€‚
- **é²æ£’æ€§å¼º**ï¼šåœ¨é«˜è¾¾50%è¾“å…¥å™ªå£°ä¸‹ï¼Œæ€§èƒ½å˜åŒ–å°äº1%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚
- **å¤šä»»åŠ¡ä¼˜åŠ¿**ï¼šé€šè¿‡å‚æ•°å…±äº«ä¸ç‹¬ç«‹è¯­ä¹‰å­¦ä¹ ç›¸ç»“åˆï¼Œåœ¨æå‡æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶é¿å…ç‰¹å¾åŒè´¨åŒ–ã€‚
- **æ•ˆç‡é«˜**ï¼šç›¸æ¯”Transformerç±»æ¨¡å‹ï¼ŒMambaæ¶æ„å®ç°äº†çº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œæ›´é€‚åˆå®æ—¶åº”ç”¨ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
å®éªŒä½¿ç”¨äº†ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„åŸå¸‚æ•°æ®é›†ï¼š
- **New York City (NYC)**ï¼šæ—¶é—´è·¨åº¦ä¸º2013å¹´å…¨å¹´ï¼ŒåŒ…å«çº¦147kèµ·äº¤é€šäº‹æ•…ã€1.73äº¿æ¬¡å‡ºç§Ÿè½¦è®¢å•ã€10.3ä¸‡æ¡é“è·¯æ®µåŠæ°”è±¡æ•°æ®ã€‚
- **Chicago**ï¼šæ—¶é—´ä¸º2016å¹´2æœˆè‡³9æœˆï¼ŒåŒ…å«çº¦44kèµ·äº‹æ•…ã€174ä¸‡æ¬¡å‡ºç§Ÿè½¦è®¢å•ã€5.6ä¸‡æ¡é“è·¯æ®µåŠæ°”è±¡è®°å½•ã€‚

ä¸¤åŸå¸‚å‡åˆ’åˆ†ä¸º20Ã—20ç½‘æ ¼ï¼ˆå®é™…æœ‰æ•ˆå•å…ƒåˆ†åˆ«ä¸º243å’Œ197ä¸ªï¼‰ï¼Œå¹¶è¿›è¡Œäº†ç›¸å¯¹æ—¶é—´å¯¹é½ä»¥æ¶ˆé™¤å¹´ä»½å·®å¼‚å½±å“ã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **é¢„æµ‹ä»»åŠ¡**ï¼šåˆ†ä¸¤ç§åœºæ™¯è¿›è¡ŒéªŒè¯ï¼š
  - å…¨å¤©é¢„æµ‹ï¼ˆAll-dayï¼‰
  - é«˜é¢‘äº‹æ•…æœŸé¢„æµ‹ï¼ˆHigh-frequency accident periodsï¼‰
- **æ»‘åŠ¨çª—å£è®¾ç½®**ï¼šè¾“å…¥12ä¸ªå†å²æ—¶æ­¥ï¼ˆæ¯å°æ—¶ï¼‰ï¼Œé¢„æµ‹ä¸‹ä¸€æ—¶åˆ»çš„é£é™©å¼ºåº¦å›¾ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **RMSE**ï¼šè¡¡é‡æ•´ä½“é¢„æµ‹è¯¯å·®
  - **Recall (%)**ï¼šåæ˜ æ¨¡å‹è¯†åˆ«çœŸå®çƒ­ç‚¹çš„èƒ½åŠ›
  - **MAP (Mean Average Precision)**ï¼šè¯„ä¼°çƒ­ç‚¹æ’åºè´¨é‡

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
å¯¹æ¯”äº†äº”ç§å‰æ²¿åŸºçº¿æ¨¡å‹ï¼š
- **GSNet**, **TWCCnet**, **ViT-Traffic**, **MG-STNet**
- æ‰€æœ‰æ¨¡å‹å‡åœ¨å•ä»»åŠ¡ï¼ˆper-cityï¼‰å’Œå¤šä»»åŠ¡ï¼ˆmulti-taskï¼‰è®¾ç½®ä¸‹è¿›è¡Œæ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆMulti-task è®¾ç½®ä¸‹ï¼‰

| æ¨¡å‹ | NYC RMSE â†“ | NYC Recall â†‘ | NYC MAP â†‘ | Chicago RMSE â†“ | Chicago Recall â†‘ | Chicago MAP â†‘ |
|------|------------|-------------|-----------|----------------|------------------|---------------|
| GSNet | 9.81 | 30.28% | 0.168 | 12.65 | 18.75% | 0.076 |
| ViT-Traffic | 9.23 | 29.74% | 0.167 | 11.42 | 17.21% | 0.075 |
| **MLA-STNet (Ours)** | **6.88** | **34.88%** | **0.196** | **8.59** | **23.94%** | **0.104** |

> âœ… **æ€§èƒ½æå‡**ï¼š
> - RMSE æœ€å¤šé™ä½ **6%**
> - Recall æå‡æœ€é«˜è¾¾ **8%**
> - MAP æé«˜è¶…è¿‡ **5%**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šï¼ŒMLA-STNet å‡æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨ Recall å’Œ MAP ä¸Šè¡¨ç°çªå‡ºï¼Œè¯´æ˜å…¶èƒ½æ›´å‡†ç¡®åœ°å®šä½é«˜é£é™©åŒºåŸŸã€‚
- å•ä»»åŠ¡ç‰ˆæœ¬ä¹ŸæŒç»­è¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼Œè¯æ˜å…¶æ¶æ„æœ¬èº«çš„ä¼˜è¶Šæ€§ã€‚
- å¤šä»»åŠ¡è®¾ç½®ä¸‹æ€§èƒ½è¿›ä¸€æ­¥æå‡ï¼Œè¡¨æ˜è·¨åŸå¸‚çŸ¥è¯†è¿ç§»çš„æœ‰æ•ˆæ€§ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰
é€šè¿‡ç§»é™¤å…³é”®ç»„ä»¶éªŒè¯å…¶ä½œç”¨ï¼š

| ç»„ä»¶ | ç§»é™¤å NYC RMSE | ç§»é™¤å Chicago Recall |
|------|------------------|------------------------|
| å®Œæ•´ MLA-STNet | 6.88 | 23.94% |
| w/o STG-MA | 7.00 (+0.12) | 21.53% |
| w/o STS-MA | 7.05 | 18.96% (-4.98%) |
| w/o STM (Mamba) | 7.10 | 19.96% |
| w/o LMA (Local Attention) | 7.13 | 20.96% |

> ğŸ” **å‘ç°**ï¼š
> - **STS-MA å¯¹ Recall å½±å“æœ€å¤§**ï¼Œè¯´æ˜å…¶åœ¨æ•æ‰è·¨åŸå¸‚è¯­ä¹‰å¼‚è´¨æ€§æ–¹é¢è‡³å…³é‡è¦ã€‚
> - **STG-MA å’Œ STM æ˜¾è‘—é™ä½ RMSE**ï¼Œä½“ç°å…¶åœ¨ç¨³å®šæ—¶ç©ºåŠ¨æ€å’Œå»ºæ¨¡é•¿ç¨‹ä¾èµ–ä¸Šçš„æœ‰æ•ˆæ€§ã€‚
> - **LMA æå‡ MAP**ï¼Œè¡¨æ˜å±€éƒ¨æ³¨æ„åŠ›æœ‰åŠ©äºç²¾ç¡®çƒ­ç‚¹å®šä½ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **MLA-STNet æˆåŠŸæ„å»ºäº†é¦–ä¸ªå¯æ‰©å±•ã€é²æ£’ä¸”å¯è§£é‡Šçš„ Cross-City Accident Prevention System**ï¼Œèƒ½å¤Ÿåœ¨å¼‚æ„åŸå¸‚é—´å®ç°ååŒé¢„æµ‹ã€‚
2. **å¤šä»»åŠ¡å­¦ä¹  + å‚æ•°å…±äº« + ç‹¬ç«‹è¯­ä¹‰ç©ºé—´çš„è®¾è®¡æ˜¾è‘—æå‡äº†æ¨¡å‹æ³›åŒ–èƒ½åŠ›**ï¼Œè§£å†³äº†è·¨åŸå¸‚æ•°æ®ä¸ä¸€è‡´çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚
3. **Mamba æ¶æ„åœ¨å¤„ç†ç¨€ç–ã€å‘¨æœŸæ€§ã€å™ªå£°å¤§çš„äº‹æ•…æ•°æ®ä¸­å±•ç°å‡ºä¼˜äº Transformer çš„æ•ˆç‡ä¸ç¨³å®šæ€§**ã€‚
4. **æ¨¡å‹åœ¨é«˜å™ªå£°ç¯å¢ƒä¸‹ä»ä¿æŒæå¼ºé²æ£’æ€§**ï¼ˆ50%å™ªå£°ä¸‹æ€§èƒ½æ³¢åŠ¨<1%ï¼‰ï¼Œé€‚åˆç°å®éƒ¨ç½²ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æç«¯ç¨€ç–åŒºåŸŸå­˜åœ¨è¿‡å¹³æ»‘æˆ–æ¬ æ‹Ÿåˆé£é™©**ï¼Œå°½ç®¡STG-MAç¼“è§£äº†è¯¥é—®é¢˜ï¼Œä½†åœ¨ä¿¡å·æå¼±åŒºåŸŸä»æœ‰æ”¹è¿›ç©ºé—´ã€‚
- **ä¸åŒåŸå¸‚çš„é«˜å³°æ—¶æ®µç›¸ä½é”™ä½**ï¼ˆphase misalignmentï¼‰ä¼šå½±å“è”åˆæ—¶åºå»ºæ¨¡æ•ˆæœï¼Œå³ä½¿ç»è¿‡ç›¸å¯¹æ—¶é—´å¯¹é½ã€‚
- **è¯­ä¹‰æ¨¡æ€ä¸å¹³è¡¡**ï¼šå¦‚NYCæœ‰POIæ•°æ®è€ŒChicagoç¼ºå¤±ï¼Œå½±å“STS-MAçš„è¡¨è¾¾å®Œæ•´æ€§ã€‚
- **è®¡ç®—å¼€é”€ä»è¾ƒé«˜**ï¼šè™½Mambaé«˜æ•ˆï¼Œä½†å¤šè§†å›¾å›¾ç»“æ„æ„å»ºä¸è‡ªé€‚åº”ç›¸ä¼¼çŸ©é˜µæ›´æ–°å¸¦æ¥é¢å¤–è´Ÿæ‹…ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **å¼€å‘åŠ¨æ€å› æœå›¾ç»“æ„**ï¼Œä»¥æ•æ‰éšæ—¶é—´æ¼”åŒ–çš„äº¤é€šã€ç¤¾ä¼šä¸æ”¿ç­–äº¤äº’ã€‚
2. **æ‹“å±•è‡³è”é‚¦å­¦ä¹ ä¸éšç§ä¿æŠ¤ç¯å¢ƒ**ï¼Œæ”¯æŒè·¨è¾–åŒºå®‰å…¨åä½œè€Œä¸æ³„éœ²åŸå§‹æ•°æ®ã€‚
3. **é›†æˆå®æ—¶æ¨ç†æµæ°´çº¿**ï¼Œæ”¯æŒå¤§è§„æ¨¡ITSç³»ç»Ÿä¸­çš„æ—©æœŸé¢„è­¦ä¸å†³ç­–æ”¯æŒã€‚
4. **å¢å¼ºå¯è§£é‡Šæ€§ä¸ä¸ç¡®å®šæ€§ä¼°è®¡**ï¼Œæå‡æ¨¡å‹åœ¨å…¬å…±æ”¿ç­–åˆ¶å®šä¸­çš„å¯ä¿¡åº¦ä¸é€æ˜åº¦ã€‚

---

> ğŸ“Œ **æ€»ç»“**ï¼š  
> MLA-STNet æ˜¯è¿ˆå‘æ™ºèƒ½åŒ–ã€ååŒåŒ–åŸå¸‚å®‰å…¨ç®¡ç†çš„é‡è¦ä¸€æ­¥ã€‚å®ƒä¸ä»…åœ¨æŠ€æœ¯ä¸Šå®ç°äº†å¯¹å¤æ‚æ—¶ç©ºäº‹æ•…æ¨¡å¼çš„ç²¾å‡†å»ºæ¨¡ï¼Œæ›´åœ¨ç³»ç»Ÿå±‚é¢æ¨åŠ¨äº†ä»â€œå­¤å²›å¼æ²»ç†â€å‘â€œè·¨åŸŸååŒé¢„é˜²â€çš„èŒƒå¼è½¬å˜ï¼Œä¸ºæœªæ¥æ™ºæ…§äº¤é€šç³»ç»Ÿçš„å»ºè®¾æä¾›äº†åšå®çš„æ–¹æ³•è®ºåŸºç¡€ä¸å®è·µè·¯å¾„ã€‚

</details>

---

### 6. [Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks](https://arxiv.org/abs/2601.05616)

**Authors**: ShaoZhen Liu, Xinting Huang, Houwen Peng, Xin Chen, Xinyang Song, Qi Li, Zhenan Sun  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.05616v1  

#### Abstract
In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This p...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†ä¸»æµç ”ç©¶å¤šä¾èµ– **Reinforcement Learning (RL)** æ¥æå‡é•¿é“¾æ¨ç†èƒ½åŠ›ï¼Œè€Œå¿½è§†äº† **Supervised Fine-Tuning (SFT)** çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- ä¾èµ–å¤–éƒ¨å¤§æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰è¿›è¡Œæ•°æ®è’¸é¦ï¼Œæˆæœ¬é«˜ä¸”å—é™äºæ•™å¸ˆæ¨¡å‹çš„èƒ½åŠ›ï¼›
- éš¾é¢˜é‡‡æ ·ä¸è¶³ï¼Œè®­ç»ƒæ•°æ®åå‘ç®€å•é—®é¢˜ï¼›
- ç¼ºä¹å¯¹æ¨¡å‹è‡ªæˆ‘çº é”™ã€å›æº¯ã€å­ç›®æ ‡åˆ†è§£ç­‰å†…åœ¨æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ¿€æ´»ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
æœ¬æ–‡æå‡ºä¸€ç§**åŒé˜¶æ®µè‡ªæ¼”è¿› SFT æ¡†æ¶**ï¼ˆDual-Phase SFTï¼‰ï¼Œé€šè¿‡æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ•°æ®å®ç°è¿­ä»£ä¼˜åŒ–ï¼Œæ— éœ€å¤–éƒ¨æ ‡æ³¨æˆ–å¤§æ¨¡å‹è’¸é¦ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰**å¤šè½®å¯¹è¯å¼é•¿ Chain-of-Thought (CoT) æ•°æ®æ„å»º**
- å¼•å…¥ multi-turn dialogue ç­–ç•¥ï¼Œè®©æ¨¡å‹åœ¨å¤šä¸ªå›åˆä¸­å®Œæˆï¼š
  - åˆå§‹ä½œç­” â†’ è‡ªæˆ‘è¯„ä¼° â†’ å›æº¯ä¿®æ­£ â†’ å­ç›®æ ‡åˆ†è§£ â†’ åå‘æ¨ç†
- æ˜¾å¼åµŒå…¥å››ç§å…³é”®æ¨ç†èƒ½åŠ›ï¼š**Verification**, **Backtracking**, **Subgoal Setting**, **Backward Chaining**
- é‡‡ç”¨ä¸¤é˜¶æ®µåˆæˆæµç¨‹ï¼ˆè§ Figure 3ï¼‰ï¼š
  - ç¬¬ä¸€é˜¶æ®µç”Ÿæˆå€™é€‰ç­”æ¡ˆ `Answer1`
  - ç¬¬äºŒé˜¶æ®µè§¦å‘åæ€æœºåˆ¶ç”Ÿæˆæ”¹è¿›ç­”æ¡ˆ `Answer2` å’Œè¿‡æ¸¡æ–‡æœ¬ `Transition Text`

#### ï¼ˆ2ï¼‰**åŸºäºè§„åˆ™çš„é«˜è´¨é‡æ ·æœ¬ç­›é€‰æœºåˆ¶**
- å°†ç”Ÿæˆæ ·æœ¬æŒ‰æ­£ç¡®æ€§åˆ†ä¸ºå››ç±»ï¼š
  - True-to-True, False-to-Trueï¼ˆç†æƒ³å­¦ä¹ ç›®æ ‡ï¼‰, True-to-False, False-to-False
- æ„å»ºå¹³è¡¡æ•°æ®é›†ï¼ˆcorrect:incorrect = 1:1ï¼‰ï¼Œé¿å… reward hacking è¡Œä¸º

#### ï¼ˆ3ï¼‰**éš¾åº¦æ„ŸçŸ¥æ‹’ç»é‡‡æ ·ï¼ˆDifficulty-Aware Rejection Samplingï¼‰**
- åœ¨ç¬¬äºŒé˜¶æ®µï¼Œé’ˆå¯¹æœªè§£éš¾é¢˜åŠ¨æ€å¢åŠ é‡‡æ ·æ¬¡æ•°ï¼ˆå¦‚ nâ‚=2, nâ‚‚=10, nâ‚ƒ=100ï¼‰
- åŠ¨æ€èšç„¦äºâ€œéš¾è§£é—®é¢˜â€ï¼Œç¼“è§£ä¼ ç»Ÿ rejection sampling å¯¹æ˜“é¢˜çš„åå¥½åå·®
- æ”¶é›†é«˜ç½®ä¿¡åº¦æ­£ç¡®å“åº”ç”¨äºè¿›ä¸€æ­¥å¾®è°ƒ

#### ï¼ˆ4ï¼‰**æ ‡è®°å¼•å¯¼çš„æ•°æ®èåˆç­–ç•¥**
- åœ¨è¾“å…¥ä¸­æ·»åŠ ç‰¹æ®Šæ ‡è®°åŒºåˆ†æ•°æ®æ¥æºï¼š
  - `\n\nUsing the solution style from multi-turns data.`
  - `\n\nUsing the solution style from rejection data.`
- å¸®åŠ©æ¨¡å‹è¯†åˆ«ä¸åŒæ¨ç†æ¨¡å¼ï¼Œåœ¨æµ‹è¯•æ—¶å¯é€šè¿‡åˆ‡æ¢æ ‡è®°æ§åˆ¶è¾“å‡ºé£æ ¼

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | æœ¬æ–¹æ³• | ä¼ ç»Ÿæ–¹æ³• |
|------|--------|----------|
| æ•°æ®æ¥æº | å®Œå…¨è‡ªç”Ÿæˆï¼ˆzero external dependencyï¼‰ | ä¾èµ– GPT-4 è’¸é¦æ•°æ® |
| æˆæœ¬ | æä½ï¼ˆä»…éœ€ base model æ¨ç†ï¼‰ | é«˜æ˜‚ï¼ˆè°ƒç”¨ API æˆ–è¿è¡Œå¤§æ¨¡å‹ï¼‰ |
| æ¨ç†é•¿åº¦ | æ‰©å±•è‡³ 4â€“14Ã— baseline | é€šå¸¸è¾ƒçŸ­ |
| å¤æ‚é—®é¢˜è¦†ç›– | ä¸»åŠ¨å¢å¼ºéš¾é¢˜é‡‡æ · | éš¾é¢˜æ˜“è¢«å¿½ç•¥ |
| æ€§èƒ½ä¸Šé™ | è¾¾åˆ°è’¸é¦æ¨¡å‹ 75â€“91% å‡†ç¡®ç‡ï¼Œä»…ç”¨ 16â€“24% è¾“å‡ºé•¿åº¦ | æ›´é«˜å‡†ç¡®ç‡ä½†æåº¦å†—é•¿ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **DeepScaleR dataset**ï¼ˆç»è¿‡è´¨é‡è¿‡æ»¤åçº¦ 15k é«˜è´¨é‡æ•°å­¦é¢˜ï¼‰
  - å…¶ä¸­ 10k ç”¨äºç¬¬ä¸€é˜¶æ®µå¤šè½® CoT æ•°æ®ç”Ÿæˆ
  - 5k ç”¨äºç¬¬äºŒé˜¶æ®µ rejection sampling æ•°æ®ç”Ÿæˆ

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **åŸºç¡€æ¨¡å‹**ï¼šQwen2.5-7B-Instruct
- **è¯„ä¼°åŸºå‡†ï¼ˆ7ä¸ªï¼‰**ï¼š
  - AIME24ï¼ˆç«èµ›çº§æ•°å­¦ï¼‰
  - AMC23ï¼ˆç¾å›½æ•°å­¦ç«èµ›ï¼‰
  - GSM8Kï¼ˆå°å­¦åº”ç”¨é¢˜ï¼‰
  - MATH500ï¼ˆé«˜ä¸­æ•°å­¦ç»¼åˆï¼‰
  - SVAMPï¼ˆå˜ä½“åº”ç”¨é¢˜ï¼‰
  - TabMWPï¼ˆè¡¨æ ¼ç›¸å…³æ•°å­¦é¢˜ï¼‰
  - Gaokao2023enï¼ˆé«˜è€ƒè‹±æ–‡ç‰ˆï¼‰

- **è¯„ä¼°åè®®**ï¼š
  - ä½¿ç”¨ vLLM æ¨ç†å¼•æ“ï¼Œtemperature=0.6, top_p=1.0
  - å¯¹ AIME24/AMC23 é‡‡ç”¨ 8 æ¬¡é‡‡æ ·å–æœ€ä¼˜ï¼›å…¶ä½™å•æ¬¡é‡‡æ ·
  - æµ‹è¯•ä¸‰ç§äº¤äº’æ ¼å¼ï¼š
    1. ç›´æ¥æ±‚è§£
    2. å¤šè½®å¯¹è¯ + rejection sampling
    3. å¸¦æ ‡è®°çš„å¤šè½®å¯¹è¯

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿æ–¹æ³• | æè¿° |
|---------|------|
| Qwen2.5-7B-Instruct | åŸå§‹åŸºç¡€æ¨¡å‹ |
| + distilled | ä½¿ç”¨ r1-distilled-32b æ¨¡å‹è’¸é¦çš„ 15k æ•°æ®å¾®è°ƒï¼ˆå¼ºåŸºçº¿ï¼‰ |
| + self-rewarding | Xiong et al. (2025a) æå‡ºçš„è‡ªå¥–åŠ±æ–¹æ³• |
| + think twice | Tian et al. (2025) æå‡ºçš„å¤šè½®æµ‹è¯•æ—¶é—´æ€è€ƒæ–¹æ³• |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

| æ–¹æ³• | AIME24 (%) | AMC23 (%) | GSM8K (%) | MATH500 (%) | Gaokao2023en (%) |
|------|------------|-----------|-----------|-------------|------------------|
| Baseline | 6.70 | 50.00 | 91.70 | 74.20 | 62.60 |
| + self-rewarding | 6.70 | 52.50 | 89.50 | 72.00 | 61.30 |
| + think twice | 10.00 | 50.00 | 90.10 | 73.40 | 62.60 |
| **+ Dmulti (Ours)** | **16.70** | 52.50 | 90.60 | 75.40 | 62.90 |
| **+ Dmulti+rej (Ours)** | 13.30 | 50.00 | 91.10 | **76.00** | 64.40 |
| **+ rejection marker** | **16.70** | 52.50 | 91.70 | **76.60** | 63.40 |
| + distilled (Upper Bound) | **26.70** | **67.50** | **93.00** | **84.20** | **75.10** |

> æ³¨ï¼šæ‰€æœ‰æ•°å€¼å¯èƒ½å­˜åœ¨ Â±0.1% çš„èˆå…¥è¯¯å·®ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å…³é”®å¯¹æ¯”
- **åœ¨ AIME24 ä¸Š**ï¼š
  - `Dmulti` ç›¸æ¯” baseline æå‡ **149%**ï¼ˆ6.70% â†’ 16.70%ï¼‰
  - è™½ä½äºè’¸é¦æ¨¡å‹ï¼ˆ26.70%ï¼‰ï¼Œä½†å·²æ˜¾è‘—è¶…è¶Šå…¶ä»– SFT æ–¹æ³•
- **åœ¨ MATH500 ä¸Š**ï¼š
  - `Dmulti+rej + rejection marker` è¾¾åˆ° 76.60%ï¼Œæ¥è¿‘è’¸é¦æ¨¡å‹ 84.20%
- **åœ¨ Gaokao2023en ä¸Š**ï¼š
  - `Dmulti+rej` è¾¾åˆ° 64.40%ï¼Œä¼˜äºå¤šæ•°åŸºçº¿ï¼Œä½†ä»æ˜æ˜¾ä½äºè’¸é¦æ¨¡å‹ï¼ˆ75.10%ï¼‰

### å“åº”é•¿åº¦åˆ†æï¼ˆTable 2ï¼‰
| æ–¹æ³• | AIME24 (tokens) | MATH500 (tokens) | GSM8K (tokens) |
|------|------------------|------------------|----------------|
| Baseline | 1246.33 | 581.53 | 303.91 |
| + Dmulti | 4257.62 | 2016.70 | 1141.43 |
| + Dmulti+rej | 3946.01 | 1230.42 | 378.64 |
| + distilled | 19327.74 | 5869.80 | 2043.77 |

- `Dmulti` å¹³å‡ç”Ÿæˆé•¿åº¦è¾¾ baseline çš„ **4â€“14Ã—**
- `Dmulti+rej` åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œç›¸æ¯” `Dmulti` å®ç° **21â€“68% çš„é•¿åº¦å‹ç¼©**
- è’¸é¦æ¨¡å‹æç«¯å†—é•¿ï¼ˆAIME24 è¶… 19k tokensï¼‰ï¼Œæ•ˆç‡ä½ä¸‹

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 3ï¼‰

| æ–¹æ³• | AIME24 (%) | AMC23 (%) | MATH500 (%) |
|------|------------|-----------|-------------|
| Tsft (Dmulti) | 16.70 | 52.50 | 75.40 |
| Tsft + Drej (Sequential) | 10.00 | 47.50 | 76.40 |
| T + Drej (Only) | 10.00 | 42.50 | 74.40 |
| Dmulti+rej (w/o marker) | 10.00 | 45.50 | 73.00 |
| **Dmulti+rej + multi-turn marker** | 13.30 | **57.50** | 75.20 |
| **Dmulti+rej + rejection marker** | **16.70** | 52.50 | **76.60** |

> ç»“è®ºï¼š
- å•ç‹¬ä½¿ç”¨ rejection sampling æ•ˆæœæœ‰é™ï¼ˆT+Drejï¼‰
- é¡ºåºå¾®è°ƒï¼ˆå…ˆ Dmulti å† Drejï¼‰å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼ˆå°¤å…¶ AIME24ï¼‰
- **å¸¦æ ‡è®°çš„è”åˆè®­ç»ƒæ•ˆæœæœ€ä½³**ï¼ŒéªŒè¯äº† marker çš„é‡è¦æ€§
- ä¸åŒ marker é€‚ç”¨äºä¸åŒä»»åŠ¡ï¼š
  - `multi-turn marker` æå‡äº¤äº’å¼æ¨ç†ï¼ˆAMC23ï¼‰
  - `rejection marker` æå‡å½¢å¼åŒ–è¯æ˜ï¼ˆMATH500ï¼‰

### æ•°æ®é‡æ‰©å±•å®éªŒï¼ˆTable 4 & 5ï¼‰
| æ•°æ®é‡ | GSM8K (%) | MATH500 (%) | GSM8K len | MATH500 len |
|-------|-----------|--------------|-----------|--------------|
| 0.5k | 91.30 | 75.00 | 348.93 | 894.87 |
| 1k | 90.40 | 75.20 | 11,695.97 | 15,375.14 |
| 2k | 91.20 | 73.20 | 857.27 | 2,687.33 |
| 4k | 91.10 | 75.20 | 819.71 | 1,892.91 |
| **8k** | **92.30** | **76.00** | **941.94** | **2,032.73** |

> å‘ç°ï¼š
- å‡†ç¡®ç‡éšæ•°æ®é‡å¢é•¿å‘ˆéçº¿æ€§ä¸Šå‡è¶‹åŠ¿ï¼ˆpositive scalingï¼‰
- å“åº”é•¿åº¦å…ˆæ¿€å¢åå›è½ï¼Œæœ€ç»ˆæ›´ç®€æ´é«˜æ•ˆ
- è¡¨æ˜è¯¥æ–¹æ³•å…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼ˆscalabilityï¼‰

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **SFT å¯æœ‰æ•ˆæ¿€æ´» LLM çš„æ·±å±‚æ¨ç†èƒ½åŠ›**  
   æ— éœ€ RL æˆ–å¤–éƒ¨è’¸é¦ï¼Œä»…é è‡ªç”Ÿæˆçš„ long CoT æ•°æ®å³å¯å¤§å¹…æå‡å¤æ‚æ¨ç†æ€§èƒ½ã€‚

2. âœ… **å¤šè½®è‡ªæˆ‘ä¿®æ­£æœºåˆ¶æ˜¯å…³é”®**  
   é€šè¿‡ multi-turn dialogue å®ç° backtracking ä¸ verificationï¼Œä½¿æ¨¡å‹å­¦ä¼šä»é”™è¯¯ä¸­å­¦ä¹ å¹¶é‡æ„è§£å†³æ–¹æ¡ˆã€‚

3. âœ… **éš¾åº¦æ„ŸçŸ¥æ‹’ç»é‡‡æ ·æ˜¾è‘—æå‡éš¾é¢˜å¤„ç†èƒ½åŠ›**  
   åŠ¨æ€èšç„¦æœªè§£å†³é—®é¢˜ï¼Œè§£å†³äº†ä¼ ç»Ÿ rejection sampling ä¸­â€œéš¾é¢˜æ°¸è¿œå¾—ä¸åˆ°å……åˆ†é‡‡æ ·â€çš„ç“¶é¢ˆã€‚

4. âœ… **æ ‡è®°åŒ–æ•°æ®èåˆä¼˜äºé¡ºåºå¾®è°ƒ**  
   åŒæ—¶è®­ç»ƒ multi-turn å’Œ rejection æ•°æ®ï¼Œå¹¶åŠ å…¥ style markerï¼Œèƒ½æ›´å¥½ä¿ç•™ä¸¤ç§æ¨ç†è·¯å¾„çš„ä¼˜åŠ¿ã€‚

5. âœ… **å®ç°æ€§èƒ½ä¸æ•ˆç‡çš„è‰¯å¥½å¹³è¡¡**  
   åœ¨ä»…ä½¿ç”¨ 16â€“24% çš„è¾“å‡ºé•¿åº¦ä¸‹ï¼Œè¾¾åˆ°è’¸é¦æ¨¡å‹ 75â€“91% çš„å‡†ç¡®ç‡ï¼Œæå…·å®ç”¨ä»·å€¼ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–äººå·¥è®¾è®¡æ¨¡æ¿**ï¼šprompt ç»“æ„å’Œæµç¨‹éœ€æ‰‹åŠ¨å®šä¹‰ï¼Œç¼ºä¹è‡ªåŠ¨åŒ–èƒ½åŠ›
- **ä¸¤è½®é™åˆ¶éš¾ä»¥åº”å¯¹æ·±åº¦æ¨ç†**ï¼šå¯¹äº AIME24 è¿™ç±»ææ·±æ¨ç†ä»»åŠ¡ï¼Œä¸¤è½®å¯¹è¯å¯èƒ½ä¸è¶³ä»¥æ”¯æ’‘å®Œæ•´æ€ç»´é“¾
- **é¢†åŸŸè¿ç§»èƒ½åŠ›å¾…éªŒè¯**ï¼šç›®å‰é›†ä¸­åœ¨æ•°å­¦ä»»åŠ¡ï¼Œæ˜¯å¦é€‚ç”¨äºä»£ç ã€ç§‘å­¦æ¨ç†ç­‰å°šä¸æ˜ç¡®

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼€å‘åŸºäº meta-learning çš„è‡ªåŠ¨æ¨¡æ¿ç”Ÿæˆæœºåˆ¶
- é›†æˆ symbolic reasoning engine ä»¥æ”¯æŒä¸¥æ ¼æ•°å­¦è¯æ˜
- è®¾è®¡å¯è§£é‡Šçš„éš¾åº¦è¯„ä¼°æŒ‡æ ‡ï¼Œå®ç° adaptive sampling
- æ¢ç´¢è·¨ä»»åŠ¡ã€è·¨é¢†åŸŸçš„é€šç”¨ self-evolved reasoning æ¡†æ¶

---

> ğŸ”š **æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§èµ„æºé«˜æ•ˆã€å®Œå…¨è‡ªä¾èµ–çš„åŒé˜¶æ®µ SFT æ¡†æ¶ï¼ŒæˆåŠŸå°† LLM çš„å†…åœ¨æ¨ç†èƒ½åŠ›ç³»ç»ŸåŒ–æ¿€å‘ï¼Œåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šå–å¾—æ˜¾è‘—æå‡ï¼Œä¸ºæ‘†è„±å¯¹å¤§æ¨¡å‹è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ çš„ä¾èµ–æä¾›äº†æ–°èŒƒå¼ã€‚ä»£ç å°†å¼€æºï¼Œå…·æœ‰è¾ƒå¼ºå¯å¤ç°æ€§å’Œå·¥ç¨‹è½åœ°æ½œåŠ›ã€‚

</details>

---

### 7. [A Dual Pipeline Machine Learning Framework for Automated Multi Class Sleep Disorder Screening Using Hybrid Resampling and Ensemble Learning](https://arxiv.org/abs/2601.05814)

**Authors**: Md Sultanul Islam Ovi, Muhsina Tarannum Munfa, Miftahul Alam Adib, Syed Sabbir Hasan  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.05814v1  

#### Abstract
Accurate classification of sleep disorders, particularly insomnia and sleep apnea, is important for reducing long term health risks and improving patient quality of life. However, clinical sleep studies are resource intensive and are difficult to scale for population level screening. This paper pres...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³**è‡ªåŠ¨åŒ–å¤šç±»ç¡çœ éšœç¢ç­›æŸ¥ä¸­çš„ä¸¤å¤§æŒ‘æˆ˜**ï¼š
- **ä¸´åºŠèµ„æºé™åˆ¶**ï¼šä¼ ç»Ÿçš„é‡‘æ ‡å‡†è¯Šæ–­æ–¹æ³•ï¼ˆå¦‚å¤šå¯¼ç¡çœ å›¾ Polysomnography, PSGï¼‰æˆæœ¬é«˜ã€è€—æ—¶é•¿ï¼Œéš¾ä»¥ç”¨äºå¤§è§„æ¨¡äººç¾¤ç­›æŸ¥ã€‚
- **æ•°æ®ä¸å¹³è¡¡ä¸å¼‚è´¨æ€§**ï¼šåŸºäºç”Ÿæ´»æ–¹å¼çš„ç¡çœ å¥åº·æ•°æ®é€šå¸¸å­˜åœ¨ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¦‚â€œå¥åº·â€æ ·æœ¬è¿œå¤šäºâ€œå¤±çœ â€æˆ–â€œç¡çœ å‘¼å¸æš‚åœâ€ï¼‰ï¼Œä¸”ç‰¹å¾é—´å­˜åœ¨å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œå¯¼è‡´æ¨¡å‹å¯¹å°‘æ•°ç±»åˆ«çš„æ•æ„Ÿåº¦ä½ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ç§**Dual Pipeline Machine Learning Framework**ï¼ˆåŒç®¡é“æœºå™¨å­¦ä¹ æ¡†æ¶ï¼‰ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ç‰¹å¾å·¥ç¨‹åˆ†ä¸ºä¸¤æ¡å¹¶è¡Œè·¯å¾„ï¼Œåˆ†åˆ«å¤„ç†çº¿æ€§å’Œéçº¿æ€§ç‰¹å¾ä¾èµ–ï¼š

1. **ç»Ÿè®¡ç®¡é“ï¼ˆPipeline 1ï¼‰**  
   - **ç›®æ ‡**ï¼šä¼˜åŒ–çº¿æ€§å¯åˆ†æ€§ã€‚
   - **æµç¨‹**ï¼š`RobustScaler` â†’ `Mutual Information` ç‰¹å¾é€‰æ‹© â†’ `LDA`ï¼ˆçº¿æ€§åˆ¤åˆ«åˆ†æï¼‰é™ç»´ã€‚
   - **é€‚ç”¨æ¨¡å‹**ï¼šé€‚åˆå‡è®¾çº¿æ€§åˆ†ç¦»çš„æ¨¡å‹ï¼ˆå¦‚ Logistic Regression, KNNï¼‰ã€‚

2. **åŒ…è£…å™¨ç®¡é“ï¼ˆPipeline 2ï¼‰**  
   - **ç›®æ ‡**ï¼šæ•æ‰éçº¿æ€§ç‰¹å¾äº¤äº’ã€‚
   - **æµç¨‹**ï¼š`MinMaxScaler` â†’ `Boruta` ç‰¹å¾é€‰æ‹©ï¼ˆåŸºäº Random Forest çš„æ‰€æœ‰ç›¸å…³ç‰¹å¾ç­›é€‰ï¼‰â†’ `Autoencoder` éçº¿æ€§é™ç»´ã€‚
   - **é€‚ç”¨æ¨¡å‹**ï¼šé€‚åˆå¤æ‚éçº¿æ€§ç»“æ„çš„æ¨¡å‹ï¼ˆå¦‚ Extra Trees, MLPï¼‰ã€‚

æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä»¥ä¸‹å…³é”®æŠ€æœ¯ï¼š
- **æ··åˆé‡é‡‡æ ·ç­–ç•¥ SMOTETomek**ï¼šç»“åˆ SMOTEï¼ˆåˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·ï¼‰å’Œ Tomek Linksï¼ˆæ¸…é™¤è¾¹ç•Œå™ªå£°ï¼‰ï¼Œæ—¢å¢åŠ å°‘æ•°ç±»æ ·æœ¬åˆå‡€åŒ–å†³ç­–è¾¹ç•Œã€‚
- **ä¸¥æ ¼çš„ç»Ÿè®¡éªŒè¯**ï¼šä½¿ç”¨ **Wilcoxon Signed Rank Test** éªŒè¯æ€§èƒ½æå‡çš„æ˜¾è‘—æ€§ï¼Œé¿å…å› éšæœºåˆ’åˆ†å¯¼è‡´çš„å¶ç„¶æ€§ç»“è®ºã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´é«˜çš„åˆ†ç±»ç²¾åº¦**ï¼šåœ¨ç›¸åŒæ•°æ®é›†ä¸Šè¶…è¶Šäº†è¿‘æœŸåŸºå‡†æ¨¡å‹ï¼ˆå¦‚ Rahman et al. 2025 çš„ 97.33%ï¼‰ã€‚
- **æ›´å¼ºçš„é²æ£’æ€§**ï¼šé€šè¿‡åŒç®¡é“è®¾è®¡ç³»ç»Ÿæ¯”è¾ƒä¸åŒç‰¹å¾å·¥ç¨‹ç­–ç•¥çš„å½±å“ï¼Œæå‡äº†æ¨¡å‹ç¨³å®šæ€§ã€‚
- **æ›´ä½çš„æ¨ç†å»¶è¿Ÿ**ï¼šæ‰€æœ‰æœ€ä¼˜æ¨¡å‹çš„æ¨ç†æ—¶é—´å‡ä½äº 400 æ¯«ç§’ï¼Œé€‚ç”¨äºå®æ—¶ç­›æŸ¥åœºæ™¯ã€‚
- **æ›´ä¸¥è°¨çš„æ–¹æ³•è®º**ï¼šä¸ä»…æŠ¥å‘Šå‡†ç¡®ç‡ï¼Œè¿˜è¿›è¡Œæ¶ˆèå®éªŒå’Œç»Ÿè®¡æ£€éªŒï¼Œå¢å¼ºäº†ç»“æœå¯ä¿¡åº¦ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **åç§°**ï¼šSleep Health and Lifestyle Datasetï¼ˆæ¥è‡ª Kaggleï¼‰
- **æ¥æº**ï¼šå…¬å¼€å¯ç”¨ï¼Œç”± Tharmalingam åˆ›å»ºï¼ˆæ³¨ï¼šä¸ºåˆæˆæ•°æ®ï¼Œç”¨äºæ¼”ç¤ºç›®çš„ï¼‰
- **è§„æ¨¡**ï¼š374 ä¸ªæ ·æœ¬ï¼Œ13 ä¸ªç‰¹å¾ï¼ˆ8 æ•°å€¼ + 5 åˆ†ç±»ï¼‰
- **ç›®æ ‡å˜é‡**ï¼šä¸‰åˆ†ç±»ç¡çœ éšœç¢æ ‡ç­¾
  - `None`ï¼ˆå¥åº·ï¼‰ï¼š219 ä¾‹
  - `Sleep Apnea`ï¼ˆç¡çœ å‘¼å¸æš‚åœï¼‰ï¼š78 ä¾‹
  - `Insomnia`ï¼ˆå¤±çœ ï¼‰ï¼š77 ä¾‹
- **ç‰¹ç‚¹**ï¼šå­˜åœ¨æ˜æ˜¾ç±»åˆ«ä¸å¹³è¡¡ã€‚

### å®éªŒè®¾ç½®
- **æ•°æ®é¢„å¤„ç†**ï¼š
  - ç¼ºå¤±å€¼æ£€æŸ¥ï¼šæ— ç¼ºå¤±ã€‚
  - ç±»åˆ«ç¼–ç ï¼šOne-Hotï¼ˆå¦‚ Occupationï¼‰ã€Label Encodingï¼ˆå¦‚ Genderï¼‰ã€‚
  - ç‰¹å¾å·¥ç¨‹ï¼šæ„é€ äº† 8 ä¸ªäº¤äº’ç‰¹å¾ï¼ˆå¦‚ `Stress_sleep_interaction`, `Sleep_Heart_ratio` ç­‰ï¼‰ã€‚
  - æ•°æ®åˆ’åˆ†ï¼š80% è®­ç»ƒé›†ï¼Œ20% æµ‹è¯•é›†ï¼Œé‡‡ç”¨ **Stratified 8-Fold Cross Validation** ä¿è¯æ¯æŠ˜ä¸­ç±»åˆ«æ¯”ä¾‹ä¸€è‡´ã€‚
- **é‡é‡‡æ ·**ï¼šåœ¨è®­ç»ƒé›†ä¸Šåº”ç”¨ **SMOTETomek** å¹³è¡¡ç±»åˆ«åˆ†å¸ƒï¼ˆè§ä¸‹è¡¨ï¼‰ã€‚

| Classes         | Original Count | Resampled Count |
|------------------|----------------|-----------------|
| Insomnia (0)     | 62             | 175             |
| None (1)         | 175            | 173             |
| Sleep Apnea (2)  | 62             | 171             |

- **æ¨¡å‹ç§ç±»**ï¼šå…±æµ‹è¯• 9 ç§ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œæ¶µç›–ï¼š
  - çº¿æ€§æ¨¡å‹ï¼šLogistic Regression
  - éå‚æ•°æ¨¡å‹ï¼šKNN
  - ç¥ç»ç½‘ç»œï¼šMLP
  - é›†æˆæ ‘æ¨¡å‹ï¼šRandom Forest, Extra Trees, XGBoost, LightGBM, Gradient Boosting, AdaBoost

- **è¶…å‚æ•°ä¼˜åŒ–**ï¼šä½¿ç”¨ `RandomizedSearchCV` è¿›è¡Œè°ƒå‚ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **ä¸»è¦æŒ‡æ ‡**ï¼š
  - Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰
  - Precisionï¼ˆç²¾ç¡®ç‡ï¼‰
  - Recallï¼ˆå¬å›ç‡ / æ•æ„Ÿåº¦ï¼‰
  - F1-Scoreï¼ˆF1 åˆ†æ•°ï¼‰
- **æ•ˆç‡æŒ‡æ ‡**ï¼š
  - Training Timeï¼ˆè®­ç»ƒæ—¶é—´ï¼‰
  - Testing Time / Inference Latencyï¼ˆæ¨ç†å»¶è¿Ÿï¼‰
- **ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ**ï¼š
  - **Wilcoxon Signed Rank Test**ï¼šç”¨äºæ¯”è¾ƒæ”¹è¿›å‰åæ¨¡å‹æ€§èƒ½å·®å¼‚æ˜¯å¦æ˜¾è‘—ï¼ˆÎ± = 0.05ï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- æœ¬æ–‡ä¸å¤šä¸ªè¿‘æœŸç ”ç©¶è¿›è¡Œäº†æ¨ªå‘æ¯”è¾ƒï¼ŒåŒ…æ‹¬ï¼š
  - Rahman et al. (2025)ï¼šGradient Boostingï¼Œå‡†ç¡®ç‡ 97.33%
  - Monowar et al. (2025)ï¼šEnsemble Modelï¼Œå‡†ç¡®ç‡ 96.88%
  - Hidayat et al. (2023)ï¼šRandom Forestï¼Œå‡†ç¡®ç‡ 88%
  - Alshammari et al. (2024)ï¼šANNï¼Œå‡†ç¡®ç‡ 92.92%

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
åœ¨ä¸¤ä¸ªç®¡é“ä¸­ï¼Œå¤šä¸ªæ¨¡å‹è¾¾åˆ°äº† **98.667% çš„æœ€é«˜å‡†ç¡®ç‡**ï¼š

#### Pipeline 1ï¼ˆç»Ÿè®¡ç®¡é“ï¼‰æœ€ä½³è¡¨ç°ï¼š
| Model               | Configuration           | Accuracy | F1-Score | Recall  | Precision |
|----------------------|--------------------------|----------|----------|---------|-----------|
| KNN                  | MI + SMOTETomek         | 98.667%  | 97.850%  | 97.917% | 97.917%   |
| XGBoost              | MI + SMOTETomek         | 98.667%  | 97.850%  | 97.917% | 97.917%   |
| LightGBM             | MI + SMOTETomek         | 98.667%  | 97.850%  | 97.917% | 97.917%   |

#### Pipeline 2ï¼ˆåŒ…è£…å™¨ç®¡é“ï¼‰æœ€ä½³è¡¨ç°ï¼š
| Model               | Configuration             | Accuracy | F1-Score | Recall  | Precision |
|----------------------|----------------------------|----------|----------|---------|-----------|
| **Extra Trees**      | Boruta + SMOTETomek       | **98.667%** | 97.840%  | 97.778% | **98.039%** |

> âœ… **æœ€é«˜ç²¾åº¦æ¨¡å‹**ï¼š**Extra Trees** å’Œ **KNN** å‡è¾¾åˆ° **98.667% å‡†ç¡®ç‡**ï¼Œåˆ·æ–°å½“å‰ SOTAã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
| Study                     | Model               | Accuracy |
|---------------------------|---------------------|----------|
| This study                | Extra Trees         | **98.667%** |
| Rahman et al. (2025)      | Gradient Boosting   | 97.33%   |
| Monowar et al. (2025)     | Ensemble Model      | 96.88%   |
| Alshammari et al. (2024)  | ANN                 | 92.92%   |
| Hidayat et al. (2023)     | Random Forest       | 88%      |

ğŸ‘‰ **ç»“è®º**ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ç›¸åŒæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚

### æ¶ˆèå®éªŒç»“æœ
é€šè¿‡é€æ­¥æ·»åŠ ç»„ä»¶ï¼ŒéªŒè¯å„æ¨¡å—è´¡çŒ®ï¼š

#### å¯¹ KNNï¼ˆPipeline 1ï¼‰çš„æ¶ˆèåˆ†æï¼š
| Configuration                        | Accuracy | F1-Score |
|--------------------------------------|----------|----------|
| Baseline                             | 92.000%  | 89.526%  |
| + RobustScaler                       | 94.667%  | 92.142%  |
| + SMOTETomek                         | 89.333%  | 85.604%  |
| + Mutual Information (MI)            | 96.000%  | 94.308%  |
| + MI + SMOTETomek                    | **98.667%** | **97.850%** |
| + MI + LDA                           | 96.000%  | 93.521%  |

âœ… **å‘ç°**ï¼š**MI ç‰¹å¾é€‰æ‹© + SMOTETomek** æ˜¯æ€§èƒ½è·ƒå‡çš„å…³é”®ç»„åˆã€‚

#### å¯¹ Extra Treesï¼ˆPipeline 2ï¼‰çš„æ¶ˆèåˆ†æï¼š
| Configuration                        | Accuracy | F1-Score |
|--------------------------------------|----------|----------|
| Baseline                             | 96.000%  | 93.521%  |
| + MinMaxScaler                       | 96.000%  | 93.439%  |
| + Boruta                             | 96.000%  | 93.521%  |
| + Boruta + SMOTETomek                | **98.667%** | **97.840%** |
| + Boruta + Autoencoder               | 93.333%  | 92.285%  |

âœ… **å‘ç°**ï¼š**Boruta + SMOTETomek** æ˜¾è‘—æå‡æ€§èƒ½ï¼›è€ŒåŠ å…¥ Autoencoder åè€Œå¯¼è‡´ä¸‹é™ï¼Œè¯´æ˜è¿‡åº¦å‹ç¼©å¯èƒ½æŸå¤±å…³é”®ä¿¡æ¯ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **åŒç®¡é“æ¡†æ¶æœ‰æ•ˆæå‡æ€§èƒ½**ï¼šé€šè¿‡å¹¶è¡Œå¤„ç†çº¿æ€§å’Œéçº¿æ€§ç‰¹å¾ä¾èµ–ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æŒ–æ˜æ•°æ®æ½œåŠ›ï¼Œæ˜¾è‘—æé«˜åˆ†ç±»ç²¾åº¦ã€‚
2. **SMOTETomek å¯¹å°‘æ•°ç±»è¯†åˆ«è‡³å…³é‡è¦**ï¼šå°¤å…¶å¯¹äº KNN å’Œé›†æˆæ ‘æ¨¡å‹ï¼Œæ··åˆé‡é‡‡æ ·èƒ½æ˜¾è‘—æå‡ Recall å’Œ F1 åˆ†æ•°ã€‚
3. **ç‰¹å¾é€‰æ‹©æ¯”é™ç»´æ›´é‡è¦**ï¼šåœ¨æœ¬ä»»åŠ¡ä¸­ï¼Œ`Mutual Information` å’Œ `Boruta` çš„ä½œç”¨è¿œå¤§äº `LDA` æˆ– `Autoencoder`ï¼Œåè€…ç”šè‡³å¯èƒ½æŸå®³æ€§èƒ½ã€‚
4. **æ¨¡å‹å¯¹é¢„å¤„ç†é«˜åº¦æ•æ„Ÿ**ï¼šKNN å’Œ MLP æåº¦ä¾èµ–å½’ä¸€åŒ–ï¼Œè€Œæ ‘æ¨¡å‹å¯¹æ­¤ä¸æ•æ„Ÿã€‚
5. **æ€§èƒ½æå‡å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§**ï¼šWilcoxon æ£€éªŒæ˜¾ç¤º p = 0.00391 << 0.05ï¼Œè¯æ˜æ”¹è¿›æ˜¯ç³»ç»Ÿæ€§çš„è€Œéå¶ç„¶ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æ•°æ®é‡è¾ƒå°**ï¼šä»… 374 ä¸ªæ ·æœ¬ï¼Œå¯èƒ½å­˜åœ¨æ³›åŒ–é£é™©ã€‚
- **æ•°æ®ä¸ºåˆæˆç”Ÿæˆ**ï¼šè™½ä¾¿äºç ”ç©¶ï¼Œä½†çœŸå®ä¸–ç•Œæ•°æ®å¯èƒ½æ›´å…·å™ªå£°å’Œå¤æ‚æ€§ã€‚
- **æœªè€ƒè™‘æ—¶é—´åºåˆ—ç‰¹æ€§**ï¼šè¯¥æ•°æ®ä¸ºé™æ€å¿«ç…§ï¼Œæ— æ³•åæ˜ é•¿æœŸç¡çœ æ¨¡å¼å˜åŒ–ã€‚
- **Autoencoder æ•ˆæœä¸ä½³**ï¼šåœ¨æ ‘æ¨¡å‹ä¸Šè¡¨ç°å·®ï¼Œè¡¨æ˜éçº¿æ€§å‹ç¼©éœ€è°¨æ…ä½¿ç”¨ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. åœ¨æ›´å¤§è§„æ¨¡ã€å¤šä¸­å¿ƒçš„çœŸå®ä¸´åºŠæ•°æ®ä¸ŠéªŒè¯æ¡†æ¶æœ‰æ•ˆæ€§ã€‚
2. å°†è¯¥æ¶æ„æ‰©å±•è‡³å¯ç©¿æˆ´è®¾å¤‡çš„è¿ç»­ç”Ÿç†ä¿¡å·ï¼ˆå¦‚ PPGã€ECGï¼‰èåˆåˆ†æã€‚
3. æ¢ç´¢è½»é‡åŒ–éƒ¨ç½²æ–¹æ¡ˆï¼Œæ¨åŠ¨ç§»åŠ¨å¥åº·ï¼ˆmHealthï¼‰åº”ç”¨è½åœ°ã€‚
4. åŠ å¼ºå…¬å¹³æ€§åˆ†æï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæ€§åˆ«ã€å¹´é¾„ã€èŒä¸šç¾¤ä½“ä¸­è¡¨ç°å‡è¡¡ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼šæœ¬æ–‡æå‡ºçš„ **Dual Pipeline Framework + SMOTETomek** ç»„åˆï¼Œåœ¨ Sleep Health and Lifestyle æ•°æ®é›†ä¸Šå®ç°äº† **98.667% çš„ SOTA å‡†ç¡®ç‡**ï¼Œå¹¶é€šè¿‡ä¸¥è°¨çš„æ¶ˆèå®éªŒå’Œç»Ÿè®¡æ£€éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œä¸ºä½æˆæœ¬ã€é«˜ç²¾åº¦çš„è‡ªåŠ¨åŒ–ç¡çœ éšœç¢åˆç­›æä¾›äº†å¯é è§£å†³æ–¹æ¡ˆã€‚

</details>

---

### 8. [The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection](https://arxiv.org/abs/2601.05371)

**Authors**: Md Shafiqul Islam, Shakti Prasad Padhy, Douglas Allaire, Raymundo Arr\'oyave  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.05371v1  

#### Abstract
Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive ste...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šThe Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
Gaussian Process (GP) å›å½’çš„æ€§èƒ½é«˜åº¦ä¾èµ–äºåæ–¹å·® **kernel** çš„é€‰æ‹©ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•åœ¨ kernel é€‰æ‹©ä¸Šå­˜åœ¨ä»¥ä¸‹ç“¶é¢ˆï¼š
- æ‰‹åŠ¨è®¾è®¡æˆ–å°è§„æ¨¡æœç´¢æ•ˆç‡ä½ã€ä¸å¯æ‰©å±•ï¼›
- ç»„åˆ kernelï¼ˆå¦‚ `SE + PER Ã— RQ`ï¼‰å¯¼è‡´ç»„åˆçˆ†ç‚¸ï¼Œéš¾ä»¥ç©·ä¸¾ï¼›
- å•çº¯åŸºäºç¬¦å·ç»“æ„ï¼ˆsymbolic formï¼‰æˆ–è¶…å‚æ•°ä¼˜åŒ–çš„æ–¹æ³•å¿½ç•¥äº†ä¸åŒ kernel åœ¨å‡½æ•°åˆ†å¸ƒå±‚é¢çš„ç›¸ä¼¼æ€§ï¼›
- ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ LLM å¼•å¯¼æœç´¢ï¼‰å®¹æ˜“å› â€œè¯­æ³•è„†å¼±æ€§â€ï¼ˆsyntactic brittlenessï¼‰å¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ ¸å¿ƒæ€æƒ³
æœ¬æ–‡æå‡ºä¸€ç§**å‡ ä½•è§†è§’ä¸‹çš„ kernel é€‰æ‹©æ¡†æ¶**ï¼Œå°†ç¦»æ•£çš„ kernel æœç´¢ç©ºé—´è½¬åŒ–ä¸ºè¿ç»­çš„ã€å¯å¾®åˆ†çš„â€œ**kernel manifold**â€ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ Bayesian Optimization (BO)ã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
1. **æ„å»º kernel-of-kernels å‡ ä½•ç©ºé—´**  
   - å®šä¹‰ä¸¤ä¸ª GP å…ˆéªŒä¹‹é—´çš„æ¦‚ç‡å‘æ•£åº¦ï¼ˆprobabilistic divergenceï¼‰ï¼Œä¾‹å¦‚ **Jensen-Shannon (JS) divergence** æˆ– **Hellinger distance**ï¼Œä½œä¸º kernel é—´çš„è·ç¦»ã€‚
   - å¯¹è¶…å‚æ•°è¿›è¡Œè¾¹ç¼˜åŒ–ï¼ˆquasi-Monte Carlo é‡‡æ ·ï¼‰ï¼Œå¾—åˆ°å¯¹è¶…å‚æ•°å˜åŒ–é²æ£’çš„è·ç¦»çŸ©é˜µã€‚

2. **å¤šç»´ç¼©æ”¾ï¼ˆMultidimensional Scaling, MDSï¼‰åµŒå…¥**  
   - å°†ç¦»æ•£çš„ kernel åº“é€šè¿‡ MDS æ˜ å°„åˆ°ä¸€ä¸ªä½ç»´æ¬§å‡ é‡Œå¾—ç©ºé—´ï¼ˆå³â€œkernel manifoldâ€ï¼‰ã€‚
   - åœ¨è¯¥ç©ºé—´ä¸­ï¼Œkernel ä¹‹é—´çš„æ¬§æ°è·ç¦»åæ˜ å…¶è¯±å¯¼å‡½æ•°åˆ†å¸ƒçš„ç›¸ä¼¼æ€§ã€‚

3. **åœ¨åµŒå…¥æµå½¢ä¸Šæ‰§è¡Œ Bayesian Optimization**  
   - BO çš„è¾“å…¥æ˜¯ MDS åæ ‡ï¼ˆè€ŒéåŸå§‹ kernel è¡¨è¾¾å¼ï¼‰ï¼›
   - ç›®æ ‡å‡½æ•°ä¸º **log marginal likelihood (LML)**ï¼›
   - æ¯æ¬¡ BO æè®®ä¸€ä¸ªè¿ç»­åæ ‡åï¼Œâ€œsnapâ€ åˆ°æœ€è¿‘çš„ç¦»æ•£ kernel ä¸Šè¿›è¡Œè¯„ä¼°ï¼›
   - ä½¿ç”¨ **multi-scale RBF kernel-of-kernels** ä½œä¸º surrogate modelï¼Œæ•æ‰å±€éƒ¨ä¸å…¨å±€ç»“æ„ã€‚

4. **å‡ ä½•çŸ«æ­£æŠ€æœ¯æå‡åµŒå…¥è´¨é‡**  
   - åˆ†æä¸åŒ divergence çš„å‡ ä½•æ€§è´¨ï¼ˆæ˜¯å¦æ»¡è¶³ metricã€æ˜¯å¦æœ‰è´Ÿç‰¹å¾å€¼ç­‰ï¼‰ï¼›
   - é’ˆå¯¹éæ¬§å‡ ä½•ï¼ˆå¦‚çƒé¢æµå½¢ï¼‰é‡‡ç”¨ **chordal mapping**ï¼›
   - é’ˆå¯¹è·ç¦»å‹ç¼©é—®é¢˜ï¼ˆdistance concentrationï¼‰ä½¿ç”¨ **log-warping** å˜æ¢å¢å¼ºåŠ¨æ€èŒƒå›´ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | å±€é™æ€§ | æœ¬æ–‡ä¼˜åŠ¿ |
|------|--------|---------|
| æ‰‹åŠ¨è®¾è®¡ / è´ªå¿ƒæœç´¢ [Duvenaud et al.] | æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œç¼ºä¹æ¢ç´¢æœºåˆ¶ | è‡ªåŠ¨åŒ–ã€åŸºäºä¸ç¡®å®šæ€§çš„æ¢ç´¢ |
| BO over symbolic space [Malkomes et al.] | ç¼ºä¹æœ‰æ„ä¹‰çš„å‡ ä½•ç»“æ„ | æ„å»ºåŠŸèƒ½ä¸€è‡´çš„å‡ ä½•ç©ºé—´ |
| LLM-guided search [CAKE] | â€œè¯­æ³•è„†æ€§â€ï¼Œçªå˜å¯èƒ½å¯¼è‡´è¡Œä¸ºå‰§å˜ | åŸºäºåŠŸèƒ½ç›¸ä¼¼æ€§çš„å¹³æ»‘è¿‡æ¸¡ |
| ABC-SMC æ–¹æ³• [Abdessalem et al.] | ä¸ä¾èµ–ä¼¼ç„¶ï¼Œä½†è®¡ç®—å¼€é”€å¤§ | åˆ©ç”¨ LML è¿›è¡Œç²¾ç¡®ä¼˜åŒ– |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿æ€»ç»“**ï¼š  
> æœ¬æ–¹æ³•å°† kernel é€‰æ‹©ä»â€œç¬¦å·æ“ä½œâ€è½¬å˜ä¸ºâ€œåŠŸèƒ½ç©ºé—´ä¸­çš„è¿ç»­ä¼˜åŒ–â€ï¼Œå®ç°äº†æ›´ç¨³å®šã€é«˜æ•ˆä¸”å¯è§£é‡Šçš„è‡ªåŠ¨ kernel å‘ç°ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†
å®éªŒæ¶µç›–åˆæˆå‡½æ•°ä¸çœŸå®ä¸–ç•Œæ—¶é—´åºåˆ—æ•°æ®ï¼š

| ç±»å‹ | åç§° | ç‰¹å¾ |
|------|------|------|
| åˆæˆåŸºå‡† | Eggholder, Ackley, Dropwave, Schwefel, Rastrigin, Levy, Bukin | å¤šå³°ã€éœ‡è¡ã€éå¹³ç¨³ã€å¤æ‚ç»“æ„ |
| æ—¶é—´åºåˆ— | International Airline Passengers, Mauna Loa COâ‚‚, Thermal History (è‡ªç ”) | å­£èŠ‚æ€§ã€è¶‹åŠ¿ã€å™ªå£°ã€ä¸è§„åˆ™å‘¨æœŸ |

æ­¤å¤–è¿˜åŒ…æ‹¬ä¸€ä¸ª**å¢æåˆ¶é€ ï¼ˆAdditive Manufacturing, AMï¼‰æ¡ˆä¾‹ç ”ç©¶**ï¼Œé¢„æµ‹æ¿€å…‰åŠŸç‡ä¸æ‰«æé€Ÿåº¦ä¸‹çš„ç†”æ± å®½åº¦ï¼ˆmelt-pool geometryï¼‰ï¼Œç”± Thermo-CalcÂ® TCAM æ¨¡æ‹Ÿç”Ÿæˆã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### ä¸»è¦ä»»åŠ¡ï¼š
- **Kernel Selection**ï¼šä»é¢„å®šä¹‰ grammar ç”Ÿæˆçš„ç»„åˆ kernel åº“ä¸­é€‰å‡ºæœ€ä¼˜ kernelï¼›
- **ä¸‹æ¸¸ BO æ€§èƒ½æµ‹è¯•**ï¼šä½¿ç”¨é€‰å®š kernel ä½œä¸º surrogateï¼Œåœ¨ benchmark å‡½æ•°ä¸Šè¿è¡Œ BOï¼Œæ¯”è¾ƒæ”¶æ•›é€Ÿåº¦ä¸æœ€ç»ˆæ€§èƒ½ã€‚

#### è¯„ä¼°æŒ‡æ ‡ï¼š
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Log Marginal Likelihood (LML)** | kernel æ‹Ÿåˆä¼˜åº¦çš„æ ¸å¿ƒç›®æ ‡å‡½æ•° |
| **Best Observed Objective Value** | ä¸‹æ¸¸ BO ä¸­æ‰¾åˆ°çš„æœ€ä½³å‡½æ•°å€¼ |
| **Hypervolume (HV)** | å¤šç›®æ ‡ä¼˜åŒ–ä¸­è¡¡é‡å¸•ç´¯æ‰˜å‰æ²¿è¦†ç›–é¢ç§¯ |
| **Computational Time** | åŒ…æ‹¬ LLM æŸ¥è¯¢å»¶è¿Ÿåœ¨å†…çš„æ€»è€—æ—¶ |

#### åŸºçº¿æ–¹æ³•å¯¹æ¯”ï¼š
| æ–¹æ³• | æè¿° |
|------|------|
| **Random Search** | éšæœºä» kernel åº“ä¸­é‡‡æ · |
| **RBF-BO** | ä½¿ç”¨å•å°ºåº¦ RBF kernel-of-kernels çš„ BO |
| **Multiscale RBF-BO** | æœ¬æ–‡æå‡ºçš„å¤šå°ºåº¦ kernel-of-kernels BOï¼ˆä¸»æ–¹æ³•ï¼‰ |
| **LLM-GA** | åŸºäº LLM çš„é—ä¼ ç®—æ³•ï¼ˆCAKE æ¡†æ¶æ”¹è¿›ç‰ˆï¼‰ï¼Œå« depth-constrained å’Œ unrestricted ä¸¤ç§è®¾ç½® |
| **Fixed RBF Kernel** | æ ‡å‡† RBF kernelï¼Œç”¨äºä¸‹æ¸¸ BO å¯¹æ¯” |

æ‰€æœ‰å®éªŒé‡å¤å¤šæ¬¡ï¼ˆé€šå¸¸ 5 æ¬¡ä»¥ä¸Šï¼‰ï¼ŒæŠ¥å‘Šå‡å€¼ Â± æ ‡å‡†å·®ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 ä¸ Fig. 16ï¼‰

#### åœ¨å¤šä¸ª benchmark ä¸Šçš„è¡¨ç°ï¼ˆè¿­ä»£ 12 æ¬¡åçš„ Max LMLï¼‰ï¼š

| Function | Multiscale RBF-BO | RBF-BO | LLM-GA (p=0.7) | Random |
|---------|-------------------|--------|---------------|--------|
| Ackley | **-1.5 Â± 0.0** | **-1.5 Â± 0.0** | -2139.1 Â± 1883.3 | -582.3 Â± 1099.0 |
| Bukin | **-1.5 Â± 0.0** | **-1.5 Â± 0.0** | -998.5 Â± 683.6 | -734.9 Â± 705.1 |
| Dropwave | **-1.3 Â± 0.0** | **-1.3 Â± 0.0** | -1.3 Â± 0.0 | -1.3 Â± 0.0 |
| Eggholder | **-1.5 Â± 0.0** | -263.7 Â± 524.5 | -1533.8 Â± 876.9 | -735.0 Â± 679.3 |
| Levy | **-1.5 Â± 0.0** | -990.5 Â± 1977.9 | -3957.4 Â± 1977.9 | -3037.9 Â± 2339.9 |
| Rastrigin | **-1.36 Â± 0.01** | -1.35 Â± 0.01 | -2968.4 Â± 2422.5 | -1979.4 Â± 2422.5 |
| Schwefel | **-1.5 Â± 0.0** | **-1.5 Â± 0.0** | -1112.7 Â± 558.8 | -1520.4 Â± 1812.5 |

> âœ… **è§‚å¯Ÿ**ï¼šMultiscale RBF-BO åœ¨ç»å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€é«˜ LMLï¼Œä¸”æ–¹å·®æå°ï¼›LLM-GA è¡¨ç°æœ€å·®ï¼Œæ³¢åŠ¨æå¤§ã€‚

---

### ğŸ” ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

#### ï¼ˆ1ï¼‰vs LLM-GA
- **æ€§èƒ½å·®è·æ˜¾è‘—**ï¼šLLM-GA çš„å¹³å‡ LML æ¯” proposed method å·®å‡ ä¸ªæ•°é‡çº§ï¼›
- **é«˜æ–¹å·®**ï¼šLLM è¾“å‡ºä¸ç¨³å®šï¼Œå¯¼è‡´æœç´¢è·¯å¾„ä¸å¯å¤ç°ï¼›
- **æ•æ„Ÿäº mutation rate**ï¼šåªæœ‰å½“ `p=0.7` æ—¶è¡¨ç°ç›¸å¯¹è¾ƒå¥½ï¼Œéœ€ç²¾ç»†è°ƒå‚ï¼›
- **è®¡ç®—æˆæœ¬æ›´é«˜**ï¼šLLM-GA å¹³å‡è€—æ—¶æ˜¯ BO æ–¹æ³•çš„ **3.4â€“5.7 å€**ï¼ˆè§ Fig. 17ï¼‰ï¼Œä¸»è¦æ¥è‡ª API æ¨ç†å»¶è¿Ÿã€‚

#### ï¼ˆ2ï¼‰vs å•å°ºåº¦ RBF-BO
- å¤šå°ºåº¦ kernel-of-kernels æ˜¾è‘—åŠ å¿«æ”¶æ•›é€Ÿåº¦ï¼Œå°¤å…¶åœ¨å¤æ‚åœ°å½¢ï¼ˆå¦‚ Eggholder, Bukinï¼‰ä¸Šï¼›
- æ›´å¥½åœ°å¹³è¡¡å±€éƒ¨ç»†èŠ‚ä¸å…¨å±€è¶‹åŠ¿å»ºæ¨¡èƒ½åŠ›ã€‚

#### ï¼ˆ3ï¼‰vs Random Search
- æ‰€æœ‰ BO æ–¹æ³•å‡è¿œä¼˜äºéšæœºæœç´¢ï¼›
- å³ä½¿åœ¨æœ€éš¾çš„ä»»åŠ¡ï¼ˆThermal Historyï¼‰ä¸Šï¼Œmultiscale BO ä»èƒ½å¿«é€Ÿæå‡ LMLã€‚

---

### ğŸ” æ¶ˆèå®éªŒä¸åˆ†æï¼ˆéšå«åœ¨æ–‡ä¸­ï¼‰

| åˆ†æç»´åº¦ | ç»“æœ |
|--------|------|
| **MDS åµŒå…¥ä¿çœŸåº¦** | Fig. 12 æ˜¾ç¤ºé‡å»ºè·ç¦»ä¸çœŸå® Hellinger è·ç¦»é«˜åº¦ä¸€è‡´ï¼Œè¯´æ˜åµŒå…¥æœ‰æ•ˆ |
| **èšç±»ç»“æ„éªŒè¯** | k-means ä¸å±‚æ¬¡èšç±»æ˜¾ç¤ºåŒç±» kernel åœ¨åµŒå…¥ç©ºé—´ä¸­ç´§å¯†èšé›†ï¼ˆFig. 13â€“15ï¼‰ |
| **divergence é€‰æ‹©å½±å“** | âˆšJS divergence æœ€ä½³ï¼šäº§ç”Ÿæ­£å®š Gram çŸ©é˜µï¼Œæ— éœ€å˜æ¢å³å¯åµŒå…¥ï¼ˆFig. 10ï¼‰ï¼›Hellinger å’Œ KL éœ€ log-warping æˆ– chordal mapping æ‰èƒ½ç¨³å®šåµŒå…¥ |
| **embedding dimension** | MDS åæ ‡å‰ 10â€“20 ç»´å³å¯æ•è·å¤§éƒ¨åˆ†ç»“æ„ï¼ˆFig. 11ï¼‰ï¼Œè¡¨æ˜ kernel manifold å®é™…ä½ç»´ |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **kernel ä¹‹é—´å­˜åœ¨å¯å­¦ä¹ çš„åŠŸèƒ½å‡ ä½•ç»“æ„**  
   ä¸åŒ kernel è¯±å¯¼çš„ GP prior å¯ä»¥é€šè¿‡æ¦‚ç‡å‘æ•£åº¦é‡åŒ–ï¼Œå¹¶å½¢æˆå…·æœ‰æ„ä¹‰çš„é‚»åŸŸå…³ç³»ã€‚

2. **âˆšJS divergence æ˜¯æ„å»º kernel manifold çš„ç†æƒ³åº¦é‡**  
   - æ˜¯çœŸæ­£çš„ metricï¼›
   - äº§ç”Ÿçš„è·ç¦»çŸ©é˜µå¯ç²¾ç¡®åµŒå…¥æ¬§å‡ é‡Œå¾—ç©ºé—´ï¼›
   - æ— éœ€é¢å¤–å˜æ¢å³å¯è·å¾—ç¨³å®š BO åœºæ™¯ã€‚

3. **MDS + BO æ¡†æ¶æ˜¾è‘—ä¼˜äºç¬¦å·æœç´¢ä¸ LLM æ–¹æ³•**  
   - æ”¶æ•›æ›´å¿«ã€æ–¹å·®æ›´ä½ï¼›
   - æ›´é€‚åˆå¤„ç†å¤æ‚ã€éå¹³ç¨³æ•°æ®ï¼›
   - åœ¨çœŸå®å·¥ç¨‹åœºæ™¯ï¼ˆAMï¼‰ä¸­å¤§å¹…æå‡é¢„æµ‹ç²¾åº¦ä¸ä¸ç¡®å®šæ€§æ ¡å‡†ã€‚

4. **ä¼˜åŒ–åçš„ kernel ç›´æ¥æå‡ä¸‹æ¸¸ BO æ€§èƒ½**  
   - åœ¨ Dropwave å’Œ Ackley ä¸Šï¼Œä½¿ç”¨ task-specific kernel çš„ BO æ”¶æ•›é€Ÿåº¦æ˜æ˜¾å¿«äº RBF baselineï¼ˆFig. 21ï¼‰ï¼›
   - åœ¨å¤šç›®æ ‡æ‰“å°å¯è¡Œæ€§ä¼˜åŒ–ä¸­ï¼Œhypervolume å¢é•¿æ›´å¿«ã€æœ€ç»ˆè§£æ›´ä¼˜ï¼ˆFig. 22ï¼‰ã€‚

5. **LLM-guided search åœ¨ kernel ä¼˜åŒ–ä¸­è¡¨ç°ä¸ä½³**  
   - å°½ç®¡åœ¨å…¶ä»–ä»»åŠ¡ä¸­æœ‰æ½œåŠ›ï¼Œä½†åœ¨ kernel ç»“æ„æœç´¢ä¸­å—â€œè¯­æ³•è„†æ€§â€ä¸¥é‡å½±å“ï¼›
   - ç¼ºä¹å¯¹åŠŸèƒ½è¿ç»­æ€§çš„æ„ŸçŸ¥ï¼Œæ˜“ç ´åå·²æœ‰ä¼˜è‰¯ç»“æ„ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **ä¾èµ–é¢„å®šä¹‰çš„ kernel grammar**  
   è™½ç„¶çµæ´»ï¼Œä½†ä»å—é™äºåˆå§‹ç»„åˆè§„åˆ™ï¼Œæ— æ³•å‘ç°å…¨æ–°ç±»å‹çš„ kernelã€‚

2. **MDS åµŒå…¥æ˜¯ä¸€æ¬¡æ€§ã€é™æ€çš„**  
   åµŒå…¥åŸºäºå…ˆéªŒ GP åˆ†å¸ƒï¼Œæœªéšæ•°æ®åŠ¨æ€è°ƒæ•´ï¼›è‹¥ä»»åŠ¡å·®å¼‚å·¨å¤§ï¼Œå¯èƒ½éœ€è¦é‡æ–°æ„å»º manifoldã€‚

3. **è®¡ç®—é¢„å¤„ç†å¼€é”€è¾ƒå¤§**  
   æ„å»º kernel-kernel è·ç¦»çŸ©é˜µéœ€å¤§é‡ MC é‡‡æ ·ä¸ divergence è®¡ç®—ï¼Œè™½ä»…ä¸€æ¬¡ï¼Œä½†å¯¹äºè¶…å¤§è§„æ¨¡ kernel åº“ä»æ˜‚è´µã€‚

4. **snap-to-nearest-kernel å¯èƒ½é™åˆ¶æ¢ç´¢èƒ½åŠ›**  
   è™½é¿å…äº†è§£ç éš¾é¢˜ï¼Œä½†ä¹Ÿé™åˆ¶äº†åœ¨è¿ç»­ç©ºé—´ä¸­çœŸæ­£â€œæ’å€¼â€æ–° kernel çš„å¯èƒ½æ€§ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **åŠ¨æ€æ›´æ–° kernel manifold**  
   æ ¹æ®å½“å‰æ•°æ®é›†åœ¨çº¿è°ƒæ•´ kernel é—´è·ç¦»ï¼Œå®ç°ä»»åŠ¡è‡ªé€‚åº”åµŒå…¥ã€‚

2. **ç»“åˆ deep kernel learning**  
   å°†æ­¤å‡ ä½•æ¡†æ¶åº”ç”¨äºç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ kernelï¼Œå®ç°ç«¯åˆ°ç«¯ç»“æ„å‘ç°ã€‚

3. **æ‰©å±•è‡³å…¶ä»–æ¨¡å‹ç±»**  
   å¦‚ Neural Processesã€Deep GPs ç­‰ï¼Œæ„å»ºé€šç”¨â€œmodel manifoldâ€ã€‚

4. **æ”¯æŒå¯å¾®åˆ† kernel æ’å€¼**  
   è®¾è®¡ decoder æˆ– interpolation ruleï¼Œå…è®¸åœ¨ manifold ä¸Šç”Ÿæˆæ–°çš„åˆæ³• kernel è¡¨è¾¾å¼ã€‚

5. **é›†æˆç‰©ç†çº¦æŸä¸é¢†åŸŸçŸ¥è¯†**  
   åœ¨è·ç¦»è®¡ç®—ä¸­å¼•å…¥ææ–™ç§‘å­¦ã€æ§åˆ¶ç†è®ºç­‰å…ˆéªŒï¼ŒæŒ‡å¯¼æ›´åˆç†çš„ kernel æ„é€ ã€‚

---

## æ€»ç»“

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> æœ¬æ–‡æå‡ºäº†ä¸€ç§å°† kernel é€‰æ‹©è§†ä¸ºâ€œåŠŸèƒ½åˆ†å¸ƒç©ºé—´ä¸­å‡ ä½•å¯¼èˆªâ€çš„æ–°èŒƒå¼ï¼Œé€šè¿‡ **divergence + MDS + BO** æ„å»º **kernel manifold**ï¼Œå®ç°äº†æ¯” LLM å’Œä¼ ç»Ÿæ–¹æ³•æ›´é«˜æ•ˆã€ç¨³å®šã€å¯è§£é‡Šçš„è‡ªåŠ¨ kernel å‘ç°ã€‚

è¯¥æ–¹æ³•ä¸ä»…æå‡äº† GP æ¨¡å‹æœ¬èº«çš„æ€§èƒ½ï¼Œä¹Ÿä¸ºè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰ã€ç§‘å­¦å»ºæ¨¡ä¸ BO è‡ªèº«æä¾›äº†æ›´å¼ºæœ‰åŠ›çš„ surrogate æ„å»ºå·¥å…·ã€‚ä»£ç å·²å¼€æºï¼š[https://github.com/shafiqmmee/Kernels-BO](https://github.com/shafiqmmee/Kernels-BO)ã€‚

</details>

---

### 9. [Efficient Inference for Noisy LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2601.05420)

**Authors**: Yiqun T Chen, Sizhu Lu, Sijia Li, Moran Guo, Shengyi Li  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.05420v1  

#### Abstract
Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have re...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šEfficient Inference for Noisy LLM-as-a-Judge Evaluation**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
è¯¥è®ºæ–‡èšç„¦äºâ€œ**LLM-as-a-Judge**â€èŒƒå¼ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š**LLM ä½œä¸ºè‡ªåŠ¨è¯„åˆ¤å™¨æ—¶å­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼ˆsystematic errorsï¼‰**ï¼Œå…¶è¾“å‡ºæ˜¯çœŸå®äººç±»æ ‡ç­¾çš„æœ‰å™ªä»£ç†ï¼ˆnoisy surrogateï¼‰ã€‚ç›´æ¥ä½¿ç”¨è¿™äº›æœ‰ååˆ¤æ–­ä¼šå¯¼è‡´è¯„ä¼°ç»“æœåå·®å¤§ã€ç½®ä¿¡åŒºé—´æ— æ•ˆã€‚

å…·ä½“é—®é¢˜åŒ…æ‹¬ï¼š
- å¦‚ä½•åœ¨ä»…æœ‰å°‘é‡äººç±»æ ‡æ³¨ï¼ˆcalibration setï¼‰çš„æƒ…å†µä¸‹ï¼Œå¯¹å¤§è§„æ¨¡ LLM åˆ¤æ–­ï¼ˆevaluation setï¼‰è¿›è¡Œæœ‰æ•ˆæ ¡å‡†ï¼Ÿ
- å¦‚ä½•åœ¨ä¿è¯æ¨æ–­æœ‰æ•ˆæ€§çš„åŒæ—¶ï¼Œå®ç°æ›´é«˜çš„ç»Ÿè®¡æ•ˆç‡ï¼ˆå³æ›´å°çš„æ–¹å·®ï¼‰ï¼Ÿ

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**
è®ºæ–‡ä»**åŠå‚æ•°æ•ˆç‡ç†è®ºï¼ˆsemiparametric efficiency theoryï¼‰** å‡ºå‘ï¼Œç»Ÿä¸€å¹¶æ¯”è¾ƒäº†ä¸¤ç±»ä¸»æµæ ¡å‡†æ–¹æ³•ï¼š

1. **Measurement-error correction**ï¼ˆå¦‚ Rogan-Gladen estimatorï¼‰
2. **Surrogate-outcome æ–¹æ³•**ï¼ˆå¦‚ Prediction-Powered Inference, PPIï¼‰

#### **æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š**
- **æå‡ºåŸºäº Efficient Influence Function (EIF) çš„æœ€ä¼˜ä¼°è®¡æ¡†æ¶**ï¼Œä¸º LLM-as-a-Judge åœºæ™¯ä¸‹çš„å‡å€¼å‚æ•°ï¼ˆå¦‚å¹³å‡å¾—åˆ†ã€èƒœç‡ï¼‰ä¼°è®¡æä¾›äº†ç†è®ºä¸Šçš„æ•ˆç‡ä¸‹ç•Œã€‚
- **è¯æ˜åœ¨äºŒå…ƒè¾“å‡ºåœºæ™¯ä¸‹ï¼Œæœ€ä¼˜è°ƒå‚çš„ PPI++ ä¸ EIF ä¼°è®¡å™¨æ¸è¿‘ç­‰ä»·**ï¼Œä¸”äºŒè€…å‡è¾¾åˆ°åŠå‚æ•°æ•ˆç‡ç•Œï¼Œä¼˜äºä¼ ç»Ÿçš„ Rogan-Gladen (RG) æ–¹æ³•ã€‚
- **æ­ç¤º PPI ç±»æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯å¯¹æ¡ä»¶æœŸæœ› $ \mu(\hat{Y}) = E[Y|\hat{Y}] $ çš„ä¸åŒå‡½æ•°å½¢å¼é€¼è¿‘**ï¼š
  - PPI ä½¿ç”¨æ’ç­‰æ˜ å°„ $ \hat{Y} $
  - PPI++ ä½¿ç”¨çº¿æ€§æ˜ å°„ $ \lambda \hat{Y} $
  - RePPI / EIF å¯ä½¿ç”¨çµæ´»æ¨¡å‹ï¼ˆå¦‚ GAMã€splineï¼‰éå‚æ•°ä¼°è®¡ $ \mu(\hat{Y}) $
- **å¼€æºå®ç°**ï¼šæä¾›ä»£ç åº“ `https://github.com/yiqunchen/debias-llm-as-a-judge`ï¼Œä¾¿äºå¤ç°ä¸åº”ç”¨ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| æ–¹æ³• | ä¼˜åŠ¿ |
|------|------|
| **PPI / PPI++** | ä¸éœ€æ˜¾å¼å»ºæ¨¡è¯¯åˆ†ç±»æœºåˆ¶ï¼ˆmisclassification modelï¼‰ï¼Œé€šè¿‡æ®‹å·®æ ¡å‡†å®ç°æ— åæ¨æ–­ï¼Œç¨³å®šæ€§é«˜ |
| **EIF-based estimator** | åœ¨äºŒå…ƒæƒ…å†µä¸‹è¾¾åˆ°ç†è®ºæœ€å°æ–¹å·®ï¼Œåœ¨è¿ç»­/ç¦»æ•£è¯„åˆ†åœºæ™¯å¯é€šè¿‡çµæ´»å›å½’è¿›ä¸€æ­¥æå‡æ•ˆç‡ |
| **ä¼˜äº RG** | é¿å…äº† RG ä¸­å›  $ q_0 + q_1 - 1 $ æ¥è¿‘ 0 è€Œå¯¼è‡´çš„æ–¹å·®çˆ†ç‚¸é—®é¢˜ï¼Œå°¤å…¶åœ¨ä½è´¨é‡ judge æˆ–å°æ ‡æ³¨é¢„ç®—ä¸‹è¡¨ç°æ›´ç¨³å¥ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **æ•°æ®é›†**
1. **æ¨¡æ‹Ÿæ•°æ®ï¼ˆSimulation Studyï¼‰**
   - **äºŒå…ƒè¾“å‡º**ï¼šç”Ÿæˆ $ Y \sim \text{Bern}(\theta) $ï¼Œ$ \hat{Y} $ ç”±å›ºå®šæ•æ„Ÿåº¦ $ q_1 $ å’Œç‰¹å¼‚åº¦ $ q_0 $ çš„è¯¯åˆ†ç±»æ¨¡å‹ç”Ÿæˆã€‚
   - **è¿ç»­è¾“å‡º**ï¼šæ‰©å±• Ji et al. [2025] è®¾ç½®ï¼Œ$ Y \in \mathbb{R} $ è¿ç»­ï¼Œ$ \hat{Y} \in \{1,2,3\} $ ç¦»æ•£è¯„åˆ†ï¼Œç ”ç©¶å°ºåº¦ä¸åŒ¹é…ä¸‹çš„æ ¡å‡†é—®é¢˜ã€‚
   - æ€»æ ·æœ¬é‡ $ N=2000 $ï¼Œæ ‡æ³¨æ¯”ä¾‹ $ m/N \in \{1\%, 5\%, 10\%\} $

2. **çœŸå®æ•°æ®ï¼ˆReal Data Applicationï¼‰**
   - ä½¿ç”¨ **arena-human-preference-140k** æ•°æ®é›†ï¼ˆæ¥è‡ª Chatbot Arenaï¼‰
   - é€‰å–ä¸‰ç»„é«˜é¢‘æ¨¡å‹å¯¹ï¼š
     - Claude Opus 4 vs. Gemini 2.5 Flash
     - Claude Opus 4 vs. Gemini 2.5 Pro
     - Claude Opus 4 vs. Qwen3-235B
   - ä½¿ç”¨ **GPT-4o-mini** å’Œ **GPT-5.2** ä½œä¸º LLM Judge ç”Ÿæˆ $ \hat{Y} $
   - æ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼šéšæœºæŠ½å– 10% ä½œä¸º calibration setï¼Œå…¶ä½™ 90% ä»…ä¿ç•™ $ \hat{Y} $

---

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**

#### **è¯„ä¼°ç›®æ ‡**
ä¼°è®¡äººç±»åå¥½èƒœç‡ $ \theta = E[Y] $ï¼Œå¹¶æ„é€ æœ‰æ•ˆçš„ 90% ç½®ä¿¡åŒºé—´ã€‚

#### **å¯¹æ¯”æ–¹æ³•**
| æ–¹æ³• | æè¿° |
|------|------|
| **Naive** | ç›´æ¥ç”¨ LLM åˆ¤æ–­å‡å€¼ $ \bar{\hat{Y}} $ ä¼°è®¡ $ \theta $ |
| **Rogan-Gladen (RG)** | åŸºäºæ··æ·†çŸ©é˜µåæ¼”çš„ä¼ ç»Ÿæµ‹é‡è¯¯å·®æ ¡æ­£æ–¹æ³• |
| **PPI** | Prediction-Powered Inferenceï¼Œä½¿ç”¨ $ \hat{Y} $ ä½œä¸ºä»£ç† |
| **PPI++** | å¼•å…¥å¯è°ƒå‚æ•° $ \lambda $ çš„ PPI å˜ä½“ï¼Œä¼˜åŒ–æ–¹å·® |
| **MLE** | è”åˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆåœ¨äºŒå…ƒæƒ…å†µä¸‹ç­‰ä»·äº EIFï¼‰ |
| **EIF** | åŸºäº Efficient Influence Function æ„é€ çš„ä¼°è®¡å™¨ |
| **EIF-linear / EIF-GAM / EIF-spline** | ä¸åŒæ–¹å¼ä¼°è®¡ $ E[Y|\hat{Y}] $ çš„ EIF å˜ä½“ |

#### **è¯„ä¼°æŒ‡æ ‡**
- **Bias**ï¼šä¼°è®¡åå·®
- **Coverage**ï¼š90% CI åŒ…å«çœŸå€¼çš„æ¯”ä¾‹ï¼ˆç†æƒ³ä¸º 0.9ï¼‰
- **CI Width**ï¼šç½®ä¿¡åŒºé—´çš„å¹³å‡å®½åº¦ï¼ˆè¶Šçª„è¶Šå¥½ï¼‰

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆäºŒå…ƒè¾“å‡ºï¼‰**

| æ–¹æ³• | Bias | Coverage | CI Widthï¼ˆç›¸å¯¹ï¼‰ |
|------|------|----------|------------------|
| **Naive** | é«˜åå·®ï¼ˆä¾èµ– $ q_0, q_1 $ï¼‰ | ä¸¥é‡ under-coverage | æœ€çª„ä½†æ— æ•ˆ |
| **RG** | å°æ ·æœ¬ä¸‹åé«˜ | å€¾å‘ over-coverage | æå®½ï¼ˆå°¤å…¶å½“ $ q_0+q_1 \approx 1 $ï¼‰ |
| **PPI** | æ¥è¿‘é›¶ | æ¥è¿‘ 90% | è¾ƒå®½ |
| **PPI++ / EIF / MLE** | æ¥è¿‘é›¶ | æ¥è¿‘ 90% | **æœ€çª„ï¼ˆæ¯” PPI çª„ 35â€“55%ï¼‰** |

> âœ… **æ ¸å¿ƒå‘ç°**ï¼šåœ¨ $ q_0 = q_1 = 0.6 $ï¼ˆä½è´¨é‡ judgeï¼‰æ—¶ï¼ŒRG çš„ CI å®½åº¦å¯è¾¾ EIF çš„ **10 å€ä»¥ä¸Š**ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

| å¯¹æ¯”ç»´åº¦ | ç»“æœ |
|---------|------|
| **PPI vs RG** | $ V_{\text{PPI}} \leq V_{\text{RG}} $ï¼Œä¸”ä¸¥æ ¼å°äºé™¤é $ q_0 = q_1 = 1 $ï¼ˆå®Œç¾ judgeï¼‰ |
| **PPI++ vs PPI** | PPI++ æ˜¾è‘—é™ä½æ–¹å·®ï¼Œå°¤å…¶åœ¨ä½æ ‡æ³¨é¢„ç®—ä¸‹ |
| **PPI++ vs EIF/MLEï¼ˆäºŒå…ƒï¼‰** | æ¸è¿‘ç­‰ä»·ï¼Œå®éªŒè¯æ˜æ€§èƒ½å‡ ä¹ä¸€è‡´ |
| **RG vs æ‰€æœ‰å…¶ä»–æ–¹æ³•** | æ–¹å·®æœ€å¤§ï¼Œå°¤å…¶åœ¨å° $ m $ æˆ–ä½ $ q_0+q_1 $ æ—¶ä¸ç¨³å®š |

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆè¿ç»­è¾“å‡ºåœºæ™¯ï¼‰**

åœ¨ $ Y \in \mathbb{R}, \hat{Y} \in \{1,2,3\} $ åœºæ™¯ä¸‹æµ‹è¯•ä¸åŒ $ E[Y|\hat{Y}] $ å»ºæ¨¡æ–¹å¼ï¼š

| æ–¹æ³• | Coverage | CI Width |
|------|----------|----------|
| **Naive** | éšåå·®å¢å¤§è¿…é€Ÿé™è‡³ 0% | çª„ä½†æ— æ•ˆ |
| **PPI** | ä¿æŒ ~90% | æœ€å®½ |
| **PPI++ / EIF-linear** | ~90% | ä¸­ç­‰ |
| **EIF-GAM / EIF-spline / per-category** | ~90% | **æœ€çª„ï¼ˆæ•ˆç‡æœ€é«˜ï¼‰** |

> ğŸ” **å‘ç°**ï¼šå½“çœŸå® $ E[Y|\hat{Y}] $ éçº¿æ€§æ—¶ï¼Œä½¿ç”¨çµæ´»æ¨¡å‹ï¼ˆGAM/splineï¼‰èƒ½æ˜¾è‘—æå‡æ•ˆç‡ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **PPI++ å’Œ EIF æ˜¯å½“å‰æœ€ä¼˜é€‰æ‹©**ï¼š
   - åœ¨äºŒå…ƒè¾“å‡ºä¸‹ï¼ŒPPI++ ç»è¿‡æœ€ä¼˜è°ƒå‚åä¸ EIF å’Œ MLE æ¸è¿‘ç­‰ä»·ï¼Œè¾¾åˆ°ç†è®ºæœ€å°æ–¹å·®ã€‚
   - åœ¨ä¸€èˆ¬åœºæ™¯ä¸‹ï¼ŒEIF æ¡†æ¶å…è®¸ä½¿ç”¨çµæ´»æ¨¡å‹ä¼°è®¡ $ E[Y|\hat{Y}] $ï¼Œè¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚

2. **RG æ–¹æ³•æ•ˆç‡ä½ä¸‹**ï¼š
   - å…¶æ–¹å·®å— $ 1/(q_0 + q_1 - 1)^2 $ æ”¾å¤§ï¼Œåœ¨ judge èƒ½åŠ›æ¥è¿‘éšæœºæˆ–æ ‡æ³¨é¢„ç®—å°æ—¶è¡¨ç°æå·®ã€‚

3. **PPI è™½ç®€å•ä½†éæœ€ä¼˜**ï¼š
   - è™½ç„¶æ— åä¸”æ˜“äºå®ç°ï¼Œä½†å…¶éšå«å‡è®¾ $ E[Y|\hat{Y}] \approx \hat{Y} $ å¯¼è‡´æ•ˆç‡æŸå¤±ã€‚

4. **æ ¡å‡†æ–¹æ³•åº”é€‚é…è¾“å‡ºç±»å‹**ï¼š
   - äºŒå…ƒä»»åŠ¡å¯ç”¨ PPI++ï¼›
   - å¤šçº§è¯„åˆ†æˆ–è¿ç»­è¯„åˆ†ä»»åŠ¡å»ºè®®ä½¿ç”¨éå‚æ•°æ ¡å‡†ï¼ˆå¦‚ GAMã€splineï¼‰ä»¥æ•æ‰å¤æ‚å…³ç³»ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ– MCAR å‡è®¾**ï¼šè¦æ±‚ calibration set æ˜¯éšæœºæŠ½æ ·ï¼Œè‹¥å­˜åœ¨é€‰æ‹©åå·®ï¼ˆselection biasï¼‰åˆ™å¯èƒ½å¤±æ•ˆã€‚
- **æœªå¤„ç† instance-level heterogeneity**ï¼šå‡è®¾è¯¯åˆ†ç±»æ¦‚ç‡å…¨å±€æ’å®šï¼Œæœªè€ƒè™‘è¾“å…¥ç‰¹å¾ç›¸å…³çš„åŠ¨æ€è¯¯å·®ã€‚
- **æœªæ¶µç›–åˆ†å¸ƒåç§»ï¼ˆdistribution shiftï¼‰**ï¼šè®­ç»ƒä¸éƒ¨ç½²é—´çš„æ•°æ®åˆ†å¸ƒå˜åŒ–ï¼ˆå¦‚æ–‡åŒ–å·®å¼‚ï¼‰ä¼šå½±å“æ ¡å‡†æ•ˆæœã€‚
- **è¿ç»­è¾“å‡ºä¸‹ç¼ºä¹ MLE å¯¹åº”ç‰©**ï¼šæ— æ³•åƒäºŒå…ƒæƒ…å†µé‚£æ ·è¿›è¡Œå®Œå…¨å‚æ•°åŒ–å»ºæ¨¡ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **Instance-dependent misclassification models**ï¼š
   - å¼•å…¥åå˜é‡ $ X $ï¼Œå»ºæ¨¡ $ E[Y|X,\hat{Y}] $ï¼Œæå‡æ ¡å‡†ç²¾åº¦ã€‚
2. **å¤š Judge Ensemble Methods**ï¼š
   - èåˆå¤šä¸ª LLM judges çš„åˆ¤æ–­ï¼Œé€šè¿‡åŠ æƒæˆ–å­¦ä¹ ç»„åˆè§„åˆ™æé«˜é²æ£’æ€§ã€‚
3. **Distribution Shift Robustness**ï¼š
   - ç ”ç©¶ covariate shift ä¸ label shift ä¸‹çš„è‡ªé€‚åº”æ ¡å‡†ç­–ç•¥ã€‚
4. **æ‰©å±•è‡³æ›´å¤æ‚å‚æ•°**ï¼š
   - å½“å‰èšç„¦å‡å€¼å‚æ•°ï¼Œæœªæ¥å¯æ¨å¹¿è‡³ rankingã€varianceã€conditional expectations ç­‰ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡é€šè¿‡ **semiparametric efficiency theory** ç»Ÿä¸€äº† LLM-as-a-Judge çš„æ ¡å‡†æ–¹æ³•ï¼Œè¯æ˜ **PPI++ å’Œ EIF åœ¨äºŒå…ƒä»»åŠ¡ä¸­è¾¾åˆ°æœ€ä¼˜æ•ˆç‡**ï¼Œå¹¶æ¨èåœ¨å®è·µä¸­ä¼˜å…ˆä½¿ç”¨è¿™äº›æ–¹æ³•æ›¿ä»£ä¼ ç»Ÿçš„ Rogan-Gladen ä¼°è®¡å™¨ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚æˆ– LLM judge è´¨é‡è¾ƒä½çš„åœºæ™¯ä¸­ã€‚

</details>

---

### 10. [Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding](https://arxiv.org/abs/2601.05724)

**Authors**: Yuxuan Zhou, Fei Huang, Heng Li, Fengyi Wu, Tianyu Wang, Jianwei Zhang, Junyang Lin, Zhi-Qi Cheng  
**Category**: cs.AI  
**Published**: 2026-01-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.05724v1  

#### Abstract
Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rel...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹ **Speculative Decoding (SD)** ä¸­çš„ **verificationç“¶é¢ˆ** å’Œ **joint intractability** é—®é¢˜å±•å¼€ç ”ç©¶ã€‚åœ¨ä¼ ç»Ÿçš„ token-wise speculative decoding ä¸­ï¼ŒéªŒè¯è¿‡ç¨‹é€ä¸ª token è¿›è¡Œï¼Œè™½ç„¶èƒ½ä¿è¯åˆ†å¸ƒä¿çœŸï¼ˆdistribution fidelityï¼‰ï¼Œä½†æ¥å—ç‡è¾ƒä½ï¼Œé™åˆ¶äº†æ¨ç†åŠ é€Ÿæ•ˆæœã€‚è€Œåºåˆ—çº§è”åˆéªŒè¯ï¼ˆsequence-level verificationï¼‰è™½å¯æå‡æ¥å—ç‡ï¼Œä½†ç”±äºéœ€è¦è®¡ç®—å®Œæ•´åºåˆ—çš„è”åˆæ¦‚ç‡ï¼Œåœ¨è‡ªå›å½’æ¨¡å‹ä¸­é¢ä¸´ **è®¡ç®—ä¸å¯è¡Œï¼ˆjoint intractabilityï¼‰**ã€‚

ç°æœ‰æ–¹æ³•å¦‚ Blockwise Verification è™½ç„¶éƒ¨åˆ†ç¼“è§£è¯¥é—®é¢˜ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ç¼ºé™·ï¼š
- æ¥å—ç‡æœªè¾¾ç†è®ºä¸Šé™ï¼›
- æœºåˆ¶ä¸é€æ˜ï¼Œéš¾ä»¥è§£é‡Šï¼›
- éš¾ä»¥ä¸å…¶ä»–æ¡†æ¶ï¼ˆå¦‚ multi-draftï¼‰å…¼å®¹ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šHierarchical Speculative Decoding (HSD)

ä½œè€…æå‡º **Hierarchical Speculative Decoding (HSD)**ï¼Œä¸€ç§**ç†è®ºä¸Šæ— æŸï¼ˆprovably losslessï¼‰** çš„éªŒè¯æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- å¼•å…¥ **å±‚æ¬¡åŒ–åˆ†æ”¯é‡é‡‡æ ·ç­–ç•¥ï¼ˆhierarchical branch resamplingï¼‰**ï¼Œé€šè¿‡æ„å»ºå¤šå±‚çº§çš„ resampling åˆ†å¸ƒï¼Œé€æ­¥è¡¥å¿ draft åˆ†å¸ƒä¸ target åˆ†å¸ƒä¹‹é—´çš„æ¦‚ç‡è´¨é‡å·®å¼‚ã€‚
- åˆ©ç”¨ **branch divergence** å’Œ **asymmetry** æ¦‚å¿µï¼Œé‡åŒ–å±€éƒ¨åˆ†æ”¯ä¸­çš„â€œç¼ºå¤±â€ä¸â€œè¿‡å‰©â€æ¦‚ç‡è´¨é‡ï¼Œå¹¶é€šè¿‡å±‚æ¬¡ç»“æ„å®ç°è·¨åˆ†æ”¯çš„æ¦‚ç‡å†åˆ†é…ã€‚
- è®¾è®¡ **Capped Branch Resampling** æœºåˆ¶ï¼Œåœ¨ä¿æŒåˆ†å¸ƒä¿çœŸçš„å‰æä¸‹ï¼Œå°†å¤æ‚çš„é€’å½’é‡é‡‡æ ·ç®€åŒ–ä¸º**å•æ¬¡é‡é‡‡æ ·æ“ä½œ**ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ä¼˜åŠ¿è¯´æ˜ |
|------|----------|
| **ç†è®ºæ€§è´¨** | è¯æ˜äº† HSD æ˜¯ **lossless** çš„ï¼Œå³å®Œå…¨æ¢å¤ç›®æ ‡åˆ†å¸ƒ $p(x)$ï¼Œæ— ç²¾åº¦æŸå¤±ã€‚ |
| **æ•ˆç‡æå‡** | æ˜¾è‘—æé«˜ **expected number of accepted tokens**ï¼Œä»è€Œæå‡ decoding speedã€‚ |
| **é€šç”¨æ€§ä¸å…¼å®¹æ€§** | å¯æ— ç¼é›†æˆåˆ°å¤šç§ speculative decoding æ¡†æ¶ä¸­ï¼ˆå¦‚ EAGLE-3ã€multi-draftï¼‰ï¼Œä¸”ä¸ drafting é˜¶æ®µæ­£äº¤ã€‚ |
| **å¯è§£é‡Šæ€§** | å±‚æ¬¡åŒ–è®¾è®¡æä¾›äº†æ¸…æ™°çš„æ¦‚ç‡è¡¥å¿è·¯å¾„ï¼Œå¢å¼ºäº†æ–¹æ³•çš„å¯è§£é‡Šæ€§ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
å®éªŒåœ¨ä¸‰ä¸ªå…¸å‹ä»»åŠ¡ä¸Šè¿›è¡Œï¼Œæ¶µç›–æ•°å­¦ã€ä»£ç ç”Ÿæˆå’Œæ–‡æœ¬æ‘˜è¦ï¼š
- **GSM8K**ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡
- **HumanEval**ï¼šä»£ç ç”Ÿæˆä»»åŠ¡
- **CNN/DailyMail**ï¼šæ–‡æœ¬æ‘˜è¦ä»»åŠ¡

### å®éªŒè®¾ç½®
- **æ¨¡å‹é…ç½®**ï¼š
  - Draft Modelï¼š`Qwen2.5-0.5B`
  - Target Modelsï¼š`Qwen2.5-14B`, `32B`, `72B`ï¼ˆGPTQ-8bit é‡åŒ–ï¼‰
- **æ¸©åº¦ï¼ˆtemperatureï¼‰**ï¼šé»˜è®¤ä¸º 1.0
- **ç¡¬ä»¶å¹³å°**ï¼šå•å¼  NVIDIA H20 æˆ– H200 GPU
- **draft length**ï¼šé»˜è®¤ä¸º 10

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **Block Efficiency (BE)** | æ¯æ¬¡ target model è°ƒç”¨ç”Ÿæˆçš„å¹³å‡ token æ•°ï¼Œåæ˜ å†…åœ¨æ•ˆç‡ |
| **Decoding Speed (DS)** | æ¯ç§’ç”Ÿæˆçš„ token æ•°ï¼Œåæ˜ å®é™…ååé‡ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Token-wise Verification** (Leviathan et al., 2023)ï¼šæ ‡å‡† speculative decoding
- **Blockwise Verification** (Sun et al., 2024)ï¼šå½“å‰ä¸»æµçš„ lossless åºåˆ—çº§éªŒè¯æ–¹æ³•
- **Multi-draft è®¾ç½®ä¸‹çš„ Tokenwise**ï¼šç”¨äºè¯„ä¼° HSD åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ‰©å±•èƒ½åŠ›

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

#### åœ¨ Qwen2.5 ç³»åˆ—ä¸Šçš„å¹³å‡æå‡ï¼ˆvs. Tokenwiseï¼‰
| æ•°æ®é›† | Block Efficiency æå‡ | Decoding Speed æå‡ |
|--------|------------------------|----------------------|
| GSM8K | +3.3% ~ 5.4% | +3.3% ~ 10.7% |
| HumanEval | +3.3% ~ 12.3% | +4.4% ~ 11.4% |
| CNN/DailyMail | +4.2% ~ 8.4% | +3.6% ~ 7.2% |
| **å¹³å‡** | **~6.2%** | **~6.7%** |

> âœ… HSD åœ¨æ‰€æœ‰æ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡ä¸Šå‡ä¸€è‡´ä¼˜äº Tokenwise å’Œ Blockwiseã€‚

#### ä¸ Blockwise å¯¹æ¯”
- HSD åœ¨ BE ä¸Šå¹³å‡é¢†å…ˆ Blockwise **çº¦ 3-4 ä¸ªç™¾åˆ†ç‚¹**
- åœ¨ HumanEval ä¸Šï¼ŒHSD è¾¾åˆ° **5.49 BE @32B**ï¼Œè¿œè¶… Blockwise çš„ 5.15

---

### Multi-draft åœºæ™¯ä¸‹çš„è¡¨ç°ï¼ˆTable 2ï¼‰
åœ¨ multi-draft è®¾ç½®ä¸‹ï¼ˆ11 ä¸ªå€™é€‰è‰æ¡ˆï¼‰ï¼ŒHSD ä¾ç„¶è¡¨ç°å‡ºè‰²ï¼š
| æŒ‡æ ‡ | æå‡å¹…åº¦ |
|------|---------|
| Block Efficiency | +2.8% ~ 3.8% |
| Decoding Speed | +2.0% ~ 8.9% |
> ğŸ“Œ è¡¨æ˜ HSD å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œä¸å…¶ä»–æŠ€æœ¯çš„å…¼å®¹æ€§ã€‚

---

### æ‰©å±•å®éªŒç»“æœï¼ˆTable 4ï¼‰

#### åœ¨ LLaMA-3 æ¨¡å‹å®¶æ—ä¸Šçš„è¡¨ç°
| æ–¹æ³• | Block Efficiency | Decoding Speed |
|------|------------------|----------------|
| Tokenwise | 6.83 | 8.41 |
| Blockwise | 7.32 (+7.2%) | 8.87 (+5.5%) |
| **HSD (Ours)** | **7.43 (+8.8%)** | **9.18 (+9.2%)** |

> âœ… éªŒè¯äº† HSD åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

#### é›†æˆè‡³ EAGLE-3 æ¡†æ¶
| æ–¹æ³• | Block Efficiency | Decoding Speed |
|------|------------------|----------------|
| EAGLE-3 | 3.40 | 71.59 |
| **EAGLE-3 + HSD (Ours)** | **3.55 (+4.4%)** | **80.49 (+12.4%)** |

> â­ **å…³é”®äº®ç‚¹**ï¼šä»…æ›¿æ¢éªŒè¯æ¨¡å—ï¼Œå°±åœ¨ EAGLE-3 ä¸Šå®ç° **è¶…è¿‡ 12% çš„æ€§èƒ½å¢ç›Š**ï¼Œè¾¾åˆ° SOTA è§£ç æ•ˆç‡ã€‚

---

### æ¶ˆèå®éªŒç»“æœ

#### ä¸åŒæ¸©åº¦çš„å½±å“ï¼ˆTable 3aï¼‰
- åœ¨ $t \in \{0.6, 0.8, 1.0\}$ ä¸‹ï¼ŒHSD å§‹ç»ˆä¼˜äºåŸºçº¿ï¼Œè¡¨æ˜å…¶å¯¹é‡‡æ ·ç­–ç•¥é²æ£’ã€‚

#### ä¸åŒ draft length çš„å½±å“ï¼ˆTable 3bï¼‰
| Draft Length | HSD BE | Tokenwise BE | æå‡ |
|--------------|--------|---------------|------|
| 5 | 4.59 | 4.48 | +2.5% |
| 10 | 6.65 | 6.44 | +3.3% |
| 15 | **7.88** | 7.61 | **+3.5%** |

> ğŸ”º éšç€ draft length å¢åŠ ï¼ŒHSD çš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚

#### Capping æœºåˆ¶æ¶ˆèï¼ˆTable A.3ï¼‰
ç§»é™¤ capping ä¼šå¯¼è‡´ï¼š
- å°‘é‡æ•ˆç‡æå‡ï¼ˆå› æ›´æ¿€è¿›æ¥å—ï¼‰
- ä½† **accuracy ä¸‹é™**ï¼ˆå¦‚ GSM8K ä» 84.96% â†’ 84.40%ï¼‰
> âœ… è¯æ˜ **capping æ˜¯ç»´æŒ distribution fidelity çš„å…³é”®è®¾è®¡**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **HSD æˆåŠŸå…‹æœäº† joint intractability**ï¼šé€šè¿‡å±‚æ¬¡åŒ–åˆ†æ”¯é‡é‡‡æ ·ï¼Œå®ç°äº†åœ¨ä¸ç‰ºç‰²åˆ†å¸ƒä¿çœŸçš„å‰æä¸‹æœ€å¤§åŒ–æ¥å— token æ•°ã€‚
2. **HSD æ˜¯ç›®å‰æœ€é«˜æ•ˆçš„ lossless verification æ–¹æ³•ä¹‹ä¸€**ï¼šåœ¨å¤šä¸ªæ¨¡å‹å’Œä»»åŠ¡ä¸Š consistently æå‡ 6â€“12%ï¼Œå°¤å…¶åœ¨ EAGLE-3 ä¸Šå®ç° **>12% çš„ç«¯åˆ°ç«¯åŠ é€Ÿ**ã€‚
3. **æ–¹æ³•å…·æœ‰å¼ºé€šç”¨æ€§å’Œå¯é›†æˆæ€§**ï¼šå¯è½»æ¾åµŒå…¥ç°æœ‰ speculative decoding æ¡†æ¶ï¼Œæ— éœ€ä¿®æ”¹ drafting é€»è¾‘ã€‚
4. **ç†è®ºä¸å®è·µç»Ÿä¸€**ï¼šç®—æ³•è®¾è®¡åŸºäºä¸¥æ ¼çš„æ¦‚ç‡è®ºæ¨å¯¼ï¼Œä¸”å®é™…å¼€é”€æä½ï¼ˆéªŒè¯é˜¶æ®µ <1% æ€»è€—æ—¶ï¼‰ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰å®ç°ä¾èµ–äºå¯¹ draft åºåˆ—çš„æ‰€æœ‰å‰ç¼€è¿›è¡Œæ¦‚ç‡è®¿é—®ï¼Œå¯èƒ½åœ¨æç«¯é•¿åºåˆ—æˆ–ç‰¹æ®Šæ¶æ„ä¸­å—é™ã€‚
- è™½ç„¶å…¼å®¹ multi-draftï¼Œä½†åœ¨é«˜å¹¶å‘ draft åœºæ™¯ä¸‹éœ€é¢å¤–å¤„ç† prefix matchingï¼ˆè§ Algorithm 3ï¼‰ã€‚
- å¯¹ extremely low-probability drafts çš„å¤„ç†æ•ˆç‡ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ HSD ä¸ **distillation-based drafters** çš„ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡ drafting è´¨é‡ã€‚
- å°† HSD åº”ç”¨äº **vision-language models** æˆ– **speech generation** ç­‰å¤šæ¨¡æ€åœºæ™¯ã€‚
- ç ”ç©¶åŠ¨æ€è°ƒæ•´ **capping threshold** æˆ– **hierarchy depth** çš„è‡ªé€‚åº”ç­–ç•¥ã€‚
- å¼€å‘æ›´é«˜æ•ˆçš„ CUDA kernel å®ç°ï¼Œè¿›ä¸€æ­¥å‹ç¼© verification overheadã€‚

---

> ğŸ”— **å¼€æºä¿¡æ¯**ï¼šä»£ç å·²å…¬å¼€äº GitHubï¼š[https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding](https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding)

</details>

---

### 11. [Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR](https://arxiv.org/abs/2601.05607)

**Authors**: Zijun Min, Bingshuai Liu, Ante Wang, Long Zhang, Anxiang Zeng, Haibo Zhang, Jinsong Su  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.05607v1  

#### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GR...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

åœ¨ **Reinforcement Learning with Verifiable Rewards (RLVR)** æ¡†æ¶ä¸­ï¼Œç°æœ‰çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•å­˜åœ¨**ç²’åº¦ä¸åŒ¹é…**çš„é—®é¢˜ï¼š

- **GRPO** ä½¿ç”¨ **token-level importance ratios** è¿›è¡Œæ›´æ–°ï¼Œä¿ç•™äº†ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…ï¼ˆcredit assignmentï¼‰ï¼Œä½†ç”±äºå…¶ä¸åºåˆ—çº§å¥–åŠ±ï¼ˆsequence-level rewardsï¼‰ä¸ä¸€è‡´ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­é‡è¦æ€§æ¯”ç‡æ–¹å·®é«˜ã€æ›´æ–°ä¸ç¨³å®šã€‚
- **GSPO** æ”¹ç”¨ **sequence-level importance ratios**ï¼Œæå‡äº†ç¨³å®šæ€§å¹¶æ›´å¥½åœ°å¯¹é½åºåˆ—çº§å¥–åŠ±ï¼Œä½†ç‰ºç‰²äº† token çº§åˆ«çš„ä¿¡ç”¨åˆ†é…ï¼Œä½¿å¾—æ¨¡å‹éš¾ä»¥å­¦ä¹ åˆ°å…³é”®æ¨ç†æ­¥éª¤ä¸­çš„ç»†å¾®å·®å¼‚ã€‚

å› æ­¤ï¼Œå•ä¸€é‡‡ç”¨ token-level æˆ– sequence-level ä¼˜åŒ–å‡æ— æ³•å…¼é¡¾**è¡¨è¾¾èƒ½åŠ›**ä¸**è®­ç»ƒç¨³å®šæ€§**ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **Dynamic Hybrid Policy Optimization (DHPO)**ï¼Œä¸€ç§ç»Ÿä¸€çš„ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒæœºåˆ¶èåˆ token-level å’Œ sequence-level ä¼˜åŠ¿ï¼š

#### âœ… **æ··åˆé‡è¦æ€§æ¯”ç‡ï¼ˆHybrid Importance Ratioï¼‰**
å°† token-level å’Œ sequence-level çš„é‡è¦æ€§æ¯”ç‡è¿›è¡ŒåŠ æƒç»„åˆï¼š
$$
m_{i,t}(\theta) = w_{i,t} \cdot r_{i,t}(\theta) + (1 - w_{i,t}) \cdot s_{i,t}(\theta)
$$
å…¶ä¸­ $ w_{i,t} \in [0,1] $ æ˜¯æ··åˆæƒé‡ï¼Œæ§åˆ¶ä¸¤ç§ç²’åº¦çš„è´¡çŒ®æ¯”ä¾‹ã€‚

ä½œè€…æ¢ç´¢äº†ä¸¤ç§æ··åˆæ–¹å¼ï¼š
- **Averaged Mixing (DHPO-A)**ï¼šå›ºå®šæƒé‡ $ w_{i,t} = 0.5 $ï¼Œç®€å•å¹³å‡ã€‚
- **Entropy-guided Mixing (DHPO-E)**ï¼šåŸºäºå½“å‰ç­–ç•¥è¾“å‡ºçš„ token ç†µåŠ¨æ€è°ƒæ•´æƒé‡ã€‚å½“ä¸ç¡®å®šæ€§é«˜æ—¶æ›´ä¾èµ– token-level ä¿¡å·ä»¥ä¿ƒè¿›æ¢ç´¢ï¼›å½“ç½®ä¿¡åº¦ä¸Šå‡åè½¬å‘ sequence-level ä¿¡å·ä»¥å¢å¼ºå…¨å±€ä¸€è‡´æ€§ã€‚

#### âœ… **åˆ†æ”¯ç‰¹å¼‚æ€§å‰ªè£ï¼ˆBranch-Specific Clippingï¼‰**
ä¼ ç»Ÿ PPO é£æ ¼çš„å‰ªè£è‹¥ç›´æ¥åº”ç”¨äºæ··åˆæ¯”ç‡ï¼Œå¯èƒ½å› ä¸¤åˆ†æ”¯æ•°å€¼èŒƒå›´å·®å¼‚å¤§è€Œå¤±æ•ˆã€‚ä¸ºæ­¤ï¼ŒDHPO åœ¨**æ··åˆå‰åˆ†åˆ«å¯¹ä¸¤ä¸ªåˆ†æ”¯ç‹¬ç«‹å‰ªè£**ï¼š
$$
\hat{m}_{i,t}(\theta) = w_{i,t} \cdot \text{clip}(r_{i,t}, 1-\epsilon^{\text{token}}_{\text{low}}, 1+\epsilon^{\text{token}}_{\text{high}}) + (1-w_{i,t}) \cdot \text{clip}(s_{i,t}, 1-\epsilon^{\text{seq}}_{\text{low}}, 1+\epsilon^{\text{seq}}_{\text{high}})
$$
è¯¥è®¾è®¡é˜²æ­¢ä»»ä¸€åˆ†æ”¯çš„å¼‚å¸¸å€¼ä¸»å¯¼æ›´æ–°ï¼Œæå‡è®­ç»ƒé²æ£’æ€§ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºé™· | DHPO å¦‚ä½•æ”¹è¿› |
|------|------|------|----------------|
| **GRPO** | ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ï¼Œåˆ©äºå±€éƒ¨æ¢ç´¢ | æ–¹å·®é«˜ï¼Œæ˜“ç†µåç¼©ï¼ˆentropy collapseï¼‰ | å¼•å…¥ sequence-level ç¨³å®šé¡¹ï¼Œé™ä½æ–¹å·® |
| **GSPO** | æ›´æ–°ç¨³å®šï¼Œå¯¹é½åºåˆ—å¥–åŠ± | å¿½ç•¥ token å·®å¼‚ï¼Œæ›´æ–°ç²—ç³™ | ä¿ç•™ token-level ä¿¡å·ï¼Œå¢å¼ºè¡¨è¾¾åŠ› |
| **DHPO** | âœ… åŠ¨æ€å¹³è¡¡ä¸¤è€… âœ… æ›´å¼ºæ³›åŒ–èƒ½åŠ› âœ… æ›´å¥åº·è®­ç»ƒåŠ¨æ€ | â€”â€” | ç»Ÿä¸€äºä¸€ä¸ª clipped surrogate objective |

> **æ ¸å¿ƒæ€æƒ³**ï¼šä¸æ˜¯åœ¨ token-level å’Œ sequence-level ä¹‹é—´åšé€‰æ‹©ï¼Œè€Œæ˜¯å®ç°**è¿ç»­æ’å€¼**ï¼ˆcontinuous interpolationï¼‰ï¼Œè®©ä¼˜åŒ–è¿‡ç¨‹â€œæ„ŸçŸ¥è½¨è¿¹â€ä¸”â€œå…³æ³¨ç»†èŠ‚â€ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

æ‰€æœ‰å®éªŒåŸºäº **SimpleRL æ•°æ®é›†**ï¼ˆ8,192 ä¸ªæ ·æœ¬ï¼‰ï¼Œå¹¶åœ¨ä»¥ä¸‹ **7 ä¸ªæ•°å­¦æ¨ç† benchmark** ä¸Šè¯„ä¼°æ€§èƒ½ï¼š

- **AIME 2024 / 2025**ï¼ˆAvg@32ï¼‰
- **AMC 2023**ï¼ˆAvg@4ï¼‰
- **OlympiadBench**
- **MATH-500**
- **Minerva Math**
- **GSM8K**

è¿™äº›ä»»åŠ¡å…·æœ‰å¤šæ­¥æ¨ç†ã€å¯éªŒè¯è§£çš„ç‰¹ç‚¹ï¼Œé€‚åˆ RLVR èŒƒå¼ã€‚

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

| è®¾ç½®é¡¹ | æè¿° |
|--------|------|
| **æ¨¡å‹ç³»åˆ—** | Qwen3 ç³»åˆ—ï¼š`Qwen3-1.7B-Base`, `Qwen3-4B-Base`, `Qwen3-30B-A3B-Base`ï¼ˆå« MoEï¼‰ |
| **è®­ç»ƒæ¡†æ¶** | VERL + vLLM rollout engine |
| **ç¡¬ä»¶é…ç½®** | 32Ã— NVIDIA H100 GPUsï¼ˆ4èŠ‚ç‚¹Ã—8å¡ï¼‰ |
| **å“åº”é•¿åº¦** | è®­ç»ƒæ—¶æœ€å¤§ 4,096 tokensï¼›è¯„ä¼°æ—¶æ‰©å±•è‡³ 16K tokens |
| **é‡‡æ ·å‚æ•°** | æ¸©åº¦=1.0ï¼Œæ¯ prompt ç”Ÿæˆ 16 æ¡å“åº” |
| **å­¦ä¹ ç‡** | Actor å­¦ä¹ ç‡ä¸º $1 \times 10^{-6}$ |
| **è¯„ä¼°æ–¹å¼** | Pass@1 å¹³å‡å¤šæ¬¡é‡‡æ ·ç»“æœï¼ˆå¦‚ Avg@32 æˆ– Avg@4ï¼‰ |

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

æ¯”è¾ƒäº†å¤šä¸ªä»£è¡¨æ€§ RLVR æ–¹æ³•ï¼š

| æ–¹æ³• | ç‰¹ç‚¹ |
|------|------|
| **GRPO** | Token-level ratio + group advantage |
| **GSPO** | Sequence-level ratio + geometric mean |
| **GMPO** | Geometric mean aggregation over tokens |
| **CISPO** | Clipped importance sampling with asymmetric bounds |
| **DHPO-A** | DHPO with averaged mixing |
| **DHPO-E** | DHPO with entropy-guided mixing |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰**

#### ğŸ”¹ åœ¨ **Qwen3-30B-A3B-Base** ä¸Šçš„è¡¨ç°ï¼ˆå•ä½ï¼š%ï¼‰

| Method | AIME24 | AIME25 | AMC23 | Olympiad | MATH500 | Minerva | GSM8K | **AVG** |
|--------|--------|--------|-------|----------|---------|---------|-------|--------|
| GRPO   | 22.5   | 14.6   | 75.0  | 51.6     | 85.6    | 39.3    | 95.0  | 54.8   |
| GSPO   | 25.3   | 15.4   | 74.4  | 49.6     | 85.8    | 43.8    | 93.7  | 55.4   |
| GMPO   | 30.3   | 21.5   | 75.0  | 56.7     | 90.2    | 41.9    | 95.3  | 58.7   |
| **DHPO-A** | **32.4** | 24.1 | 75.6 | 54.2 | 89.2 | 43.8 | 95.5 | **59.2** |
| **DHPO-E** | **34.4** | **26.5** | **76.9** | **52.3** | **92.4** | **40.8** | **94.8** | **59.7** |

> âœ… **DHPO-E åœ¨ AIME24 ä¸Šä» 22.5% â†’ 34.4%ï¼Œç»å¯¹æå‡ 11.9%ï¼**

#### ğŸ”¹ åœ¨ **Qwen3-1.7B-Base** ä¸Šçš„è¡¨ç°

| Method | AIME24 | AIME25 | AMC23 | Olympiad | AVG |
|--------|--------|--------|-------|----------|-----|
| GRPO   | 9.0    | 7.0    | 41.2  | 33.3     | 39.7 |
| GSPO   | 9.2    | 7.2    | 41.9  | 33.9     | 39.3 |
| **DHPO-E** | **15.9** | **9.1** | **52.5** | **39.7** | **44.3** |

> âœ… **å¹³å‡è¶…è¶Š GRPO 4.6%ï¼Œå°æ¨¡å‹ä¸Šä»æ˜¾è‘—æœ‰æ•ˆ**

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

- DHPO åœ¨ **æ‰€æœ‰æ¨¡å‹è§„æ¨¡** å’Œ **å‡ ä¹æ‰€æœ‰åŸºå‡†ä»»åŠ¡** ä¸Šå‡ä¼˜äº GRPO å’Œ GSPOã€‚
- ç›¸è¾ƒäº GRPOï¼Œå¹³å‡æå‡çº¦ **4.9%**ï¼›
- ç›¸è¾ƒäº GSPOï¼Œå¹³å‡æå‡çº¦ **4.3%**ï¼›
- å°¤å…¶åœ¨ **å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ AIMEã€OlympiadBenchï¼‰** ä¸Šä¼˜åŠ¿æ˜æ˜¾ï¼Œè¯´æ˜å…¶æ›´æ“…é•¿é•¿é“¾æ¨ç†å»ºæ¨¡ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

#### ğŸ“Š **Branch-Specific Clipping vs Unified Clipping**

| å®éªŒåç§° | è®¾è®¡ |
|--------|------|
| **Unified Clip** | å¯¹æ··åˆåçš„æ¯”ç‡ç»Ÿä¸€å‰ªè£ |
| **Branch-Specific Clip** | åˆ†åˆ«å‰ªè£ token-level å’Œ sequence-level æ¯”ç‡åå†æ··åˆ |

##### ç»“æœï¼ˆè§ Figure 2 & 3ï¼‰ï¼š
- **Branch-Specific Clip æ˜¾è‘—å»¶ç¼“ç†µåç¼©**ï¼Œç»´æŒæ›´é«˜ policy entropyã€‚
- è®­ç»ƒæ›²çº¿æ›´åŠ å¹³æ»‘ï¼Œæ³¢åŠ¨æ›´å°ï¼ˆlower oscillationï¼‰ã€‚
- æœ€ç»ˆå‡†ç¡®ç‡ç›¸è¿‘ï¼Œä½†æ”¶æ•›è·¯å¾„æ›´ç¨³å®šã€‚

> ğŸ’¡ è¡¨æ˜ï¼š**åˆ†ç¦»å‰ªè£èƒ½æ›´å¥½æ§åˆ¶æ–¹å·®ï¼Œé¿å…æŸä¸€æç«¯æ¯”ç‡ç ´åæ•´ä½“æ›´æ–°æ–¹å‘ã€‚**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **çº¯ token-level æˆ– sequence-level ä¼˜åŒ–å‡æœ‰å±€é™**ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­éœ€è¦äºŒè€…ååŒã€‚
2. **DHPO æˆåŠŸå®ç°äº†ä¸¤ç§ç²’åº¦çš„åŠ¨æ€èåˆ**ï¼Œæ—¢èƒ½æ•æ‰å…³é”® token çš„å½±å“ï¼Œåˆèƒ½ä¿æŒå…¨å±€ä¸€è‡´æ€§ã€‚
3. **Entropy-guided mixing æ›´å…·é€‚åº”æ€§**ï¼šæ—©æœŸé¼“åŠ±æ¢ç´¢ï¼ŒåæœŸèšç„¦ç¨³å®šæ›´æ–°ï¼Œé¿å…è¿‡æ—©é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚
4. **Branch-specific clipping æ˜¯ç¨³å®šè®­ç»ƒçš„å…³é”®ç»„ä»¶**ï¼Œæœ‰æ•ˆç¼“è§£äº†ä¸åŒå°ºåº¦æ¯”ç‡ä¹‹é—´çš„å¹²æ‰°ã€‚
5. DHPO å±•ç°å‡ºè‰¯å¥½çš„ **scalability**ï¼Œåœ¨ 1.7B åˆ° 30B è§„æ¨¡çš„æ¨¡å‹ä¸Šå‡å–å¾—ä¸€è‡´å¢ç›Šã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

1. **è¯„ä¼°å±€é™äº Qwen3 å®¶æ—**ï¼šæœªåœ¨å…¶ä»–æ¶æ„ï¼ˆå¦‚ Llamaã€Phiï¼‰ã€åˆ†è¯å™¨æˆ–é¢„è®­ç»ƒç›®æ ‡ä¸åŒçš„æ¨¡å‹ä¸ŠéªŒè¯ï¼Œé€šç”¨æ€§æœ‰å¾…è¿›ä¸€æ­¥æ£€éªŒã€‚
2. **è®¡ç®—èµ„æºé™åˆ¶**ï¼šä»…ä¸éƒ¨åˆ†å…¸å‹ RLVR æ–¹æ³•å¯¹æ¯”ï¼Œæœªè¦†ç›–å…¨éƒ¨å˜ä½“ï¼ˆå¦‚ DAPOã€STEER ç­‰ï¼‰ã€‚
3. **æ··åˆæœºåˆ¶ä»ä¸ºå¯å‘å¼è®¾è®¡**ï¼šentropy-guided mixing è™½ç„¶åˆç†ï¼Œä½†ç¼ºä¹ç†è®ºæœ€ä¼˜æ€§ä¿è¯ã€‚
4. **è¶…å‚æ•æ„Ÿæ€§æœªçŸ¥**ï¼šä¾‹å¦‚å‰ªè£è¾¹ç•Œçš„é€‰æ‹©æ˜¯å¦éœ€éšæ¨¡å‹è§„æ¨¡è°ƒæ•´ï¼Œå°šæœªç³»ç»Ÿç ”ç©¶ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. æ¢ç´¢æ›´å¤æ‚çš„æ··åˆå‡½æ•°ï¼ˆå¦‚ç¥ç»ç½‘ç»œé¢„æµ‹æƒé‡ï¼‰ã€‚
2. æ‰©å±•è‡³éæ•°å­¦ç±»å¯éªŒè¯ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€é€»è¾‘è¯æ˜ï¼‰ã€‚
3. ç»“åˆå…¶ä»–ç¨³å®šæŠ€æœ¯ï¼ˆå¦‚ KL æ§åˆ¶ã€reward shapingï¼‰è¿›ä¸€æ­¥æå‡é²æ£’æ€§ã€‚
4. åœ¨æ›´å¤šæ¨¡å‹å®¶æ—å’Œ tokenizer ä¸Šæµ‹è¯•æ³›åŒ–èƒ½åŠ›ã€‚
5. ç†è®ºåˆ†ææ··åˆç­–ç•¥ä¸‹çš„æ”¶æ•›æ€§è´¨ä¸åå·®-æ–¹å·®æƒè¡¡ã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **DHPO æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆé‡è¦æ€§æ¯”ç‡æœºåˆ¶ï¼Œåœ¨ token-level ä¸ sequence-level ä¹‹é—´å®ç°åŠ¨æ€å¹³è¡¡ï¼Œå¹¶é€šè¿‡åˆ†æ”¯å‰ªè£ä¿éšœç¨³å®šæ€§ï¼Œåœ¨å¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šä¸»æµ RLVR æ–¹æ³•ã€‚**

</details>

---

### 12. [PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering](https://arxiv.org/abs/2601.05465)

**Authors**: Yu Liu, Wenxiao Zhang, Cong Cao, Wenxuan Lu, Fangfang Yuan, Diandian Guo, Kun Peng, Qiang Sun, Kaiyan Zhang, Yanbing Liu, Jin B. Hong, Bowen Zhou, Zhiyuan Ma  
**Category**: cs.AI  
**Published**: 2026-01-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.05465v1  

#### Abstract
Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šPRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹**å¼€æ”¾åŸŸå¤šè·³é—®ç­”ï¼ˆopen-domain multi-hop QAï¼‰**ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š
- **æ£€ç´¢å´©æºƒï¼ˆRetrieval Collapseï¼‰**ï¼šåœ¨å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šè¿›è¡Œè¿­ä»£æ£€ç´¢æ—¶ï¼Œè‹¥ç¼ºä¹æ¨ç†å¼•å¯¼çš„è§„åˆ’ï¼Œç³»ç»Ÿéš¾ä»¥å®šä½åŒ…å«â€œæ¡¥æ¥ç­”æ¡ˆâ€ï¼ˆbridge answersï¼‰çš„ä¸­é—´è¯æ®ï¼Œå¯¼è‡´åç»­æ¨ç†å¤±è´¥ã€‚
- **å­¦ä¹ ä¸ç¨³å®šæ€§ï¼ˆLearning Instabilityï¼‰**ï¼šç«¯åˆ°ç«¯çš„è½¨è¿¹è®­ç»ƒå­˜åœ¨ä¿¡ç”¨åˆ†é…å¼±ã€æ¨¡å—é—´é”™è¯¯å®šä½å›°éš¾ç­‰é—®é¢˜ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆç‰¹å®šæ•°æ®é›†å¯å‘å¼è§„åˆ™ï¼Œå½±å“æ³›åŒ–æ€§å’Œç¨³å®šæ€§ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ¶æ„
æå‡º **PRISMA**ï¼ˆPlan-Retrieve-Inspect-Solve-Memoize Architectureï¼‰ï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¼•å¯¼çš„**è§£è€¦å¼å¤šæ™ºèƒ½ä½“æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### ï¼ˆ1ï¼‰æ¶æ„è®¾è®¡ï¼šåä½œå¼æ¨ç†é—­ç¯
PRISMA æ¨¡ä»¿äººç±»ç ”ç©¶è€…çš„å·¥ä½œæµï¼Œæ„å»ºäº†ä¸€ä¸ªç”±äº”ä¸ªæ™ºèƒ½ä½“ç»„æˆçš„é—­ç¯ç³»ç»Ÿï¼š
- **Planner**ï¼šå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºä¾èµ–æ„ŸçŸ¥çš„å­é—®é¢˜é“¾ã€‚
- **Retriever**ï¼šæ‰§è¡Œä¸‰é˜¶æ®µçº§è”æ£€ç´¢ï¼ˆdense â†’ hybrid â†’ cross-encoderï¼‰ä»¥æé«˜å¬å›ç‡ã€‚
- **Inspector**ï¼ˆåŒé˜¶æ®µï¼‰ï¼š
  - *Context Inspector*ï¼šæ£€æŸ¥å­é—®é¢˜è´¨é‡å’Œæ£€ç´¢æ–‡æ¡£å……åˆ†æ€§ï¼Œè§¦å‘é‡å†™æˆ–æ‰©å±•æ£€ç´¢ï¼›
  - *Reasoning Inspector*ï¼šéªŒè¯æ±‚è§£å™¨è¾“å‡ºçš„ç­”æ¡ˆæ˜¯å¦åŸºäºè¯æ®ã€é€»è¾‘æ­£ç¡®ï¼Œå¹¶å¯è§¦å‘é‡è¯•ã€‚
- **Solver**ï¼šåŸºäºæ£€ç´¢åˆ°çš„è¯æ®ç”Ÿæˆæœ‰ä¾æ®çš„ç­”æ¡ˆã€‚
- **Memoizer**ï¼šç¼“å­˜å·²è§£å†³çš„å­é—®é¢˜è·¯å¾„ï¼Œæ”¯æŒè·¨é—®é¢˜å¤ç”¨ï¼Œæå‡æ•ˆç‡ä¸ä¿¡ç”¨å½’å› ã€‚

è¯¥è®¾è®¡å®ç°äº†**æ¨ç†é©±åŠ¨çš„åé¦ˆå¾ªç¯**ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­æ£€ç´¢ä¸æ¨ç†è„±èŠ‚çš„é—®é¢˜ã€‚

#### ï¼ˆ2ï¼‰è®­ç»ƒç­–ç•¥ï¼šä¸¤é˜¶æ®µ GRPO + OARPO
é‡‡ç”¨ **Two-Stage Group Relative Policy Optimization (GRPO)** æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼š
- **Stage Iï¼šä¸“å®¶æ ¡å‡†ï¼ˆExpert Calibrationï¼‰**
  - åˆ†åˆ«ä¼˜åŒ– Planner å’Œ Solverï¼Œä½¿å…¶æˆä¸ºå„è‡ªé¢†åŸŸçš„â€œä¸“å®¶â€ã€‚
  - ä½¿ç”¨ä»»åŠ¡ç‰¹å®šå¥–åŠ±å‡½æ•°ç‹¬ç«‹è®­ç»ƒï¼Œå¢å¼ºå­é—®é¢˜åˆ†è§£èƒ½åŠ›å’Œè¯æ®æ”¯æ’‘çš„æ¨ç†èƒ½åŠ›ã€‚
- **Stage IIï¼šæ®‹å·®å®¡è®¡å¯¹é½ï¼ˆObservation-Aware Residual Policy Optimization, OARPOï¼‰**
  - å›ºå®š Planner å’Œ Solverï¼Œè®­ç»ƒä¸€ä¸ª**è½¨è¿¹æ¡ä»¶åŒ–çš„ Inspector**ã€‚
  - åˆ©ç”¨é«˜å®¹é‡ Oracle Inspector æ•™å¸ˆæ¨¡å‹ç”Ÿæˆå®¡è®¡æ ‡ç­¾ï¼Œåœ¨æˆåŠŸè½¨è¿¹ä¸Šè®­ç»ƒ Inspector å­¦ä¹ æ£€æµ‹å¹¶ä¿®å¤æ®‹ä½™é”™è¯¯ã€‚
  - å‡å°‘ trace-levelï¼ˆæ•°æ®ï¼‰å’Œ system-levelï¼ˆç­–ç•¥ï¼‰æ®‹å·®ï¼Œå®ç°ç²¾å‡†æ¢å¤ã€‚

è¿™ä¸€ç­–ç•¥æœ‰æ•ˆç¼“è§£äº†ç«¯åˆ°ç«¯ RL ä¸­çš„ä¿¡ç”¨åˆ†é…éš¾é¢˜ï¼Œæå‡äº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ä¸é²æ£’æ€§ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | PRISMA | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ IR-CoTã€TIRESRAG-R1ï¼‰ |
|------|--------|-------------------------------|
| æ¶æ„ | å¤šæ™ºèƒ½ä½“ååŒ + åé¦ˆé—­ç¯ | å•æ™ºèƒ½ä½“æˆ–æ¾è€¦åˆæµç¨‹ |
| æ£€ç´¢æ§åˆ¶ | æ¨ç†é©±åŠ¨çš„åŠ¨æ€æ‰©å±• | é™æ€/å›ºå®šæ­¥æ•°è¿­ä»£ |
| é”™è¯¯å¤„ç† | ä¸»åŠ¨è¯Šæ–­ + æœ‰æ¡ä»¶æ¢å¤ | è¢«åŠ¨æ‰§è¡Œï¼Œæ— æ˜¾å¼çº é”™æœºåˆ¶ |
| è®­ç»ƒæ–¹å¼ | è§£è€¦å¼ä¸¤é˜¶æ®µä¼˜åŒ– | ç«¯åˆ°ç«¯è”åˆè®­ç»ƒï¼Œæ˜“è¿‡æ‹Ÿåˆ |
| æ³›åŒ–èƒ½åŠ› | å¼ºï¼ˆé€šè¿‡æ®‹å·®å­¦ä¹ æå‡è¿ç§»æ€§ï¼‰ | å¼±ï¼ˆä¾èµ–æ•°æ®é›†å¯å‘å¼ï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
å…±è¯„ä¼° **10 ä¸ªåŸºå‡†æ•°æ®é›†**ï¼Œæ¶µç›– in-distribution ä¸ out-of-distribution åœºæ™¯ï¼š

| ç±»å‹ | æ•°æ®é›† | ç‰¹ç‚¹ |
|------|-------|------|
| In-Distribution | **MuSiQue**, **HotpotQA**, **2WikiMHQA** | å¤šè·³æ¨ç†æ ‡å‡†æµ‹è¯•é›†ï¼Œç”¨äºä¸»æ€§èƒ½æ¯”è¾ƒ |
| Out-of-Distribution | **NaturalQ**, **Bamboogle**, **ChemistryV**, **Food**, **Game**, **Geography**, **Music** | æµ‹è¯•è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œéƒ¨åˆ†å…·å¯¹æŠ—æ€§æˆ–ä¸“ä¸šæ€§ |

æ‰€æœ‰å®éªŒå‡ä½¿ç”¨å®Œæ•´çš„ **DPR Wikipedia è¯­æ–™åº“ï¼ˆ21M passagesï¼‰** è¿›è¡Œæ£€ç´¢ã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹åŸºç¡€**ï¼šåŸºäº `Qwen3-4B-Instruct` è¿›è¡Œå…¨å‚æ•°å¾®è°ƒã€‚
- **æ£€ç´¢ç³»ç»Ÿ**ï¼šä¸‰çº§çº§è”æ£€ç´¢ï¼ˆBGE-M3 + hybrid reranking + BGE-reranker-v2-m3ï¼‰ã€‚
- **ç¡¬ä»¶é…ç½®**ï¼š4Ã—NVIDIA H100 GPUsï¼Œæ¯ç»„å®éªŒè¿è¡Œ 5 æ¬¡å–å¹³å‡å€¼ã€‚

#### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **EM (Exact Match)** | å®Œå…¨åŒ¹é…å‡†ç¡®ç‡ |
| **F1** | ç­”æ¡ˆè¯çº§åˆ«çš„ F1 åˆ†æ•° |
| **Retrieval Recall** | æ£€ç´¢åˆ°é»„é‡‘æ”¯æŒæ–‡æ¡£çš„æ¯”ä¾‹ |
| **Inspection Precision / Recall** | å®¡è®¡æ¨¡å—çš„ç²¾ç¡®ç‡ä¸å¬å›ç‡ |
| **Latency** | ç«¯åˆ°ç«¯å“åº”æ—¶é—´ï¼Œè¡¡é‡éƒ¨ç½²æ•ˆç‡ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
åˆ†ä¸ºä¸¤ç±»ï¼š
- **æ— éœ€è®­ç»ƒçš„æ–¹æ³•**ï¼š
  - Closed-bookï¼ˆæ— æ£€ç´¢ï¼‰
  - Single-step RAG
  - Multi-step RAGï¼ˆå¦‚ IR-CoTï¼‰
- **è®­ç»ƒè¿‡çš„ RAG ç³»ç»Ÿ**ï¼š
  - Retrobust (SFT)
  - RAG-DDR (RL)
  - QAgent (RL)
  - TIRESRAG-R1 (RL)

ç‰¹åˆ«åœ°ï¼Œè¿˜å¯¹æ¯”äº†å¤šä¸ªé—­æº API æ¨¡å‹ï¼ˆGemini-2.5-Flash, GPT-5-Medium, DeepSeek-V3.2ï¼‰ä½œä¸ºå¼ºåŸºçº¿ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 1ï¼‰
PRISMA åœ¨æ‰€æœ‰ 10 ä¸ªåŸºå‡†ä¸Šå‡è¾¾åˆ° **state-of-the-art æ€§èƒ½**ï¼š

| æ•°æ®é›† | PRISMA (EM/F1) | æœ€ä½³åŸºçº¿ (EM/F1) | æå‡å¹…åº¦ |
|--------|----------------|------------------|----------|
| **MuSiQue** | **30.6 / 39.5** | TIRESRAG-R1: 19.4 / 30.0 | **+11.2 / +9.5** |
| **HotpotQA** | **50.2 / 55.8** | RAG-DDR: 41.0 / 54.2 | +9.2 / +1.6 |
| **2WikiMHQA** | **57.0 / 60.1** | TIRESRAG-R1+: 52.8 / 59.6 | +4.2 / +0.5 |
| **NaturalQ (OOD)** | **38.6 / 53.7** | IRCoT(Gemini): 29.8 / 47.4 | **+8.8 / +6.3** |

> âœ… PRISMA ä¸ä»…è¶…è¶Šæ‰€æœ‰å¼€æºè®­ç»ƒç³»ç»Ÿï¼Œè¿˜åœ¨å¤šä¸ª hardest benchmarks ä¸Šä¼˜äºæœ€å¼ºçš„ API æ¨¡å‹ï¼ˆå¦‚ GPT-5-Mediumï¼‰ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- åœ¨ **out-of-distribution** åœºæ™¯ä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œä¾‹å¦‚åœ¨ **ChemistryV** ä¸Šè¾¾åˆ° 75.3/78.5ï¼Œæ˜¾è‘—é«˜äºå…¶ä»–æ–¹æ³•ã€‚
- å³ä½¿ä½¿ç”¨è¾ƒå°çš„ `Qwen3-4B` æ¨¡å‹ï¼Œä¹Ÿä¼˜äºæ›´å¤§è§„æ¨¡çš„ API æ¨¡å‹ï¼ˆå¦‚ GPT-5-Mediumï¼‰ï¼Œè¯´æ˜å…¶æ¶æ„ä¼˜åŠ¿è€Œéå•çº¯ä¾èµ–æ¨¡å‹å¤§å°ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 2ï¼‰

| é…ç½® | MuSiQue EM | Î”EM |
|------|------------|-----|
| **å®Œæ•´ PRISMA** | **30.6** | â€” |
| w/o Planner | 19.4 | -11.2 |
| w/o Inspector | 11.2 | **-19.4** |
| w/o Context Inspector | 27.2 | -3.4 |
| w/o Reasoning Inspector | 24.8 | -5.8 |
| æ‰€æœ‰æ™ºèƒ½ä½“æ›¿æ¢ä¸º base model | 15.0 | -15.6 |

#### å‘ç°ï¼š
- **Inspector æ˜¯æœ€å…³é”®ç»„ä»¶**ï¼Œç§»é™¤åæ€§èƒ½ä¸‹é™è¿‘ 63%ï¼Œè¡¨æ˜ä¸»åŠ¨å®¡è®¡æœºåˆ¶å¯¹å¯é æ€§è‡³å…³é‡è¦ã€‚
- **Planner æ˜¾è‘—æå‡æ£€ç´¢æœ‰æ•ˆæ€§**ï¼Œç¼ºå¤±æ—¶ EM ä¸‹é™ 36.6%ï¼Œä½“ç°ä¾èµ–æ„ŸçŸ¥åˆ†è§£çš„é‡è¦æ€§ã€‚
- æ‰€æœ‰æ¨¡å—çš„ä¸“é¡¹è®­ç»ƒå‡æœ‰è´¡çŒ®ï¼Œå°¤å…¶æ˜¯ **Solver è®­ç»ƒå¸¦æ¥æœ€å¤§å•æ¨¡å—å¢ç›Š**ï¼ˆ-8.2 EMï¼‰ï¼Œåæ˜ å¤šæ–‡æ¡£åˆæˆæ˜¯ç“¶é¢ˆã€‚

æ­¤å¤–ï¼Œ**æŒ‰è·³æ•°åˆ†å±‚åˆ†æï¼ˆTable 9ï¼‰** æ˜¾ç¤ºï¼š
- åœ¨ **4-hop é—®é¢˜** ä¸Šï¼ŒPRISMA è¾¾åˆ° 4.5% EMï¼Œè€Œ IRCoT ä»…ä¸º 1.1%ï¼Œæå‡è¾¾ **4.1 å€**ã€‚
- è¡¨æ˜ PRISMA æ›´æ“…é•¿å¤„ç†é«˜å¤æ‚åº¦æ¨ç†é“¾ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **è§£è€¦å¼å¤šæ™ºèƒ½ä½“ + åé¦ˆé—­ç¯ > ç«¯åˆ°ç«¯é»‘ç®±è®­ç»ƒ**
   - PRISMA é€šè¿‡å°†è§„åˆ’ã€æ£€ç´¢ã€æ±‚è§£ã€å®¡è®¡åˆ†ç¦»ï¼Œå¹¶å¼•å…¥ Inspector çš„åŒé‡åé¦ˆæœºåˆ¶ï¼Œå®ç°äº†æ›´å¯æ§ã€å¯è§£é‡Šçš„æ¨ç†æµç¨‹ã€‚
2. **ä¸¤é˜¶æ®µ GRPO + OARPO æœ‰æ•ˆè§£å†³ä¿¡ç”¨åˆ†é…é—®é¢˜**
   - Stage I å…ˆå»ºç«‹ä¸“å®¶èƒ½åŠ›ï¼ŒStage II å†è®­ç»ƒ Inspector å¯¹æ®‹å·®è¿›è¡Œç²¾ç»†åŒ–ä¿®å¤ï¼Œé¿å…äº† RL ä¸­å¸¸è§çš„å¥–åŠ±ç¨€ç–ä¸è¯¯å¯¼é—®é¢˜ã€‚
3. **ä¸»åŠ¨å®¡è®¡æœºåˆ¶å¤§å¹…æå‡é²æ£’æ€§**
   - Inspector åœ¨ 56.8% çš„æˆåŠŸæ¡ˆä¾‹ä¸­å‘æŒ¥äº†ä½œç”¨ï¼ˆTable 12ï¼‰ï¼Œå…¶ä¸­ Context Inspector è§¦å‘æ£€ç´¢æ‰©å±•å ä¸»å¯¼ï¼ˆ48.2%ï¼‰ï¼ŒReasoning Inspector æ”¹è¿›ç­”æ¡ˆå  25.1%ã€‚
4. **å°æ¨¡å‹ä¹Ÿèƒ½èƒœè¿‡å¤§æ¨¡å‹ API**
   - å°½ç®¡ä½¿ç”¨ `Qwen3-4B`ï¼ŒPRISMA åœ¨å¤šä¸ª hard benchmarks ä¸Šè¶…è¿‡ GPT-5-Medium å’Œ Geminiï¼Œè¯æ˜æ¶æ„ä¼˜åŒ–çš„ä»·å€¼ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **éƒ¨ç½²èµ„æºéœ€æ±‚é«˜**
   - éœ€è¦å¤§é‡ GPU å†…å­˜å’Œè®¡ç®—èµ„æºï¼Œé™åˆ¶äº†è½»é‡åŒ–éƒ¨ç½²ã€‚
2. **è®­ç»ƒæ•æ„Ÿä¸”å¤æ‚**
   - å„æ¨¡å—å¥–åŠ±å‡½æ•°éœ€ç²¾ç»†è°ƒå‚ï¼Œä»»æ„ç»„ä»¶å¤±è°ƒä¼šå½±å“æ•´ä½“æ€§èƒ½ã€‚
3. **ä»å—é™äºè¯­æ–™è¦†ç›–ä¸æ­§ä¹‰é—®é¢˜**
   - å¦‚ Table 5 æ‰€ç¤ºï¼Œå½“ Planner äº§ç”Ÿè¯­ä¹‰æ¨¡ç³Šå­é—®é¢˜ï¼ˆå¦‚â€œä¸»è§’â€æŒ‡ä»£ä¸æ¸…ï¼‰ï¼Œå³ä½¿æ–‡æ¡£ç›¸å…³ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´ä¸å¯é€†çš„é”™è¯¯ä¼ æ’­ã€‚
4. **æ— æ³•å®Œå…¨æ›¿ä»£å¤§æ¨¡å‹å…ˆéªŒçŸ¥è¯†**
   - åœ¨æŸäº›å¸¸è¯†å¯†é›†å‹ä»»åŠ¡ä¸­ï¼Œä»éš¾å…¨é¢è¶…è¶Šæœ€æ–° API æ¨¡å‹ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼€å‘æ›´ç´§å‡‘çš„åŸºç¡€æ¨¡å‹ä»¥é™ä½éƒ¨ç½²æˆæœ¬ã€‚
- å¼•å…¥æ›´å¼ºçš„ Planner æ¶ˆæ­§æœºåˆ¶ï¼ˆå¦‚ç»“åˆçŸ¥è¯†å›¾è°±çº¦æŸï¼‰ã€‚
- æ¢ç´¢è‡ªåŠ¨åŒ–å¥–åŠ±å‡½æ•°è®¾è®¡ä¸è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚
- å°† PRISMA æ¶æ„æ¨å¹¿è‡³å…¶ä»–çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚äº‹å®æ ¸æŸ¥ã€ç§‘å­¦é—®ç­”ï¼‰ã€‚

---

> ğŸ”— **ä»£ç åœ°å€**ï¼šhttps://github.com/Ameame1/PRISMA  
> ğŸ“š **ä¸€å¥è¯æ€»ç»“**ï¼šPRISMA é€šè¿‡**è§£è€¦å¤šæ™ºèƒ½ä½“æ¶æ„ + ä¸¤é˜¶æ®µ RL ä¼˜åŒ– + åŒé‡å®¡è®¡æœºåˆ¶**ï¼Œå®ç°äº†å¼€æ”¾åŸŸå¤šè·³é—®ç­”çš„ state-of-the-art æ€§èƒ½ï¼Œåœ¨å¤æ‚æ¨ç†ä¸è·¨åŸŸæ³›åŒ–æ–¹é¢å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ã€‚

</details>

---

### 13. [Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.05705)

**Authors**: Ali Farjami, Luca Redondi, Marco Valentino  
**Category**: cs.AI  
**Published**: 2026-01-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.05705v1  

#### Abstract
Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-sym...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç°æœ‰çš„ç¥ç»ç¬¦å·ï¼ˆneuro-symbolicï¼‰è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰ç³»ç»Ÿé€šå¸¸ä¾èµ–äºå›ºå®šçš„é€»è¾‘å½¢å¼ï¼ˆå¦‚ä¸€é˜¶é€»è¾‘ FOLï¼‰ï¼Œå°†é€»è¾‘è§†ä¸ºé™æ€èƒŒæ™¯å±‚ã€‚è¿™ç§è®¾è®¡é™åˆ¶äº†ç³»ç»Ÿçš„**é²æ£’æ€§ã€é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§**ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠè§„èŒƒæ€§æ¨ç†ï¼ˆnormative reasoningï¼‰çš„é¢†åŸŸï¼ˆå¦‚ä¼¦ç†ã€æ³•å¾‹ï¼‰æ—¶ï¼Œä¼ ç»Ÿé€»è¾‘éš¾ä»¥æœ‰æ•ˆè¡¨è¾¾ä¹‰åŠ¡ï¼ˆobligationï¼‰ã€è®¸å¯ï¼ˆpermissionï¼‰ã€ä¾‹å¤–ï¼ˆexceptionsï¼‰ç­‰æ¦‚å¿µã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§**é€»è¾‘å‚æ•°åŒ–ï¼ˆlogic-parametricï¼‰ç¥ç»ç¬¦å· NLI æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- å°†â€œé€»è¾‘â€æœ¬èº«ä½œä¸ºå¯æ§åˆ¶çš„å‚æ•°ï¼Œè€Œéå›ºå®šç»„ä»¶ã€‚
- åˆ©ç”¨ **LogiKEy æ–¹æ³•è®º**ï¼Œå°†å¤šç§ç»å…¸ä¸éç»å…¸é€»è¾‘ï¼ˆå¦‚æ¨¡æ€é€»è¾‘ KDã€æ¡ä»¶é€»è¾‘ DDLEã€DDL_CJï¼‰è¯­ä¹‰åµŒå…¥åˆ°é«˜é˜¶é€»è¾‘ï¼ˆHOLï¼‰ä¸­ï¼Œå®ç°ç»Ÿä¸€çš„å½¢å¼åŒ–ä¸éªŒè¯å¹³å°ã€‚
- åŒºåˆ†ä¸¤ç§æ¨ç†ç­–ç•¥ï¼š
  - **Logic-external**ï¼šé€šè¿‡åœ¨ FOL ä¸­æ·»åŠ å…¬ç†æ¥ç¼–ç è§„èŒƒè§„åˆ™ï¼ˆå¦‚â€œä¹‰åŠ¡è•´å«è®¸å¯â€éœ€æ˜¾å¼å£°æ˜ï¼‰ã€‚
  - **Logic-internal**ï¼šåˆ©ç”¨é€»è¾‘è‡ªèº«çš„ç»“æ„ï¼ˆå¦‚ KD å†…ç½®å…¬ç† `Op â†’ Pp`ï¼‰è‡ªåŠ¨æ”¯æŒè§„èŒƒæ¨ç†ã€‚

è¯¥æ¡†æ¶æ”¯æŒ LLM ä¸å®šç†è¯æ˜å™¨ï¼ˆTPï¼‰ä¹‹é—´çš„è¿­ä»£äº¤äº’ï¼ŒåŒ…æ‹¬è‡ªåŠ¨å½¢å¼åŒ–ï¼ˆautoformalizationï¼‰ã€è¯­æ³•ä¸€è‡´æ€§æ£€æŸ¥ã€å®šç†è¯æ˜å’Œè§£é‡Šç²¾ç‚¼ï¼ˆexplanation refinementï¼‰å››ä¸ªé˜¶æ®µã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´é«˜çš„çµæ´»æ€§ä¸æ¨¡å—æ€§**ï¼šå¯åŠ¨æ€åˆ‡æ¢ä¸åŒé€»è¾‘ä»¥é€‚é…ä»»åŠ¡éœ€æ±‚ã€‚
- **æ›´å¼ºçš„è¡¨ç°åŠ›ä¸æ•ˆç‡**ï¼šé€»è¾‘å†…éƒ¨ç»“æ„æ›´å¥‘åˆè§„èŒƒæ¨ç†æ¨¡å¼ï¼Œå‡å°‘å¯¹è„†å¼±äººå·¥å…¬ç†çš„ä¾èµ–ã€‚
- **æ›´å¥½çš„éªŒè¯æˆåŠŸç‡ä¸æ”¶æ•›é€Ÿåº¦**ï¼šå°¤å…¶åœ¨ä¼¦ç†ç±»ä»»åŠ¡ä¸­ï¼Œé€»è¾‘å†…å»ºæ–¹æ³•æ˜¾è‘—ä¼˜äºå¤–å»ºæ–¹æ³•ã€‚
- **æ­ç¤ºé€»è¾‘é€‰æ‹©å¯¹æ¨ç†è¡Œä¸ºçš„æ ¹æœ¬å½±å“**ï¼šä¸ºæ„å»ºâ€œé€»è¾‘è‡ªé€‚åº”â€ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
ä½œè€…æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼š
- **BENR (Bioethical Explanations and Normative Reasoning)**ï¼šåŒ…å« 103 ä¸ªæ¡ˆä¾‹ï¼Œèšç„¦ç”Ÿç‰©ä¼¦ç†ä¸­çš„è§„èŒƒæ€§æ¨ç†ç»“æ„ã€‚
  - è¦†ç›–å¤šç§æ¨ç†æ¨¡å¼ï¼šç»å…¸é€»è¾‘ã€å¸¸è¯†æ¨ç†ã€ç¼ºçœæ¨ç†ã€æ¨¡æ€æ¨ç†ã€(bio)ethics æ¨ç†ã€‚
  - å¼ºè°ƒè§£é‡Šçš„å†…éƒ¨ç»“æ„ï¼Œå¦‚ä»ä¸€èˆ¬åŸåˆ™å®ä¾‹åŒ–åˆæ­¥ç†ç”±ï¼ˆprima-facie reasonsï¼‰ã€å†²çªè§£å†³ã€undercut ç­‰ã€‚

### å®éªŒè®¾ç½®
- **æ¨¡å‹**ï¼šGPT-4o å’Œ DeepSeek-V1ï¼Œåœ¨ç›¸åŒæç¤ºä¸‹è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚
- **é€»è¾‘å½¢å¼**ï¼šè¯„ä¼°å››ç§é€»è¾‘ç³»ç»Ÿï¼š
  - **FOL**ï¼ˆä¸€é˜¶é€»è¾‘ï¼‰
  - **KD**ï¼ˆåŸºæœ¬é“ä¹‰é€»è¾‘ï¼Œå« `Op â†’ Pp` å…¬ç†ï¼‰
  - **DDLE**ï¼ˆåŸºäºåå¥½çš„æ¡ä»¶é“ä¹‰é€»è¾‘ï¼‰
  - **DDL_CJ**ï¼ˆCarmo & Jones åŒå…ƒé“ä¹‰é€»è¾‘ï¼Œæ”¯æŒäº‹å®åˆ†ç¦»ï¼‰
- **æµç¨‹**ï¼šé‡‡ç”¨å››é˜¶æ®µ pipelineï¼š
  1. **Autoformalization**ï¼šLLM å°†è‡ªç„¶è¯­è¨€ä¸‰å…ƒç»„ `{premise, explanation, hypothesis}` æ˜ å°„ä¸ºé€»è¾‘å…¬å¼ã€‚
  2. **Syntax & Consistency Check**ï¼šæ£€æŸ¥å½¢å¼åŒ–è¾“å‡ºçš„è¯­æ³•æ­£ç¡®æ€§ä¸å‰æä¸€è‡´æ€§ã€‚
  3. **Theorem Proving**ï¼šä½¿ç”¨ Isabelle/HOL è¿›è¡Œè‡ªåŠ¨å®šç†è¯æ˜ã€‚
  4. **Explanation Refinement**ï¼šè‹¥è¯æ˜å¤±è´¥ï¼Œæå–å¤±è´¥æ­¥éª¤å¹¶åé¦ˆç»™ LLM ç”Ÿæˆæ”¹è¿›è§£é‡Šï¼Œæœ€å¤šè¿­ä»£ 3 æ¬¡ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **æœ‰æ•ˆè§£é‡Šç”ŸæˆæˆåŠŸç‡ï¼ˆSuccess Rateï¼‰**ï¼šæœ€ç»ˆèƒ½è¢«å½¢å¼éªŒè¯çš„è§£é‡Šæ¯”ä¾‹ã€‚
- **å¹³å‡ç²¾ç‚¼è¿­ä»£æ¬¡æ•°ï¼ˆRefinement Iterationsï¼‰**ï¼šè¶Šå°‘è¶Šå¥½ï¼Œåæ˜ æ”¶æ•›æ•ˆç‡ã€‚
- **æ±‚è§£æ—¶é—´ï¼ˆSolving Timeï¼‰**ï¼šæˆåŠŸè¿è¡Œçš„å¹³å‡è€—æ—¶ã€‚
- **å¥æ³•é”™è¯¯ç‡ï¼ˆSyntactic Error Rateï¼‰**ï¼šå½¢å¼åŒ–è¿‡ç¨‹ä¸­çš„è¯­æ³•é”™è¯¯é¢‘ç‡ã€‚
- **é¢†åŸŸç‰¹å¼‚æ€§è¡¨ç°**ï¼šæŒ‰æ¨ç†ç±»å‹ï¼ˆå¦‚ commonsense vs. ethicsï¼‰ç»†åˆ†æ€§èƒ½ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- ä¸»è¦å¯¹æ¯”ä¸åŒé€»è¾‘ä¸‹çš„åŒä¸€æ¡†æ¶è¡¨ç°ï¼Œä½“ç°â€œé€»è¾‘å³å‚æ•°â€çš„æ•ˆæœã€‚
- ç‰¹åˆ«å¯¹æ¯” **logic-externalï¼ˆFOL + å…¬ç†ï¼‰** vs. **logic-internalï¼ˆKD/DDLE/DDL_CJï¼‰** ç­–ç•¥ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 3ï¼‰

| Logic    | DeepSeek (%) | GPT-4o (%) |
|----------|--------------|------------|
| FOL      | 42.72        | 39.81      |
| **KD**   | **76.70**    | **77.67**  |
| DDLE     | 60.19        | 51.46      |
| DDL_CJ   | 61.17        | 74.76      |

- **GPT-4o + KD è¾¾åˆ°æœ€é«˜æˆåŠŸç‡ 77.67%**ï¼Œè¿œè¶… FOL è¡¨ç°ã€‚

### æ”¶æ•›æ•ˆç‡ï¼ˆè§ Table 4ï¼‰

| Logic    | DeepSeek | GPT-4o |
|----------|----------|--------|
| FOL      | 0.91     | 0.83   |
| **KD**   | **0.43** | **0.60** |
| DDLE     | 0.60     | 0.64   |
| DDL_CJ   | 0.65     | 0.71   |

- **KD æ‰€éœ€ç²¾ç‚¼è¿­ä»£æœ€å°‘**ï¼Œè¯´æ˜å…¶é€»è¾‘ç»“æ„æ›´åˆ©äºå¿«é€Ÿæ”¶æ•›ã€‚

### è®¡ç®—æ•ˆç‡ï¼ˆè§ Table 5ï¼‰

| Logic    | DeepSeek (s) | GPT-4o (s) |
|----------|---------------|-------------|
| FOL      | 108.99        | 103.74      |
| KD       | 65.25         | 58.83       |
| DDLE     | 69.45         | 65.34       |
| **DDL_CJ** | **60.18**     | **49.80**   |

- **DDL_CJ å’Œ KD æ˜¾è‘—é™ä½è¿è¡Œæ—¶é—´**ï¼Œå°½ç®¡è¡¨è¾¾èƒ½åŠ›æ›´å¼ºã€‚

### é²æ£’æ€§ï¼ˆè§ Table 6ï¼‰

| Logic    | DeepSeek (%) | GPT-4o (%) |
|----------|---------------|-------------|
| FOL      | 3.90          | 2.90        |
| KD       | 7.80          | **1.90**    |
| DDLE     | 16.50         | 12.60       |
| DDL_CJ   | 19.40         | 5.80        |

- **FOL å¥æ³•æœ€ç¨³å®š**ï¼Œä½†éªŒè¯é€šè¿‡ç‡ä½ï¼›**GPT-4o + KD é”™è¯¯ç‡æœ€ä½ï¼ˆ<2%ï¼‰**ï¼Œå…¼é¡¾é²æ£’ä¸é«˜æ•ˆã€‚

### é¢†åŸŸç‰¹å¼‚æ€§è¡¨ç°ï¼ˆè§ Figure 4 / Table 7ï¼‰
- **Commonsense Reasoning**ï¼šFOL è¡¨ç°æœ€ä½³ï¼ˆå¦‚ 90% æˆåŠŸç‡ï¼‰ï¼Œå› å…¶å¯¹å…·ä½“äº‹ä»¶å»ºæ¨¡èƒ½åŠ›å¼ºã€‚
- **Ethical / Deontic Domains**ï¼šKDã€DDL_CJ æ˜¾è‘—ä¼˜äº FOLï¼ˆå¦‚åœ¨ (Bio)ethics å­é›†ä¸­ï¼ŒKD è¾¾ 76.6%ï¼ŒFOL ä»… ~13%ï¼‰ã€‚
- **Modalities & Default Reasoning**ï¼šéç»å…¸é€»è¾‘ï¼ˆå°¤å…¶æ˜¯ KDï¼‰åœ¨å¤„ç†ä¹‰åŠ¡ã€å¯èƒ½æ€§ã€ä¾‹å¤–ç­‰æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **é€»è¾‘é€‰æ‹©å…·æœ‰å†³å®šæ€§å½±å“**ï¼šä¸åŒçš„é€»è¾‘å½¢å¼æ·±åˆ»å½±å“ LLM é©±åŠ¨çš„æ¨ç†è´¨é‡ã€æ•ˆç‡ä¸å¯éªŒè¯æ€§ã€‚
2. âœ… **Logic-internal æ–¹æ³•ä¼˜äº logic-external**ï¼š
   - å†…å»ºè§„èŒƒç»“æ„ï¼ˆå¦‚ KD çš„ `Op â†’ Pp`ï¼‰ä½¿æ¨ç†æ›´è‡ªç„¶ã€ç²¾ç‚¼æ›´å¯é ã€‚
   - å¤–éƒ¨æ·»åŠ å…¬ç†çš„æ–¹å¼è„†å¼±ä¸”æ˜“å‡ºé”™ï¼Œéš¾ä»¥æ•æ‰å¤æ‚è§„èŒƒå…³ç³»ã€‚
3. âœ… **é€»è¾‘å‚æ•°åŒ–æå‡æ•´ä½“æ€§èƒ½**ï¼š
   - åœ¨è§„èŒƒæ¨ç†ä»»åŠ¡ä¸­ï¼Œ**KD å®ç°æœ€é«˜æˆåŠŸç‡ä¸æœ€å¿«æ”¶æ•›**ã€‚
   - **DDL_CJ åœ¨è¡¨è¾¾å¤æ‚åå¥½ä¸å†²çªè§£å†³æ–¹é¢æ½œåŠ›å·¨å¤§**ã€‚
4. âœ… **é¢†åŸŸä¾èµ–æ€§æ˜æ˜¾**ï¼š
   - **FOL æ›´é€‚åˆå¸¸è¯†ä¸æè¿°æ€§æ¨ç†**ï¼›
   - **æ¨¡æ€ä¸é“ä¹‰é€»è¾‘æ›´é€‚åˆä¼¦ç†ã€æ³•å¾‹ç­‰è§„èŒƒæ€§é¢†åŸŸ**ã€‚
5. âœ… **LogiKEy + HOL æ¶æ„å¯è¡Œä¸”å¼ºå¤§**ï¼šæ”¯æŒå¤šé€»è¾‘ç»Ÿä¸€åµŒå…¥ä¸éªŒè¯ï¼Œä¸ºç¥ç»ç¬¦å·ç³»ç»Ÿæä¾›åšå®åŸºç¡€ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æ¨¡å‹èŒƒå›´æœ‰é™**ï¼šä»…æµ‹è¯• GPT-4o å’Œ DeepSeek-V1ï¼Œå°æ¨¡å‹æˆ–å…¶å®ƒæ¶æ„å¯èƒ½è¡¨ç°ä¸åŒã€‚
- **é€»è¾‘è¦†ç›–ä¸å…¨**ï¼šæœªæ¶µç›–é‡åŒ–æ¨¡æ€é€»è¾‘ã€éå•è°ƒé€»è¾‘ã€æ¦‚ç‡æ¨ç†ç­‰æ›´å¤æ‚å½¢å¼ã€‚
- **ç²¾ç‚¼è·¯å¾„å•ä¸€**ï¼šå½“å‰ refinement ç­–ç•¥ç”±ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼Œæœªæ¢ç´¢æ‰€æœ‰å¯èƒ½ä¿®å¤è·¯å¾„ï¼Œä¹Ÿæœªè¯„ä¼°äººç±»åˆç†æ€§ã€‚
- **è¾“å…¥ç®€åŒ–**ï¼šä½¿ç”¨ç»“æ„åŒ–è‡ªç„¶è¯­è¨€ï¼ŒçœŸå®ä¸–ç•Œæ–‡æœ¬å¯èƒ½å¸¦æ¥æ›´å¤§æŒ‘æˆ˜ï¼ˆæ­§ä¹‰ã€æ–‡åŒ–å·®å¼‚ç­‰ï¼‰ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ„å»º **logic-adaptive reasoning architectures**ï¼šæ ¹æ®è¾“å…¥è‡ªåŠ¨é€‰æ‹©æˆ–ç»„åˆæœ€ä¼˜é€»è¾‘ã€‚
- æ‰©å±•è‡³æ›´å¤šé€»è¾‘ç³»ç»Ÿï¼ˆå¦‚é»˜è®¤é€»è¾‘ã€è®ºè¯æ¡†æ¶ã€æ¦‚ç‡é€»è¾‘ï¼‰ã€‚
- å¼•å…¥ **multi-logic ensemble æˆ– switching mechanism**ï¼Œå®ç°åŠ¨æ€é€»è¾‘è°ƒåº¦ã€‚
- æ¢ç´¢ **human-in-the-loop refinement**ï¼Œæå‡è§£é‡Šçš„å¯ç†è§£æ€§ä¸å¯ä¿¡åº¦ã€‚
- å°†è¯¥èŒƒå¼åº”ç”¨äºå®é™…åœºæ™¯ï¼ˆå¦‚æ³•å¾‹åˆè§„ã€åŒ»ç–—å†³ç­–æ”¯æŒï¼‰ã€‚

---

> **æ€»ç»“**ï¼šæœ¬è®ºæ–‡å¼€åˆ›æ€§åœ°å°†â€œé€»è¾‘â€ä½œä¸ºå¯æ§å‚æ•°å¼•å…¥ç¥ç»ç¬¦å· NLI æ¡†æ¶ï¼Œè¯æ˜äº† **logic-internal è®¾è®¡åœ¨è§„èŒƒæ¨ç†ä¸­çš„ä¼˜è¶Šæ€§**ï¼Œå¹¶é€šè¿‡ BENR æ•°æ®é›†å’Œ LogiKEy å¹³å°æä¾›äº†ç³»ç»Ÿå®è¯ã€‚ç ”ç©¶ä¸ä»…æå‡äº†å¯éªŒè¯æ¨ç†çš„æ€§èƒ½ï¼Œæ›´ä¸ºä¸‹ä¸€ä»£**é€»è¾‘è‡ªé€‚åº”ã€æ¨¡å—åŒ–ã€å¯è§£é‡Šçš„ AI æ¨ç†ç³»ç»Ÿ**æŒ‡æ˜äº†æ–¹å‘ã€‚

</details>

---

### 14. [HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search](https://arxiv.org/abs/2601.05903)

**Authors**: Zihang Tian, Rui Li, Jingsen Zhang, Xiaohe Bo, Wei Huo, Xu Chen  
**Category**: cs.CL  
**Published**: 2026-01-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.05903v1  

#### Abstract
Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAP...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

ç°æœ‰çš„ **LLM routing** æ–¹æ³•é€šå¸¸åªå…³æ³¨åœ¨ä¸åŒ LLM æ¶æ„ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼ˆå¦‚ LLaMAã€Qwenã€Mistralï¼‰ï¼Œè€Œå¿½ç•¥äº† **æ¨¡å‹å‚æ•°é…ç½®** å¯¹ä»»åŠ¡æ€§èƒ½çš„å…³é”®å½±å“ã€‚è¿™ç§ä»…åŸºäºæ¶æ„çš„ç¦»æ•£è·¯ç”±æ–¹å¼é™åˆ¶äº†æ¨¡å‹å¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”èƒ½åŠ›ã€‚

ä¾‹å¦‚ï¼ŒæŸä¸ªæ¨¡å‹åœ¨é»˜è®¤å‚æ•°ä¸‹è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨ç»è¿‡é€‚å½“è°ƒå‚åå¯èƒ½æˆä¸ºæœ€ä¼˜é€‰æ‹©ã€‚ä¼ ç»Ÿæ–¹æ³•æ— æ³•æ•æ‰è¿™ç§æ½œåŠ›ï¼Œå¯¼è‡´æ¬¡ä¼˜å†³ç­–ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **HAPS**ï¼ˆHierarchical LLM Routing with Joint Architecture and Parameter Searchï¼‰ï¼Œä¸€ç§**è”åˆæœç´¢ LLM æ¶æ„ä¸å‚æ•°**çš„åˆ†å±‚è·¯ç”±æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†è·¯ç”±ç©ºé—´ä»çº¯ç¦»æ•£æ‰©å±•ä¸ºâ€œç¦»æ•£ + è¿ç»­â€çš„è”åˆå†³ç­–ç©ºé—´ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

- **åˆ†å±‚è·¯ç”±æ¶æ„**ï¼š
  - **é«˜å±‚è·¯ç”±å™¨**ï¼ˆHigh-level Routerï¼‰ï¼šåŸºäº LLaMA çš„åˆ†ç±»å™¨ï¼Œè´Ÿè´£é€‰æ‹©åˆé€‚çš„ LLM æ¶æ„ã€‚
  - **ä½å±‚è·¯ç”±å™¨**ï¼ˆLow-level Routerï¼‰ï¼šå‚æ•°ç”Ÿæˆç½‘ç»œï¼ˆParameter Generation Networkï¼‰ï¼Œä¸ºé€‰å®šçš„æ¶æ„ç”Ÿæˆé€‚é…çš„ LoRA å‚æ•°ã€‚

- **å‚æ•°å…±äº«æœºåˆ¶**ï¼š
  - ä½å±‚è·¯ç”±å™¨çš„éƒ¨åˆ†å‚æ•°ç›´æ¥ç»§æ‰¿è‡ªé«˜å±‚è·¯ç”±å™¨ï¼Œå®ç°è·¨å±‚çº§çš„çŸ¥è¯†å…±äº«ï¼Œæå‡æ³›åŒ–èƒ½åŠ›å’ŒååŒä¼˜åŒ–æ•ˆæœã€‚

- **è”åˆä¼˜åŒ–ç›®æ ‡**ï¼š
  - é‡‡ç”¨ **Reward-Augmented Maximum Likelihood Estimation**ï¼ˆRAMLï¼‰è¿›è¡Œç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œç»Ÿä¸€ä¼˜åŒ–æ¶æ„é€‰æ‹©ä¸å‚æ•°ç”Ÿæˆã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| æ–¹é¢ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ RouteLLMã€GraphRouterï¼‰ | HAPS |
|------|----------------------------------------|------|
| å†³ç­–ç©ºé—´ | ä»…ç¦»æ•£ï¼ˆæ¶æ„é€‰æ‹©ï¼‰ | ç¦»æ•£ + è¿ç»­ï¼ˆæ¶æ„ + å‚æ•°ï¼‰ |
| å‚æ•°çµæ´»æ€§ | å›ºå®šå‚æ•° | è¾“å…¥æ¡ä»¶åŒ–çš„åŠ¨æ€å‚æ•°ç”Ÿæˆ |
| çŸ¥è¯†å…±äº« | æ¶æ„ä¸å‚æ•°ç‹¬ç«‹ä¼˜åŒ– | é«˜ä½å±‚å‚æ•°å…±äº«ï¼ŒååŒå¢å¼º |
| æ€§èƒ½æ½œåŠ› | å—é™äºé¢„è®¾æ¨¡å‹é…ç½® | å¯æŒ–æ˜æ¨¡å‹åœ¨éé»˜è®¤é…ç½®ä¸‹çš„æ½œåŠ› |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- **HotpotQA**ï¼šå¤šè·³é—®ç­”æ•°æ®é›†ï¼Œæµ‹è¯•æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚
- **MMLU**ï¼šæ¶µç›–57ä¸ªå­¦ç§‘çš„å¤§è§„æ¨¡å¤šä»»åŠ¡ç†è§£åŸºå‡†ï¼Œè¯„ä¼°å¹¿æ³›çŸ¥è¯†è¦†ç›–èƒ½åŠ›ã€‚

ä¸¤ä¸ªæ•°æ®é›†å‡è¿›è¡Œäº†é‡‡æ ·å¤„ç†ï¼ˆè®­ç»ƒé›†3kï¼ŒéªŒè¯é›†1kï¼Œæµ‹è¯•é›†100ï¼‰ï¼Œä»¥æ§åˆ¶å˜é‡å¹¶èšç„¦è·¯ç”±ç­–ç•¥æœ¬èº«çš„æ•ˆæœã€‚

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

- **ç³»ç»Ÿæ¶æ„**ï¼šåŒæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆTeacher-Studentï¼‰ï¼Œæ¨¡æ‹Ÿåä½œå¼æ¨ç†æµç¨‹ã€‚
  - **Student**ï¼šè´Ÿè´£ç”Ÿæˆç­”æ¡ˆæˆ–æ‰§è¡Œæ£€ç´¢ã€‚
  - **Teacher**ï¼šæä¾›åé¦ˆï¼Œå†³å®šæ˜¯å¦ç»§ç»­æˆ–æäº¤ç­”æ¡ˆã€‚
- **å€™é€‰æ¨¡å‹ç»„åˆ**ï¼šå…±5ç»„ï¼ŒåŒ…æ‹¬ï¼š
  - å¼€æºå¯¹ï¼šL-Qï¼ˆLlama-3.1 vs Qwen2.5ï¼‰ã€M-Qã€L-M
  - æ··åˆæºå¯¹ï¼šL-Gï¼ˆLlama vs GPT-4.1 Nanoï¼‰ã€Q-Dï¼ˆQwen vs DeepSeek V3ï¼‰
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - HotpotQAï¼šToken-level F1 Score
  - MMLUï¼šAccuracy

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

- **Random**ï¼šéšæœºé€‰æ‹©æ¨¡å‹
- **RouteLLM**ï¼šåŸºäºåå¥½æ•°æ®çš„äºŒå…ƒè·¯ç”±
- **GraphRouter**ï¼šåŸºäºå¼‚æ„å›¾çš„ä»»åŠ¡-æŸ¥è¯¢-LLMå…³ç³»å»ºæ¨¡
- **IRT-Router**ï¼šåŸºäºé¡¹ç›®ååº”ç†è®ºå»ºæ¨¡æŸ¥è¯¢éš¾åº¦ä¸æ¨¡å‹èƒ½åŠ›

æ‰€æœ‰åŸºçº¿å‡é€‚é…è‡³ç›¸åŒçš„åŒä»£ç†æ¡†æ¶ä¸‹è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 2ï¼‰**

| Method | HotpotQA (F1) L-Q | M-Q | L-M | MMLU (Acc) L-Q | M-Q | L-M |
|--------|-------------------|-----|-----|---------------|-----|-----|
| Random | 38.94 | 11.44 | 23.26 | 71 | 65 | 67 |
| RouteLLM | 40.67 | 36.10 | 38.95 | 79 | 74 | 69 |
| GraphRouter | 41.31 | 23.07 | 26.75 | 71 | 67 | 67 |
| IRT-Router | 37.34 | 34.09 | 38.92 | 74 | 77 | 73 |
| **HAPS** | **43.16** | **39.70** | **40.58** | **79** | **78** | **74** |

> âœ… **HAPS åœ¨6é¡¹è®¾ç½®ä¸­å–å¾—5é¡¹æœ€ä½³æ€§èƒ½**ï¼Œå°¤å…¶åœ¨ HotpotQA ä¸Šæ˜¾è‘—é¢†å…ˆã€‚

#### æ€§èƒ½å¢ç›Šåˆ†æï¼š
- åœ¨ HotpotQA ä¸Šï¼Œç›¸æ¯”æœ€å¼ºåŸºçº¿ï¼ˆRouteLLMï¼‰ï¼š
  - L-Qï¼š+1.85 F1
  - M-Qï¼š+3.60 F1
  - L-Mï¼š+1.63 F1
- åœ¨ MMLU ä¸Šï¼ŒHAPS åœ¨ M-Q å’Œ L-M ä¸Šåˆ†åˆ«é«˜å‡º 1.0%ï¼Œå¹¶åœ¨ L-Q ä¸ŠæŒå¹³æœ€å¼ºåŸºçº¿ã€‚

### **æ¶ˆèå®éªŒç»“æœï¼ˆTable 3 & Figure 3ï¼‰**

| å˜ä½“ | HotpotQA (F1) L-Q | M-Q | L-M | MMLU (Acc) L-Q | M-Q | L-M |
|------|-------------------|-----|-----|---------------|-----|-----|
| w/o High-Level (Random) | 40.13 | 26.37 | 25.44 | 72 | 68 | 61 |
| w/o High-Level (Fixed) æœ€ä½³é™æ€ | 42.45 | 35.51 | 37.84 | 78 | 78 | 71 |
| w/o Low-Level Router | 33.93 | 33.67 | 36.32 | 70 | 70 | 68 |
| **HAPS (Full)** | **43.16** | **39.70** | **40.58** | **79** | **78** | **74** |

#### å…³é”®å‘ç°ï¼š
- ç§»é™¤é«˜å±‚è·¯ç”±å™¨ â†’ æ€§èƒ½ä¸‹é™æœ€å¤šè¾¾ **15%**
- å›ºå®šåˆ†é…ç­–ç•¥ â†’ åœ¨ä¸åŒ¹é…åœºæ™¯ä¸‹æ€§èƒ½æš´è·Œè¶… **30%**
- ç§»é™¤ä½å±‚å‚æ•°ç”Ÿæˆ â†’ æ‰€æœ‰ä»»åŠ¡æ€§èƒ½å¤§å¹…é€€åŒ–ï¼Œè¯´æ˜**è¾“å…¥æ¡ä»¶åŒ–å‚æ•°ç”Ÿæˆè‡³å…³é‡è¦**

### **å‚æ•°å…±äº«çš„é‡è¦æ€§ï¼ˆFigure 3ï¼‰**

- **HAPS (w/o Parameter Sharing)**ï¼ˆè§£è€¦è®­ç»ƒï¼‰ï¼š
  - HotpotQA F1 ä¸‹é™é«˜è¾¾ **4.33%**
  - MMLU Accuracy ä¸‹é™ **6.00%**
- ç»“è®ºï¼šå‚æ•°å…±äº«å¸¦æ¥çš„**è¡¨ç¤ºå¯¹é½**ï¼ˆRepresentation Alignmentï¼‰å’Œ**ååŒé€‚åº”**ï¼ˆCo-Adaptationï¼‰æ˜¯æ€§èƒ½æå‡çš„å…³é”®ã€‚

### **LoRA æ³¨å…¥æ·±åº¦çš„å½±å“ï¼ˆFigure 4ï¼‰**

- å°† LoRA åº”ç”¨äºæœ€å $k$ å±‚çš„ `o_proj` æ¨¡å—ï¼š
  - $l_1$ï¼šæµ…å±‚æ³¨å…¥
  - $l_2$ï¼šä¸¤å±‚æ³¨å…¥ï¼ˆæ¨èï¼‰
  - $l_3$ï¼šä¸‰å±‚æ³¨å…¥

#### å‘ç°ï¼š
- L-Q å’Œ L-Mï¼š$l_2$ è¡¨ç°æœ€å¥½ï¼Œ$l_3$ å‡ºç°æ€§èƒ½ä¸‹é™ï¼ˆè¿‡æ‹Ÿåˆé£é™©ï¼‰
- M-Qï¼šéšæ·±åº¦å¢åŠ æŒç»­æå‡ï¼Œè¡¨æ˜å¼‚æ„æ€§å¼ºçš„æ¨¡å‹å—ç›Šäºæ›´æ·±å¯¹é½
- æ¨èä½¿ç”¨ $l_2$ ä½œä¸ºé²æ£’é»˜è®¤é…ç½®

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **è”åˆæ¶æ„ä¸å‚æ•°æœç´¢æ˜¾è‘—ä¼˜äºä»…æ¶æ„è·¯ç”±**  
   HAPS é€šè¿‡æ‰©å±•å†³ç­–ç©ºé—´ï¼Œåœ¨å¤šä¸ªä»»åŠ¡å’Œæ¨¡å‹ç»„åˆä¸Š consistently è¶…è¶Š SOTA è·¯ç”±æ–¹æ³•ã€‚

2. **è¾“å…¥æ¡ä»¶åŒ–å‚æ•°ç”Ÿæˆæ˜¯å…³é”®**  
   åŠ¨æ€ç”Ÿæˆ LoRA å‚æ•°ä½¿æ¨¡å‹èƒ½æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´è¡Œä¸ºï¼Œé‡Šæ”¾â€œéé»˜è®¤é…ç½®â€ä¸‹çš„æ½œåŠ›ã€‚

3. **é«˜ä½å±‚å‚æ•°å…±äº«å¸¦æ¥ååŒå¢ç›Š**  
   å…±äº« backbone å®ç°ç‰¹å¾å¯¹é½ï¼Œå¹¶é€šè¿‡æ¢¯åº¦ä¼ æ’­å®ç° co-adaptationï¼Œé¿å…å­¤ç«‹ä¼˜åŒ–å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

4. **HAPS æ”¯æŒæ··åˆå¼€æº/é—­æºéƒ¨ç½²**  
   é€šè¿‡å¼•å…¥æŒ‡ç¤ºå‘é‡å±è”½é—­æºæ¨¡å‹çš„æ¢¯åº¦æ›´æ–°ï¼Œå¯åœ¨å®é™…ç³»ç»Ÿä¸­çµæ´»åº”ç”¨ã€‚

5. **é«˜æ•ˆæ¨ç†è®¾è®¡ä¿éšœå®ç”¨æ€§**  
   - å‘é‡åŒ– LoRA æ³¨å…¥ï¼ˆBatched BMMï¼‰
   - è¯·æ±‚æ¡¶åŒ–ï¼ˆRequest Bucketingï¼‰
   - é¿å…é€æ ·æœ¬åŠ è½½æƒé‡ï¼Œä¿æŒé«˜åå

### **æ–¹æ³•çš„å±€é™æ€§**

1. **ä¾èµ–é«˜è´¨é‡å¥–åŠ±ä¿¡å·**ï¼šè®­ç»ƒä¾èµ–äºå‡†ç¡®çš„ä»»åŠ¡æ€§èƒ½åé¦ˆï¼Œå™ªå£°æˆ–ç¨€ç–å¥–åŠ±ä¼šå½±å“ç¨³å®šæ€§ã€‚
2. **å‚æ•°æ•ˆç‡ vs. æ€§èƒ½ä¸Šé™**ï¼šä½¿ç”¨ LoRA ç­‰ PEFT æ–¹æ³•è™½å®ç”¨ï¼Œä½†å¯èƒ½ä½äºå…¨å‚æ•°å¾®è°ƒçš„æé™æ€§èƒ½ã€‚
3. **ä»»åŠ¡åˆ†å¸ƒæ•æ„Ÿæ€§**ï¼šåœ¨ä»»åŠ¡åˆ†å¸ƒã€å€™é€‰æ± æˆ–é¢„ç®—çº¦æŸå˜åŒ–æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰å¾…è¿›ä¸€æ­¥éªŒè¯ã€‚
4. **å¯å¤ç°æ€§æŒ‘æˆ˜**ï¼šè‹¥ä¾èµ–éç¡®å®šæ€§æˆ–é—­æº APIï¼ˆå¦‚ GPT-4ï¼‰ï¼Œç²¾ç¡®å¤ç°å®éªŒå­˜åœ¨å›°éš¾ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**

- æ‰©å±•åˆ°æ›´å¤šç±»å‹çš„ PEFT æ–¹æ³•ï¼ˆå¦‚ Adapterã€Prompt Tuningï¼‰
- æ¢ç´¢æ›´å¤æ‚çš„å¤šçº§è·¯ç”±ç»“æ„ï¼ˆå¦‚ä»»åŠ¡åˆ†è§£ + å­ä»»åŠ¡è·¯ç”±ï¼‰
- å¼•å…¥åœ¨çº¿å­¦ä¹ æœºåˆ¶ï¼Œæ”¯æŒåŠ¨æ€å€™é€‰æ¨¡å‹æ± 
- ç»“åˆæˆæœ¬æ„ŸçŸ¥è·¯ç”±ï¼Œæ„å»ºæ›´ç²¾ç»†çš„ performance-cost trade-off æ§åˆ¶ç­–ç•¥

---

> ğŸ”— **ä»£ç å·²å¼€æº**ï¼šhttps://github.com/zihangtian/HAPS

</details>

---

### 15. [Performance-Portable Optimization and Analysis of Multiple Right-Hand Sides in a Lattice QCD Solver](https://arxiv.org/abs/2601.05816)

**Authors**: Shiting Long, Gustavo Ramirez-Hidalgo, Stepan Nassyr, Jose Jimenez-Merchan, Andreas Frommer, Dirk Pleiter  
**Category**: cs.DC  
**Published**: 2026-01-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.05816v1  

#### Abstract
Managing the high computational cost of iterative solvers for sparse linear systems is a known challenge in scientific computing. Moreover, scientific applications often face memory bandwidth constraints, making it critical to optimize data locality and enhance the efficiency of data transport. We e...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šPerformance-Portable Optimization and Analysis of Multiple Right-Hand Sides in a Lattice QCD Solver

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡é’ˆå¯¹**Lattice QCDï¼ˆLQCDï¼‰æ±‚è§£å™¨ä¸­è¿­ä»£æ±‚è§£ç¨€ç–çº¿æ€§ç³»ç»Ÿçš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜**ï¼Œç‰¹åˆ«æ˜¯ç”±äºç°ä»£HPCæ¶æ„å†…å­˜å¸¦å®½å—é™è€Œå¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚ä¼ ç»Ÿå®ç°å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- ä¾èµ–è€æ—§çš„128ä½SSEæŒ‡ä»¤ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç°ä»£CPUçš„512ä½SIMDèƒ½åŠ›ï¼›
- ç¼ºä¹å¯¹å¤šå³ç«¯é¡¹ï¼ˆmultiple right-hand sides, rhsï¼‰çš„æœ‰æ•ˆæ”¯æŒï¼›
- åœ¨ä¸åŒæ¶æ„ï¼ˆx86 vs Armï¼‰ä¸Šç¼ºä¹æ€§èƒ½å¯ç§»æ¤æ€§ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹
1. **å¼•å…¥rhs blockingæœºåˆ¶åˆ°DD-aAMGæ±‚è§£å™¨ä¸­**ï¼š
   - åœ¨**Wilson-Diracç®—å­è¯„ä¼°æ ¸å‡½æ•°**å’Œ**GMRESæ±‚è§£å™¨**ä¸­åŒæ—¶å®ç°äº†å¤šrhså¤„ç†ï¼›
   - æ”¯æŒæœ‰æ— odd-even preconditioningçš„æƒ…å†µï¼Œæå‡ç®—æ³•çµæ´»æ€§ã€‚

2. **è®¾è®¡é¢å‘SIMDä¼˜åŒ–çš„æ•°æ®å¸ƒå±€æ¥å£**ï¼š
   - å¼•å…¥ä¸¤ç§æ•°æ®å¸ƒå±€ï¼ˆLayout 1: column-majorï¼›Layout 2: row-majorï¼‰ï¼Œå¹¶é€šè¿‡çµæ´»æ¥å£åˆ‡æ¢ï¼›
   - æå‡º**Layout 2ä¸“ä¸ºloop vectorizationä¼˜åŒ–**ï¼Œæ˜¾è‘—æé«˜AVX/SVEç­‰SIMDæŒ‡ä»¤åˆ©ç”¨ç‡ã€‚

3. **é¦–æ¬¡åœ¨LQCDä¸­æ¢ç´¢Arm SMEï¼ˆScalable Matrix Extensionï¼‰çš„åº”ç”¨æ½œåŠ›**ï¼š
   - é’ˆå¯¹çŸ©é˜µä¹˜æ³•æ“ä½œå®ç°åŸºäºSMEçš„fmopaæŒ‡ä»¤åŠ é€Ÿï¼›
   - è®¾è®¡äº†ä¸¤ç§å˜ä½“ï¼ˆneg-A å’Œ neg-Mï¼‰å¹¶è¿›è¡Œæ¨¡æ‹Ÿè¯„ä¼°ã€‚

4. **ç³»ç»Ÿæ€§çš„æ€§èƒ½åˆ†ææ¡†æ¶**ï¼š
   - ç»“åˆrooflineæ¨¡å‹ã€PAPIæ€§èƒ½è®¡æ•°å™¨ã€STREAMå¸¦å®½æµ‹è¯•ç­‰å·¥å…·ï¼Œæ·±å…¥å‰–ææ€§èƒ½é™åˆ¶å› ç´ ï¼›
   - æ­ç¤ºç¼–è¯‘å™¨è¡Œä¸ºã€å†…å­˜è¯»å†™æ¯”ã€cacheæ•ˆåº”ç­‰å¯¹å®é™…æ€§èƒ½çš„å½±å“ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **æ€§èƒ½å¯ç§»æ¤æ€§** | æ”¯æŒx86ï¼ˆJUWELSï¼‰ã€Arm A64FXï¼ˆOokamiï¼‰ã€Kunpeng 920ï¼ˆHAICGUï¼‰ä¸‰å¤§å¹³å°ï¼Œå®ç°è·¨æ¶æ„é«˜æ•ˆè¿è¡Œ |
| **SIMDåˆ©ç”¨æ•ˆç‡** | Layout 2ç›¸æ¯”åŸå§‹Layout 1ï¼Œåœ¨JUWELSä¸Šå®ç°æ›´é«˜512-bit AVXæŒ‡ä»¤ååé‡ |
| **ç®—æœ¯å¼ºåº¦æå‡** | å¤šrhså‡å°‘é‡å¤è®¿å­˜ï¼Œç†è®ºç®—æœ¯å¼ºåº¦éšblocking size `b`çº¿æ€§å¢é•¿ï¼ˆè§å…¬å¼AI(b)ï¼‰ |
| **å‰ç»æ€§ç¡¬ä»¶é€‚é…** | æ¢ç´¢SMEæŒ‡ä»¤é›†ç”¨äºLQCDæ ¸å¿ƒè®¡ç®—ï¼Œå±•ç¤ºå…¶æ½œåœ¨åŠ é€Ÿèƒ½åŠ› |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **Latticeé…ç½®**ï¼š
  - å¤šèŠ‚ç‚¹å®éªŒï¼š`128Ã—64Â³`ï¼ˆå…¸å‹LQCDè§„æ¨¡ï¼‰
  - å•èŠ‚ç‚¹å®éªŒï¼š`64Ã—16Â³`ï¼ˆé€‚åˆå•æœºæµ‹è¯•ï¼‰

### å®éªŒå¹³å°
| å¹³å° | æ¶æ„ | CPU | å†…å­˜å¸¦å®½ï¼ˆSTREAM Triadï¼‰ |
|------|------|-----|--------------------------|
| **JUWELS** | x86 | 2Ã—Intel Xeon Platinum 8168 | 155 GB/s |
| **Ookami** | Arm | A64FX | 619 GB/s |
| **HAICGU** | Arm | 2Ã—Kunpeng 920 | 218 GB/s |

> æ‰€æœ‰å¹³å°å‡ä½¿ç”¨GCCç¼–è¯‘å™¨ï¼ˆv13~v14ï¼‰ï¼Œå¯ç”¨`-O3`åŠå¯¹åº”SIMDæ ‡å¿—ï¼ˆå¦‚`-mavx512f`, `-march=armv8.2-a+sve`ï¼‰ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **æ€§èƒ½æŒ‡æ ‡**ï¼š
  - æ¯èŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ / ååç‡
  - **Architectural Efficiency**ï¼ˆå®æµ‹æ€§èƒ½ / Rooflineç†è®ºä¸Šé™ï¼‰
- **åº•å±‚åˆ†ææŒ‡æ ‡**ï¼š
  - PAPIæ€§èƒ½è®¡æ•°å™¨ï¼ˆFP_ARITH_512B_PACKED_DOUBLE, L2ç¼“å­˜refill/writebackç­‰ï¼‰
  - æœ‰æ•ˆå†…å­˜å¸¦å®½ï¼ˆé€šè¿‡L2 cacheæµé‡ä¼°ç®—ï¼‰
  - æ±‡ç¼–ä»£ç åˆ†æï¼ˆloop unrolling, SIMD usageï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **åŸå§‹ç‰ˆæœ¬**ï¼šæœªä¼˜åŒ–çš„DD-aAMGï¼ˆç­‰ä»·äº`b=1`, Layout 1ï¼‰
- **å¯¹æ¯”æ–¹æ¡ˆ**ï¼š
  - ä¸åŒblocking size (`b=1,2,4,8,16`)
  - ä¸¤ç§æ•°æ®å¸ƒå±€ï¼ˆLayout 1 vs Layout 2ï¼‰
  - æ˜¯å¦å¯ç”¨odd-even preconditioning
  - SMEå®ç° vs SVE auto-vectorized vs BLIS ZGEMM

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰Wilson-Diracç®—å­æ ¸å‡½æ•°æ€§èƒ½ï¼ˆmulti-nodeï¼‰
| å¹³å° | æœ€ä½³åŠ é€Ÿæ¯”ï¼ˆvs åŸå§‹ï¼‰ | æœ€ä¼˜é…ç½® | Architectural Efficiency |
|------|------------------------|---------|----------------------------|
| **JUWELS** | **~10%** | Layout 2, b=16 | è¾¾72% â†’ 52%ï¼ˆéšbå¢å¤§ä¸‹é™ï¼‰ |
| **Ookami** | **~15%** | Layout 1, b=16 | æœ€é«˜ä»…27%ï¼ˆè¿œä½äºç†è®ºå¸¦å®½ï¼‰ |
| **HAICGU** | **~24%** | Layout 1, b=16 | é«˜è¾¾69%ï¼Œè¡¨ç°æœ€ç¨³å®š |

> æ³¨ï¼šå°½ç®¡Ookamiæ‹¥æœ‰æœ€é«˜å†…å­˜å¸¦å®½ï¼ˆ619 GB/sï¼‰ï¼Œä½†å®é™…æ•ˆç‡æœ€ä½ï¼Œè¡¨æ˜éš¾ä»¥é¥±å’Œå¸¦å®½ã€‚

#### ï¼ˆ2ï¼‰Batched GMRESæ±‚è§£å™¨æ€§èƒ½
- åœ¨JUWELSä¸Šï¼Œbatched GMRESæœ€å¤§æé€Ÿçº¦ **7%**ï¼ˆä½äºkernelå±‚é¢çš„10%ï¼‰ï¼ŒåŸå› ï¼š
  - å­˜å‚¨é¢å¤–å‘é‡ï¼ˆV, Hï¼‰å¯¼è‡´ç®—æœ¯å¼ºåº¦é™ä½ï¼›
  - odd-even preconditioningå¸¦æ¥éè¿ç»­è®¿å­˜æ¨¡å¼ï¼Œæ€§èƒ½ä»…ä¸ºæ— é¢„æ¡ä»¶çš„70%ï¼›
  - ä½†è¿­ä»£æ¬¡æ•°å‡åŠï¼Œæ€»ä½“ä»æ›´é«˜æ•ˆã€‚

#### ï¼ˆ3ï¼‰SMEæ¨¡æ‹Ÿæ€§èƒ½è¯„ä¼°ï¼ˆåŸºäºQEMU emulatorï¼‰
| æ–¹æ³• | æŒ‡ä»¤æˆæœ¬åŠ æƒåç›¸å¯¹SVEåŠ é€Ÿæ¯” |
|------|------------------------------|
| **neg-A (æœ€ä¼˜)** | **2.75Ã—** |
| **neg-M** | **1.30Ã—** |
| **SVE (auto-vectorized)** | 1.0Ã— |
| **BLIS ZGEMM** | ~0.28Ã—ï¼ˆå³SMEå¿«9.97Ã—ï¼‰ |

> è¡¨æ˜SMEåœ¨ç†æƒ³æ¡ä»¶ä¸‹å…·å¤‡æ˜¾è‘—è®¡ç®—ä¼˜åŠ¿ï¼Œå°¤å…¶é€‚ç”¨äºé«˜ç®—æœ¯å¼ºåº¦åœºæ™¯ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **Layoutå½±å“åˆ†æ**ï¼š
  - JUWELSï¼šLayout 2åœ¨`b>2`æ—¶ä¼˜äºLayout 1ï¼Œä¸”512-bit AVXæŒ‡ä»¤æ•°é‡æ›´å¤šï¼›
  - Ookamiï¼šLayout 2è™½è§¦å‘SVEæŒ‡ä»¤ï¼Œä½†æ€§èƒ½åè€Œä¸å¦‚ä»…ç”¨NEONçš„Layout 1ï¼Œæ¨æµ‹SVEæŒ‡ä»¤å¼€é”€å¤§ï¼›
  - HAICGUï¼šä»…æ”¯æŒ128-bit SIMDï¼ŒLayout 1ç•¥ä¼˜ã€‚

- **Compilerè¡Œä¸ºå½±å“**ï¼š
  - GCCå¯¹ä¸åŒ`b`ç”Ÿæˆå·®å¼‚æå¤§çš„æ±‡ç¼–ä»£ç ï¼ˆloop unrollingç¨‹åº¦ã€å¯„å­˜å™¨åˆ†é…ï¼‰ï¼›
  - å¯¼è‡´å®é™…ç®—æœ¯å¼ºåº¦åç¦»ç†è®ºå€¼ï¼Œä½¿rooflineé¢„æµ‹è¿‡äºä¹è§‚ã€‚

- **å†…å­˜å¸¦å®½å˜åŒ–åˆ†æï¼ˆOokamiï¼‰**ï¼š
  - éšç€`b`å¢åŠ ï¼Œ**read-to-write ratioä¸‹é™**ï¼Œwriteå‹åŠ›ä¸Šå‡ï¼›
  - æµ‹å¾—æœ‰æ•ˆå¸¦å®½ä»`b=1`æ—¶çš„127 GB/sé™è‡³`b=16`æ—¶çš„74 GB/sï¼ˆâ†“40%ï¼‰ï¼›
  - è§£é‡Šä¸ºä½•æ€§èƒ½æœªéšAIçº¿æ€§å¢é•¿ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **å¤šrhsèƒ½æå‡æ€§èƒ½ï¼Œä½†æ”¶ç›Šå—åˆ¶äºç¡¬ä»¶çº¦æŸ**ï¼š
   - ç†è®ºä¸Šå¯é€šè¿‡æé«˜ç®—æœ¯å¼ºåº¦æ”¹å–„å†…å­˜ç»‘å®šé—®é¢˜ï¼›
   - å®é™…ä¸­å› **å†™å¯†é›†åŠ å‰§ã€cacheå±€éƒ¨æ€§ä¸‹é™ã€ç½‘ç»œé€šä¿¡å¼€é”€**ç­‰å› ç´ é™åˆ¶å¢ç›Šã€‚

2. âœ… **æ•°æ®å¸ƒå±€å¯¹SIMDæ•ˆç‡è‡³å…³é‡è¦**ï¼š
   - Row-majorï¼ˆLayout 2ï¼‰æ›´é€‚åˆSIMDå¤„ç†ï¼›
   - ä½†åœ¨æŸäº›æ¶æ„ï¼ˆå¦‚Ookamiï¼‰ä¸Šå¯èƒ½å› ç¼–è¯‘å™¨æˆ–å¾®æ¶æ„ç‰¹æ€§åè¢«æ‹–ç´¯ã€‚

3. âš ï¸ **æ€§èƒ½å¯ç§»æ¤æ€§é¢ä¸´æŒ‘æˆ˜**ï¼š
   - è™½ç„¶ä»£ç å¯åœ¨x86/Armä¸Šè¿è¡Œï¼Œä½†**æœ€ä¼˜é…ç½®å› å¹³å°è€Œå¼‚**ï¼›
   - Ookamiè™½å¸¦å®½æé«˜ï¼Œå´æœ€éš¾è¾¾åˆ°å¸¦å®½é¥±å’Œï¼Œæš´éœ²è½¯ä»¶-ç¡¬ä»¶ååŒè°ƒä¼˜éš¾é¢˜ã€‚

4. ğŸ”® **SMEå±•ç°å·¨å¤§æ½œåŠ›ï¼Œä½†éœ€é«˜å¸¦å®½æ”¯æ’‘**ï¼š
   - æ¨¡æ‹Ÿæ˜¾ç¤ºSMEå¯æ¯”SVEå¿«2.75å€ï¼Œæ¯”BLISå¿«è¿‘10å€ï¼›
   - å®é™…å‘æŒ¥éœ€è¦è¶³å¤Ÿé«˜çš„å†…å­˜å¸¦å®½åŒ¹é…å…¶è®¡ç®—ååã€‚

5. ğŸ§© **ç¼–è¯‘å™¨è¡Œä¸ºä¸å¯å¿½è§†**ï¼š
   - Auto-vectorizationç»“æœé«˜åº¦ä¾èµ–blocking sizeå’Œå¾ªç¯ç»“æ„ï¼›
   - ä¸åŒ`b`ä¸‹ç”Ÿæˆçš„æŒ‡ä»¤åºåˆ—å·®å¼‚å¤§ï¼Œå½±å“æ€§èƒ½å¯é¢„æµ‹æ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æœªé‡‡ç”¨block Krylov subspace solver**ï¼šä»…å®ç°batchedç‹¬ç«‹æ±‚è§£ï¼Œæœªèƒ½äº«å—ç®—æ³•çº§æ”¶æ•›åŠ é€Ÿï¼›
- **ç¼ºä¹çœŸå®SMEç¡¬ä»¶éªŒè¯**ï¼šå½“å‰ä»…åŸºäºQEMUæ¨¡æ‹Ÿï¼Œå°šæœªåœ¨ç‰©ç†èŠ¯ç‰‡ä¸Šæµ‹è¯•ï¼›
- **OpenMPå¹¶è¡Œç²’åº¦æœ‰é™**ï¼šéƒ¨åˆ†æ¨¡å—ä»ä¸ºå•çº¿ç¨‹ï¼Œåˆ¶çº¦Ookamiç­‰ä¼—æ ¸å¹³å°æ‰©å±•æ€§ï¼›
- **æœªä¼˜åŒ–ä¸­é—´æ•°æ®é‡ç”¨**ï¼šlarge `b`å¯¼è‡´å¤§é‡ä¸´æ—¶å˜é‡ï¼ŒåŠ å‰§å†™å‹åŠ›ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **ä¼˜åŒ–æ•°æ®å±€éƒ¨æ€§**ï¼šé‡æ„å¾ªç¯ç»“æ„ä»¥å¤ç”¨ä¸­é—´ç»“æœï¼Œç¼“è§£å†™å¯†é›†é—®é¢˜ï¼›
2. **ä½¿ç”¨intrinsicsè¿›ä¸€æ­¥ä¼˜åŒ–å…³é”®è·¯å¾„**ï¼šå¯¹æ¯”æ‰‹åŠ¨å‘é‡åŒ–ä¸auto-vectorizationæ•ˆæœï¼›
3. **æ¢ç´¢block Krylovæ–¹æ³•çš„çœŸå®æ€§èƒ½æ”¶ç›Š**ï¼šåˆ†ç¦»ç®—æ³•ä¸æœºå™¨å±‚æ¬¡çš„ä¼˜åŒ–å½±å“ï¼›
4. **ç­‰å¾…çœŸå®SMEç¡¬ä»¶ä¸Šçº¿åå¼€å±•å®æµ‹**ï¼šéªŒè¯æ¨¡æ‹Ÿç»“è®ºçš„å®é™…å¯è¡Œæ€§ï¼›
5. **ç ”ç©¶scalabilityä¸performance trade-off**ï¼šåœ¨æ›´å¤§è§„æ¨¡å¹¶è¡Œç¯å¢ƒä¸‹è¯„ä¼°è´Ÿè½½å‡è¡¡ä¸é€šä¿¡å¼€é”€ã€‚

--- 

> **è¡¥å……è¯´æ˜**ï¼šæœ¬æ–‡æ‰€æœ‰æºç ã€è„šæœ¬ä¸æ•°æ®å·²å…¬å¼€ï¼ˆDOI: [10.5281/zenodo.17495916](https://doi.org/10.5281/zenodo.17495916)ï¼‰ï¼Œå…·å¤‡å®Œæ•´å¯å¤ç°æ€§ã€‚

</details>

---

### 16. [PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning](https://arxiv.org/abs/2601.05593)

**Authors**: Jingcheng Hu, Yinmin Zhang, Shijie Shang, Xiaobo Yang, Yue Peng, Zhewei Huang, Hebin Zhou, Xin Wu, Jie Cheng, Fanqi Wan, Xiangwen Kong, Chengyuan Yao, Kaiwen Yan, Ailin Huang, Hongyu Zhou, Qi Han, Zheng Ge, Daxin Jiang, Xiangyu Zhang, Heung-Yeung Shum  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.05593v1  

#### Abstract
We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the t...

---

### 17. [Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics](https://arxiv.org/abs/2601.05929)

**Authors**: Sidney Shapiro, Burhanuddin Panvelwala  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.05929v1  

#### Abstract
Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are diffic...

---

### 18. [Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks](https://arxiv.org/abs/2601.05984)

**Authors**: Sahibzada Saadoon Hammad, Joaqu\'in Huerta Guijarro, Francisco Ramos, Michael Gould Carlson, Sergio Trilles Oliver  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.05984v1  

#### Abstract
The rapid deployment of Internet of Things (IoT) devices has led to large-scale sensor networks that monitor environmental and urban phenomena in real time. Communities of Interest (CoIs) provide a promising paradigm for organising heterogeneous IoT sensor networks by grouping devices with similar o...

---

### 19. [LLMs as Science Journalists: Supporting Early-stage Researchers in Communicating Their Science to the Public](https://arxiv.org/abs/2601.05821)

**Authors**: Milad Alshomary, Grace Li, Anubhav Jangra, Yufang Hou, Kathleen McKeown, Smaranda Muresan  
**Category**: cs.CL  
**Published**: 2026-01-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.05821v1  

#### Abstract
The scientific community needs tools that help early-stage researchers effectively communicate their findings and innovations to the public. Although existing general-purpose Large Language Models (LLMs) can assist in this endeavor, they are not optimally aligned for it. To address this, we propose ...

---

### 20. [Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards](https://arxiv.org/abs/2601.06021)

**Authors**: Jiajie Zhang, Xin Lv, Ling Feng, Lei Hou, Juanzi Li  
**Category**: cs.CL  
**Published**: 2026-01-12  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.06021v1  

#### Abstract
Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable be...

---

### 21. [Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing](https://arxiv.org/abs/2601.05298)

**Authors**: Yeongbin Cha, Namjung Kim  
**Category**: cs.AI  
**Published**: 2026-01-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.05298v1  

#### Abstract
Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ont...

---

### 22. [Enhancing Foundation Models in Transaction Understanding with LLM-based Sentence Embeddings](https://arxiv.org/abs/2601.05271)

**Authors**: Xiran Fan, Zhimeng Jiang, Chin-Chia Michael Yeh, Yuzhong Chen, Yingtong Dou, Menghai Pan, Yan Zheng  
**Category**: cs.CL  
**Published**: 2026-01-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.05271v1  

#### Abstract
The ubiquity of payment networks generates vast transactional data encoding rich consumer and merchant behavioral patterns. Recent foundation models for transaction analysis process tabular data sequentially but rely on index-based representations for categorical merchant fields, causing substantial...

---

### 23. [MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards](https://arxiv.org/abs/2601.05488)

**Authors**: Zhiyu Shen, Ziming Wu, Fuming Lai, Shaobing Lian, Yanghui Rao  
**Category**: cs.CL  
**Published**: 2026-01-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.05488v1  

#### Abstract
Maintaining consistency in long-term dialogues remains a fundamental challenge for LLMs, as standard retrieval mechanisms often fail to capture the temporal evolution of historical states. While memory-augmented frameworks offer a structured alternative, current systems rely on static prompting of c...

---

### 24. [Ontology Neural Networks for Topologically Conditioned Constraint Satisfaction](https://arxiv.org/abs/2601.05304)

**Authors**: Jaehong Oh  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.05304v1  

#### Abstract
Neuro-symbolic reasoning systems face fundamental challenges in maintaining semantic coherence while satisfying physical and logical constraints. Building upon our previous work on Ontology Neural Networks, we present an enhanced framework that integrates topological conditioning with gradient stabi...

---

### 25. [GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting](https://arxiv.org/abs/2601.05353)

**Authors**: Shovito Barua Soumma, Hassan Ghasemzadeh  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.05353v1  

#### Abstract
Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional s...

---

### 26. [DynaSTy: A Framework for SpatioTemporal Node Attribute Prediction in Dynamic Graphs](https://arxiv.org/abs/2601.05391)

**Authors**: Namrata Banerji, Tanya Berger-Wolf  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.05391v1  

#### Abstract
Accurate multistep forecasting of node-level attributes on dynamic graphs is critical for applications ranging from financial trust networks to biological networks. Existing spatiotemporal graph neural networks typically assume a static adjacency matrix. In this work, we propose an end-to-end dynami...

---

### 27. [Buffered AUC maximization for scoring systems via mixed-integer optimization](https://arxiv.org/abs/2601.05544)

**Authors**: Moe Shiina, Shunnosuke Ikeda, Yuichi Takano  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.05544v1  

#### Abstract
A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies ha...

---

### 28. [Do Sparse Autoencoders Identify Reasoning Features in Language Models?](https://arxiv.org/abs/2601.05679)

**Authors**: George Ma, Zhongyuan Liang, Irene Y. Chen, Somayeh Sojoudi  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.05679v1  

#### Abstract
We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and L...

---

### 29. [GlueNN: gluing patchwise analytic solutions with neural networks](https://arxiv.org/abs/2601.05889)

**Authors**: Doyoung Kim, Donghee Lee, Hye-Sung Lee, Jiheon Lee, Jaeok Yi  
**Category**: cs.LG  
**Published**: 2026-01-12  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.05889v1  

#### Abstract
In many problems in physics and engineering, one encounters complicated differential equations with strongly scale-dependent terms for which exact analytical or numerical solutions are not available. A common strategy is to divide the domain into several regions (patches) and simplify the equation i...

---

### 30. [MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis](https://arxiv.org/abs/2601.05483)

**Authors**: Zixuan Xiao, Jun Ma, Siwei Zhang  
**Category**: cs.AI  
**Published**: 2026-01-12  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.05483v1  

#### Abstract
Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly in...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
