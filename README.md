# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-24 12:56:38 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Fast LLM Post-training via Decoupled and Best-of-N Speculation](https://arxiv.org/abs/2511.16193)

**Authors**: Rongxin Cheng, Kai Zhou, Xingda Wei, Siyuan Liu, Mingcong Han, Mingjing Ai, Yeju Zhou, Baoquan Zhong, Wencong Xiao, Rong Chen, Haibo Chen  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.16193v2  

Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable ...

---

### 2. [E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models](https://arxiv.org/abs/2511.17205)

**Authors**: Tao Yuan, Haoli Bai, Yinfei Pan, Xuyang Cao, Tianyu Zhang, Lu Hou, Ting Hu, Xianzhi Yu  
**Category**: cs.CL  
**Published**: 2025-11-24  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.17205v1  

With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, hi...

---

### 3. [Bench360: Benchmarking Local LLM Inference from 360{\deg}](https://arxiv.org/abs/2511.16682)

**Authors**: Linus Stuhlmann, Mauricio Fadel Argerich, Jonathan F\"urst  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.16682v1  

Running large language models (LLMs) locally is becoming increasingly common. While the growing availability of small open-source models and inference engines has lowered the entry barrier, users now face an overwhelming number of configuration choices. Identifying an optimal configuration -- balanc...

---

### 4. [Revisiting Multimodal KV Cache Compression: A Frequency-Domain-Guided Outlier-KV-Aware Approach](https://arxiv.org/abs/2511.16786)

**Authors**: Yaoxin Yang, Peng Ye, Xudong Tan, Chongjun Tu, Maosen Zhao, Jia Hao, Tao Chen  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.16786v1  

Multimodal large language models suffer from substantial inference overhead since multimodal KV Cache grows proportionally with the visual input length. Existing multimodal KV Cache compression methods mostly rely on attention score to reduce cache size, which makes them are incompatible with establ...

---

### 5. [How LLMs Learn to Reason: A Complex Network Perspective](https://arxiv.org/abs/2509.23629)

**Authors**: Sihan Hu, Xiansheng Cai, Yuan Huang, Zhiyuan Yao, Linfeng Zhang, Pan Zhang, Youjin Deng, Kun Chen  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2509.23629v2  

Training large language models with Reinforcement Learning with Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, a V-shaped response-length trajectory, and a pronounced vulnerability to catastrophic fo...

---

### 6. [Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required?](https://arxiv.org/abs/2511.17400)

**Authors**: Sukwon Yun, Heming Yao, Burkhard Hoeckendorf, David Richmond, Aviv Regev, Russell Littman  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.17400v1  

Vision Transformers ($\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channe...

---

### 7. [RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning](https://arxiv.org/abs/2509.00974)

**Authors**: Chia-Hsuan Hsu, Jun-En Ding, Hsin-Ling Hsu, Chih-Ho Hsu, Li-Hung Yao, Chun-Chieh Liao, Feng Liu, Fang-Ming Hung  
**Category**: cs.CL  
**Published**: 2025-11-24  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2509.00974v4  

Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization ...

---

### 8. [DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern](https://arxiv.org/abs/2509.24975)

**Authors**: Lekang Yang, Yuetong Liu, Yitong Zhang, Jia Li  
**Category**: cs.CL  
**Published**: 2025-11-24  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2509.24975v2  

Software development relies heavily on extensive unit testing, which makes the efficiency of automated Unit Test Generation (UTG) particularly important. However, most existing LLMs generate test cases one token at a time in each forward pass, which leads to inefficient UTG. Recently, diffusion LLMs...

---

### 9. [Holographic Knowledge Manifolds: A Novel Pipeline for Continual Learning Without Catastrophic Forgetting in Large Language Models](https://arxiv.org/abs/2509.10518)

**Authors**: Justin Arndt  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2509.10518v2  

We introduce the Holographic Knowledge Manifold (HKM), a four-phase pipeline that achieves zero catastrophic forgetting in AI knowledge representation while maintaining minimal memory growth and high efficiency. Leveraging fractal quantization, probabilistic entanglement, and dynamic diffraction chi...

---

### 10. [Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2511.16964)

**Authors**: Kirill Nagaitsev, Luka Grbcic, Samuel Williams, Costin Iancu  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.16964v1  

Maximizing performance on available GPU hardware is an ongoing challenge for modern AI inference systems. Traditional approaches include writing custom GPU kernels and using specialized model compilers to tune high-level code for specific GPU targets. Recent work shows that LLM-based multi-agent sys...

---

### 11. [FAR: Function-preserving Attention Replacement for IMC-friendly Inference](https://arxiv.org/abs/2505.21535)

**Authors**: Yuxin Ren, Maxwell D Collins, Miao Hu, Huanrui Yang  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.21535v3  

While transformers dominate modern vision and language models, their attention mechanism remains poorly suited for in-memory computing (IMC) devices due to intensive activation-to-activation multiplications and non-local memory access, leading to substantial latency and bandwidth overhead on ReRAM-b...

---

### 12. [VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference](https://arxiv.org/abs/2511.16449)

**Authors**: Ziyan Liu, Yeqiu Chen, Hongyi Cai, Tao Lin, Shuo Yang, Zheng Liu, Bo Zhao  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2511.16449v2  

Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective app...

---

### 13. [Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems](https://arxiv.org/abs/2511.17435)

**Authors**: Zengyu Zou, Jingyuan Wang, Yixuan Huang, Junjie Wu  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.17435v1  

This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the veh...

---

### 14. [Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization](https://arxiv.org/abs/2511.17489)

**Authors**: Vinay Kanakeri, Shivam Bajaj, Ashwin Verma, Vijay Gupta, Aritra Mitra  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.17489v1  

It is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it has been proposed that the learning algorithm utilize data from 'approximately similar' processes. However, since the process models are unknown, identifying which other processes are similar poses a ...

---

### 15. [Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration](https://arxiv.org/abs/2511.17123)

**Authors**: Jiaxun Fang, Li Zhang, Shaoyi Huang  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.17123v1  

Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies...

---

### 16. [Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search](https://arxiv.org/abs/2511.16681)

**Authors**: Dong Liu, Yanxuan Yu  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.16681v1  

Retrieval-Augmented Generation (RAG) systems have become a dominant approach to augment large language models (LLMs) with external knowledge. However, existing vector database (VecDB) retrieval pipelines rely on flat or single-resolution indexing structures, which cannot adapt to the varying semanti...

---

### 17. [Estimating Global Input Relevance and Enforcing Sparse Representations with a Scalable Spectral Neural Network Approach](https://arxiv.org/abs/2406.01183)

**Authors**: Lorenzo Chicchi, Lorenzo Buffoni, Diego Febbe, Lorenzo Giambagli, Raffaele Marino, Duccio Fanelli  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2406.01183v3  

In machine learning practice it is often useful to identify relevant input features. Isolating key input elements, ranked according their respective degree of relevance, can help to elaborate on the process of decision making. Here, we propose a novel method to estimate the relative importance of th...

---

### 18. [Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge](https://arxiv.org/abs/2503.09114)

**Authors**: Maximilian Abstreiter, Sasu Tarkoma, Roberto Morabito  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2503.09114v2  

The rapid rise of Language Models (LMs) has expanded the capabilities of natural language processing, powering applications from text generation to complex decision-making. While state-of-the-art LMs often boast hundreds of billions of parameters and are primarily deployed in data centers, recent tr...

---

### 19. [LLM-DSE: Searching Accelerator Parameters with LLM Agents](https://arxiv.org/abs/2505.12188)

**Authors**: Hanyu Wang, Xinrui Wu, Zijian Ding, Su Zheng, Chengyue Wang, Neha Prakriya, Tony Nowatzki, Yizhou Sun, Jason Cong  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.12188v3  

Even though high-level synthesis (HLS) tools mitigate the challenges of programming domain-specific accelerators (DSAs) by raising the abstraction level, optimizing hardware directive parameters remains a significant hurdle. Existing heuristic and learning-based methods struggle with adaptability an...

---

### 20. [Step-E: A Differentiable Data Cleaning Framework for Robust Learning with Noisy Labels](https://arxiv.org/abs/2511.17040)

**Authors**: Wenzhang Du  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.17040v1  

Training data collected in the wild often contain noisy labels and outliers that substantially degrade the performance and reliability of deep neural networks. While data cleaning is commonly applied as a separate preprocessing stage, such two-stage pipelines neither fully exploit feedback from the ...

---

### 21. [Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models](https://arxiv.org/abs/2511.16955)

**Authors**: Dailan He, Guanlin Feng, Xingtong Ge, Yazhe Niu, Yi Zhang, Bingqi Ma, Guanglu Song, Yu Liu, Hongsheng Li  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.16955v1  

Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences. However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm. Current methods address this issue by converting Ordina...

---

### 22. [OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation](https://arxiv.org/abs/2511.17126)

**Authors**: Qi Jiang, Xiaolong Qian, Yao Gao, Lei Sun, Kailun Yang, Zhonghua Yi, Wenyong Li, Ming-Hsuan Yang, Luc Van Gool, Kaiwei Wang  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.17126v1  

Emerging deep-learning-based lens library pre-training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown optical degradations. This work proposes the OmniLens++ framework,...

---

### 23. [Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning](https://arxiv.org/abs/2506.02392)

**Authors**: Yuanyao Chen, Rongsheng Chen, Fu Luo, Zhenkun Wang  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 6.5  
**Type**: replace  
**ArXiv ID**: 2506.02392v3  

Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated conside...

---

### 24. [Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning](https://arxiv.org/abs/2511.05050)

**Authors**: Masahiro Tanaka  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.05050v2  

In this study, a scalable online kernel learning framework is proposed for estimating bidirectional causal effects in systems characterized by mutual dependence and heteroskedasticity. Traditional causal inference often focuses on unidirectional effects, overlooking the common bidirectional relation...

---

### 25. [CharCom: Composable Identity Control for Multi-Character Story Illustration](https://arxiv.org/abs/2510.10135)

**Authors**: Zhongsheng Wang, Ming Lin, Zhedong Lin, Yaser Shakib, Qian Liu, Jiamou Liu  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 6.0  
**Type**: replace  
**ArXiv ID**: 2510.10135v2  

Ensuring character identity consistency across varying prompts remains a fundamental limitation in diffusion-based text-to-image generation. We propose CharCom, a modular and parameter-efficient framework that achieves character-consistent story illustration through composable LoRA adapters, enablin...

---

### 26. [Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning](https://arxiv.org/abs/2511.16202)

**Authors**: Pei Yang, Ke Zhang, Ji Wang, Xiao Chen, Yuxin Tang, Eric Yang, Lynn Ai, Bill Shi  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 6.0  
**Type**: replace  
**ArXiv ID**: 2511.16202v2  

We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflict...

---

### 27. [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL with GRPO](https://arxiv.org/abs/2510.13827)

**Authors**: Ashish Kattamuri, Ishita Prasad, Meetu Malhotra, Arpita Vats, Rahul Raja, Albert Lie  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 6.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.13827v2  

Current Text-to-SQL methods are evaluated and only focused on executable queries, overlooking the semantic alignment challenge -- both in terms of the semantic meaning of the query and the correctness of the execution results. Even execution accuracy itself shows significant drops when moving from E...

---

### 28. [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)

**Authors**: Dongdong Zhao, Qiben Xu, Ranxin Fang, Baogang Song  
**Category**: cs.AI  
**Published**: 2025-11-24  
**Score**: 6.0  
**Type**: replace-cross  
**ArXiv ID**: 2511.12233v2  

Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversi...

---

### 29. [Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems](https://arxiv.org/abs/2511.13313)

**Authors**: Sulaiman Muhammad Rashid, Ibrahim Aliyu, Jaehyung Park, Jinsul Kim  
**Category**: cs.DC  
**Published**: 2025-11-24  
**Score**: 6.0  
**Type**: replace  
**ArXiv ID**: 2511.13313v2  

The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-en...

---

### 30. [FIRM: Federated In-client Regularized Multi-objective Alignment for Large Language Models](https://arxiv.org/abs/2511.16992)

**Authors**: Fatemeh (Atena),  Nourzad (Kevin), Amirhossein Roknilamouki (Kevin), Eylem Ekici (Kevin),  Jia (Kevin),  Liu, Ness B. Shroff  
**Category**: cs.LG  
**Published**: 2025-11-24  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.16992v1  

Aligning Large Language Models (LLMs) with human values often involves balancing multiple, conflicting objectives such as helpfulness and harmlessness. Training these models is computationally intensive, and centralizing the process raises significant data privacy concerns. Federated Learning (FL) o...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
