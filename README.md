# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-09-01 12:53:08 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding](https://arxiv.org/abs/2508.21706)

**Authors**: Zhibin Wang, Zhonghui Zhang, Yuhang Zhou, Zibo Wang, Mo Zhou, Peng Jiang, Weilin Cai, Chengying Huan, Rong Gu, Sheng Zhong, Chen Tian  
**Category**: cs.DC  
**Published**: 2025-09-01  
**Score**: 6.0

arXiv:2508.21706v1 Announce Type: new 
Abstract: Recent advancements in Mixture of Experts (MoE) models have significantly increased their parameter scale as well as model performance. Extensive offloading techniques have been proposed to address the GPU memory limitations of MoE inference. However,...

---

### 2. [SpecPipe: Accelerating Pipeline Parallelism-based LLM Inference with Speculative Decoding](https://arxiv.org/abs/2504.04104)

**Authors**: Haofei Yin, Mengbai Xiao, Tinghong Li, Xiao Zhang, Dongxiao Yu, Guanghui Zhang  
**Category**: cs.LG  
**Published**: 2025-09-01  
**Score**: 6.0

arXiv:2504.04104v2 Announce Type: replace 
Abstract: The demand for large language model inference is rapidly increasing. Pipeline parallelism offers a cost-effective deployment strategy for distributed inference but suffers from high service latency. While incorporating speculative decoding to pipe...

---

### 3. [BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens](https://arxiv.org/abs/2508.17196)

**Authors**: Hao Wen, Xinrui Wu, Yi Sun, Feifei Zhang, Liye Chen, Jie Wang, Yunxin Liu, Yunhao Liu, Ya-Qin Zhang, Yuanchun Li  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 5.5

arXiv:2508.17196v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have leveraged increased test-time computation to enhance reasoning capabilities, a strategy that, while effective, incurs significant latency and resource costs, limiting their applicabili...

---

### 4. [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)

**Authors**: Weizhi Gao, Xiaorui Liu, Feiyi Wang, Dan Lu, Junqi Yin  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 4.5

arXiv:2508.21228v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated impressive performance in both research and real-world applications, but they still struggle with hallucination. Existing hallucination detection methods often perform poorly on sentence-level generatio...

---

### 5. [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)

**Authors**: Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 4.5

arXiv:2508.21476v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods li...

---

### 6. [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)

**Authors**: Liulu He, Shenli Zheng, Karwei Sun, Yijiang Liu, Yufei Zhao, Chongkang Tan, Huanrui Yang, Yuan Du, Li Du  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 4.5

arXiv:2506.15689v2 Announce Type: replace-cross 
Abstract: Rotations have become essential to state-of-the-art quantization pipelines for large language models (LLMs) by effectively smoothing outliers in weights and activations. However, further optimizing the rotation parameters offers only limited...

---

### 7. [Robustness is Important: Limitations of LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)

**Authors**: Hejia Liu, Mochen Yang, Gediminas Adomavicius  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 4.5

arXiv:2508.19563v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are being applied in a wide array of settings, well beyond the typical language-oriented use cases. In particular, LLMs are increasingly used as a plug-and-play method for fitting data and generating predictions....

---

### 8. [FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models](https://arxiv.org/abs/2505.15683)

**Authors**: Zishuai Zhang, Hainan zhang, Weihua Li, Qinnan zhang, jin Dong, Yongxin Tong, Zhiming Zheng  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 4.0

arXiv:2505.15683v2 Announce Type: replace-cross 
Abstract: Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transfo...

---

### 9. [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)

**Authors**: Jia Liu, ChangYi He, YingQiao Lin, MingMin Yang, FeiYang Shen, ShaoGuo Liu  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 4.0

arXiv:2508.11356v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models have yielded significant improvements in complex reasoning tasks such as mathematics and programming. However, these models remain heavily dependent on annotated data and exhibit limited adaptabil...

---

### 10. [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)

**Authors**: Yao Wang, Di Liang, Minlong Peng  
**Category**: cs.CL  
**Published**: 2025-09-01  
**Score**: 4.0

arXiv:2508.21741v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) is a pivotal approach to adapting large language models (LLMs) for downstream tasks; however, performance often suffers from the ``seesaw phenomenon'', where indiscriminate parameter updates yield progress on certain tasks...

---

### 11. [Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning](https://arxiv.org/abs/2508.19828)

**Authors**: Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich Sch\"utze, Volker Tresp, Yunpu Ma  
**Category**: cs.CL  
**Published**: 2025-09-01  
**Score**: 4.0

arXiv:2508.19828v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address ...

---

### 12. [RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation](https://arxiv.org/abs/2508.21378)

**Authors**: Chenduo Ying, Linkang Du, Peng Cheng, Yuanchao Shu  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.5

arXiv:2508.21378v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate remarkable capabilities in reasoning and code generation, enabling robotic manipulation to be initiated with just a single instruction. The LLM carries out various tasks by generating policy code required to ...

---

### 13. [Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land](https://arxiv.org/abs/2404.17625)

**Authors**: Simone Scardapane  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.5

arXiv:2404.17625v3 Announce Type: replace-cross 
Abstract: Neural networks surround us, in the form of large language models, speech transcription systems, molecular discovery algorithms, robotics, and much more. Stripped of anything else, neural networks are compositions of differentiable primitive...

---

### 14. [RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis](https://arxiv.org/abs/2411.18948)

**Authors**: Xue Tan, Hao Luan, Mingyu Luo, Xiaoyan Sun, Ping Chen, Jun Dai  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.5

arXiv:2411.18948v5 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving information from the relevant knowledge database, enabling them to produce responses that are more accurate and contextually appropriate. It is worth noting that t...

---

### 15. [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)

**Authors**: Shree Mitra, Ritabrata Chakraborty, Nilkanta Sahu  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.5

arXiv:2508.06107v2 Announce Type: replace-cross 
Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task due to the inherent two-dimensional structure, varying symbol scales, and complex spatial relationships among symbols. In this paper, we present a self-supervised ...

---

### 16. [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)

**Authors**: Haoze Wu, Cheng Wang, Wenshuo Zhao, Junxian He  
**Category**: cs.CL  
**Published**: 2025-09-01  
**Score**: 3.5

arXiv:2508.21188v1 Announce Type: cross 
Abstract: Recent advances in applying reinforcement learning (RL) to large language models (LLMs) have led to substantial progress. In particular, a series of remarkable yet often counterintuitive phenomena have been reported in LLMs, exhibiting patterns not ...

---

### 17. [Adaptive LLM Routing under Budget Constraints](https://arxiv.org/abs/2508.21141)

**Authors**: Pranoy Panda, Raghav Magazine, Chaitanya Devaguptapu, Sho Takemori, Vishal Sharma  
**Category**: cs.LG  
**Published**: 2025-09-01  
**Score**: 3.5

arXiv:2508.21141v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/...

---

### 18. [CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams](https://arxiv.org/abs/2508.21273)

**Authors**: Ashok Devireddy, Shunping Huang  
**Category**: cs.LG  
**Published**: 2025-09-01  
**Score**: 3.5

arXiv:2508.21273v1 Announce Type: new 
Abstract: The detection of anomalies in non-stationary time-series streams is a critical but challenging task across numerous industrial and scientific domains. Traditional models, trained offline, suffer significant performance degradation when faced with conc...

---

### 19. [Improving Fisher Information Estimation and Efficiency for LoRA-based LLM Unlearning](https://arxiv.org/abs/2508.21300)

**Authors**: Yejin Kim, Eunwon Kim, Buru Chang, Junsuk Choe  
**Category**: cs.LG  
**Published**: 2025-09-01  
**Score**: 3.5

arXiv:2508.21300v1 Announce Type: new 
Abstract: LLMs have demonstrated remarkable performance across various tasks but face challenges related to unintentionally generating outputs containing sensitive information. A straightforward approach to address this issue is to retrain the model after exclu...

---

### 20. [Faster Inference of Cell Complexes from Flows via Matrix Factorization](https://arxiv.org/abs/2508.21372)

**Authors**: Til Spreuer, Josef Hoppe, Michael T. Schaub  
**Category**: cs.LG  
**Published**: 2025-09-01  
**Score**: 3.5

arXiv:2508.21372v1 Announce Type: cross 
Abstract: We consider the following inference problem: Given a set of edge-flow signals observed on a graph, lift the graph to a cell complex, such that the observed edge-flow signals can be represented as a sparse combination of gradient and curl flows on th...

---

### 21. [Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2508.21365)

**Authors**: Yi Liao, Yu Gu, Yuan Sui, Zining Zhu, Yifan Lu, Guohua Tang, Zhongqian Sun, Wei Yang  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2508.21365v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between decla...

---

### 22. [PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning](https://arxiv.org/abs/2508.21104)

**Authors**: Wenfeng Feng, Penghong Zhao, Guochao Jiang, Chuzhan Hao, Yuewei Zhang, Hao Wang  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2508.21104v1 Announce Type: cross 
Abstract: Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estim...

---

### 23. [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)

**Authors**: Dongjun Lee, Changho Hwang, Kimin Lee  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2508.21107v1 Announce Type: cross 
Abstract: Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate te...

---

### 24. [EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control](https://arxiv.org/abs/2508.21112)

**Authors**: Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2508.21112v1 Announce Type: cross 
Abstract: The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale ...

---

### 25. [zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs](https://arxiv.org/abs/2508.21393)

**Authors**: Guofu Liao, Taotao Wang, Shengli Zhang, Jiqun Zhang, Shi Long, Dacheng Tao  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2508.21393v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) is crucial for adapting them to specific tasks, yet it remains computationally demanding and raises concerns about correctness and privacy, particularly in untrusted environments. Although parameter-efficient...

---

### 26. [MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation](https://arxiv.org/abs/2508.21435)

**Authors**: Francisco Caetano, Christiaan Viviers, Peter H. H. de With, Fons van der Sommen  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2508.21435v1 Announce Type: cross 
Abstract: Synthetic medical data offers a scalable solution for training robust models, but significant domain gaps limit its generalizability to real-world clinical settings. This paper addresses the challenge of cross-domain translation between synthetic an...

---

### 27. [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)

**Authors**: Peng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2508.21632v1 Announce Type: cross 
Abstract: We present QZhou-Embedding, a general-purpose contextual text embedding model with exceptional text representation capabilities. Built upon the Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task framework comprising specialized d...

---

### 28. [Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight](https://arxiv.org/abs/2508.21777)

**Authors**: Ugur Dinc, Jibak Sarkar, Philipp Schubert, Sabine Semrau, Thomas Weissmann, Andre Karius, Johann Brand, Bernd-Niklas Axer, Ahmed Gomaa, Pluvio Stephan, Ishita Sheth, Sogand Beirami, Annette Schwarz, Udo Gaipl, Benjamin Frey, Christoph Bert, Stefanie Corradini, Rainer Fietkau, Florian Putz  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2508.21777v1 Announce Type: cross 
Abstract: Introduction: Large language models (LLM) have shown great potential in clinical decision support. GPT-5 is a novel LLM system that has been specifically marketed towards oncology use.
  Methods: Performance was assessed using two complementary benc...

---

### 29. [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)

**Authors**: In\'es Altemir Marinas, Anastasiia Kucherenko, Andrei Kucharavy  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2508.21788v1 Announce Type: cross 
Abstract: Large language models (LLMs) rely heavily on web-scale datasets like Common Crawl, which provides over 80\% of training data for some modern models. However, the indiscriminate nature of web crawling raises challenges in data quality, safety, and et...

---

### 30. [Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods under Knowledge Incompleteness](https://arxiv.org/abs/2504.05163)

**Authors**: Dongzhuoran Zhou, Yuqicheng Zhu, Xiaxia Wang, Yuan He, Jiaoyan Chen, Steffen Staab, Evgeny Kharlamov  
**Category**: cs.AI  
**Published**: 2025-09-01  
**Score**: 3.0

arXiv:2504.05163v2 Announce Type: replace 
Abstract: Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique that enhances Large Language Model (LLM) inference in tasks like Question Answering (QA) by retrieving relevant information from knowledge graphs (KGs). However, real-wor...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative Decoding

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
