# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-03 06:35:30 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts](https://arxiv.org/abs/2602.02108)

**Authors**: Wenhao Li, Daohai Yu, Gen Luo, Yuxin Zhang, Fei Chao, Rongrong Ji, Yifan Wu, Jiaxin Liu, Ziyang Gong, Zimu Liao  
**Category**: cs.CL  
**Published**: 2026-02-03  
**Score**: 13.5  
**Type**: new  
**ArXiv ID**: 2602.02108v1  

#### Abstract
Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system th...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
è®­ç»ƒå…·æœ‰**ç™¾ä¸‡çº§ Token ä¸Šä¸‹æ–‡é•¿åº¦**çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´ä¸¥é‡çš„ **GPU å†…å­˜ç“¶é¢ˆ**ï¼Œå°¤å…¶æ˜¯ç”± **Activations** å’Œ **KV Cache** å¼•èµ·çš„å†…å­˜å¼€é”€éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚ ZeRO-3ã€Tensor Parallelism åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹ä»éœ€å¤§è§„æ¨¡é›†ç¾¤ï¼Œæˆæœ¬é«˜æ˜‚ã€‚

è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³ï¼š**å¦‚ä½•åœ¨æä½ç¡¬ä»¶èµ„æºä¸‹é«˜æ•ˆè®­ç»ƒè¶…é•¿ä¸Šä¸‹æ–‡ LLMs**ï¼Œçªç ´â€œå†…å­˜å¢™â€é™åˆ¶ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šOOMB
ä½œè€…æå‡º **OOMB**ï¼ˆOut Of the Memory Barrierï¼‰ï¼Œä¸€ä¸ªé«˜åº¦å†…å­˜é«˜æ•ˆçš„è®­ç»ƒç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ **Chunk-Recurrent Training Framework**ï¼Œå¹¶é›†æˆå¤šé¡¹ååŒä¼˜åŒ–æŠ€æœ¯ï¼š

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š
1. **Chunk-wise Recurrent Training + On-the-fly Activation Recomputation**
   - å°†é•¿åºåˆ—åˆ†å—å¤„ç†ï¼Œå‰å‘ä¼ æ’­åç«‹å³ä¸¢å¼ƒä¸­é—´æ¿€æ´»å€¼ï¼ˆactivationsï¼‰ï¼Œåå‘ä¼ æ’­æ—¶æŒ‰éœ€é‡æ–°è®¡ç®—ã€‚
   - å®ç° **O(1)** çš„ activation å†…å­˜å¤æ‚åº¦ï¼Œå½»åº•æ¶ˆé™¤ activation éšé•¿åº¦å¢é•¿çš„å†…å­˜è´Ÿæ‹…ã€‚

2. **Paged Memory Manager for KV Cache and Gradients**
   - å— PagedAttention å¯å‘ï¼Œè®¾è®¡äº†æ”¯æŒåå‘ä¼ æ’­çš„ **åˆ†é¡µ KV Cache ç®¡ç†å™¨**ã€‚
   - æ¶ˆé™¤å†…å­˜ç¢ç‰‡åŒ–å’Œé¢‘ç¹æ‹·è´æ“ä½œï¼Œæå‡å†…å­˜åˆ©ç”¨ç‡ã€‚
   - è‡ªå®šä¹‰ Triton kernel ç»•è¿‡ PyTorch Autogradï¼Œå®ç°æ¢¯åº¦åŸåœ°ç´¯åŠ ï¼ˆatomic_addï¼‰ï¼Œè¿›ä¸€æ­¥é™ä½å†…å­˜å¼€é”€ã€‚

3. **Asynchronous CPU Offloading for KV Cache**
   - å°†ä¸æ–­å¢é•¿çš„ KV Cache å’Œå…¶æ¢¯åº¦å¼‚æ­¥å¸è½½åˆ° CPU å†…å­˜ä¸­ã€‚
   - åˆ©ç”¨ **pinned memory** å’Œ **CUDA streams** å®ç°çœŸæ­£å¼‚æ­¥ä¼ è¾“ï¼Œé€šè¿‡è®¡ç®—æ©ç›–é€šä¿¡å»¶è¿Ÿï¼ˆlatency hidingï¼‰ï¼Œç«¯åˆ°ç«¯å¼€é”€ < 5%ã€‚

4. **Page-level Sparse Attention**
   - æ”¯æŒä¸¤ç§ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼š
     - å¯¹äº Qwen2.5 ç­‰ dense attention æ¨¡å‹ï¼šé‡‡ç”¨ **Top-K é¡µé¢æ£€ç´¢æœºåˆ¶** è¿‘ä¼¼å…¨æ³¨æ„åŠ›ã€‚
     - å¯¹äº GPT-OSS ç­‰åŸç”Ÿç¨€ç–æ¨¡å‹ï¼šç›´æ¥åˆ©ç”¨å…¶å±€éƒ¨æ³¨æ„åŠ›ç»“æ„ã€‚
   - æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦å’Œé€šä¿¡å¼€é”€ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒæœ‰æ•ˆæ€§ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | æ¿€æ´»å†…å­˜ | KV Cache ç®¡ç† | æ˜¯å¦éœ€è¦å¤šå¡ | æœ€å¤§ä¸Šä¸‹æ–‡ |
|------|----------|----------------|---------------|------------|
| Standard Parallel | O(N) | æ— ä¼˜åŒ– | å•å¡å—é™ | ~32Kâ€“128K |
| LongLoRA / SeCO | O(N) æˆ– O(1) | æ— æœ‰æ•ˆç®¡ç† | å•å¡ | â‰¤128K |
| Context Parallelism (e.g., Ring Attention) | O(N/P) | åˆ†å¸ƒå¼å­˜å‚¨ | å¤šå¡ï¼ˆ8+ï¼‰ | 256Kâ€“1M |
| **OOMB (æœ¬å·¥ä½œ)** | **O(1)** | **åˆ†é¡µ + å¼‚æ­¥å¸è½½ + ç¨€ç–æ³¨æ„åŠ›** | **å•å¡å³å¯** | **4M+** |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼š  
> - **å•å¼  H200 GPU å³å¯è®­ç»ƒ 4M-token ä¸Šä¸‹æ–‡çš„ Qwen2.5-7B æ¨¡å‹**ï¼Œè€ŒåŒç±»ä»»åŠ¡é€šå¸¸éœ€æ•°ç™¾ GPU çš„é›†ç¾¤ã€‚  
> - æ¯å¢åŠ  10K tokensï¼Œç«¯åˆ°ç«¯è®­ç»ƒå†…å­˜ä»…å¢åŠ  **çº¦ 10MB**ï¼Œæ¥è¿‘å¸¸æ•°å¢é•¿ã€‚  
> - æ”¯æŒçµæ´»é…ç½®ï¼ˆchunk sizeã€sparse budgetï¼‰ï¼Œå…¼é¡¾æ•ˆç‡ä¸ç²¾åº¦ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **arXiv dataset**ï¼ˆSoboleva et al., 2023ï¼‰ï¼šç”¨äºæ„å»ºé•¿ä¸Šä¸‹æ–‡è®­ç»ƒæ ·æœ¬ã€‚
- é€šè¿‡å¯¹åŸå§‹æ–‡æ¡£è¿›è¡Œæ‹¼æ¥ï¼Œæ„é€ ä¸åŒé•¿åº¦ï¼ˆä» 32K åˆ° 8M tokensï¼‰çš„è¾“å…¥åºåˆ—ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹**ï¼š`Qwen2.5-7B`ï¼ˆYang et al., 2025ï¼‰
- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA H200 / A100 GPUï¼Œéƒ¨åˆ†å®éªŒä½¿ç”¨ 4Ã—H200 + Tensor Parallelism
- **ç²¾åº¦**ï¼šbfloat16
- **ä¼˜åŒ–å™¨**ï¼šAdam (`lr=5e-5`, `betas=(0.9, 0.98)`ï¼‰
- **Batch Size**ï¼šæ¯ GPU 1 ä¸ªæ ·æœ¬ï¼ˆunit batchï¼‰
- **Page Size**ï¼š128 tokens
- **Chunk Sizes æµ‹è¯•èŒƒå›´**ï¼š512 ~ 4096 tokens

---

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Peak GPU Memory Usage** | å•è®¾å¤‡å³°å€¼æ˜¾å­˜å ç”¨ï¼ˆMB/GBï¼‰ |
| **Per-iteration Latency** | å•æ¬¡è¿­ä»£è€—æ—¶ï¼ˆç§’ï¼‰ |
| **Throughput (tokens/sec)** | æ¯ç§’å¤„ç† token æ•°é‡ |
| **Gradient Approximation Error (L2 Norm)** | ç¨€ç–æ³¨æ„åŠ›å¸¦æ¥çš„æ¢¯åº¦è¿‘ä¼¼è¯¯å·® |
| **Training Loss Curve** | ä¸åŒé…ç½®ä¸‹çš„æ”¶æ•›è¡Œä¸º |

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
1. **Standard Parallel Training**
   - ä½¿ç”¨ FlashAttention + Layer-wise Gradient Checkpointing
2. **Ring Flash Attention (RFA)**ï¼ˆLin, 2025ï¼‰
   - å½“å‰æœ€å…ˆè¿›çš„ Context Parallelism å®ç°
3. **SeCO**ï¼ˆLi et al., 2025ï¼‰
   - æ—©æœŸ chunk-wise è®­ç»ƒå°è¯•ï¼Œç¼ºä¹åº•å±‚ä¼˜åŒ–

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®

#### âœ… æè‡´å†…å­˜æ•ˆç‡
- **æ¯å¢åŠ  10K context tokensï¼Œå†…å­˜å¼€é”€ä»…ä¸Šå‡ ~10MB**
- åœ¨ **4M-token ä¸Šä¸‹æ–‡** ä¸‹ï¼š
  - ä½¿ç”¨ç¨€ç–æ³¨æ„åŠ› + CPU å¸è½½ â†’ å³°å€¼æ˜¾å­˜ç¨³å®šåœ¨ **~34GB å·¦å³**
  - è€Œæ ‡å‡†å¹¶è¡Œè®­ç»ƒåœ¨ 128K å°±å·² OOM

#### â±ï¸ æ¨ç†ä¸è®­ç»ƒé€Ÿåº¦
| Context Length | OOMB + Dense Attn | OOMB + Sparse Attn | åŠ é€Ÿæ¯” |
|----------------|--------------------|---------------------|--------|
| 64K            | ~70s/iter          | ~22s/iter           | **~3.7Ã—** |
| 256K           | ~900s/iter         | ~97s/iter           | **~9.3Ã—** |

> ğŸ’¡ ç¨€ç–æ³¨æ„åŠ›æ˜¾è‘—åŠ é€Ÿé•¿åºåˆ—è®­ç»ƒï¼Œå°¤å…¶åœ¨ >64K åœºæ™¯ä¸‹ä¼˜åŠ¿æ˜æ˜¾ã€‚

#### ğŸ“ˆ ååé‡å¯¹æ¯”ï¼ˆvs Context Parallelismï¼‰

| Context | RFA (8Ã—A100) | OOMB-Dense (1Ã—H200) | OOMB-Sparse (1Ã—H200) |
|--------|---------------|----------------------|------------------------|
| 64K    | â€”             | 936.22 t/s           | **1560.38 t/s**        |
| 128K   | â€”             | 504.12 t/s           | **1394.16 t/s**        |
| 256K   | 49 t/s        | 266.13 t/s           | **1301.60 t/s**        |

> âœ… **OOMB åœ¨å•å¡ä¸Šå®ç°äº†è¿œè¶…å¤šå¡ Context Parallelism çš„ per-device throughput**

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰Chunk Size å½±å“ï¼ˆå›¾9ï¼‰
- æ›´å¤§çš„ chunk sizeï¼ˆå¦‚ 4096ï¼‰èƒ½æ›´å¥½åˆ©ç”¨ GPU å¹¶è¡Œèƒ½åŠ›ï¼Œæé«˜ååã€‚
- ä½†æ”¶ç›Šé€’å‡ï¼Œä¸”ä¼šè½»å¾®å¢åŠ  activation å†…å­˜ã€‚
- **æ¨èé»˜è®¤å€¼ï¼š4096**ï¼Œå¹³è¡¡æ•ˆç‡ä¸å†…å­˜ã€‚

#### ï¼ˆ2ï¼‰Sparse Retrieval Budget å½±å“
- å¢å¤§ retrieval budgetï¼ˆå¦‚ä» 4K+512 åˆ° 4K+32768ï¼‰å¯é™ä½æ¢¯åº¦è¿‘ä¼¼è¯¯å·®ï¼ˆL2 error â†“ï¼‰
- ä½†åœ¨è¶…è¿‡åŸç”Ÿ context windowï¼ˆå¦‚ 128Kï¼‰åï¼Œè¯¯å·®å·®å¼‚è¶‹äºå¹³ç¼“ã€‚
- è¡¨æ˜ **é€‚åº¦ç¨€ç–å³å¯ç»´æŒè‰¯å¥½è®­ç»ƒè´¨é‡**

#### ï¼ˆ3ï¼‰CPU Offloading å¼€é”€
- å¼‚æ­¥å¸è½½æœºåˆ¶å°†ä¼ è¾“å»¶è¿Ÿæœ‰æ•ˆéšè—ï¼Œ**ç«¯åˆ°ç«¯å»¶è¿Ÿå¢åŠ  < 5%**
- ä½¿ç”¨ pinned memory + DMA å¼•æ“ç¡®ä¿é«˜å¸¦å®½ä½å»¶è¿Ÿ

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦ç»“è®º
1. **OOMB æˆåŠŸå°† activation å†…å­˜å¤æ‚åº¦é™è‡³ O(1)**ï¼Œè§£å†³äº†é•¿ä¸Šä¸‹æ–‡è®­ç»ƒä¸­æœ€å…³é”®çš„å†…å­˜ç“¶é¢ˆä¹‹ä¸€ã€‚
2. **é€šè¿‡ paged KV cache + å¼‚æ­¥å¸è½½ + page-level sparse attention çš„ååŒè®¾è®¡**ï¼Œå®ç°äº†å¯¹ KV cache çš„é«˜æ•ˆç®¡ç†ã€‚
3. **é¦–æ¬¡å®ç°åœ¨å•å¼  H200 GPU ä¸Šè®­ç»ƒ 4-million-token ä¸Šä¸‹æ–‡çš„ LLM**ï¼ˆQwen2.5-7Bï¼‰ï¼Œæ— éœ€ä¾èµ–å¤§è§„æ¨¡é›†ç¾¤ã€‚
4. **ç¨€ç–æ³¨æ„åŠ›åœ¨è®­ç»ƒä¸­æ˜¯å¯è¡Œä¸”æœ‰æ•ˆçš„**ï¼Œåœ¨åˆç† budget ä¸‹å‡ ä¹ä¸æŸå®³æ¨¡å‹æ”¶æ•›æ€§å’Œæœ€ç»ˆ lossã€‚
5. **ç›¸æ¯” Context Parallelismï¼ŒOOMB å…·æœ‰æ›´é«˜çš„ per-device æ•ˆç‡å’Œæ›´ä½çš„éƒ¨ç½²é—¨æ§›**ï¼Œæ›´é€‚åˆèµ„æºå—é™çš„ç ”ç©¶è€…ã€‚

---

### âš ï¸ å±€é™æ€§
1. **ä¸²è¡Œ chunk å¤„ç†å¼•å…¥é¢å¤–å»¶è¿Ÿ**ï¼Œä¸é€‚åˆå¯¹ååæåº¦æ•æ„Ÿçš„å¤§è§„æ¨¡é¢„è®­ç»ƒã€‚
2. **ç¨€ç–æ³¨æ„åŠ›æ˜¯ä¸€ç§è¿‘ä¼¼æ–¹æ³•**ï¼Œå¯èƒ½å½±å“éœ€è¦å…¨å±€å¯†é›†äº¤äº’çš„ä»»åŠ¡è¡¨ç°ï¼ˆå¦‚å¤æ‚æ¨ç†ï¼‰ã€‚
3. **å¼‚æ­¥å¸è½½æ€§èƒ½ä¾èµ– CPU-GPU å¸¦å®½**ï¼Œåœ¨ PCIe 3.0 æˆ–ä½å¸¦å®½ç³»ç»Ÿä¸­æ•ˆæœä¸‹é™ã€‚
4. å½“å‰éªŒè¯é›†ä¸­åœ¨ fine-tuning åœºæ™¯ï¼Œfull pretraining çš„æ‰©å±•æ€§æœ‰å¾…éªŒè¯ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ›´æ™ºèƒ½çš„ **adaptive sparse attention ç­–ç•¥**ï¼ŒåŠ¨æ€è°ƒæ•´ retrieval budgetã€‚
- ç»“åˆ **compression æŠ€æœ¯**ï¼ˆå¦‚é‡åŒ– KV Cacheï¼‰è¿›ä¸€æ­¥å‡å°‘ä¼ è¾“é‡ã€‚
- æ‰©å±•è‡³ **multi-modal long-context training** åœºæ™¯ã€‚
- å¼€å‘ **è‡ªåŠ¨è°ƒä¼˜ç³»ç»Ÿ**ï¼Œæ ¹æ®ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ chunk sizeã€page size å’Œ sparse budgetã€‚

---

> ğŸ”— **å¼€æºä¿¡æ¯**ï¼šä½œè€…å·²å…¬å¼€æºç ï¼ˆGitHub Repositoryï¼‰ï¼Œä¾¿äºå¤ç°ä¸ç¤¾åŒºæ‹“å±•ã€‚  
> ğŸŒ± **æ„ä¹‰**ï¼šOOMB æå¤§åœ°é™ä½äº†é•¿ä¸Šä¸‹æ–‡ LLM è®­ç»ƒçš„æŠ€æœ¯ä¸ç»æµé—¨æ§›ï¼Œæ¨åŠ¨äº†â€œå¯æŒç»­ AIâ€ä¸â€œæ°‘ä¸»åŒ–ç ”ç©¶â€çš„å‘å±•ã€‚

</details>

---

### 2. [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)

**Authors**: Wei Zeng, Xuchen Li, Ruili Feng, Zhen Liu, Fengwei An, Jian Zhao  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2602.00608v1  

#### Abstract
Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolution...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šScalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

å½“å‰çš„**Generative Game Engine**ï¼ˆç”Ÿæˆå¼æ¸¸æˆå¼•æ“ï¼‰è™½ç„¶åœ¨ç†è®ºä¸Šèƒ½å¤Ÿé€šè¿‡ç¥ç»ç½‘ç»œç›´æ¥â€œç”Ÿæˆåƒç´ â€æ¥æ¨¡æ‹Ÿæ¸¸æˆä¸–ç•Œï¼Œä½†å…¶å®é™…éƒ¨ç½²å—åˆ°ä¸¥é‡çš„â€œ**Memory Wall**â€ï¼ˆå†…å­˜å¢™ï¼‰é™åˆ¶ã€‚ç°æœ‰ç³»ç»Ÿå¦‚ Diamond å’Œ GameNGen åªèƒ½åœ¨æä½åˆ†è¾¨ç‡ä¸‹è¿è¡Œï¼ˆå¦‚ 64Ã—64 æˆ– 320Ã—240ï¼‰ï¼Œéš¾ä»¥æ”¯æŒæ ‡å‡†æ¸…æ™°åº¦ï¼ˆ720Ã—480ï¼‰ä¸‹çš„å®æ—¶äº¤äº’ã€‚

æ ¹æœ¬ç“¶é¢ˆåœ¨äºï¼š
- **World Model**ï¼ˆå¦‚ DiTï¼‰æ˜¯è®¡ç®—å¯†é›†å‹ï¼ˆcompute-boundï¼‰
- **Decoder**ï¼ˆå¦‚ VAEï¼‰æ˜¯å†…å­˜å¯†é›†å‹ï¼ˆmemory-boundï¼‰
ä¸¤è€…èµ„æºéœ€æ±‚ä¸åŒ¹é…ï¼Œå¯¼è‡´ä¼ ç»ŸåŒæ„ç¡¬ä»¶æ¶æ„æ— æ³•é«˜æ•ˆæ‰©å±•ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

æœ¬æ–‡æå‡ºäº†ä¸€ç§**å¯æ‰©å±•çš„ç¡¬ä»¶-ç®—æ³•ååŒè®¾è®¡æ¡†æ¶**ï¼ˆHardware-Algorithm Co-Designï¼‰ï¼Œä»ç³»ç»Ÿçº§è§£å†³é«˜åˆ†è¾¨ç‡ç”Ÿæˆéš¾é¢˜ï¼Œä¸‰å¤§æ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### ï¼ˆ1ï¼‰å¼‚æ„è®¡ç®—æ¶æ„ä¸éå¯¹ç§°èµ„æºåˆ†é…ï¼ˆHeterogeneous Architecture & Resource Allocationï¼‰

- å°† World Modelï¼ˆDiTï¼‰å’Œ Decoderï¼ˆVAEï¼‰è§£è€¦ï¼Œåˆ†åˆ«éƒ¨ç½²äºä¸åŒçš„ AI åŠ é€Ÿå™¨ä¸Šã€‚
- å¯¹ DiT ä½¿ç”¨ **Sequence Parallelism**ï¼ˆUlyssesï¼‰ï¼Œå¯¹ VAE ä½¿ç”¨ **Spatial Parallelism**ï¼Œä»¥é€‚é…å„è‡ªè®¡ç®—ç‰¹æ€§ã€‚
- å»ºç«‹ç†è®ºæ¨¡å‹ä¼˜åŒ–è®¾å¤‡åˆ†é…æ¯”ä¾‹ï¼Œåœ¨ 8 å¡é›†ç¾¤ä¸­å¾—å‡ºæœ€ä¼˜é…ç½®ä¸º **5:3**ï¼ˆ5 å¼ ç”¨äº DiTï¼Œ3 å¼ ç”¨äº VAEï¼‰ï¼Œå®ç°ååæœ€å¤§åŒ–ã€‚

> âœ… ä¼˜åŠ¿ï¼šé¿å…å•ä¸€ç»„ä»¶æˆä¸ºç“¶é¢ˆï¼Œæ˜¾è‘—æå‡æ•´ä½“ pipeline æ•ˆç‡ã€‚

#### ï¼ˆ2ï¼‰é¢å‘å†…å­˜çš„ç®—å­èåˆä¼˜åŒ–ï¼ˆMemory-Centric Operator Fusionï¼‰

- é’ˆå¯¹ VAE è§£ç é˜¶æ®µçš„â€œHBM Ping-Pongâ€é—®é¢˜ï¼Œæå‡º **Vertical Fusion**ï¼šå°† Upsample â†’ Conv2d â†’ GroupNorm â†’ SiLU èåˆä¸ºå•ä¸ª kernelï¼Œåˆ©ç”¨ç‰‡ä¸Š SRAM ç¼“å­˜ä¸­é—´ç»“æœï¼Œå‡å°‘ 75% çš„ off-chip å†…å­˜è®¿é—®ã€‚
- é’ˆå¯¹ DiT ä¸­ AdaLN å±‚çš„å°çŸ©é˜µä¹˜æ³•å¼€é”€ï¼Œæå‡º **Horizontal Fusion**ï¼šåˆå¹¶å¤šä¸ªå°æƒé‡çŸ©é˜µè¿›è¡Œä¸€æ¬¡æ€§å¤§çŸ©é˜µè¿ç®—ï¼Œæå‡è®¡ç®—å¯†åº¦ï¼ˆarithmetic intensityï¼‰è‡³ >85% å³°å€¼æ€§èƒ½ã€‚

> âœ… ä¼˜åŠ¿ï¼šæ‰“ç ´â€œå†…å­˜å¢™â€ï¼Œå¤§å¹…é™ä½å»¶è¿Ÿå¹¶æé«˜å¸¦å®½åˆ©ç”¨ç‡ã€‚

#### ï¼ˆ3ï¼‰æµå½¢æ„ŸçŸ¥æ½œå˜é‡å¤–æ¨æœºåˆ¶ï¼ˆManifold-Aware Latent Extrapolationï¼‰

- åˆ©ç”¨è¿ç»­å¸§ä¹‹é—´çš„é«˜æ—¶é—´ç›¸å…³æ€§ï¼Œåœ¨åŠ¨ä½œç¨³å®šæ—¶è·³è¿‡éƒ¨åˆ† DiT æ¨ç†æ­¥éª¤ã€‚
- åŸºäºæ½œç©ºé—´ä¸­çš„çº¿æ€§è¿åŠ¨å‡è®¾ $ z_{t+Î”t} â‰ˆ z_t + Î”t \cdot v_t $ è¿›è¡Œé¢„æµ‹ï¼Œä»…å½“è¾“å…¥å˜åŒ–è¶…è¿‡é˜ˆå€¼æ—¶æ‰è§¦å‘å®Œæ•´æ¨ç†ã€‚
- ç»“åˆ **Speculative Action Prefetching**ï¼ˆåŸºäºè½»é‡ LSTM é¢„æµ‹ç”¨æˆ·è¡Œä¸ºï¼‰ï¼Œè¿›ä¸€æ­¥æ©ç›– I/O å»¶è¿Ÿã€‚

> âœ… ä¼˜åŠ¿ï¼šæœ‰æ•ˆè§£è€¦æ„ŸçŸ¥å“åº”é€Ÿåº¦ä¸å®é™…ç”Ÿæˆé¢‘ç‡ï¼Œå®ç°è¶…ä½æ„ŸçŸ¥å»¶è¿Ÿã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | æœ¬å·¥ä½œ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ GameNGen, Diamondï¼‰ |
|------|--------|-------------------------------|
| åˆ†è¾¨ç‡ | æ”¯æŒ **720Ã—480**ï¼ˆSDï¼‰ | æœ€é«˜ä»…æ”¯æŒ 320Ã—240 æˆ–æ›´ä½ |
| å®æ—¶æ€§ | è¾¾åˆ° **26.4 FPS**ï¼ˆè¿ç»­åŸŸï¼‰å’Œ **48.3 FPS**ï¼ˆç¦»æ•£åŸŸï¼‰ | å¤šæ•°ä½äº 20 FPS |
| æ¶æ„çµæ´»æ€§ | å¼‚æ„å¹¶è¡Œ + æ˜¾å¼å†…å­˜ç®¡ç† | åŒæ„ GPU å¹³å°ï¼Œä¾èµ–éšå¼ç¼“å­˜ |
| å»¶è¿Ÿæ§åˆ¶ | æ‘Šé”€ **Motion-to-Photon Latency = 2.7ms** | é€šå¸¸ >50ms |
| æ‰©å±•èƒ½åŠ› | å¯æ‰©å±•è‡³å•†ç”¨åŠ é€Ÿå™¨é›†ç¾¤ | ä¾èµ–ä¸“ç”¨ç¡¬ä»¶ï¼ˆå¦‚ TPUï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ§ª æ•°æ®é›†ä¸æµ‹è¯•ç¯å¢ƒ

ä½¿ç”¨ä¸¤ä¸ªä»£è¡¨æ€§åŸºå‡†æ„æˆâ€œè¿ç»­-ç¦»æ•£å¯¹å¶æµ‹è¯•åºŠâ€ï¼ˆContinuous-Discrete Duality Benchmarkï¼‰ï¼š

| ç±»å‹ | åç§° | æè¿° |
|------|------|------|
| **è¿ç»­åŸŸ** | **The Matrix [9]** | é«˜ä¿çœŸ 3D èµ›è½¦æ¨¡æ‹Ÿå™¨ï¼ˆ720Ã—480ï¼‰ï¼Œå¼ºè°ƒç‰©ç†åŠ¨åŠ›å­¦ã€æ‘©æ“¦åŠ›ã€åŠ¨é‡ç­‰è¿ç»­åŠ¨æ€ |
| **ç¦»æ•£åŸŸ** | **Playable Game Generation (PGG) [10]** | 2D å¹³å°è·³è·ƒæ¸¸æˆï¼ˆ256Ã—256ï¼‰ï¼Œå¼ºè°ƒå¸ƒå°”é€»è¾‘ã€ç¢°æ’æ£€æµ‹ã€çŠ¶æ€è·ƒè¿ |

> âš ï¸ æ³¨ï¼šä¸¤ä¸ªä»»åŠ¡å‡è¦æ±‚æ¨¡å‹åœ¨æ— æ˜¾å¼è§„åˆ™å¼•æ“çš„æƒ…å†µä¸‹ç»´æŒé•¿æœŸä¸€è‡´æ€§ã€‚

---

### ğŸ›  å®éªŒè®¾ç½®

- **ç¡¬ä»¶å¹³å°**ï¼š8Ã— Huawei Ascend 910C NPUï¼ˆæ¯å¡ 64GB HBMï¼ŒFP16 ç®—åŠ› ~752 TFLOPSï¼‰ï¼Œé€šè¿‡ HCCS é«˜é€Ÿç¯å½¢äº’è¿ï¼ˆ30 GB/sï¼‰
- **è½¯ä»¶æ ˆ**ï¼šCANN 8.0, PyTorch 2.5.1, xfuserï¼ˆæ”¯æŒ Ulysses å¹¶è¡Œï¼‰
- **æ¨¡å‹ç»“æ„**ï¼š
  - World Modelï¼šDiffusion Transformer (DiT)
  - Decoderï¼šVariational Autoencoder (VAE)

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | è¯´æ˜ |
|---------|--------|------|
| **æ€§èƒ½** | FPS, Motion-to-Photon Latency (M2P) | è¡¡é‡å®æ—¶æ€§å’Œå“åº”é€Ÿåº¦ |
| **è§†è§‰è´¨é‡** | FID â†“, PSNR â†‘, SSIM â†‘, LPIPS â†“ | è¡¡é‡ç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿä¸ç»†èŠ‚ä¿ç•™ |
| **ç‰©ç†/é€»è¾‘ä¸€è‡´æ€§** | Control Sensitivity Analysis (CSA), Discrete Logic Boundary (DLB) Score | æ£€æŸ¥æ˜¯å¦ç¬¦åˆç‰©ç†è§„å¾‹æˆ–æ¸¸æˆé€»è¾‘ |
| **æ•ˆç‡** | Normalized Efficiency (FPS / 100 TFLOPS) | æ¶ˆé™¤ç¡¬ä»¶å·®å¼‚åçš„ç›¸å¯¹æ•ˆç‡æ¯”è¾ƒ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿ç³»ç»Ÿ | ç¡¬ä»¶ | åˆ†è¾¨ç‡ | FPS | Latency |
|--------|------|-------|-----|--------|
| **Diamond [6]** | RTX 3090 | 64Ã—64 | 10.0 | 100 ms |
| **GameNGen [4]** | TPU v5 | 320Ã—240 | >20 | ~50 ms |
| **PGG Baseline*** | RTX 5090 | 256Ã—256 | 29.9 | 33.5 ms |

> âœ… æœ¬æ–‡æ–¹æ³•åœ¨æ›´é«˜åˆ†è¾¨ç‡ä¸‹ä»å®ç°æ›´ä¼˜æ€§èƒ½ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

| æŒ‡æ ‡ | è¿ç»­åŸŸï¼ˆMatrixï¼‰ | ç¦»æ•£åŸŸï¼ˆPGGï¼‰ |
|------|------------------|--------------|
| **åˆ†è¾¨ç‡** | 720Ã—480 | 256Ã—256 |
| **FPS** | **26.4** | **48.3** |
| **æ‘Šé”€ M2P å»¶è¿Ÿ** | **2.7 ms** | **2.7 ms** |
| **FID** | 42.3 | **28.5** |
| **LPIPS** | 0.087 | **0.052** |
| **DLB Scoreï¼ˆé€»è¾‘æ­£ç¡®ç‡ï¼‰** | â€” | **100.0%**ï¼ˆè®­ç»ƒåˆ†å¸ƒå†…ï¼‰ |

> ğŸ’¡ åœ¨è¿ç»­åŸŸä¸­ï¼ŒYaw Rate æ§åˆ¶æ•æ„Ÿæ€§åˆ†ææ˜¾ç¤ºæ¨¡å‹å…·å¤‡â€œ**æ¶Œç°æƒ¯æ€§**â€â€”â€”è½¬å‘åè§’é€Ÿåº¦å‘ˆæŒ‡æ•°è¡°å‡ï¼Œè¡¨æ˜å…¶å·²å­¦ä¹ åˆ°åŸºæœ¬ç‰©ç†åŠ¨é‡ã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

| é˜¶æ®µ | FPS | Speedup |
|------|-----|--------|
| Baselineï¼ˆå•å¡é¡ºåºæ‰§è¡Œï¼‰ | 2.1 | 1.0x |
| + Operator Fusion | 4.5 | 2.1x |
| + Ulysses (3:5) | 16.6 | 7.9x |
| + Optimal 5:3 Partition | 19.4 | 9.2x |
| + Latent Extrapolation | 26.4 | **12.6x** |
| + Speculative Prefetching (93% hit) | 26.4 | **æœ‰æ•ˆå»¶è¿Ÿé™è‡³ 2.7ms** |

> âœ… æ€»ä½“æ€§èƒ½æå‡è¾¾ **12.6 å€**ï¼Œå…¶ä¸­ï¼š
- ç®—å­èåˆå¸¦æ¥ 2.1x åŠ é€Ÿ
- å¼‚æ„å¹¶è¡Œ + æœ€ä¼˜èµ„æºé…ç½®è´¡çŒ®é¢å¤– 4.4x
- ç®—æ³•çº§å¤–æ¨å†æé€Ÿ 1.35x

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **â€œMemory Wallâ€å¯é€šè¿‡ç¡¬ä»¶-ç®—æ³•ååŒè®¾è®¡çªç ´**  
   å•çº¯è½¯ä»¶ä¼˜åŒ–æ— æ³•è§£å†³é«˜åˆ†è¾¨ç‡ä¸‹çš„å¸¦å®½ç“¶é¢ˆï¼›å¿…é¡»ç»“åˆæ˜¾å¼å†…å­˜ç®¡ç†ï¼ˆSRAMï¼‰ã€å¼‚æ„å¹¶è¡Œå’Œå®šåˆ¶åŒ– kernel è®¾è®¡æ‰èƒ½å®ç°å¯æ‰©å±•æ€§ã€‚

2. **å¼‚æ„æ¶æ„ä¼˜äºåŒæ„æ–¹æ¡ˆ**  
   å°† compute-bound ä¸ memory-bound æ¨¡å—åˆ†ç¦»ï¼Œå¹¶é’ˆå¯¹æ€§åœ°åº”ç”¨ä¸åŒå¹¶è¡Œç­–ç•¥ï¼ˆSequence vs Spatialï¼‰ï¼Œå¯æœ€å¤§åŒ–é›†ç¾¤åˆ©ç”¨ç‡ã€‚

3. **é«˜ä¿çœŸä¸ä½å»¶è¿Ÿå¯ä»¥å…¼å¾—**  
   é€šè¿‡ **Manifold-Aware Latent Extrapolation** å’Œ **Speculative Prefetching**ï¼Œç³»ç»Ÿå®ç°äº† **26.4 FPS @ 720Ã—480** ä¸”æ‘Šé”€å»¶è¿Ÿä»… **2.7ms**ï¼Œæ»¡è¶³äººç±»æ„ŸçŸ¥æµç•…äº¤äº’çš„éœ€æ±‚ã€‚

4. **ç¥ç»ä¸–ç•Œæ¨¡å‹èƒ½è‡ªå‘å­¦ä¹ ç‰©ç†è§„å¾‹**  
   åœ¨æ— æ˜¾å¼ç‰©ç†æ±‚è§£å™¨çš„æƒ…å†µä¸‹ï¼ŒDiT æˆåŠŸå»ºæ¨¡äº†è½¦è¾†è½¬å‘æ—¶çš„æƒ¯æ€§æ•ˆåº”ï¼ŒéªŒè¯äº†â€œç¥ç»ç‰©ç†â€çš„å¯è¡Œæ€§ã€‚

5. **é€»è¾‘ä¸€è‡´æ€§å¯åœ¨è®­ç»ƒåˆ†å¸ƒå†…ä¸¥æ ¼ä¿è¯**  
   PGG æµ‹è¯•ä¸­è¾¾åˆ° **100% DLB æ­£ç¡®ç‡**ï¼Œè¯æ˜è¯¥æ¶æ„æœ‰èƒ½åŠ›ç»´æŠ¤å¤æ‚çš„æ¸¸æˆé€»è¾‘è¾¹ç•Œã€‚

---

### âš ï¸ å±€é™æ€§

1. **ä¾èµ–å¤§è§„æ¨¡åŠ é€Ÿå™¨é›†ç¾¤**  
   å½“å‰æ–¹æ¡ˆéœ€ 8 å¼ é«˜ç«¯ NPU æ‰èƒ½è¿è¡Œï¼Œé™åˆ¶äº†è¾¹ç¼˜è®¾å¤‡æˆ–æœ¬åœ°éƒ¨ç½²çš„å¯èƒ½æ€§ã€‚

2. **Out-of-Distributionï¼ˆOODï¼‰è¡Œä¸ºå¤„ç†ä¸è¶³**  
   è™½ç„¶è®­ç»ƒåˆ†å¸ƒå†…è¡¨ç°å®Œç¾ï¼Œä½†åœ¨é¢å¯¹æœªè§è¿‡çš„æ“ä½œï¼ˆå¦‚ç©¿å¢™æŒ‡ä»¤ï¼‰æ—¶å¯èƒ½å‡ºç°â€œå¹»è§‰â€ï¼ˆhallucinationï¼‰ï¼Œç¼ºä¹ç¡¬æ€§è§„åˆ™çº¦æŸã€‚

3. **èµ„æºé…ç½®ä¾èµ–æ¨¡å‹è¶…å‚**  
   æœ€ä¼˜çš„ 5:3 åˆ†é…åŸºäºç‰¹å®š attention head æ•°é‡ï¼ˆH=30ï¼‰ï¼Œæ¢æ¨¡å‹éœ€é‡æ–°è°ƒä¼˜ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **æ„å»º Hybrid Neuro-Symbolic Engine**  
   å¼•å…¥è½»é‡çº§ç¬¦å·é€»è¾‘å±‚ä½œä¸ºâ€œæ¸¸æˆè§„åˆ™ç›‘ç£è€…â€ï¼Œç¡®ä¿ OOD åœºæ™¯ä¸‹çš„å®‰å…¨æ€§ä¸åˆæ³•æ€§ã€‚

2. **å¤šæ¨¡æ€æ§åˆ¶æ¥å£æ¢ç´¢**  
   åˆ©ç”¨ Transformer çš„ cross-attention èƒ½åŠ›ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€ã€è¯­éŸ³ç­‰æ–°å‹è¾“å…¥æ–¹å¼ï¼Œæå‡äº¤äº’æ²‰æµ¸æ„Ÿã€‚

3. **è·¨å¹³å°é€šç”¨åŒ–ä¸ç¼–è¯‘å™¨æ”¯æŒ**  
   å°†å½“å‰ä¼˜åŒ–è¿ç§»è‡³ OpenAI Triton ç­‰è·¨å‚å•†ç¼–è¯‘å™¨ï¼Œå®ç°åœ¨ NVIDIA H100/NVLink ç­‰å¹³å°ä¸Šçš„éƒ¨ç½²ã€‚

4. **ç«¯ä¾§é‡åŒ–ä¸å‹ç¼©æŠ€æœ¯ç ”ç©¶**  
   æ¢ç´¢ 4-bit æƒé‡é‡åŒ– + æ¿€æ´»å¹³æ»‘ç­‰æŠ€æœ¯ï¼Œæ¨åŠ¨é«˜ä¿çœŸç¥ç»æ¸¸æˆå¼•æ“å‘æ¶ˆè´¹çº§ PC å’Œç§»åŠ¨è®¾å¤‡ä¸‹æ²‰ã€‚

---

## âœ… æ€»ç»“

æœ¬æ–‡é¦–æ¬¡å®ç°äº†åœ¨ **720Ã—480 åˆ†è¾¨ç‡ä¸‹å®æ—¶è¿è¡Œçš„ç”Ÿæˆå¼æ¸¸æˆå¼•æ“**ï¼Œé€šè¿‡ **Hardware-Algorithm Co-Design** æˆåŠŸæ‰“ç ´äº†é•¿æœŸåˆ¶çº¦è¯¥é¢†åŸŸçš„â€œMemory Wallâ€ã€‚å…¶ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯â€”â€”**å¼‚æ„æ¶æ„è®¾è®¡ã€å†…å­˜ä¸­å¿ƒåŒ–ç®—å­èåˆã€æµå½¢æ„ŸçŸ¥æ½œå˜é‡å¤–æ¨**â€”â€”å…±åŒæ”¯æ’‘èµ·ä¸€ä¸ªå…¼å…·é«˜ä¿çœŸã€ä½å»¶è¿Ÿã€å¼ºä¸€è‡´æ€§çš„ç¥ç»ä»¿çœŸç³»ç»Ÿã€‚

> ğŸ”¥ **è¿™ä¸ä»…æ˜¯æ€§èƒ½çš„é£è·ƒï¼Œæ›´æ˜¯èŒƒå¼çš„è½¬å˜ï¼šä»â€œæ¸²æŸ“å‡ ä½•â€èµ°å‘â€œç”Ÿæˆä¸–ç•Œâ€**ã€‚  
> æœªæ¥çš„æ¸¸æˆæˆ–å°†ä¸å†ç”±ç¨‹åºå‘˜ç¼–å†™ä»£ç æ„å»ºï¼Œè€Œæ˜¯ç”±ç¥ç»ç½‘ç»œâ€œæ¢¦è§â€è€Œæˆã€‚

</details>

---

### 3. [Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing](https://arxiv.org/abs/2602.02159)

**Authors**: Lingkun Long, Yushi Huang, Shihao Bai, Ruihao Gong, Jun Zhang, Ao Zhou, Jianlei Yang  
**Category**: cs.CL  
**Published**: 2026-02-03  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2602.02159v1  

#### Abstract
Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šFocus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
Diffusion Large Language Models (dLLMs) è™½ç„¶åœ¨éè‡ªå›å½’è§£ç èŒƒå¼ä¸‹å±•ç°å‡ºå¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œä½†ç”±äºå…¶**åŒå‘å…¨æ³¨æ„åŠ›æœºåˆ¶**ï¼ˆbidirectional full attentionï¼‰ï¼Œæ¨ç†è¿‡ç¨‹è®¡ç®—å¼€é”€å·¨å¤§ï¼Œå°¤å…¶åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹æ•ˆç‡ä¸¥é‡å—é™ã€‚

ç°æœ‰åŠ é€Ÿæ–¹æ³•å­˜åœ¨ä»¥ä¸‹ä¸è¶³ï¼š
- **Approximated KV cache**ï¼šè™½èƒ½å¤ç”¨ç¼“å­˜çŠ¶æ€ï¼Œä½†ä»éœ€å¯¹å®Œæ•´ä¸Šä¸‹æ–‡æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚
- **Sparse attention**ï¼šä¾èµ–å½“å‰å·²è§£ç  token ä½œä¸º query æ¥ä¼°è®¡é‡è¦æ€§ï¼Œä½†åœ¨ dLLM ä¸­ï¼Œå¾…è§£ç ä½ç½®ï¼ˆå³ unmasked ä½ç½®ï¼‰æ˜¯åŠ¨æ€ä¸”æœªçŸ¥çš„ï¼Œå¯¼è‡´ç¨€ç–ç­–ç•¥ä¸å‡†ç¡®ã€æ•ˆæœä¸ä½³ã€‚

å› æ­¤ï¼Œå¦‚ä½•**å‡†ç¡®é¢„æµ‹å³å°†è¢«è§£ç çš„ä½ç½®å¹¶æ®æ­¤é«˜æ•ˆå‰ªæå†—ä½™æ³¨æ„åŠ›è®¡ç®—**ï¼Œæˆä¸ºæå‡ dLLM æ¨ç†æ•ˆç‡çš„å…³é”®æŒ‘æˆ˜ã€‚

---

### **æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯**
æœ¬æ–‡æå‡ºäº† **Focus-dLLM** â€”â€” ä¸€ç§æ— éœ€è®­ç»ƒçš„ï¼ˆtraining-freeï¼‰ã€åŸºäºç½®ä¿¡åº¦å¼•å¯¼çš„ä¸Šä¸‹æ–‡èšç„¦æ¡†æ¶ï¼Œç”¨äºåŠ é€Ÿé•¿ä¸Šä¸‹æ–‡ dLLM æ¨ç†ã€‚

#### **æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š**

1. âœ… **Past Confidence-Guided Indicatorï¼ˆè¿‡å»ç½®ä¿¡åº¦å¼•å¯¼æŒ‡ç¤ºå™¨ï¼‰**
   - å‘ç°ï¼šç›¸é‚»å»å™ªæ­¥éª¤ä¸­ï¼Œtoken çš„ confidence score å…·æœ‰å¼ºæ­£ç›¸å…³æ€§ã€‚
   - æ€è·¯ï¼šåˆ©ç”¨å‰ä¸€æ­¥éª¤ä¸­é«˜ç½®ä¿¡åº¦çš„ masked token ä½ç½®ï¼Œæ¥é¢„æµ‹å½“å‰æ­¥éª¤å°†è¢« unmask çš„ä½ç½®ã€‚
   - å®ç°ï¼šé€‰å–ä¸Šä¸€æ­¥ top-k é«˜ç½®ä¿¡åº¦ token æ„æˆ `Lfocus`ï¼Œå¹¶é€šè¿‡å±€éƒ¨çª—å£æ‰©å±•å½¢æˆæ´»è·ƒæŸ¥è¯¢é›† `Lactive`ï¼Œä»è€Œç¼©å°æ³¨æ„åŠ›è®¡ç®—èŒƒå›´ã€‚

2. âœ… **Sink-Aware Pruning Strategyï¼ˆå…³æ³¨æ³¨æ„åŠ› sink çš„å‰ªæç­–ç•¥ï¼‰**
   - å‘ç°ï¼šdLLM ä¸­å­˜åœ¨æ˜¾è‘—çš„ **attention sink**ï¼ˆå¯¹è¯­ä¹‰è¿è´¯æ€§è‡³å…³é‡è¦çš„ tokenï¼‰ï¼Œä¸”è¿™äº› sink åœ¨ä¸åŒå±‚ä¹‹é—´å…·æœ‰é«˜åº¦ä¸€è‡´æ€§ï¼ˆcross-layer consistencyï¼‰ã€‚
   - æ€è·¯ï¼š
     - åœ¨æµ…å±‚ï¼ˆdense layersï¼‰è¯†åˆ« attention sinkï¼Œå¹¶åœ¨æ•´ä¸ªæ·±å±‚æ¨ç†è¿‡ç¨‹ä¸­å¤ç”¨å…¶ä½ç½®ã€‚
     - ç»“åˆ block-wise pruningï¼Œä»…ä¿ç•™æœ€ç›¸å…³çš„ prompt å—å’Œæ‰€æœ‰å“åº” tokenï¼Œå®ç°é«˜æ•ˆçš„ç¨€ç–æ³¨æ„åŠ›ã€‚

3. âœ… **è·¨å±‚å…±äº« sink ä»¥é™ä½å¼€é”€**
   - åˆ©ç”¨ sink çš„è·¨å±‚ç¨³å®šæ€§ï¼Œåœ¨ä¸­é—´å±‚è¯†åˆ«ä¸€æ¬¡åå³å¯å¤ç”¨äºåç»­å„å±‚ï¼Œé¿å…é‡å¤æ£€æµ‹å¸¦æ¥çš„é¢å¤–è®¡ç®—ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç‰¹æ€§ | Focus-dLLM | Fast-dLLM | Sparse-dLLM | SparseD |
|------|------------|-----------|-------------|---------|
| æ˜¯å¦éœ€è¦è®­ç»ƒ | âŒ No | âŒ No | âŒ No | âŒ No |
| èƒ½å¦ç²¾å‡†å®šä½ unmasked ä½ç½® | âœ… æ˜¯ï¼ˆé€šè¿‡ past confidenceï¼‰ | âŒ å¦ | âŒ ç²—ç²’åº¦å—çº§ä¼°è®¡ | âŒ å›ºå®šæ¨¡å¼é‡ç”¨ |
| æ˜¯å¦ä¿ç•™ attention sink | âœ… æ˜¾å¼è¯†åˆ«å¹¶ä¿ç•™ | âš ï¸ æœªè€ƒè™‘ | âŒ å¿½ç•¥ | âš ï¸ ä¸æ˜¾å¼å»ºæ¨¡ |
| æ˜¯å¦æ”¯æŒåŠ¨æ€ç¨€ç– | âœ… åŠ¨æ€æ¯æ­¥æ›´æ–° | âš ï¸ ç¼“å­˜å¤ç”¨ | âœ… åŠ¨æ€ç¼“å­˜é©±é€ | âŒ æ—©æœŸå›ºå®šæ¨¡å¼ |
| æ•ˆç‡å¢ç›Š | **æœ€é«˜è¾¾ 29.6Ã—** | ~9â€“12Ã— | ~12Ã— | ~1.5Ã— |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šFocus-dLLM å®ç°äº†æ›´ç²¾ç¡®çš„ query å®šä½ + æ›´æ™ºèƒ½çš„å†å² token ä¿ç•™æœºåˆ¶ï¼Œåœ¨å‡ ä¹æ— æŸç”šè‡³æå‡æ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç°äº†è¿œè¶…ç°æœ‰æ–¹æ³•çš„æ¨ç†åŠ é€Ÿã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ¨¡å‹**
- **UltraLLaDA** (He et al., 2025)ï¼šä¸“æ³¨äºé•¿ä¸Šä¸‹æ–‡æ‰©å±•çš„ dLLMã€‚
- **Dream-7B-Instruct** (Ye et al., 2025)ï¼šä»è‡ªå›å½’æ¨¡å‹è¿ç§»è€Œæ¥çš„ dLLMã€‚

### **æ•°æ®é›†ä¸åŸºå‡†æµ‹è¯•**
- ä¸»è¦è¯„æµ‹åŸºå‡†ï¼š**LongBench** (Bai et al., 2024)ï¼Œæ¶µç›–å¤šä»»åŠ¡é•¿æ–‡æœ¬ç†è§£ï¼š
  - å•æ–‡æ¡£é—®ç­”ï¼ˆQasper, HotpotQAï¼‰
  - å¤šæ–‡æ¡£é—®ç­”ï¼ˆMultiFieldQA-en, 2WikiMQAï¼‰
  - æ‘˜è¦ç”Ÿæˆï¼ˆGovReport, QMSumï¼‰
  - å°‘æ ·æœ¬å­¦ä¹ ï¼ˆTREC, TriviaQAï¼‰
  - åˆæˆä»»åŠ¡ï¼ˆPassageRetrievalï¼‰
  - ä»£ç è¡¥å…¨ï¼ˆLCC, RepoBench-Pï¼‰

è¯¦ç»†é…ç½®è§é™„å½• Table 4ã€‚

### **è¯„ä¼°æŒ‡æ ‡**
| ç±»å‹ | æŒ‡æ ‡ |
|------|------|
| **å‡†ç¡®æ€§** | F1, Rouge-L, Accuracy, Edit Sim ç­‰ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡ï¼›LongBench å¹³å‡å¾—åˆ†ï¼ˆAve. Scoreï¼‰ |
| **æ•ˆç‡** | è§£ç ååé‡ï¼ˆThroughput, tokens/sï¼‰<br>ç›¸å¯¹ Vanilla çš„ speedup ratio |
| **ç»¼åˆè¡¨ç°** | å‡†ç¡®æ€§ vs. ååé‡çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼ˆPareto frontierï¼‰ |

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| æ–¹æ³• | ç±»å‹ | å…³é”®æŠ€æœ¯ |
|------|------|----------|
| **Vanilla** | åŸå§‹æ¨ç† | æ— ä»»ä½•ä¼˜åŒ–ï¼Œå…¨æ³¨æ„åŠ›è®¡ç®— |
| **Fast-dLLM** | KV Cache | å—çº§è¿‘ä¼¼ KV ç¼“å­˜å¤ç”¨ |
| **Sparse-dLLM** | Sparse Attention | åŠ¨æ€ç¼“å­˜é©±é€ + å—çº§ç¨€ç– |
| **SparseD** | Sparse Attention | é¢„è®¡ç®—ç¨€ç–æ¨¡å¼å¹¶è·¨æ­¥é‡ç”¨ |

æ‰€æœ‰æ–¹æ³•ç»Ÿä¸€é‡‡ç”¨ semi-autoregressive remasking ç­–ç•¥ï¼Œblock size = 32ã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### ğŸ”¹ **åœ¨ 32K ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹çš„æ¨ç†é€Ÿåº¦æå‡**
- **Focus-dLLM è¾¾åˆ°è¶…è¿‡ 29Ã— çš„ lossless åŠ é€Ÿ**ï¼ˆç›¸å¯¹äº Vanillaï¼‰ã€‚
- åœ¨ UltraLLaDA ä¸Šï¼Œ**æ¯” Fast-dLLM å¿« 2.05Ã—**ï¼ˆå›¾5å·¦ï¼‰ã€‚
- éšç€ä¸Šä¸‹æ–‡å¢é•¿ï¼ŒåŠ é€Ÿæ¯”æŒç»­æ‰©å¤§ï¼š
  - 8K â†’ 9.4Ã— speedup
  - 32K â†’ **29.6Ã— speedup**

> ğŸ’¡ åŸå› ï¼šè¶Šé•¿åºåˆ—ä¸­å†—ä½™æ³¨æ„åŠ›è¶Šå¤šï¼ŒFocus-dLLM çš„å‰ªææ”¶ç›Šè¶Šå¤§ã€‚

---

#### ğŸ”¹ **å‡†ç¡®ç‡è¡¨ç°ï¼ˆLongBench å¹³å‡å¾—åˆ†ï¼‰**

| æ–¹æ³• | UltraLLaDA | Dream-7B-Instruct |
|------|------------|-------------------|
| Vanilla | 44.90 | 43.03 |
| Fast-dLLM | 44.74 | 42.75 |
| Sparse-dLLM | 44.86 | 42.78 |
| SparseD | 44.70 | **43.59** |
| **Focus-dLLM** | **45.14** âœ… | 42.82 |

- åœ¨ **UltraLLaDA ä¸Šè¶…è¶Šæ‰€æœ‰åŸºçº¿**ï¼ŒåŒ…æ‹¬ Vanillaï¼Œè¯´æ˜ä¸ä»…æ²¡æŸå¤±ç²¾åº¦ï¼Œåè€Œç•¥æœ‰æå‡ã€‚
- åœ¨ Dream-7B ä¸Šç•¥ä½äº SparseDï¼Œä½†**æ¢æ¥äº†é«˜è¾¾ 19.95Ã— çš„åŠ é€Ÿ**ï¼Œæ€§ä»·æ¯”æ›´é«˜ã€‚

---

#### ğŸ”¹ **Niah å®éªŒï¼ˆneedle-in-a-haystack æ£€ç´¢èƒ½åŠ›ï¼‰**
- åœ¨ UltraLLaDA ä¸Šè¿›è¡Œæœ€å¤§ 32K ä¸Šä¸‹æ–‡çš„æ£€ç´¢ä»»åŠ¡ã€‚
- **Focus-dLLM åœ¨æ·±å±‚å–å¾—é«˜äº Vanilla çš„å‡†ç¡®ç‡**ï¼Œè¡¨æ˜å…¶æœ‰æ•ˆä¿ç•™äº†å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
- æ˜¾è‘—ä¼˜äº Fast-dLLM å’Œ Sparse-dLLMã€‚

---

#### ğŸ”¹ **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰**

##### è¡¨2ï¼šç»„ä»¶æ¶ˆèï¼ˆUltraLLaDA, 16K contextï¼‰

| æ–¹æ³• | Avg. Score | Throughput (tokens/s) |
|------|------------|------------------------|
| Fast-dLLM | 44.74 | 11.03 |
| + PCGIï¼ˆç½®ä¿¡åº¦å¼•å¯¼ï¼‰ | 44.23 | 11.37 |
| + Sink-Aware Sparse Attn | 44.84 | **17.68** |
| **Focus-dLLMï¼ˆå®Œæ•´ï¼‰** | **45.14** | **17.71** |

- **PCGI å•ç‹¬ä½¿ç”¨è½»å¾®é™å‡†ä½†æé€Ÿæœ‰é™**ï¼šè¯´æ˜ä»…ç­›é€‰ query ä¸è¶³ä»¥å¤§å¹…æå‡æ•ˆç‡ã€‚
- **Sink-aware ç¨€ç–æ³¨æ„åŠ›å¸¦æ¥æ˜¾è‘—åŠ é€Ÿä¸æå‡†**ï¼šè¯æ˜ä¿ç•™å…³é”®å†å² token å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚
- **ä¸¤è€…ç»“åˆå®ç°ååŒå¢ç›Š**ï¼šç²¾å‡† query + ç²¾ç»† KV å‰ªæ = æœ€ä½³å¹³è¡¡ã€‚

##### è¡¨3ï¼šæ˜¯å¦ä¿ç•™ attention sinkï¼ˆDream-7Bï¼‰

| å­é›† | w/o sink | w/ sink | Î” |
|------|----------|---------|----|
| hotpotqa | 37.17 | 38.96 | +1.79 |
| 2wikimqa | 37.68 | 38.56 | +0.88 |
| trec | 69.50 | 70.00 | +0.50 |
| **Avg.** | **41.47** | **42.82** | **+1.35** |

âœ… æ˜ç¡®éªŒè¯ï¼š**æ˜¾å¼ä¿ç•™ attention sink å¯ç³»ç»Ÿæ€§æå‡æ€§èƒ½**ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. ğŸ§  **Token confidence åœ¨ç›¸é‚»å»å™ªæ­¥é—´é«˜åº¦ç›¸å…³** â†’ å¯ç”¨äºå¯é é¢„æµ‹ä¸‹ä¸€æ­¥å°†è¢«è§£ç çš„ä½ç½®ã€‚
2. ğŸŒ€ **Attention sink åœ¨ dLLM ä¸­æ™®éå­˜åœ¨ä¸”å…·æœ‰è·¨å±‚ä¸€è‡´æ€§** â†’ å¯åœ¨æµ…å±‚è¯†åˆ«å¹¶åœ¨æ·±å±‚å¤ç”¨ï¼Œå¤§å¹…å‡å°‘é‡å¤è®¡ç®—ã€‚
3. ğŸ¯ **ç»“åˆ past confidence å¼•å¯¼çš„ query selection ä¸ sink-aware KV pruningï¼Œå¯åœ¨å‡ ä¹æ— æŸæƒ…å†µä¸‹å®ç°æç«¯åŠ é€Ÿ**ã€‚
4. ğŸ“ˆ **åŠ é€Ÿæ¯”éšä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿è€ŒæŒ‡æ•°ä¸Šå‡**ï¼Œç‰¹åˆ«é€‚åˆè¶…é•¿æ–‡æœ¬åœºæ™¯ï¼ˆå¦‚ 32K+ï¼‰ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
1. âš ï¸ **ç›®å‰ä»…é€‚ç”¨äºçº¯æ–‡æœ¬ä»»åŠ¡**ï¼Œå°šæœªæ‹“å±•è‡³ multimodalï¼ˆå¤šæ¨¡æ€ï¼‰åœºæ™¯ã€‚
2. âš™ï¸ **è¶…å‚æ•°ä¸ºæ‰‹åŠ¨è®¾å®š**ï¼ˆå¦‚ `p`, `w`, `ldense`, `Î±`ï¼‰ï¼Œç¼ºä¹è‡ªé€‚åº”è°ƒèŠ‚æœºåˆ¶ã€‚
3. ğŸ”„ **ä¾èµ– semi-autoregressive remasking è°ƒåº¦å™¨**ï¼Œå¯èƒ½é™åˆ¶åœ¨å…¶ä»–è°ƒåº¦ç­–ç•¥ä¸‹çš„æ³›åŒ–æ€§ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. ğŸ”® å¼€å‘ **å®Œå…¨è‡ªé€‚åº”çš„å‚æ•°è°ƒæ•´æœºåˆ¶**ï¼Œæ ¹æ®ä¸åŒè¾“å…¥åŠ¨æ€ä¼˜åŒ– hyperparametersã€‚
2. ğŸŒ æ‰©å±•è‡³ **multimodal diffusion models**ï¼Œæ¢ç´¢è§†è§‰-è¯­è¨€è”åˆæ¨ç†ä¸­çš„ä¸Šä¸‹æ–‡èšç„¦ã€‚
3. ğŸ¤– æ¢ç´¢ **learnable confidence predictor** æˆ–è½»é‡å¾®è°ƒç‰ˆæœ¬ï¼Œè¿›ä¸€æ­¥æå‡é¢„æµ‹ç²¾åº¦ã€‚
4. ğŸ§© å°†æœ¬æ–¹æ³•çš„æ€æƒ³è¿ç§»åˆ°å…¶ä»–éè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ insertion-based modelsï¼‰ä¸­ã€‚

---

> âœ… **æ€»ä½“è¯„ä»·**ï¼š  
> Focus-dLLM æ˜¯é¦–ä¸ªæˆåŠŸå°† **confidence dynamics** ä¸ **attention sink consistency** ç»“åˆç”¨äº dLLM æ¨ç†åŠ é€Ÿçš„å·¥ä½œã€‚å®ƒä»¥æç®€è®¾è®¡å®ç°äº†å‰æ‰€æœªæœ‰çš„æ•ˆç‡çªç ´ï¼ˆ>29Ã— speedupï¼‰ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡ç”Ÿæˆè´¨é‡ï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡ dLLM çš„å®é™…éƒ¨ç½²æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚

ğŸ”— **ä»£ç å¼€æºåœ°å€**ï¼š[https://github.com/Longxmas/Focus-dLLM](https://github.com/Longxmas/Focus-dLLM)

</details>

---

### 4. [STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs](https://arxiv.org/abs/2602.02180)

**Authors**: Weikang Meng, Liangyu Huo, Yadan Luo, Jiawen Guan, Jingyi Zhang, Yingjian Li, Zheng Zhang  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2602.02180v1  

#### Abstract
Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fa...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰å¯¹é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œçº¿æ€§åŒ–ï¼ˆlinearizationï¼‰ä»¥é™ä½è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ $O(N^2)$ å¤æ‚åº¦æ—¶ï¼Œå­˜åœ¨ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼š
1. **Token è·¯ç”±ç­–ç•¥ç²—ç³™**ï¼šç°æœ‰ **intra-layer hybrid attention** æ–¹æ³•ï¼ˆå¦‚ LoLCATsã€Ligerï¼‰ä¾èµ–æ»‘åŠ¨çª—å£ï¼ˆsliding windowï¼‰è¿›è¡Œä½ç½®å›ºå®šçš„ token åˆ†æµï¼Œå°†å±€éƒ¨ token é€å…¥ Softmax Attentionï¼ˆSAï¼‰ï¼Œå…¶ä½™é€å…¥ Linear Attentionï¼ˆLAï¼‰ã€‚è¿™ç§**åŸºäºä½ç½®çš„è·¯ç”±**å¿½ç•¥äº† token å†…å®¹çš„é‡è¦æ€§ï¼Œå¯¼è‡´å…³é”®é•¿è·ç¦»ä¾èµ–è¢«é”™è¯¯åœ°é€å…¥ä½ä¿çœŸçš„ LA åˆ†æ”¯ã€‚
2. **ç‰¹å¾åˆ†å¸ƒåç§»ï¼ˆdistribution shiftï¼‰**ï¼šLinear Attention ä¸­å¼•å…¥çš„å¯å­¦ä¹  feature mapï¼ˆå¦‚ MLPï¼‰ä¼šæ”¹å˜é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å‘é‡èŒƒæ•°ï¼ˆnormï¼‰ï¼Œç ´åäº†åŸå§‹æ¨¡å‹â€œnorm-awareâ€çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå½±å“è¡¨ç¤ºä¸€è‡´æ€§ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šSTILL
ä½œè€…æå‡º **STILL**ï¼ˆSelecting Tokens for Intra-Layer Hybrid Linearizationï¼‰ï¼Œä¸€ç§é«˜æ•ˆçš„ LLM çº¿æ€§åŒ–æ¡†æ¶ï¼ŒåŒ…å«ä¸‰å¤§åˆ›æ–°ï¼š

#### ï¼ˆ1ï¼‰Self-Saliency Scoreï¼ˆè‡ªæ˜¾è‘—æ€§è¯„åˆ†ï¼‰
- ä¸€ç§åŸºäºå±€éƒ¨æ»‘åŠ¨çª—å£è®¡ç®—ã€ä½†èƒ½åæ˜ å…¨å±€é‡è¦æ€§çš„ token é‡è¦æ€§è¯„åˆ†æœºåˆ¶ã€‚
- åˆ©ç”¨æ»‘åŠ¨çª—å£å†…æ˜¯å¦åŒ…å«è‡ªèº« token çš„ä¸¤ä¸ªæ³¨æ„åŠ›åˆ†å¸ƒå·®å¼‚ï¼ˆ`diag` vs `nodiag`ï¼‰æ¥è¡¡é‡å…¶â€œè‡ªæˆ‘ä¾èµ–â€ç¨‹åº¦ã€‚
- é«˜åˆ† token è¢«åˆ¤å®šä¸ºâ€œæ˜¾è‘—â€ï¼Œå³ä½¿åœ¨çª—å£å¤–ä¹Ÿåº”è¿›å…¥ SA åˆ†æ”¯ï¼Œä»è€Œå®ç°**å†…å®¹æ„ŸçŸ¥çš„ token è·¯ç”±**ã€‚
- å®éªŒè¯æ˜è¯¥åˆ†æ•°å…·æœ‰å¼º**å±€éƒ¨-å…¨å±€ä¸€è‡´æ€§**ï¼Œä»…ç”¨å±€éƒ¨è®¡ç®—å³å¯å¯é é¢„æµ‹å…¨å±€é‡è¦æ€§ã€‚

#### ï¼ˆ2ï¼‰Norm-Preserved Feature Mapï¼ˆNP-Mapï¼‰
- ä¸ºè§£å†³ LA ä¸­ feature map æ‰°åŠ¨é¢„è®­ç»ƒ norm çš„é—®é¢˜ï¼Œè®¾è®¡ NP-Mapã€‚
- æ˜¾å¼è§£è€¦ç‰¹å¾çš„æ–¹å‘ä¸æ¨¡é•¿ï¼šå…ˆé€šè¿‡ MLP å˜æ¢æ–¹å‘ï¼Œå†å°†åŸå§‹è¾“å…¥çš„ norm æ³¨å…¥å›å˜æ¢åçš„ç‰¹å¾ã€‚
- å½¢å¼ä¸ºï¼š  
  $$
  \phi_{\text{NP}}(x) = [\text{softmax}(u) \parallel \text{softmax}(-u)], \quad u = f(x) \cdot \frac{\|x\|}{\|f(x)\|}
  $$
- ä¿è¯çº¿æ€§æ³¨æ„åŠ›åœ¨éè´Ÿæ˜ å°„çš„åŒæ—¶ï¼Œä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç¤ºå¼ºåº¦ã€‚

#### ï¼ˆ3ï¼‰å»¶è¿Ÿé€‰æ‹©ä¸å—çº§å¹¶è¡Œï¼ˆDelayed Selection & Chunk-Wise Parallelizationï¼‰
- åœ¨æ¨ç†é˜¶æ®µé‡‡ç”¨**å»¶è¿Ÿé€‰æ‹©ç­–ç•¥**ï¼šæ¯å¤„ç†å®Œä¸€ä¸ª chunk åæ‰ç»Ÿä¸€å†³å®šå…¶ä¸­å“ªäº› token è¿›å…¥ SA ç¼“å­˜ã€‚
- æ”¯æŒè®­ç»ƒä¸æ¨ç†ç»Ÿä¸€æ¶æ„ï¼Œæå‡ç¡¬ä»¶æ•ˆç‡ä¸å¹¶è¡Œæ€§ã€‚
- å®ç°çœŸæ­£çš„çº¿æ€§å¤æ‚åº¦ $O(N)$ã€‚

### â­ ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆLoLCATs/Ligerï¼‰ | STILL |
|------|--------------------------|-------|
| Token è·¯ç”± | å›ºå®šæ»‘åŠ¨çª—å£ï¼ˆä½ç½®é©±åŠ¨ï¼‰ | åŠ¨æ€é€‰æ‹©ï¼ˆå†…å®¹é©±åŠ¨ï¼ŒSelf-Saliency Scoreï¼‰ |
| Norm ä¿æŒ | æ— ä¿æŠ¤ï¼ŒMLP æ”¹å˜ norm | æ˜¾å¼ä¿ç•™åŸå§‹ normï¼ˆNP-Mapï¼‰ |
| æ¶æ„æ•ˆç‡ | é€ token è·¯ç”±ï¼Œä½å¹¶è¡Œ | å—çº§è·¯ç”±ï¼Œé«˜å¹¶è¡Œï¼Œè®­ç»ƒæ¨ç†ä¸€è‡´ |
| æ€§èƒ½æ¢å¤èƒ½åŠ› | é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸¥é‡é€€åŒ– | æ¥è¿‘ç”šè‡³è¶…è¶ŠåŸæ¨¡å‹ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **å¸¸è¯†ä¸é€šç”¨æ¨ç†ä»»åŠ¡**ï¼š
  - PIQA, ARC-Easy/Challenge, HellaSwag, WinoGrande, MMLUï¼ˆ5-shotï¼‰
- **é•¿ä¸Šä¸‹æ–‡ç†è§£ä»»åŠ¡**ï¼š
  - **RULER**ï¼šå•é’ˆ/å¤šé’ˆæ£€ç´¢ï¼ˆS-NIAHï¼‰ã€å¤šé”®å€¼æ£€ç´¢ï¼ˆMK/MQ/MVï¼‰ã€é—®ç­”ï¼ˆHQA/SQAï¼‰ã€å˜é‡è¿½è¸ªï¼ˆVTï¼‰ç­‰ï¼Œæœ€é•¿è¾¾ 4K tokensã€‚
  - **BABILong**ï¼šbAbI ä»»åŠ¡çš„é•¿ä¸Šä¸‹æ–‡æ‰©å±•ï¼ŒåµŒå…¥å¤§é‡æ— å…³æ–‡æœ¬ï¼Œæµ‹è¯•è¿œè·ç¦»è¯æ®æ•´åˆèƒ½åŠ›ï¼Œæœ€é•¿è‡³ 4K tokensã€‚
- **è®­ç»ƒæ•°æ®**ï¼šCleaned Alpaca æ•°æ®é›†ï¼ˆ20M tokensï¼Œåºåˆ—é•¿åº¦ 1024ï¼‰

### ğŸ§ª å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **æ•™å¸ˆæ¨¡å‹**ï¼šLlama 3 8B / Llama 3.1 8B / Llama 3.2 1B&3B / Mistral 7B
- **è®­ç»ƒæµç¨‹**ï¼ˆä¸¤é˜¶æ®µï¼‰ï¼š
  1. **Attention Transfer**ï¼šå†»ç»“æ•™å¸ˆå‚æ•°ï¼Œæœ€å°åŒ– MSE æŸå¤±å¯¹é½æ³¨æ„åŠ›å›¾ã€‚
  2. **Low-Rank Linearization**ï¼šä½¿ç”¨ LoRA å¾®è°ƒ query/key/value/output æŠ•å½±åŠé—¨æ§å‚æ•°ã€‚
- **è¯„ä¼°æ–¹å¼**ï¼šzero-shot æˆ–æ ‡å‡† few-shotï¼ˆå¦‚ MMLU 5-shotï¼‰ï¼Œä½¿ç”¨ `lm-evaluation-harness` ç»Ÿä¸€è¯„æµ‹ã€‚
- **ç¡¬ä»¶æ•ˆç‡æµ‹è¯•**ï¼šåœ¨ NVIDIA H200 ä¸Šæµ‹é‡ä¸åŒåºåˆ—é•¿åº¦ï¼ˆ1Kâ€“64Kï¼‰ä¸‹çš„å³°å€¼å†…å­˜ä¸ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **Subquadratic from Scratch** | Mamba, Mamba2, RWKV-6, Hawk, Griffin |
| **Inter-layer Hybrid** | StripedHyena, Zamba, RecurrentGemma |
| **Linearized from Pretrained** | LoLCATs, Liger-GLA, Hedgehog-LoLCATs, SUPRA, T2R |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰å¸¸è¯†ä¸é€šç”¨æ¨ç†ï¼ˆLlama 3.1 8B æ•™å¸ˆï¼‰
| æ–¹æ³• | MMLU (5-shot) | å¹³å‡å¾—åˆ†ï¼ˆå« MMLUï¼‰ |
|------|---------------|--------------------|
| åŸå§‹ Llama 3.1 8B | 68.0 | 74.2 |
| LoLCATs | 50.8 | 69.9 |
| Liger-GLA | 46.9 | 68.3 |
| **STILL (Ours)** | **61.3** | **72.5** |

> STILL åœ¨ä¸é‡æ–°é¢„è®­ç»ƒçš„å‰æä¸‹ï¼Œ**æ¥è¿‘åŸå§‹æ¨¡å‹æ€§èƒ½**ï¼Œä¸”æ˜¾è‘—ä¼˜äºæ‰€æœ‰çº¿æ€§åŒ–åŸºçº¿ã€‚

#### ï¼ˆ2ï¼‰é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼ˆ4K contextï¼‰

##### RULER - S-NIAH-1ï¼ˆå•é’ˆæ£€ç´¢ï¼‰
| æ–¹æ³•ï¼ˆcache â‰¤512ï¼‰ | å‡†ç¡®ç‡ @4K |
|------------------|-----------|
| LoLCATs | 8.8 |
| Liger-GLA | 0.0 |
| **STILL** | **86.2** |
| Full Attention | 100 |

> STILL è¾¾åˆ° full attention çš„ **86.2% æ€§èƒ½**ï¼Œè¿œè¶…åŸºçº¿ã€‚

##### RULER - Extendedï¼ˆç»¼åˆè®°å¿†ä¸çŠ¶æ€è¿½è¸ªï¼‰
| æ–¹æ³• | å¹³å‡å¾—åˆ† |
|------|---------|
| Mamba2 | 44.7 |
| LoLCATs | 7.2 |
| Liger-GLA | 2.1 |
| **STILL** | **47.9** |

> è¡¨æ˜ STILL å…·å¤‡æ›´å¼ºçš„**é•¿ç¨‹çŠ¶æ€è·Ÿè¸ªä¸ä¸Šä¸‹æ–‡æ•´åˆèƒ½åŠ›**ã€‚

##### BABILong - QA5ï¼ˆå¤æ‚æ¨ç†ï¼‰
| æ–¹æ³•ï¼ˆcache=256ï¼‰ | å‡†ç¡®ç‡ @4K |
|------------------|-----------|
| SWA / LoLCATs | 6 |
| **STILL** | **24** |

> æå‡é«˜è¾¾ **300%**ï¼Œæ˜¾ç¤ºå…¶åœ¨å¤æ‚æ¨ç†ä¸­æœ‰æ•ˆèšåˆè¿œè·ç¦»è¯æ®ã€‚

#### ï¼ˆ3ï¼‰æ•ˆç‡æŒ‡æ ‡
- **è®­ç»ƒæˆæœ¬**ï¼šä»…éœ€ **0.04B è®­ç»ƒ tokens**ï¼ˆç›¸æ¯”ä»å¤´è®­ç»ƒ >1000Bï¼‰ã€‚
- **å†…å­˜å‡å°‘**ï¼šè¶…è¿‡ 8K tokens æ—¶ï¼Œå¹³å‡ **é™ä½ 45% å†…å­˜**ã€‚
- **è§£ç åŠ é€Ÿ**ï¼šç›¸åŒæ¡ä»¶ä¸‹å®ç° **28% è§£ç é€Ÿåº¦æå‡**ã€‚
- **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæœ€é•¿ **64K tokens** æ¨ç†ï¼Œè€Œæ ‡å‡† Softmax åœ¨ 32K å³ OOMã€‚

### ğŸ” æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰
åœ¨ BABILongï¼ˆ1K contextï¼‰ä¸ŠéªŒè¯æ¨¡å—è´¡çŒ®ï¼š

| é…ç½® | å‡†ç¡®ç‡ | ä¸‹é™ |
|------|--------|------|
| å®Œæ•´ STILL | 27.5 | â€” |
| ç§»é™¤ Gate | 21.7 | -5.8 |
| ç§»é™¤ NP-Map | 22.4 | -4.9 |
| ç§»é™¤ä¸¤è€… | 19.2 | -8.3 |
| ç§»é™¤ Self-Saliency Score | 12.5 | **-15.0** |

> ç»“è®ºï¼š**Self-Saliency Score æ˜¯æœ€å…³é”®ç»„ä»¶**ï¼Œå„æ¨¡å—äº’è¡¥ï¼Œå…±åŒæå‡æ€§èƒ½ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å†…å®¹æ„ŸçŸ¥è·¯ç”±ä¼˜äºä½ç½®å›ºå®šè·¯ç”±**ï¼šé€šè¿‡ Self-Saliency Score å®ç°çš„åŠ¨æ€ token é€‰æ‹©ï¼Œèƒ½æ›´å‡†ç¡®è¯†åˆ«å¯¹æ¨ç†è‡³å…³é‡è¦çš„ tokenï¼ˆå¦‚å¦å®šè¯ not/no/neverã€æƒ…æ€åŠ¨è¯ will/mayã€é‡åŒ–è¯ all/anyï¼‰ï¼Œæ˜¾è‘—æå‡é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚
2. **Norm ä¿æŒè‡³å…³é‡è¦**ï¼šé¢„è®­ç»ƒæ¨¡å‹çš„â€œnorm-awareâ€ç‰¹æ€§æ˜¯é«˜æ€§èƒ½çš„å…³é”®ï¼ŒNP-Map æˆåŠŸä¿ç•™è¿™ä¸€å±æ€§ï¼Œé¿å… LA å¼•èµ·çš„è¡¨ç¤ºæ¼‚ç§»ã€‚
3. **é«˜æ•ˆçº¿æ€§åŒ–å¯è¡Œ**ï¼šSTILL ä»…ç”¨æå°‘é‡è®­ç»ƒï¼ˆ0.04B tokensï¼‰ï¼Œå³å¯å°†é¢„è®­ç»ƒ LLM é«˜æ•ˆçº¿æ€§åŒ–ï¼Œåœ¨æ€§èƒ½ã€å†…å­˜ã€é€Ÿåº¦ä¹‹é—´å–å¾—ä¼˜è¶Šå¹³è¡¡ã€‚
4. **é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›å¤§å¹…æ¢å¤**ï¼šåœ¨ RULER å’Œ BABILong ä¸Šï¼ŒSTILL æ¢å¤äº†ä»¥å¾€çº¿æ€§åŒ–æ–¹æ³•ä¸¢å¤±çš„é•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›ï¼Œè¾¾åˆ°æ¥è¿‘ full attention çš„æ°´å¹³ã€‚

### âš ï¸ å±€é™æ€§
- ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹çš„æ³¨æ„åŠ›å›¾è¿›è¡Œè’¸é¦ï¼Œè‹¥æ•™å¸ˆæ¨¡å‹æœ¬èº«æ³¨æ„åŠ›è´¨é‡å·®ï¼Œåˆ™å¯èƒ½é™åˆ¶æ€§èƒ½ä¸Šé™ã€‚
- Self-Saliency Score çš„è®¾è®¡åŸºäºæ³¨æ„åŠ›å¯¹è§’é¡¹æ•æ„Ÿæ€§ï¼Œå…¶ç†è®ºè§£é‡Šä»æœ‰æ·±åŒ–ç©ºé—´ã€‚
- chunk-wise è®¾è®¡è™½æé«˜æ•ˆç‡ï¼Œä½†åœ¨æç«¯ç¨€ç–é€‰æ‹©ä¸‹å¯èƒ½å­˜åœ¨ç¼“å­˜ç®¡ç†å¼€é”€ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- å°† STILL æ‰©å±•è‡³ encoder-decoder æ¶æ„ï¼ˆå¦‚ T5ã€BARTï¼‰ã€‚
- æ¢ç´¢æ›´è½»é‡åŒ–çš„ saliency ä¼°è®¡å™¨ï¼Œè¿›ä¸€æ­¥é™ä½æ¨ç†å»¶è¿Ÿã€‚
- ç»“åˆå…¶ä»–é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚ FlashAttentionã€PagedAttentionï¼‰ä¼˜åŒ–å®é™…éƒ¨ç½²ã€‚
- æ¢ç´¢ saliency score åœ¨æ¨¡å‹å‹ç¼©ã€å‰ªæã€æ¿€æ´»åˆ†æä¸­çš„é€šç”¨ä»·å€¼ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> STILL é€šè¿‡ **Self-Saliency Score** å®ç°å†…å®¹æ„ŸçŸ¥çš„ token é€‰æ‹©ï¼Œç»“åˆ **NP-Map** ä¿æŒé¢„è®­ç»ƒ norm ç»“æ„ï¼Œå¹¶é‡‡ç”¨ **chunk-wise å¹¶è¡Œæ¶æ„**ï¼Œå®ç°äº†é«˜æ•ˆã€é«˜æ€§èƒ½çš„ LLM çº¿æ€§åŒ–ï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å®é™…éƒ¨ç½²æä¾›äº†å¼ºæœ‰åŠ›çš„æ–°æ–¹æ¡ˆã€‚

</details>

---

### 5. [Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training](https://arxiv.org/abs/2602.01872)

**Authors**: Chongyang Xu, Christoph Siebenbrunner, Laurent Bindschaedler  
**Category**: cs.DC  
**Published**: 2026-02-03  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2602.01872v1  

#### Abstract
Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iterati...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
åˆ†å¸ƒå¼ GNN è®­ç»ƒé¢ä¸´ä¸¥é‡çš„**é€šä¿¡ç“¶é¢ˆ**ï¼šéšç€å›¾è§„æ¨¡å¢å¤§å’Œæ¨¡å‹åŠ æ·±ï¼Œè·¨åˆ†åŒºè¾¹ï¼ˆcross-partition edgesï¼‰å¯¼è‡´æ¯è½®è¿­ä»£éƒ½éœ€è¦é¢‘ç¹åœ°ä»è¿œç¨‹èŠ‚ç‚¹è·å–ç‰¹å¾å’Œæ¿€æ´»å€¼ï¼ˆfeature/activation fetchesï¼‰ï¼Œè¿™åœ¨ç½‘ç»œå¸¦å®½ä¸Šé€ æˆå·¨å¤§å‹åŠ›ï¼Œä¸¥é‡é™åˆ¶äº†è®­ç»ƒçš„å¯æ‰©å±•æ€§ã€‚

ç°æœ‰è§£å†³æ–¹æ¡ˆå¦‚ METIS åˆ†åŒºã€NVLink é«˜é€Ÿäº’è”ã€ç¼“å­˜æœºåˆ¶ç­‰ï¼Œè¦ä¹ˆé¢„å¤„ç†å¼€é”€å¤§ï¼Œè¦ä¹ˆä¾èµ–æ˜‚è´µç¡¬ä»¶ï¼Œä¸”æ— æ³•ä»æ ¹æœ¬ä¸Šæ¶ˆé™¤é€šä¿¡å¼€é”€ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
Grappa æå‡ºäº†ä¸€ç§å…¨æ–°çš„åˆ†å¸ƒå¼ GNN è®­ç»ƒæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯ **Gradient-Only Communicationï¼ˆæ¢¯åº¦ä»…é€šä¿¡ï¼‰** èŒƒå¼ï¼š

- **éš”ç¦»è®­ç»ƒï¼ˆIsolated Trainingï¼‰**ï¼šæ¯ä¸ªåˆ†åŒºåœ¨æ¯æ¬¡è¿­ä»£ä¸­ç‹¬ç«‹è®­ç»ƒï¼Œä¸è¿›è¡Œä»»ä½•è·¨åˆ†åŒºçš„ç‰¹å¾æˆ–æ¿€æ´»äº¤æ¢ï¼Œä»…åœ¨å…¨å±€æ›´æ–°é˜¶æ®µåŒæ­¥æ¢¯åº¦ã€‚
- **åŠ¨æ€é‡åˆ†åŒºï¼ˆDynamic Repartitioningï¼‰**ï¼šé€šè¿‡å‘¨æœŸæ€§åœ°å¯¹å›¾è¿›è¡Œé‡æ–°åˆ†åŒºï¼ˆå½¢æˆâ€œè¶…çº§è½®æ¬¡â€ super-epochsï¼‰ï¼Œä½¿æ¯ä¸ªèŠ‚ç‚¹æœ‰æœºä¼šçœ‹åˆ°ä¸åŒçš„é‚»å±…é›†åˆï¼Œä»è€Œæ¢å¤å…¨å±€ä¿¡æ¯è¦†ç›–ã€‚
- **è¦†ç›–ç‡æ ¡æ­£æ¢¯åº¦èšåˆï¼ˆCoverage-Corrected Gradient Aggregationï¼‰**ï¼šå¼•å…¥åŸºäºé‡è¦æ€§é‡‡æ ·ï¼ˆimportance samplingï¼‰çš„è½»é‡çº§æ‰¹çº§åˆ«ï¼ˆbatch-levelï¼‰æ¢¯åº¦åŠ æƒæœºåˆ¶ï¼Œè¡¥å¿å› å±€éƒ¨è®­ç»ƒå¸¦æ¥çš„é‡‡æ ·åå·®ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æè‡´é™ä½é€šä¿¡å¼€é”€**ï¼šå®Œå…¨æ¶ˆé™¤æ¯è½®è¿­ä»£ä¸­çš„è·¨åˆ†åŒº neighbor trafficï¼Œä»…ä¿ç•™æ¢¯åº¦åŒæ­¥ã€‚
- **æ— éœ€ç‰¹æ®Šç¡¬ä»¶**ï¼šä¸ä¾èµ–é«˜å¸¦å®½äº’è”ï¼ˆå¦‚ NVLink/RDMAï¼‰æˆ–å¤æ‚ç¼“å­˜ç³»ç»Ÿã€‚
- **æ¨¡å‹æ— å…³æ€§ï¼ˆmodel-agnosticï¼‰**ï¼šæ”¯æŒä»»æ„ GNN æ¶æ„ï¼ˆå¦‚ GraphSAGE, GCN, GATï¼‰ã€‚
- **çµæ´»æ‰§è¡Œæ¨¡å¼**ï¼šæ”¯æŒ phase-parallel æ‰§è¡Œï¼Œåœ¨å•æœºä¸Šä¹Ÿèƒ½è®­ç»ƒä¸‡äº¿è¾¹çº§åˆ«çš„è¶…å¤§è§„æ¨¡å›¾ã€‚
- **ç†è®ºä¿éšœ**ï¼šè¯æ˜æ‰€æå‡ºçš„æ‰¹çº§åˆ«æ ¡æ­£ä¼°è®¡å™¨æ˜¯æ¸è¿‘æ— åçš„ï¼Œå¹¶æœ€å°åŒ–å‡æ–¹è¯¯å·®ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
| æ•°æ®é›† | ç±»å‹ | èŠ‚ç‚¹æ•° | è¾¹æ•° | ç‰¹å¾ç»´åº¦ |
|-------|------|--------|--------|----------|
| OGBN-Ar | å­¦æœ¯å¼•ç”¨å›¾ | 0.16M | 1.11M | 128 |
| Reddit | ç¤¾äº¤ç½‘ç»œ | 0.22M | 109.3M | 602 |
| OGBN-Pr | å•†å“è´­ä¹°å›¾ | 2.34M | 58.99M | 100 |
| OGBN-Pa | è®ºæ–‡å¼•ç”¨å›¾ | 105.92M | 1.51B | 128 |
| RMAT-X | åˆæˆå›¾ï¼ˆå¹‚å¾‹åˆ†å¸ƒï¼‰ | $2^X$ | $2^{X+4}$ | 128 |

å…¶ä¸­ RMAT-36 åŒ…å«çº¦ **1.1 ä¸‡äº¿æ¡è¾¹**ï¼Œç”¨äºæµ‹è¯•æé™è§„æ¨¡ä¸‹çš„å¯è¡Œæ€§ã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **ç¡¬ä»¶ç¯å¢ƒ**ï¼š
  - åˆ†å¸ƒå¼é›†ç¾¤ï¼š8 å°æœºå™¨ï¼Œæ¯å° 2Ã—V100 GPUï¼ˆPCIeï¼‰ï¼Œ764GB å†…å­˜ï¼Œ10Gb/s ç½‘ç»œã€‚
  - å•èŠ‚ç‚¹é«˜æ€§èƒ½å¹³å°ï¼š8Ã—H100 GPUï¼ˆå…¨ NVLink è¿æ¥ï¼‰ã€‚
- **è®­ç»ƒé…ç½®**ï¼š
  - Batch size: 1000
  - Hidden dimension: 128
  - Learning rate: 0.003
  - Dropout: 0.5
  - Epochs: 500
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **ç³»ç»Ÿå±‚é¢**ï¼šå¹³å‡ epoch æ—¶é—´ã€ååé‡ã€å¯æ‰©å±•æ€§ã€å†…å­˜å ç”¨ã€‚
  - **æ¨¡å‹å±‚é¢**ï¼šæµ‹è¯•å‡†ç¡®ç‡ï¼ˆtest accuracyï¼‰ã€æ”¶æ•›é€Ÿåº¦ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿ç³»ç»Ÿ | ç±»å‹ | ç‰¹ç‚¹ |
|---------|------|------|
| **DGL** | åˆ†å¸ƒå¼é‡‡æ ·è®­ç»ƒ | æ”¯æŒ full-graph å’Œ mini-batchï¼Œå…¸å‹ä»£è¡¨ |
| **MGG** | å•èŠ‚ç‚¹å…¨å›¾è®­ç»ƒ | åˆ©ç”¨ NVLink åŠ é€Ÿï¼Œä½†æ— æ³•åˆ†å¸ƒå¼æ‰©å±• |
| **Cluster-GCN** | é¢„èšç±»é‡‡æ · | é€šè¿‡ METIS é¢„åˆ†ç»„å‡å°‘é€šä¿¡ï¼Œä½†ä»…é€‚ç”¨äº GCN |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
- **å¹³å‡åŠ é€Ÿæ¯”**ï¼šç›¸æ¯”å½“å‰æœ€å…ˆè¿›çš„ç³»ç»Ÿï¼ˆå¦‚ DGLï¼‰ï¼ŒGrappa å¹³å‡å®ç° **4Ã— æ›´å¿«** çš„è®­ç»ƒé€Ÿåº¦ï¼Œæœ€é«˜å¯è¾¾ **13Ã— åŠ é€Ÿ**ã€‚
- **å¯æ‰©å±•æ€§**ï¼šåœ¨æœ€å¤š 64 ä¸ªåˆ†åŒºä¸‹ä»ä¿æŒè¿‘ä¹çº¿æ€§çš„æ‰©å±•æ•ˆç‡ã€‚
- **è¶…å¤§è§„æ¨¡è®­ç»ƒèƒ½åŠ›**ï¼š
  - åœ¨å•ä¸ª V100 æœºå™¨ä¸Šä»¥ phase-parallel æ¨¡å¼å®Œæˆ **RMAT-36ï¼ˆ1.1 ä¸‡äº¿è¾¹ï¼‰** ä¸Šçš„ä¸€è½® Sage-3 è®­ç»ƒï¼Œè€—æ—¶ **3.6 å°æ—¶/epoch**ã€‚
  - è¡¨æ˜å¯åœ¨å•†å“åŒ–ç¡¬ä»¶ä¸Šè®­ç»ƒä¼ ç»Ÿæ–¹æ³•æ— æ³•å®¹çº³çš„å·¨å‹å›¾ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
#### âœ… vs DGLï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰
| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| Epoch time | Grappa æ¯” DGL å¿« 1â€“13Ã—ï¼ˆå¹³å‡ 4Ã—ï¼‰ |
| Test accuracy | å¯¹äºæ·±å±‚æ¨¡å‹ï¼ˆ3â€“4 å±‚ï¼‰ï¼ŒGrappa æ˜¾è‘—ä¼˜äº DGLï¼ˆä¾‹å¦‚ GCN-4 å‡†ç¡®ç‡é«˜å‡º ~20%ï¼‰ |
| å¯æ‰©å±•æ€§ | Grappa æ¥è¿‘çº¿æ€§æ‰©å±•ï¼›DGL å› é€šä¿¡å¼€é”€å¢é•¿è€Œæ‰©å±•æ€§å·® |

> **åŸå› **ï¼šGrappa é¿å…äº†æ¯è½®è¿­ä»£ä¸­é«˜æ˜‚çš„è¿œç¨‹ fetch å¼€é”€ã€‚

#### âœ… vs MGGï¼ˆå•èŠ‚ç‚¹å…¨å›¾è®­ç»ƒï¼‰
| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| å°å›¾æ€§èƒ½ | MGG åœ¨å°å›¾ä¸Šæ›´å¿«ï¼ˆåˆ©ç”¨ NVLinkï¼‰ |
| å¤§å›¾æ”¯æŒ | MGG åœ¨ OGBN-Pa ä¸Šå› æ˜¾å­˜ä¸è¶³å´©æºƒï¼›Grappa æˆåŠŸè¿è¡Œ |
| å†…å­˜æ•ˆç‡ | Grappa ä¸è¦æ±‚å°†æ•´ä¸ªå›¾åŠ è½½è¿› HBM |

> **ç»“è®º**ï¼šGrappa æ›´é€‚åˆå¤šèŠ‚ç‚¹ã€å¤§å›¾åœºæ™¯ï¼Œä¸å—é™äº GPU æ˜¾å­˜ã€‚

#### âœ… vs Cluster-GCNï¼ˆé¢„èšç±»é‡‡æ ·ï¼‰
| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| Epoch time | Cluster-GCN æ›´å¿«ï¼ˆçº¦ä¸€åŠæ—¶é—´ï¼‰ |
| Test accuracy | Grappa æ›´é«˜ï¼ˆ+2.7%ï¼‰ |
| é€šç”¨æ€§ | Cluster-GCN ä»…æ”¯æŒ GCNï¼›Grappa æ”¯æŒæ‰€æœ‰ä¸»æµ GNN æ¨¡å‹ |
| æ€»ä½“æ—¶é—´ | è‹¥è®¡å…¥é¢„èšç±»è€—æ—¶ï¼ˆ881 ç§’ï¼‰ï¼ŒGrappa æ€»ä½“æ›´ä¼˜ |

> **æ„ä¹‰**ï¼šéªŒè¯äº†â€œä½é€šä¿¡â€è®¾è®¡è·¯å¾„çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶å¼ºè°ƒé€šç”¨æ€§ä¼˜åŠ¿ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰
åœ¨ Reddit æ•°æ®é›†ä¸Šå¯¹ Grappa çš„ä¸‰ä¸ªå…³é”®ç»„ä»¶è¿›è¡Œæ¶ˆèåˆ†æï¼ˆSage-3, 16 åˆ†åŒºï¼‰ï¼š

| æ–¹æ³• | Test Accuracy |
|------|----------------|
| **Grappaï¼ˆå®Œæ•´ç‰ˆï¼‰** | **0.9589** |
| Grappa-UWï¼ˆæ— è¦†ç›–ç‡æ ¡æ­£ï¼‰ | 0.9636 |
| Grappa-FPï¼ˆå›ºå®šåˆ†åŒºï¼‰ | 0.9556 |
| Grappa-50Sï¼ˆ50% è¿œç¨‹é‡‡æ ·ï¼‰ | 0.9635 |

> âš ï¸ æ³¨æ„ï¼šè™½ç„¶å»æ‰æ ¡æ­£åç²¾åº¦ç•¥å‡ï¼Œä½†è¿™æºäºè¿‡æ‹Ÿåˆé£é™©å¢åŠ ã€‚ä½œè€…æŒ‡å‡º Grappa çš„éš”ç¦»è®­ç»ƒå…·æœ‰ç±»ä¼¼ **dropout çš„æ­£åˆ™åŒ–æ•ˆæœ**ï¼Œæœ‰åŠ©äºæ³›åŒ–ï¼Œå°¤å…¶åœ¨æ·±å±‚æ¨¡å‹ä¸­è¡¨ç°æ›´ä½³ã€‚

æ­¤å¤–ï¼Œé‡åˆ†åŒºåˆ‡æ¢å¼€é”€ä»…ä¸ºæ€»è®­ç»ƒæ—¶é—´çš„ **4%â€“12%**ï¼Œè¿œå°äºé€šä¿¡èŠ‚çœçš„æˆæœ¬ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **æ¢¯åº¦ä»…é€šä¿¡å¯è¡Œä¸”é«˜æ•ˆ**ï¼šé€šè¿‡éš”ç¦»è®­ç»ƒ + åŠ¨æ€é‡åˆ†åŒº + æ¢¯åº¦æ ¡æ­£ï¼Œå¯ä»¥å®‰å…¨ç§»é™¤æ‰€æœ‰è·¨åˆ†åŒºä¸­é—´çŠ¶æ€é€šä¿¡ï¼Œæå¤§æå‡è®­ç»ƒæ•ˆç‡ã€‚
2. **éš”ç¦»è®­ç»ƒæœ‰ç›Šæ³›åŒ–**ï¼šå±€éƒ¨è®­ç»ƒè¡Œä¸ºç±»ä¼¼äºè¾“å…¥å±‚ dropoutï¼Œèƒ½æœ‰æ•ˆé˜²æ­¢æ·±å±‚ GNN è¿‡æ‹Ÿåˆï¼Œåè€Œæå‡æµ‹è¯•å‡†ç¡®ç‡ã€‚
3. **æ‰¹çº§åˆ«æ ¡æ­£è¶³å¤Ÿæœ‰æ•ˆ**ï¼šæå‡ºçš„ batch-level importance weighting æ–¹æ³•åœ¨ä¿æŒä¸ç†æƒ³ node-level ä¿®æ­£æœ€å° MSE åå·®çš„åŒæ—¶ï¼Œå…¼å®¹ PyTorch ç­‰ä¸»æµæ¡†æ¶ã€‚
4. **phase-parallel æ¨¡å¼çªç ´å®¹é‡é™åˆ¶**ï¼šå…è®¸åœ¨å•æœºä¸Šé¡ºåºè®­ç»ƒå¤šä¸ªåˆ†åŒºï¼Œå®ç°äº† **trillion-edge-scale å›¾çš„ç«¯åˆ°ç«¯è®­ç»ƒ**ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **é‡åˆ†åŒºå¸¦æ¥é¢å¤–å¼€é”€**ï¼šå°½ç®¡å¯æ§ï¼Œä½†åœ¨æçŸ­ epoch åœºæ™¯ä¸‹å¯èƒ½å½±å“æ•ˆç‡ã€‚
- **ä¾èµ– CPU-GPU æ•°æ®æ¬è¿**ï¼šç›®å‰é‡‡æ ·ä»åœ¨ CPU å®Œæˆï¼ŒI/O å¯èƒ½æˆä¸ºæ½œåœ¨ç“¶é¢ˆã€‚
- **ç†è®ºå‡è®¾ä¾èµ–é•¿æœŸè¦†ç›–æ¦‚ç‡ä¸ºæ­£**ï¼šæç«¯ä¸å¹³è¡¡åˆ†åŒºå¯èƒ½å¯¼è‡´æŸäº›é‚»å±…é•¿æœŸæœªè¢«è¦†ç›–ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- è®¾è®¡æ›´æ™ºèƒ½çš„ **repartitioning switching heuristic**ï¼Œæ ¹æ®å®é™…è¦†ç›–æƒ…å†µåŠ¨æ€è°ƒæ•´åˆ‡æ¢æ—¶æœºã€‚
- æ¢ç´¢ **å¼‚æ­¥æ¢¯åº¦èšåˆ** æˆ– **è”é‚¦å­¦ä¹ ç»“åˆ**ï¼Œè¿›ä¸€æ­¥é™ä½åŒæ­¥æˆæœ¬ã€‚
- å°† phase-parallel æ€æƒ³æ¨å¹¿è‡³å…¶ä»–å¤§è§„æ¨¡ ML æ¨¡å‹è®­ç»ƒä¸­ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **Grappa é€šè¿‡â€œæ¢¯åº¦ä»…é€šä¿¡ + åŠ¨æ€é‡åˆ†åŒº + è¦†ç›–ç‡æ ¡æ­£â€çš„ä¸‰é‡æœºåˆ¶ï¼Œåœ¨ä¸ç‰ºç‰²æ¨¡å‹ç²¾åº¦çš„å‰æä¸‹ï¼Œå°†åˆ†å¸ƒå¼ GNN è®­ç»ƒé€Ÿåº¦æå‡ 4Ã— ä»¥ä¸Šï¼Œå¹¶é¦–æ¬¡å®ç°åœ¨å•æœºä¸Šè®­ç»ƒä¸‡äº¿è¾¹å›¾çš„å¯è¡Œæ€§ï¼Œä¸ºè¶…å¤§è§„æ¨¡å›¾å­¦ä¹ æä¾›äº†æ–°çš„å·¥ç¨‹èŒƒå¼ã€‚**

</details>

---

### 6. [Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs](https://arxiv.org/abs/2602.00879)

**Authors**: Hao Mark Chen, Zhiwen Mo, Royson Lee, Qianzhou Wang, Da Li, Shell Xu Hu, Wayne Luk, Timothy Hospedales, Hongxiang Fan  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2602.00879v1  

#### Abstract
Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens gene...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šDynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs**

---

## 1. **è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### âœ… **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
- åœ¨ **Mixture-of-Experts (MoE)** æ¶æ„ä¸ **diffusion LLMs (dLLMs)** ç»“åˆçš„å¹¶è¡Œè§£ç åœºæ™¯ä¸­ï¼Œå­˜åœ¨â€œ**ä¸“å®¶çˆ†ç‚¸ (expert explosion)**â€é—®é¢˜ï¼š
  - éšç€å¹¶è¡Œç”Ÿæˆçš„ token æ•°é‡ $N$ å¢åŠ ï¼Œæ¯ä¸ª token ç‹¬ç«‹è·¯ç”±å¯¼è‡´æ¿€æ´»çš„**å”¯ä¸€ä¸“å®¶ (unique experts)** æ•°é‡è¿‘ä¹çº¿æ€§å¢é•¿ã€‚
  - è¿™å¸¦æ¥äº†å·¨å¤§çš„ **HBM åˆ° SRAM çš„æƒé‡åŠ è½½å¼€é”€**ï¼Œä½¿æ¨ç†è¿›å…¥ **memory-bound regime**ï¼ŒæŠµæ¶ˆäº† MoE å’Œå¹¶è¡Œè§£ç å¸¦æ¥çš„æ•ˆç‡å¢ç›Šã€‚

### ğŸ†• **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
æå‡º **Dynamic Expert Sharing (DES)**ï¼Œä¸€ç§å…¨æ–°çš„ MoE ä¼˜åŒ–èŒƒå¼ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- å°†ä¼˜åŒ–ç›®æ ‡ä» **token-level çš„ç¨€ç–è®¡ç®—** è½¬å‘ **sequence-level çš„ä¸“å®¶å…±äº« (coreset selection)**ã€‚
- åŠ¨æ€è¯†åˆ«ä¸€ä¸ªç´§å‡‘ã€é«˜å®ç”¨æ€§çš„ **ä¸“å®¶å…±ç”¨å­é›† (shared coreset)** æ¥æœåŠ¡æ•´ä¸ªå¹¶è¡Œ token åºåˆ—ï¼Œä»è€Œæœ€å¤§åŒ–ä¸“å®¶å¤ç”¨ã€æœ€å°åŒ–å†…å­˜æµé‡ã€‚

å…·ä½“å®ç°ä¸¤ç§ç­–ç•¥ï¼š
1. **DES-Seq (Intra-Sequence Sharing)**  
   - å¯¹ block å†…æ‰€æœ‰ token çš„ Top-k ä¸“å®¶å–å¹¶é›†ä½œä¸ºå…±ç”¨ coresetã€‚
   - ç®€å•æœ‰æ•ˆï¼Œæ”¯æŒå‘é‡åŒ–æ‰§è¡Œã€‚

2. **DES-Vote (Saliency-Aware Voting)**  
   - å¼•å…¥åŸºäº **router æƒé‡èšåˆæŠ•ç¥¨æœºåˆ¶**ï¼Œè®© tokens â€œé›†ä½“é€‰ä¸¾â€æœ€é‡è¦çš„ä¸“å®¶ã€‚
   - æ›´æ™ºèƒ½åœ°æ•æ‰åºåˆ—çº§è¯­ä¹‰å¤æ‚æ€§ï¼Œä¼˜å…ˆä¿ç•™å¯¹æ•´ä½“ä»»åŠ¡æœ€å…³é”®çš„ä¸“å®¶ã€‚

### âš¡ **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
- **è¶…è¶Šä¼ ç»Ÿ token-centric æ–¹æ³•**ï¼šå¦‚ NAEEã€MC-MoE ç­‰åŠ¨æ€è·³è¿‡æ–¹æ³•ï¼Œåœ¨ dLLM ä¸Šè¡¨ç°å·®ï¼ˆå‡†ç¡®ç‡ä¸‹é™ä¸¥é‡ï¼‰ï¼Œå› å…¶æœªè€ƒè™‘è·¨ token çš„å†—ä½™ã€‚
- **çœŸæ­£ç¼“è§£ memory bottleneck**ï¼šé€šè¿‡å‡å°‘ unique expert loadï¼Œæ˜¾è‘—é™ä½ HBM æµé‡ï¼Œè€Œéä»…èŠ‚çœ FLOPsã€‚
- **è§£è€¦å†…å­˜å¼€é”€ä¸å¹¶è¡Œåº¦**ï¼šé¦–æ¬¡å®ç° memory overhead ä¸éš block size æ˜¾è‘—å¢é•¿ï¼Œä¸ºå¤§è§„æ¨¡å¹¶è¡Œæä¾›å¯æ‰©å±•åŸºç¡€ã€‚

---

## 2. **æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### ğŸ“š **ä½¿ç”¨çš„æ•°æ®é›†**
åœ¨å››ä¸ªéœ€è¦é•¿æ–‡æœ¬ç”Ÿæˆå’Œå¤šæ ·åŒ–æ¨ç†èƒ½åŠ›çš„åŸºå‡†ä¸Šè¯„ä¼°ï¼š
- **HumanEval**ï¼šä»£ç ç”Ÿæˆ
- **MBPP**ï¼šPython ç¼–ç¨‹ä»»åŠ¡
- **GSM8K**ï¼šå°å­¦æ•°å­¦åº”ç”¨é¢˜
- **MATH500**ï¼šé«˜ä¸­åŠä»¥ä¸Šéš¾åº¦æ•°å­¦é—®é¢˜

### âš™ï¸ **å®éªŒè®¾ç½®**
- **æ¨¡å‹**ï¼š
  - `LLaDA2.0-Mini` (16B, MoE)
  - `LLaDA-MoE-7B` (7B, MoE)
- **ç¡¬ä»¶å¹³å°**ï¼š
  - NVIDIA B200 GPU + Intel Xeon 6960P CPU
  - ä½¿ç”¨ CUDA 13.1 å’Œ **Nsight Systems** è¿›è¡Œæ€§èƒ½å‰–æ
- **æ¡†æ¶**ï¼š
  - åŸºäº `dInfer` æ¡†æ¶ï¼Œé‡‡ç”¨ `Fast-dLLM` çš„ KV cache æ–¹æ³•
  - å¹¶è¡Œ block size è®¾ä¸º 32ï¼ˆå« 16 prefix + 16 suffix tokensï¼‰

### ğŸ¯ **è¯„ä¼°æŒ‡æ ‡**
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Unique Activated Experts (T)** | æ¯å±‚å¹³å‡æ¿€æ´»çš„ç‹¬ç‰¹ä¸“å®¶æ•°ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ |
| **MoE Kernel Latency** | MoE å±‚å‰å‘ä¼ æ’­å»¶è¿Ÿï¼ˆmsï¼‰ |
| **End-to-End GPU Kernel Time** | æ•´ä½“ GPU æ‰§è¡Œæ—¶é—´ |
| **Relative Accuracy (R.Acc %)** | ç›¸å¯¹äº vanilla æ¨¡å‹çš„æ€§èƒ½ç™¾åˆ†æ¯” |
| **Memory Footprint (GB)** | MoE ç»„ä»¶çš„å¹³å‡å†…å­˜å ç”¨ |

### ğŸ” **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **Vanilla MoE dLLM**ï¼šæ ‡å‡†å®ç°ï¼Œæ— ä¼˜åŒ–
- **Top-K (k=4)**ï¼šç›´æ¥å‡å°‘æ¯ token æ¿€æ´»ä¸“å®¶æ•°
- **NAEE**ï¼šåŸºäºç´¯ç§¯æ¦‚ç‡é˜ˆå€¼è·³è¿‡ä½åˆ†ä¸“å®¶
- **MC-MoE**ï¼šä¿ç•™é‡è¦ token çš„ä¸“å®¶
- *æ³¨ï¼šæ‰€æœ‰ baseline å‡é€‚é…è‡³ dLLM è®¾ç½®ä»¥ç¡®ä¿å…¬å¹³æ¯”è¾ƒ*

---

## 3. **ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### ğŸ“Š **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 å’Œ Figure 5ï¼‰**

#### åœ¨ `LLaDA2.0-Mini` ä¸Šçš„ç»“æœï¼š
| æ–¹æ³• | Avg. T â†“ | Mem. (GB) â†“ | R.Acc (%) â†‘ |
|------|--------|------------|-------------|
| Vanilla | 84 | 0.98 | 100.0 |
| NAEE (Î²=0.6) | 65 | 0.76 | 46.6 |
| DES-Seq (k=3) | 45 | 0.53 | 97.2 |
| **DES-Vote (Î²=0.15)** | **38** | **0.45** | **99.5** |

#### åœ¨ `LLaDA-MoE-7B` ä¸Šçš„ç»“æœï¼š
| æ–¹æ³• | Avg. T â†“ | Mem. (GB) â†“ | R.Acc (%) â†‘ |
|------|--------|------------|-------------|
| Vanilla | 59 | 0.35 | 100.0 |
| MC-MoE (Î²=0.6) | 53 | 0.31 | 81.5 |
| **DES-Vote (Î²=0.6)** | **38** | **0.22** | **97.6** |

### ğŸ“ˆ **æ€§èƒ½æå‡æ€»ç»“**
- **ä¸“å®¶æ•°é‡å‡å°‘**ï¼šDES-Vote æœ€å¤šå‡å°‘ **55% çš„ unique expert æ¿€æ´»**ã€‚
- **å»¶è¿Ÿé™ä½**ï¼š
  - MoE kernel latency ä¸‹é™é«˜è¾¾ **38.0%**ï¼ˆLLaDA2.0-Miniï¼‰
  - æ€» GPU kernel æ—¶é—´å‡å°‘ **8.2â€“14.3%**
- **ç²¾åº¦ä¿æŒæä½³**ï¼šDES-Vote ä¿æŒ **99%+ çš„ vanilla å‡†ç¡®ç‡**ï¼Œéƒ¨åˆ†ä»»åŠ¡ç”šè‡³ç•¥æœ‰æå‡ï¼ˆå¯èƒ½å› æ­£åˆ™åŒ–æ•ˆåº”ï¼‰ã€‚
- **ä¼˜äºæ‰€æœ‰ baseline**ï¼šNAEE/MC-MoE å¯¼è‡´å‡†ç¡®ç‡æš´è·Œè‡³ ~46%ï¼Œè€Œ DES æ˜¾è‘—ä¼˜äº DES-Seqï¼Œè¯æ˜ voting æœºåˆ¶æ›´ä¼˜ã€‚

### ğŸ” **æ¶ˆèå®éªŒç»“æœ**
#### ï¼ˆ1ï¼‰**Coreset Size å½±å“ï¼ˆFigure 7aï¼‰**
- å‡†ç¡®ç‡éš coreset size å‡å°è€Œç¼“æ…¢ä¸‹é™ã€‚
- DES-Vote åœ¨ç›¸åŒå¤§å°ä¸‹å§‹ç»ˆä¼˜äº DES-Seqï¼Œä¸”å¯é€šè¿‡è¿ç»­å‚æ•° $\beta$ çµæ´»è°ƒèŠ‚ï¼Œçªç ´ DES-Seq çš„â€œæ¯ token è‡³å°‘ä¸€ä¸“å®¶â€é™åˆ¶ã€‚

#### ï¼ˆ2ï¼‰**Block Size é²æ£’æ€§ï¼ˆFigure 7bï¼‰**
- DES-Vote åœ¨ block size ä» 8 åˆ° 64 å˜åŒ–æ—¶ï¼Œå‡†ç¡®ç‡æ³¢åŠ¨æå°ã€‚
- è¡¨æ˜å…¶èƒ½æœ‰æ•ˆæ³›åŒ–åˆ°ä¸åŒå¹¶è¡Œåº¦ï¼Œå®ç°äº† **memory overhead ä¸ parallelism çš„è§£è€¦**ã€‚

#### ï¼ˆ3ï¼‰**Per-Token Computation å½±å“ï¼ˆFigure 8ï¼‰**
- å³ä½¿ unique expert æ•°ç›¸åŒï¼Œå°† per-token K ä» 8 é™åˆ° 4 ä¼šå¯¼è‡´æ˜æ˜¾å‡†ç¡®ç‡ä¸‹é™ã€‚
- è¯´æ˜ **re-activating å…±äº«ä¸“å®¶ä¸­çš„é top-k æˆå‘˜ä»æœ‰åŠ©äºæ¢å¤æ€§èƒ½**ï¼ŒéªŒè¯äº† coreset å†…å†åˆ†é…çš„ä»·å€¼ã€‚

#### ï¼ˆ4ï¼‰**Custom Kernel åŠ é€Ÿæ•ˆæœï¼ˆFigure 6ï¼‰**
- è‡ªå®šä¹‰èåˆ kernel ç›¸æ¯” PyTorch å®ç°ï¼Œåœ¨ coreset selection ä¸Šå®ç° **6Ã— é€Ÿåº¦æå‡**ï¼Œæ¶ˆé™¤è°ƒåº¦ä¸å†…å­˜ç“¶é¢ˆã€‚

---

## 4. **å…³é”®ç»“è®ºå’Œå‘ç°**

### âœ… **ä¸»è¦å‘ç°**
1. **â€œä¸“å®¶çˆ†ç‚¸â€æ˜¯ MoE dLLM çš„æ ¹æœ¬ç“¶é¢ˆ**ï¼šç‹¬ç«‹ token è·¯ç”±å¯¼è‡´ unique expert æ•°éš block size çº¿æ€§å¢é•¿ï¼Œå¼•å‘ä¸¥é‡ memory-bound é—®é¢˜ã€‚
2. **Sequence-level å…±äº«ä¼˜äº Token-level ç¨€ç–åŒ–**ï¼šDES é€šè¿‡æ„å»ºå…±äº« coresetï¼Œæ˜¾è‘—å‡å°‘ HBM æµé‡ï¼Œå¸¦æ¥çœŸå®ç«¯åˆ°ç«¯åŠ é€Ÿã€‚
3. **Saliency-Aware Voting æ›´ä¼˜**ï¼šDES-Vote åˆ©ç”¨ router æƒé‡è¿›è¡ŒåŠ æƒæŠ•ç¥¨ï¼Œæ¯”ç®€å•å–å¹¶é›†ï¼ˆDES-Seqï¼‰æ›´èƒ½æ•è·å…³é”®ä¸“å®¶ï¼Œå®ç°æ›´é«˜å‡†ç¡®ç‡ä¸æ›´ä½è´Ÿè½½ã€‚
4. **æˆåŠŸè§£è€¦ memory ä¸ parallelism**ï¼šDES ä½¿å¾—å†…å­˜å¼€é”€ä¸å†éš block size å¢é•¿ï¼Œä¸ºæœªæ¥æ›´å¤§è§„æ¨¡å¹¶è¡Œè§£ç é“ºå¹³é“è·¯ã€‚

### âš ï¸ **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ– router åˆ†å¸ƒä¸€è‡´æ€§å‡è®¾**ï¼šè‹¥ block å†… tokens è¯­ä¹‰å·®å¼‚æå¤§ï¼ˆå¦‚å¤šä»»åŠ¡æ··åˆï¼‰ï¼Œå…±äº« coreset å¯èƒ½å¤±æ•ˆã€‚
- **å¼•å…¥é¢å¤–æ§åˆ¶é€»è¾‘å¼€é”€**ï¼šè™½ç„¶ custom kernel å·²ä¼˜åŒ–ï¼Œä½†åœ¨æå° batch æˆ–ä½å¹¶è¡Œåº¦åœºæ™¯ä¸‹æ”¶ç›Šå¯èƒ½æœ‰é™ã€‚
- **éœ€è°ƒå‚ï¼ˆå¦‚ Î² æˆ– kï¼‰**ï¼šæœ€ä¼˜ coreset å¤§å°ä¾èµ–ä»»åŠ¡å’Œæ¨¡å‹ï¼Œéœ€ç»éªŒæˆ–æœç´¢ç¡®å®šã€‚

### ğŸ”® **æœªæ¥å·¥ä½œæ–¹å‘**
- æ¢ç´¢ **adaptive coreset sizing**ï¼šæ ¹æ®è¾“å…¥å¤æ‚åº¦è‡ªåŠ¨è°ƒæ•´ Mcoreã€‚
- æ‰©å±•è‡³ **multi-modal MoE models**ï¼šç ”ç©¶è§†è§‰-è¯­è¨€ç­‰è·¨æ¨¡æ€åœºæ™¯ä¸‹çš„ expert sharingã€‚
- ç»“åˆ **offline pruning + online sharing**ï¼šå½¢æˆè”åˆä¼˜åŒ– pipelineã€‚
- æ¨å¹¿è‡³å…¶ä»– **non-diffusion parallel decoding models**ï¼Œå¦‚ speculative decoding ä¸­çš„ draft modelã€‚

---

> **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **DES æå‡ºäº†ä¸€ç§é¢å‘ MoE dLLM çš„ sequence-level åŠ¨æ€ä¸“å®¶å…±äº«æœºåˆ¶ï¼Œé€šè¿‡ DES-Seq å’Œ DES-Vote å®ç°ä¸“å®¶å…±ç”¨å­é›†é€‰æ‹©ï¼Œåœ¨å‡ ä¹ä¸æŸå¤±å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œæœ€å¤šå‡å°‘ 55% çš„ unique expert æ¿€æ´»å’Œ 38% çš„ MoE å»¶è¿Ÿï¼Œä»æ ¹æœ¬ä¸Šç¼“è§£äº†å¹¶è¡Œè§£ç ä¸­çš„ memory bottleneckã€‚**

</details>

---

### 7. [HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2602.00777)

**Authors**: Xuan Ai, Qingqing Yang, Peng Wang, Lei Deng, Lin Zhang, Renhai Chen, Gong Zhang  
**Category**: cs.CL  
**Published**: 2026-02-03  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.00777v1  

#### Abstract
Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often r...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šHyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­é¢ä¸´ä¸¤å¤§ç“¶é¢ˆï¼š
- **Attentionæœºåˆ¶çš„äºŒæ¬¡è®¡ç®—å¤æ‚åº¦**ï¼ˆquadratic computation complexityï¼‰
- **Key-Value (KV) cache çš„å·¨å¤§å†…å­˜å¼€é”€**

ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›ï¼ˆsparse attentionï¼‰æ–¹æ³•è™½ç„¶è¯•å›¾é€šè¿‡ä»…å¤„ç†é«˜åˆ†tokenæ¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†é€šå¸¸å­˜åœ¨ä»¥ä¸‹ç¼ºé™·ï¼š
- é‡‡ç”¨å›ºå®šæ¨¡å¼ï¼ˆå¦‚æ»‘åŠ¨çª—å£ï¼‰æˆ–æ¿€è¿›å‰ªæç­–ç•¥ï¼Œå®¹æ˜“ä¸¢å¤±å…³é”®ä¿¡æ¯ï¼›
- å¿½ç•¥ä¸åŒTransformerå±‚ä¹‹é—´çš„ç»“æ„å·®å¼‚ï¼Œç»Ÿä¸€åº”ç”¨ç›¸åŒçš„ç¨€ç–åŒ–ç­–ç•¥ï¼›
- æ¯ä¸€å±‚ç‹¬ç«‹è¿›è¡Œé‡è¦æ€§è¯„åˆ†ä¸æ’åºï¼Œå¸¦æ¥é‡å¤è®¡ç®—å¼€é”€ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
æœ¬æ–‡æå‡º **HyLRA (Hybrid Layer Reuse Attention)**ï¼Œä¸€ç§åŸºäº**å±‚é—´ç›¸ä¼¼æ€§åˆ†æ**çš„æ··åˆæ³¨æ„åŠ›æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ¥æºäºå¯¹æ³¨æ„åŠ›è¡Œä¸ºçš„å®è¯åˆ†æï¼Œå‘ç°äº†ä¸¤ä¸ªå…³é”®ç‰¹æ€§ï¼š

- **Intra-layer sensitivityï¼ˆå±‚å†…æ•æ„Ÿæ€§ï¼‰**ï¼šæŸäº›å±‚å¯¹è¿‘ä¼¼è¯¯å·®é«˜åº¦æ•æ„Ÿï¼Œå¿…é¡»ä¿ç•™ full attention ä»¥é˜²æ­¢ç‰¹å¾å¤±çœŸã€‚
- **Inter-layer similarityï¼ˆå±‚é—´ç›¸ä¼¼æ€§ï¼‰**ï¼šç›¸é‚»å±‚ä¹‹é—´å…±äº«å¤§é‡å…³é”®tokenï¼Œè¡¨æ˜æ³¨æ„åŠ›åˆ†å¸ƒå…·æœ‰å±€éƒ¨ç¨³å®šæ€§ã€‚

åŸºäºæ­¤ï¼ŒHyLRA è®¾è®¡äº†ä¸€ç§**ç¦»çº¿åŠ¨æ€è§„åˆ’ï¼ˆoffline dynamic programmingï¼‰ç®—æ³•**ï¼Œä¸ºæ¯ä¸€å±‚ç”Ÿæˆæœ€ä¼˜æ‰§è¡Œç­–ç•¥ï¼š
- åœ¨â€œæ•æ„Ÿå±‚â€æ‰§è¡Œæ ‡å‡† full attentionï¼›
- åœ¨â€œå®¹å¿å±‚â€ç›´æ¥å¤ç”¨å‰ä¸€ä¸ªæ•æ„Ÿå±‚çš„ top-k token ç´¢å¼•ï¼Œè·³è¿‡å†—ä½™çš„ token selection å’Œ softmax è®¡ç®—ã€‚

è¯¥æ–¹æ³•å®ç°äº†ï¼š
- **è®¡ç®—æ•ˆç‡æå‡**ï¼šé¿å…æ¯å±‚é‡å¤è®¡ç®— attention scoresï¼›
- **ç²¾åº¦ä¿éšœ**ï¼šåœ¨å…³é”®å±‚ä¿æŒ full precisionï¼›
- **çµæ´»æ€§**ï¼šæ”¯æŒ token-level å’Œ block-level sparsityã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | HyLRA | å…¶ä»–æ–¹æ³•ï¼ˆå¦‚ Jump3ã€H2Oã€StreamingLLMï¼‰ |
|------|-------|----------------------------------------|
| å±‚æ„ŸçŸ¥èƒ½åŠ› | âœ… æ˜¾å¼å»ºæ¨¡å±‚å¼‚è´¨æ€§ | âŒ å¤šæ•°ä¸º layer-agnostic |
| å†—ä½™è®¡ç®—æ¶ˆé™¤ | âœ… åˆ©ç”¨ inter-layer ç›¸ä¼¼æ€§å¤ç”¨ç´¢å¼• | âŒ æ¯å±‚ç‹¬ç«‹é€‰æ‹© top-k |
| KV Cache å®‰å…¨æ€§ | âœ… ä¸ä¸¢å¼ƒä»»ä½• KV ç¼“å­˜é¡¹ | âŒ éƒ¨åˆ†æ–¹æ³•æ°¸ä¹…åˆ é™¤ token å¯¼è‡´ä¸Šä¸‹æ–‡ä¸¢å¤± |
| æ€§èƒ½-æ•ˆç‡æƒè¡¡ | â­ è¾¾æˆæ›´ä¼˜çš„ Pareto frontier | âš ï¸ å¾€å¾€ç‰ºç‰²æ˜¾è‘—å‡†ç¡®ç‡æ¢å–é€Ÿåº¦ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **LongBench v2**ï¼ˆBai et al., 2024ï¼‰ï¼šç»¼åˆæ€§é•¿ä¸Šä¸‹æ–‡è¯„æµ‹åŸºå‡†ï¼Œæ¶µç›–å…­ç±»ä»»åŠ¡ï¼š
  - Single-document QA
  - Multi-document QA
  - Long-dialogue History Understanding
  - Long Structured Data Understanding
  - Long In-context Learning
  - Code Repository Understanding
- åŒ…å«å¤šç§éš¾åº¦ç­‰çº§ï¼ˆEasy / Hardï¼‰å’Œä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆShort / Medium / Longï¼‰ï¼Œæœ€å¤§å¯è¾¾ **60K tokens**

### æ¨¡å‹
- **DeepSeek-R1**ï¼šé«˜æ€§èƒ½æ¨ç†å¯¼å‘æ¨¡å‹
- **QWQ-32B-W8A8**ï¼šQwen æ¨¡å‹çš„é‡åŒ–ç‰ˆæœ¬ï¼ˆ8-bit weights & activationsï¼‰ï¼Œç”¨äºæµ‹è¯•å†…å­˜å—é™åœºæ™¯

### å®éªŒè®¾ç½®
- æ‰€æœ‰å®éªŒè¿è¡Œäºé…å¤‡ **8å—åŠ é€Ÿå™¨è®¾å¤‡** çš„é«˜æ€§èƒ½èŠ‚ç‚¹ï¼Œæ€»æ˜¾å­˜è¾¾ **768GB HBM3**
- ä¸Šä¸‹æ–‡é•¿åº¦èŒƒå›´ï¼šä» **8K åˆ° 60K tokens**
- Top-k é¢„ç®—å›ºå®šä¸º **2,048 tokens**

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Performance (Score)** | å„ä»»åŠ¡å¹³å‡å¾—åˆ†ï¼Œè¡¡é‡ç”Ÿæˆè´¨é‡ |
| **Time to First Token (TTFT)** | é¦–ä¸ª token ç”Ÿæˆå»¶è¿Ÿ |
| **Generation Throughput** | å•ä½æ—¶é—´ç”Ÿæˆ token æ•°é‡ï¼ˆtokens/sï¼‰ |
| **Overall Throughput** | åŒ…æ‹¬é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„æ•´ä½“ååé‡ |
| **Speedup** | ç›¸å¯¹äº full attention çš„åŠ é€Ÿæ¯” |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Full Attention**ï¼šæ ‡å‡† dense attentionï¼Œä½œä¸ºæ€§èƒ½ä¸Šé™
- **Jump3**ï¼šé™æ€è·³è·ƒç­–ç•¥ï¼Œæ¯3å±‚æ‰§è¡Œä¸€æ¬¡ full attentionï¼Œå…¶ä½™å±‚å¤ç”¨ç´¢å¼•ï¼ˆHyLRA çš„éè‡ªé€‚åº”å˜ä½“ï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 & Table 3ï¼‰

#### ğŸ“Š ç»¼åˆæ€§èƒ½æ¯”è¾ƒï¼ˆLongBench v2 å¹³å‡å¾—åˆ†ï¼‰
| Model | Method | Overall Score |
|-------|--------|----------------|
| DeepSeek-R1 | Full | 44.33 |
| DeepSeek-R1 | Jump3 | 42.54 |
| DeepSeek-R1 | **HyLRA** | **46.32** âœ… |
| QWQ-32B-W8A8 | Full | 45.73 |
| QWQ-32B-W8A8 | Jump3 | 40.76 |
| QWQ-32B-W8A8 | **HyLRA** | **45.03** âœ… |

> ğŸ’¡ **ç»“è®º**ï¼šHyLRA ä¸ä»…ä¼˜äºæ‰€æœ‰ sparse æ–¹æ³•ï¼Œç”šè‡³åœ¨å¤šä¸ªä»»åŠ¡ä¸Š**è¶…è¿‡ full attention**ï¼Œè¯´æ˜å…¶èƒ½æœ‰æ•ˆè¿‡æ»¤å™ªå£°å¹¶èšç„¦å…³é”®ä¸Šä¸‹æ–‡ã€‚

#### â± æ•ˆç‡è¡¨ç°ï¼ˆSequence Length = 60Kï¼‰
| æŒ‡æ ‡ | Full | Jump3 | **HyLRA** |
|------|------|--------|----------|
| TTFT (ms) | 57.69 | 53.98 | 56.62 |
| Generation Throughput | 177 | 273 | **258** |
| Overall Throughput | 2835 | 4730 | **4128** |
| Speedup vs Full | 1.00x | 1.54x | **1.45x** |

> ğŸ”º **åŠ é€Ÿæ•ˆæœéšåºåˆ—å¢é•¿è€Œå¢å¼º**ï¼šåœ¨ 8K æ—¶ä»…å¿« 6%ï¼Œä½†åœ¨ 60K æ—¶è¾¾åˆ° **1.45Ã— æ•´ä½“åŠ é€Ÿ**

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ç»“æœ
- **ç›¸æ¯” Jump3**ï¼š
  - åœ¨ç›¸åŒ top-k é¢„ç®—ä¸‹ï¼ŒHyLRA **å¹³å‡é«˜å‡º 3â€“5 åˆ†**ï¼›
  - è™½ç„¶ Jump3 ååæ›´é«˜ï¼ˆå› æ›´é¢‘ç¹è·³è¿‡ full attentionï¼‰ï¼Œä½†**ä»£ä»·æ˜¯æ˜¾è‘—æ€§èƒ½ä¸‹é™**ï¼›
  - HyLRA é€šè¿‡åŠ¨æ€è§„åˆ’æ‰¾åˆ°æ›´ä¼˜è·¯å¾„ï¼Œåœ¨**æ•ˆç‡ä¸å‡†ç¡®æ€§é—´å–å¾—æ›´å¥½å¹³è¡¡**ã€‚
- **ç›¸æ¯” full attention**ï¼š
  - å‡ ä¹æ— æŸæ€§èƒ½ï¼ˆ<1% accuracy degradationï¼‰ï¼›
  - å®ç°é«˜è¾¾ **46% çš„æ¨ç†ååæå‡**ï¼ˆè§æ‘˜è¦ï¼‰ã€‚

### æ¶ˆèå®éªŒï¼ˆéšå«åˆ†æï¼‰
å°½ç®¡æœªæ˜ç¡®åˆ—å‡ºæ¶ˆèè¡¨ï¼Œæ–‡ä¸­é€šè¿‡ä»¥ä¸‹æ–¹å¼éªŒè¯è®¾è®¡æœ‰æ•ˆæ€§ï¼š
- **æ•æ„Ÿæ€§åˆ†æå›¾ï¼ˆFigure 1ï¼‰**ï¼šè¯æ˜ Layer 0 å¯¹ç¨€ç–åŒ–æåº¦æ•æ„Ÿï¼ˆRNMSE=0.25ï¼‰ï¼Œè€Œ Layer 7 å¯å®‰å…¨ç¨€ç–ï¼ˆRNMSE=0.016ï¼‰â†’ æ”¯æŒ layer-adaptive ç­–ç•¥å¿…è¦æ€§ã€‚
- **å±‚é—´é‡å çƒ­åŠ›å›¾ï¼ˆFigure 2ï¼‰**ï¼šæ˜¾ç¤ºè¿ç»­å±‚é—´ top-k token é‡å ç‡é«˜ â†’ æ”¯æŒ index reuse åˆç†æ€§ã€‚
- **åŠ¨æ€è§„åˆ’è·¯å¾„å¯è§†åŒ–ï¼ˆFigure 3ï¼‰**ï¼šå±•ç¤ºå¦‚ä½•é€šè¿‡ reset jump å’Œ vertical move æ„å»ºæœ€ä¼˜ç­–ç•¥ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **LLM å„å±‚å¯¹æ³¨æ„åŠ›ç¨€ç–åŒ–çš„å®¹å¿åº¦å­˜åœ¨æ˜¾è‘—å·®å¼‚**ï¼š
   - æµ…å±‚ï¼ˆå¦‚ Layer 0ï¼‰é€šå¸¸æ˜¯æ•æ„Ÿå±‚ï¼Œéœ€ full attentionï¼›
   - æ·±å±‚å¤šä¸ºå®¹å¿å±‚ï¼Œå¯é«˜æ•ˆå¤ç”¨å‰å±‚ç´¢å¼•ã€‚

2. **æ³¨æ„åŠ›æ¨¡å¼åœ¨ç›¸é‚»å±‚é—´å…·æœ‰å¼ºä¸€è‡´æ€§**ï¼š
   - top-k token é›†åˆé«˜åº¦é‡å ï¼Œæ”¯æŒè·¨å±‚ç´¢å¼•å¤ç”¨ã€‚

3. **ç»Ÿä¸€ç¨€ç–ç­–ç•¥æ˜¯æ¬¡ä¼˜çš„**ï¼š
   - å›ºå®šé—´éš”è·³è¿‡ï¼ˆå¦‚ Jump3ï¼‰æ— æ³•é€‚åº”æ¨¡å‹å†…éƒ¨å¼‚æ„æ€§ï¼›
   - HyLRA çš„ profile-driven åŠ¨æ€ç­–ç•¥èƒ½å®ç°å…¨å±€æœ€ä¼˜é…ç½®ã€‚

4. **æ— éœ€ä¸¢å¼ƒ KV cache å³å¯å®ç°é«˜æ•ˆæ¨ç†**ï¼š
   - ä¼ ç»Ÿæ–¹æ³•ä¾èµ– cache evictionï¼Œé£é™©é«˜ï¼›
   - HyLRA ä¿ç•™å®Œæ•´ KV cacheï¼Œä»…å‡å°‘è®¿é—®é‡ï¼Œæ›´å®‰å…¨å¯é ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–ç¦»çº¿ profiling**ï¼šéœ€è¦é¢„å…ˆè¿è¡Œ full attention è·å–å„å±‚ attention åˆ†å¸ƒï¼Œå¢åŠ éƒ¨ç½²å‰æˆæœ¬ï¼›
- **å‡è®¾ attention pattern ç¨³å®š**ï¼šè‹¥è¾“å…¥å¼•èµ·å‰§çƒˆå˜åŒ–ï¼Œreuse å¯èƒ½å¤±æ•ˆï¼›
- **ä¸é€‚ç”¨äºæçŸ­ä¸Šä¸‹æ–‡**ï¼šåœ¨ 8K ä»¥ä¸‹ä¼˜åŠ¿ä¸æ˜æ˜¾ï¼Œæ›´é€‚åˆè¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ>30Kï¼‰åœºæ™¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **åœ¨çº¿è‡ªé€‚åº”ç­–ç•¥**ï¼šç»“åˆè¿è¡Œæ—¶åé¦ˆåŠ¨æ€è°ƒæ•´ reuse ç­–ç•¥ï¼Œå‡å°‘å¯¹ç¦»çº¿ profile çš„ä¾èµ–ï¼›
- **æ‰©å±•è‡³ vision-language æ¨¡å‹**ï¼šæ¢ç´¢å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„è·¨å±‚æ³¨æ„åŠ›å¤ç”¨ï¼›
- **ç¡¬ä»¶ååŒä¼˜åŒ–**ï¼šè¿›ä¸€æ­¥ç»“åˆ GPU memory hierarchyï¼Œæå‡ block-level reuse çš„å¸¦å®½åˆ©ç”¨ç‡ï¼›
- **æ”¯æŒè®­ç»ƒé˜¶æ®µåº”ç”¨**ï¼šç ”ç©¶æ˜¯å¦å¯åœ¨è®­ç»ƒä¸­å¼•å…¥ç±»ä¼¼æœºåˆ¶ä»¥é™ä½å¼€é”€ã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **HyLRA é€šè¿‡æ´å¯Ÿ LLM ä¸­ attention çš„å±‚å†…æ•æ„Ÿæ€§ä¸å±‚é—´ç›¸ä¼¼æ€§ï¼Œæå‡ºä¸€ç§ hybrid layer-wise reuse æœºåˆ¶ï¼Œåœ¨å‡ ä¹ä¸æŸå¤±ç²¾åº¦çš„å‰æä¸‹ï¼Œå®ç°é«˜è¾¾ 1.45Ã— çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†åŠ é€Ÿï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰ sparse attention æ–¹æ³•ã€‚**

</details>

---

### 8. [PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching](https://arxiv.org/abs/2602.00509)

**Authors**: Qianchao Zhu, Xucheng Ye, Yuliang Liu, Haodong Ouyang, Chengru Song  
**Category**: cs.DC  
**Published**: 2026-02-03  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.00509v1  

#### Abstract
Mixture-of-Experts models have become a dominant architecture for scaling Large Language Models by activating only a sparse subset of experts per token. However, latency-critical MoE inference faces a fundamental tension: while expert parallelism improves memory efficiency, it also amplifies executi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šPROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching**

---

## 1. **è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### âœ… **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

Mixture-of-Experts (MoE) æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µé¢ä¸´ä¸¥é‡çš„**è®¡ç®—ä¸é€šä¿¡åŒé‡ç“¶é¢ˆ**ï¼Œå°¤å…¶æ˜¯åœ¨å»¶è¿Ÿæ•æ„Ÿçš„åœ¨çº¿æœåŠ¡åœºæ™¯ä¸­ï¼š

- **ç©ºé—´ä¸å¹³è¡¡ï¼ˆSpatial Imbalanceï¼‰**ï¼šç”±äºè¾“å…¥è¯­ä¹‰å·®å¼‚ï¼ŒæŸäº›ä¸“å®¶è¢«é¢‘ç¹æ¿€æ´»ï¼Œå½¢æˆâ€œçƒ­ç‚¹â€ï¼ˆhotspotsï¼‰ï¼Œå¯¼è‡´éƒ¨åˆ† GPU è´Ÿè½½è¿‡é‡ã€‚
- **æ—¶é—´æ³¢åŠ¨æ€§ï¼ˆTemporal Volatilityï¼‰**ï¼šè¿ç»­æ‰¹å¤„ç†ï¼ˆcontinuous batchingï¼‰ä¸‹è¯·æ±‚åŠ¨æ€åŠ å…¥å’Œé€€å‡ºï¼Œä½¿å¾—ä¸“å®¶è´Ÿè½½åˆ†å¸ƒå¿«é€Ÿå˜åŒ–ï¼Œé™æ€æˆ–åŸºäºå†å²ç»Ÿè®¡çš„å¹³è¡¡ç­–ç•¥å¤±æ•ˆã€‚
- **åŒæƒ©ç½šæ•ˆåº”ï¼ˆDouble Penaltyï¼‰**ï¼šçƒ­ç‚¹ GPU åŒæ—¶æ‰¿å—**è®¡ç®—è´Ÿè½½è¿‡é«˜**å’Œ**All-to-All é€šä¿¡æ‹¥å¡**ï¼Œä¸¥é‡æ‹–æ…¢æ•´ä½“æ¨ç†é€Ÿåº¦ã€‚

ç°æœ‰æ–¹æ³•å¦‚ä¸“å®¶å¤åˆ¶ï¼ˆexpert replicationï¼‰ã€åŠ¨æ€å¸è½½ï¼ˆdynamic offloadingï¼‰ç­‰å­˜åœ¨ä»¥ä¸‹ç¼ºé™·ï¼š
- å†…å­˜å¼€é”€å¤§ï¼Œä¸ KV Cache ç«äº‰ï¼›
- ååº”å¼è°ƒæ•´æ»åäºè´Ÿè½½çªå˜ï¼›
- æ§åˆ¶å¼€é”€æš´éœ²åœ¨å…³é”®è·¯å¾„ä¸Šï¼Œç ´å CUDA Graph åŠ é€Ÿï¼›
- éœ€è¦è®­ç»ƒæ—¶ä¿®æ”¹è·¯ç”±æœºåˆ¶ï¼Œè¿åè¯­ä¹‰æ­£ç¡®æ€§ã€‚

---

### ğŸš€ **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡º **PROBE** â€”â€” ä¸€ç§é¢å‘ MoE æ¨ç†çš„å®æ—¶ååŒä¼˜åŒ–ç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒæ˜¯ **Continuous Lookahead Pipelining** æ¶æ„ï¼Œå°†æ§åˆ¶æµï¼ˆé¢„æµ‹ã€è§„åˆ’ã€é¢„å–ï¼‰å®Œå…¨éšè—åœ¨ä¸»æ‰§è¡Œæµä¹‹å¤–ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

1. **Gate-Initialized Lookahead Predictorï¼ˆé—¨æ§åˆå§‹åŒ–å‰å‘é¢„æµ‹å™¨ï¼‰**
   - åˆ©ç”¨ç›®æ ‡å±‚å†»ç»“çš„ router å‚æ•°ä½œä¸ºå…ˆéªŒï¼Œç»“åˆè½»é‡çº§å¯è®­ç»ƒæ®‹å·® MLPï¼ŒåŸºäºå‰ä¸€å±‚éšè—çŠ¶æ€é¢„æµ‹ä¸‹ä¸€å±‚ä¸“å®¶æ¿€æ´»æ¨¡å¼ã€‚
   - å®ç°çº¦ **90% çš„ Top-K å‡†ç¡®ç‡**ï¼Œä¸”å¼€é”€æä½ã€‚

2. **Hardware-Aware Balance Planning Solverï¼ˆç¡¬ä»¶æ„ŸçŸ¥è´Ÿè½½å‡è¡¡è§„åˆ’å™¨ï¼‰**
   - å°†ä¸“å®¶å¤åˆ¶å†³ç­–å»ºæ¨¡ä¸ºèµ„æºåˆ†é…é—®é¢˜ï¼Œåœ¨æ»¡è¶³â€œéšè—çª—å£â€çº¦æŸçš„å‰æä¸‹ï¼ŒåŠ¨æ€å†³å®šæ˜¯å¦å¤åˆ¶åŠå¦‚ä½•åˆ†é… tokenã€‚
   - å¼•å…¥è´ªå¿ƒå†å¹³è¡¡ç®—æ³•ï¼ˆGreedy Rebalancingï¼‰ï¼Œç¡®ä¿ä¼ è¾“æ—¶é—´ä¸è¶…è¿‡å½“å‰å±‚è®¡ç®—æ—¶é—´ï¼Œé¿å…é˜»å¡å…³é”®è·¯å¾„ã€‚

3. **Phase-Locked Co-Scheduling Policyï¼ˆç›¸ä½é”å®šååŒè°ƒåº¦ç­–ç•¥ï¼‰**
   - è®¾è®¡åŒè½¨æ‰§è¡Œæ¶æ„ï¼ˆDeterministic Track + Control Planeï¼‰ï¼š
     - **é¢„æµ‹ä¸è§„åˆ’é˜¶æ®µ**ï¼šåœ¨ All-to-All é€šä¿¡æœŸé—´å¹¶è¡Œè¿è¡Œï¼›
     - **ä¸“å®¶é¢„å–é˜¶æ®µ**ï¼šé€šè¿‡ split-phase transmission æŠ€æœ¯ï¼Œåˆ©ç”¨ MoE è®¡ç®—å’Œ Attention é˜¶æ®µçš„å¸¦å®½ç©ºçª—è¿›è¡Œ P2P æƒé‡ä¼ è¾“ï¼Œé¿å¼€ All-to-All é€šä¿¡é«˜å³°ã€‚
   - å®Œå…¨æ¶ˆé™¤æ§åˆ¶å¼€é”€å¯¹ä¸»æµç¨‹çš„å½±å“ï¼Œå¹¶ä¿è¯ä¸ CUDA Graph å…¼å®¹ã€‚

---

### ğŸ” **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ DeepSeek-EPLB, Grace-MoEï¼‰ | PROBE |
|------|----------------------------------------|-------|
| **å“åº”æ–¹å¼** | ååº”å¼ï¼ˆä¾èµ–å†å²ç»Ÿè®¡ï¼‰ | **å‰å‘å¼ï¼ˆå®æ—¶é¢„æµ‹ï¼‰** |
| **æ›´æ–°é¢‘ç‡** | å•æ¬¡æˆ–ä½é¢‘é‡å¹³è¡¡ | **æ¯å±‚æŒç»­é¢„æµ‹ä¸æ›´æ–°** |
| **å†…å­˜æ•ˆç‡** | å›ºå®šå†—ä½™æ§½ä½ï¼Œæµªè´¹æ˜¾å­˜ | **åŠ¨æ€å¤ç”¨ slotï¼Œå¾ªç¯åˆ©ç”¨** |
| **å…³é”®è·¯å¾„å½±å“** | å­˜åœ¨æš´éœ²å¼€é”€ï¼Œç ´åå›¾æ•è· | **é›¶æš´éœ²å¼€é”€ï¼Œå…¼å®¹ CUDA Graph** |
| **é€‚åº”èƒ½åŠ›** | å¯¹çªå‘è¯­ä¹‰æ¼‚ç§»ä¸é²æ£’ | **é«˜åŠ¨æ€è´Ÿè½½ä¸‹ä»ç¨³å®šé«˜æ•ˆ** |

---

## 2. **æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### ğŸ“š **ä½¿ç”¨çš„æ•°æ®é›†**

- **Chinese**ï¼šæ¥è‡ª Alpaca-zh ç­‰å¼€æºä¸­æ–‡è¯­æ–™ï¼›
- **Code**ï¼šæ¥è‡ª CodeAlpaca20k å’Œ OpenAI-HumanEvalï¼›
- **Repeatï¼ˆåˆæˆæ•°æ®é›†ï¼‰**ï¼šé‡å¤å°‘é‡ prompt ä»¥æ¨¡æ‹Ÿæç«¯ä¸“å®¶å€¾æ–œåœºæ™¯ï¼Œç”¨äºå‹åŠ›æµ‹è¯•ã€‚

---

### âš™ï¸ **å®éªŒè®¾ç½®**

- **ç¡¬ä»¶å¹³å°**ï¼š8Ã—NVIDIA Hopper GPUï¼ˆH100çº§åˆ«ï¼‰ï¼ŒNVSwitch äº’è”ï¼ˆ900 GB/sï¼‰ï¼›
- **æ¨¡å‹é…ç½®**ï¼š
  - **GPT-OSS-120B**ï¼š128 ä¸ªä¸“å®¶ï¼ŒTop-4 æ¿€æ´»ï¼ŒBF16ï¼›
  - **Qwen3-MoE-235B**ï¼š128 ä¸ªä¸“å®¶ï¼ŒTop-8 æ¿€æ´»ï¼ŒBF16ï¼›
- **å¹¶è¡Œç­–ç•¥**ï¼šAttention ä½¿ç”¨ DPï¼ŒMoE å±‚ä½¿ç”¨ EPï¼ˆExpert Parallelismï¼‰ï¼Œ`ep=8`ï¼›
- **å®ç°æ¡†æ¶**ï¼šåŸºäº SGLang + DeepEP æ„å»ºï¼Œæ”¯æŒ CUDA Graphï¼›

---

### ğŸ“Š **è¯„ä¼°æŒ‡æ ‡**

| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Prefill Latency (TTFT)** | Time-To-First-Tokenï¼Œè¡¡é‡é¦– token å»¶è¿Ÿ |
| **Decoding Throughput** | è§£ç é˜¶æ®µå¹³å‡ååé‡ï¼ˆtokens/sï¼‰ |
| **Imbalance Ratio (IR)** | æœ€å¤§è´Ÿè½½ / å¹³å‡è´Ÿè½½ï¼Œåæ˜ è´Ÿè½½åæ–œç¨‹åº¦ |
| **End-to-End Latency Breakdown** | å¾®æ“ä½œçº§æ—¶é—´çº¿åˆ†æï¼ˆDispatch, Compute, Combine, Prefetch ç­‰ï¼‰ |
| **Predictor Accuracy** | Top-Kã€Top-Half-K Hit Rateã€Recall@2Ã— ç­‰ |

---

### ğŸ†š **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| åŸºçº¿ | ç‰¹ç‚¹ |
|------|------|
| **SGLang (Default EP)** | é™æ€åˆ†ç‰‡æ”¾ç½®ï¼Œæ— åŠ¨æ€å¹³è¡¡ |
| **DeepSeek-EPLB** | åŸºäºå†å²ç»Ÿè®¡çš„è´Ÿè½½å‡è¡¡ï¼Œæ¯å±‚é¢„ç•™ 2 ä¸ªå†—ä½™ slotï¼Œä¸€æ¬¡é‡å¹³è¡¡ |

---

## 3. **ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### ğŸ“ˆ **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… **Prefill é˜¶æ®µåŠ é€Ÿæ•ˆæœ**

- åœ¨ `GPT-OSS-120B` ä¸Šï¼ŒPROBE ç›¸æ¯” SGLang æœ€å¤šé™ä½ **32% çš„ TTFT**ï¼ˆå³ **1.32Ã— æ›´å¿«**ï¼‰ï¼›
- åœ¨é«˜åæ–œçš„ `Repeat` æ•°æ®é›†ä¸Šä¼˜åŠ¿æ›´æ˜æ˜¾ï¼Œå› èƒ½åŠæ—¶åº”å¯¹çªå‘çƒ­ç‚¹ï¼›
- EPLB æœªå‚ä¸æ­¤æ¯”è¾ƒï¼Œå› å…¶åœ¨ Prefill é˜¶æ®µæ˜“è§¦å‘ OOM æˆ–æ— æ³•å®Œæˆé‡å¹³è¡¡ã€‚

#### âœ… **Decoding é˜¶æ®µååæå‡**

- ç›¸æ¯” EPLBï¼Œåœ¨ç›¸åŒ batch size ä¸‹æœ€é«˜æå‡ **26% ååé‡**ï¼ˆ**1.26Ã—**ï¼‰ï¼›
- åœ¨ `Repeat` æ•°æ®é›†ä¸­ï¼ŒEPLB å‡ºç°æ˜¾è‘—å»¶è¿Ÿå°–å³°ï¼Œè€Œ PROBE ä¿æŒå¹³ç¨³ï¼›
- åœ¨ `Chinese` å’Œ `Code` åˆ‡æ¢çš„å‹åŠ›æµ‹è¯•ä¸­ï¼ŒPROBE æ— éœ€ warm-up å³ç»´æŒé«˜æ€§èƒ½ã€‚

#### âœ… **è´Ÿè½½å‡è¡¡æ”¹å–„**

- å¹³å‡ Imbalance Ratio ä» **2.13 â†’ 1.09**ï¼›
- è®¡ç®—å»¶è¿Ÿçš„æœ€å¤§/å¹³å‡æ¯”ä» **2.27 â†’ 1.18**ï¼Œæ˜¾è‘—å‡å°‘åŒæ­¥ç­‰å¾…æ—¶é—´ï¼›
- Combine é˜¶æ®µå»¶è¿Ÿä¸‹é™ä¸»è¦æºäº straggler æ¶ˆé™¤ï¼Œè€Œéé€šä¿¡æœ¬èº«åŠ é€Ÿã€‚

#### âœ… **é¢„æµ‹å™¨ç²¾åº¦**

- Top-K å‡†ç¡®ç‡è¾¾ **87â€“94%**ï¼Œè¿œé«˜äºä»…ä½¿ç”¨å†»ç»“ router çš„ ~75%ï¼›
- Top-Half-K Hit Rate å’Œ Recall@2Ã— æ¥è¿‘ **100%**ï¼Œè¡¨æ˜é¢„æµ‹é«˜åº¦å¯é ï¼Œæå°‘æ¼æŠ¥é‡è¦ä¸“å®¶ã€‚

#### âœ… **æ§åˆ¶å¼€é”€éšè—éªŒè¯**

- å›¾ 11 æ˜¾ç¤ºï¼š**Predictã€Planã€Prefetch å…¨éƒ¨è¢«æˆåŠŸéšè—**åœ¨éå…³é”®èµ„æºå‘¨æœŸå†…ï¼š
  - Predict + Gather ä¸ Dispatch å¹¶è¡Œï¼›
  - Planner ä¸ MoE Compute é‡å ï¼›
  - Prefetch åˆ†ä¸¤é˜¶æ®µåµŒå…¥ MoE Compute å’Œä¸‹ä¸€å±‚ Attention ä¸­ï¼›
- æ•´ä½“æœªå¼•å…¥é¢å¤– idle æ—¶é—´ã€‚

---

### ğŸ”¬ **æ¶ˆèå®éªŒä¸å…³é”®è®¾è®¡éªŒè¯**

è™½ç„¶æ–‡ä¸­æœªæ˜ç¡®åˆ—å‡ºç‹¬ç«‹æ¶ˆèè¡¨æ ¼ï¼Œä½†ä»å¤šä¸ªå®éªŒå¯æ¨æ–­å‡ºå„ç»„ä»¶å¿…è¦æ€§ï¼š

| ç»„ä»¶ | ç§»é™¤åæœ |
|------|---------|
| **Lookahead Predictor** | è‹¥ä¾èµ–å†å²ç»Ÿè®¡ï¼ˆå¦‚ EPLBï¼‰ï¼Œåœ¨è¯­ä¹‰åˆ‡æ¢åæ€§èƒ½éª¤é™ï¼ˆè§ Fig.9ï¼‰ |
| **Hardware-Aware Planning** | è‹¥ä¸é™åˆ¶ transfer volumeï¼Œä¼šè¶…å‡º hiding windowï¼Œé€ æˆé˜»å¡ |
| **Split-Phase Prefetching** | è‹¥åœ¨ All-to-All æœŸé—´ä¼ è¾“æƒé‡ï¼Œå°†åŠ å‰§ç½‘ç»œæ‹¥å¡ |
| **Online Distillation** | ä»…ç”¨å†»ç»“ router å‡†ç¡®ç‡ä¸‹é™çº¦ 15%ï¼Œå¯¼è‡´æ— æ•ˆé¢„å–å¢å¤š |

---

## 4. **å…³é”®ç»“è®ºå’Œå‘ç°**

### âœ… **ä¸»è¦å‘ç°**

1. **MoE æ¨ç†ç“¶é¢ˆæœ¬è´¨æ˜¯â€œæ—¶ç©ºè€¦åˆâ€çš„è´Ÿè½½å¤±è¡¡**ï¼š
   - ä¸ä»…æ˜¯é™æ€åæ–œï¼Œæ›´æ˜¯**è¯­ä¹‰é©±åŠ¨ä¸‹çš„é«˜é¢‘è¿ç§»çƒ­ç‚¹**ï¼›
   - å¿…é¡»åŒæ—¶è§£å†³ç©ºé—´ straggler å’Œæ—¶é—´ volatilityã€‚

2. **å‰å‘é¢„æµ‹ä¼˜äºååº”å¼è°ƒæ•´**ï¼š
   - åˆ©ç”¨ Transformer å±‚é—´éšè—çŠ¶æ€è¿ç»­æ€§ï¼Œå¯ä»¥é«˜ç²¾åº¦é¢„æµ‹ä¸‹ä¸€ä¸“å®¶åˆ†å¸ƒï¼›
   - â€œé¢„æµ‹ + è§„åˆ’ + é¢„å–â€æµæ°´çº¿å¯åœ¨ä¸å½±å“ä¸»æµç¨‹å‰æä¸‹å®ç°è¿‘å®æ—¶é‡é…ç½®ã€‚

3. **çœŸæ­£çš„é›¶å¼€é”€å¹³è¡¡éœ€è½¯ç¡¬ååŒè®¾è®¡**ï¼š
   - å¿…é¡»ç»“åˆç¡¬ä»¶ç‰¹æ€§ï¼ˆcompute/bandwidth ratioï¼‰æ¥é™åˆ¶å¤åˆ¶è§„æ¨¡ï¼›
   - è°ƒåº¦å¿…é¡»ä¸æ‰§è¡Œç›¸ä½å¯¹é½ï¼Œæ‰èƒ½å®ç°å®Œå…¨éšè—ã€‚

4. **CUDA Graph å…¼å®¹æ€§è‡³å…³é‡è¦**ï¼š
   - åŠ¨æ€é€»è¾‘è‹¥å¼•å…¥ host-device åŒæ­¥æˆ–å˜é‡æ§åˆ¶æµï¼Œå°†ç ´åå›¾æ•è·ï¼›
   - PROBE é€šè¿‡ GPU-native solver å’Œå›ºå®š kernel pattern æˆåŠŸè§„é¿ã€‚

---

### âš ï¸ **æ–¹æ³•çš„å±€é™æ€§**

1. **ä¾èµ– router å¯å¯¼å‡ºä¸å†»ç»“**ï¼š
   - éœ€è®¿é—®åŸå§‹æ¨¡å‹çš„ gate å‚æ•°ï¼Œå¯èƒ½ä¸é€‚ç”¨äºé»‘ç›’éƒ¨ç½²åœºæ™¯ã€‚

2. **é¢„æµ‹è¯¯å·®ä»å­˜åœ¨**ï¼š
   - è™½ç„¶ Top-K å‡†ç¡®ç‡é«˜ï¼Œä½†åœ¨æç«¯åˆ†å¸ƒæ¼‚ç§»ä¸‹ä»æœ‰è¯¯åˆ¤é£é™©ï¼›
   - å½“å‰æ–¹æ¡ˆæœªå¼•å…¥ fallback æœºåˆ¶å¤„ç†é¢„æµ‹å¤±è´¥ã€‚

3. **æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡é›†ç¾¤å°šå¾…éªŒè¯**ï¼š
   - å®éªŒé™äºå•èŠ‚ç‚¹ 8 GPUï¼Œè·¨èŠ‚ç‚¹ P2P ä¼ è¾“å»¶è¿Ÿæ›´é«˜ï¼Œå¯èƒ½å‹ç¼© hiding windowã€‚

4. **ä»…é’ˆå¯¹æ¨ç†ä¼˜åŒ–ï¼Œæœªè¦†ç›–è®­ç»ƒåœºæ™¯**ã€‚

---

### ğŸ”® **æœªæ¥å·¥ä½œæ–¹å‘**

1. **å°† lookahead æ‰§è¡Œæ¨å¹¿è‡³å…¶ä»–å­ç³»ç»Ÿ**ï¼š
   - å¦‚æå‰é¢„åŠ è½½ KV Cache åˆ†å¸ƒã€é¢„åˆ†é…å†…å­˜å—ç­‰ï¼›
   - æ„å»ºç»Ÿä¸€çš„â€œé¢„æµ‹é©±åŠ¨æ‰§è¡Œâ€èŒƒå¼ã€‚

2. **ç»“åˆ disaggregation æ¶æ„**ï¼š
   - åœ¨ä¸“å®¶åˆ†ç¦»å­˜å‚¨ï¼ˆdisaggregated storageï¼‰ç³»ç»Ÿä¸­ï¼Œæå‰æ‹‰å–è¿œç¨‹ä¸“å®¶å‚æ•°ã€‚

3. **å¢å¼ºé¢„æµ‹é²æ£’æ€§**ï¼š
   - å¼•å…¥ä¸ç¡®å®šæ€§ä¼°è®¡æˆ–è‡ªé€‚åº”æ ¡å‡†æ¨¡å—ï¼ŒåŠ¨æ€è°ƒæ•´å¤åˆ¶æ¿€è¿›ç¨‹åº¦ã€‚

4. **æ”¯æŒæ›´å¤š MoE å˜ä½“**ï¼š
   - å¦‚ Hierarchical MoEã€Soft MoEã€Routing Networks with Memory ç­‰å¤æ‚è·¯ç”±ç»“æ„ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> PROBE é€šè¿‡ **real-time predictive prefetching + phase-locked co-scheduling**ï¼Œé¦–æ¬¡å®ç°äº†åœ¨ä¸å½±å“å…³é”®è·¯å¾„çš„å‰æä¸‹ï¼Œå¯¹ MoE æ¨ç†ä¸­çš„è®¡ç®—ä¸é€šä¿¡è¿›è¡Œ**å®æ—¶ååŒå¹³è¡¡**ï¼Œæ˜¾è‘—æå‡äº†å»¶è¿Ÿå’Œååæ€§èƒ½ï¼Œå°¤å…¶åœ¨é«˜åŠ¨æ€è´Ÿè½½ä¸‹è¡¨ç°å“è¶Šï¼Œä¸ºä¸‡äº¿å‚æ•° MoE æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 9. [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)

**Authors**: Yang Liu, Chuan Zhou, Yancheng Chen, Shuai Zhang, Xixun Lin, Xiaoqing Wang  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.01090v1  

#### Abstract
Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šHard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»„åˆä¼˜åŒ–ï¼ˆCombinatorial Optimization, COï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨¡å¼è¯†åˆ«å’Œåºåˆ—ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶æœ¬è´¨æ˜¯**æ— çº¦æŸçš„ç”Ÿæˆæ¨¡å‹**ï¼Œæ— æ³•ä¿è¯è¾“å‡ºæ»¡è¶³ç¡¬æ€§çº¦æŸæ¡ä»¶ï¼ˆå¦‚å®¹é‡é™åˆ¶ã€å“ˆå¯†é¡¿è·¯å¾„ã€ç‹¬ç«‹é›†ç­‰ï¼‰ã€‚è¿™å¯¼è‡´ç”Ÿæˆçš„è§£å¸¸å¸¸ä¸å¯è¡Œï¼ˆinfeasibleï¼‰ï¼Œä¸¥é‡åˆ¶çº¦äº†å…¶åœ¨ç‰©æµã€åˆ¶é€ ã€åº”æ€¥è°ƒåº¦ç­‰å®‰å…¨æ•æ„Ÿé¢†åŸŸçš„å®é™…éƒ¨ç½²ã€‚

ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¥–åŠ±å¡‘å½¢ï¼ˆreward shapingï¼‰æˆ–åå¤„ç†é‡‡æ ·ï¼ˆrejection samplingï¼‰æ¥â€œé¼“åŠ±â€å¯è¡Œæ€§ï¼Œä½†è¿™äº›æ–¹æ³•åªèƒ½æä¾›æ¦‚ç‡æ€§ä¿éšœï¼Œ**æ— æ³•å®ç°100%å¯è¡Œæ€§**ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ï¼šFALCON**
æœ¬æ–‡æå‡ºäº† **FALCON**ï¼ˆFeasibility-Aware Language-based Combinatorial Optimization with Adaptive Inferenceï¼‰ï¼Œé¦–ä¸ªèƒ½**ç†è®ºä¿è¯100%å¯è¡Œæ€§**çš„LLM-based COæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸‰å±‚ååŒæœºåˆ¶ï¼š

1. **Grammar-Constrained Decodingï¼ˆè¯­æ³•çº¦æŸè§£ç ï¼‰**  
   - ä½¿ç”¨**ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•**ï¼ˆCFGï¼‰å®šä¹‰æ¯ä¸ªCOé—®é¢˜çš„æœ‰æ•ˆè¾“å‡ºæ ¼å¼ï¼ˆå¦‚æ‹¬å·åŒ¹é…ã€èŠ‚ç‚¹ç´¢å¼•èŒƒå›´ï¼‰ã€‚
   - åœ¨è§£ç æ—¶é€šè¿‡**ä¸‹æ¨è‡ªåŠ¨æœº**ï¼ˆPDAï¼‰åŠ¨æ€å±è”½éæ³•tokenï¼Œç¡®ä¿è¾“å‡º**è¯­æ³•æ­£ç¡®**ï¼ˆsyntactically validï¼‰ã€‚

2. **Feasibility Repair Layerï¼ˆå¯è¡Œæ€§ä¿®å¤å±‚ï¼‰**  
   - è®¾è®¡**é—®é¢˜ç‰¹å®šçš„ä¿®å¤ç®—å­**ï¼ˆrepair operatorï¼‰ï¼Œå°†ä»»ä½•è¿åè¯­ä¹‰çº¦æŸçš„è§£æ˜ å°„åˆ°å¯è¡ŒåŸŸã€‚
   - ä¿®å¤ç®—å­æ»¡è¶³ä¸‰å¤§æ€§è´¨ï¼š
     - **å¯è¡Œæ€§**ï¼ˆFeasibilityï¼‰ï¼šè¾“å‡ºå¿…ä¸ºå¯è¡Œè§£ã€‚
     - **å¹‚ç­‰æ€§**ï¼ˆIdempotenceï¼‰ï¼šè¾“å…¥å·²å¯è¡Œåˆ™ä¸ä¿®æ”¹ã€‚
     - **æœ‰ç•Œå±€éƒ¨æ€§**ï¼ˆBounded Localityï¼‰ï¼šä¿®å¤ä»£ä»·ä¸è¿åç¨‹åº¦æˆæ­£æ¯”ã€‚
   - ä¾‹å¦‚ï¼šTSPä¸­ç§»é™¤é‡å¤èŠ‚ç‚¹å¹¶æ’å…¥ç¼ºå¤±èŠ‚ç‚¹ï¼›CVRPä¸­æ‹†åˆ†è¶…è½½è·¯çº¿ã€‚

3. **Adaptive Best-of-N Samplingï¼ˆè‡ªé€‚åº”å¤šé‡‡æ ·ï¼‰**  
   - åŠ¨æ€è°ƒæ•´é‡‡æ ·æ•°é‡ $N$ï¼ŒåŸºäº**è§£ä¸€è‡´æ€§**ï¼ˆsolution consistencyï¼‰ä¼°è®¡å®ä¾‹éš¾åº¦ã€‚
   - é«˜ä¸€è‡´æ€§ï¼ˆæ¨¡å‹è‡ªä¿¡ï¼‰â†’ å°‘é‡‡æ ·æ—©ç»ˆæ­¢ï¼›ä½ä¸€è‡´æ€§ï¼ˆæ¨¡å‹ä¸ç¡®å®šï¼‰â†’ å¤šé‡‡æ ·æ¢ç´¢ã€‚
   - æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚

æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†æ–°çš„è®­ç»ƒç®—æ³• **BOPO**ï¼ˆBest-anchored Objective-guided Preference Optimizationï¼‰ï¼š
- åˆ©ç”¨**ç›®æ ‡å€¼å·®è·**ï¼ˆobjective gapï¼‰åŠ æƒåå¥½å¯¹ï¼Œæä¾›æ›´å¯†é›†çš„ç›‘ç£ä¿¡å·ã€‚
- ä¼˜äºä¼ ç»ŸRLHFæˆ–GRPOæ–¹æ³•ï¼Œæ”¶æ•›æ›´å¿«ä¸”è´¨é‡æ›´é«˜ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ç°æœ‰æ–¹æ³• | FALCON |
|------|--------|--------|
| **å¯è¡Œæ€§ä¿éšœ** | è½¯çº¦æŸï¼Œ<100% | **ç†è®ºä¿è¯100%** |
| **è§£ç æ§åˆ¶** | æ— æˆ–ç®€å•mask | **CFG + PDAç²¾ç¡®æ§åˆ¶** |
| **è®­ç»ƒä¿¡å·å¯†åº¦** | ç¨€ç–å¥–åŠ±æˆ–å¹³å‡ä¼˜åŠ¿ | **ç›®æ ‡å·®è·åŠ æƒï¼Œä¿¡æ¯æ›´ä¸°å¯Œ** |
| **æ¨ç†æ•ˆç‡** | å›ºå®šé‡‡æ ·æ•° | **è‡ªé€‚åº”åˆ†é…è®¡ç®—èµ„æº** |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
åœ¨**ä¸ƒä¸ªNP-hard COé—®é¢˜**ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¦†ç›–ä¸‰å¤§é¢†åŸŸï¼š

| é—®é¢˜ | ç±»å‹ | è§„æ¨¡ | åˆ†å¸ƒ | å‚è€ƒæ±‚è§£å™¨ |
|------|------|------|------|------------|
| TSP | è·¯ç”± | 10â€“100èŠ‚ç‚¹ | Uniform, GM | LKH-3 |
| OP (Orienteering Problem) | è·¯ç”± | 10â€“100èŠ‚ç‚¹ | Uniform, GM | COMPASS |
| CVRP | è·¯ç”± | 10â€“100èŠ‚ç‚¹ | Uniform, GM | LKH-3 |
| MIS | å›¾è®º | 50â€“500èŠ‚ç‚¹ | ER, BA | Gurobi |
| MVC | å›¾è®º | 50â€“500èŠ‚ç‚¹ | ER, BA | Gurobi |
| PFSP | è°ƒåº¦ | 10â€“100ä½œä¸š | Taillard | QIG |
| JSSP | è°ƒåº¦ | 6â€“30ä½œä¸š | Taillard | OR-Tools |

- **è®­ç»ƒé›†**ï¼šå„é—®é¢˜50ä¸‡å®ä¾‹ã€‚
- **æµ‹è¯•é›†**ï¼šå„é—®é¢˜100ä¸ªå®ä¾‹ã€‚

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### **è¯„ä¼°æŒ‡æ ‡**
1. **å¯è¡Œæ€§ç‡**ï¼ˆFeasibility Rate, Fea.%ï¼‰ï¼šæ»¡è¶³æ‰€æœ‰çº¦æŸçš„è§£çš„æ¯”ä¾‹ã€‚
2. **æœ€ä¼˜æ€§å·®è·**ï¼ˆOptimality Gap, Gap%ï¼‰ï¼š$(f(\hat{x}) - f(x^*)) / f(x^*) \times 100\%$ï¼Œè¶Šä½è¶Šå¥½ã€‚
3. **æ¨ç†æ—¶é—´**ï¼ˆTime/sï¼‰ï¼šå¹³å‡æ¯å®ä¾‹è€—æ—¶ã€‚

#### **FALCONé…ç½®**
- **æœ€å°é‡‡æ ·æ•°** $N_{\min}=8$
- **æœ€å¤§é‡‡æ ·æ•°** $N_{\max}=64$
- **ç½®ä¿¡é˜ˆå€¼** $T=0.85$
- **é‡‡æ ·æ¸©åº¦** $\tau=0.7$

#### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
åˆ†ä¸ºå››ç±»ï¼š

1. **é€šç”¨LLMï¼ˆé›¶æ ·æœ¬ï¼‰**  
   - GPT-4o, GPT-4o-mini, Claude-3.5-Sonnet/Haiku, DeepSeek-V3, Llama-3.3-70B, Qwen2.5-72B

2. **æ¨ç†å¢å¼ºLLM**  
   - GPT-o3-mini, GPT-o1, DeepSeek-R1

3. **LLMä¼˜åŒ–æ–¹æ³•**  
   - OPRO, LMEA, PHP, SGE

4. **ç¥ç»COæ±‚è§£å™¨**  
   - SFT Only, GRPO, LLMCoSolver (N=1/N=8)

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ªTable 2ï¼‰**

| æ–¹æ³• | å¹³å‡å¯è¡Œæ€§ | å¹³å‡Gap% | æœ€ä¼˜æ€§ |
|------|-----------|---------|-------|
| **FALCON (Adaptive)** | **100%** | **0.92%** | âœ… æœ€ä½³ |
| FALCON (N=8) | 100% | 0.95% | â€” |
| LLMCoSolver (N=8) | 94â€“100% | 1.07â€“8.20% | â€” |
| GRPO | 91â€“98% | 1.34â€“4.25% | â€” |
| GPT-4o | 6â€“88% | 20.57â€“306% | âŒ |

> **FALCONåœ¨æ‰€æœ‰7ä¸ªé—®é¢˜ä¸Šå‡è¾¾åˆ°100%å¯è¡Œæ€§ï¼Œä¸”æœ€ä¼˜æ€§å·®è·æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚**

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- **ç›¸æ¯”é€šç”¨LLM**ï¼šFALCONçš„Gapé™ä½**æ•°åå€**ï¼ˆå¦‚TSPä»33.79%é™è‡³0.92%ï¼‰ï¼Œå¯è¡Œæ€§ä»<100%æå‡è‡³**ç¡®å®šæ€§100%**ã€‚
- **ç›¸æ¯”ç¥ç»æ±‚è§£å™¨**ï¼šåœ¨CVRPã€MISç­‰å¤æ‚é—®é¢˜ä¸Šï¼ŒFALCONçš„Gapä»ä¼˜äºLLMCoSolver (N=8)ï¼ŒåŒæ—¶ä¿è¯100%å¯è¡Œæ€§ã€‚
- **ä¸é¢†åŸŸå¯å‘å¼å¯¹æ¯”**ï¼ˆTable 3ï¼‰ï¼š
  - FALCONåœ¨å¤§è§„æ¨¡TSPä¸Šè¡¨ç°ç¨³å®šï¼ˆGap=1.08%ï¼‰ï¼Œè€ŒACOé€€åŒ–è‡³36.69%ã€‚
  - åœ¨JSSPä¸Šï¼ŒFALCON (Adaptive) Gap=6.98%ï¼Œè¿œä¼˜äºFIFOï¼ˆ39.00%ï¼‰ã€‚

### **æ¶ˆèå®éªŒç»“æœï¼ˆTable 4ï¼‰**

| é…ç½® | TSP Gap | CVRP Gap | å¯è¡Œæ€§ |
|------|--------|---------|--------|
| **FALCON (Full)** | 0.92% | 3.52% | 100% |
| w/o Grammar | 0.95% | 3.67% | 100% |
| **w/o Repair** | 0.89% | 3.41% | **94%/85%** |
| w/o Adaptive | 0.91% | 3.48% | 100% |
| **w/o BOPO** | 1.85% | 5.89% | 100% |
| SFT Only | 2.30% | 6.02% | 89%/59% |

> **å…³é”®å‘ç°**ï¼š
> - **ä¿®å¤å±‚**æ˜¯å®ç°100%å¯è¡Œæ€§çš„**å…³é”®**ï¼Œç§»é™¤åå¯è¡Œæ€§éª¤é™ã€‚
> - **BOPO**å¯¹è§£çš„è´¨é‡å½±å“æœ€å¤§ï¼Œç§»é™¤åGapç¿»å€ã€‚
> - è‡ªé€‚åº”é‡‡æ ·èŠ‚çœ40%ä»¥ä¸Šè®¡ç®—é‡ï¼Œå‡ ä¹æ— è´¨é‡æŸå¤±ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **100%å¯è¡Œæ€§å¯è¢«ä¸¥æ ¼ä¿è¯**ï¼šé€šè¿‡**è¯­æ³•çº¦æŸ + ä¿®å¤ç®—å­**çš„ä¸¤é˜¶æ®µè®¾è®¡ï¼ŒFALCONé¦–æ¬¡å®ç°äº†LLM-based COçš„**ç¡®å®šæ€§å¯è¡Œæ€§**ã€‚
2. **å¯è¡Œæ€§ä¸é«˜è´¨é‡å¯å…¼å¾—**ï¼šå°½ç®¡å¼•å…¥ä¿®å¤å¯èƒ½ç•¥å¾®åŠ£åŒ–è§£ï¼Œä½†**BOPOè®­ç»ƒå‡ºçš„æ¨¡å‹æœ¬èº«é«˜åº¦å¯è¡Œ**ï¼Œä¿®å¤é¢‘ç‡ä½ï¼ˆ2.8%â€“18.5%ï¼‰ï¼Œå› æ­¤è´¨é‡æŸå¤±æå°ã€‚
3. **è‡ªé€‚åº”é‡‡æ ·é«˜æ•ˆæ™ºèƒ½**ï¼šåŸºäºä¸€è‡´æ€§ï¼ˆconsistencyï¼‰çš„éš¾åº¦ä¼°è®¡èƒ½æœ‰æ•ˆåˆ†é…è®¡ç®—èµ„æºï¼Œ**å¹³å‡å‡å°‘58%é‡‡æ ·æ•°**ï¼Œæ¨ç†æ—¶é—´ä¸‹é™55%ã€‚
4. **BOPOä¼˜äºGRPO**ï¼šç›®æ ‡å·®è·åŠ æƒä½¿æ¢¯åº¦æ›´æ–°æ›´å…·ä¿¡æ¯é‡ï¼Œåœ¨å›°éš¾é—®é¢˜ï¼ˆå¦‚CVRPã€MISï¼‰ä¸Šæå‡æ˜¾è‘—ï¼ˆå›¾2æ˜¾ç¤ºæœ€é«˜29% Gapä¸‹é™ï¼‰ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
1. **ä¿®å¤ç®—å­éœ€é—®é¢˜å®šåˆ¶**ï¼šè™½ç„¶æ¡†æ¶é€šç”¨ï¼Œä½†ä¿®å¤é€»è¾‘éœ€é’ˆå¯¹æ¯ç±»é—®é¢˜æ‰‹å·¥è®¾è®¡ï¼Œ**æ³›åŒ–åˆ°å…¨æ–°é—®é¢˜ç±»å‹éœ€é¢å¤–å¼€å‘**ã€‚
2. **ä¾èµ–é«˜è´¨é‡è®­ç»ƒæ•°æ®**ï¼šSFTé˜¶æ®µä¾èµ–ä¸“å®¶æ±‚è§£å™¨ï¼ˆå¦‚LKHã€Gurobiï¼‰ç”Ÿæˆçš„é«˜è´¨é‡è§£ï¼Œè‹¥å‚è€ƒè§£è´¨é‡å·®ï¼Œä¼šå½±å“æœ€ç»ˆæ€§èƒ½ã€‚
3. **ä¿®å¤å¯èƒ½å¼•å…¥æ¬¡ä¼˜æ€§**ï¼šè™½ç„¶ç†è®ºä¸Šç•Œå­˜åœ¨ï¼Œä½†åœ¨æç«¯æƒ…å†µä¸‹ä¿®å¤å¯èƒ½å¯¼è‡´è¾ƒå¤§è´¨é‡æŸå¤±ï¼ˆå°½ç®¡å®è·µä¸­ç½•è§ï¼‰ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **è‡ªåŠ¨åŒ–ä¿®å¤ç®—å­ç”Ÿæˆ**ï¼šæ¢ç´¢å¦‚ä½•ä»é—®é¢˜æè¿°è‡ªåŠ¨åˆæˆä¿®å¤é€»è¾‘ï¼Œæå‡æ¡†æ¶é€šç”¨æ€§ã€‚
2. **ç«¯åˆ°ç«¯å¯å¾®ä¿®å¤**ï¼šç ”ç©¶æ˜¯å¦å¯å°†ä¿®å¤è¿‡ç¨‹åµŒå…¥æ¨¡å‹å†…éƒ¨ï¼Œå®ç°å¯å¾®è®­ç»ƒã€‚
3. **æ‰©å±•è‡³å¸¦ä¸ç¡®å®šæ€§æˆ–åŠ¨æ€COé—®é¢˜**ï¼šå¦‚åŠ¨æ€è½¦è¾†è·¯å¾„é—®é¢˜ï¼ˆDVRPï¼‰ã€éšæœºè°ƒåº¦ç­‰ã€‚
4. **ç»“åˆå½¢å¼éªŒè¯**ï¼šåˆ©ç”¨FALCONçš„ç¡®å®šæ€§ç‰¹æ€§ï¼Œè¿›ä¸€æ­¥é›†æˆå½¢å¼åŒ–æ–¹æ³•ä»¥æä¾›æ›´å¼ºçš„å®‰å…¨ä¿éšœã€‚

---

> **æ€»ç»“**ï¼šFALCONé€šè¿‡â€œ**ç¡¬çº¦æŸ + è½¯ç”Ÿæˆ**â€çš„åˆ†ç¦»æ€æƒ³ï¼ŒæˆåŠŸè§£å†³äº†LLMåœ¨ç»„åˆä¼˜åŒ–ä¸­**å¯è¡Œæ€§ä¸å¯æ§**çš„æ ¹æœ¬éš¾é¢˜ï¼Œä¸ºLLMåœ¨ç°å®ä¸–ç•Œé«˜å¯é æ€§åœºæ™¯ä¸­çš„éƒ¨ç½²æä¾›äº†åšå®åŸºç¡€ã€‚

</details>

---

### 10. [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)

**Authors**: Jiarui Zhang, Yuchen Yang, Ran Yan, Zhiyu Mei, Liyuan Zhang, Daifeng Li, Wei Fu, Jiaxuan Gao, Shusheng Xu, Yi Wu, Binhang Yuan  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.00482v1  

#### Abstract
Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identica...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šAREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
åœ¨åŸºäº **Reinforcement Learning (RL)** å¯¹ **Large Language Models (LLMs)** è¿›è¡Œåè®­ç»ƒï¼ˆpost-trainingï¼‰æ—¶ï¼Œç³»ç»Ÿä¼šç”Ÿæˆå¤§é‡ rollout åºåˆ—ã€‚è¿™äº›åºåˆ—é€šå¸¸å…±äº«å¾ˆé•¿çš„å‰ç¼€ï¼ˆå¦‚ç›¸åŒçš„ prompt æˆ–åˆå§‹å¯¹è¯ï¼‰ï¼Œä½†ä¼ ç»Ÿ RL æ¡†æ¶å°†æ¯ä¸ªåºåˆ—ç‹¬ç«‹å¤„ç†ï¼Œå¯¼è‡´å¯¹ç›¸åŒå‰ç¼€è¿›è¡Œé‡å¤çš„ **forward å’Œ backward è®¡ç®—**ï¼Œé€ æˆä¸¥é‡çš„è®¡ç®—å’Œå†…å­˜æµªè´¹ã€‚

æ­¤å¤–ï¼Œè™½ç„¶å‰ç¼€å…±äº«å¤©ç„¶å½¢æˆä¸€æ£µ **prefix tree**ï¼Œä½†ç°æœ‰çš„ **tree attention** æ–¹æ³•éœ€è¦å®Œå…¨ç‰©åŒ–ï¼ˆfully materializeï¼‰æ•´ä¸ª attention maskï¼Œå…¶è®¡ç®—å’Œå†…å­˜å¼€é”€éšæ€» token æ•°é‡å‘ˆ **äºŒæ¬¡æ–¹å¢é•¿**ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡ RL åœºæ™¯ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

è®ºæ–‡æå‡º **AREAL-DTA**ï¼ˆDynamic Tree Attention over AReaLï¼‰ï¼Œä¸€ç§é«˜æ•ˆåˆ©ç”¨å‰ç¼€å…±äº«ã€æå‡ RL è®­ç»ƒæ•ˆç‡çš„æ–°æ¡†æ¶ï¼Œæ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### **Contribution 1: DFS-based åŠ¨æ€æ ‘éå†æœºåˆ¶**
- å°†æ‰€æœ‰ rollout åºåˆ—ç»„ç»‡æˆä¸€æ£µ **prefix tree**ã€‚
- é‡‡ç”¨ **Depth-First Search (DFS)** ç­–ç•¥åŠ¨æ€éå†è¯¥æ ‘ï¼Œåœ¨ **forward å’Œ backward è¿‡ç¨‹ä¸­ä»…ç»´æŠ¤ä¸€æ¡ä»æ ¹åˆ°å¶çš„è·¯å¾„**ï¼ˆåŒ…æ‹¬ token å’Œ KV Cacheï¼‰ã€‚
- åœ¨è®¿é—®å¶èŠ‚ç‚¹æ—¶ç«‹å³è®¡ç®— loss å¹¶åå‘ä¼ æ’­ï¼Œéšåå¼¹å‡ºåˆ†æ”¯ï¼Œé‡Šæ”¾éå…±äº«éƒ¨åˆ†çš„æ¿€æ´»çŠ¶æ€ã€‚
- **ä¼˜åŠ¿**ï¼š
  - å†…å­˜å ç”¨ä»…ä¸æœ€é•¿åºåˆ—é•¿åº¦æˆæ­£æ¯”ï¼ˆè€Œéæ‰€æœ‰ token æ€»æ•°ï¼‰ï¼Œé¿å…äº†å†…å­˜çˆ†ç‚¸ã€‚
  - å…±äº«å‰ç¼€çš„è®¡ç®—å’Œæ¢¯åº¦ä»…æ‰§è¡Œä¸€æ¬¡ï¼Œå®ç°é«˜æ•ˆå¤ç”¨ã€‚

#### **Contribution 2: è´Ÿè½½å‡è¡¡çš„åˆ†å¸ƒå¼æ‰¹å¤„ç†æœºåˆ¶ï¼ˆLoad-Balanced Distributed Batchingï¼‰**
- åœ¨å¤š GPU å¼‚æ­¥è®­ç»ƒåœºæ™¯ä¸‹ï¼Œè®¾è®¡äº†ä¸€ç§ **åŸºäº DFS é¡ºåºçš„è¿ç»­åˆ†ç»„ç­–ç•¥**ï¼Œå°† rollout åˆ†é…åˆ°å¤šä¸ª trainer GPUã€‚
- é€šè¿‡ **binary search + è´ªå¿ƒæ‰«æ** å®ç°æœ€å°åŒ–æœ€å¤§ç»„æˆæœ¬çš„åˆ’åˆ†ï¼Œç¡®ä¿å„ GPU è´Ÿè½½å‡è¡¡ã€‚
- **ä¼˜åŠ¿**ï¼š
  - æœ€å¤§é™åº¦ä¿ç•™å‰ç¼€å…±äº«ï¼ˆå‡å°‘è·¨ GPU å‰ç¼€å¤åˆ¶ï¼‰ã€‚
  - å‡å°‘ GPU ç©ºé—²æ—¶é—´ï¼Œæå‡æ•´ä½“ååã€‚

#### **Contribution 3: å¤šé¡¹ç³»ç»Ÿçº§ä¼˜åŒ–**
- **Chunked Backpropagation**ï¼šå¯¹é•¿åºåˆ—åˆ†å—åå‘ä¼ æ’­ï¼Œè¿›ä¸€æ­¥æ§åˆ¶å†…å­˜ã€‚
- **Avoid KV Cache for Leaf Nodes**ï¼šä¸ä¸ºå¶èŠ‚ç‚¹ç¼“å­˜ KVï¼Œå‡å°‘å†—ä½™è®¡ç®—ã€‚
- **Greedy DFS éå†é¡ºåºä¼˜åŒ–**ï¼šä¼˜å…ˆå¤„ç†æ›´é•¿åˆ†æ”¯ï¼Œæå‡å†…å­˜å’Œè®¡ç®—æ•ˆç‡ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | ç¼ºé™· | AREAL-DTA çš„æ”¹è¿› |
|------|------|------------------|
| Vanilla RL Training | æ— å‰ç¼€å…±äº«ï¼Œé‡å¤è®¡ç®—ä¸¥é‡ | æ˜¾è‘—å‡å°‘å†—ä½™è®¡ç®— |
| Tree Training [5] | éœ€å­˜å‚¨æ•´æ£µæ ‘çš„ç‰©åŒ–ç»“æ„ï¼Œå†…å­˜å¼€é”€å¤§ | åŠ¨æ€ DFS éå†ï¼Œä»…ç»´æŠ¤å½“å‰è·¯å¾„ï¼Œå†…å­˜ä½ |
| Full Tree Attention | attention mask äºŒæ¬¡å¢é•¿ï¼Œä¸å¯æ‰©å±• | æ— éœ€å®Œæ•´ maskï¼Œè®¡ç®—è½»é‡ |

> âœ… **AREAL-DTA åœ¨ä¿æŒæ­£ç¡®æ¢¯åº¦ç´¯ç§¯çš„åŒæ—¶ï¼Œå®ç°äº†å†…å­˜ä¸è®¡ç®—çš„åŒé‡é«˜æ•ˆæ€§**ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†ä¸ä»»åŠ¡
- ä½¿ç”¨ **T2-bench [27]** ä½œä¸º RL è®­ç»ƒå·¥ä½œè´Ÿè½½åŸºå‡†ã€‚
- åœ¨ **TauBench** æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆç”¨äº reward æ›²çº¿åˆ†æï¼‰ã€‚
- ä»»åŠ¡ç±»å‹ï¼šé¢å‘ **è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›å¢å¼º** çš„ RL å¾®è°ƒï¼Œæ¶µç›–æ•°å­¦æ¨ç†ã€ç¨‹åºåˆæˆç­‰å¤æ‚ä»»åŠ¡ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹æ¶æ„**ï¼šåŸºäº **QWEN-3** ç³»åˆ—æ¨¡å‹ï¼Œæµ‹è¯•å‚æ•°è§„æ¨¡åŒ…æ‹¬ **1.7B, 4B, 8B, 14B**ã€‚
- **ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆcontext lengthï¼‰**ï¼š16Kã€‚
- **RL ç®—æ³•**ï¼šPPOï¼ˆProximal Policy Optimizationï¼‰ã€‚
- **è®­ç»ƒæ¡†æ¶**ï¼šåŸºäº **AREAL [2]** æ„å»ºï¼Œæ”¯æŒå¼‚æ­¥ rollout ç”Ÿæˆä¸æ¨¡å‹æ›´æ–°ã€‚
- **ç¡¬ä»¶ç¯å¢ƒ**ï¼šæ¯å°æœºå™¨é…å¤‡ **8 å— NVIDIA H800 GPU**ã€‚

---

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **End-to-End Throughput** | å®Œæ•´ RL è®­ç»ƒæµç¨‹çš„åŠ é€Ÿæ¯”ï¼ˆä»¥å•ä½æ—¶é—´å†…å®Œæˆçš„è®­ç»ƒæ­¥æ•°è¡¡é‡ï¼‰ |
| **Training Throughput** | å•ä¸ªè®­ç»ƒ worker çš„ååæå‡å€æ•° |
| **Peak GPU Memory Usage** | åå‘ä¼ æ’­é˜¶æ®µçš„æœ€å¤§æ˜¾å­˜å ç”¨ |
| **Reward Curve** | æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å¥–åŠ±å˜åŒ–è¶‹åŠ¿ï¼Œç”¨äºè¯„ä¼°è®­ç»ƒç¨³å®šæ€§ |
| **Ablation Study** | å„ç»„ä»¶å¯¹æ€§èƒ½çš„å½±å“ï¼ˆå¦‚ DFSã€chunkingã€load balancingï¼‰ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **AREAL**ï¼šå½“å‰æœ€å…ˆè¿›çš„å¼‚æ­¥ RL è®­ç»ƒç³»ç»Ÿï¼Œä½œä¸ºä¸»åŸºçº¿ã€‚
- **Dense + CKPT**ï¼šæ ‡å‡†å¯†é›†è®­ç»ƒ + Activation Checkpointingï¼ˆåº”å¯¹é•¿åºåˆ— OOMï¼‰ã€‚
- **Tree Training [5]**ï¼šå·²æœ‰å°è¯•åˆ©ç”¨å‰ç¼€å…±äº«çš„æ–¹æ³•ï¼Œä½†é™æ€æ‰“åŒ…ã€æ‰©å±•æ€§å·®ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

#### âœ… ç«¯åˆ°ç«¯è®­ç»ƒååæå‡
| æ¨¡å‹ | ç›¸æ¯” AREAL çš„ E2E åŠ é€Ÿæ¯” |
|------|------------------------|
| Qwen3-1.7B | **1.28Ã—** |
| Qwen3-8B | **2.28Ã—** |

> ğŸ’¡ æé€Ÿä¸»è¦æ¥æºäºï¼šæ›´é«˜çš„è®­ç»ƒååå…è®¸å°†æ›´å¤š GPU åˆ†é…ç»™ rollout ç”Ÿæˆé˜¶æ®µï¼Œä»è€Œæ•´ä½“åŠ é€Ÿ pipelineã€‚

#### âœ… å• worker è®­ç»ƒååæå‡
- **æœ€é«˜è¾¾ 8.31Ã—** çš„å•å¡è®­ç»ƒååæå‡ï¼ˆåœ¨ 2-bench ä¸Šï¼‰ã€‚
- é›†ç¾¤æ•´ä½“è®­ç»ƒååæå‡ **6.20Ã—**ã€‚

#### âœ… æ˜¾å­˜ä½¿ç”¨æ˜¾è‘—é™ä½
- **å³°å€¼ GPU å†…å­˜ä¸‹é™è¶…è¿‡ 50%**ã€‚
- å› æ­¤ **æ— éœ€å¯ç”¨ Activation Checkpointing**ï¼Œé¿å…äº†é¢å¤–çš„ recomputation å¼€é”€ã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰åå‘ä¼ æ’­ä¼˜åŒ–ç»„ä»¶æ•ˆæœï¼ˆFigure 4 & 5ï¼‰
| æ–¹æ³• | ç›¸å¯¹ Dense+CKPT çš„å¹³å‡åŠ é€Ÿæ¯” |
|------|----------------------------|
| Vanilla Tree æ–¹æ³• | 6.59Ã— |
| + Cut Tailï¼ˆä¸ç¼“å­˜å¶èŠ‚ç‚¹ KVï¼‰ | 7.53Ã— |
| + DFS éå†é¡ºåºä¼˜åŒ– | 7.74Ã— |
| + Larger Backward Chunk Size (LB) | **8.31Ã—** |

> âœ… æ‰€æœ‰ä¼˜åŒ–å‡å¸¦æ¥æ­£å‘å¢ç›Šï¼Œä¸”å¯å åŠ ã€‚

#### ï¼ˆ2ï¼‰è´Ÿè½½å‡è¡¡ç®—æ³•å¯¹æ¯”ï¼ˆFigure 6â€“8ï¼‰
- å¯¹æ¯”ä¸¤ç§ baselineï¼š
  - `Greedy FFD` æŒ‰ token æ•°åˆ†é…
  - `FFD by ntree tokens` æŒ‰æ ‘ token æˆæœ¬åˆ†é…
- **AREAL-DTA çš„ DFS-order + contiguous partitioning** è¡¨ç°æœ€ä¼˜ã€‚
- è‹¥å…³é—­è¯¥æœºåˆ¶ï¼Œç³»ç»Ÿæ€§èƒ½ä¸‹é™ **11.93%**ã€‚

#### ï¼ˆ3ï¼‰å‰ç¼€åˆå¹¶æ”¶ç›Š
- å³ä½¿åªæ˜¯ç®€å•åˆå¹¶å®Œå…¨åŒ…å«çš„å‰ç¼€ï¼Œä¹Ÿèƒ½å¸¦æ¥ **1.67Ã—** çš„åŠ é€Ÿã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å‰ç¼€å…±äº«æ˜¯ RL è®­ç»ƒä¸­çš„é‡å¤§ä¼˜åŒ–æœºä¼š**ï¼šæ•°ç™¾æ¡ rollout å…±äº«å‰ç¼€çš„ç°è±¡æ™®éå­˜åœ¨ï¼Œåˆç†åˆ©ç”¨å¯å¤§å¹…èŠ‚çœè®¡ç®—èµ„æºã€‚
2. **åŠ¨æ€ DFS éå†ä¼˜äºé™æ€æ ‘æ‰“åŒ…**ï¼šAREAL-DTA é€šè¿‡è¿è¡Œæ—¶åŠ¨æ€æ„å»ºè®¡ç®—å›¾ï¼Œé¿å…äº†å…¨æ ‘ç‰©åŒ–çš„é«˜å¼€é”€ï¼Œå®ç°äº†çœŸæ­£çš„å¯æ‰©å±•æ€§ã€‚
3. **å†…å­˜æ•ˆç‡ç›´æ¥è½¬åŒ–ä¸ºé€Ÿåº¦ä¼˜åŠ¿**ï¼šæ›´ä½çš„æ˜¾å­˜å ç”¨ä½¿å¾—æ›´å¤§ batch size å’Œæ›´å¤š rollout æˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶å…å»äº† activation checkpointing çš„å¼€é”€ã€‚
4. **è´Ÿè½½å‡è¡¡å¯¹å¤š GPU æ‰©å±•è‡³å…³é‡è¦**ï¼šåˆç†çš„åˆ†ç»„ç­–ç•¥ä¸ä»…èƒ½å¹³è¡¡è´Ÿè½½ï¼Œè¿˜èƒ½æœ€å¤§é™åº¦ä¿ç•™å‰ç¼€å…±äº«ç»“æ„ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ– rollout çš„é«˜å‰ç¼€é‡åˆç‡**ï¼šè‹¥ä¸åŒ rollout å·®å¼‚æå¤§ã€å…±äº«å‰ç¼€çŸ­ï¼Œåˆ™ä¼˜åŒ–ç©ºé—´æœ‰é™ã€‚
- **DFS éå†å¼•å…¥ä¸€å®šè°ƒåº¦å¼€é”€**ï¼šè™½ç„¶æ€»ä½“æ”¶ç›Šæ˜¾è‘—ï¼Œä½†åœ¨æçŸ­åºåˆ—åœºæ™¯ä¸‹å¯èƒ½ä¸å¦‚æ ‡å‡† batching ç®€æ´ã€‚
- **ç›®å‰é›†æˆäº AREAL æ¡†æ¶**ï¼šé€šç”¨æ€§ä¾èµ–äºå¼‚æ­¥ RL æ¶æ„çš„æ”¯æŒï¼ŒåŒæ­¥è®­ç»ƒéœ€é€‚é…ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³å…¶ä»– RL ç®—æ³•**ï¼šå¦‚ GRPOã€SAC ç­‰æ˜¯å¦åŒæ ·å—ç›Šã€‚
2. **æ”¯æŒåŠ¨æ€æ ‘å‰ªæä¸ä¼˜å…ˆçº§é‡‡æ ·**ï¼šç»“åˆ reward ä¿¡å·åŠ¨æ€è°ƒæ•´ rollout ç”Ÿæˆä¸è®­ç»ƒä¼˜å…ˆçº§ã€‚
3. **è·¨èŠ‚ç‚¹å¤§è§„æ¨¡æ‰©å±•**ï¼šæ¢ç´¢åœ¨ä¸‡å¡çº§åˆ«é›†ç¾¤ä¸­çš„é€šä¿¡ä¸è°ƒåº¦ä¼˜åŒ–ã€‚
4. **ä¸ speculative decoding ç»“åˆ**ï¼šè”åˆä¼˜åŒ– inference-time ä¸ training-time çš„ tree ç»“æ„åˆ©ç”¨ã€‚

---

## âœ… æ€»ç»“
**AREAL-DTA æ˜¯é¦–ä¸ªåœ¨å¤§è§„æ¨¡ RL è®­ç»ƒä¸­é«˜æ•ˆåˆ©ç”¨ prefix sharing çš„ç³»ç»Ÿçº§è§£å†³æ–¹æ¡ˆ**ã€‚å®ƒé€šè¿‡ **DFS-based åŠ¨æ€æ ‘æ³¨æ„åŠ›æœºåˆ¶** å’Œ **è´Ÿè½½å‡è¡¡çš„åˆ†å¸ƒå¼æ‰¹å¤„ç†**ï¼Œå®ç°äº†é«˜è¾¾ **8.31Ã— çš„è®­ç»ƒååæå‡** ä¸ **è¶… 50% çš„æ˜¾å­˜èŠ‚çœ**ï¼Œä¸º LLM çš„é«˜æ•ˆ RL è®­ç»ƒæä¾›äº†æ–°çš„èŒƒå¼ã€‚

</details>

---

### 11. [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)

**Authors**: Zheng Zhang, Ao Lu, Yuanhao Zeng, Ziwei Shan, Jinjin Guo, Lufei Li, Yexin Li, Kan Ren  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.01791v1  

#### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequen...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning  
**â€”â€” æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰åœ¨å¼€æ”¾åŸŸä»»åŠ¡ï¼ˆå¦‚åŒ»å­¦å’¨è¯¢ã€åˆ›æ„å†™ä½œï¼‰ä¸­ï¼Œ**Reinforcement Learning with Verifiable Rewards (RLVR)** éš¾ä»¥ç›´æ¥åº”ç”¨ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡ç¼ºä¹æ˜ç¡®çš„â€œæ­£ç¡®ç­”æ¡ˆâ€ï¼Œè¯„ä»·å…·æœ‰ä¸»è§‚æ€§ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å¼•å…¥ **LLM-as-a-Judge** æä¾›åºåˆ—çº§ï¼ˆsequence-levelï¼‰å¥–åŠ±è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§ç“¶é¢ˆï¼š

1. **ç¨€ç–å¥–åŠ±ï¼ˆSparse Rewardsï¼‰**ï¼šä»…åœ¨ç”Ÿæˆç»“æŸæ—¶æä¾›æ•´ä½“åé¦ˆï¼Œæ— æ³•æŒ‡å¯¼å¤æ‚é•¿æ–‡æœ¬æ¨ç†è¿‡ç¨‹ä¸­çš„ç»†ç²’åº¦ä¼˜åŒ–ã€‚
2. **é»‘ç›’è¯„åˆ¤ï¼ˆBlack-box Judgeï¼‰**ï¼šå°† Judge è§†ä¸ºé»‘ç®±ï¼Œå¿½ç•¥äº†å…¶å†…éƒ¨ä¸°å¯Œçš„ä¸­é—´è¯„ä¼°ä¿¡å·ã€‚

è¿™å¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ã€ä¿¡ç”¨åˆ†é…ä¸ç²¾ç¡®ï¼Œå°¤å…¶ä¸åˆ©äºå¯¹è¿‡ç¨‹è´¨é‡è¦æ±‚é«˜çš„ä»»åŠ¡ï¼ˆå¦‚åŒ»ç–—å»ºè®®éœ€æ¯ä¸€æ­¥éƒ½ç§‘å­¦ä¸¥è°¨ï¼‰ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šGRAD2REWARD
ä½œè€…æå‡º **GRAD2REWARD**ï¼Œä¸€ç§ä» Judge æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­æå–**å¯†é›†è¿‡ç¨‹å¥–åŠ±ï¼ˆdense process rewardsï¼‰** çš„æ–°æ¡†æ¶ï¼Œæ ¸å¿ƒæ€æƒ³å¦‚ä¸‹ï¼š

- **æ¢¯åº¦å½’å› ï¼ˆGradient-based Attributionï¼‰**ï¼šåˆ©ç”¨å•æ¬¡åå‘ä¼ æ’­è®¡ç®—æ¯ä¸ªç”Ÿæˆ token å¯¹ Judge å†³ç­–çš„å½±å“ç¨‹åº¦ï¼Œå¾—åˆ° token-level çš„è´¡çŒ®åˆ†æ•°ã€‚
- **Token-level Reward æ„å»º**ï¼š
  $$
  b_t = g_t \cdot e_t,\quad r_t = \alpha_t \cdot r(x,o)
  $$
  å…¶ä¸­ $g_t$ æ˜¯ log-probability å…³äº token embedding çš„æ¢¯åº¦ï¼Œ$e_t$ æ˜¯å¯¹åº”åµŒå…¥ï¼Œ$b_t$ ç» Softmax å½’ä¸€åŒ–ååŠ æƒåˆ†é…åŸå§‹åºåˆ—çº§å¥–åŠ±ã€‚
- **è‡ªè¯„åˆ¤æœºåˆ¶ï¼ˆSelf-judging Mechanismï¼‰**ï¼šJudge å›ºå®šä¸ºåˆå§‹ç­–ç•¥æ¨¡å‹å‰¯æœ¬ï¼Œæ— éœ€ä¾èµ–æ›´å¼ºçš„å¤–éƒ¨ Judgeï¼Œå®ç°è‡ªæˆ‘ç›‘ç£ä¸‹çš„æŒç»­æ”¹è¿›ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ Vanilla-GRPO, RuscaRLï¼‰ | GRAD2REWARD |
|------|-------------------------------|-------------|
| å¥–åŠ±å¯†åº¦ | ç¨€ç–ï¼ˆä»…åºåˆ—çº§ï¼‰ | å¯†é›†ï¼ˆtoken-levelï¼‰ |
| Judge ä½¿ç”¨æ–¹å¼ | é»‘ç›’è¾“å‡ºå†³ç­– | ç™½ç›’æŒ–æ˜æ¢¯åº¦ä¿¡å· |
| æ˜¯å¦éœ€è¦å¤–éƒ¨å¼º Judge | æ˜¯ï¼ˆé€šå¸¸ç”¨æ›´å¤§æ¨¡å‹ï¼‰ | å¦ï¼ˆself-judgingï¼‰ |
| æ˜¯å¦éœ€è®­ç»ƒä¸“ç”¨ PRM | æ˜¯ï¼ˆå¦‚ PURE, PQMï¼‰ | å¦ï¼ˆé›¶æˆæœ¬æ„å»º dense rewardï¼‰ |
| é€‚ç”¨èŒƒå›´ | å¤šé™äºå¯éªŒè¯é¢†åŸŸï¼ˆå¦‚æ•°å­¦ï¼‰ | æ”¯æŒå¼€æ”¾åŸŸ + å¯éªŒè¯åŸŸ |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šæ›´é«˜æ•ˆè®­ç»ƒã€æ›´é«˜æ¨ç†è´¨é‡ã€æ›´å¼ºæ³›åŒ–èƒ½åŠ›ã€æ›´ä½éƒ¨ç½²é—¨æ§›ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
æ¶µç›–å¤šä¸ªå¼€æ”¾åŸŸä»»åŠ¡ï¼Œåˆ†ä¸ºä¸¤ç±»ï¼š

#### å¼€æ”¾å¼ä»»åŠ¡ï¼ˆOpen-ended Tasksï¼‰
| æ•°æ®é›† | ä»»åŠ¡æè¿° |
|--------|----------|
| **HealthBench** | åŒ»ç–—å¥åº·é—®ç­”ï¼Œå¼ºè°ƒå®‰å…¨æ€§ä¸å‡†ç¡®æ€§ |
| **RaR-Medicine** | åŒ»å­¦å’¨è¯¢ï¼Œå«å¤šç»´åº¦è¯„åˆ†æ ‡å‡†ï¼ˆrubricsï¼‰ |
| **ResearchQA** | å­¦æœ¯ç ”ç©¶ç±»é•¿æ–‡æœ¬é—®ç­”ï¼Œè·¨75ä¸ªå­¦ç§‘ |
| **RaR-Science** | ç§‘å­¦é—®é¢˜è§£ç­”ï¼Œæ³¨é‡é€»è¾‘ä¸äº‹å®ä¸€è‡´æ€§ |

#### å¯éªŒè¯ä»»åŠ¡ï¼ˆVerifiable Domainsï¼Œç”¨äºæ‰©å±•åˆ†æï¼‰
| æ•°æ®é›† | æè¿° |
|--------|------|
| **MATH500**, **Minerva Math**, **OlympiadBench**, **AIME24/25**, **AMC23** | æ•°å­¦æ¨ç†åŸºå‡†ï¼Œæœ‰æ ‡å‡†ç­”æ¡ˆ |
| **GPQA-Diamond** | é«˜éš¾åº¦ç§‘å­¦é—®ç­”ï¼Œæ”¯æŒè‡ªåŠ¨éªŒè¯ |

---

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹å®¶æ—**ï¼š
  - Qwen2.5 ç³»åˆ—ï¼š1.5B, 3B, 7B
  - Llama-3.1/3.2-8B
  - æµ‹è¯•ç”¨æ›´å¼ºæ¨¡å‹ï¼ˆå¦‚ Qwen3-30B, Mistral-Small-24Bï¼‰ä½œä¸º **test grader**
- **è®­ç»ƒæ–¹å¼**ï¼š
  - å…¨å‚æ•°å¾®è°ƒï¼ˆfull-parameter fine-tuningï¼‰
  - ä½¿ç”¨ **veRL** æ¡†æ¶å®ç° RL ä¼˜åŒ–
  - Batch size: 32 prompts, æ¯ prompt é‡‡æ · 8 æ¡å“åº”
- **è¯„ä¼°åè®®**ï¼š
  - ä½¿ç”¨ **OpenAI Simple-Evals** å·¥å…·åŒ…
  - Test grader åŸºäº rubric æ‰“åˆ†ï¼ŒæŠ¥å‘Šå¹³å‡å¾—åˆ†
  - è®­ç»ƒä¸æµ‹è¯• prompt æ¨¡æ¿ä¸åŒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç®€ä»‹ |
|------|------|
| **Vanilla-GRPO** | ä½¿ç”¨ LLM-as-a-Judge çš„ sequence-level reward è¿›è¡Œ GRPO ä¼˜åŒ– |
| **RuscaRL** | å½“å‰ SOTA æ–¹æ³•ï¼Œåˆ©ç”¨ rubric å¼•å¯¼æ¢ç´¢ç©ºé—´ |
| **DAPO / RLOO** | å…¶ä»– RLHF æ¡†æ¶ï¼Œç”¨äºéªŒè¯é€šç”¨æ€§ |
| **PRM / PURE / PQM** | åœ¨æ•°å­¦é¢†åŸŸè®­ç»ƒçš„ Process Reward Modelï¼Œä»£è¡¨ dense reward åŸºçº¿ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTable 1ï¼‰

| Model | HealthBench â†‘ | RaR-Medicine â†‘ | ResearchQA â†‘ | RaR-Science â†‘ |
|-------|----------------|----------------|---------------|----------------|
| Qwen2.5-1.5B-Instruct (vanilla) | 32.2 | 27.7 | 41.2 | 36.2 |
| â†’ +Vanilla-GRPO | 39.5 | 32.7 | 53.1 | 41.6 |
| â†’ +RuscaRL | 40.7 | 34.3 | 53.9 | 44.1 |
| â†’ **+GRAD2REWARD (Ours)** | **44.5** | **35.5** | **55.0** | **43.5** |

> âœ… å°æ¨¡å‹ä¸Šæå‡æ˜¾è‘—ï¼š+3.8 pts vs RuscaRL on HealthBenchï¼›+1.1 on ResearchQA

| Llama-3.1-8B-Instruct | HealthBench | RaR-Medicine | ResearchQA | RaR-Science |
|------------------------|--------------|---------------|-------------|---------------|
| Vanilla-GRPO | 47.8 | 56.7 | 65.9 | 54.5 |
| RuscaRL | 48.6 | 61.1 | 67.0 | 56.2 |
| **GRAD2REWARD** | **51.1** | **62.1** | **68.9** | **56.7** |

> âœ… å¤§æ¨¡å‹ä»èƒ½ç¨³å®šå¢ç›Šï¼Œå°¤å…¶åœ¨ HealthBench ä¸Šé¢†å…ˆ 2.5 pts

---

### ğŸ” æ³›åŒ–æ€§ä¸é²æ£’æ€§æµ‹è¯•
- **æ¢ test graderï¼ˆMistral-Small-24Bï¼‰**ï¼šæ€§èƒ½è¶‹åŠ¿ä¸€è‡´ï¼Œè¯æ˜æ— è¿‡æ‹Ÿåˆ
- **è·¨æ•°æ®é›†è¿ç§»ï¼ˆTable 6ï¼‰**
  - RaR-Medicine â†’ HealthBenchï¼š**42.1** vs 37.6 (Vanilla), 37.3 (RuscaRL)
  - RaR-Science â†’ GPQA-Diamondï¼ˆå¯éªŒè¯ï¼‰ï¼š**26.1** vs 24.7 / 25.6
  > âœ… æ˜¾ç¤ºå‡ºè‰¯å¥½çš„è·¨ä»»åŠ¡ã€è·¨æ ¼å¼æ³›åŒ–èƒ½åŠ›

---

### ğŸ” æ¶ˆèå®éªŒï¼ˆAblation Study, Table 2ï¼‰
æ¯”è¾ƒä¸åŒæ¢¯åº¦å½’å› æ–¹å¼ï¼š

| æ–¹æ³• | HealthBench | RaR-Science |
|------|-------------|------------|
| Vanilla-GRPO | 32.2 | 36.2 |
| L1 norm (grad only) | 38.7 | 37.8 |
| L2 norm (grad only) | 38.0 | 40.0 |
| **GRAD2REWARD (grad Ã— embed)** | **44.5** | **43.5** |

> âœ… è¯´æ˜ **Gradient Ã— Embedding** è®¾è®¡ä¼˜äºå•çº¯æ¢¯åº¦èŒƒæ•°ï¼Œå› å…¶ä¿ç•™æ–¹å‘ä¿¡æ¯ï¼Œç†è®ºæ›´åˆç†ã€‚

---

### â±ï¸ è®­ç»ƒæ•ˆç‡åˆ†æï¼ˆFigure 2ï¼‰
- **æ”¶æ•›é€Ÿåº¦åŠ å¿« 1.7Ã— ~ 1.9Ã—**
  - è¾¾åˆ°ç›¸åŒæ€§èƒ½æ‰€éœ€è®­ç»ƒæ­¥æ•°å‡å°‘è¿‘ä¸€åŠ
- **æ¸è¿‘æ€§èƒ½æ›´é«˜**
  - æœ€ç»ˆå¾—åˆ†ç›¸å¯¹æå‡è¾¾ **12â€“13%**

> ğŸ’¡ åŸå› ï¼šå¯†é›†å¥–åŠ±æä¾›æ›´ç²¾ç»†çš„ credit assignmentï¼ŒåŠ é€Ÿç­–ç•¥æ›´æ–°ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å¯†é›†å¥–åŠ±æ˜¾è‘—æå‡å¼€æ”¾åŸŸæ¨ç†è´¨é‡**ï¼šç›¸æ¯”ç¨€ç–å¥–åŠ±ï¼Œtoken-level supervision èƒ½æœ‰æ•ˆä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹çš„æ¯ä¸€ä¸ªç¯èŠ‚ã€‚
2. **Judge å†…éƒ¨è•´å«ä¸°å¯Œè¿‡ç¨‹åé¦ˆ**ï¼šé€šè¿‡æ¢¯åº¦å½’å› å¯â€œè§£ç â€å…¶éšå¼è¯„ä¼°é€»è¾‘ï¼Œæ— éœ€é¢å¤–æ ‡æ³¨æˆ–è®­ç»ƒ PRMã€‚
3. **Self-judging æœºåˆ¶å¯è¡Œä¸”é«˜æ•ˆ**ï¼šå³ä½¿ä¸ç”¨æ›´å¼º Judgeï¼Œä¹Ÿèƒ½å®ç°ä¼˜å¼‚æ€§èƒ½ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ã€‚
4. **æ–¹æ³•å…·å¤‡å¼ºæ³›åŒ–æ€§**ï¼š
   - è·¨æ¨¡å‹è§„æ¨¡ï¼ˆ1.5B â†’ 8Bï¼‰
   - è·¨ä»»åŠ¡ç±»å‹ï¼ˆåŒ»ç–—ã€ç§‘ç ”ã€ç§‘å­¦ï¼‰
   - è·¨é¢†åŸŸï¼ˆå¼€æ”¾åŸŸ + å¯éªŒè¯æ•°å­¦ï¼‰
5. **ä¼˜äºä¸“é—¨è®­ç»ƒçš„ PRM**ï¼šåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ˆTable 5ï¼‰ï¼ŒGRAD2REWARD åœ¨ 5/6 åŸºå‡†ä¸Šè¶…è¶Š PURE/PQM/PRMã€‚

---

### âš ï¸ å±€é™æ€§
1. **ä¾èµ– Judge çš„åˆ¤åˆ«èƒ½åŠ›**ï¼šè‹¥åˆå§‹ç­–ç•¥æœ¬èº«åˆ¤æ–­åŠ›å¼±ï¼Œå¯èƒ½æä¾›å™ªå£°åé¦ˆã€‚
2. **æ¢¯åº¦å½’å› å‡è®¾çº¿æ€§è¿‘ä¼¼æˆç«‹**ï¼šé«˜é˜¶éçº¿æ€§å½±å“æœªè¢«å»ºæ¨¡ã€‚
3. **è®¡ç®—å¼€é”€ç•¥å¢**ï¼šè™½åªéœ€ä¸€æ¬¡ backward passï¼Œä½†åœ¨é•¿åºåˆ—ä¸‹ä»æœ‰å»¶è¿Ÿã€‚
4. **å®‰å…¨é£é™©æ§åˆ¶éœ€å¤–æ¥æœºåˆ¶**ï¼šä¸èƒ½å®Œå…¨æ›¿ä»£äººå·¥å®¡æ ¸ï¼Œå°¤å…¶åœ¨åŒ»ç–—ã€æ³•å¾‹ç­‰é«˜å±åœºæ™¯ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³ Agent-based Long-horizon Tasks**ï¼šåº”ç”¨äºå¤šæ­¥å†³ç­–ã€å·¥å…·è°ƒç”¨ç­‰å¤æ‚ AI Agent åœºæ™¯ã€‚
2. **ç»“åˆ Safety Guardrails**ï¼šé›†æˆå¯æ§è§£ç ã€æ¯’æ€§è¿‡æ»¤ç­‰æ¨¡å—ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹åˆè§„ã€‚
3. **åŠ¨æ€è°ƒæ•´ Temperature æˆ– Sampling Strategy**ï¼šåŸºäº attribution score åŠ¨æ€èšç„¦å…³é”® tokenã€‚
4. **æ¢ç´¢å¤š Judge Ensemble**ï¼šèåˆå¤šä¸ª Judge çš„æ¢¯åº¦ä¿¡å·ä»¥æé«˜ç¨³å®šæ€§ã€‚
5. **ç†è®ºæ·±åŒ–**ï¼šå»ºç«‹ attribution-based reward ä¸ MDP æœ€ä¼˜ç­–ç•¥ä¹‹é—´çš„å½¢å¼åŒ–è”ç³»ã€‚

---

## æ€»ç»“ä¸€å¥è¯
> **GRAD2REWARD æˆåŠŸå°† LLM-as-a-Judge çš„â€œé»‘ç›’åˆ¤å†³â€è½¬åŒ–ä¸ºâ€œç™½ç›’è¿‡ç¨‹æŒ‡å¯¼â€ï¼Œå®ç°äº†æ— éœ€å¤–éƒ¨å¼ºæ¨¡å‹ã€æ— éœ€æ ‡æ³¨æ•°æ®ã€æ— éœ€è®­ç»ƒ PRM çš„é«˜è´¨é‡å¯†é›†å¥–åŠ±ç”Ÿæˆï¼Œåœ¨å¼€æ”¾åŸŸä¸å¯éªŒè¯ä»»åŠ¡ä¸­å‡å±•ç°å‡ºå“è¶Šæ€§èƒ½ä¸å¹¿æ³›é€‚ç”¨æ€§ã€‚**

</details>

---

### 12. [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)

**Authors**: Minhua Lin, Hanqing Lu, Zhan Shi, Bing He, Rui Mao, Zhiwei Zhang, Zongyu Wu, Xianfeng Tang, Hui Liu, Zhenwei Dai, Xiang Zhang, Suhang Wang, Benoit Dumoulin, Jian Pei  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.00359v1  

#### Abstract
As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but do...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**Position: Agentic Evolution is the Path to Evolving LLMs**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

å½“å‰çš„ **Large Language Models (LLMs)** åœ¨éƒ¨ç½²åˆ°å¼€æ”¾ã€åŠ¨æ€çš„çœŸå®ä¸–ç•Œç¯å¢ƒæ—¶é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§æŒ‘æˆ˜ï¼š**train-deploy gap**ï¼ˆè®­ç»ƒ-éƒ¨ç½²ç¯å¢ƒå·®è·ï¼‰ã€‚  
- æ¨¡å‹åœ¨é™æ€ã€æœ‰é™çš„æ•°æ®ä¸Šè®­ç»ƒï¼Œæ— æ³•åº”å¯¹éƒ¨ç½²åä¸æ–­å˜åŒ–çš„APIã€æ ¼å¼ã€ç”¨æˆ·éœ€æ±‚ç­‰ã€‚
- ç°æœ‰çš„é€‚åº”æ–¹æ³•ï¼ˆå¦‚åœ¨çº¿å¾®è°ƒæˆ–è®°å¿†ç§¯ç´¯ï¼‰è¦ä¹ˆä¸ç¨³å®šï¼ˆå¦‚ç¾éš¾æ€§é—å¿˜ï¼‰ï¼Œè¦ä¹ˆæ•ˆç‡ä½ä¸‹ï¼ˆå¦‚ä¸Šä¸‹æ–‡é¥±å’Œã€é‡å¤æ¨ç†ï¼‰ã€‚

### ğŸš€ æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

æå‡º **Agentic Evolutionï¼ˆä»£ç†å¼è¿›åŒ–ï¼‰** èŒƒå¼ï¼Œå¹¶æ„å»ºé€šç”¨æ¡†æ¶ **A-Evolve**ï¼Œå°†æ¨¡å‹çš„éƒ¨ç½²æœŸæ”¹è¿›è§†ä¸ºä¸€ä¸ª**ç›®æ ‡å¯¼å‘ã€è‡ªä¸»å†³ç­–çš„ä¼˜åŒ–è¿‡ç¨‹**ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- å°†ä¼ ç»Ÿçš„å›ºå®šæ›´æ–°è§„åˆ™ $ F_{\text{Evolve}} $ æ›¿æ¢ä¸ºä¸€ä¸ªæ˜¾å¼çš„ **Evolver Agent**ï¼ˆè¿›åŒ–ä»£ç†ï¼‰ï¼Œè¯¥ä»£ç†å…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š
  - **Goal-Oriented Principle**ï¼ˆç›®æ ‡å¯¼å‘ï¼‰ï¼šä¸»åŠ¨è¯Šæ–­å¤±è´¥åŸå› ï¼Œå®šä½å¯ä¿®å¤çš„ç»„ä»¶ï¼ˆå¦‚å·¥å…·ã€çŸ¥è¯†ã€éªŒè¯é€»è¾‘ï¼‰ã€‚
  - **Autonomy Principle**ï¼ˆè‡ªä¸»æ€§ï¼‰ï¼šå†³å®šä½•æ—¶è§¦å‘è¿›åŒ–ï¼Œæ˜¯å¦æäº¤æ›´æ–°ï¼Œé¿å…ç›²ç›®ä¿®æ”¹ã€‚
  - **Compositional Principle**ï¼ˆç»„åˆæ€§ï¼‰ï¼šç”Ÿæˆæ¨¡å—åŒ–ã€ç»“æ„åŒ–çš„æ”¹è¿›ï¼ˆå¦‚å¯æ‰§è¡Œå·¥å…·ã€æµ‹è¯•ç”¨ä¾‹ï¼‰ï¼Œå¹¶é€šè¿‡éªŒè¯æœºåˆ¶ç¡®ä¿å®‰å…¨æ€§ã€‚

#### æ¡†æ¶ A-Evolve åŒ…å«ä¸‰å¤§ç»„ä»¶ï¼š
1. **Persistent Artifact State $ \mathcal{T}_s $**ï¼šéå‚æ•°åŒ–çŠ¶æ€ï¼ŒåŒ…æ‹¬ï¼š
   - **Knowledge Registry (K)**ï¼šç»“æ„åŒ–çŸ¥è¯†ï¼ˆå¦‚APIè§„èŒƒï¼‰
   - **Tool Registry (T)**ï¼šå¯æ‰§è¡Œå‡½æ•°
   - **Validation Registry (V)**ï¼šå•å…ƒæµ‹è¯•ã€å›å½’æ£€æŸ¥
2. **Solve-Evolve Loop**ï¼šåˆ†ç¦»ä»»åŠ¡æ±‚è§£ï¼ˆSolveï¼‰ä¸ç³»ç»Ÿè¿›åŒ–ï¼ˆEvolveï¼‰ï¼Œç¡®ä¿è¿›åŒ–æ˜¯å—æ§è¿‡ç¨‹ã€‚
3. **Evolver Agent**ï¼šç”±å››ä¸ªåä½œæ¨¡å—ç»„æˆï¼š
   - **Diagnose** â†’ **Plan** â†’ **Update** â†’ **Verify**

### â­ ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³•ç±»å‹ | ä»£è¡¨ | ç¼ºé™· | A-Evolve çš„ä¼˜åŠ¿ |
|--------|------|------|----------------|
| **Parametric Fine-tuning** | Test-time training, LoFiT | ç¾éš¾æ€§é—å¿˜ã€ä¸å¯å®¡è®¡ã€æƒé‡æ›´æ–°ä¸é€æ˜ | æ›´æ–°æ˜¾å¼ã€å¯éªŒè¯ã€ä¿ç•™åŸå§‹æ¨¡å‹ç¨³å®šæ€§ |
| **Non-parametric Heuristic** | APE, AWM, Memory Append | ä¸Šä¸‹æ–‡é¥±å’Œã€æ£€ç´¢å™ªå£°ã€æ— å› æœç†è§£ | ç»“æ„åŒ–çŸ¥è¯†ç®¡ç†ï¼Œæ”¯æŒå¤ç”¨ä¸æ²»ç† |
| **Static Pipelines** | å›ºå®šæç¤ºå·¥ç¨‹ã€æœç´¢å¼ä¼˜åŒ– | æ— æ³•å¤„ç†å¤æ‚ç»“æ„æ€§å˜æ›´ | æ”¯æŒå¤šæ­¥è§„åˆ’ä¸è·¨ç»„ä»¶åè°ƒ |

> âœ… **æœ¬è´¨åŒºåˆ«**ï¼šä¼ ç»Ÿæ–¹æ³•æ˜¯â€œæœºæ¢°å¼å“åº”â€ï¼Œè€Œ A-Evolve æ˜¯â€œæ™ºèƒ½ä½“çº§å†³ç­–â€ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†

- **AppWorld** (Trivedi et al., 2024)ï¼šä¸€ä¸ªç”¨äºè¯„ä¼°äº¤äº’å¼ç¼–ç æ™ºèƒ½ä½“çš„åŸºå‡†å¹³å°ã€‚
  - æ¨¡æ‹ŸçœŸå®åº”ç”¨ç”Ÿæ€ï¼ˆå¦‚ Amazon, Spotify, Venmo, Gmailï¼‰
  - æä¾› 457 ä¸ª API å’Œçº¦ 750 ä¸ªä»»åŠ¡
  - åŒ…å« **test-normal** å’Œ **test-challenge** åˆ†å‰²ï¼Œåè€…è¦æ±‚ä½¿ç”¨è®­ç»ƒæœªè§çš„åº”ç”¨

- å®éªŒè®¾ç½®ï¼š
  - **è®­ç»ƒé›†**ï¼š50 ä¸ªä»»åŠ¡ï¼ˆç”¨äºè¿›åŒ–å­¦ä¹ ï¼‰
  - **æµ‹è¯•é›†**ï¼š50 ä¸ªä»»åŠ¡ï¼ˆè¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼‰

### ğŸ§ª å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### è¯„ä¼°åè®®ï¼š
- å›ºå®š **solve-time compute budget**ï¼ˆæœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ã€æ­¥éª¤æ•°ã€token æ•°ï¼‰
- å›ºå®š **evolve-time compute budget**ï¼ˆæ¯è½®è¿›åŒ–çš„ token å’Œå·¥å…·è°ƒç”¨ä¸Šé™ï¼‰
- æ‰€æœ‰æ–¹æ³•åœ¨æ­¤é¢„ç®—ä¸‹å…¬å¹³æ¯”è¾ƒ

#### ä¸»è¦è¯„ä¼°æŒ‡æ ‡ï¼š
| æŒ‡æ ‡ | å…¬å¼ | å«ä¹‰ |
|------|------|------|
| **TGC (Task Goal Completion)** | $ \frac{1}{N} \sum_{i=1}^N \mathbb{I}(s_i = 1) $ | å®Œå…¨æˆåŠŸå®Œæˆçš„ä»»åŠ¡æ¯”ä¾‹ï¼ˆä¸¥æ ¼æ ‡å‡†ï¼‰ |
| **APT (Average Passed Tests)** | $ \frac{1}{N} \sum_{i=1}^N s_i $ | å¹³å‡é€šè¿‡çš„å•å…ƒæµ‹è¯•æ¯”ä¾‹ï¼ˆåæ˜ æ¸è¿›æ”¹è¿›ï¼‰ |

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿æ–¹æ³• | ç±»å‹ | æè¿° |
|---------|------|------|
| **Vanilla** | æ— è¿›åŒ– | ç›´æ¥è°ƒç”¨ solverï¼Œæ— ä»»ä½•æŒä¹…åŒ–æ”¹è¿› |
| **APE** (Zhou et al., 2022) | éå‚æ•°å¯å‘å¼ | åŸºäºæœç´¢çš„æç¤ºè¯è¿›åŒ–ï¼Œé€šè¿‡ä»»åŠ¡è¯„åˆ†é€‰æ‹©æŒ‡ä»¤ |
| **AWM** (Wang et al., 2024) | éå‚æ•°å¯å‘å¼ | ä»å†å²è½¨è¿¹ä¸­æå–å¯å¤ç”¨çš„å·¥ä½œæµä½œä¸ºè®°å¿† |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

| Method | Solver | TGC (%) | APT (%) |
|--------|--------|--------|--------|
| Vanilla | Haiku 4.5 | 32 | 51.16 |
| APE | Haiku 4.5 | 30 | 56.00 |
| AWM | Haiku 4.5 | 46 | 65.76 |
| **A-Evolve** | **Haiku 4.5** | **64** | **84.31** |
| Vanilla | Gemini 3 Flash | 56 | 80.45 |
| AWM | Gemini 3 Flash | 52 | 87.75 |
| **A-Evolve** | **Gemini 3 Flash** | **82** | **92.05** |

> âœ… **ç»“è®º**ï¼šA-Evolve åœ¨æ‰€æœ‰ solver ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå°¤å…¶å¯¹å°æ¨¡å‹æå‡å·¨å¤§ï¼ˆHaiku 4.5 çš„ TGC æå‡ +32ptsï¼‰ã€‚

### ğŸ” ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

- **èƒ½åŠ›æ”¾å¤§æ•ˆåº”ï¼ˆCapability Multiplierï¼‰**ï¼š
  - A-Evolve èƒ½å°†è¾ƒå°æ¨¡å‹ï¼ˆå¦‚ Haiku 4.5ï¼‰çš„èƒ½åŠ›æå‡è‡³æ¥è¿‘ç”šè‡³è¶…è¿‡æ›´å¤§æ¨¡å‹ï¼ˆå¦‚ Sonnet 4ï¼‰çš„åŸå§‹æ°´å¹³ã€‚
  - è¡¨æ˜ï¼š**æŒä¹…åŒ–ç¨‹åºæ€§æ”¹è¿›å¯ä»¥åª²ç¾æ¨¡å‹è§„æ¨¡æ‰©å±•å¸¦æ¥çš„æ”¶ç›Š**ã€‚

- **ç¼©å°å®¹é‡å·®è·ï¼ˆNarrowing the Capacity Gapï¼‰**ï¼š
  - Haiku 4.5 + A-Evolve â†’ 64% TGC
  - Vanilla Sonnet 4 â†’ 42% TGC
  - **å°æ¨¡å‹ + è¿›åŒ– > å¤§æ¨¡å‹ + é™æ€æ¨ç†**

### ğŸ”§ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

åœ¨ **Fig. 3** ä¸­å¯¹ A-Evolve è¿›è¡Œæ¨¡å—ç§»é™¤åˆ†æï¼š

| å˜ä½“ | æè¿° | æ€§èƒ½å½±å“ |
|------|------|----------|
| **A-Evolve/D** | ç§»é™¤ Diagnose | æ— æ³•è¯†åˆ«æ ¹æœ¬åŸå› ï¼Œä»…åšè¡¨é¢ä¿®è¡¥ |
| **A-Evolve/A** | ç§»é™¤ Analysis Tools | ä¾èµ–å•æ¬¡è½¨è¿¹æ¨æ–­ï¼Œé”™è¿‡è·¨ä»»åŠ¡æ¨¡å¼ |
| **A-Evolve/P** | ç§»é™¤ Plan | æ— æ³•åè°ƒå¤šç»„ä»¶æ›´æ–°ï¼Œå¯¼è‡´ä¸ä¸€è‡´ |
| **A-Evolve/V** | ç§»é™¤ Verify | æäº¤ç¼ºé™·å·¥å…·ï¼Œæ±¡æŸ“ä¸Šä¸‹æ–‡ï¼Œå¼•å‘é€€åŒ– |

> â— æœ€ä¸¥é‡çš„æ˜¯ **ç§»é™¤éªŒè¯ï¼ˆA-Evolve/Vï¼‰**ï¼Œå¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œè¯´æ˜ **Verification æ˜¯ç¨³å®šé•¿æœŸè¿›åŒ–çš„å…³é”®å®ˆé—¨å‘˜**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ğŸ¯ è®ºæ–‡çš„ä¸»è¦å‘ç°

1. **Agentic Evolution æ˜¯è§£å†³ train-deploy gap çš„å¿…ç„¶è·¯å¾„**ï¼š
   - é™æ€è®­ç»ƒå’Œå•çº¯æ‰©å¤§æ¨ç†è®¡ç®—æ— æ³•åº”å¯¹æŒç»­å˜åŒ–çš„ç°å®ç¯å¢ƒã€‚
   - å¿…é¡»å¼•å…¥**è‡ªä¸»ã€ç›®æ ‡é©±åŠ¨çš„è¿›åŒ–æœºåˆ¶**æ¥å®ç°å¯æŒç»­é€‚åº”ã€‚

2. **è¿›åŒ–æœ¬èº«åº”è¢«è§†ä¸ºå¯æ‰©å±•çš„ç¬¬ä¸‰ç»´åº¦**ï¼š
   - æå‡º **Evolution-Scaling Hypothesis**ï¼š
     > â€œé€‚åº”èƒ½åŠ›â€éšç€åˆ†é…ç»™è¿›åŒ–è¿‡ç¨‹çš„è®¡ç®—èµ„æºï¼ˆ$ C_{\text{evolve}} $ï¼‰å¢åŠ è€Œç³»ç»Ÿæ€§æå‡ã€‚
   - å®éªŒéªŒè¯ï¼šå¢åŠ è¿›åŒ–æ­¥æ•°æˆ–ä½¿ç”¨æ›´å¼º evolverï¼ˆå¦‚ Opus > Sonnet > Haikuï¼‰ï¼Œæ€§èƒ½å•è°ƒä¸Šå‡ã€‚

3. **ç»“æ„åŒ–ã€å¯éªŒè¯çš„æ”¹è¿›ä¼˜äºé»‘ç®±å¾®è°ƒæˆ–æ–‡æœ¬è®°å¿†**ï¼š
   - A-Evolve é€šè¿‡ç”Ÿæˆ **verified tools, reusable workflows, structured knowledge**ï¼Œå®ç°äº†**æ¨ç†æˆæœ¬çš„æ‘Šé”€**ï¼ˆamortizationï¼‰ã€‚
   - æˆåŠŸå°†â€œæ¯æ¬¡éƒ½è¦é‡æ–°æ€è€ƒâ€çš„é—®é¢˜è½¬åŒ–ä¸ºâ€œä¸€æ¬¡è§£å†³ï¼Œæ°¸ä¹…å¤ç”¨â€ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

- **è®¡ç®—å¼€é”€è¾ƒé«˜**ï¼šç»´æŠ¤ Evolver Agent å’ŒéªŒè¯æµç¨‹éœ€è¦é¢å¤– computeï¼ŒçŸ­æœŸä¸å¦‚ heuristic æ–¹æ³•é«˜æ•ˆã€‚
- **ä¾èµ–é«˜è´¨é‡åé¦ˆä¿¡å·**ï¼šè‹¥ç¯å¢ƒç¼ºä¹æ˜ç¡®é”™è¯¯æˆ–æµ‹è¯•ç”¨ä¾‹ï¼Œè¯Šæ–­å¯èƒ½å¤±æ•ˆã€‚
- **å½“å‰å®ç°åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚ API è°ƒç”¨ï¼‰æ›´æœ‰æ•ˆ**ï¼Œé€šç”¨æ€§æœ‰å¾…è¿›ä¸€æ­¥éªŒè¯ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **Benchmark Development**ï¼š
   - è®¾è®¡ä¸“é—¨è¡¡é‡â€œæ¼”åŒ–èƒ½åŠ›â€çš„æ–°åŸºå‡†ï¼Œå…³æ³¨**æ”¹è¿›çš„æŒä¹…æ€§ã€å¤ç”¨ç‡ã€æŠ—åˆ†å¸ƒåç§»èƒ½åŠ›**ã€‚

2. **Framework Optimization**ï¼š
   - æå‡ Diagnoseã€Planã€Verify æ¨¡å—çš„æ•ˆç‡ä¸é²æ£’æ€§ï¼Œé™ä½ $ C_{\text{evolve}} $ å¼€é”€ã€‚

3. **ç†è®ºå»ºæ¨¡**ï¼š
   - å»ºç«‹ agentic evolution çš„å½¢å¼åŒ–ç†è®ºï¼Œä¾‹å¦‚å°†å…¶å»ºæ¨¡ä¸ºç»„åˆç¨‹åºç©ºé—´ä¸Šçš„ä¼˜åŒ–é—®é¢˜ã€‚
   - æ¢ç´¢å…¶ç›¸å¯¹äºéä»£ç†æ–¹æ³•çš„ regret boundsã€‚

4. **å®‰å…¨ä¸å¯¹é½æœºåˆ¶å¼ºåŒ–**ï¼š
   - å¼ºåŒ– Validation Gateï¼Œå¼•å…¥äººç±»å®¡æ ¸ hook æˆ– alignment constraintsï¼Œé˜²æ­¢ç›®æ ‡æ¼‚ç§»ï¼ˆgoal driftï¼‰ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯

> **Agentic Evolution å°† LLM çš„éƒ¨ç½²æœŸé€‚åº”ä»â€œè¢«åŠ¨ååº”â€å‡çº§ä¸ºâ€œä¸»åŠ¨è¿›åŒ–â€**ï¼Œé€šè¿‡ A-Evolve æ¡†æ¶å®ç°**å¯æ²»ç†ã€å¯æ‰©å±•ã€å¯æŒç»­çš„æ™ºèƒ½ä½“æˆé•¿è·¯å¾„**ï¼Œæ˜¯è¿ˆå‘çœŸæ­£å¼€æ”¾ç¯å¢ƒä¸­ AGI çš„å…³é”®ä¸€æ­¥ã€‚

</details>

---

### 13. [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)

**Authors**: Yue Zhong, Jiawen Kang, Yongju Tong, Hong-Ning Dai, Dong In Kim, Abbas Jamalipour, Shengli Xie  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.01131v1  

#### Abstract
With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous netw...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹**ä½ç©ºç»æµ**ï¼ˆLow-Altitude Economy, LAEï¼‰ä¸­æ— äººæœº**ï¼ˆUAVï¼‰**ä½œä¸ºç©ºä¸­åŸºç«™æ—¶é¢ä¸´çš„èµ„æºåˆ†é…ä¸ç‰©ç†æ§åˆ¶ç¨³å®šæ€§ä¹‹é—´çš„çŸ›ç›¾é—®é¢˜ã€‚ä¼ ç»Ÿé€šä¿¡ç³»ç»Ÿå¤šä»¥ååé‡ä¸ºä¸­å¿ƒï¼Œä½†åœ¨ç´§æ€¥æ•‘æ´ç­‰å®‰å…¨å…³é”®ä»»åŠ¡ä¸­ï¼Œ**é€šä¿¡å»¶è¿Ÿç›´æ¥å½±å“æ§åˆ¶ç³»ç»Ÿçš„ç¨³å®šæ€§**ï¼ˆå¦‚ä½ç½®è·Ÿè¸ªè¯¯å·®ï¼‰ï¼Œè€Œç°æœ‰ç ”ç©¶å¤§å¤šå°†é€šä¿¡ä¸æ§åˆ¶è§£è€¦å¤„ç†ï¼Œç¼ºä¹é—­ç¯ååŒè®¾è®¡ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
1. **æ„å»ºäº† Sensing-Communication-Computing-Control (SCÂ³) é—­ç¯æ¡†æ¶**  
   é¦–æ¬¡åœ¨LAEåœºæ™¯ä¸­æ˜¾å¼å»ºæ¨¡äº†ä»æ„ŸçŸ¥åˆ°æ§åˆ¶çš„ç«¯åˆ°ç«¯å»¶è¿Ÿä¼ æ’­æœºåˆ¶ï¼Œå°†ç‰©ç†å±‚çš„æ§åˆ¶ç¨³å®šæ€§éœ€æ±‚ä¸é€»è¾‘å±‚çš„é€šä¿¡èµ„æºåˆ†é…æ·±åº¦èåˆã€‚

2. **åŸºäº Lyapunov ç¨³å®šæ€§ç†è®ºè¿›è¡Œæ§åˆ¶-é€šä¿¡æ˜ å°„**  
   åˆ©ç”¨ Lyapunov å‡½æ•°å¯¹ UAV çš„çŠ¶æ€è¯¯å·®è¿›è¡Œå»ºæ¨¡ï¼Œæ¨å¯¼å‡ºæ»¡è¶³ç³»ç»Ÿç¨³å®šæ€§çš„**æœ€å¤§å…è®¸é€šä¿¡å»¶è¿Ÿè¾¹ç•Œ**ï¼ˆ`T_budget`ï¼‰ï¼Œä»è€Œå°†æŠ½è±¡çš„â€œç¨³å®šæ€§è¦æ±‚â€è½¬åŒ–ä¸ºå¯é‡åŒ–çš„é€šä¿¡èµ„æºçº¦æŸã€‚

3. **æå‡º Stackelberg åšå¼ˆæ¨¡å‹å®ç°åˆ†å±‚èµ„æºç®¡ç†**  
   å°† UAV è®¾ä¸ºé¢†å¯¼è€…ï¼ˆLeaderï¼‰ï¼ŒåŠ¨æ€å®šä»·å¸¦å®½ï¼›ç”¨æˆ·ä½œä¸ºè·Ÿéšè€…ï¼ˆFollowerï¼‰ï¼Œæ ¹æ®ä»·æ ¼å’Œè‡ªèº«ç´§æ€¥ç¨‹åº¦ä¼˜åŒ–å¸¦å®½è¯·æ±‚ã€‚è¯¥æœºåˆ¶å®ç°äº†è´Ÿè½½å‡è¡¡ä¸ç¨³å®šæ€§ä¿éšœçš„ç»Ÿä¸€ã€‚

4. **è®¾è®¡è½»é‡çº§ Pruning-based PPO ç®—æ³•ç”¨äºè¾¹ç¼˜éƒ¨ç½²**  
   é’ˆå¯¹æ ‡å‡† DRL æ¨¡å‹å‚æ•°é‡å¤§ã€èƒ½è€—é«˜çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§é›†æˆ**åŠ¨æ€ç»“æ„åŒ–å‰ªæ**ï¼ˆdynamic structured pruningï¼‰çš„ PPO ç®—æ³•ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å‹ç¼©ç½‘ç»œè§„æ¨¡ï¼Œæ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿå’Œèƒ½è€—ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | æœ¬æ–‡æ–¹æ³• | ç°æœ‰æ–¹æ³• |
|------|---------|--------|
| **ç³»ç»Ÿè§†è§’** | SCÂ³ é—­ç¯ï¼Œè”åˆä¼˜åŒ–æ§åˆ¶ä¸é€šä¿¡ | å¤šä¸ºé€šä¿¡å•ç›®æ ‡ä¼˜åŒ– |
| **ç¨³å®šæ€§ä¿éšœ** | æ˜¾å¼å¼•å…¥ Lyapunov æ¡ä»¶ä½œä¸ºç¡¬çº¦æŸ | å¿½ç•¥æˆ–å¼±åŒ–æ§åˆ¶ç¨³å®šæ€§ |
| **æ¿€åŠ±æœºåˆ¶** | å®šä»·ç­–ç•¥ç”±ç¨³å®šæ€§é©±åŠ¨ï¼Œå…·å¤‡è·¨åŸŸè°ƒæ§èƒ½åŠ› | ä»…åŸºäºèµ„æºç¨€ç¼ºæ€§å®šä»· |
| **ç®—æ³•æ•ˆç‡** | åŠ¨æ€å‰ªæå®ç°è½»é‡åŒ–ï¼Œé€‚åˆ UAV è¾¹ç¼˜éƒ¨ç½² | ä½¿ç”¨å…¨è¿æ¥é‡å‹ç½‘ç»œï¼Œéš¾ä»¥å®æ—¶è¿è¡Œ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### å®éªŒç¯å¢ƒä¸ä»¿çœŸè®¾ç½®
- **ä»¿çœŸå¹³å°**ï¼šè‡ªå®šä¹‰ Python/MATLAB è”åˆä»¿çœŸç¯å¢ƒï¼Œæ¨¡æ‹Ÿå¤š UAV ä¸å¤šç”¨æˆ·äº¤äº’ã€‚
- **åœºæ™¯è®¾å®š**ï¼šåœ°é¢åŸºç¡€è®¾æ–½ç˜«ç—ªä¸‹çš„ç¾ååº”æ€¥å“åº”åœºæ™¯ï¼ŒUAV ä½œä¸ºç©ºä¸­åŸºç«™æä¾›é€šä¿¡ä¸æ§åˆ¶æœåŠ¡ã€‚
- **ç³»ç»Ÿå‚æ•°**ï¼ˆéƒ¨åˆ†å…³é”®å€¼è§ Table Iï¼‰ï¼š
  - UAV æ•°é‡ï¼š3 æˆ– 12
  - ç”¨æˆ·æ•°é‡ï¼š5 ~ 20
  - é‡‡æ ·å‘¨æœŸ `Îµ_n`ï¼š0.5s
  - æ§åˆ¶æ—¶é—´å¸¸æ•° `Ï„_n`ï¼š0.005s
  - Lyapunov è¡°å‡ç‡ `Ï_n`ï¼š0.95
  - æ€»å¸¦å®½èŒƒå›´ï¼š[15, 25] MHz
  - æ•°æ®åŒ…å¤§å°ï¼š[40, 64] kbits

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Test Reward** | UAV çš„å¹³å‡æ•ˆç”¨ï¼ˆUtilityï¼‰ï¼Œç»¼åˆè€ƒè™‘æ”¶ç›Šä¸æˆæœ¬ |
| **Bandwidth Utilization** | å¸¦å®½ä½¿ç”¨æ•ˆç‡åŠè´Ÿè½½å‡è¡¡æƒ…å†µ |
| **Latency Distribution** | ç«¯åˆ°ç«¯ SCÂ³ å»¶è¿Ÿåˆ†å¸ƒï¼Œæ˜¯å¦æ»¡è¶³ç¨³å®šæ€§è¦æ±‚ |
| **Convergence Speed** | ç®—æ³•è¾¾åˆ°ç¨³å®šç­–ç•¥æ‰€éœ€çš„è¿­ä»£æ¬¡æ•° |
| **Inference Latency / Model Size** | å‰ªæå‰åæ¨¡å‹çš„è®¡ç®—å¼€é”€å¯¹æ¯” |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Standard PPO**ï¼šæœªå‰ªæçš„æ ‡å‡†å¤šæ™ºèƒ½ä½“ PPO ç®—æ³•
- **Greedy Algorithm**ï¼šUAV å›ºå®šé«˜ä»·æˆ–æŒ‰è´Ÿè½½è°ƒæ•´ä»·æ ¼ï¼Œç”¨æˆ·è´ªå¿ƒç”³è¯·
- **Random Algorithm**ï¼šéšæœºå®šä»·ä¸èµ„æºè¯·æ±‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ
#### ï¼ˆ1ï¼‰å­¦ä¹ æ€§èƒ½ä¸æ”¶æ•›æ€§ï¼ˆFig. 3ï¼‰
- **Pruning-based PPO** åœ¨çº¦ 400 è½®åæ”¶æ•›ï¼Œæœ€ç»ˆæµ‹è¯•å¥–åŠ±è¾¾ **~7.8**
- æ ‡å‡† PPO æ”¶æ•›è¾ƒæ…¢ä¸”æ³¢åŠ¨æ›´å¤§ï¼Œæœ€ç»ˆå¥–åŠ±çº¦ä¸º **7.6**
- Greedy å’Œ Random ç®—æ³•åœæ»åœ¨ä½æ°´å¹³ï¼ˆ< 4.0ï¼‰
> âœ… **ä¼˜åŠ¿**ï¼šå‰ªæä¸ä»…æœªæŸå®³æ€§èƒ½ï¼Œåè€Œå› æ­£åˆ™åŒ–æ•ˆæœæå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒå‰ªæèµ·å§‹æ—¶æœºçš„å½±å“ï¼ˆFig. 4ï¼‰
- æœ€ä¼˜è®¾ç½®ä¸º **pruning start epoch `tâ‚€ = 50`**ï¼ˆæ—©æœŸå‰ªæï¼‰
- æ­¤æ—¶æ¨¡å‹èƒ½ä»ä¸€å¼€å§‹å°±é€‚åº”ç¨€ç–ç»“æ„ï¼Œé¿å…åæœŸç»“æ„è°ƒæ•´å¸¦æ¥çš„éœ‡è¡
- `tâ‚€ = 200` æ—¶æ€§èƒ½æœ€å·®ï¼Œè¯´æ˜ä¸­æœŸå‰ªæä¼šç ´åæ­£åœ¨å½¢æˆçš„ç­–ç•¥æƒ¯æ€§

#### ï¼ˆ3ï¼‰ä¸åŒç”¨æˆ·/UAV è§„æ¨¡ä¸‹çš„è¡¨ç°ï¼ˆFig. 6ï¼‰
- å½“ç”¨æˆ·æ•°å¢åŠ ï¼ˆå›ºå®š UAV=3ï¼‰æ—¶ï¼ŒUAV æ•ˆç”¨æ˜¾è‘—ä¸Šå‡ â†’ è¡¨æ˜æœºåˆ¶èƒ½æœ‰æ•ˆæ¿€åŠ±æœåŠ¡æ›´å¤šç”¨æˆ·
- å½“ UAV æ•°å¢åŠ ï¼ˆå›ºå®šç”¨æˆ·=15ï¼‰æ—¶ï¼Œ**å•ä¸ª UAV å¹³å‡æ•ˆç”¨ä¸‹é™** â†’ åæ˜ å¸‚åœºé¥±å’Œæ•ˆåº”ï¼Œç¬¦åˆ Stackelberg ç»æµè§„å¾‹

#### ï¼ˆ4ï¼‰èµ„æºåˆ†é…åˆç†æ€§éªŒè¯ï¼ˆFig. 2ï¼‰
- é«˜ä¼˜å…ˆçº§ç”¨æˆ·ï¼ˆé«˜ urgency weightï¼‰è·å¾—æ›´é«˜å¸¦å®½é…é¢
- ä¿¡é“è´¨é‡å¥½ï¼ˆé«˜ SNRï¼‰çš„ç”¨æˆ·ä¹Ÿè·å¾—æ›´å¤šèµ„æº
- æ‰€æœ‰ç”¨æˆ·çš„å®é™…åˆ†é…å¸¦å®½å‡é«˜äºç”± Lyapunov æ¨å¯¼å‡ºçš„æœ€å°éœ€æ±‚ï¼ˆ`K_min`ï¼‰ï¼Œç¡®ä¿äº†ç¨³å®šæ€§åº•çº¿

#### ï¼ˆ5ï¼‰æ¨¡å‹å‹ç¼©æ•ˆæœï¼ˆéšå«äºç®—æ³•æè¿°ï¼‰
- é€šè¿‡åŠ¨æ€ç»“æ„åŒ–å‰ªæï¼Œç¥ç»ç½‘ç»œå‚æ•°é‡å‡å°‘ **>50%**
- æ¨ç†å»¶è¿Ÿé™ä½ï¼Œæ›´é€‚åˆéƒ¨ç½²åœ¨èƒ½æºå—é™çš„ UAV å¹³å°

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **æ§åˆ¶ç¨³å®šæ€§å¯é€šè¿‡ Lyapunov ç†è®ºç²¾ç¡®è½¬åŒ–ä¸ºé€šä¿¡èµ„æºçº¦æŸ**ï¼Œä¸ºâ€œæ§åˆ¶-é€šä¿¡â€è”åˆè®¾è®¡æä¾›äº†ç†è®ºåŸºç¡€ã€‚
2. **Stackelberg åšå¼ˆæ˜¯å»ºæ¨¡ UAV ä¸ç”¨æˆ·é—´èµ„æºåšå¼ˆçš„æœ‰æ•ˆå·¥å…·**ï¼Œå…¶å‡è¡¡ç‚¹è‡ªç„¶å®ç°äº†ç´§æ€¥ä»»åŠ¡ä¼˜å…ˆã€éç´§æ€¥ä»»åŠ¡ä½æˆæœ¬æ¥å…¥çš„åˆ†å±‚æœåŠ¡æœºåˆ¶ã€‚
3. **åŠ¨æ€ç»“æ„åŒ–å‰ªæå¯åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹å¤§å¹…å‹ç¼© DRL æ¨¡å‹**ï¼Œè§£å†³äº†é‡å‹ DRL éš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²çš„é—®é¢˜ã€‚
4. **æ—©æœŸå‰ªæä¼˜äºæ™šæœŸå‰ªæ**ï¼Œè¡¨æ˜â€œè¾¹è®­ç»ƒè¾¹å‹ç¼©â€çš„ç­–ç•¥æ›´æœ‰åˆ©äºç­–ç•¥ç¨³å®šæ€§å’Œæ¨¡å‹è½»é‡åŒ–ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å‡è®¾çº¿æ€§æ§åˆ¶æ¨¡å‹**ï¼šåŸºäº LTI ç³»ç»Ÿå»ºæ¨¡ï¼Œå¯èƒ½æ— æ³•å®Œå…¨åæ˜ å¤æ‚éçº¿æ€§é£è¡ŒåŠ¨åŠ›å­¦ã€‚
- **é›†ä¸­å¼è®­ç»ƒåˆ†å¸ƒå¼æ‰§è¡Œ**ï¼ˆCTDEï¼‰ä¾èµ–å…¨å±€ä¿¡æ¯å›ä¼ ï¼Œå¯¹é€šä¿¡å¯é æ€§æœ‰ä¸€å®šè¦æ±‚ã€‚
- **æœªè€ƒè™‘èƒ½é‡åŠ¨æ€å˜åŒ–**ï¼šå½“å‰æ¨¡å‹å‡è®¾ UAV èƒ½é‡å……è¶³ï¼Œæœªå»ºæ¨¡ç”µæ± è¡°å‡è¿‡ç¨‹ã€‚
- **é™æ€æ‹“æ‰‘å‡è®¾**ï¼šç”¨æˆ·ç§»åŠ¨æ€§å»ºæ¨¡æœ‰é™ï¼Œæœªæ·±å…¥åˆ†æé«˜é€Ÿç§»åŠ¨ä¸‹çš„é²æ£’æ€§ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **è”åˆå»ºæ¨¡èƒ½é‡ä¸æ—¶å˜å¸¦å®½èµ„æº**ï¼Œç ”ç©¶å¤åˆèµ„æºæ³¢åŠ¨ä¸‹çš„ç«¯åˆ°ç«¯å»¶è¿Ÿæ§åˆ¶ã€‚
2. **æ·±åŒ–åŒå‘ååŒè®¾è®¡**ï¼šæ¢ç´¢æ§åˆ¶æŒ‡ä»¤åé¦ˆå¦‚ä½•åå‘å½±å“é€šä¿¡è°ƒåº¦å†³ç­–ã€‚
3. **æ‰©å±•è‡³å¼‚æ„ UAV ç½‘ç»œ**ï¼šæ”¯æŒä¸åŒç±»å‹ï¼ˆå›ºå®šç¿¼ vs å¤šæ—‹ç¿¼ï¼‰ã€ä¸åŒèƒ½åŠ›çš„æ··åˆç¼–é˜Ÿã€‚
4. **å¼•å…¥ç”Ÿæˆå¼ AI è¾…åŠ©å†³ç­–**ï¼šåˆ©ç”¨ LLM æˆ–æ‰©æ•£æ¨¡å‹å¢å¼ºä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„ç­–ç•¥ç”Ÿæˆèƒ½åŠ›ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬è®ºæ–‡å¼€åˆ›æ€§åœ°å°† **Lyapunov ç¨³å®šæ€§ç†è®º** å¼•å…¥ LAE ä¸­çš„èµ„æºåˆ†é…é—®é¢˜ï¼Œæ„å»ºäº†é¦–ä¸ª **SCÂ³ é—­ç¯æ§åˆ¶-é€šä¿¡ååŒæ¡†æ¶**ï¼Œå¹¶é€šè¿‡ **è½»é‡åŒ–çš„å‰ªæå‹ PPO ç®—æ³•** å®ç°äº†é«˜æ•ˆç¨³å®šçš„ Stackelberg åšå¼ˆæ±‚è§£ï¼Œä¸ºå®‰å…¨å…³é”®å‹ä½ç©ºåº”ç”¨æä¾›äº†åšå®çš„ç†è®ºä¸æŠ€æœ¯æ”¯æ’‘ã€‚

</details>

---

### 14. [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)

**Authors**: Qishuai Wen, Zhiyuan Huang, Xianghan Meng, Wei He, Chun-Guang Li  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.01219v1  

#### Abstract
The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weigh...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿçš„ **Transformer** ä¸­çš„ **attention** æ“ä½œå…·æœ‰ $O(N^2)$ çš„è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦ï¼ˆ$N$ ä¸ºåºåˆ—é•¿åº¦ï¼‰ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å…¶åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸­çš„æ‰©å±•èƒ½åŠ›ã€‚å°½ç®¡å·²æœ‰å¤§é‡é«˜æ•ˆ attention æ–¹æ³•è¢«æå‡ºï¼Œä½†ç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„è§†è§’æ¥ç³»ç»Ÿç†è§£è¿™äº›æ–¹æ³•ã€‚

æœ¬æ–‡ä» **fast-weight scaling** çš„è§’åº¦å‡ºå‘ï¼ŒæŒ‡å‡º full attention å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå®½åº¦ä¸º $N$ çš„ä¸¤å±‚ fast-weight MLPï¼Œéšç€ä¸Šä¸‹æ–‡å¢é•¿ï¼Œå…¶è¡¨è¾¾èƒ½åŠ›å¢å¼ºï¼Œä½†åŠ¨æ€æƒé‡ï¼ˆå³ key-value å¯¹ï¼‰çš„æ‰©å±•ä»£ä»·é«˜æ˜‚ã€‚

### âœ… æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
ä½œè€…æå‡ºäº† **MiTA (Mixture of Top-$k$ Activations)** æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸¤ç§é«˜æ•ˆçš„ fast-weight scaling ç­–ç•¥â€”â€”**routing** å’Œ **compression**â€”â€”ç»“åˆèµ·æ¥ï¼š

- **Compressionï¼ˆå‹ç¼©ï¼‰**ï¼šé€šè¿‡ä¸€ç»„å°‘é‡çš„ **landmark queries** å°†å®Œæ•´çš„ key-value ç¼“å­˜å‹ç¼©æˆä¸€ä¸ªç´§å‡‘çš„å…¨å±€æ¨¡å—ï¼ˆshared expertï¼‰ï¼Œæä¾›å¯¹ä¸Šä¸‹æ–‡çš„ç²—ç•¥ä½†å…¨å±€çš„æ‘˜è¦ã€‚
- **Routingï¼ˆè·¯ç”±ï¼‰**ï¼šæ¯ä¸ª landmark query åŠ¨æ€åœ°æ”¶é›†ä¸å…¶æœ€ç›¸å…³çš„ top-$k$ key-value å¯¹ï¼Œæ„å»º **deformable fast-weight experts**ï¼Œå®ç°ç»†ç²’åº¦ã€è¯­ä¹‰æ„ŸçŸ¥çš„ç¨€ç–è®¿é—®ã€‚

æœ€ç»ˆï¼Œæ¯ä¸ªæŸ¥è¯¢åŒæ—¶è®¿é—®ï¼š
1. å…±äº«çš„å‹ç¼©ä¸“å®¶ï¼ˆæ¥è‡ª landmark queriesï¼‰
2. è¢«è·¯ç”±åˆ°çš„ä¸€ä¸ªæˆ–å¤šä¸ªå˜å½¢ä¸“å®¶ï¼ˆtop-$k$ æ¿€æ´»å¯¹ï¼‰

è¯¥è®¾è®¡å®ç°äº† **â€œå…¨å±€æ‘˜è¦ + å±€éƒ¨ç²¾ç¡®æ£€ç´¢â€** çš„ç»“åˆã€‚

### âœ… ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | ç¼ºé™· | MiTA å¦‚ä½•æ”¹è¿› |
|------|------|---------------|
| **Linear/Memory-based Attention**ï¼ˆå¦‚ Linear Attention, Agent Attentionï¼‰ | ä»…å‹ç¼©ï¼Œä¸¢å¤±ç»†èŠ‚ä¿¡æ¯ï¼Œè¡¨è¾¾èƒ½åŠ›å—é™ | å¼•å…¥ routing åˆ†æ”¯ä¿ç•™å±€éƒ¨é«˜æ¿€æ´»åŒºåŸŸï¼Œæå‡è¡¨è¾¾åŠ› |
| **Sparse/MoE Attention**ï¼ˆå¦‚ MoBA, NSAï¼‰ | å›ºå®šå—åˆ’åˆ†æˆ–é™æ€æ¨¡å¼ï¼Œç¼ºä¹è¯­ä¹‰é€‚åº”æ€§ | æ„å»º **query-aware çš„ deformable experts**ï¼Œæ›´çµæ´»ç²¾å‡† |
| **Top-$k$ Attention**ï¼ˆå¦‚ Spark Attentionï¼‰ | æ¯ä¸ª query æ„å»ºç‹¬ç«‹ä¸“å®¶ï¼Œå…± $N$ ä¸ªä¸“å®¶ï¼Œç¡¬ä»¶ä¸å‹å¥½ | ä½¿ç”¨å›ºå®šæ•°é‡ $m$ ä¸ªå…±äº«ä¸“å®¶ï¼Œé…åˆ routingï¼Œæ˜¾è‘—é™ä½å¼€é”€ |

> ğŸ”‘ **æ ¸å¿ƒä¼˜åŠ¿**ï¼š**é¦–æ¬¡å°† compression ä¸ routing ç»Ÿä¸€äº fast-weight scaling æ¡†æ¶ä¸‹ï¼Œå¹¶æ„é€ å¯è°ƒæ•°é‡çš„ deformable expertsï¼Œå…¼é¡¾æ•ˆç‡ã€è¡¨è¾¾èƒ½åŠ›å’Œç¡¬ä»¶å‹å¥½æ€§ã€‚**

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **ImageNet-1K**ï¼šç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œè¯„ä¼°æ¨¡å‹ä»é›¶è®­ç»ƒå’Œå¾®è°ƒæ€§èƒ½ã€‚
- **ADE20K**ï¼šç”¨äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œæµ‹è¯•åœ¨é•¿åºåˆ—å¯†é›†é¢„æµ‹ä¸‹çš„è¡¨ç°ã€‚
- **Long Range Arena (LRA)**ï¼šäº”ä¸ªé•¿ç¨‹ä¾èµ–ä»»åŠ¡ï¼ˆListOps, Text, Retrieval, Image, Pathfinderï¼‰ï¼Œä¸“é—¨ç”¨äºè¯„æµ‹é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚
- åˆæˆæ•°æ®é›†ï¼šç”¨äºæµ‹é‡æç«¯é•¿åºåˆ—ï¼ˆæœ€é«˜è¾¾ $2^{20} \approx 1M$ tokensï¼‰ä¸‹çš„æ¨ç†ååé‡ã€‚

### âš™ï¸ å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
| ä»»åŠ¡ | è®¾ç½® | ä¸»è¦æŒ‡æ ‡ |
|------|------|---------|
| å›¾åƒåˆ†ç±» | ViT-T/S/B/L æ¶æ„ï¼›è¾“å…¥åˆ†è¾¨ç‡ 224Ã—224ï¼›è®­ç»ƒ 300 epoch æˆ–å¾®è°ƒ 50 epoch | Top-1 Accuracy, MACs, #Params |
| è¯­ä¹‰åˆ†å‰² | ViT ç¼–ç å™¨ + MiTA è§£ç å™¨ï¼›åˆ†è¾¨ç‡ 512Ã—512ï¼›ä¸ Segmenter å¯¹æ¯” | mIoU (Single Scale / Multi Scale) |
| é•¿åºåˆ—å»ºæ¨¡ | LRA benchmarkï¼›æ—  DWC è¾…åŠ©ï¼›å…¬å¹³æ¯”è¾ƒ | å„å­ä»»åŠ¡å‡†ç¡®ç‡åŠå¹³å‡å¾—åˆ† |
| æ¨ç†é€Ÿåº¦ | ä¸‰å±‚æ•° Transformerï¼›RTX 4090 GPUï¼›batch=1 | Inference Throughput (samples/sec) vs $N$ |

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Full Attention (ViT)**ï¼šæ ‡å‡† Transformer æ³¨æ„åŠ›ã€‚
- **Agent Attention**ï¼šå…¸å‹ compression æ–¹æ³•ï¼Œä»…ä¿ç•™ landmark æŸ¥è¯¢ã€‚
- **Linear Attention**ï¼šåŸºäºæ ¸å‡½æ•°è¿‘ä¼¼çš„çº¿æ€§æ³¨æ„åŠ›ã€‚
- **Reformer, Linformer, Performer, NystromFormer**ï¼šä¸»æµé«˜æ•ˆ attention æ–¹æ³•ã€‚
- **MoBA, NSA**ï¼šä»£è¡¨ routing ç±»æ–¹æ³•ï¼ˆblock-based MoEï¼‰ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### ï¼ˆ1ï¼‰ImageNet-1K åˆ†ç±»æ€§èƒ½ï¼ˆä»å¤´è®­ç»ƒï¼‰
| æ–¹æ³• | Top-1 Acc (%) | MACs (G) | #Params (M) |
|------|----------------|----------|-------------|
| ViT-T | 73.1 | 1.2 | 5.7 |
| **MiTA-ViT-T (25+25)** | **72.9 â†’ 74.8*** | **1.1** | **5.7** |
| ViT-S | 77.9 | 4.6 | 22.1 |
| **MiTA-ViT-S (25+25)** | **78.5 â†’ 79.3*** | **4.4** | **22.1** |

> *æ³¨ï¼šåŠ  â–³ è¡¨ç¤ºå¼•å…¥ depth-wise convolutionï¼›åŠ  Bias æŒ‡ä½¿ç”¨ Agent biasã€‚MiTA åœ¨æ›´ä½ MACs ä¸‹æ¥è¿‘ç”šè‡³è¶…è¶ŠåŸç”Ÿ ViT æ€§èƒ½ã€‚*

#### ï¼ˆ2ï¼‰ImageNet-1K å¾®è°ƒæ€§èƒ½ï¼ˆé¢„è®­ç»ƒè‡ª ImageNet-21Kï¼‰
| æ–¹æ³• | Tiny | Small | Base | Large |
|------|------|--------|-------|--------|
| ViT | 76.9 | 81.2 | 84.4 | 85.9 |
| Agent-ViT (m=49) | 74.6 | 79.7 | 81.4 | 83.5 |
| **MiTA-ViT (25+25)** | **75.6** | **80.9** | **82.8** | **85.5** |

> âœ… MiTA æ˜¾è‘—ä¼˜äº Agent Attentionï¼Œä¸”ä¸ full attention æ›´å…¼å®¹ï¼Œè¿ç§»æ•ˆæœæ›´å¥½ã€‚

#### ï¼ˆ3ï¼‰ADE20K è¯­ä¹‰åˆ†å‰²æ€§èƒ½
| æ–¹æ³• | mIoU (SS/MS) | #Params (M) |
|------|--------------|------------|
| Segmenter-T | 38.1 / 38.8 | 7.0 |
| **SEG-T-MiTA (36+36)** | **38.8 / 39.6** | **6.0** |
| Segmenter-B | 48.8 / 50.0 | 106 |
| **SEG-B-MiTA (36+36)** | **48.8 / 49.9** | **96** |

> âœ… MiTA åœ¨æ›´å°‘å‚æ•°ä¸‹è¾¾åˆ°ç›¸å½“ç”šè‡³ç•¥ä¼˜æ€§èƒ½ï¼ŒéªŒè¯å…¶åœ¨é•¿åºåˆ—å¯†é›†ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚

#### ï¼ˆ4ï¼‰Long Range Arena (LRA) å¹³å‡å‡†ç¡®ç‡
| æ–¹æ³• | Avg Accuracy (%) |
|------|------------------|
| Standard Transformer | 59.37 |
| Agent Attention | 58.40 |
| **MiTA Attention (m=k=16)** | **59.26** |

> âœ… MiTA å‡ ä¹è¿½å¹³ full attentionï¼Œåœ¨æ‰€æœ‰äº”é¡¹ä»»åŠ¡ä¸­ä¿æŒç¨³å¥è¡¨ç°ï¼Œè¿œè¶…å…¶ä»–é«˜æ•ˆ attentionã€‚

#### ï¼ˆ5ï¼‰æ¨ç†ååé‡ï¼ˆæé•¿åºåˆ—ï¼‰
- åœ¨ $N = 2^{20}$ï¼ˆçº¦ 100 ä¸‡ tokensï¼‰æ—¶ï¼Œ**MiTA çš„ååé‡æ¯” fused full attention é«˜å‡ºä¸¤ä¸ªæ•°é‡çº§**ã€‚
- å³ä½¿æ²¡æœ‰å®šåˆ¶ CUDA kernel ä¼˜åŒ–ï¼ŒMiTA å·²å±•ç°å‡ºæ˜¾è‘—çš„ wall-clock speedupã€‚

---

### ğŸ” æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰Generalization across $m$ å’Œ $k$
- å›ºå®šè®­ç»ƒ $(m, k) = (25,25)$ï¼Œåœ¨æ¨ç†æ—¶è°ƒæ•´ $m, k$ï¼š
  - **å¢å¤§ $m$ æˆ– $k$**ï¼šæ€§èƒ½æŒç»­æå‡ï¼Œå¯è¾¾åŸ baseline çš„ 100%+ï¼›
  - **å‡å° $m$ æˆ– $k$**ï¼šæ€§èƒ½ä¸‹é™æ˜æ˜¾ã€‚
- ç»“è®ºï¼š**å¯åœ¨è®­ç»ƒæ—¶ç”¨è¾ƒå° $m,k$ æå‡æ•ˆç‡ï¼Œæ¨ç†æ—¶æ”¾å¤§ä»¥è·å¾—æ›´é«˜ç²¾åº¦**ï¼Œå…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

#### ï¼ˆ2ï¼‰Checkpoint Swappingï¼ˆè·¨ attention æœºåˆ¶æ³›åŒ–ï¼‰
- ä½¿ç”¨ MiTA è®­ç»ƒçš„ checkpointï¼Œåœ¨æ¨ç†æ—¶åˆ‡æ¢ä¸º full attentionï¼Œæ€§èƒ½æŸå¤±å°ï¼›
- åä¹‹ï¼Œfull attention è®­ç»ƒçš„æ¨¡å‹æ¢ä¸º MiTAï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™ã€‚
- ç»“è®ºï¼š**MiTA å­¦ä¹ åˆ°çš„ slow weights æ›´é€šç”¨ï¼Œå‘ full attention æ³›åŒ–èƒ½åŠ›å¼º**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Fast-weight scaling æ˜¯ç†è§£é«˜æ•ˆ attention çš„ç»Ÿä¸€æ¡†æ¶**ï¼š
   - æ‰€æœ‰é«˜æ•ˆ attention æ–¹æ³•å‡å¯å½’ç±»ä¸º **scaling by routing** æˆ– **scaling by compression**ã€‚
   - å¤šæ•°æ–¹æ³•åªé‡‡ç”¨å…¶ä¸€ï¼Œè€Œ **MiTA æˆåŠŸèåˆä¸¤è€…**ã€‚

2. **Deformable Experts + Fixed Number + Routing = æ›´ä¼˜æƒè¡¡**ï¼š
   - ä½¿ç”¨ landmark queries æ„é€  content-aware çš„ deformable expertsï¼Œæ¯”å›ºå®šå—æ›´çµæ´»ï¼›
   - æ§åˆ¶ä¸“å®¶æ•°é‡ $m \ll N$ï¼Œæ¯”æ¯ query ä¸€ä¸ªä¸“å®¶æ›´é«˜æ•ˆï¼›
   - åˆ©ç”¨ routing å®ç°ç»„åˆå¼ç¨€ç–æ¨¡å¼ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›ã€‚

3. **MiTA åœ¨å¤šç§ä»»åŠ¡ä¸Šåª²ç¾ full attentionï¼Œä¸”å…·å¤‡æ›´å¼ºæ³›åŒ–æ€§**ï¼š
   - åœ¨åˆ†ç±»ã€åˆ†å‰²ã€é•¿åºåˆ—å»ºæ¨¡ä¸­å‡å–å¾—ä¼˜å¼‚ç»“æœï¼›
   - æ”¯æŒè®­ç»ƒ-æ¨ç†é—´ $m,k$ è°ƒæ•´ï¼Œé€‚åˆå¼¹æ€§éƒ¨ç½²ï¼›
   - checkpoint æ›´æ˜“è¿ç§»åˆ° full attentionï¼Œè¯´æ˜å…¶å­¦ä¹ è´¨é‡æ›´é«˜ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **gather æ“ä½œå­˜åœ¨å†…å­˜ç“¶é¢ˆ**ï¼štop-$k$ gather å¯¼è‡´ä¸è§„åˆ™å†…å­˜è®¿é—®ï¼Œå¯èƒ½æˆä¸ºå®é™…éƒ¨ç½²ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼ˆè™½å¯é€šè¿‡ä¼˜åŒ–ç¼“è§£ï¼‰ã€‚
- **ç›®å‰ä»…å®ç° $s=1$ è·¯ç”±**ï¼šå³æ¯ä¸ª query æœ€å¤šè®¿é—®ä¸€ä¸ªé¢å¤– expertï¼Œè¿›ä¸€æ­¥å¢åŠ  $s$ å¯èƒ½å¸¦æ¥æ”¶ç›Šä½†å®ç°æ›´å¤æ‚ã€‚
- **æœªåœ¨è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸ŠéªŒè¯**ï¼šå½“å‰å®éªŒé›†ä¸­äºè§†è§‰ä»»åŠ¡ï¼Œéœ€æ›´å¤š NLP éªŒè¯ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **ä¼˜åŒ– top-$k$ gather çš„ CUDA å®ç°**ï¼Œæå‡ç«¯åˆ°ç«¯æ¨ç†æ•ˆç‡ã€‚
2. **æ¢ç´¢ $s > 1$ çš„å¤šä¸“å®¶è·¯ç”±ç­–ç•¥**ï¼Œç ”ç©¶ç»„åˆä¸“å®¶å¸¦æ¥çš„è¡¨è¾¾å¢ç›Šã€‚
3. **å°† MiTA åº”ç”¨äº LLM é¢„è®­ç»ƒä¸é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡**ï¼ŒéªŒè¯å…¶åœ¨è‡ªç„¶è¯­è¨€é¢†åŸŸçš„æ½œåŠ›ã€‚
4. **ç»“åˆä½ç½®ç¼–ç æˆ– RoPE**ï¼Œè¿›ä¸€æ­¥å¢å¼ºå…¶å¤–æ¨ï¼ˆextrapolationï¼‰èƒ½åŠ›ã€‚

---

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> MiTA é€šè¿‡ **compress-and-route** ç­–ç•¥ï¼Œåœ¨ fast-weight scaling è§†è§’ä¸‹ç»Ÿä¸€äº†é«˜æ•ˆ attention è®¾è®¡èŒƒå¼ï¼Œæå‡ºäº†ä¸€ç§å…¼å…· **å…¨å±€æ¦‚æ‹¬èƒ½åŠ›** ä¸ **å±€éƒ¨ç²¾ç»†æ£€ç´¢èƒ½åŠ›** çš„æ–°å‹ attention æœºåˆ¶ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­é€¼è¿‘ full attention æ€§èƒ½çš„åŒæ—¶å¤§å¹…æå‡æ•ˆç‡ï¼Œä¸ºä¸‹ä¸€ä»£é«˜æ•ˆ Transformer æ¶æ„æä¾›äº†æ–°æ€è·¯ã€‚

</details>

---

### 15. [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)

**Authors**: Jinbin Bai, Yixuan Li, Yuchen Zhu, Yi Xin, Qingyu Shi, Aosong Feng, Xiaohong Liu, Molei Tao, Jianru Xue, Xiangtai Li, Ming-Hsuan Yang  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.01842v1  

#### Abstract
Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šPRISM: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
å½“å‰ä¸»æµçš„ Test-Time Scaling (TTS) æ–¹æ³•å¤§å¤šåŸºäº **autoregressive (AR) decoding** èŒƒå¼è®¾è®¡ï¼Œä¾èµ–äºä»å·¦åˆ°å³çš„é€æ­¥ç”Ÿæˆä¸å›æº¯æœºåˆ¶ã€‚ç„¶è€Œï¼Œ**discrete diffusion language models (dLLMs)** é‡‡ç”¨å¹¶è¡Œã€éè‡ªå›å½’çš„è¿­ä»£å»å™ªæœºåˆ¶ï¼Œåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿›è¡Œå…¨å±€åŒå‘å»ºæ¨¡ï¼Œè¿™ä½¿å¾—ä¼ ç»Ÿ TTS æ–¹æ³•éš¾ä»¥ç›´æ¥åº”ç”¨ã€‚

å…·ä½“æŒ‘æˆ˜åŒ…æ‹¬ï¼š
- **å®½åº¦æ‰©å±•ï¼ˆwidth scalingï¼‰æˆæœ¬é«˜æ˜‚**ï¼šæœ´ç´ çš„ Best-of-N æ–¹æ³•éœ€è¦å¯¹ N æ¡è½¨è¿¹æ‰§è¡Œå®Œæ•´çš„ T æ­¥å»å™ªï¼Œå¯¼è‡´è®¡ç®—å¤æ‚åº¦ä¸º $O(NT)$ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
- **å¤–éƒ¨éªŒè¯å™¨å¼€é”€å¤§**ï¼šå¼•å…¥é¢å¤–çš„ reward model æˆ– verifier ä¼šæ˜¾è‘—å¢åŠ  GPU å†…å­˜å ç”¨å’Œç³»ç»Ÿå¤æ‚æ€§ã€‚
- **ä¸­é—´çŠ¶æ€ä¸å¯è§£é‡Š**ï¼šdLLMs åœ¨å»å™ªè¿‡ç¨‹ä¸­å­˜åœ¨å¤§é‡ `[MASK]`ï¼Œç ´åäº†æ ‡å‡† PRMï¼ˆProcess Reward Modelï¼‰æ‰€ä¾èµ–çš„â€œå‰ç¼€ç»“æ„â€ï¼Œä½¿å…¶æ— æ³•æœ‰æ•ˆæ‰“åˆ†ã€‚

å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸ä¾èµ–å¤–éƒ¨æ¨¡å‹çš„å‰æä¸‹ï¼Œé«˜æ•ˆåœ°å®ç° dLLMs çš„ test-time compute æ‰©å±•ï¼Œæ˜¯ä¸€ä¸ªå°šæœªå……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
ä½œè€…æå‡º **PRISM**ï¼ˆPruning, Remasking, and Integrated Self-verification Methodï¼‰ï¼Œä¸€ä¸ªä¸“ä¸º dLLMs è®¾è®¡çš„é«˜æ•ˆ TTS æ¡†æ¶ï¼ŒåŒ…å«ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶ï¼š

#### **(1) Hierarchical Trajectory Search (HTS)**
- å°†å»å™ªè¿‡ç¨‹åˆ’åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š
  - **Stochastic Exploration**ï¼ˆé«˜å™ªå£°é˜¶æ®µï¼‰ï¼šä¿ç•™ N æ¡ç‹¬ç«‹è½¨è¿¹ï¼Œè¿›è¡Œå¤šæ ·åŒ–æ¢ç´¢ã€‚
  - **Progressive Thinning**ï¼ˆä¸­ä½å™ªå£°é˜¶æ®µï¼‰ï¼šé€šè¿‡å‡ ä½•è¡°å‡ç­–ç•¥åŠ¨æ€å‰ªæï¼Œå°†æ´»è·ƒè½¨è¿¹æ•°ä» N å¿«é€Ÿæ”¶ç¼©è‡³ Kã€‚
  - **Final Refinement**ï¼ˆç²¾ç»†ä¼˜åŒ–é˜¶æ®µï¼‰ï¼šä»…å¯¹ K æ¡é«˜è´¨é‡è½¨è¿¹å®Œæˆå‰©ä½™å»å™ªã€‚
- å®ç°è¿‘ä¼¼çº¿æ€§çš„è®¡ç®—å¢é•¿ï¼šå®é™…å¤æ‚åº¦çº¦ä¸º $O(N + KT)$ï¼Œè¿œä¼˜äº Best-of-N çš„ $O(NT)$ã€‚

#### **(2) Local Branching via Partial Remasking**
- åœ¨å‰ªæåï¼Œå¯¹é«˜åˆ†ç§å­è½¨è¿¹è¿›è¡Œå±€éƒ¨åˆ†æ”¯ï¼ˆlocal branchingï¼‰ï¼š
  - åˆ©ç”¨é¢„æµ‹ç†µè¯†åˆ«ä½ç½®ä¿¡åº¦ tokenã€‚
  - éšæœºé‡é®è”½éƒ¨åˆ†ä½ç½®ï¼Œä¿ç•™é«˜ç½®ä¿¡åº¦çš„â€œé€»è¾‘éª¨æ¶â€ï¼ˆlogic skeletonï¼‰ã€‚
  - åœ¨ç›¸åŒè§£é¢˜æ¡†æ¶ä¸‹æ¢ç´¢å¤šæ ·åŒ–çš„å®ç°è·¯å¾„ã€‚
- ç›¸æ¯”å®Œå…¨é‡å¯ï¼Œè¯¥æ“ä½œå¤ç”¨å½“å‰çŠ¶æ€ï¼Œæ§åˆ¶é¢å¤–å¼€é”€ã€‚

#### **(3) Self-Verified Feedback (SVF)**
- å¼•å…¥è½»é‡çº§è‡ªéªŒè¯æœºåˆ¶ï¼Œ**å¤ç”¨åŒä¸€ä¸ª dLLM è‡ªèº«ä½œä¸ºäºŒåˆ†ç±»éªŒè¯å™¨**ã€‚
- æ„é€ ä¸“ç”¨çš„ Yes/No è‡ªæˆ‘è¯„ä¼° promptï¼Œè¯¢é—®ï¼šâ€œè¯¥ä¸­é—´å®Œæˆæ˜¯å¦å¯èƒ½å¯¼å‘æ­£ç¡®ç­”æ¡ˆï¼Ÿâ€
- è¾“å‡º SVF score ç”¨äºæŒ‡å¯¼å‰ªæä¸é€‰æ‹©ï¼Œæ— éœ€åŠ è½½å¤–éƒ¨ verifierã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | PRISM | Best-of-N | å¤–éƒ¨ Verifier æ–¹æ³• |
|------|-------|----------|------------------|
| **è®¡ç®—æ•ˆç‡** | âœ… è¿‘çº¿æ€§æ‰©å±• $O(N + KT)$ | âŒ äºŒæ¬¡å¢é•¿ $O(NT)$ | âŒ åŒæ · $O(NT)$ |
| **å†…å­˜å¼€é”€** | âœ… æ— é¢å¤–æ¨¡å‹ | âœ… æ—  | âŒ éœ€åŠ è½½é¢å¤– verifier |
| **å…¼å®¹æ€§** | âœ… é€‚é… dLLMs ä¸­é—´çŠ¶æ€ | âš ï¸ ä¸é€‚é… | âŒ é€šå¸¸ä¸é€‚é… |
| **éƒ¨ç½²ç®€æ˜“æ€§** | âœ… å•æ¨¡å‹ç«¯åˆ°ç«¯ | âœ… | âŒ å¤šæ¨¡å—è€¦åˆ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
æ¶µç›–æ•°å­¦æ¨ç†ä¸ä»£ç ç”Ÿæˆä¸¤å¤§ä»»åŠ¡ç±»åˆ«ï¼š

| ç±»åˆ« | æ•°æ®é›† | æè¿° |
|------|--------|------|
| **Math Reasoning** | GSM8K | å°å­¦ç®—æœ¯æ–‡å­—é¢˜ï¼Œéœ€å¤šæ­¥ç¬¦å·æ¨ç† |
| | MATH-500 | ç«èµ›çº§æ•°å­¦éš¾é¢˜ï¼Œå…± 500 é“ |
| **Code Generation** | HumanEval | æ‰‹å†™ Python å‡½æ•°é—®é¢˜ï¼Œdocstring æè¿°éœ€æ±‚ |
| | MBPP | æ—¥å¸¸ç¼–ç¨‹ä»»åŠ¡ï¼Œå«è‡ªç„¶è¯­è¨€æç¤ºä¸å•å…ƒæµ‹è¯• |

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### **æ¨¡å‹**
ä½¿ç”¨ä¸‰ç§ä¸»æµ dLLMsï¼š
- `LLaDA-8B-Instruct`
- `Dream-7B-Instruct`
- `LLaDA-2.0-mini`

#### **è¯„ä¼°æŒ‡æ ‡**
- æ•°å­¦ä»»åŠ¡ï¼š**Accuracy**
- ç¼–ç¨‹ä»»åŠ¡ï¼š**Pass@1**
- è®¡ç®—æˆæœ¬ï¼š**Number of Function Evaluations (NFE)**ï¼Œå³å»å™ªæ­¥æ•°æ€»å’Œ

#### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **Single-trajectory decoding (N=1)**ï¼šåŸºç¡€ baseline
- **Best-of-N (N=4,8,16)**ï¼šé‡‡æ · N æ¡ç‹¬ç«‹è½¨è¿¹ï¼Œé€šè¿‡ majority voting é€‰å‡ºæœ€ä¼˜

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆä»¥ LLaDA-8B-Instruct ä¸ºä¾‹ï¼‰**

| æ–¹æ³• | GSM8K Acc (%) | NFE | HumanEval Pass@1 (%) | NFE |
|------|---------------|-----|------------------------|-----|
| N=1 (baseline) | 67.58 | 256 | 54.88 | 512 |
| Best-of-4 | 69.32 | 1024 | 77.44 | 2048 |
| Best-of-8 | 82.73 | 2048 | 81.71 | 4096 |
| Best-of-16 | 87.50 | 4096 | 82.32 | 8192 |
| **PRISM (K=8)** | **85.30** | **1048** | **79.27** | **2480** |

> æ³¨ï¼šPRISM åœ¨ **ä»… 1/4 çš„ NFE å¼€é”€ä¸‹** è¾¾åˆ°äº†æ¥è¿‘ Best-of-16 çš„æ€§èƒ½ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- **æ•ˆç‡ä¼˜åŠ¿æ˜¾è‘—**ï¼š
  - åœ¨ GSM8K ä¸Šï¼ŒPRISM (K=8) ä»¥ **1048 NFE** è¾¾åˆ° 85.3%ï¼Œè€Œ Best-of-16 éœ€ **4096 NFE** æ‰è¾¾ 87.5%ï¼ŒèŠ‚çœ **è¶… 4Ã— è®¡ç®—é‡**ã€‚
  - åœ¨ MATH å’Œ MBPP ä¸Šï¼ŒPRISM å¸¸èƒ½åœ¨ **ä¸åˆ° 1/3 æ¨ç†é¢„ç®—** ä¸‹åŒ¹é…ç”šè‡³è¶…è¶Š Best-of-16ã€‚
- **æ€§èƒ½ç¨³å®šæå‡**ï¼š
  - åœ¨æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹å’Œå››ä¸ªä»»åŠ¡ä¸Šï¼ŒPRISM å‡æ˜¾è‘—ä¼˜äº N=1 baselineã€‚
  - ä¾‹å¦‚åœ¨ LLaDA-8B ä¸Šï¼ŒPRISM(K=8) å°† GSM8K æå‡ **+17.72pp**ï¼ŒMATH500 æå‡ **+16.4pp**ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

#### **ç›®æ ‡å®½åº¦ K çš„å½±å“**
- æ€§èƒ½éš K å¢åŠ å•è°ƒä¸Šå‡ï¼Œä½†è¾¹é™…æ”¶ç›Šé€’å‡ã€‚
- **K=8 æ˜¯æœ€ä½³å¹³è¡¡ç‚¹**ï¼šåœ¨ HumanEval ä¸Šè¾¾åˆ° 79.27%ï¼ˆvs Best-of-16 çš„ 82.32%ï¼‰ï¼Œé€Ÿåº¦æå‡ **3.3Ã—**ã€‚

#### **å‰ªæçª—å£ï¼ˆPruning Windowï¼‰**
- æœ€ä½³åŒºé—´ä¸º **[0.1T, 0.6T]**ï¼ˆæ—©æœŸåˆ°ä¸­æœŸå»å™ªé˜¶æ®µï¼‰ã€‚
- è¿‡æ—©å‰ªæä¸¢å¤±å¤šæ ·æ€§ï¼›è¿‡æ™šåˆ™æµªè´¹è®¡ç®—èµ„æºã€‚

#### **å‰ªæé¢‘ç‡ï¼ˆPruning Interval iï¼‰**
- **i=3** è¡¨ç°æœ€ä½³ï¼šæ¯ 3 æ­¥æ‰§è¡Œä¸€æ¬¡ SVF æ‰“åˆ†ä¸åˆ†æ”¯ã€‚
- è¿‡äºé¢‘ç¹ï¼ˆi=1ï¼‰æ˜“è¯¯åˆ æ½œåŠ›è½¨è¿¹ï¼›è¿‡äºç¨€ç–ï¼ˆi=5ï¼‰å‰Šå¼±è‡ªé€‚åº”èƒ½åŠ›ã€‚

#### **è¡°å‡é€Ÿç‡ d ä¸å¹¸å­˜è€…æ•°é‡ S**
- **d=1.8**ï¼ˆä¸­ç­‰è¡°å‡ï¼‰æ•ˆæœæœ€å¥½ã€‚
- **S=4** å¹³è¡¡å¤šæ ·æ€§ä¸èšç„¦æ€§ï¼ŒS è¿‡å°é™åˆ¶æ¢ç´¢ï¼Œè¿‡å¤§ç¨€é‡Šåˆ†æ”¯è´¨é‡ã€‚

#### **Self-Verified Feedback (SVF) çš„æœ‰æ•ˆæ€§**
- SVF è°ƒç”¨æ¬¡æ•°å  NFE ä¸è¶³ 10%ï¼Œå¼€é”€æä½ã€‚
- æ›¿æ¢ä¸ºå¤–éƒ¨ verifierï¼ˆå¦‚ Qwen-7B/Qwen3-8Bï¼‰è™½å¯ç•¥ææ€§èƒ½ï¼ˆå¦‚ 87.35% vs 85.30%ï¼‰ï¼Œä½†éœ€é¢å¤–åŠ è½½å¤§æ¨¡å‹ï¼Œè¶…å‡ºå•å¡å®¹é‡ï¼ˆ>40GBï¼‰ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **Hierarchical Search æ˜¾è‘—æå‡æ•ˆç‡**ï¼šå°†è®¡ç®—é›†ä¸­åœ¨â€œé€»è¾‘éª¨æ¶å½¢æˆâ€çš„å…³é”®é˜¶æ®µï¼Œé¿å…å‡åŒ€åˆ†é…ã€‚
2. âœ… **Partial Remasking å®ç°å¯æ§å¤šæ ·æ€§**ï¼šåœ¨ä¿ç•™é«˜ç½®ä¿¡ç»“æ„çš„åŒæ—¶æ¢ç´¢æ›¿ä»£å®ç°ï¼Œä¼˜äºéšæœºé‡å¯ã€‚
3. âœ… **Self-Verification å¯è¡Œä¸”é«˜æ•ˆ**ï¼šåŒä¸€ dLLM å¯å……å½“è½»é‡ verifierï¼Œæ— éœ€å¤–éƒ¨æ¨¡å‹å³å¯å®ç°å¯é æ‰“åˆ†ã€‚
4. âœ… **PRISM å®ç°å¼ºæ€§èƒ½-æ•ˆç‡æƒè¡¡**ï¼šåœ¨å¤§å¹…é™ä½ NFE çš„åŒæ—¶ï¼Œé€¼è¿‘ç”šè‡³è¶…è¶Š Best-of-N çš„æ€§èƒ½ä¸Šé™ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¾èµ–è‰¯å¥½çš„ SVF prompt è®¾è®¡**ï¼šè‹¥è‡ªæˆ‘åˆ¤æ–­æœ¬èº«æœ‰åè§æˆ–è¿‡åº¦è‡ªä¿¡ï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯å‰ªæã€‚
- **å¯¹å»å™ªè°ƒåº¦æ•æ„Ÿ**ï¼šHTS æ•ˆæœä¾èµ–äºåˆé€‚çš„å‰ªæçª—å£è®¾å®šï¼Œä¸åŒä»»åŠ¡å¯èƒ½éœ€è°ƒå‚ã€‚
- **æœªè§£å†³é•¿åºåˆ—æ‰©å±•é—®é¢˜**ï¼šç›®å‰ focus åœ¨ width scalingï¼Œlength scaling ä»å—é™äºå›ºå®šé•¿åº¦ç”Ÿæˆã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
- æ¢ç´¢ **length-aware HTS**ï¼šç»“åˆé•¿åº¦æ‰©å±•ä¸å®½åº¦æœç´¢ã€‚
- å¼•å…¥ **learnable pruning policy**ï¼šç”¨å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨å†³å®šä½•æ—¶å‰ªæã€ä¿ç•™è°ã€‚
- æ‰©å±•è‡³ **multi-modal dLLMs**ï¼šåº”ç”¨äºæ–‡æœ¬-å›¾åƒè”åˆç”Ÿæˆåœºæ™¯ã€‚
- ç ”ç©¶ **zero-shot SVF generalization**ï¼šä½¿ self-verification èƒ½è·¨ä»»åŠ¡è¿ç§»ã€‚

---

> ğŸ”— **ä»£ç å¼€æºåœ°å€**ï¼š[https://github.com/viiika/Prism](https://github.com/viiika/Prism)

</details>

---

### 16. [Low-latency Federated LLM Fine-tuning Over Wireless Networks](https://arxiv.org/abs/2602.01024)

**Authors**: Zhiwen Pang, Kang Wei, Long Shi, Zhe Wang, Jun Li, Feng Shu  
**Category**: cs.DC  
**Published**: 2026-02-03  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.01024v1  

#### Abstract
Recently, federated large language models (LLMs) have drawn significant attention thanks to coupled capabilities of LLMs and federated learning (FL) that address privacy concerns in collaborative fine-tuning. However, due to large-scale parameters of LLMs, existing federated LLM fine-tuning framewor...

---

### 17. [SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2602.01027)

**Authors**: Xin Nie, Haicheng Zhang, Liang Dong, Beining Feng, Jinhong Weng, Guiling Sun  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.01027v1  

#### Abstract
Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or int...

---

### 18. [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)

**Authors**: Hyesung Jeon, Hyeongju Ha, Jae-Joon Kim  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.01053v1  

#### Abstract
Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented tr...

---

### 19. [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)

**Authors**: Yunjie Pan, Yongyi Yang, Hanmei Yang, Scott Mahlke  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.01410v1  

#### Abstract
Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristi...

---

### 20. [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)

**Authors**: Ratul Ali  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.00053v1  

#### Abstract
Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency...

---

### 21. [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)

**Authors**: Fabian P. Kr\"uger, Andrea Hunklinger, Adrian Wolny, Tim J. Adler, Igor Tetko, Santiago David Villalba  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.00663v1  

#### Abstract
Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, su...

---

### 22. [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)

**Authors**: Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou, Xiaoyue Ma, Jianing Li, Yao Zhu  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.00815v1  

#### Abstract
Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-in...

---

### 23. [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)

**Authors**: Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang, Weijie Liu, Clive Bai, Kai Yang, Yangkun Chen, Saiyong Yang, Xiangyang Ji  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.01970v1  

#### Abstract
Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, cu...

---

### 24. [Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees](https://arxiv.org/abs/2602.00987)

**Authors**: Sawan Kumar, Souvik Chakraborty  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.00987v1  

#### Abstract
Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding model...

---

### 25. [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)

**Authors**: Yiliu He, Tianle Li, Binghao Ji, Zhiyuan Liu, Di Huang  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.01082v1  

#### Abstract
Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large langu...

---

### 26. [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)

**Authors**: Yiming Dong, Kun Fu, Haoyu Li, Xinyuan Zhu, Yurou Liu, Lijing Shao, Jieping Ye, Zheng Wang  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.01103v1  

#### Abstract
Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severel...

---

### 27. [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)

**Authors**: Shaoxiong Yang, Junting Li, Mengyuan Zhang, Chao Li, Wei Liu, Jian Luan  
**Category**: cs.AI  
**Published**: 2026-02-03  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.01222v1  

#### Abstract
Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we...

---

### 28. [WorldCup Sampling for Multi-bit LLM Watermarking](https://arxiv.org/abs/2602.01752)

**Authors**: Yidan Wang, Yubing Ren, Yanan Cao, Li Guo  
**Category**: cs.CL  
**Published**: 2026-02-03  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.01752v1  

#### Abstract
As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steeri...

---

### 29. [Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach](https://arxiv.org/abs/2602.02355)

**Authors**: Amirreza Kazemi, Seyed Mohammad Azimi-Abarghouyi, Gabor Fodor, Carlo Fischione  
**Category**: cs.DC  
**Published**: 2026-02-03  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.02355v1  

#### Abstract
Hierarchical federated learning (HFL) has emerged as a key architecture for large-scale wireless and Internet of Things systems, where devices communicate with nearby edge servers before reaching the cloud. In these environments, uplink bandwidth and latency impose strict communication limits, there...

---

### 30. [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)

**Authors**: Shaorong Zhang, Longxuan Yu, Rob Brekelmans, Luhan Tang, Salman Asif, Greg Ver Steeg  
**Category**: cs.LG  
**Published**: 2026-02-03  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.00286v1  

#### Abstract
Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework ...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
