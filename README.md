# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-03 12:49:44 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning](https://arxiv.org/abs/2510.02212)

**Authors**: Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, Nathan Kallus  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 11.5

arXiv:2510.02212v1 Announce Type: cross 
Abstract: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify t...

---

### 2. [QSpec: Speculative Decoding with Complementary Quantization Schemes](https://arxiv.org/abs/2410.11305)

**Authors**: Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, Chuan Wu  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 11.5

arXiv:2410.11305v3 Announce Type: replace-cross 
Abstract: Quantization is widely adopted to accelerate inference and reduce memory consumption in large language models (LLMs). While activation-weight joint quantization enables efficient low-precision decoding, it suffers from substantial performanc...

---

### 3. [HiSpec: Hierarchical Speculative Decoding for LLMs](https://arxiv.org/abs/2510.01336)

**Authors**: Avinash Kumar, Sujay Sanghavi, Poulami Das  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 10.5

arXiv:2510.01336v1 Announce Type: cross 
Abstract: Speculative decoding accelerates LLM inference by using a smaller draft model to speculate tokens that a larger target model verifies. Verification is often the bottleneck (e.g. verification is $4\times$ slower than token generation when a 3B model ...

---

### 4. [Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction](https://arxiv.org/abs/2510.01817)

**Authors**: Adam Filipek  
**Category**: cs.CL  
**Published**: 2025-10-03  
**Score**: 10.5

arXiv:2510.01817v1 Announce Type: cross 
Abstract: The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to se...

---

### 5. [CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation](https://arxiv.org/abs/2502.10940)

**Authors**: Ziyue Liu, Ruijie Zhang, Zhengyang Wang, Mingsong Yan, Zi Yang, Paul Hovland, Bogdan Nicolae, Franck Cappello, Sui Tang, Zheng Zhang  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 9.5

arXiv:2502.10940v3 Announce Type: replace-cross 
Abstract: The full-size MLPs and the projection layers in attention introduce tremendous model sizes of large language models (LLMs), consuming extensive computational resources in pre-training. We empirically observe that the activations of pre-train...

---

### 6. [Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters](https://arxiv.org/abs/2510.01256)

**Authors**: Lingling Zeng, Gen Zhang, Jialin Peng, Xiang Xu, Yuan Xu, Lijun Ma  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 9.0

arXiv:2510.01256v1 Announce Type: cross 
Abstract: As AI cluster sizes continue to expand and the demand for large-language-model (LLM) training and inference workloads grows rapidly, traditional scheduling systems face significant challenges in balancing resource utilization, scheduling efficiency,...

---

### 7. [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656)

**Authors**: Jiashun Liu, Johan Obando-Ceron, Han Lu, Yancheng He, Weixun Wang, Wenbo Su, Bo Zheng, Pablo Samuel Castro, Aaron Courville, Ling Pan  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 9.0

arXiv:2510.01656v1 Announce Type: cross 
Abstract: Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing them with average advantage baselines. This shift is largely pragmatic: conventional value functions are computationally expensive to train at LLM scale and often fail under ...

---

### 8. [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)

**Authors**: Vaibhav Singh, Zafir Khalid, Edouard Oyallon, Eugene Belilovsky  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 9.0

arXiv:2507.09029v3 Announce Type: replace-cross 
Abstract: Pre-training large neural networks at scale imposes heavy memory demands on accelerators and often requires costly communication. We introduce Subnetwork Data Parallelism (SDP), a distributed training framework that partitions a model into s...

---

### 9. [Large Language Models Inference Engines based on Spiking Neural Networks](https://arxiv.org/abs/2510.00133)

**Authors**: Adarsha Balaji, Sandeep Madireddy  
**Category**: cs.LG  
**Published**: 2025-10-03  
**Score**: 9.0

arXiv:2510.00133v2 Announce Type: replace 
Abstract: Foundational models based on the transformer architecture are currently the state-of-the-art in general language modeling, as well as in scientific areas such as material science and climate. However, training and deploying these models is computa...

---

### 10. [Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression](https://arxiv.org/abs/2510.01450)

**Authors**: Yifei Zuo, Yutong Yin, Zhichen Zeng, Ang Li, Banghua Zhu, Zhaoran Wang  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 8.5

arXiv:2510.01450v1 Announce Type: cross 
Abstract: Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater comp...

---

### 11. [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)

**Authors**: Yang Liu, Zaid Abulawi, Abhiram Garimidi, Doyeong Lim  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 8.0

arXiv:2510.01398v1 Announce Type: new 
Abstract: Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies. There is also heightened interest in developing data-driven...

---

### 12. [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)

**Authors**: Yusuf Kalayci, Vinod Raman, Shaddin Dughmi  
**Category**: cs.CL  
**Published**: 2025-10-03  
**Score**: 8.0

arXiv:2510.01394v1 Announce Type: cross 
Abstract: Large language model (LLM) generation often requires balancing output quality against inference cost, especially when using multiple generations. We introduce a new framework for inference-time optimization based on the classical Pandora's Box probl...

---

### 13. [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)

**Authors**: Isaac Peterson, Christopher Allred, Jacob Morrey, Mario Harper  
**Category**: cs.LG  
**Published**: 2025-10-03  
**Score**: 8.0

arXiv:2510.01264v1 Announce Type: new 
Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems cooperating in dynamic environments. While prior work has focused on these collaborative settings, adversarial interactions are equally critical for real-world applications such a...

---

### 14. [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)

**Authors**: Weijia Dou, Xu Zhang, Yi Bin, Jian Liu, Bo Peng, Guoqing Wang, Yang Yang, Heng Tao Shen  
**Category**: cs.LG  
**Published**: 2025-10-03  
**Score**: 8.0

arXiv:2510.02186v1 Announce Type: cross 
Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to 3D semantic segmentation expose a persistent trade-off. Directly projecting 2D features into 3D yields noisy and fragmented predictions, whereas enforcing geometric cohere...

---

### 15. [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)

**Authors**: Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 7.5

arXiv:2510.01555v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a Kullback-Leibler (KL) divergence loss to stabilize training and prevent overfitting. However, in methods such as GRPO, its implementation may be guided by principles from numerical value ...

---

### 16. [KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting](https://arxiv.org/abs/2510.02084)

**Authors**: Kuiye Ding, Fanda Fan, Zheya Wang, Hongxiao Li, Yifan Wang, Lei Wang, Chunjie Luo, Jianfeng Zhan  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 7.5

arXiv:2510.02084v1 Announce Type: cross 
Abstract: In the World Wide Web, reliable time series forecasts provide the forward-looking signals that drive resource planning, cache placement, and anomaly response, enabling platforms to operate efficiently as user behavior and content distributions evolv...

---

### 17. [Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs](https://arxiv.org/abs/2509.25779)

**Authors**: Siyu Zhu, Yanbin Jiang, Hejian Sang, Shao Tang, Qingquan Song, Biao He, Rohit Jain, Zhipeng Wang, Alborz Geramifard  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 7.5

arXiv:2509.25779v2 Announce Type: replace 
Abstract: We investigated Agentic RL with large language models on the \textsc{TravelPlanner} benchmark. Our approach, \textsc{Planner-R1}, achieved a \textbf{56.9\%} final-pass rate with only 180 training queries, a $2.7\times$ improvement over GPT-5's $21...

---

### 18. [ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models](https://arxiv.org/abs/2510.01290)

**Authors**: Akshat Ramachandran, Marina Neseem, Charbel Sakr, Rangharajan Venkatesan, Brucek Khailany, Tushar Krishna  
**Category**: cs.LG  
**Published**: 2025-10-03  
**Score**: 7.5

arXiv:2510.01290v1 Announce Type: new 
Abstract: The long-output context generation of large reasoning models enables extended chain of thought (CoT) but also drives rapid growth of the key-value (KV) cache, quickly overwhelming GPU memory. To address this challenge, we propose ThinKV, a thought-ada...

---

### 19. [Drop-Muon: Update Less, Converge Faster](https://arxiv.org/abs/2510.02239)

**Authors**: Kaja Gruntkowska, Yassine Maziane, Zheng Qu, Peter Richt\'arik  
**Category**: cs.LG  
**Published**: 2025-10-03  
**Score**: 7.5

arXiv:2510.02239v1 Announce Type: new 
Abstract: Conventional wisdom in deep learning optimization dictates updating all layers at every step-a principle followed by all recent state-of-the-art optimizers such as Muon. In this work, we challenge this assumption, showing that full-network updates can...

---

### 20. [ShapeGen3DCP: A Deep Learning Framework for Layer Shape Prediction in 3D Concrete Printing](https://arxiv.org/abs/2510.02009)

**Authors**: Giacomo Rizzieri, Federico Lanteri, Liberato Ferrara, Massimiliano Cremonesi  
**Category**: cs.LG  
**Published**: 2025-10-03  
**Score**: 7.5

arXiv:2510.02009v1 Announce Type: cross 
Abstract: This work introduces ShapeGen3DCP, a deep learning framework for fast and accurate prediction of filament cross-sectional geometry in 3D Concrete Printing (3DCP). The method is based on a neural network architecture that takes as input both material...

---

### 21. [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)

**Authors**: Sipeng Zhang, Longfei Yun, Zilong Wang, Jingbo Shang, Letian Peng  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2510.01427v1 Announce Type: new 
Abstract: At the core of Deep Research is knowledge mining, the task of extracting structured information from massive unstructured text in response to user instructions. Large language models (LLMs) excel at interpreting such instructions but are prohibitively...

---

### 22. [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)

**Authors**: Daniel Zhao, Abhilash Shankarampeta, Lanxiang Hu, Tajana Rosing, Hao Zhang  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2510.01528v1 Announce Type: new 
Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and clustering techniques to analyze the internal token representations of large language models (LLMs) and guide generations in mathematical reasoning tasks. Our approach first train...

---

### 23. [Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant Neural Network](https://arxiv.org/abs/2510.02108)

**Authors**: Jinshuo Zhang, Yafei Wang, Xinping Yi, Wenjin Wang, Shi Jin, Symeon Chatzinotas, Bj\"orn Ottersten  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2510.02108v1 Announce Type: cross 
Abstract: Although symbol-level precoding (SLP) based on constructive interference (CI) exploitation offers performance gains, its high complexity remains a bottleneck. This paper addresses this challenge with an end-to-end deep learning (DL) framework with l...

---

### 24. [ExGRPO: Learning to Reason from Experience](https://arxiv.org/abs/2510.02245)

**Authors**: Runzhe Zhan, Yafu Li, Zhi Wang, Xiaoye Qu, Dongrui Liu, Jing Shao, Derek F. Wong, Yu Cheng  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2510.02245v1 Announce Type: cross 
Abstract: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computati...

---

### 25. [StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](https://arxiv.org/abs/2509.22558)

**Authors**: Chenyu Zhou, Tianyi Xu, Jianghao Lin, Dongdong Ge  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2509.22558v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitat...

---

### 26. [Communication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation](https://arxiv.org/abs/2509.26399)

**Authors**: Le-Tuan Nguyen, Minh-Duong Nguyen, Seon-Geun Jeong, Dung D. Le, Quoc-Viet Pham  
**Category**: cs.AI  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2509.26399v3 Announce Type: replace 
Abstract: With the rapid emergence of foundation models and the increasing need for fine-tuning across distributed environments, Federated Low-Rank Adaptation (FedLoRA) has recently gained significant attention. Despite enormous potential, current FedLoRA m...

---

### 27. [ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning](https://arxiv.org/abs/2510.01585)

**Authors**: Haochen You, Baojing Liu  
**Category**: cs.CL  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2510.01585v1 Announce Type: new 
Abstract: While Transformer architectures have demonstrated impressive scalability across domains, they continue to face challenges in long-context reasoning, computational efficiency, and structural generalization - largely due to rigid layer stacking, dense a...

---

### 28. [Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO](https://arxiv.org/abs/2510.01616)

**Authors**: Yu-Cheng Chih, Ming-Tao Duan, Yong-Hao Hou  
**Category**: cs.CL  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2510.01616v1 Announce Type: new 
Abstract: Small Language Models (SLMs) enable cost-effective, on-device and latency-sensitive AI applications, yet their deployment in Traditional Chinese (TC) remains hindered by token-level instability - models unpredictably emit non-TC characters or code-swi...

---

### 29. [PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2510.01472)

**Authors**: Hengyi Zhu, Grace Li Zhang, Shaoyi Huang  
**Category**: cs.LG  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2510.01472v1 Announce Type: new 
Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) requires joint optimization of accuracy and latency under device constraints. Traditional supernet-based methods require multiple GPU days per dataset. Large Language Model (LLM)-driven approaches avo...

---

### 30. [Support Basis: Fast Attention Beyond Bounded Entries](https://arxiv.org/abs/2510.01643)

**Authors**: Maryam Aliakbarpour, Vladimir Braverman, Junze Yin, Haochen Zhang  
**Category**: cs.LG  
**Published**: 2025-10-03  
**Score**: 7.0

arXiv:2510.01643v1 Announce Type: new 
Abstract: The quadratic complexity of softmax attention remains a central bottleneck in scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a sub-quadratic attention approximation algorithm, but it works only under the restrictive boun...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
