# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-12-01 08:35:20 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)

**Authors**: Jun Wang, Yunxiang Yao, Wenwei Kuang, Runze Mao, Zhenhao Sun, Zhuang Tao, Ziyang Zhang, Dengyu Li, Jiajun Chen, Zhili Wang, Kai Cui, Congzhi Cai, Longwen Lan, Ken Zhang  
**Category**: cs.DC  
**Published**: 2025-12-01  
**Score**: 13.0  
**Type**: new  
**ArXiv ID**: 2511.22481v1  

Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to ...

---

### 2. [Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models](https://arxiv.org/abs/2511.21759)

**Authors**: Linye Wei, Wenjue Chen, Pingzhi Tang, Xiaotian Guo, Le Ye, Runsheng Wang, Meng Li  
**Category**: cs.CL  
**Published**: 2025-12-01  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2511.21759v1  

Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism...

---

### 3. [Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks](https://arxiv.org/abs/2511.23167)

**Authors**: Chenyu Liu, Zhaoyang Zhang, Zirui Chen, Zhaohui Yang  
**Category**: cs.DC  
**Published**: 2025-12-01  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2511.23167v1  

Split learning (SL) offloads main computing tasks from multiple resource-constrained user equippments (UEs) to the base station (BS), while preserving local data privacy. However, its computation and communication processes remain sequential, resulting in limited system efficiency. To overcome this ...

---

### 4. [Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.22105)

**Authors**: Saad Masrur, Ismail Guvenc, David Lopez Perez  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2511.22105v1  

Dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks is essential for maximizing energy efficiency (EE) under stringent quality-of-service (QoS) constraints. However, existing optimization and reinforcement learning (RL) approaches rely on aggregated, static base station (BS) t...

---

### 5. [TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices](https://arxiv.org/abs/2511.22138)

**Authors**: Mohd Ariful Haque (Clark Atlanta University), Fahad Rahman (United International University), Kishor Datta Gupta (Clark Atlanta University), Khalil Shujaee (Clark Atlanta University), Roy George (Clark Atlanta University)  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2511.22138v1  

This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and descr...

---

### 6. [Training-Free Loosely Speculative Decoding: Accepting Semantically Correct Drafts Beyond Exact Match](https://arxiv.org/abs/2511.22972)

**Authors**: Jinze Li, Yixing Xu, Guanchen Li, Shuo Yang, Jinfeng Xu, Xuanwu Yin, Dong Li, Edith C. H. Ngai, Emad Barsoum  
**Category**: cs.CL  
**Published**: 2025-12-01  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.22972v1  

Large language models (LLMs) achieve strong performance across diverse tasks but suffer from high inference latency due to their autoregressive generation. Speculative Decoding (SPD) mitigates this issue by verifying candidate tokens in parallel from a smaller draft model, yet its strict exact-match...

---

### 7. [FLUX: Efficient Descriptor-Driven Clustered Federated Learning under Arbitrary Distribution Shifts](https://arxiv.org/abs/2511.22305)

**Authors**: Dario Fenoglio, Mohan Li, Pietro Barbiero, Nicholas D. Lane, Marc Langheinrich, Martin Gjoreski  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.22305v1  

Federated Learning (FL) enables collaborative model training across multiple clients while preserving data privacy. Traditional FL methods often use a global model to fit all clients, assuming that clients' data are independent and identically distributed (IID). However, when this assumption does no...

---

### 8. [ORION: Teaching Language Models to Reason Efficiently in the Language of Thought](https://arxiv.org/abs/2511.22891)

**Authors**: Kumar Tanmay, Kriti Aggarwal, Paul Pu Liang, Subhabrata Mukherjee  
**Category**: cs.AI  
**Published**: 2025-12-01  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.22891v1  

Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits ...

---

### 9. [TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies](https://arxiv.org/abs/2511.23225)

**Authors**: Guang Liang, Jie Shao, Ningyuan Tang, Xinyao Liu, Jianxin Wu  
**Category**: cs.CL  
**Published**: 2025-12-01  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.23225v1  

Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventi...

---

### 10. [Physics-Informed Spiking Neural Networks via Conservative Flux Quantization](https://arxiv.org/abs/2511.21784)

**Authors**: Chi Zhang, Lin Wang  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.21784v1  

Real-time, physically-consistent predictions on low-power edge devices is critical for the next generation embodied AI systems, yet it remains a major challenge. Physics-Informed Neural Networks (PINNs) combine data-driven learning with physics-based constraints to ensure the model's predictions are...

---

### 11. [SingleQuant: Efficient Quantization of Large Language Models in a Single Pass](https://arxiv.org/abs/2511.22316)

**Authors**: Jinying Xiao, Bin Ji, Shasha Li, Xiaodong Liu, Ma Jun, Ye Zhong, Wei Li, Xuan Xie, Qingbo Wu, Jie Yu  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.22316v1  

Large Language Models (LLMs) quantization facilitates deploying LLMs in resource-limited settings, but existing methods that combine incompatible gradient optimization and quantization truncation lead to serious convergence pathology. This prolongs quantization time and degrades LLMs' task performan...

---

### 12. [GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels](https://arxiv.org/abs/2511.22793)

**Authors**: Bhavya Sai Nukapotula, Rishabh Tripathi, Seth Pregler, Dileep Kalathil, Srinivas Shakkottai, Theodore S. Rappaport  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.22793v1  

Channel state information (CSI) is essential for adaptive beamforming and maintaining robust links in wireless communication systems. However, acquiring CSI incurs significant overhead, consuming up to 25\% of spectrum resources in 5G networks due to frequent pilot transmissions at sub-millisecond i...

---

### 13. [Experts are all you need: A Composable Framework for Large Language Model Inference](https://arxiv.org/abs/2511.22955)

**Authors**: Shrihari Sridharan, Sourjya Roy, Anand Raghunathan, Kaushik Roy  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.22955v1  

Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decou...

---

### 14. [Distributed Dynamic Associative Memory via Online Convex Optimization](https://arxiv.org/abs/2511.23347)

**Authors**: Bowen Wang, Matteo Zecchin, Osvaldo Simeone  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.23347v1  

An associative memory (AM) enables cue-response recall, and it has recently been recognized as a key mechanism underlying modern neural architectures such as Transformers. In this work, we introduce the concept of distributed dynamic associative memory (DDAM), which extends classical AM to settings ...

---

### 15. [Fast dynamical similarity analysis](https://arxiv.org/abs/2511.22828)

**Authors**: Arman Behrad, Mitchell Ostrow, Mohammad Taha Fakharian, Ila Fiete, Christian Beste, Shervin Safavi  
**Category**: cs.AI  
**Published**: 2025-12-01  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.22828v1  

To understand how neural systems process information, it is often essential to compare one circuit with another, one brain with another, or data with a model. Traditional similarity measures ignore the dynamical processes underlying neural representations. Dynamical similarity methods offer a framew...

---

### 16. [Dripper: Token-Efficient Main HTML Extraction with a Lightweight LM](https://arxiv.org/abs/2511.23119)

**Authors**: Mengjie Liu, Jiahui Peng, Pei Chu, Jiantao Qiu, Ren Ma, He Zhu, Rui Min, Lindong Lu, Wenchang Ning, Linfeng Hou, Kaiwen Liu, Yuan Qu, Zhenxiang Li, Chao Xu, Zhongying Tu, Wentao Zhang, Conghui He  
**Category**: cs.CL  
**Published**: 2025-12-01  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.23119v1  

Accurately and efficiently extracting main content from general web pages is of great significance for obtaining training data for large models. Using well-pre-trained decoder-only generative language models offers excellent document comprehension capabilities, thereby effectively enhancing parsing ...

---

### 17. [Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics](https://arxiv.org/abs/2511.21848)

**Authors**: Eric Leonardis, Akira Nagamori, Ayesha Thanawalla, Yuanjia Yang, Joshua Park, Hutton Saunders, Eiman Azim, Talmo Pereira  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.21848v1  

The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high...

---

### 18. [Modeling Quantum Autoencoder Trainable Kernel for IoT Anomaly Detection](https://arxiv.org/abs/2511.21932)

**Authors**: Swathi Chandrasekhar, Shiva Raj Pokhrel, Swati Kumari, Navneet Singh  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.21932v1  

Escalating cyber threats and the high-dimensional complexity of IoT traffic have outpaced classical anomaly detection methods. While deep learning offers improvements, computational bottlenecks limit real-time deployment at scale. We present a quantum autoencoder (QAE) framework that compresses netw...

---

### 19. [TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE](https://arxiv.org/abs/2511.22853)

**Authors**: Jiawen Wei, Lan Jiang, Pengbo Wei, Ziwen Ye, Teng Song, Chen Chen, Guangrui Ma  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.22853v1  

Time series data is ubiquitous, with forecasting applications spanning from finance to healthcare. Beyond popular deterministic methods, generative models are gaining attention due to advancements in areas like image synthesis and video generation, as well as their inherent ability to provide probab...

---

### 20. [Evolutionary Discovery of Heuristic Policies for Traffic Signal Control](https://arxiv.org/abs/2511.23122)

**Authors**: Ruibing Wang, Shuhan Guo, Zeen Li, Zhen Wang, Quanming Yao  
**Category**: cs.AI  
**Published**: 2025-12-01  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.23122v1  

Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasonin...

---

### 21. [ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast](https://arxiv.org/abs/2511.21969)

**Authors**: Matteo Bjornsson, Taylor Hardin, Taylor Heinecke, Marcin Furtak, David L. Millman, Mike P. Wittie  
**Category**: cs.DC  
**Published**: 2025-12-01  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.21969v1  

Distributed ledger technologies (DLTs) rely on distributed consensus mechanisms to reach agreement over the order of transactions and to provide immutability and availability of transaction data. Distributed consensus suffers from performance limitations of network communication between participatin...

---

### 22. [Closed-Loop Transformers: Autoregressive Modeling as Iterative Latent Equilibrium](https://arxiv.org/abs/2511.21882)

**Authors**: Akbar Anbar Jafari, Gholamreza Anbarjafari  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.21882v1  

Contemporary autoregressive transformers operate in open loop: each hidden state is computed in a single forward pass and never revised, causing errors to propagate uncorrected through the sequence. We identify this open-loop bottleneck as a fundamental architectural limitation underlying well-docum...

---

### 23. [An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface](https://arxiv.org/abs/2511.22108)

**Authors**: Zhou Biyan, Arindam Basu  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.22108v1  

The number of simultaneously recorded neurons follows an exponentially increasing trend in implantable brain-machine interfaces (iBMIs). Integrating the neural decoder in the implant is an effective data compression method for future wireless iBMIs. However, the non-stationarity of the system makes ...

---

### 24. [Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges](https://arxiv.org/abs/2511.22483)

**Authors**: Guanxi Lu, Hao Mark Chen, Zhiqiang Que, Wayne Luk, Hongxiang Fan  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.22483v1  

Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, a...

---

### 25. [Masked Diffusion for Generative Recommendation](https://arxiv.org/abs/2511.23021)

**Authors**: Kulin Shah, Bhuvesh Kumar, Neil Shah, Liam Collins  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.23021v1  

Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR w...

---

### 26. [Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent](https://arxiv.org/abs/2511.23436)

**Authors**: Jianzhe Lin, Zeyu Pan, Yun Zhu, Ruiqi Song, Jining Yang  
**Category**: cs.AI  
**Published**: 2025-12-01  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.23436v1  

We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperI...

---

### 27. [An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction](https://arxiv.org/abs/2511.22460)

**Authors**: Yifan Lei, Jiahua Luo, Tingyu Jiang, Bo Zhang, Lifeng Wang, Dapeng Liu, Zhaoren Wu, Haijie Gu, Huan Yu, Jie Jiang  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.22460v1  

In large-scale advertising recommendation systems, retrieval serves as a critical component, aiming to efficiently select a subset of candidate ads relevant to user behaviors from a massive ad inventory for subsequent ranking and recommendation. The Embedding-Based Retrieval (EBR) methods modeled by...

---

### 28. [Quantized-Tinyllava: a new multimodal foundation model enables efficient split learning](https://arxiv.org/abs/2511.23402)

**Authors**: Jiajun Guo, Xin Luo, Jie Liu  
**Category**: cs.LG  
**Published**: 2025-12-01  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.23402v1  

Split learning is well known as a method for resolving data privacy concerns by training a model on distributed devices, thereby avoiding data sharing that raises privacy issues. However, high network communication costs are always an impediment to split learning, especially for large foundation mod...

---

### 29. [Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation](https://arxiv.org/abs/2511.22311)

**Authors**: Fiona Y. Wang, Di Sheng Lee, David L. Kaplan, Markus J. Buehler  
**Category**: cs.AI  
**Published**: 2025-12-01  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2511.22311v1  

Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-...

---

### 30. [Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.23148)

**Authors**: Mian Ibad Ali Shah, Marcos Eduardo Cruz Victorio, Maeve Duffy, Enda Barrett, Karl Mason  
**Category**: cs.AI  
**Published**: 2025-12-01  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2511.23148v1  

The integration of renewable energy resources in rural areas, such as dairy farming communities, enables decentralized energy management through Peer-to-Peer (P2P) energy trading. This research highlights the role of P2P trading in efficient energy distribution and its synergy with advanced optimiza...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
