# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-26 12:57:24 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [ROOT: Robust Orthogonalized Optimizer for Neural Network Training](https://arxiv.org/abs/2511.20626)

**Authors**: Wei He, Kai Han, Hang Zhou, Hanting Chen, Zhicheng Liu, Xinghao Chen, Yunhe Wang  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2511.20626v1  

The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer ...

---

### 2. [N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory](https://arxiv.org/abs/2511.18723)

**Authors**: Longfei Wang, Junyan Liu, Fan Zhang, Jiangwen Wei, Yuanhua Tang, Jie Sun, Xiaodong Luo  
**Category**: cs.AI  
**Published**: 2025-11-26  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.18723v1  

Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&amp;B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (...

---

### 3. [E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems](https://arxiv.org/abs/2511.20564)

**Authors**: Rui Xue, Shichao Zhu, Liang Qin, Guangmou Pan, Yang Song, Tianfu Wu  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.20564v1  

Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-traine...

---

### 4. [SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference](https://arxiv.org/abs/2511.19457)

**Authors**: Ziyang Zhang, Jie Liu, Luca Mottola  
**Category**: cs.DC  
**Published**: 2025-11-26  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2511.19457v1  

The resource demands of deep neural network (DNN) models introduce significant performance challenges, especially when deployed on resource-constrained edge devices. Existing solutions like model compression often sacrifice accuracy, while specialized hardware remains costly and inflexible. Hybrid i...

---

### 5. [Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios](https://arxiv.org/abs/2511.20340)

**Authors**: Luohe Shi, Zuchao Li, Lefei Zhang, Baoyuan Qi, Guoming Liu, Hai Zhao  
**Category**: cs.CL  
**Published**: 2025-11-26  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.20340v1  

Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a sma...

---

### 6. [AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains](https://arxiv.org/abs/2511.19450)

**Authors**: M. Zeeshan Haider, Tayyaba Noreen, M. D. Assuncao, Kaiwen Zhang  
**Category**: cs.DC  
**Published**: 2025-11-26  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.19450v1  

Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cr...

---

### 7. [Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management](https://arxiv.org/abs/2511.20172)

**Authors**: Xinjun Yang, Qingda Hu, Junru Li, Feifei Li, Yuqi Zhou, Yicong Zhu, Qiuru Lin, Jian Dai, Yang Kong, Jiayu Zhang, Guoqiang Xu, Qiang Liu  
**Category**: cs.DC  
**Published**: 2025-11-26  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.20172v1  

The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to ...

---

### 8. [SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space](https://arxiv.org/abs/2511.20102)

**Authors**: Zhenyi Shen, Junru Lu, Lin Gui, Jiazheng Li, Yulan He, Di Yin, Xing Sun  
**Category**: cs.CL  
**Published**: 2025-11-26  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.20102v1  

The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation...

---

### 9. [Opt4GPTQ: Co-Optimizing Memory and Computation for 4-bit GPTQ Quantized LLM Inference on Heterogeneous Platforms](https://arxiv.org/abs/2511.19438)

**Authors**: Yaozheng Zhang, Wei Wang, Jie Kong, Jiehan Zhou, Huanqing Cui  
**Category**: cs.DC  
**Published**: 2025-11-26  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.19438v1  

The increasing adoption of large language model (LLMs) on heterogeneous computing platforms poses significant challenges for achieving high inference efficiency. To address the low inference efficiency of LLMs across diverse heterogeneous platforms, this paper proposes a practical optimization metho...

---

### 10. [Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments](https://arxiv.org/abs/2511.19479)

**Authors**: Sangam Ghimire, Paribartan Timalsina, Nirjal Bhurtel, Bishal Neupane, Bigyan Byanju Shrestha, Subarna Bhattarai, Prajwal Gaire, Jessica Thapa, Sudan Jha  
**Category**: cs.DC  
**Published**: 2025-11-26  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.19479v1  

As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast compu...

---

### 11. [MSTN: Fast and Efficient Multivariate Time Series Model](https://arxiv.org/abs/2511.20577)

**Authors**: Sumit S Shevtekar, Chandresh K Maurya, Gourab Sil  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.20577v1  

Real-world time-series data is highly non stationary and complex in dynamics that operate across multiple timescales, ranging from fast, short-term changes to slow, long-term trends. Most existing models rely on fixed-scale structural priors, such as patch-based tokenization, fixed frequency transfo...

---

### 12. [NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations](https://arxiv.org/abs/2511.18793)

**Authors**: Yejing Wang, Shengyu Zhou, Jinyu Lu, Ziwei Liu, Langming Liu, Maolin Wang, Wenlin Zhang, Feng Li, Wenbo Su, Pengjie Wang, Jian Xu, Xiangyu Zhao  
**Category**: cs.AI  
**Published**: 2025-11-26  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.18793v1  

Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services an...

---

### 13. [Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models](https://arxiv.org/abs/2511.19822)

**Authors**: Wentao Hu, Mingkuan Zhao, Shuangyong Song, Xiaoyan Zhu, Xin Lai, Jiayin Wang  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.19822v1  

Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static m...

---

### 14. [Wireless Power Transfer and Intent-Driven Network Optimization in AAVs-assisted IoT for 6G Sustainable Connectivity](https://arxiv.org/abs/2511.18368)

**Authors**: Yue Hu, Xiaoming He, Rui Yuan, Shahid Mumtaz  
**Category**: cs.AI  
**Published**: 2025-11-26  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.18368v1  

Autonomous Aerial Vehicle (AAV)-assisted Internet of Things (IoT) represents a collaborative architecture in which AAV allocate resources over 6G links to jointly enhance user-intent interpretation and overall network performance. Owing to this mutual dependence, improvements in intent inference and...

---

### 15. [Latent Collaboration in Multi-Agent Systems](https://arxiv.org/abs/2511.20639)

**Authors**: Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, Gaotang Li, Katherine Tieu, Pan Lu, Ke Shen, Hanghang Tong, Yejin Choi, Jingrui He, James Zou, Mengdi Wang, Ling Yang  
**Category**: cs.CL  
**Published**: 2025-11-26  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.20639v1  

Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly...

---

### 16. [Efficient Parallel Implementation of the Pilot Assignment Problem in Massive MIMO Systems](https://arxiv.org/abs/2511.20511)

**Authors**: Eman Alqudah, Ashfaq Khokhar  
**Category**: cs.DC  
**Published**: 2025-11-26  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.20511v1  

The assignment of the pilot sequence is a critical challenge in massive MIMO systems, as sharing the same pilot sequence among multiple users causes interference, which degrades the accuracy of the channel estimation. This problem, equivalent to the NP-hard graph coloring problem, directly impacts r...

---

### 17. [Hierarchical Spatio-Temporal Attention Network with Adaptive Risk-Aware Decision for Forward Collision Warning in Complex Scenarios](https://arxiv.org/abs/2511.19952)

**Authors**: Haoran Hu, Junren Shi, Shuo Jiang, Kun Cheng, Xia Yang, Changhao Piao  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.19952v1  

Forward Collision Warning systems are crucial for vehicle safety and autonomous driving, yet current methods often fail to balance precise multi-agent interaction modeling with real-time decision adaptability, evidenced by the high computational cost for edge deployment and the unreliability stemmin...

---

### 18. [MTA: A Merge-then-Adapt Framework for Personalized Large Language Model](https://arxiv.org/abs/2511.20072)

**Authors**: Xiaopeng Li, Yuanjin Zheng, Wanyu Wang, wenlin zhang, Pengyue Jia, Yiqi Wang, Maolin Wang, Xuetao Wei, Xiangyu Zhao  
**Category**: cs.CL  
**Published**: 2025-11-26  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.20072v1  

Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly wit...

---

### 19. [CafeQ: Calibration-free Quantization via Learned Transformations and Adaptive Rounding](https://arxiv.org/abs/2511.19705)

**Authors**: Ziteng Sun, Adrian Benton, Samuel Kushnir, Asher Trockman, Vikas Singh, Suhas Diggavi, Ananda Theertha Suresh  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.19705v1  

Post-training quantization is an effective method for reducing the serving cost of large language models, where the standard approach is to use a round-to-nearest quantization level scheme. However, this often introduces large errors due to outliers in the weights. Proposed mitigation mechanisms inc...

---

### 20. [From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection](https://arxiv.org/abs/2511.20500)

**Authors**: Sidahmed Benabderrahmane, Talal Rahwan  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.20500v1  

Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well i...

---

### 21. [HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs](https://arxiv.org/abs/2511.18760)

**Authors**: Azim Ospanov, Zijin Feng, Jiacheng Sun, Haoli Bai, Xin Shen, Farzan Farnia  
**Category**: cs.AI  
**Published**: 2025-11-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.18760v1  

Informal mathematics has been central to modern large language model (LLM) reasoning, offering flexibility and enabling efficient construction of arguments. However, purely informal reasoning is prone to logical gaps and subtle errors that are difficult to detect and correct. In contrast, formal the...

---

### 22. [Exploiting the Experts: Unauthorized Compression in MoE-LLMs](https://arxiv.org/abs/2511.19480)

**Authors**: Pinaki Prasad Guha Neogi, Ahmad Mohammadshirazi, Dheeraj Kulshrestha, Rajiv Ramnath  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.19480v1  

Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning...

---

### 23. [A Systematic Study of Compression Ordering for Large Language Models](https://arxiv.org/abs/2511.19495)

**Authors**: Shivansh Chhawri, Rahul Mahadik, Suparna Rooj  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.19495v1  

Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects...

---

### 24. [TREASURE: A Transformer-Based Foundation Model for High-Volume Transaction Understanding](https://arxiv.org/abs/2511.19693)

**Authors**: Chin-Chia Michael Yeh, Uday Singh Saini, Xin Dai, Xiran Fan, Shubham Jain, Yujie Fan, Jiarui Sun, Junpeng Wang, Menghai Pan, Yingtong Dou, Yuzhong Chen, Vineeth Rakesh, Liang Wang, Yan Zheng, Mahashweta Das  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.19693v1  

Payment networks form the backbone of modern commerce, generating high volumes of transaction records from daily activities. Properly modeling this data can enable applications such as abnormal behavior detection and consumer-level insights for hyper-personalized experiences, ultimately improving pe...

---

### 25. [Training-Free Active Learning Framework in Materials Science with Large Language Models](https://arxiv.org/abs/2511.19730)

**Authors**: Hongchen Wang, Rafael Espinosa Casta\~neda, Jay R. Werber, Yao Fehlis, Edward Kim, Jason Hattrick-Simpers  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.19730v1  

Active learning (AL) accelerates scientific discovery by prioritizing the most informative experiments, but traditional machine learning (ML) models used in AL suffer from cold-start limitations and domain-specific feature engineering, restricting their generalizability. Large language models (LLMs)...

---

### 26. [Scalable Data Attribution via Forward-Only Test-Time Inference](https://arxiv.org/abs/2511.19803)

**Authors**: Sibo Ma, Julian Nyarko  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.19803v1  

Data attribution seeks to trace model behavior back to the training examples that shaped it, enabling debugging, auditing, and data valuation at scale. Classical influence-function methods offer a principled foundation but remain impractical for modern networks because they require expensive backpro...

---

### 27. [QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression](https://arxiv.org/abs/2511.20099)

**Authors**: Lei Huang, Rui Zhang, Jiaming Guo, Yang Zhang, Di Huang, Shuyao Cheng, Pengwei Jin, Chongxiao Li, Zidong Du, Xing Hu, Qi Guo, Yunji Chen  
**Category**: cs.LG  
**Published**: 2025-11-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.20099v1  

Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstrea...

---

### 28. [Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures](https://arxiv.org/abs/2511.17833)

**Authors**: Yunsheng Bai, Haoxing Ren  
**Category**: cs.AI  
**Published**: 2025-11-26  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.17833v1  

Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate respo...

---

### 29. [$\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers](https://arxiv.org/abs/2511.19987)

**Authors**: Xinyu Wang, Hanwei Wu, Qingchen Hu, Zhenghan Tai, Jingrui Tian, Lei Ding, Jijun Chi, Hailin He, Tung Sum Thomas Kwok, Yufei Cui, Sicheng Lyu, Muzhi Li, Mingze Li, Xinyue Yu, Ling Zhou, Peng Lu  
**Category**: cs.CL  
**Published**: 2025-11-26  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.19987v1  

Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R...

---

### 30. [Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED](https://arxiv.org/abs/2511.19456)

**Authors**: Anton Reinhard, Simeon Ehrig, Ren\'e Widera, Michael Bussmann, Uwe Hernandez Acosta  
**Category**: cs.DC  
**Published**: 2025-11-26  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2511.19456v1  

Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into acco...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
