# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-24 12:54:26 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [dInfer: An Efficient Inference Framework for Diffusion Language Models](https://arxiv.org/abs/2510.08666)

**Authors**: Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, Xinyuan Zhang, Zhen Tao, Haibo Feng, Ziyun Jiang, Ying Xu, Zenan Huang, Yihong Zhuang, Haokai Xu, Jiaqi Hu, Zhenzhong Lan, Junbo Zhao, Jianguo Li, Da Zheng  
**Category**: cs.CL  
**Published**: 2025-10-23  
**Score**: 12.5  
**Type**: replace  
**ArXiv ID**: 2510.08666v3  

Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack o...

---

### 2. [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)

**Authors**: Huawei Bai, Yifan Huang, Wenqi Shi, Ansheng You, Feifan Shao, Tengfei Han, Minghui Yu  
**Category**: cs.DC  
**Published**: 2025-10-24  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2510.20111v1  

The training efficiency and scalability of language models on massive clusters currently remain a critical bottleneck. Mainstream approaches like ND parallelism are often cumbersome and complex, while flexible alternatives such as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by commu...

---

### 3. [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)

**Authors**: Yu Hin Chan, Hao Yang, Shiyu Shen, Xingyu Fan, Shengzhe Lyu, Patrick S. Y. Hung, Ray C. C. Cheung  
**Category**: cs.DC  
**Published**: 2025-10-24  
**Score**: 11.0  
**Type**: cross  
**ArXiv ID**: 2510.20243v1  

Privacy-preserving machine learning (PPML) is an emerging topic to handle secure machine learning inference over sensitive data in untrusted environments. Fully homomorphic encryption (FHE) enables computation directly on encrypted data on the server side, making it a promising approach for PPML. Ho...

---

### 4. [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)

**Authors**: Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Jingliang Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Xinfeng Xie, Yulun Wang, Bruce Wu, Jingyi Yang, Mingran Yang, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 10.5  
**Type**: cross  
**ArXiv ID**: 2510.20171v1  

The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hin...

---

### 5. [A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks](https://arxiv.org/abs/2510.20683)

**Authors**: Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 10.5  
**Type**: cross  
**ArXiv ID**: 2510.20683v1  

Brain-computer interfaces (BCIs) promise to enable vital functions, such as speech and prosthetic control, for individuals with neuromotor impairments. Central to their success are neural decoders, models that map neural activity to intended behavior. Current learning-based decoding approaches fall ...

---

### 6. [RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.19262)

**Authors**: Heng Xu, Zhiwei Yu, Chengze Du, Ying Zhou, Letian Li, Haojie Wang, Weiqiang Cheng, Jialong Li  
**Category**: cs.DC  
**Published**: 2025-10-24  
**Score**: 9.5  
**Type**: replace  
**ArXiv ID**: 2510.19262v2  

Training Mixture-of-Experts (MoE) models introduces sparse and highly imbalanced all-to-all communication that dominates iteration time. Conventional load-balancing methods fail to exploit the deterministic topology of Rail architectures, leaving multi-NIC bandwidth underutilized. We present RailS, ...

---

### 7. [CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs](https://arxiv.org/abs/2505.12944)

**Authors**: Jan Hagnberger, Daniel Musekamp, Mathias Niepert  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.12944v2  

Solving time-dependent Partial Differential Equations (PDEs) using a densely discretized spatial domain is a fundamental problem in various scientific and engineering disciplines, including modeling climate phenomena and fluid dynamics. However, performing these computations directly in the physical...

---

### 8. [Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and NVIDIA Data Center GPUs](https://arxiv.org/abs/2507.00418)

**Authors**: Mohammad Firas Sada, John J. Graham, Elham E Khoda, Mahidhar Tatineni, Dmitry Mishin, Rajesh K. Gupta, Rick Wagner, Larry Smarr, Thomas A. DeFanti, Frank W\"urthwein  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2507.00418v2  

This study presents a benchmarking analysis of the Qualcomm Cloud AI 100 Ultra (QAic) accelerator for large language model (LLM) inference, evaluating its energy efficiency (throughput per watt), performance, and hardware scalability against NVIDIA A100 GPUs (in 4x and 8x configurations) within the ...

---

### 9. [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)

**Authors**: Ziheng Deng, Xue Liu, Jiantong Jiang, Yankai Li, Qingxu Deng, Xiaochun Yang  
**Category**: cs.DC  
**Published**: 2025-10-24  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2510.19301v2  

The Viterbi algorithm is a key operator for structured sequence inference in modern data systems, with applications in trajectory analysis, online recommendation, and speech recognition. As these workloads increasingly migrate to resource-constrained edge platforms, standard Viterbi decoding remains...

---

### 10. [Ask a Strong LLM Judge when Your Reward Model is Uncertain](https://arxiv.org/abs/2510.20369)

**Authors**: Zhenghao Xu, Qin Lu, Qingru Zhang, Liang Qiu, Ilgee Hong, Changlong Yu, Wenlin Yao, Yao Liu, Haoming Jiang, Lihong Li, Hyokun Yun, Tuo Zhao  
**Category**: cs.LG  
**Published**: 2025-10-24  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2510.20369v1  

Reward model (RM) plays a pivotal role in reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs). However, classical RMs trained on human preferences are vulnerable to reward hacking and generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong L...

---

### 11. [KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products](https://arxiv.org/abs/2506.04432)

**Authors**: Zixuan Xia, Aram Davtyan, Paolo Favaro  
**Category**: cs.LG  
**Published**: 2025-10-24  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2506.04432v2  

We propose KOALA++, a scalable Kalman-based optimization algorithm that explicitly models structured gradient uncertainty in neural network training. Unlike second-order methods, which rely on expensive second order gradient calculation, our method directly estimates the parameter covariance matrix ...

---

### 12. [A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization](https://arxiv.org/abs/2510.20291)

**Authors**: LinFeng Li, Jian Zhao, Zepeng Yang, Yuhang Song, Bojun Lin, Tianle Zhang, Yuchen Yuan, Chi Zhang, Xuelong Li  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.20291v1  

We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone Navigation. The task retrieves the most relevant geo-referenced image from a large multi-platform corpus (satellite/drone/ground) given a natural-language query. Two obstacles are severe inter-platform heterogeneity and a dom...

---

### 13. [Quantization-Aware Neuromorphic Architecture for Efficient Skin Disease Classification on Resource-Constrained Devices](https://arxiv.org/abs/2507.15958)

**Authors**: Haitian Wang, Xinyu Wang, Yiren Wang, Zichen Geng, Xian Zhang, Yu Zhang, Bo Miao  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2507.15958v2  

Accurate and efficient skin lesion classification on edge devices is critical for accessible dermatological care but remains challenging due to computational, energy, and privacy constraints. We introduce QANA, a novel quantization-aware neuromorphic architecture for incremental skin lesion classifi...

---

### 14. [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)

**Authors**: Yeh Keng Hao, Hsu Tzu Wei, Sun Min  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.16396v2  

With the increasing ubiquity of AR/VR devices, the deployment of deep learning models on edge devices has become a critical challenge. These devices require real-time inference, low power consumption, and minimal latency. Many framework designers face the conundrum of balancing efficiency and perfor...

---

### 15. [Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning](https://arxiv.org/abs/2510.19338)

**Authors**: Ling Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, Jun Zhou  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.19338v2  

In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a...

---

### 16. [BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.18927)

**Authors**: Zhiheng Xi, Xin Guo, Yang Nan, Enyu Zhou, Junrui Shen, Wenxiang Chen, Jiaqi Liu, Jixuan Huang, Zhihao Zhang, Honglin Guo, Xun Deng, Zhikai Lei, Miao Zheng, Guoteng Wang, Shuo Zhang, Peng Sun, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang  
**Category**: cs.CL  
**Published**: 2025-10-23  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.18927v1  

Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy decli...

---

### 17. [Alternatives to the Laplacian for Scalable Spectral Clustering with Group Fairness Constraints](https://arxiv.org/abs/2510.20220)

**Authors**: Iv\'an Ojeda-Ruiz, Young Ju-Lee, Malcolm Dickens, Leonardo Cambisaca  
**Category**: cs.LG  
**Published**: 2025-10-24  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2510.20220v1  

Recent research has focused on mitigating algorithmic bias in clustering by incorporating fairness constraints into algorithmic design. Notions such as disparate impact, community cohesion, and cost per population have been implemented to enforce equitable outcomes. Among these, group fairness (bala...

---

### 18. [Structured Generative Modeling with the Thermodynamic Kolmogorov-Arnold Model](https://arxiv.org/abs/2506.14167)

**Authors**: Prithvi Raj  
**Category**: cs.LG  
**Published**: 2025-10-24  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2506.14167v5  

Learning an energy-based model (EBM) in the latent space of a top-down generative model offers a versatile framework for generation across multiple data modalities. However, it remains unclear how its interpretability can be used to guide model design, improve generative quality, and reduce training...

---

### 19. [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)

**Authors**: Junfeng Gong, Zhiyi Wei, Junying Chen, Cheng Liu, Huawei Li  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.19873v1  

Despite significant evolution of CUDA programming and domain-specific libraries, effectively utilizing GPUs with massively parallel engines remains difficult. Large language models (LLMs) show strong potential in generating optimized CUDA code from sequential code. However, using LLMs in practice fa...

---

### 20. [GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare](https://arxiv.org/abs/2510.08872)

**Authors**: Siqi Zhu, David Zhang, Pedro Cisneros-Velarde, Jiaxuan You  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2510.08872v2  

Large Language Models (LLMs) have achieved remarkable progress in reasoning, yet sometimes produce responses that are suboptimal for users in tasks such as writing, information seeking, or providing practical guidance. Conventional alignment practices typically assume that maximizing model reward al...

---

### 21. [DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference](https://arxiv.org/abs/2510.19669)

**Authors**: Xiang Liu, Xuming Hu, Xiaowen Chu, Eunsol Choi  
**Category**: cs.CL  
**Published**: 2025-10-23  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.19669v1  

Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their efficiency, enabling them to reach high performance without overthinking. First, we analyze the entropy of t...

---

### 22. [CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](https://arxiv.org/abs/2510.19670)

**Authors**: Hasan Akgul, Mari Eplik, Javier Rojas, Aina Binti Abdullah, Pieter van der Merwe  
**Category**: cs.CL  
**Published**: 2025-10-23  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.19670v1  

We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy...

---

### 23. [Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks](https://arxiv.org/abs/2510.04404)

**Authors**: Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel  
**Category**: cs.DC  
**Published**: 2025-10-24  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2510.04404v2  

Modern distributed systems demand low-latency, fault-tolerant event processing that exceeds traditional messaging architecture limits. While frameworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and serverless event buses have matured significantly, no unified comparative study...

---

### 24. [Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning](https://arxiv.org/abs/2502.02770)

**Authors**: Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao  
**Category**: cs.LG  
**Published**: 2025-10-24  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2502.02770v3  

Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because i...

---

### 25. [A decomposition-based robust training of physics-informed neural networks for nearly incompressible linear elasticity](https://arxiv.org/abs/2505.21994)

**Authors**: Josef Dick, Seungchan Ko, Quoc Thong Le Gia, Kassem Mustapha, Sanghyeon Park  
**Category**: cs.LG  
**Published**: 2025-10-24  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.21994v2  

Due to divergence instability, the accuracy of low-order conforming finite element methods for nearly incompressible elasticity equations deteriorates as the Lam\'e coefficient $\lambda\to\infty$, or equivalently as the Poisson ratio $\nu\to1/2$. This phenomenon, known as locking or non-robustness, ...

---

### 26. [Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control](https://arxiv.org/abs/2505.24161)

**Authors**: Zijie Xu, Tong Bu, Zecheng Hao, Jianhao Ding, Zhaofei Yu  
**Category**: cs.LG  
**Published**: 2025-10-24  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.24161v2  

Spiking Neural Networks (SNNs) offer low-latency and energy-efficient decision making on neuromorphic hardware, making them attractive for Reinforcement Learning (RL) in resource-constrained edge devices. However, most RL algorithms for continuous control are designed for Artificial Neural Networks ...

---

### 27. [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)

**Authors**: Fengyuan Yu, Yuyuan Li, Xiaohua Feng, Junjie Fang, Tao Wang, Chaochao Chen  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.20327v1  

With the growing demand for safeguarding sensitive user information in recommender systems, recommendation attribute unlearning is receiving increasing attention. Existing studies predominantly focus on single-attribute unlearning. However, privacy protection requirements in the real world often inv...

---

### 28. [Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation](https://arxiv.org/abs/2510.20812)

**Authors**: Yuhan Liu, Lianhui Qin, Shengjie Wang  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.20812v1  

Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing crit...

---

### 29. [Bi-Mamba: Towards Accurate 1-Bit State Space Models](https://arxiv.org/abs/2411.11843)

**Authors**: Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, Zhiqiang Shen  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2411.11843v2  

The typical Selective State-Space Model (SSM) used in Mamba addresses several limitations of Transformers, such as the quadratic computational complexity with respect to sequence length and the significant memory requirements during inference due to the key-value (KV) cache. However, the increasing ...

---

### 30. [UMoE: Unifying Attention and FFN with Shared Experts](https://arxiv.org/abs/2505.07260)

**Authors**: Yuanhang Yang, Chaozheng Wang, Jing Li  
**Category**: cs.AI  
**Published**: 2025-10-24  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.07260v2  

Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model perfo...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
