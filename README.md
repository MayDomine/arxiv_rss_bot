# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-06 12:54:27 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)

**Authors**: Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Hans Vandierendonck, Deepu John, Bo Ji, Dimitrios Nikolopoulos  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 12.5  
**Type**: replace-cross  
**ArXiv ID**: 2506.09397v5  

The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization...

---

### 2. [Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction](https://arxiv.org/abs/2508.02558)

**Authors**: Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu  
**Category**: cs.CL  
**Published**: 2025-11-06  
**Score**: 11.0  
**Type**: replace  
**ArXiv ID**: 2508.02558v2  

Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial mem...

---

### 3. [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884)

**Authors**: Zijian Zhang, Rong Wang, Shiyang Li, Yuebo Luo, Mingyi Hong, Caiwen Ding  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2511.01884v2  

Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, ...

---

### 4. [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)

**Authors**: Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2511.03092v1  

The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy....

---

### 5. [HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs](https://arxiv.org/abs/2501.02625)

**Authors**: Saleh Ashkboos, Mahdi Nikdan, Soroush Tabesh, Roberto L. Castro, Torsten Hoefler, Dan Alistarh  
**Category**: cs.LG  
**Published**: 2025-11-06  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2501.02625v3  

Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which can have large weight and activation outli...

---

### 6. [A unified physics-informed generative operator framework for general inverse problems](https://arxiv.org/abs/2511.03241)

**Authors**: Gang Bao, Yaohua Zang  
**Category**: cs.LG  
**Published**: 2025-11-06  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.03241v1  

Solving inverse problems governed by partial differential equations (PDEs) is central to science and engineering, yet remains challenging when measurements are sparse, noisy, or when the underlying coefficients are high-dimensional or discontinuous. Existing deep learning approaches either require e...

---

### 7. [Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes](https://arxiv.org/abs/2505.02184)

**Authors**: Matthew T. Dearing, Yiheng Tao, Xingfu Wu, Zhiling Lan, Valerie Taylor  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2505.02184v2  

While large language models (LLMs) are increasingly used for generating parallel scientific codes, most efforts emphasize functional correctness, often overlooking performance, especially energy efficiency. We propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-effic...

---

### 8. [Traversal Verification for Speculative Tree Decoding](https://arxiv.org/abs/2505.12398)

**Authors**: Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, Zhongchao Shi  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.12398v2  

Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be ...

---

### 9. [TensorHyper-VQC: A Tensor-Train-Guided Hypernetwork for Robust and Scalable Variational Quantum Computing](https://arxiv.org/abs/2508.01116)

**Authors**: Jun Qi, Chao-Han Yang, Pin-Yu Chen, Min-Hsiu Hsieh  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2508.01116v2  

Variational Quantum Computing (VQC) faces fundamental scalability barriers, primarily due to the presence of barren plateaus and its sensitivity to quantum noise. To address these challenges, we introduce TensorHyper-VQC, a novel tensor-train (TT)-guided hypernetwork framework that significantly imp...

---

### 10. [FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs](https://arxiv.org/abs/2511.00807)

**Authors**: Xuan He, Zequan Fang, Jinzhao Lian, Danny H. K. Tsang, Baosen Zhang, Yize Chen  
**Category**: cs.DC  
**Published**: 2025-11-06  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2511.00807v2  

The ever-increasing computation and energy demand for LLM and AI agents call for holistic and efficient optimization of LLM serving systems. In practice, heterogeneous GPU clusters can be deployed in a geographically distributed manner, while LLM load also observes diversity in terms of both query t...

---

### 11. [POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding](https://arxiv.org/abs/2511.03464)

**Authors**: Mihriban Kocak Balik, Pekka Marttinen, Negar Safinianaini  
**Category**: cs.LG  
**Published**: 2025-11-06  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.03464v1  

Integrating different molecular layers, i.e., multiomics data, is crucial for unraveling the complexity of diseases; yet, most deep generative models either prioritize predictive performance at the expense of interpretability or enforce interpretability by linearizing the decoder, thereby weakening ...

---

### 12. [Efficient Reasoning via Thought-Training and Thought-Free Inference](https://arxiv.org/abs/2511.03408)

**Authors**: Canhui Wu, Qiong Cao, Chao Xue, Wei Xi, Xiaodong He  
**Category**: cs.CL  
**Published**: 2025-11-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.03408v1  

Recent advances in large language models (LLMs) have leveraged explicit Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most existing methods primarily compress verbose reasoning outputs. These Long-to-Short transformations aim to improve efficiency, but still rely on explic...

---

### 13. [On scalable and efficient training of diffusion samplers](https://arxiv.org/abs/2505.19552)

**Authors**: Minkyu Kim, Kiyoung Seong, Dongyeop Woo, Sungsoo Ahn, Minsu Kim  
**Category**: cs.LG  
**Published**: 2025-11-06  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2505.19552v3  

We address the challenge of training diffusion models to sample from unnormalized energy distributions in the absence of data, the so-called diffusion samplers. Although these approaches have shown promise, they struggle to scale in more demanding scenarios where energy evaluations are expensive and...

---

### 14. [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)

**Authors**: Gaia Grosso, Sai Sumedh R. Hindupur, Thomas Fel, Samuel Bright-Thonney, Philip Harris, Demba Ba  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.03095v1  

Modern artificial intelligence has revolutionized our ability to extract rich and versatile data representations across scientific disciplines. Yet, the statistical properties of these representations remain poorly controlled, causing misspecified anomaly detection (AD) methods to falter. Weak or ra...

---

### 15. [PerfDojo: Automated ML Library Generation for Heterogeneous Architectures](https://arxiv.org/abs/2511.03586)

**Authors**: Andrei Ivanov, Siyuan Shen, Gioele Gottardo, Marcin Chrapek, Afif Boudaoud, Timo Schneider, Luca Benini, Torsten Hoefler  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.03586v1  

The increasing complexity of machine learning models and the proliferation of diverse hardware architectures (CPUs, GPUs, accelerators) make achieving optimal performance a significant challenge. Heterogeneity in instruction sets, specialized kernel requirements for different data types and model fe...

---

### 16. [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)

**Authors**: Mohsen Ahmadzadeh, Kaichang Chen, Georges Gielen  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.03697v1  

Analog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniqu...

---

### 17. [FaStfact: Faster, Stronger Long-Form Factuality Evaluations in LLMs](https://arxiv.org/abs/2510.12839)

**Authors**: Yingjia Wan, Haochen Tan, Xiao Zhu, Xinyu Zhou, Zhiwei Li, Qingsong Lv, Changxuan Sun, Jiaqi Zeng, Yi Xu, Jianqiao Lu, Yinhong Liu, Zhijiang Guo  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.12839v2  

Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to efficiency bottlenecks and reliability concerns. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawback...

---

### 18. [Neural Beamforming with Doppler-Aware Sparse Attention for High Mobility Environments](https://arxiv.org/abs/2511.03632)

**Authors**: Cemil Vahapoglu, Timothy J. O'Shea, Wan Liu, Sennur Ulukus  
**Category**: cs.LG  
**Published**: 2025-11-06  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.03632v1  

Beamforming has significance for enhancing spectral efficiency and mitigating interference in multi-antenna wireless systems, facilitating spatial multiplexing and diversity in dense and high mobility scenarios. Traditional beamforming techniques such as zero-forcing beamforming (ZFBF) and minimum m...

---

### 19. [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)

**Authors**: Xiangyu Sun, Amin Yousefpour, Shirin Hosseinmardi, Ramin Bostanabad  
**Category**: cs.LG  
**Published**: 2025-11-06  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2507.09968v2  

Machine learning (ML) techniques have recently gained significant attention for solving compliance minimization (CM) problems. However, these methods typically provide poor feature boundaries, are very expensive, and lack a systematic mechanism to control the design complexity. Herein, we address th...

---

### 20. [GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models](https://arxiv.org/abs/2511.03251)

**Authors**: Zhibin Wang, Zhixing Zhang, Shuqi Wang, Xuanting Xie, Zhao Kang  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2511.03251v1  

Graph Neural Networks (GNNs) have demonstrated impressive performance on task-specific benchmarks, yet their ability to generalize across diverse domains and tasks remains limited. Existing approaches often struggle with negative transfer, scalability issues, and high adaptation costs. To address th...

---

### 21. [RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based Autonomous Navigation](https://arxiv.org/abs/2505.14526)

**Authors**: Matteo El-Hariry, Antoine Richard, Ricard M. Castan, Luis F. W. Batista, Matthieu Geist, Cedric Pradalier, Miguel Olivares-Mendez  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2505.14526v2  

Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing frameworks and benchmarks are often constrained to...

---

### 22. [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)

**Authors**: Joseph Ramsey, Bryan Andrews, Peter Spirtes  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.04263v3  

Learning causal structure from observational data is especially challenging when latent variables or selection bias are present. The Fast Causal Inference (FCI) algorithm addresses this setting but performs exhaustive conditional independence tests across many subsets, often leading to spurious inde...

---

### 23. [IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs](https://arxiv.org/abs/2511.03237)

**Authors**: Souvik Rana, Arul Menezes, Ashish Kulkarni, Chandra Khatri, Shubham Agarwal  
**Category**: cs.CL  
**Published**: 2025-11-06  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2511.03237v1  

Tokenizers play a crucial role in determining the performance, training efficiency, and the inference cost of Large Language Models (LLMs). Designing effective tokenizers for multilingual LLMs is particularly challenging due to diverse scripts and rich morphological variation. While subword methods ...

---

### 24. [L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3](https://arxiv.org/abs/2511.01602)

**Authors**: Xinyue Yang, Chen Zheng, Yaoyang Hou, Renhao Zhang, Yinyan Zhang, Yanjun Wu, Heng Zhang  
**Category**: cs.LG  
**Published**: 2025-11-06  
**Score**: 6.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.01602v2  

Configuration tuning is critical for database performance. Although recent advancements in database tuning have shown promising results in throughput and latency improvement, challenges remain. First, the vast knob space makes direct optimization unstable and slow to converge. Second, reinforcement ...

---

### 25. [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)

**Authors**: Ming Gu, Ziwei Wang, Sicen Lai, Zirui Gao, Sheng Zhou, Jiajun Bu  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2511.03471v1  

Ensuring web accessibility is crucial for advancing social welfare, justice, and equality in digital spaces, yet the vast majority of website user interfaces remain non-compliant, due in part to the resource-intensive and unscalable nature of current auditing practices. While WCAG-EM offers a struct...

---

### 26. [Proof-of-Spiking-Neurons(PoSN): Neuromorphic Consensus for Next-Generation Blockchains](https://arxiv.org/abs/2511.02868)

**Authors**: M. Z. Haider, M. U Ghouri, Tayyaba Noreen, M. Salman  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 6.0  
**Type**: cross  
**ArXiv ID**: 2511.02868v1  

Blockchain systems face persistent challenges of scalability, latency, and energy inefficiency. Existing consensus protocols such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) either consume excessive resources or risk centralization. This paper proposes \textit{Proof-of-Spiking-Neurons (PoSN)}, a...

---

### 27. [A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies](https://arxiv.org/abs/2511.03201)

**Authors**: Hassan Wasswa, Hussein Abbass, Timothy Lynar  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 6.0  
**Type**: cross  
**ArXiv ID**: 2511.03201v1  

In an effort to counter the increasing IoT botnet-based attacks, state-of-the-art deep learning methods have been proposed and have achieved impressive detection accuracy. However, their computational intensity restricts deployment on resource-constrained IoT devices, creating a critical need for li...

---

### 28. [Flow matching for reaction pathway generation](https://arxiv.org/abs/2507.10530)

**Authors**: Ping Tuo, Jiale Chen, Ju Li  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 6.0  
**Type**: replace-cross  
**ArXiv ID**: 2507.10530v4  

Elucidating reaction mechanisms hinges on efficiently generating transition states (TSs), products, and complete reaction networks. Recent generative models, such as diffusion models for TS sampling and sequence-based architectures for product generation, offer faster alternatives to quantum-chemist...

---

### 29. [Activation Transport Operators](https://arxiv.org/abs/2508.17540)

**Authors**: Andrzej Szablewski, Marek Masiak  
**Category**: cs.AI  
**Published**: 2025-11-06  
**Score**: 6.0  
**Type**: replace-cross  
**ArXiv ID**: 2508.17540v2  

The residual stream mediates communication between transformer decoder layers via linear reads and writes of non-linear computations. While sparse-dictionary learning-based methods locate features in the residual stream, and activation patching methods discover circuits within the model, the mechani...

---

### 30. [Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives](https://arxiv.org/abs/2508.02853)

**Authors**: Yinuo Xu, Veronica Derricks, Allison Earl, David Jurgens  
**Category**: cs.CL  
**Published**: 2025-11-06  
**Score**: 6.0  
**Type**: replace  
**ArXiv ID**: 2508.02853v3  

We present an approach to modeling annotator disagreement in subjective NLP tasks through both architectural and data-centric innovations. Our model, DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert subnetworks based on annotator demographics, enabling it to better represent s...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
