# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-07 12:52:54 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)

**Authors**: Rongxiang Wang, Kangyuan Shu, Felix Xiaozhu Lin  
**Category**: cs.DC  
**Published**: 2025-11-07  
**Score**: 14.0  
**Type**: new  
**ArXiv ID**: 2511.04477v1  

Deploying large language models (LLMs) on end-user devices is gaining importance due to benefits in responsiveness, privacy, and operational cost. Yet the limited memory and compute capability of mobile and desktop GPUs make efficient execution difficult. Recent observations suggest that the interna...

---

### 2. [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)

**Authors**: Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Hans Vandierendonck, Deepu John, Bo Ji, Dimitrios Nikolopoulos  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 12.5  
**Type**: replace-cross  
**ArXiv ID**: 2506.09397v5  

The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization...

---

### 3. [SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference](https://arxiv.org/abs/2510.10302)

**Authors**: Liangkun Chen, Zijian Wen, Tian Wu, Xiaoxi Zhang, Chuan Wu  
**Category**: cs.DC  
**Published**: 2025-11-07  
**Score**: 11.0  
**Type**: replace  
**ArXiv ID**: 2510.10302v2  

The Mixture-of-Experts (MoE) architecture has been widely adopted in large language models (LLMs) to reduce computation cost through model sparsity. Employing speculative decoding (SD) can further accelerate MoE inference by drafting multiple tokens per step and verifying them in parallel. However, ...

---

### 4. [TwIST: Rigging the Lottery in Transformers with Independent Subnetwork Training](https://arxiv.org/abs/2511.03983)

**Authors**: Michael Menezes, Barbara Su, Xinze Feng, Yehya Farhat, Hamza Shili, Anastasios Kyrillidis  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2511.03983v1  

We introduce TwIST, a distributed training framework for efficient large language model (LLM) sparsification. TwIST trains multiple subnetworks in parallel, periodically aggregates their parameters, and resamples new subnetworks during training. This process identifies high-quality subnetworks ("gol...

---

### 5. [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)

**Authors**: Mingyu Sung, Vikas Palakonda, Suhwan Im, Sunghwan Moon, Il-Min Kim, Sangseok Yun, Jae-Mo Kang  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2511.04002v1  

Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing ...

---

### 6. [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884)

**Authors**: Zijian Zhang, Rong Wang, Shiyang Li, Yuebo Luo, Mingyi Hong, Caiwen Ding  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2511.01884v2  

Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, ...

---

### 7. [Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models](https://arxiv.org/abs/2503.22879)

**Authors**: Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu  
**Category**: cs.CL  
**Published**: 2025-11-07  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2503.22879v4  

State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To o...

---

### 8. [Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference](https://arxiv.org/abs/2511.04286)

**Authors**: Matteo Cercola, Valeria Capretti, Simone Formentin  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2511.04286v1  

Learning from human preferences is a cornerstone of aligning machine learning models with subjective human judgments. Yet, collecting such preference data is often costly and time-consuming, motivating the need for more efficient learning paradigms. Two established approaches offer complementary adv...

---

### 9. [SySMOL: Co-designing Algorithms and Hardware for Neural Networks with Heterogeneous Precisions](https://arxiv.org/abs/2311.14114)

**Authors**: Cyrus Zhou, Pedro Savarese, Zack Hassman, Vaughn Richard, Michael DiBrino, Michael Maire, Yanjing Li  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2311.14114v3  

Ultra-low-precision inference can sharply reduce memory and latency but often degrades accuracy and relies on specialized hardware. We present SONIQ, a system-optimized, noise-injected quantization framework that learns per-channel mixed precision for both weights and activations while training unde...

---

### 10. [Communication Efficient LLM Pre-training with SparseLoCo](https://arxiv.org/abs/2508.15706)

**Authors**: Amir Sarfi, Benjamin Th\'erien, Joel Lidin, Eugene Belilovsky  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 9.5  
**Type**: replace  
**ArXiv ID**: 2508.15706v2  

Communication-efficient distributed training algorithms have received considerable interest recently due to their benefits for training Large Language Models (LLMs) in bandwidth-constrained settings, such as across datacenters and over the internet. Despite reducing communication frequency, these me...

---

### 11. [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)

**Authors**: Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2511.03092v2  

The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy....

---

### 12. [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)

**Authors**: Miko{\l}aj Langner, Jan Eliasz, Ewa Rudnicka, Jan Koco\'n  
**Category**: cs.CL  
**Published**: 2025-11-07  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2511.03830v1  

We introduce a method for efficient multi-label text classification with large language models (LLMs), built on reformulating classification tasks as sequences of dichotomic (yes/no) decisions. Instead of generating all labels in a single structured response, each target dimension is queried indepen...

---

### 13. [Declarative Data Pipeline for Large Scale ML Services](https://arxiv.org/abs/2508.15105)

**Authors**: Yunzhao Yang, Runhui Wang, Xuanqing Liu, Adit Krishnan, Yefan Tao, Yuqian Deng, Kuangyou Yao, Peiyuan Sun, Henrik Johnson, Aditi sinha, Davor Golac, Gerald Friedland, Usman Shakeel, Daryl Cooke, Joe Sullivan, Madhusudhanan Chandrasekaran, Chris Kong  
**Category**: cs.DC  
**Published**: 2025-11-07  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2508.15105v4  

Modern distributed data processing systems struggle to balance performance, maintainability, and developer productivity when integrating machine learning at scale. These challenges intensify in large collaborative environments due to high communication overhead and coordination complexity. We presen...

---

### 14. [ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads](https://arxiv.org/abs/2511.04162)

**Authors**: Xiaokai Wang, Shaoyuan Huang, Yuting Li, Xiaofei Wang  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.04162v1  

Deep neural networks (DNNs) form the cornerstone of modern AI services, supporting a wide range of applications, including autonomous driving, chatbots, and recommendation systems. As models increase in size and complexity, DNN workloads like training and inference tasks impose unprecedented demands...

---

### 15. [Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes](https://arxiv.org/abs/2505.02184)

**Authors**: Matthew T. Dearing, Yiheng Tao, Xingfu Wu, Zhiling Lan, Valerie Taylor  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2505.02184v2  

While large language models (LLMs) are increasingly used for generating parallel scientific codes, most efforts emphasize functional correctness, often overlooking performance, especially energy efficiency. We propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-effic...

---

### 16. [Traversal Verification for Speculative Tree Decoding](https://arxiv.org/abs/2505.12398)

**Authors**: Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, Zhongchao Shi  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.12398v2  

Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be ...

---

### 17. [TensorHyper-VQC: A Tensor-Train-Guided Hypernetwork for Robust and Scalable Variational Quantum Computing](https://arxiv.org/abs/2508.01116)

**Authors**: Jun Qi, Chao-Han Yang, Pin-Yu Chen, Min-Hsiu Hsieh  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2508.01116v2  

Variational Quantum Computing (VQC) faces fundamental scalability barriers, primarily due to the presence of barren plateaus and its sensitivity to quantum noise. To address these challenges, we introduce TensorHyper-VQC, a novel tensor-train (TT)-guided hypernetwork framework that significantly imp...

---

### 18. [Arrow: Adaptive Scheduling Mechanisms for Disaggregated LLM Inference Architecture](https://arxiv.org/abs/2505.11916)

**Authors**: Yu Wu, Tongxuan Liu, Yuting Zeng, Siyu Wu, Jun Xiong, Xianzhe Dong, Hailong Yang, Ke Zhang, Jing Li  
**Category**: cs.DC  
**Published**: 2025-11-07  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2505.11916v2  

Existing large language model (LLM) serving systems typically employ Prefill-Decode disaggregated architecture to prevent computational interference between the prefill and decode phases. However, in real-world LLM serving scenarios, significant fluctuations in request input/output lengths lead to i...

---

### 19. [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)

**Authors**: Xinlu Zhang, Yansha Deng, Toktam Mahmoodi  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.04653v1  

Federated learning (FL) offers new opportunities in machine learning, particularly in addressing data privacy concerns. In contrast to conventional event-based federated learning, time-triggered federated learning (TT-Fed), as a general form of both asynchronous and synchronous FL, clusters users in...

---

### 20. [GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs](https://arxiv.org/abs/2411.14133)

**Authors**: Advik Raj Basani, Xiao Zhang  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2411.14133v3  

LLMs have shown impressive capabilities across various natural language processing tasks, yet remain vulnerable to input prompts, known as jailbreak attacks, carefully designed to bypass safety guardrails and elicit harmful responses. Traditional methods rely on manual heuristics but suffer from lim...

---

### 21. [Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference](https://arxiv.org/abs/2505.22913)

**Authors**: Donghyeon Joo, Helya Hosseini, Ramyad Hadidi, Bahar Asgari  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2505.22913v2  

We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly ef...

---

### 22. [Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning](https://arxiv.org/abs/2511.02818)

**Authors**: Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2511.02818v2  

Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as...

---

### 23. [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)

**Authors**: Yuantian Shao, Yuanteng Chen, Peisong Wang, Jianlin Yu, Jing Lin, Yiwu Yao, Zhihui Wei, Jian Cheng  
**Category**: cs.CL  
**Published**: 2025-11-07  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.04063v1  

Quantization plays a crucial role in accelerating the inference of large-scale models, and rotational matrices have been shown to effectively improve quantization performance by smoothing outliers. However, end-to-end fine-tuning of rotational optimization algorithms incurs high computational costs ...

---

### 24. [Action Deviation-Aware Inference for Low-Latency Wireless Robots](https://arxiv.org/abs/2510.02851)

**Authors**: Jeyoung Park, Yeonsub Lim, Seungeun Oh, Jihong Park, Jinho Choi, Seong-Lyun Kim  
**Category**: cs.DC  
**Published**: 2025-11-07  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.02851v2  

To support latency-sensitive AI applications ranging from autonomous driving to industrial robot manipulation, 6G envisions distributed ML with computational resources in mobile, edge, and cloud connected over hyper-reliable low-latency communication (HRLLC). In this setting, speculative decoding ca...

---

### 25. [On scalable and efficient training of diffusion samplers](https://arxiv.org/abs/2505.19552)

**Authors**: Minkyu Kim, Kiyoung Seong, Dongyeop Woo, Sungsoo Ahn, Minsu Kim  
**Category**: cs.LG  
**Published**: 2025-11-07  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2505.19552v4  

We address the challenge of training diffusion models to sample from unnormalized energy distributions in the absence of data, the so-called diffusion samplers. Although these approaches have shown promise, they struggle to scale in more demanding scenarios where energy evaluations are expensive and...

---

### 26. [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)

**Authors**: Gaia Grosso, Sai Sumedh R. Hindupur, Thomas Fel, Samuel Bright-Thonney, Philip Harris, Demba Ba  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.03095v1  

Modern artificial intelligence has revolutionized our ability to extract rich and versatile data representations across scientific disciplines. Yet, the statistical properties of these representations remain poorly controlled, causing misspecified anomaly detection (AD) methods to falter. Weak or ra...

---

### 27. [PerfDojo: Automated ML Library Generation for Heterogeneous Architectures](https://arxiv.org/abs/2511.03586)

**Authors**: Andrei Ivanov, Siyuan Shen, Gioele Gottardo, Marcin Chrapek, Afif Boudaoud, Timo Schneider, Luca Benini, Torsten Hoefler  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.03586v1  

The increasing complexity of machine learning models and the proliferation of diverse hardware architectures (CPUs, GPUs, accelerators) make achieving optimal performance a significant challenge. Heterogeneity in instruction sets, specialized kernel requirements for different data types and model fe...

---

### 28. [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)

**Authors**: Mohsen Ahmadzadeh, Kaichang Chen, Georges Gielen  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2511.03697v1  

Analog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniqu...

---

### 29. [FaStfact: Faster, Stronger Long-Form Factuality Evaluations in LLMs](https://arxiv.org/abs/2510.12839)

**Authors**: Yingjia Wan, Haochen Tan, Xiao Zhu, Xinyu Zhou, Zhiwei Li, Qingsong Lv, Changxuan Sun, Jiaqi Zeng, Yi Xu, Jianqiao Lu, Yinhong Liu, Zhijiang Guo  
**Category**: cs.AI  
**Published**: 2025-11-07  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.12839v2  

Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to efficiency bottlenecks and reliability concerns. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawback...

---

### 30. [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)

**Authors**: Mohammad Atif Quamar, Mohammad Areeb, Mikhail Kuznetsov, Muslum Ozgur Ozmen, Z. Berkay Celik  
**Category**: cs.CL  
**Published**: 2025-11-07  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.03827v1  

Aligning large language models with human values is crucial for their safe deployment; however, existing methods, such as fine-tuning, are computationally expensive and suboptimal. In contrast, inference-time approaches like Best-of-N sampling require practically infeasible computation to achieve op...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
