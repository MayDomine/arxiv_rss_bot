# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-03 12:54:27 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding](https://arxiv.org/abs/2508.21706)

**Authors**: Zhibin Wang, Zhonghui Zhang, Yuhang Zhou, Zibo Wang, Mo Zhou, Peng Jiang, Weilin Cai, Chengying Huan, Rong Gu, Sheng Zhong, Chen Tian  
**Category**: cs.DC  
**Published**: 2025-11-03  
**Score**: 11.5  
**Type**: replace  
**ArXiv ID**: 2508.21706v2  

Recent advancements in Mixture of Experts (MoE) models have significantly increased their parameter scale as well as model performance. Extensive offloading techniques have been proposed to address the GPU memory limitations of MoE inference. However, due to the I/O bottleneck and sparse computation...

---

### 2. [SpecAttn: Speculating Sparse Attention](https://arxiv.org/abs/2510.27641)

**Authors**: Harsh Shah  
**Category**: cs.CL  
**Published**: 2025-11-03  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2510.27641v1  

Large Language Models (LLMs) face significant computational bottlenecks during inference due to the quadratic complexity of self-attention mechanisms, particularly as context lengths increase. We introduce SpecAttn, a novel training-free approach that seamlessly integrates with existing speculative ...

---

### 3. [FlowMesh: A Service Fabric for Composable LLM Workflows](https://arxiv.org/abs/2510.26913)

**Authors**: Junyi Shen, Noppanat Wadlom, Lingfeng Zhou, Dequan Wang, Xu Miao, Lei Fang, Yao Lu  
**Category**: cs.DC  
**Published**: 2025-11-03  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2510.26913v1  

AI deployment increasingly resembles a pipeline of data transformation, fine-tuning, and agent interactions rather than a monolithic LLM job; recent examples include RLHF/RLAIF training and agentic workflows. To cope with this shift, we propose FlowMesh, a multi-tenant service fabric that executes a...

---

### 4. [Declarative Data Pipeline for Large Scale ML Services](https://arxiv.org/abs/2508.15105)

**Authors**: Yunzhao Yang, Runhui Wang, Xuanqing Liu, Adit Krishnan, Yefan Tao, Yuqian Deng, Kuangyou Yao, Peiyuan Sun, Henrik Johnson, Aditi sinha, Davor Golac, Gerald Friedland, Usman Shakeel, Daryl Cooke, Joe Sullivan, Chris Kong  
**Category**: cs.DC  
**Published**: 2025-11-03  
**Score**: 10.5  
**Type**: replace  
**ArXiv ID**: 2508.15105v2  

Modern distributed data processing systems face significant challenges in balancing system performance with code maintainability and developer productivity, particularly when integrating machine learning capabilities at scale. In large collaborative environments, these challenges are amplified by hi...

---

### 5. [RaanA: A Fast, Flexible, and Data-Efficient Post-Training Quantization Algorithm](https://arxiv.org/abs/2504.03717)

**Authors**: Yongyi Yang, Jianyang Gao, Wei Hu  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2504.03717v2  

Post-training Quantization (PTQ) has become a widely used technique for improving inference efficiency of large language models (LLMs). However, existing PTQ methods generally suffer from crucial limitations such as heavy calibration data requirements and inflexible choice of target number of bits. ...

---

### 6. [RDMA Point-to-Point Communication for LLM Systems](https://arxiv.org/abs/2510.27656)

**Authors**: Nandor Licker (Perplexity AI), Kevin Hu (Perplexity AI), Vladimir Zaytsev (Perplexity AI), Lequn Chen (Perplexity AI)  
**Category**: cs.DC  
**Published**: 2025-11-03  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2510.27656v1  

Emerging Large Language Model (LLM) system patterns, such as disaggregated inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement fine-tuning, require flexible point-to-point communication beyond simple collectives. Existing implementations are locked to specific Network Interfa...

---

### 7. [BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for Scalable and Efficient Text Summarization](https://arxiv.org/abs/2510.27516)

**Authors**: Desta Haileselassie Hagos, Legand L. Burge, Anietie Andy, Anis Yazidi, Vladimir Vlassov  
**Category**: cs.CL  
**Published**: 2025-11-03  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2510.27516v1  

Transformer-based architectures have advanced text summarization, yet their quadratic complexity limits scalability on long documents. This paper introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a novel framework that combines sparse attention, adaptive spans, and bilinear at...

---

### 8. [CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs](https://arxiv.org/abs/2510.26843)

**Authors**: Zhiyuan Ning, Jiawei Shao, Ruge Xu, Xinfei Guo, Jun Zhang, Chi Zhang, Xuelong Li  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2510.26843v1  

Speculative decoding has become a widely adopted as an effective technique for lossless inference acceleration when deploying large language models (LLMs). While on-the-fly self-speculative methods offer seamless integration and broad utility, they often fall short of the speed gains achieved by met...

---

### 9. [BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs](https://arxiv.org/abs/2510.26892)

**Authors**: Mahsa Valizadeh, Rui Tuo, James Caverlee  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2510.26892v1  

Generative Adversarial Networks (GANs) are proficient at generating synthetic data but continue to suffer from mode collapse, where the generator produces a narrow range of outputs that fool the discriminator but fail to capture the full data distribution. This limitation is particularly problematic...

---

### 10. [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594)

**Authors**: Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, Jianfei Chen  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.11594v2  

The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, wh...

---

### 11. [Synergistic Tensor and Pipeline Parallelism](https://arxiv.org/abs/2510.27257)

**Authors**: Mengshi Qi, Jiaxuan Peng, Jie Zhang, Juan Zhu, Yong Li, Huadong Ma  
**Category**: cs.DC  
**Published**: 2025-11-03  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2510.27257v1  

In the machine learning system, the hybrid model parallelism combining tensor parallelism (TP) and pipeline parallelism (PP) has become the dominant solution for distributed training of Large Language Models~(LLMs) and Multimodal LLMs (MLLMs). However, TP introduces significant collective communicat...

---

### 12. [Fints: Efficient Inference-Time Personalization for LLMs with Fine-Grained Instance-Tailored Steering](https://arxiv.org/abs/2510.27206)

**Authors**: Kounianhua Du, Jianxing Liu, Kangning Zhang, Wenxiang Jiao, Yuan Lu, Jiarui Jin, Weiwen Liu, Yong Yu, Weinan Zhang  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2510.27206v1  

The rapid evolution of large language models (LLMs) has intensified the demand for effective personalization techniques that can adapt model behavior to individual user preferences. Despite the non-parametric methods utilizing the in-context learning ability of LLMs, recent parametric adaptation met...

---

### 13. [Mixture-of-Transformers Learn Faster: A Theoretical Study on Classification Problems](https://arxiv.org/abs/2510.27004)

**Authors**: Hongbo Li, Qinhang Wu, Sen Lin, Yingbin Liang, Ness B. Shroff  
**Category**: cs.LG  
**Published**: 2025-11-03  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2510.27004v1  

Mixture-of-Experts (MoE) models improve transformer efficiency but lack a unified theoretical explanation, especially when both feed-forward and attention layers are allowed to specialize. To this end, we study the Mixture-of-Transformers (MoT), a tractable theoretical framework in which each transf...

---

### 14. [AERO: Entropy-Guided Framework for Private LLM Inference](https://arxiv.org/abs/2410.13060)

**Authors**: Nandan Kumar Jha, Brandon Reagen  
**Category**: cs.LG  
**Published**: 2025-11-03  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2410.13060v3  

Privacy-preserving computation enables language model inference directly on encrypted data yet suffers from prohibitive latency and communication overheads, primarily due to nonlinear functions. Removing nonlinearities, however, can trigger one of two failure modes restricting the potential for nonl...

---

### 15. [Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications](https://arxiv.org/abs/2510.27186)

**Authors**: Zixuan Hu, Yongxian Wei, Li Shen, Zhenyi Wang, Lei Li, Chun Yuan, Dacheng Tao  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.27186v1  

Model inversion, which aims to reconstruct the original training data from pre-trained discriminative models, is especially useful when the original training data is unavailable due to privacy, usage rights, or size constraints. However, existing dense inversion methods attempt to reconstruct the en...

---

### 16. [HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving](https://arxiv.org/abs/2504.10724)

**Authors**: Avinash Kumar, Shashank Nag, Jason Clemons, Lizy John, Poulami Das  
**Category**: cs.CL  
**Published**: 2025-11-03  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2504.10724v2  

Early-Exit Large Language Models (EE-LLMs) enable high throughput inference by allowing tokens to exit early at intermediate layers. However, their throughput is limited by the computational and memory savings. Existing EE-LLM frameworks rely on a single model and therefore, their token generation l...

---

### 17. [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)

**Authors**: Shoutao Guo, Shaolei Zhang, Qingkai Fang, Zhengrui Ma, Min Zhang, Yang Feng  
**Category**: cs.CL  
**Published**: 2025-11-03  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2507.14815v2  

The rapid advancement of Large Language Models (LLMs) has spurred significant progress in Large Speech-Language Models (LSLMs), enhancing their capabilities in both speech understanding and generation. While existing LSLMs often concentrate on augmenting speech generation or tackling a diverse array...

---

### 18. [Scaling Tractable Probabilistic Circuits: A Systems Perspective](https://arxiv.org/abs/2406.00766)

**Authors**: Anji Liu, Kareem Ahmed, Guy Van den Broeck  
**Category**: cs.LG  
**Published**: 2025-11-03  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2406.00766v2  

Probabilistic Circuits (PCs) are a general framework for tractable deep generative models, which support exact and efficient probabilistic inference on their learned distributions. Recent modeling and training advancements have enabled their application to complex real-world tasks. However, the time...

---

### 19. [Glia: A Human-Inspired AI for Automated Systems Design and Optimization](https://arxiv.org/abs/2510.27176)

**Authors**: Pouya Hamadanian, Pantea Karimi, Arash Nasr-Esfahany, Kimia Noorbakhsh, Joseph Chandler, Ali ParandehGheibi, Mohammad Alizadeh, Hari Balakrishnan  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.27176v1  

Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reason...

---

### 20. [Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks](https://arxiv.org/abs/2510.27208)

**Authors**: Jiaxin Zhang, Zehong Zhu, Junye Deng, Yunqin Li, and Bowen Wang  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.27208v1  

Villages areas hold significant importance in the study of human-land relationships. However, with the advancement of urbanization, the gradual disappearance of spatial characteristics and the homogenization of landscapes have emerged as prominent issues. Existing studies primarily adopt a single-di...

---

### 21. [Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V](https://arxiv.org/abs/2510.27364)

**Authors**: Meftun Akarsu, Kerem Catay, Sedat Bin Vedat, Enes Kutay Yarkan, Ilke Senturk, Arda Sar, Dafne Eksioglu  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.27364v1  

We present a practical pipeline for fine-tuning open-source video diffusion transformers to synthesize cinematic scenes for television and film production from small datasets. The proposed two-stage process decouples visual style learning from motion generation. In the first stage, Low-Rank Adaptati...

---

### 22. [Spiking Neural Networks: The Future of Brain-Inspired Computing](https://arxiv.org/abs/2510.27379)

**Authors**: Sales G. Aribe Jr  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.27379v1  

Spiking Neural Networks (SNNs) represent the latest generation of neural computation, offering a brain-inspired alternative to conventional Artificial Neural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals, SNNs operate using distinct spike events, making them inherently more...

---

### 23. [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)

**Authors**: Vladyslav Moroshan, Julien Siems, Arber Zela, Timur Carstensen, Frank Hutter  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.25502v2  

Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear...

---

### 24. [Minitron-SSM: Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning](https://arxiv.org/abs/2504.11409)

**Authors**: Ali Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, Marcin Chochowski, Yashaswi Karnati, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi Olabiyi, Daniel Korzekwa, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, Bryan Catanzaro, Ashwath Aithal, Nima Tajbakhsh, Pavlo Molchanov  
**Category**: cs.CL  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2504.11409v2  

Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training...

---

### 25. [DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference](https://arxiv.org/abs/2510.19669)

**Authors**: Xiang Liu, Xuming Hu, Xiaowen Chu, Eunsol Choi  
**Category**: cs.CL  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2510.19669v2  

Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their efficiency, enabling them to reach high performance without overthinking. First, we analyze the entropy of t...

---

### 26. [SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference](https://arxiv.org/abs/2510.27182)

**Authors**: Zongshun Zhang, Ibrahim Matta  
**Category**: cs.DC  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.27182v1  

Dynamic offloading of Machine Learning (ML) model partitions across different resource orchestration services, such as Function-as-a-Service (FaaS) and Infrastructure-as-a-Service (IaaS), can balance processing and transmission delays while minimizing costs of adaptive inference applications. Howeve...

---

### 27. [Hybrid Dual-Batch and Cyclic Progressive Learning for Efficient Distributed Training](https://arxiv.org/abs/2509.26092)

**Authors**: Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu, Jan-Jan Wu  
**Category**: cs.DC  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2509.26092v2  

Distributed machine learning is critical for training deep learning models on large datasets with numerous parameters. Current research primarily focuses on leveraging additional hardware resources and powerful computing units to accelerate the training process. As a result, larger batch sizes are o...

---

### 28. [An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning](https://arxiv.org/abs/2510.26709)

**Authors**: Chuyan Chen, Chenyang Ma, Zhangxin Li, Yutong He, Yanjie Dong, Kun Yuan  
**Category**: cs.DC  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2510.26709v2  

Communication remains a central bottleneck in large-scale distributed machine learning, and gradient sparsification has emerged as a promising strategy to alleviate this challenge. However, existing gradient compressors face notable limitations: Rand-$K$ discards structural information and performs ...

---

### 29. [Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores](https://arxiv.org/abs/2510.27145)

**Authors**: Sein Kwon, Seulgi Baek, Hyunseo Yang, Youngwan Jo, Sanghyun Park  
**Category**: cs.LG  
**Published**: 2025-11-03  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.27145v1  

Database Management Systems (DBMSs) are fundamental for managing large-scale and heterogeneous data, and their performance is critically influenced by configuration parameters. Effective tuning of these parameters is essential for adapting to diverse workloads and maximizing throughput while minimiz...

---

### 30. [Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase](https://arxiv.org/abs/2510.27002)

**Authors**: Mihir Mahajan, Alfred Nguyen, Franz Srambical, Stefan Bauer  
**Category**: cs.AI  
**Published**: 2025-11-03  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.27002v1  

While world models are increasingly positioned as a pathway to overcoming data scarcity in domains such as robotics, open training infrastructure for world modeling remains nascent. We introduce Jasmine, a performant JAX-based world modeling codebase that scales from single hosts to hundreds of acce...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
