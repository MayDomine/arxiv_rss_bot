# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-04 12:57:19 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term Memory Access](https://arxiv.org/abs/2504.16795)

**Authors**: Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu  
**Category**: cs.CL  
**Published**: 2025-11-04  
**Score**: 13.0  
**Type**: replace  
**ArXiv ID**: 2504.16795v2  

A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms...

---

### 2. [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)

**Authors**: Hao Zhang, Aining Jia, Weifeng Bu, Yushu Cai, Kai Sheng, Hao Chen, Xin He  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 12.0  
**Type**: replace  
**ArXiv ID**: 2508.04405v2  

Large Language Models (LLMs) demonstrate exceptional performance but entail significant memory and computational costs, restricting their practical deployment. While existing INT4/INT8 quantization reduces these costs, they often degrade accuracy or lack optimal efficiency. INT6 quantization offers ...

---

### 3. [AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs](https://arxiv.org/abs/2511.00796)

**Authors**: Ran Yan, Youhe Jiang, Tianyuan Wu, Jiaxuan Gao, Zhiyu Mei, Wei Fu, Haohui Mai, Wei Wang, Yi Wu, Binhang Yuan  
**Category**: cs.DC  
**Published**: 2025-11-04  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2511.00796v1  

Maximizing training throughput and cost-efficiency of RL for LLMs is essential to democratize this advanced technique. One promising but challenging approach is to deploy such a computational workflow over heterogeneous GPUs. Unlike conventional large-scale LLM pretraining, RL training generally dec...

---

### 4. [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)

**Authors**: Jungyeon Koh, Hyun Jong Yang  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2511.01695v1  

The growing demand for on-device large language model (LLM) inference highlights the need for efficient mobile edge computing (MEC) solutions, especially in resource-constrained settings. Speculative decoding offers a promising solution by partitioning token generation between a lightweight draft mo...

---

### 5. [RaanA: A Fast, Flexible, and Data-Efficient Post-Training Quantization Algorithm](https://arxiv.org/abs/2504.03717)

**Authors**: Yongyi Yang, Jianyang Gao, Wei Hu  
**Category**: cs.AI  
**Published**: 2025-11-04  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2504.03717v2  

Post-training Quantization (PTQ) has become a widely used technique for improving inference efficiency of large language models (LLMs). However, existing PTQ methods generally suffer from crucial limitations such as heavy calibration data requirements and inflexible choice of target number of bits. ...

---

### 6. [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)

**Authors**: Dinghong Song, Yuan Feng, Yiwei Wang, Shangye Chen, Cyril Guyot, Filip Blagojevic, Hyeran Jeon, Pengfei Su, Dong Li  
**Category**: cs.CL  
**Published**: 2025-11-04  
**Score**: 10.0  
**Type**: replace  
**ArXiv ID**: 2510.25979v2  

Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model en...

---

### 7. [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)

**Authors**: Jin Huang, Yuchao Jin, Le An, Josh Park  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 10.0  
**Type**: replace  
**ArXiv ID**: 2506.07416v2  

This paper introduces an efficient Vision-Language Model (VLM) pipeline specifically optimized for deployment on embedded devices, such as those used in robotics and autonomous driving. The pipeline significantly reduces the computational overhead by jointly leveraging patch selection to filter irre...

---

### 8. [Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.00294)

**Authors**: Shutong Wu, Jiawei Zhang  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 10.0  
**Type**: replace  
**ArXiv ID**: 2510.00294v2  

Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges lik...

---

### 9. [SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware](https://arxiv.org/abs/2510.15542)

**Authors**: Ivan Kartashov, Mariia Pushkareva, Iakov Karandashev  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 10.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.15542v2  

This paper introduces SpikeFit, a novel training method for Spiking Neural Networks (SNNs) that enables efficient inference on neuromorphic hardware, considering all its stringent requirements: the number of neurons and synapses that can fit on a single device, and lower bit-width representations (e...

---

### 10. [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)

**Authors**: Zicheng Xu, Guanchu Wang, Yu-Neng Chuang, Guangyao Zheng, Alexander S. Szalay, Zirui Liu, Vladimir Braverman  
**Category**: cs.CL  
**Published**: 2025-11-04  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2511.00640v1  

Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reas...

---

### 11. [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)

**Authors**: Chengying Huan, Ziheng Meng, Yongchao Liu, Zhengyi Yang, Yun Zhu, Yue Yun, Shipeng Li, Rong Gu, Xiabao Wu, Haitao Zhang, Chuntao Hong, Shaonan Ma, Guihai Chen, Chen Tian  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.01633v1  

Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context...

---

### 12. [CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs](https://arxiv.org/abs/2510.26843)

**Authors**: Zhiyuan Ning, Jiawei Shao, Ruge Xu, Xinfei Guo, Jun Zhang, Chi Zhang, Xuelong Li  
**Category**: cs.AI  
**Published**: 2025-11-04  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2510.26843v1  

Speculative decoding has become a widely adopted as an effective technique for lossless inference acceleration when deploying large language models (LLMs). While on-the-fly self-speculative methods offer seamless integration and broad utility, they often fall short of the speed gains achieved by met...

---

### 13. [BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs](https://arxiv.org/abs/2510.26892)

**Authors**: Mahsa Valizadeh, Rui Tuo, James Caverlee  
**Category**: cs.AI  
**Published**: 2025-11-04  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2510.26892v1  

Generative Adversarial Networks (GANs) are proficient at generating synthetic data but continue to suffer from mode collapse, where the generator produces a narrow range of outputs that fool the discriminator but fail to capture the full data distribution. This limitation is particularly problematic...

---

### 14. [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594)

**Authors**: Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, Jianfei Chen  
**Category**: cs.AI  
**Published**: 2025-11-04  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.11594v2  

The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, wh...

---

### 15. [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)

**Authors**: Fali Wang, Jihai Chen, Shuhua Yang, Runxue Bao, Tianxiang Zhao, Zhiwei Zhang, Xianfeng Tang, Hui Liu, Qi He, Suhang Wang  
**Category**: cs.CL  
**Published**: 2025-11-04  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2511.00086v1  

Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking...

---

### 16. [Fints: Efficient Inference-Time Personalization for LLMs with Fine-Grained Instance-Tailored Steering](https://arxiv.org/abs/2510.27206)

**Authors**: Kounianhua Du, Jianxing Liu, Kangning Zhang, Wenxiang Jiao, Yuan Lu, Jiarui Jin, Weiwen Liu, Yong Yu, Weinan Zhang  
**Category**: cs.AI  
**Published**: 2025-11-04  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2510.27206v1  

The rapid evolution of large language models (LLMs) has intensified the demand for effective personalization techniques that can adapt model behavior to individual user preferences. Despite the non-parametric methods utilizing the in-context learning ability of LLMs, recent parametric adaptation met...

---

### 17. [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)

**Authors**: Jameson Sandler, Jacob K. Christopher, Thomas Hartvigsen, Nando Fioretto  
**Category**: cs.CL  
**Published**: 2025-11-04  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2511.00606v1  

Speculative decoding has become the standard approach for accelerating Large Language Model (LLM) inference. It exploits a lossless draft-then-verify procedure to circumvent the latency of autoregressive decoding, achieving impressive speed-ups. Yet, current speculative decoding approaches remain li...

---

### 18. [Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding](https://arxiv.org/abs/2503.01422)

**Authors**: Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, Rui Wang  
**Category**: cs.CL  
**Published**: 2025-11-04  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2503.01422v3  

Test-time scaling enhances large language model performance by allocating additional compute resources during inference. Best-of-N (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However...

---

### 19. [Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning](https://arxiv.org/abs/2502.03304)

**Authors**: Qitao Tan, Jun Liu, Zheng Zhan, Caiwei Ding, Yanzhi Wang, Xiaolong Ma, Jaewoo Lee, Jin Lu, Geng Yuan  
**Category**: cs.CL  
**Published**: 2025-11-04  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2502.03304v4  

Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes...

---

### 20. [Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems](https://arxiv.org/abs/2511.00336)

**Authors**: Siva Sai, Manish Prasad, Animesh Bhargava, Vinay Chamola, Rajkumar Buyya  
**Category**: cs.DC  
**Published**: 2025-11-04  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.00336v1  

The rapid growth of Internet of Medical Things (IoMT) devices has resulted in significant security risks, particularly the risk of malware attacks on resource-constrained devices. Conventional deep learning methods are impractical due to resource limitations, while Federated Learning (FL) suffers fr...

---

### 21. [Declarative Data Pipeline for Large Scale ML Services](https://arxiv.org/abs/2508.15105)

**Authors**: Yunzhao Yang, Runhui Wang, Xuanqing Liu, Adit Krishnan, Yefan Tao, Yuqian Deng, Kuangyou Yao, Peiyuan Sun, Henrik Johnson, Aditi sinha, Davor Golac, Gerald Friedland, Usman Shakeel, Daryl Cooke, Joe Sullivan, Madhu Chandra, Chris Kong  
**Category**: cs.DC  
**Published**: 2025-11-04  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2508.15105v3  

Modern distributed data processing systems struggle to balance performance, maintainability, and developer productivity when integrating machine learning at scale. These challenges intensify in large collaborative environments due to high communication overhead and coordination complexity. We presen...

---

### 22. [Multi-head Temporal Latent Attention](https://arxiv.org/abs/2505.13544)

**Authors**: Keqi Deng, Philip C. Woodland  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2505.13544v3  

While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Mu...

---

### 23. [Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications](https://arxiv.org/abs/2510.27186)

**Authors**: Zixuan Hu, Yongxian Wei, Li Shen, Zhenyi Wang, Lei Li, Chun Yuan, Dacheng Tao  
**Category**: cs.AI  
**Published**: 2025-11-04  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.27186v1  

Model inversion, which aims to reconstruct the original training data from pre-trained discriminative models, is especially useful when the original training data is unavailable due to privacy, usage rights, or size constraints. However, existing dense inversion methods attempt to reconstruct the en...

---

### 24. [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)

**Authors**: Juan Gabriel Kostelec, Qinghai Guo  
**Category**: cs.CL  
**Published**: 2025-11-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.00576v1  

Transformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we pre...

---

### 25. [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)

**Authors**: Min Fang, Zhihui Fu, Qibin Zhao, Jun Wang  
**Category**: cs.CL  
**Published**: 2025-11-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.01282v1  

Speculative decoding (SD) has emerged as an effective technique to accelerate large language model (LLM) inference without compromising output quality. However, the achievable speedup largely depends on the effectiveness of the drafting model. While model-based methods like EAGLE-2 are accurate but ...

---

### 26. [FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs](https://arxiv.org/abs/2511.00807)

**Authors**: Xuan He, Zequan Fang, Jinzhao Lian, Danny H. K. Tsang, Baosen Zhang, Yize Chen  
**Category**: cs.DC  
**Published**: 2025-11-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.00807v1  

The ever-increasing computation and energy demand for LLM and AI agents call for holistic and efficient optimization of LLM serving systems. In practice, heterogeneous GPU clusters can be deployed in a geographically distributed manner, while LLM load also observes diversity in terms of both query t...

---

### 27. [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)

**Authors**: Elvin Hajizada, Danielle Rager, Timothy Shea, Leobardo Campos-Macias, Andreas Wild, Eyke H\"ullermeier, Yulia Sandamirskaya, Mike Davies  
**Category**: cs.DC  
**Published**: 2025-11-04  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.01553v1  

AI systems on edge devices face a critical challenge in open-world environments: adapting when data distributions shift and novel classes emerge. While offline training dominates current paradigms, online continual learning (OCL)--where models learn incrementally from non-stationary streams without ...

---

### 28. [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)

**Authors**: Yuchen Zhang, Hanyue Du, Chun Cao, Jingwei Xu  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.00101v1  

Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning ...

---

### 29. [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)

**Authors**: Qiang Li, Jin Niu, Lina Yu  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.00549v1  

Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to captu...

---

### 30. [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)

**Authors**: Song Gao, Shusen Jing, Shuai Zhang, Yue Wang, Xiangwei Zhou, Songyang Zhang  
**Category**: cs.LG  
**Published**: 2025-11-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2511.01743v1  

Recent advancements in large artificial intelligence models (LAMs) are driving significant innovations in mobile edge computing within next-generation wireless networks. However, the substantial demands for computational resources and large-scale training data required to train LAMs conflict with th...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
