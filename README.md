# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-02 12:49:54 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.00207)

**Authors**: Yunqi Gao, Bing Hu, Mahdi Boloursaz Mashhadi, A-Long Jin, Yanfeng Zhang, Pei Xiao, Rahim Tafazolli, Merouane Debbah  
**Category**: cs.DC  
**Published**: 2025-10-02  
**Score**: 16.0

arXiv:2510.00207v1 Announce Type: new 
Abstract: The parameter size of modern large language models (LLMs) can be scaled up via the sparsely-activated Mixture-of-Experts (MoE) technique to avoid excessive increase of the computational costs. To further improve training efficiency, pipelining computa...

---

### 2. [An Efficient, Reliable and Observable Collective Communication Library in Large-scale GPU Training Clusters](https://arxiv.org/abs/2510.00991)

**Authors**: Ziteng Chen (Infrawaves), Xiaohe Hu (Infrawaves), Menghao Zhang (Beihang University), Yanmin Jia (Infrawaves), Yan Zhang (Infrawaves), Mingjun Zhang (Infrawaves), Da Liu (Infrawaves), Fangzheng Jiao (Beihang University), Jun Chen (Infrawaves), He Liu (Infrawaves), Aohan Zeng (Tsinghua University), Shuaixing Duan (Zhipu AI), Ruya Gu (Infrawaves), Yang Jing (Infrawaves), Bowen Han (China Unicom Research Institute), Jiahao Cao (Tsinghua University), Wei Chen (Infrawaves), Wenqi Xie (Infrawaves), Jinlong Hou (Shanghai Innovation Institute), Yuan Cheng (Shanghai Innovation Institute), Bohua Xu (China Unicom Research Institute), Mingwei Xu (Tsinghua University), Chunming Hu (Beihang University)  
**Category**: cs.DC  
**Published**: 2025-10-02  
**Score**: 12.0

arXiv:2510.00991v1 Announce Type: new 
Abstract: Large-scale LLM training requires collective communication libraries to exchange data among distributed GPUs. As a company dedicated to building and operating large-scale GPU training clusters, we encounter several challenges when using NCCL in produc...

---

### 3. [Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.00294)

**Authors**: Shutong Wu, Jiawei Zhang  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 11.0

arXiv:2510.00294v1 Announce Type: cross 
Abstract: Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context...

---

### 4. [Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training](https://arxiv.org/abs/2510.00183)

**Authors**: Ween Yang, Jason Liu, Suli Wang, Xinyuan Song, Lynn Ai, Eric Yang, Tianyu Shi  
**Category**: cs.DC  
**Published**: 2025-10-02  
**Score**: 10.5

arXiv:2510.00183v1 Announce Type: new 
Abstract: The rapid expansion of distributed Artificial Intelligence (AI) workloads beyond centralized data centers creates a demand for new communication substrates. These substrates must operate reliably in heterogeneous and permissionless environments, where...

---

### 5. [ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training](https://arxiv.org/abs/2510.00606)

**Authors**: Xueze Kang, Guangyu Xiang, Yuxin Wang, Hao Zhang, Yuchu Fang, Yuhang Zhou, Zhenheng Tang, Youhui Lv, Eliran Maman, Mark Wasserman, Alon Zameret, Zhipeng Bian, Shushu Chen, Zhiyou Yu, Jin Wang, Xiaoyu Wu, Yang Zheng, Chen Tian, Xiaowen Chu  
**Category**: cs.DC  
**Published**: 2025-10-02  
**Score**: 10.5

arXiv:2510.00606v1 Announce Type: new 
Abstract: Large-scale LLM pretraining today spans $10^{5}$--$10^{6}$ accelerators, making failures commonplace and elasticity no longer optional. We posit that an elastic-native training system must simultaneously ensure (i) Parameter Consistency, (ii) low Mean...

---

### 6. [AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size](https://arxiv.org/abs/2509.26432)

**Authors**: Guanxi Lu, Hao Mark Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 10.0

arXiv:2509.26432v2 Announce Type: replace-cross 
Abstract: Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive...

---

### 7. [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)

**Authors**: Yixuan Even Xu, Yash Savani, Fei Fang, J. Zico Kolter  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 9.5

arXiv:2504.13818v3 Announce Type: replace-cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as the leading approach for enhancing reasoning capabilities in large language models. However, it faces a fundamental compute and memory asymmetry: rollout generation is emba...

---

### 8. [Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in LLM Reasoning](https://arxiv.org/abs/2510.00819)

**Authors**: Luckeciano C. Melo, Alessandro Abate, Yarin Gal  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 9.0

arXiv:2510.00819v1 Announce Type: cross 
Abstract: Reinforcement Learning, particularly through policy gradient methods, has played a central role in enabling reasoning capabilities of Large Language Models. However, the optimization stability of policy gradients in this setting remains understudied...

---

### 9. [Scaling Linear Attention with Sparse State Expansion](https://arxiv.org/abs/2507.16577)

**Authors**: Yuqi Pan, Yongqi An, Zheng Li, Yuhong Chou, Ruijie Zhu, Xiaohui Wang, Mingxuan Wang, Jinqiao Wang, Guoqi Li  
**Category**: cs.CL  
**Published**: 2025-10-02  
**Score**: 9.0

arXiv:2507.16577v2 Announce Type: replace-cross 
Abstract: The Transformer architecture, despite its widespread success, struggles with long-context scenarios due to quadratic computation and linear memory growth. While various linear attention variants mitigate these efficiency constraints by compr...

---

### 10. [Large Language Models Inference Engines based on Spiking Neural Networks](https://arxiv.org/abs/2510.00133)

**Authors**: Adarsha Balaji, Sandeep Madireddy  
**Category**: cs.LG  
**Published**: 2025-10-02  
**Score**: 9.0

arXiv:2510.00133v1 Announce Type: new 
Abstract: Foundational models based on the transformer architecture are currently the state-of-the-art in general language modeling, as well as in scientific areas such as material science and climate. However, training and deploying these models is computation...

---

### 11. [Gated X-TFC: Soft Domain Decomposition for Forward and Inverse Problems in Sharp-Gradient PDEs](https://arxiv.org/abs/2510.01039)

**Authors**: Vikas Dwivedi, Enrico Schiassi, Monica Sigovan, Bruno Sixou  
**Category**: cs.LG  
**Published**: 2025-10-02  
**Score**: 9.0

arXiv:2510.01039v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) and related methods struggle to resolve sharp gradients in singularly perturbed boundary value problems without resorting to some form of domain decomposition, which often introduce complex interface penalties....

---

### 12. [ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools](https://arxiv.org/abs/2510.00023)

**Authors**: Quy Minh Le, Minh Sao Khue Luu, Khanh-Tung Tran, Duc-Hai Nguyen, Hoang-Quoc-Viet Pham, Quan Le, Hoang Thanh Lam, Hoang D. Nguyen  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 8.5

arXiv:2510.00023v1 Announce Type: new 
Abstract: Effective tool use is essential for agentic AI, yet training agents to utilize tools remains challenging due to manually designed rewards, limited training data, and poor multi-tool selection, resulting in slow adaptation, wasted computational resourc...

---

### 13. [Semantic-Driven AI Agent Communications: Challenges and Solutions](https://arxiv.org/abs/2510.00381)

**Authors**: Kaiwen Yu, Mengying Sun, Zhijin Qin, Xiaodong Xu, Ping Yang, Yue Xiao, Gang Wu  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 8.5

arXiv:2510.00381v1 Announce Type: new 
Abstract: With the rapid growth of intelligent services, communication targets are shifting from humans to artificial intelligent (AI) agents, which require new paradigms to enable real-time perception, decision-making, and collaboration. Semantic communication...

---

### 14. [DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search](https://arxiv.org/abs/2509.25454)

**Authors**: Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin Choi  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 8.5

arXiv:2509.25454v2 Announce Type: replace 
Abstract: Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in perf...

---

### 15. [Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution](https://arxiv.org/abs/2510.00636)

**Authors**: Alessio Devoto, Maximilian Jeblick, Simon J\'egou  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 7.5

arXiv:2510.00636v1 Announce Type: new 
Abstract: Memory consumption of the Key-Value (KV) cache represents a major bottleneck for efficient large language model inference. While attention-score-based KV cache pruning shows promise, it faces critical practical limitations: attention scores from futur...

---

### 16. [TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks](https://arxiv.org/abs/2510.00225)

**Authors**: Yue Meng, Fei Chen, Chuchu Fan  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 7.5

arXiv:2510.00225v1 Announce Type: cross 
Abstract: Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and in...

---

### 17. [Latent Collective Preference Optimization: A General Framework for Robust LLM Alignment](https://arxiv.org/abs/2509.24159)

**Authors**: Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, Chao Yu  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 7.5

arXiv:2509.24159v2 Announce Type: replace 
Abstract: Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone technology for aligning Large Language Models (LLMs) with human values. However, these methods are all underpinned by a...

---

### 18. [GRID: Scalable Task-Agnostic Prompt-Based Continual Learning for Language Models](https://arxiv.org/abs/2507.14725)

**Authors**: Anushka Tiwari, Sayantan Pal, Rohini K. Srihari, Kaiyi Ji  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 7.5

arXiv:2507.14725v3 Announce Type: replace-cross 
Abstract: Prompt-based continual learning (CL) provides a parameter-efficient approach for adapting large language models (LLMs) across task sequences. However, most existing methods rely on task-aware inference and maintain a growing set of task-spec...

---

### 19. [Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning](https://arxiv.org/abs/2509.26383)

**Authors**: Jinyeop Song, Song Wang, Julian Shun, Yada Zhu  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 7.5

arXiv:2509.26383v2 Announce Type: replace-cross 
Abstract: Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multip...

---

### 20. [Prompt Curriculum Learning for Efficient LLM Post-Training](https://arxiv.org/abs/2510.01135)

**Authors**: Zhaolin Gao, Joongwon Kim, Wen Sun, Thorsten Joachims, Sid Wang, Richard Yuanzhe Pang, Liang Tan  
**Category**: cs.CL  
**Published**: 2025-10-02  
**Score**: 7.5

arXiv:2510.01135v1 Announce Type: cross 
Abstract: We introduce Prompt Curriculum Learning (PCL), a lightweight reinforcement learning (RL) algorithm that selects intermediate-difficulty prompts using a learned value model to post-train language models. Since post-training LLMs via RL remains sensit...

---

### 21. [Composer: A Search Framework for Hybrid Neural Architecture Design](https://arxiv.org/abs/2510.00379)

**Authors**: Bilge Acun, Prasoon Sinha, Newsha Ardalani, Sangmin Bae, Alicia Golden, Chien-Yu Lin, Meghana Madhyastha, Fei Sun, Neeraja J. Yadwadkar, Carole-Jean Wu  
**Category**: cs.LG  
**Published**: 2025-10-02  
**Score**: 7.5

arXiv:2510.00379v1 Announce Type: new 
Abstract: Hybrid model architectures that combine computational primitives (e.g., Attention, MLP) in different ratios have shown promising performance beyond Transformers. Some studies have shown that different interleavings of primitives can affect model quali...

---

### 22. [LoRAFusion: Efficient LoRA Fine-Tuning for LLMs](https://arxiv.org/abs/2510.00206)

**Authors**: Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 7.0

arXiv:2510.00206v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly reduces GPU memory usage while maintaining competitive fine-tuned model quality on downstream tasks...

---

### 23. [CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs](https://arxiv.org/abs/2510.01037)

**Authors**: Yongcheng Zeng, Zexu Sun, Bokai Ji, Erxue Min, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Haifeng Zhang, Xu Chen, Jun Wang  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 7.0

arXiv:2510.01037v1 Announce Type: cross 
Abstract: Curriculum learning plays a crucial role in enhancing the training efficiency of large language models (LLMs) on reasoning tasks. However, existing methods often fail to adequately account for variations in prompt difficulty or rely on simplistic fi...

---

### 24. [Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards](https://arxiv.org/abs/2510.01167)

**Authors**: Yiran Shen, Yu Xia, Jonathan Chang, Prithviraj Ammanabrolu  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 7.0

arXiv:2510.01167v1 Announce Type: cross 
Abstract: Aligning large language models to human preferences is inherently multidimensional, yet most pipelines collapse heterogeneous signals into a single optimizeable objective. We seek to answer what it would take to simultaneously align a model across v...

---

### 25. [A Predictive and Synergistic Two-Layer Scheduling Framework for LLM Serving](https://arxiv.org/abs/2509.23384)

**Authors**: Yue Zhang, Yuansheng Chen, Xuan Mo, Alex Xi, Jialun Li, WeiGang Wu  
**Category**: cs.DC  
**Published**: 2025-10-02  
**Score**: 7.0

arXiv:2509.23384v3 Announce Type: replace 
Abstract: LLM inference serving typically scales out with a two-tier architecture: a cluster router distributes requests to multiple inference engines, each of which then in turn performs its own internal scheduling. However, this commonly used paradigm suf...

---

### 26. [Randomized Matrix Sketching for Neural Network Training and Gradient Monitoring](https://arxiv.org/abs/2510.00442)

**Authors**: Harbir Antil, Deepanshu Verma  
**Category**: cs.LG  
**Published**: 2025-10-02  
**Score**: 7.0

arXiv:2510.00442v1 Announce Type: new 
Abstract: Neural network training relies on gradient computation through backpropagation, yet memory requirements for storing layer activations present significant scalability challenges. We present the first adaptation of control-theoretic matrix sketching to ...

---

### 27. [Guided Speculative Inference for Efficient Test-Time Alignment of LLMs](https://arxiv.org/abs/2506.04118)

**Authors**: Jonathan Geuter, Youssef Mroueh, David Alvarez-Melis  
**Category**: cs.LG  
**Published**: 2025-10-02  
**Score**: 7.0

arXiv:2506.04118v2 Announce Type: replace 
Abstract: We propose Guided Speculative Inference (GSI), a novel algorithm for efficient reward-guided decoding in large language models. GSI combines soft best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative samples from a small auxil...

---

### 28. [The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm](https://arxiv.org/abs/2507.18553)

**Authors**: Jiale Chen, Yalda Shabanzadeh, Elvir Crn\v{c}evi\'c, Torsten Hoefler, Dan Alistarh  
**Category**: cs.LG  
**Published**: 2025-10-02  
**Score**: 7.0

arXiv:2507.18553v2 Announce Type: replace 
Abstract: Quantizing the weights of large language models (LLMs) from 16-bit to lower bitwidth is the de facto approach to deploy massive transformers onto more affordable accelerators. While GPTQ emerged as one of the standard methods for one-shot post-tra...

---

### 29. [Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling](https://arxiv.org/abs/2510.00028)

**Authors**: Ye Qiao, Haocheng Xu, Xiaofan Zhang, Sitao Huang  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 6.5

arXiv:2510.00028v1 Announce Type: cross 
Abstract: Extending the context window support of large language models (LLMs) is crucial for tasks with long-distance dependencies. RoPE-based interpolation and extrapolation methods, such as linear scaling and frequency-aware schemes, enable longer input le...

---

### 30. [HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling](https://arxiv.org/abs/2510.00054)

**Authors**: Xianjie Liu, Yiman Hu, Yixiong Zou, Liang Wu, Jian Xu, Bo Zheng  
**Category**: cs.AI  
**Published**: 2025-10-02  
**Score**: 6.5

arXiv:2510.00054v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding tasks. However, their performance on high-resolution images remains suboptimal. While existing approaches often attribute this limitation to perceptual co...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
