# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-10-31 12:53:21 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference](https://arxiv.org/abs/2509.24257)

**Authors**: Ke Wang, Zishuo Zhao, Xinyuan Song, Bill Shi, Libin Xia, Chris Tong, Lynn Ai, Felix Qu, Eric Yang  
**Category**: cs.LG  
**Published**: 2025-10-31  
**Score**: 10.5  
**Type**: replace-cross  
**ArXiv ID**: 2509.24257v2  

Decentralized inference provides a scalable and resilient paradigm for serving large language models (LLMs), enabling distributed resource utilization and reducing reliance on centralized providers. However, in a permissionless environment without trusted nodes, ensuring the correctness of model out...

---

### 2. [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)

**Authors**: Dinghong Song (University of California, Merced, USA), Yuan Feng (University of California, Merced, USA), Yiwei Wang (University of California, Merced, USA), Shangye Chen (University of California, Merced, USA), Cyril Guyot (Western Digital Research, USA), Filip Blagojevic (Western Digital Research, USA), Hyeran Jeon (University of California, Merced, USA), Pengfei Su (University of California, Merced, USA), Dong Li (University of California, Merced, USA)  
**Category**: cs.CL  
**Published**: 2025-10-31  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2510.25979v1  

Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model en...

---

### 3. [STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments](https://arxiv.org/abs/2510.26148)

**Authors**: Kexing Liu  
**Category**: cs.LG  
**Published**: 2025-10-31  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2510.26148v1  

Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI) presents a privacy-preserving, contactless sensing approach suitable for smart homes, healthcare monitoring, and mobile IoT systems. However, existing methods often encounter computational inefficiency, high latency, and limi...

---

### 4. [TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference](https://arxiv.org/abs/2505.11329)

**Authors**: Raja Gond, Nipun Kwatra, Ramachandran Ramjee  
**Category**: cs.DC  
**Published**: 2025-10-31  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2505.11329v4  

Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLink. Multiple techniques have been proposed to mitigate these overheads by decomposing computations into finer-grained tasks and overlapping com...

---

### 5. [DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving](https://arxiv.org/abs/2509.01083)

**Authors**: Mingyu Yang, Jae-Young Choi, Kihyo Moon, Minsung Jang, Eunjoo Jeon  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2509.01083v3  

Speculative decoding accelerates large language model inference, but its reliance on a fixed speculation length is suboptimal in large-batch serving environments with diverse requests. This paper explores a new direction for dynamic adaptation by investigating a novel class of post-hoc, diagnostic s...

---

### 6. [Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing](https://arxiv.org/abs/2510.26089)

**Authors**: Fazel Arasteh, Arian Haghparast, Manos Papagelis  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.26089v1  

Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congest...

---

### 7. [Defeating the Training-Inference Mismatch via FP16](https://arxiv.org/abs/2510.26788)

**Authors**: Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.26788v1  

Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show t...

---

### 8. [Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment](https://arxiv.org/abs/2503.15937)

**Authors**: Gaole Dai, Shiqi Jiang, Ting Cao, Yuanchun Li, Yuqing Yang, Rui Tan, Mo Li, Lili Qiu  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 8.0  
**Type**: replace  
**ArXiv ID**: 2503.15937v4  

We propose V-Droid, a mobile GUI task automation agent. Unlike previous mobile agents that utilize Large Language Models (LLMs) as generators to directly generate actions at each step, V-Droid employs LLMs as verifiers to evaluate candidate actions before making final decisions. To realize this nove...

---

### 9. [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)

**Authors**: Yeh Keng Hao, Hsu Tzu Wei, Sun Min  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.16396v3  

With the increasing ubiquity of AR/VR devices, the deployment of deep learning models on edge devices has become a critical challenge. These devices require real-time inference, low power consumption, and minimal latency. Many framework designers face the conundrum of balancing efficiency and perfor...

---

### 10. [Wireless Sensor Networks as Parallel and Distributed Hardware Platform for Artificial Neural Networks](https://arxiv.org/abs/2510.26492)

**Authors**: Gursel Serpen  
**Category**: cs.DC  
**Published**: 2025-10-31  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2510.26492v1  

We are proposing fully parallel and maximally distributed hardware realization of a generic neuro-computing system. More specifically, the proposal relates to the wireless sensor networks technology to serve as a massively parallel and fully distributed hardware platform to implement and realize art...

---

### 11. [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)

**Authors**: Darius Masoum Zadeh-Jousdani, Elvin Hajizada, Eyke H\"ullermeier  
**Category**: cs.LG  
**Published**: 2025-10-31  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2510.25993v1  

Robotic systems operating at the edge require efficient online learning algorithms that can continuously adapt to changing environments while processing streaming sensory data. Traditional backpropagation, while effective, conflicts with biological plausibility principles and may be suboptimal for c...

---

### 12. [Data-Efficient RLVR via Off-Policy Influence Guidance](https://arxiv.org/abs/2510.26491)

**Authors**: Erle Zhu, Dazhi Jiang, Yuan Wang, Xujun Li, Jiale Cheng, Yuxian Gu, Yilin Niu, Aohan Zeng, Jie Tang, Minlie Huang, Hongning Wang  
**Category**: cs.LG  
**Published**: 2025-10-31  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2510.26491v1  

Data selection is a critical aspect of Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of large language models (LLMs). Current data selection methods are largely heuristic-based, lacking theoretical guarantees and generalizability. This work proposes a...

---

### 13. [ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion](https://arxiv.org/abs/2510.25818)

**Authors**: Sungho Koh, SeungJu Cha, Hyunwoo Oh, Kwanyoung Lee, Dong-Jin Kim  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.25818v1  

Text-to-image diffusion models often exhibit degraded performance when generating images beyond their training resolution. Recent training-free methods can mitigate this limitation, but they often require substantial computation or are incompatible with recent Diffusion Transformer models. In this p...

---

### 14. [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)

**Authors**: Kang Chen, Yaoning Wang, Kai Xiong, Zhuoka Feng, Wenhe Sun, Haotian Chen, Yixin Cao  
**Category**: cs.CL  
**Published**: 2025-10-31  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.26277v1  

Large language models (LLMs) commonly boost reasoning via sample-evaluate-ensemble decoders, achieving label free gains without ground truth. However, prevailing strategies score candidates using only external outputs such as token probabilities, entropies, or self evaluations, and these signals can...

---

### 15. [Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model](https://arxiv.org/abs/2510.26622)

**Authors**: Biao Zhang, Yong Cheng, Siamak Shakeri, Xinyi Wang, Min Ma, Orhan Firat  
**Category**: cs.CL  
**Published**: 2025-10-31  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.26622v1  

Recent large language model (LLM) research has undergone an architectural shift from encoder-decoder modeling to nowadays the dominant decoder-only modeling. This rapid transition, however, comes without a rigorous comparative analysis especially \textit{from the scaling perspective}, raising concer...

---

### 16. [An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning](https://arxiv.org/abs/2510.26709)

**Authors**: Chuyan Chen, Chenyang Ma, Zhangxin Li, Yutong He, Yanjie Dong, Kun Yuan  
**Category**: cs.DC  
**Published**: 2025-10-31  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.26709v1  

Communication remains a central bottleneck in large-scale distributed machine learning, and gradient sparsification has emerged as a promising strategy to alleviate this challenge. However, existing gradient compressors face notable limitations: Rand-$K$\ discards structural information and performs...

---

### 17. [Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization](https://arxiv.org/abs/2510.26633)

**Authors**: Colin Doumont, Victor Picheny, Viacheslav Borovitskiy, Henry Moss  
**Category**: cs.LG  
**Published**: 2025-10-31  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2510.26633v1  

Bayesian Optimization (BO) has the potential to solve various combinatorial tasks, ranging from materials science to neural architecture search. However, BO requires specialized kernels to effectively model combinatorial domains. Recent efforts have introduced several combinatorial kernels, but the ...

---

### 18. [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)

**Authors**: Qianli Shen, Daoyuan Chen, Yilun Huang, Zhenqing Ling, Yaliang Li, Bolin Ding, Jingren Zhou  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2510.26374v1  

Reinforcement finetuning (RFT) is a key technique for aligning Large Language Models (LLMs) with human preferences and enhancing reasoning, yet its effectiveness is highly sensitive to which tasks are explored during training. Uniform task sampling is inefficient, wasting computation on tasks that a...

---

### 19. [Efficient Regression-Based Training of Normalizing Flows for Boltzmann Generators](https://arxiv.org/abs/2506.01158)

**Authors**: Danyal Rehman, Oscar Davis, Jiarui Lu, Jian Tang, Michael Bronstein, Yoshua Bengio, Alexander Tong, Avishek Joey Bose  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2506.01158v2  

Simulation-free training frameworks have been at the forefront of the generative modelling revolution in continuous spaces, leading to large-scale diffusion and flow matching models. However, such modern generative models suffer from expensive inference, inhibiting their use in numerous scientific a...

---

### 20. [MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding](https://arxiv.org/abs/2510.25327)

**Authors**: Runxi Huang, Mingxuan Yu, Mingyu Tsoi, Xiaomin Ouyang  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.25327v2  

Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex int...

---

### 21. [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)

**Authors**: Tomas Hrycej, Bernhard Bermeitinger, Massimo Pavone, G\"otz-Henrik Wiegand, Siegfried Handschuh  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.25366v2  

The key task of machine learning is to minimize the loss function that measures the model fit to the training data. The numerical methods to do this efficiently depend on the properties of the loss function. The most decisive among these properties is the convexity or non-convexity of the loss funct...

---

### 22. [How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices](https://arxiv.org/abs/2510.18480)

**Authors**: Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao  
**Category**: cs.CL  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2510.18480v2  

Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limi...

---

### 23. [Hysteresis Activation Function for Efficient Inference](https://arxiv.org/abs/2411.10573)

**Authors**: Moshe Kimhi, Idan Kashani, Avi Mendelson, Chaim Baskin  
**Category**: cs.CL  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2411.10573v3  

The widely used ReLU is favored for its hardware efficiency, {as the implementation at inference is a one bit sign case,} yet suffers from issues such as the ``dying ReLU'' problem, where during training, neurons fail to activate and constantly remain at zero, as highlighted by Lu et al. Traditional...

---

### 24. [SHARE: Optimizing Secure Hub Allocation and Routing Efficiency in Payment Channel Networks](https://arxiv.org/abs/2501.04236)

**Authors**: Lingxiao Yang, Xuewen Dong, Wei Wang, Yong Yu, Sheng Gao, Qiang Qu, Yulong Shen  
**Category**: cs.DC  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2501.04236v2  

Payment channel hub (PCH), by leveraging a powerful hub to reliably provide off-chain payment services, offers an effective enhancement to payment channel networks (PCNs). However, existing approaches typically rely on a single hub to relay transactions and provide relationship anonymity between par...

---

### 25. [MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning](https://arxiv.org/abs/2505.23254)

**Authors**: Yong-Cheng Liaw, Shuo-Han Chen  
**Category**: cs.DC  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2505.23254v3  

Owing to the huge success of generative artificial intelligence (AI), large language models (LLMs) have emerged as a core subclass, underpinning applications such as question answering, text generation, and code completion. While fine-tuning these models on domain-specific data can yield significant...

---

### 26. [A geometric framework for momentum-based optimizers for low-rank training](https://arxiv.org/abs/2506.17475)

**Authors**: Steffen Schotth\"ofer, Timon Klein, Jonas Kusch  
**Category**: cs.LG  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2506.17475v3  

Low-rank pre-training and fine-tuning have recently emerged as promising techniques for reducing the computational and storage costs of large neural networks. Training low-rank parameterizations typically relies on conventional optimizers such as heavy ball momentum methods or Adam. In this work, we...

---

### 27. [Distributed optimization: designed for federated learning](https://arxiv.org/abs/2508.08606)

**Authors**: Wenyou Guo, Ting Qu, Chunrong Pan, George Q. Huang  
**Category**: cs.LG  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2508.08606v3  

Federated learning (FL), as a distributed collaborative machine learning (ML) framework under privacy-preserving constraints, has garnered increasing research attention in cross-organizational data collaboration scenarios. This paper proposes a class of distributed optimization algorithms based on t...

---

### 28. [Unified Error Correction Code Transformer with Low Complexity](https://arxiv.org/abs/2410.03364)

**Authors**: Yongli Yan, Jieao Zhu, Tianyue Zheng, Zhuo Xu, Chao Jiang, Linglong Dai  
**Category**: cs.LG  
**Published**: 2025-10-31  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2410.03364v4  

Channel coding is vital for reliable sixth-generation (6G) data transmission, employing diverse error correction codes for various application scenarios. Traditional decoders require dedicated hardware for each code, leading to high hardware costs. Recently, artificial intelligence (AI)-driven appro...

---

### 29. [One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning](https://arxiv.org/abs/2510.26167)

**Authors**: Renhao Li, Jianhong Tu, Yang Su, Hamid Alinejad-Rokny, Derek F. Wong, Junyang Lin, Min Yang  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2510.26167v1  

Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightwe...

---

### 30. [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)

**Authors**: Jiazhen Yuan, Wei Zhao, Zhengbiao Bai  
**Category**: cs.AI  
**Published**: 2025-10-31  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2510.26270v1  

Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environmen...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
