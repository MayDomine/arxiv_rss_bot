# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-04 06:35:05 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs](https://arxiv.org/abs/2602.03495)

**Authors**: Zeyu Zhu, Gang Li, Peisong Wang, Zitao Mo, Minnan Pei, Zhuoran Song, Xiaoyao Liang, Jian Cheng  
**Category**: cs.DC  
**Published**: 2026-02-04  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2602.03495v1  

#### Abstract
Mixture of Experts (MoE) architectures significantly enhance the capacity of LLMs without proportional increases in computation, but at the cost of a vast parameter size. Offloading MoE expert parameters to host memory and leveraging both CPU and GPU computation has recently emerged as a promising d...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜

Mixture of Experts (**MoE**) æ¶æ„æ˜¾è‘—æå‡äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®¹é‡ï¼Œä½†ç”±äºå…¶å‚æ•°é‡å·¨å¤§ï¼Œåœ¨èµ„æºå—é™çš„æœ¬åœ°PCï¼ˆå¦‚é…å¤‡æ¶ˆè´¹çº§GPUçš„è®¾å¤‡ï¼‰ä¸Šéƒ¨ç½²é¢ä¸´ä¸¥é‡æŒ‘æˆ˜ã€‚ç°æœ‰ **MoE offloading** æ¡†æ¶å­˜åœ¨ä»¥ä¸‹ä¸‰å¤§æ ¹æœ¬æ€§æ•ˆç‡é—®é¢˜ï¼š

1. **é™æ€ä¸“å®¶åˆ†é…å¯¼è‡´ CPU-GPU è´Ÿè½½å¤±è¡¡**  
   ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ Fiddlerã€HybriMoEï¼‰é‡‡ç”¨åŸºäºé¢„è®¾é˜ˆå€¼çš„é™æ€åˆ†é…ç­–ç•¥ï¼Œå°†é«˜è´Ÿè½½ä¸“å®¶å›ºå®šåœ¨ GPU ä¸Šï¼Œä½è´Ÿè½½åœ¨ CPU ä¸Šï¼Œå¿½ç•¥äº†è¾“å…¥åŠ¨æ€å˜åŒ–å¸¦æ¥çš„è´Ÿè½½æ³¢åŠ¨ï¼Œé€ æˆä¸¥é‡çš„èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ã€‚

2. **é¢„å–ï¼ˆprefetchingï¼‰å‡†ç¡®æ€§å·®**  
   ç°æœ‰é¢„å–æœºåˆ¶æœªèƒ½æœ‰æ•ˆé¢„æµ‹â€œé«˜è´Ÿè½½ä¸“å®¶â€ï¼Œå¯¼è‡´å¤§é‡æ— æ•ˆçš„ PCIe æ•°æ®ä¼ è¾“ï¼Œåè€Œå¢åŠ å»¶è¿Ÿã€‚

3. **GPU ç¼“å­˜ç­–ç•¥å¿½è§†è´Ÿè½½åŠ¨æ€æ€§**  
   ç¼“å­˜æ›¿æ¢ç­–ç•¥ï¼ˆå¦‚ LRU æˆ–åŸºäºæ¿€æ´»åˆ†æ•°ï¼‰æœªè€ƒè™‘ä¸“å®¶è´Ÿè½½çš„æ—¶é—´ç›¸å…³æ€§ï¼Œç¼“å­˜å‘½ä¸­ç‡ä½ï¼Œæ— æ³•æœ‰æ•ˆå‡å°‘é€šä¿¡å¼€é”€ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä½œè€…æå‡º **DALI** â€”â€”ä¸€ä¸ª**é¢å‘æœ¬åœ°å¼‚æ„ç¡¬ä»¶ã€æ„ŸçŸ¥å·¥ä½œè´Ÿè½½åŠ¨æ€æ€§çš„ MoE æ¨ç†å¸è½½æ¡†æ¶**ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

#### ï¼ˆ1ï¼‰**Greedy Assignment Strategyï¼ˆè´ªå¿ƒåˆ†é…ç­–ç•¥ï¼‰**
- å°†ä¸“å®¶åˆ†é…å»ºæ¨¡ä¸º **0-1 æ•´æ•°ä¼˜åŒ–é—®é¢˜**ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ– CPU å’Œ GPU æ‰§è¡Œæ—¶é—´çš„æœ€å¤§å€¼ï¼ˆ`min max(T_cpu, T_gpu)`ï¼‰ï¼Œä»¥å®ç°è´Ÿè½½å‡è¡¡ã€‚
- è®¾è®¡é«˜æ•ˆçš„ **è´ªå¿ƒç®—æ³•** è¿‘ä¼¼æ±‚è§£è¯¥é—®é¢˜ï¼Œåœ¨æä½å¼€é”€ä¸‹è·å¾—æ¥è¿‘æœ€ä¼˜çš„åˆ†é…æ–¹æ¡ˆã€‚
- åŠ¨æ€å†³å®šæ¯ä¸ª token çš„ä¸“å®¶åœ¨ CPU/GPU ä¸Šçš„æ‰§è¡Œä½ç½®ï¼Œå……åˆ†åˆ©ç”¨å¼‚æ„è®¡ç®—èµ„æºã€‚

#### ï¼ˆ2ï¼‰**Residual-Based Prefetchingï¼ˆåŸºäºæ®‹å·®çš„é¢„å–ï¼‰**
- åˆ©ç”¨ Transformer å±‚é—´ **residual connection** ä¸­çš„éšè—çŠ¶æ€å·®å¼‚ï¼ˆå³ `res_vec`ï¼‰ï¼Œå¯¹ä¸‹ä¸€å±‚çš„é—¨æ§å‡½æ•°è¾“å…¥è¿›è¡Œæ ¡æ­£ã€‚
- é€šè¿‡æ ¡æ­£åçš„ç‰¹å¾æ›´å‡†ç¡®åœ°é¢„æµ‹ä¸‹ä¸€å±‚ä¸­å°†è¢«é«˜é¢‘æ¿€æ´»çš„**é«˜è´Ÿè½½ä¸“å®¶**ï¼Œä»è€Œæå‡é¢„å–ç²¾åº¦ã€‚
- æ— éœ€å¾®è°ƒï¼Œ`res_vec` å¯é€šè¿‡å°è§„æ¨¡æ ¡å‡†æ•°æ®é›†ç¦»çº¿æ„å»ºå¹¶å¤ç”¨ã€‚

#### ï¼ˆ3ï¼‰**Workload-Aware Cache Replacementï¼ˆè´Ÿè½½æ„ŸçŸ¥ç¼“å­˜æ›¿æ¢ï¼‰**
- è§‚å¯Ÿåˆ°é«˜è´Ÿè½½ä¸“å®¶åœ¨ç›¸é‚» token ä¹‹é—´å…·æœ‰å¼º**æ—¶é—´å±€éƒ¨æ€§**ï¼ˆtemporal correlationï¼‰ã€‚
- æå‡ºæ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œç»Ÿè®¡çª—å£å†…å„ä¸“å®¶çš„å·¥ä½œè´Ÿè½½å¾—åˆ†ï¼Œå¹¶æ®æ­¤æ›´æ–° GPU ç¼“å­˜ï¼šä¼˜å…ˆä¿ç•™é«˜è´Ÿè½½å†å²å¾—åˆ†çš„ä¸“å®¶ã€‚
- æ˜¾è‘—æé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼Œå‡å°‘ä¸å¿…è¦çš„ PCIe ä¼ è¾“ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ HybriMoEï¼‰ | DALI |
|------|------------------------|------|
| åˆ†é…ç­–ç•¥ | é™æ€ã€åŸºäºé˜ˆå€¼ | **åŠ¨æ€ã€è´Ÿè½½æ„ŸçŸ¥ã€è¿‘ä¼¼æœ€ä¼˜** |
| é¢„å–æœºåˆ¶ | åŸºäºåŸå§‹è¾“å…¥æˆ–ç»Ÿè®¡ç‰¹å¾ï¼Œç²¾åº¦ä½ | **åˆ©ç”¨æ®‹å·®ä¿¡æ¯æ ¡æ­£è¾“å…¥ï¼Œç²¾åº¦æ˜¾è‘—æå‡** |
| ç¼“å­˜ç­–ç•¥ | LRU æˆ–æ¿€æ´»åˆ†æ•°é©±åŠ¨ | **åŸºäºå†å²è´Ÿè½½åŠ¨æ€æ›´æ–°ï¼Œå‘½ä¸­ç‡æ›´é«˜** |
| ç³»ç»Ÿèµ„æºåˆ©ç”¨ç‡ | å­˜åœ¨ä¸¥é‡ CPU-GPU è´Ÿè½½å¤±è¡¡ | **å®ç°è‰¯å¥½è´Ÿè½½å¹³è¡¡ï¼Œç¡¬ä»¶åˆ©ç”¨ç‡æœ€å¤§åŒ–** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

- **C4**ï¼šç”¨äºæ¨ç†é€Ÿåº¦åŸºå‡†æµ‹è¯•ï¼ˆspeed benchmarkingï¼‰ã€‚
- **Wikitext**ï¼šç”¨äºæ„å»º **residual vector** çš„æ ¡å‡†æ•°æ®é›†ï¼ˆé‡‡æ · 1K åºåˆ—ï¼‰ã€‚
- **EleutherAI LM Evaluation Harness**ï¼šç”¨äºä¸‹æ¸¸ä»»åŠ¡æ³›åŒ–èƒ½åŠ›æµ‹è¯•ï¼ˆARC-e, ARC-c, OBQA, RTEï¼‰ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®

- **ç¡¬ä»¶å¹³å°**ï¼š
  - CPU: AMD EPYC 7532 (64 cores)
  - GPU: NVIDIA RTX 3090 (24GB HBM)
  - å†…å­˜: 256GB DDR4 DRAM
  - æ¥å£: PCIe 4.0 Ã—16 (~32 GB/s)

- **è¯„ä¼°é˜¶æ®µ**ï¼š
  - **Prefill Phase**ï¼šå¤„ç†è¾“å…¥æç¤ºï¼ˆpromptï¼‰
  - **Decoding Phase**ï¼šè‡ªå›å½’ç”Ÿæˆè¾“å‡º

- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Tokens per second (tokens/s)**ï¼šä¸»æ€§èƒ½æŒ‡æ ‡
  - **Speedup**ï¼šç›¸å¯¹äºåŸºçº¿çš„åŠ é€Ÿæ¯”
  - **Cache Hit Rate**
  - **Prefetch Accuracy**

- **æ§åˆ¶å˜é‡**ï¼š
  - æ‰€æœ‰æ¡†æ¶ä½¿ç”¨ç›¸åŒæ•°é‡çš„ CPU æ ¸å¿ƒï¼ˆ16ï¼‰å’Œçº¿ç¨‹ï¼ˆ32ï¼‰
  - GPU å†…å­˜ä½¿ç”¨ä¿æŒå¯æ¯”æ€§ï¼ˆé€šè¿‡è°ƒæ•´ç¼“å­˜ä¸“å®¶æ•°æˆ–é©»ç•™å±‚æ•°ï¼‰

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿ | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **llama.cpp**, **KTransformers** | Layer-wise Hybrid | å°†å‰è‹¥å¹²å±‚æ”¾ CPUï¼Œå…¶ä½™æ”¾ GPUï¼Œä¸æ”¯æŒä¸“å®¶ç²’åº¦è°ƒåº¦ |
| **MoE-Lightning** | Offline Search | åŸºäºæ€§èƒ½åˆ†ææ¨¡å‹ç¦»çº¿æœç´¢éƒ¨ç½²ç­–ç•¥ï¼Œçµæ´»æ€§å·® |
| **HybriMoE** | Expert-wise Hybrid | å½“å‰æœ€å…ˆè¿›çš„ CPU-GPU ååŒæ¡†æ¶ï¼Œæ”¯æŒä¸“å®¶é¢„å–ä¸ç¼“å­˜ |
| **Fiddler** | Expert-wise Hybrid | ç±»ä¼¼ HybriMoEï¼Œä½†æ€§èƒ½è¾ƒå·®ï¼Œæ–‡ä¸­ä»…ä½œå‚è€ƒ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆå¹³å‡åŠ é€Ÿæ¯”ï¼‰

| æ–¹æ³• | Prefill é˜¶æ®µåŠ é€Ÿæ¯” | Decoding é˜¶æ®µåŠ é€Ÿæ¯” |
|------|--------------------|---------------------|
| vs. **llama.cpp** | **7.62Ã—** | **3.97Ã—** |
| vs. **KTransformers** | **3.80Ã—** | **2.16Ã—** |
| vs. **MoE-Lightning** | **2.45Ã—** | **1.48Ã—** |
| vs. **HybriMoE** | **2.00Ã—** | **1.32Ã—** |

> åœ¨å¤šç§ MoE æ¨¡å‹ï¼ˆDeepSeek-V2, Qwen-3, Mixtral-8x7Bï¼‰å’Œä¸åŒ batch size ä¸‹å‡å–å¾—ä¸€è‡´é¢†å…ˆã€‚

---

### ğŸ” ä¸åŸºçº¿æ–¹æ³•çš„è¯¦ç»†å¯¹æ¯”

#### ï¼ˆ1ï¼‰**æ¶ˆèå®éªŒï¼šå„æ¨¡å—è´¡çŒ®åˆ†è§£ï¼ˆFig. 19ï¼‰**

ä»¥ Mixtral å’Œ Qwen ä¸ºä¾‹ï¼Œä»â€œNaiveâ€ï¼ˆæ‰€æœ‰ä¸“å®¶åœ¨ CPUï¼‰é€æ­¥æ·»åŠ ä¼˜åŒ–ï¼š

| æ­¥éª¤ | åŠ é€Ÿæ¯” | è¯´æ˜ |
|------|-------|------|
| Naive â†’ +Greedy Assignment | **4.1Ã—** | æœ€å¤§å¢ç›Šæ¥æºï¼Œæ˜¾è‘—æ”¹å–„è´Ÿè½½å‡è¡¡ |
| â†’ +Residual-Based Prefetching | ~4.5Ã— (+9%) | é¢„å–å¸¦æ¥è¾¹é™…æ”¶ç›Šï¼Œå—é™äºé¢å¤–è®¡ç®—å¼€é”€ |
| â†’ +Workload-Aware Cache | **6.2Ã—**ï¼ˆæ€»å¢ç›Š **~38%**ï¼‰ | ç¼“å­˜å‘½ä¸­ç‡æå‡æ˜¾è‘—é™ä½ PCIe æµé‡ |

> è¡¨æ˜ **Greedy Assignment æ˜¯æ ¸å¿ƒé©±åŠ¨åŠ›**ï¼Œè€Œç¼“å­˜ä¼˜åŒ–æ˜¯é‡è¦è¡¥å……ã€‚

#### ï¼ˆ2ï¼‰**é¢„å–å‡†ç¡®ç‡å¯¹æ¯”ï¼ˆTable 2ï¼‰**

åœ¨ Mixtral ä¸Šé¢„æµ‹ Top-2 é«˜è´Ÿè½½ä¸“å®¶æ—¶ï¼š

| æ–¹æ³• | Batch=8 | Batch=16 | Batch=32 | Batch=64 |
|------|---------|----------|----------|----------|
| EdgeMoE (statistical) | 40.3% | 40.1% | 44.7% | 48.5% |
| HybriMoE (feature-based) | 65.2% | 63.7% | 58.9% | 56.4% |
| **DALI (ours)** | **>80%**ï¼ˆè¶‹åŠ¿æ›´é«˜ï¼‰ | â€”â€” | â€”â€” | â€”â€” |

> DALI çš„ **Residual-Based Prefetching** æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

#### ï¼ˆ3ï¼‰**ç¼“å­˜å‘½ä¸­ç‡å¯¹æ¯”ï¼ˆFig. 17bï¼‰**

åœ¨ Mixtral ä¸Šï¼Œå½“ç¼“å­˜æ¯”ä¾‹ä¸º 50% æ—¶ï¼š

- **LRU**: ~30%
- **HybriMoE (score-based)**: ~37%
- **DALI (workload-aware)**: **>60%**

> è´Ÿè½½æ„ŸçŸ¥ç­–ç•¥ä½¿ç¼“å­˜å‘½ä¸­ç‡ç¿»å€ï¼Œç›´æ¥å‡å°‘é€šä¿¡å‹åŠ›ã€‚

#### ï¼ˆ4ï¼‰**è´Ÿè½½å‡è¡¡æ•ˆæœï¼ˆAppendix A.1ï¼‰**

- åœ¨ DeepSeek ä¸Šï¼Œbatch=64 æ—¶ï¼š
  - å¯ç”¨ Greedy Assignment åï¼Œ**GPU æ‰§è¡Œæ—¶é—´å‡å°‘ 20 ç§’**
  - CPU æ—¶é—´ä»…å¢åŠ  6.66 ç§’
  - æ€»ä½“å»¶è¿Ÿå¤§å¹…ä¸‹é™ï¼Œä¸”è´Ÿè½½æ›´åŠ å‡è¡¡

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **MoE æ¨ç†ä¸­çš„è´Ÿè½½é«˜åº¦åŠ¨æ€ï¼Œé™æ€ç­–ç•¥å·²ä¸å¯è¡Œ**  
   è¾“å…¥ä¾èµ–æ€§å¼ºå¯¼è‡´æ¯ token çš„ä¸“å®¶æ¿€æ´»æ¨¡å¼ä¸åŒï¼Œå¿…é¡»é‡‡ç”¨è¿è¡Œæ—¶åŠ¨æ€å†³ç­–ã€‚

2. **åŠ¨æ€è´Ÿè½½æ„ŸçŸ¥åˆ†é…èƒ½æå¤§æå‡å¼‚æ„ç³»ç»Ÿåˆ©ç”¨ç‡**  
   Greedy Assignment å®ç°äº†æ¥è¿‘æœ€ä¼˜çš„è´Ÿè½½å¹³è¡¡ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ CPU/GPU â€œä¸€ä¸ªå¿™ä¸€ä¸ªé—²â€çš„ç“¶é¢ˆã€‚

3. **è·¨å±‚æ®‹å·®ä¿¡æ¯å¯ç”¨äºå¢å¼ºé¢„å–å‡†ç¡®æ€§**  
   åˆ©ç”¨ `residual connection` ä¸­çš„éšè—çŠ¶æ€å·®å€¼ï¼Œå¯æœ‰æ•ˆé€¼è¿‘ä¸‹ä¸€å±‚çœŸå®è¾“å…¥ï¼Œä»è€Œæå‡ä¸“å®¶é¢„æµ‹è´¨é‡ã€‚

4. **é«˜è´Ÿè½½ä¸“å®¶å…·æœ‰å¼ºæ—¶é—´å±€éƒ¨æ€§**  
   ç›¸é‚» token å¾ˆå¯èƒ½æ¿€æ´»ç›¸åŒçš„é«˜è´Ÿè½½ä¸“å®¶ï¼Œè¿™ä¸€è§‚å¯Ÿæ”¯æ’‘äº† Workload-Aware Cache çš„æœ‰æ•ˆæ€§ã€‚

5. **ä¸‰è€…ååŒå¸¦æ¥æ˜¾è‘—ç«¯åˆ°ç«¯åŠ é€Ÿ**  
   DALI åœ¨ prefill å’Œ decoding é˜¶æ®µå‡å®ç° **2â€“7.6Ã— çš„åŠ é€Ÿ**ï¼Œè¿œè¶…å½“å‰ SOTAã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

1. **ä¾èµ– warm-up profiling è·å–æ‰§è¡Œæ—¶é—´è¡¨**  
   éœ€é¢„å…ˆæµ‹é‡ `t_cpu(w)` å’Œ `t_gpu(w)`ï¼Œè™½å¯å¤ç”¨ï¼Œä½†åœ¨æ–°ç¡¬ä»¶ä¸Šéœ€é‡æ–°æ ¡å‡†ã€‚

2. **æ®‹å·®å‘é‡å‡è®¾è¾“å…¥åˆ†å¸ƒç¨³å®š**  
   è‹¥ä¸‹æ¸¸ä»»åŠ¡ä¸ Wikitext å·®å¼‚è¿‡å¤§ï¼Œå¯èƒ½å½±å“é¢„å–æ•ˆæœï¼ˆä½†å®éªŒæ˜¾ç¤ºä»å…·è¾ƒå¼ºæ³›åŒ–æ€§ï¼‰ã€‚

3. **ç›®å‰é’ˆå¯¹å•æœºå•å¡åœºæ™¯è®¾è®¡**  
   å¤š GPU æˆ–åˆ†å¸ƒå¼ç¯å¢ƒå°šæœªéªŒè¯ï¼Œæ‰©å±•æ€§æœ‰å¾…æ¢ç´¢ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **æ‹“å±•è‡³å¤š GPU å’ŒæœåŠ¡å™¨é›†ç¾¤ç¯å¢ƒ**  
   æ¢ç´¢ DALI åœ¨ GH200ã€NVLink é«˜å¸¦å®½ç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚

2. **ç»“åˆé‡åŒ–ä¸ç¨€ç–åŒ–æŠ€æœ¯**  
   è¿›ä¸€æ­¥å‹ç¼©ä¸“å®¶å‚æ•°å¤§å°ï¼Œé™ä½ä¼ è¾“ä¸å­˜å‚¨æˆæœ¬ã€‚

3. **è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜ï¼ˆå¦‚ wsize, usizeï¼‰**  
   æ ¹æ®åºåˆ—é•¿åº¦ã€æ¨¡å‹ç»“æ„è‡ªé€‚åº”é€‰æ‹©æœ€ä½³ç¼“å­˜æ›´æ–°é¢‘ç‡ã€‚

4. **æ¢ç´¢æ›´å¤æ‚çš„è°ƒåº¦ç®—æ³•**  
   å¦‚å¼ºåŒ–å­¦ä¹ æˆ–è½»é‡çº§åœ¨çº¿ä¼˜åŒ–å™¨ï¼Œæ›¿ä»£è´ªå¿ƒç­–ç•¥ä»¥é€¼è¿‘å…¨å±€æœ€ä¼˜ã€‚

---

## æ€»ç»“

**DALI** æ˜¯é¦–ä¸ªå…¨é¢è€ƒè™‘ **MoE è´Ÿè½½åŠ¨æ€æ€§** ä¸ **æœ¬åœ°PCå¼‚æ„ç¡¬ä»¶ç‰¹æ€§** çš„æ¨ç†æ¡†æ¶ã€‚å®ƒé€šè¿‡ **Greedy Assignment**ã€**Residual-Based Prefetching** å’Œ **Workload-Aware Cache** ä¸‰å¤§åˆ›æ–°æœºåˆ¶ï¼Œå®ç°äº† CPU ä¸ GPU çš„é«˜æ•ˆååŒï¼Œåœ¨æ¶ˆè´¹çº§è®¾å¤‡ä¸Šæ˜¾è‘—åŠ é€Ÿ MoE æ¨¡å‹æ¨ç†ï¼Œæ¨åŠ¨äº† LLM çš„æœ¬åœ°åŒ–éƒ¨ç½²è¿›ç¨‹ã€‚å…¶å®éªŒå……åˆ†ã€è®¾è®¡åˆç†ï¼Œä»£è¡¨äº† MoE offloading é¢†åŸŸçš„é‡è¦è¿›å±•ã€‚

</details>

---

### 2. [Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL](https://arxiv.org/abs/2602.03839)

**Authors**: Erfan Miahi, Eugene Belilovsky  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2602.03839v1  

#### Abstract
Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šUnderstanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

åœ¨**å¸¦å®½å—é™çš„åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ ï¼ˆDistributed RLï¼‰**ç³»ç»Ÿä¸­ï¼Œè®­ç»ƒå™¨ï¼ˆtrainerï¼‰éœ€è¦é¢‘ç¹åœ°å°†æ›´æ–°åçš„ç­–ç•¥æ¨¡å‹æƒé‡åŒæ­¥ç»™å¤šä¸ªæ¨ç†èŠ‚ç‚¹ï¼ˆinference workersï¼‰ã€‚å¯¹äºä¸€ä¸ª7Bå‚æ•°çš„æ¨¡å‹ï¼Œåœ¨16-bitç²¾åº¦ä¸‹ï¼Œæ¯æ¬¡åŒæ­¥éœ€ä¼ è¾“ **14 GB** æ•°æ®ã€‚è¿™å¯¼è‡´ï¼š

- åœ¨æ™®é€šç½‘ç»œï¼ˆå¦‚å…¬å…±äº’è”ç½‘ï¼‰ä¸Šé€šä¿¡å¼€é”€å·¨å¤§ï¼›
- é«˜GPUåˆ©ç”¨ç‡è¦æ±‚æé«˜å¸¦å®½ï¼ˆå¦‚20 Gbit/sï¼‰ï¼Œè¿œè¶…å…¸å‹éƒ¨ç½²ç¯å¢ƒèƒ½åŠ›ï¼›
- æˆä¸ºåˆ†å¸ƒå¼RLæ‰©å±•æ€§çš„ä¸»è¦ç“¶é¢ˆã€‚

ä¼ ç»Ÿæ¢¯åº¦å‹ç¼©ï¼ˆgradient compressionï¼‰æ–¹æ³•ä¸é€‚ç”¨äºæ­¤åœºæ™¯ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ç”¨äº**æ¢¯åº¦èšåˆ**è€Œé**æƒé‡å¹¿æ’­**ï¼Œä¸”å¤šä¸ºæœ‰æŸå‹ç¼©ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šPULSE

è®ºæ–‡æå‡ºäº† **PULSE**ï¼ˆ**Patch Updates via Lossless Sparse Encoding**ï¼‰ï¼Œä¸€ç§**æ— æŸç¨€ç–ç¼–ç **çš„æƒé‡åŒæ­¥æ–¹æ³•ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- è§‚å¯Ÿåˆ°ï¼š**RLè®­ç»ƒä¸­çš„æƒé‡æ›´æ–°å…·æœ‰æé«˜çš„ç¨€ç–æ€§**ï¼ˆ>99%å‚æ•°æœªå˜ï¼‰ã€‚
- ä»…ä¼ è¾“**å‘ç”Ÿå˜åŒ–çš„å‚æ•°ç´¢å¼•**ï¼ˆindicesï¼‰å’Œ**æ–°çš„å€¼**ï¼ˆvaluesï¼‰ï¼Œè€Œéæ•´ä¸ªæ¨¡å‹æƒé‡ã€‚
- é‡‡ç”¨ç¨€ç–è¡¨ç¤º + Deltaç¼–ç  + ç±»å‹é™çº§ + é€šç”¨å‹ç¼©ï¼ˆå¦‚zstdï¼‰å®ç°é«˜æ•ˆä¼ è¾“ã€‚

#### åˆ›æ–°ç‚¹ï¼š
1. **é¦–æ¬¡ç³»ç»Ÿæ€§å®è¯åˆ†æRLè®­ç»ƒä¸­step-levelå’Œmulti-stepçš„æƒé‡æ›´æ–°ç¨€ç–æ€§**ï¼Œæ­ç¤ºå…¶é«˜ç¨€ç–æ€§çš„æœºåˆ¶ã€‚
2. **æå‡ºå¹¶å®ç°PULSE**ï¼Œæ˜¯é¦–ä¸ªé’ˆå¯¹**æƒé‡åŒæ­¥**è€Œéæ¢¯åº¦åŒæ­¥çš„**æ— æŸç¨€ç–åŒ–æ–¹æ³•**ã€‚
3. **å­˜å‚¨å®é™…å€¼è€Œéå¢é‡ï¼ˆdeltaï¼‰**ï¼Œé¿å…æµ®ç‚¹ç´¯åŠ æ¼‚ç§»ï¼ˆfloating-point driftï¼‰ï¼Œä¿è¯**bit-identicalé‡å»º**ã€‚
4. æ”¯æŒå¼‚æ­¥ã€å»ä¸­å¿ƒåŒ–éƒ¨ç½²ï¼Œé€‚ç”¨äºçœŸå®ä¸–ç•Œç½‘ç»œç¯å¢ƒã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | æ˜¯å¦é€‚ç”¨æƒé‡åŒæ­¥ | æ˜¯å¦æ— æŸ | å‹ç¼©ç‡ | æŠ—æ¼‚ç§» | é€‚ç”¨åœºæ™¯ |
|------|------------------|----------|--------|--------|-----------|
| Gradient Compression (e.g., DeepGC) | âŒ ä¸»è¦ç”¨äºæ¢¯åº¦èšåˆ | âŒ æœ‰æŸ | ä¸­ç­‰ | âŒ | åŒæ­¥è®­ç»ƒ |
| Entropy Coding (e.g., Huffman) | âš ï¸ é€šç”¨å‹ç¼© | âœ… å¯æ— æŸ | ä½~ä¸­ | âœ… | é€šç”¨ |
| Subnetwork Update [21] | âš ï¸ è¿‘ä¼¼é˜ˆå€¼æ£€æµ‹ | âŒ æœ‰æŸ | ~3â€“20Ã— | âŒ | ç‰¹å®šä»»åŠ¡ |
| **PULSE (æœ¬æ–‡)** | âœ… ä¸“ä¸ºæƒé‡å¹¿æ’­è®¾è®¡ | âœ… **æ— æŸ** | **>100Ã—** | âœ… **å¼ºå¥** | **å»ä¸­å¿ƒåŒ–RL** |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼š  
> - **é€šä¿¡å‡å°‘100å€ä»¥ä¸Š**ï¼ˆ14GB â†’ ~108MBï¼‰ï¼›  
> - **ä¿æŒbit-identicalè®­ç»ƒåŠ¨æ€**ï¼›  
> - **æ— éœ€è¯¯å·®åé¦ˆï¼ˆerror feedbackï¼‰æœºåˆ¶**ï¼›  
> - **é€‚ç”¨äºä½å¸¦å®½ã€é«˜å»¶è¿Ÿã€å¼‚æ­¥ç½‘ç»œ**ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

| ä»»åŠ¡ | æ•°æ®é›† | æè¿° |
|------|--------|------|
| æ•°å­¦æ¨ç† | **MATH** | åŒ…å«7,500é“é«˜ä¸­åŠä»¥ä¸Šæ•°å­¦é¢˜ï¼Œç”¨äºRLVRï¼ˆReinforcement Learning with Verifiable Rewardsï¼‰ |
| ä»£ç ç”Ÿæˆ | **MBPP** (Mostly Basic Python Problems) | 774ä¸ªç¼–ç¨‹ä»»åŠ¡ï¼Œå¥–åŠ±åŸºäºå•å…ƒæµ‹è¯•é€šè¿‡ç‡å’Œè¯­æ³•æ­£ç¡®æ€§ |

---

### âš™ï¸ å®éªŒè®¾ç½®

| ç»„ä»¶ | é…ç½® |
|------|------|
| **æ¨¡å‹** | Qwen2.5-Instruct (0.5B, 1.5B, 7B), Llama-3.2-Instruct (3B), Gemma-3-4B-it |
| **ç®—æ³•** | **GRPO**ï¼ˆGroup Relative Policy Optimizationï¼‰ï¼Œç”¨äºreasoningä»»åŠ¡ |
| **ç²¾åº¦** | **BF16**ï¼ˆbfloat16ï¼‰æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP32ä¼˜åŒ–å™¨ä¸»æƒé‡ï¼‰ |
| **å­¦ä¹ ç‡** | é»˜è®¤ `3e-6`ï¼Œå»ä¸­å¿ƒåŒ–å®éªŒä¸­ä½¿ç”¨ `1e-6` æå‡ç¨³å®šæ€§ |
| **è®­ç»ƒæ­¥æ•°** | 400 stepsï¼ˆéªŒè¯å·²æ”¶æ•›ï¼‰ |
| **Batché…ç½®** | 32 prompts Ã— 16 rollouts |
| **ç¡¬ä»¶** | A100/H200 GPUï¼Œå…¬ç½‘å¸¦å®½çº¦400 Mb/s |

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **Weight Update Sparsity** | è¿ç»­ä¸¤æ­¥é—´**bitwiseç›¸åŒ**çš„å‚æ•°æ¯”ä¾‹ï¼š<br>$ S_k(t) = \frac{1}{d} \sum_{i=1}^d [\theta_{t+i} = \theta_t] $ |
| **Communication Reduction** | ä¼ è¾“å¤§å°ç›¸å¯¹äºå®Œæ•´checkpointçš„å‹ç¼©æ¯” |
| **Pass@1 Accuracy** | éªŒè¯é›†ä¸Šå•æ¬¡ç”Ÿæˆæ­£ç¡®çš„æ¯”ä¾‹ |
| **SHA-256 Hash Match** | éªŒè¯è§£ç åæƒé‡æ˜¯å¦bit-identical |
| **End-to-End Sync Latency** | æ¨ç†èŠ‚ç‚¹å®Œæˆä¸€æ¬¡åŒæ­¥æ‰€éœ€æ—¶é—´ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

- **Full Weight Synchronization**ï¼ˆ14GBå…¨é‡ä¼ è¾“ï¼‰
- **Gradient Compression Methods**ï¼ˆå¦‚DeepGC, QSGDï¼‰â€”â€”ä½œä¸ºèƒŒæ™¯å¯¹æ¯”ï¼Œä½†ä¸ç›´æ¥å¯æ¯”
- **Subnetwork Update** [21]ï¼šåŸºäºé˜ˆå€¼è¿‘ä¼¼è¯†åˆ«â€œæ´»è·ƒå­ç½‘ç»œâ€ï¼Œæœ‰æŸ

> PULSEæ— ç›´æ¥ç«äº‰è€…ï¼Œå› å…¶æ˜¯é¦–ä¸ª**æ— æŸã€åŸºäºstep-levelç¨€ç–æ€§**çš„æƒé‡åŒæ­¥æ–¹æ¡ˆã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **å¹³å‡æ¯æ­¥æƒé‡æ›´æ–°ç¨€ç–æ€§** | **~99%**ï¼ˆæ‰€æœ‰æ¨¡å‹è§„æ¨¡ä¸‹ä¸€è‡´ï¼‰ |
| **æœ€å°ç¨€ç–æ€§ï¼ˆæœ€å·®æƒ…å†µï¼‰** | >98%ï¼ˆå³ä½¿åœ¨k=8å¼‚æ­¥å»¶è¿Ÿä¸‹ï¼‰ |
| **PULSEå¹³å‡å‹ç¼©ç‡** | **79Ã—**ï¼ˆzstd-1ï¼‰è‡³ **100Ã—**ï¼ˆLLaMA3.2ï¼‰ |
| **å®é™…ä¸Šä¼ å¤§å°ï¼ˆ7Bæ¨¡å‹ï¼‰** | **å¹³å‡108 MB**ï¼ˆvs 14 GBï¼‰â†’ **>100Ã— å‡å°‘** |
| **é€šä¿¡éœ€æ±‚é™ä½** | ä» **20 Gbit/s â†’ 0.2 Gbit/s**ï¼ˆç»´æŒ90% GPUåˆ©ç”¨ç‡ï¼‰ |
| **ç«¯åˆ°ç«¯åŒæ­¥å»¶è¿Ÿï¼ˆå¿«è·¯å¾„ï¼‰** | **~3.9ç§’**ï¼ˆä¸‹è½½+è§£å‹+åº”ç”¨ï¼‰ |
| **å†·å¯åŠ¨æ¢å¤æ—¶é—´** | ~280ç§’ï¼ˆéœ€ä¸‹è½½å®Œæ•´checkpointï¼‰ |

> ğŸ’¡ åœ¨å»ä¸­å¿ƒåŒ–ç½‘ç»œä¸­ï¼Œ**PULSEå®ç°è¶…è¿‡100å€å¸¦å®½èŠ‚çœ**ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒæ­£å¸¸è¿›è¡Œã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰ç¨€ç–æ€§æ¥æºåˆ†æï¼ˆSection 3.3.1ï¼‰

| å› ç´  | å½±å“ |
|------|------|
| **æ¢¯åº¦å¯†åº¦** | ~99%éé›¶ â†’ **æ¢¯åº¦æœ¬èº«æ˜¯ç¨ å¯†çš„** |
| **BF16ç²¾åº¦é™åˆ¶** | æ›´æ–°å¿…é¡»è¶…è¿‡ $|w|/256$ æ‰èƒ½ç”Ÿæ•ˆ |
| **å°å­¦ä¹ ç‡ï¼ˆ~3e-6ï¼‰** | å¤§å¤šæ•°æ›´æ–°è¢«â€œå¸æ”¶â€ï¼ˆabsorbedï¼‰ |
| **å¤§æƒé‡ï¼ˆmedian ~0.012ï¼‰** | è¶…è¿‡96%æƒé‡æ— æ³•è¢«æ›´æ–° |

> âœ… **ç»“è®º**ï¼šç¨€ç–æ€§æºäº **BF16 + å°å­¦ä¹ ç‡** çš„äº¤äº’ï¼Œè€Œéæ¢¯åº¦ç¨€ç–ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒå­¦ä¹ ç‡çš„å½±å“ï¼ˆFigure 4aï¼‰

- å­¦ä¹ ç‡ä» `5e-7` â†’ `5e-6`ï¼Œç¨€ç–æ€§ä» ~99.5% â†“ è‡³ ~97%
- è¡¨æ˜**å­¦ä¹ ç‡æ˜¯æ§åˆ¶ç¨€ç–æ€§çš„å…³é”®å› ç´ **

#### ï¼ˆ3ï¼‰ç­–ç•¥é™ˆæ—§æ€§ï¼ˆPolicy Stalenessï¼‰å½±å“ï¼ˆFigure 4bï¼‰

- å³ä½¿å»¶è¿Ÿ32æ­¥ï¼Œ**æ¯æ­¥ç¨€ç–æ€§ä» >98.5%**
- æ”¯æŒ**é«˜åº¦å¼‚æ­¥è®­ç»ƒæ¶æ„**

#### ï¼ˆ4ï¼‰å‹ç¼©ç»„ä»¶è´¡çŒ®ï¼ˆTable 5ï¼‰

| æ­¥éª¤ | å‹ç¼©å¢ç›Šï¼ˆç›¸å¯¹COOï¼‰ |
|------|------------------|
| Raw COO | 2.71Ã— |
| + Delta Encoding | +13.3% â†’ 3.07Ã— |
| + Type Downscaling | +8.5% â†’ **3.33Ã—** |

> æ€»è®¡æå‡ **+22.9%** å‹ç¼©ç‡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **RLå¾®è°ƒä¸­æƒé‡æ›´æ–°æåº¦ç¨€ç–**ï¼š
   - å¹³å‡**99%å‚æ•°ä¸å˜**ï¼Œè·¨æ¨¡å‹ï¼ˆ0.5Bâ€“7Bï¼‰ã€æ¶æ„ï¼ˆQwen/Llama/Gemmaï¼‰ä¸€è‡´ã€‚
   - ç¨€ç–æ€§ç”± **BF16ç²¾åº¦** å’Œ **ä¿å®ˆå­¦ä¹ ç‡** å…±åŒå†³å®šï¼Œæ˜¯ä¸€ç§**å†…åœ¨æœºåˆ¶**è€Œéå¶ç„¶ç°è±¡ã€‚

2. **ç¨€ç–æ€§ç¨³å®šä¸”é²æ£’**ï¼š
   - æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ç¨€ç–æ€§æ³¢åŠ¨å°äº0.4%ï¼›
   - å¯¹**å¼‚æ­¥å»¶è¿Ÿï¼ˆup to 32 stepsï¼‰** ä¸æ•æ„Ÿï¼Œé€‚åˆå»ä¸­å¿ƒåŒ–éƒ¨ç½²ã€‚

3. **PULSEå®ç°æ— æŸé«˜å‹ç¼©**ï¼š
   - ä»…ä¼ è¾“å˜åŒ–éƒ¨åˆ†ï¼Œç»“åˆç¨€ç–ç¼–ç ä¸é€šç”¨å‹ç¼©ï¼Œå®ç° **>100Ã— é€šä¿¡å‡å°‘**ï¼›
   - **å­˜å‚¨å®é™…å€¼è€Œédelta**ï¼Œé¿å…æµ®ç‚¹æ¼‚ç§»ï¼Œä¿è¯**bit-identicalé‡å»º**ï¼›
   - å·²åœ¨çœŸå®å»ä¸­å¿ƒåŒ–ç½‘ç»œï¼ˆgrailå¹³å°ï¼‰ä¸­éªŒè¯æˆåŠŸã€‚

4. **å¸¦å®½ç“¶é¢ˆè¢«æ‰“ç ´**ï¼š
   - æƒé‡åŒæ­¥æ‰€éœ€å¸¦å®½ä» **20 Gbit/sé™è‡³0.2 Gbit/s**ï¼›
   - ä½¿å¾—**æ™®é€šå®½å¸¦ç½‘ç»œ**ä¹Ÿèƒ½æ”¯æŒé«˜æ•ˆåˆ†å¸ƒå¼RLè®­ç»ƒã€‚

---

### âš ï¸ å±€é™æ€§

| å±€é™ | è¯´æ˜ |
|------|------|
| **ä¾èµ–Adamç±»ä¼˜åŒ–å™¨** | åˆ†æåŸºäºAdamWï¼ŒSGDç­‰å¯èƒ½ä¸å…·å¤‡ç›¸åŒç¨€ç–æ€§ï¼ˆè§A.6ï¼‰ |
| **å½“å‰èšç„¦å•è½®å¯¹è¯ä»»åŠ¡** | å¤šè½®RLã€é•¿åºåˆ—åé¦ˆåœºæ™¯å°šæœªéªŒè¯ |
| **ä»…éªŒè¯GRPO/PPOç±»ç®—æ³•** | DPOç­‰åå¥½ä¼˜åŒ–ç®—æ³•éœ€è¿›ä¸€æ­¥ç ”ç©¶ |
| **é”šç‚¹ï¼ˆanchorï¼‰æ¢å¤æˆæœ¬é«˜** | å†·å¯åŠ¨éœ€ä¸‹è½½14GBï¼Œä¸é€‚åˆé¢‘ç¹åŠ å…¥èŠ‚ç‚¹ |

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **æ‰©å±•è‡³å…¶ä»–RLç®—æ³•**ï¼šå¦‚PPOã€DPOã€ILQLç­‰ï¼›
2. **æ”¯æŒå¤šè½®å¯¹è¯ä¸é•¿æœŸè®°å¿†**åœºæ™¯ä¸‹çš„æ›´æ–°æ¨¡å¼åˆ†æï¼›
3. **æ¢ç´¢æ›´ä½ç²¾åº¦æ ¼å¼**ï¼ˆå¦‚FP8ï¼‰å¯¹ç¨€ç–æ€§çš„æ”¾å¤§æ•ˆåº”ï¼›
4. **è‡ªé€‚åº”anchoré—´éš”**ï¼šæ ¹æ®ç½‘ç»œçŠ¶å†µåŠ¨æ€è°ƒæ•´ï¼›
5. **ç»“åˆæ¢¯åº¦ç¨€ç–åŒ–**ï¼šæ„å»ºç«¯åˆ°ç«¯é€šä¿¡é«˜æ•ˆRLæ¡†æ¶ã€‚

---

## æ€»ç»“

> **PULSEæ­ç¤ºäº†RLè®­ç»ƒä¸­â€œé™é»˜æ›´æ–°â€çš„æœ¬è´¨ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºé€šä¿¡æ•ˆç‡çš„é©å‘½æ€§çªç ´ã€‚å®ƒä¸ä»…è§£å†³äº†å»ä¸­å¿ƒåŒ–RLçš„å…³é”®ç“¶é¢ˆï¼Œæ›´ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åä½œè®­ç»ƒæä¾›äº†å¯è¡Œè·¯å¾„ã€‚**

âœ… **ä¸€å¥è¯æ€»ç»“**ï¼š  
åˆ©ç”¨RLè®­ç»ƒä¸­**>99%çš„æƒé‡æ›´æ–°ç¨€ç–æ€§**ï¼ŒPULSEå®ç°äº†**æ— æŸã€>100Ã—å‹ç¼©çš„æƒé‡åŒæ­¥**ï¼Œä½¿å»ä¸­å¿ƒåŒ–RLåœ¨æ™®é€šç½‘ç»œä¸Šæˆä¸ºç°å®ã€‚

</details>

---

### 3. [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)

**Authors**: Wei Zeng, Xuchen Li, Ruili Feng, Zhen Liu, Fengwei An, Jian Zhao  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2602.00608v1  

#### Abstract
Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolution...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šScalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
ä¼ ç»Ÿè§†é¢‘æ¸¸æˆä¾èµ–æ˜¾å¼çš„ç‰©ç†å¼•æ“å’Œå…‰æ …åŒ–æ¸²æŸ“ç®¡çº¿ï¼Œè€Œ**ç”Ÿæˆå¼æ¸¸æˆå¼•æ“**ï¼ˆGenerative Game Engineï¼‰é€šè¿‡ç¥ç»ç½‘ç»œç›´æ¥ä»åŠ¨ä½œè¾“å…¥ç”Ÿæˆæœªæ¥å¸§ï¼Œå®ç°â€œåƒç´ çº§æ¨¡æ‹Ÿâ€ã€‚ç„¶è€Œï¼Œå½“å‰è¿™ç±»ç³»ç»Ÿå—é™äº **"Memory Wall"**ï¼ˆå†…å­˜å¢™ï¼‰ï¼Œå³é«˜åˆ†è¾¨ç‡ç”Ÿæˆè¿‡ç¨‹ä¸­å·¨å¤§çš„å†…å­˜å¸¦å®½éœ€æ±‚å¯¼è‡´å»¶è¿Ÿè¿‡é«˜ï¼Œéš¾ä»¥å®ç°å®æ—¶äº¤äº’ã€‚

ç°æœ‰æ–¹æ³•å¦‚ GameNGen å’Œ Diamond ä»…èƒ½åœ¨ä½åˆ†è¾¨ç‡ï¼ˆå¦‚ 64Ã—64 æˆ– 320Ã—240ï¼‰ä¸‹è¿è¡Œï¼Œæ— æ³•æ»¡è¶³æ ‡å‡†æ¸…æ™°åº¦ï¼ˆ720Ã—480ï¼‰åŠä»¥ä¸Šçš„éœ€æ±‚ã€‚æœ¬æ–‡æ—¨åœ¨çªç ´è¿™ä¸€**åˆ†è¾¨ç‡ç“¶é¢ˆ**ï¼Œä½¿ç”Ÿæˆå¼æ¸¸æˆå¼•æ“åœ¨é«˜åˆ†è¾¨ç‡ä¸‹ä»èƒ½ä¿æŒä½å»¶è¿Ÿã€é«˜å¸§ç‡çš„å®æ—¶å“åº”èƒ½åŠ›ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ç§**å¯æ‰©å±•çš„ç¡¬ä»¶-ç®—æ³•ååŒè®¾è®¡æ¡†æ¶**ï¼ˆHardware-Algorithm Co-Designï¼‰ï¼Œæ„å»ºäº†ä¸€ä¸ªå¼‚æ„è®¡ç®—æ¶æ„æ¥è§£è€¦è®¡ç®—å¯†é›†å‹ä¸å†…å­˜å¯†é›†å‹æ¨¡å—ï¼Œå¹¶å¼•å…¥ä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯ï¼š

#### ï¼ˆ1ï¼‰å¼‚æ„èµ„æºåˆ†é…ç­–ç•¥ï¼ˆHeterogeneous Pipeline Parallelismï¼‰
- å°†ç”Ÿæˆæµç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š
  - **World Model (DiT)**ï¼šè´Ÿè´£åŸºäºåŠ¨ä½œé¢„æµ‹æ½œåœ¨ç©ºé—´çŠ¶æ€ï¼Œå±äº **compute-bound**ã€‚
  - **Decoder (VAE)**ï¼šå°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒï¼Œå±äº **memory-bound**ã€‚
- å¯¹ DiT ä½¿ç”¨ **Sequence Parallelism**ï¼ˆUlyssesï¼‰ï¼Œå¯¹ VAE ä½¿ç”¨ **Spatial Parallelism**ï¼Œåˆ†åˆ«ä¼˜åŒ–å…¶å¹¶è¡Œæ•ˆç‡ã€‚
- å»ºç«‹ç†è®ºæ¨¡å‹åˆ†æä¸åŒè®¾å¤‡åˆ†é…ä¸‹çš„ååé‡ï¼Œæ¨å¯¼å‡ºæœ€ä¼˜èµ„æºé…ç½®ï¼ˆ5:3 åˆ†é…ï¼‰ã€‚

#### ï¼ˆ2ï¼‰å†…å­˜ä¸­å¿ƒçš„æ“ä½œèåˆä¼˜åŒ–ï¼ˆMemory-Centric Operator Fusionï¼‰
- é’ˆå¯¹ VAE è§£ç å™¨å®æ–½ **Vertical Fusion**ï¼šå°† `Upsample â†’ Conv2d â†’ GroupNorm â†’ SiLU` èåˆä¸ºå•ä¸ª kernelï¼Œåˆ©ç”¨ç‰‡ä¸Š SRAM å‡å°‘ HBM è®¿é—®æ¬¡æ•°ï¼Œé™ä½ 75% å†…å­˜å¸¦å®½å‹åŠ›ã€‚
- é’ˆå¯¹ DiT ä¸­çš„ AdaLN å±‚å®æ–½ **Horizontal Fusion**ï¼šåˆå¹¶å¤šä¸ªå°çŸ©é˜µä¹˜æ³•æ“ä½œï¼Œæå‡è®¡ç®—å¯†åº¦å’Œç®—æœ¯å¼ºåº¦ï¼ˆarithmetic intensity >85%ï¼‰ã€‚

#### ï¼ˆ3ï¼‰æµå½¢æ„ŸçŸ¥çš„æ½œåœ¨å¤–æ¨æœºåˆ¶ï¼ˆManifold-Aware Latent Extrapolationï¼‰
- åˆ©ç”¨è¿ç»­å¸§ä¹‹é—´çš„**æ—¶é—´å†—ä½™æ€§**ï¼Œå‡è®¾æ½œåœ¨ç©ºé—´è½¨è¿¹å±€éƒ¨çº¿æ€§ã€‚
- å½“ç”¨æˆ·åŠ¨ä½œç¨³å®šæ—¶ï¼Œè·³è¿‡éƒ¨åˆ† DiT æ¨ç†æ­¥éª¤ï¼Œç›´æ¥é€šè¿‡å‰ä¸€æ—¶åˆ»çš„è¿åŠ¨å‘é‡ $ v_t = z_t - z_{t-1} $ å¤–æ¨ä¸‹ä¸€å¸§æ½œåœ¨ç  $ z_{t+1} \approx z_t + \lambda v_t $ã€‚
- åœ¨åŠ¨ä½œçªå˜æ—¶è‡ªåŠ¨å›é€€åˆ°å®Œæ•´æ¨ç†ï¼Œä¿è¯é€»è¾‘ä¸€è‡´æ€§ã€‚

æ­¤å¤–ï¼Œè¿˜ç»“åˆäº† **Speculative Action Prefetching**ï¼ˆåŸºäºè½»é‡ LSTM é¢„æµ‹ç”¨æˆ·è¡Œä¸ºä»¥é¢„ç”Ÿæˆå¸§ï¼‰ï¼Œè¿›ä¸€æ­¥é™ä½æ„ŸçŸ¥å»¶è¿Ÿã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | æœ¬å·¥ä½œä¼˜åŠ¿ |
|------|-----------|
| **åˆ†è¾¨ç‡** | æ”¯æŒ **720Ã—480**ï¼Œç›¸è¾ƒ Diamond (64Ã—64) æå‡ **50Ã— åƒç´ æ•°** |
| **å®æ—¶æ€§** | å®ç° **26.4 FPSï¼ˆè¿ç»­ï¼‰å’Œ 48.3 FPSï¼ˆç¦»æ•£ï¼‰**ï¼Œæ¥è¿‘å®æ—¶é˜ˆå€¼ï¼ˆ30 FPSï¼‰ |
| **å»¶è¿Ÿæ§åˆ¶** | æ‘Šé”€æœ‰æ•ˆå»¶è¿Ÿé™è‡³ **2.7 ms**ï¼Œè¿œä½äºäººç±»æ„ŸçŸ¥é˜ˆå€¼ |
| **ç³»ç»Ÿè®¾è®¡** | å¼‚æ„æ¶æ„ + æ˜¾å¼å†…å­˜ç®¡ç†ï¼Œçªç ´é€šç”¨ GPU æ¶æ„çš„â€œéšå¼ç¼“å­˜â€é™åˆ¶ |
| **é€šç”¨æ€§** | æ–¹æ³•é€‚ç”¨äºä»»ä½•æ”¯æŒæ˜¾å­˜ç¼–ç¨‹çš„ AI åŠ é€Ÿå™¨ï¼ˆå¦‚ Ascend, H100/NVLinkï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
è®ºæ–‡é‡‡ç”¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ç¯å¢ƒï¼Œæ„æˆâ€œè¿ç»­-ç¦»æ•£å¯¹å¶â€æµ‹è¯•åºŠï¼ˆContinuous-Discrete Duality Benchmarkï¼‰ï¼š

- **Matrix (Continuous Domain)**  
  - æ¥æºï¼š[9] The Matrix: Infinite-horizon world generation with real-time moving control  
  - ç±»å‹ï¼šé«˜ä¿çœŸ 3D èµ›è½¦æ¨¡æ‹Ÿå™¨ï¼Œå¼ºè°ƒè¿ç»­åŠ¨åŠ›å­¦ï¼ˆæ‘©æ“¦ã€åŠ¨é‡ç­‰ï¼‰  
  - åˆ†è¾¨ç‡ï¼š**720Ã—480**

- **PGG (Playable Game Generation, Discrete Domain)**  
  - æ¥æºï¼š[10] Playable Game Generation  
  - ç±»å‹ï¼š2D å¹³å°è·³è·ƒæ¸¸æˆï¼Œå¼ºè°ƒå¸ƒå°”é€»è¾‘ã€ç¢°æ’æ£€æµ‹ã€ç¦»æ•£çŠ¶æ€è½¬ç§»  
  - åˆ†è¾¨ç‡ï¼š**256Ã—256**

---

### å®éªŒè®¾ç½®
- **ç¡¬ä»¶å¹³å°**ï¼š8 å¡åä¸º Ascend 910C AI åŠ é€Ÿå™¨é›†ç¾¤ï¼ˆæ¯å¡ 64GB HBMï¼ŒFP16 ç®—åŠ› 752 TFLOPSï¼‰
- **äº’è¿æ‹“æ‰‘**ï¼šHCCS é«˜é€Ÿç¯å½¢äº’è”ï¼ˆ30 GB/s per linkï¼‰
- **è½¯ä»¶æ ˆ**ï¼šCANN 8.0, PyTorch 2.5.1, xfuserï¼ˆç”¨äº Ulysses å¹¶è¡Œï¼‰
- **é›†ç¾¤é…ç½®**ï¼š5 å¼ ç”¨äº DiTï¼Œ3 å¼ ç”¨äº VAEï¼ˆç»ç†è®ºå»ºæ¨¡å¾—å‡ºæœ€ä¼˜æ¯”ä¾‹ï¼‰

---

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | è¯´æ˜ |
|--------|---------|------|
| **æ€§èƒ½** | FPS, Motion-to-Photon Latency (M2P) | è¡¡é‡ç”Ÿæˆé€Ÿåº¦ä¸å“åº”å»¶è¿Ÿ |
| **è§†è§‰è´¨é‡** | FID â†“, PSNR â†‘, SSIM â†‘, LPIPS â†“ | è¡¡é‡ç”Ÿæˆç”»é¢çš„çœŸå®æ„Ÿä¸ç»†èŠ‚ä¿ç•™ |
| **ç‰©ç†åˆç†æ€§** | Control Sensitivity Analysis (CSA) | è¿ç»­åŸŸä¸­è½¬å‘è¾“å…¥ä¸è½¦è¾†åèˆªç‡çš„å…³ç³» |
| **é€»è¾‘ä¸€è‡´æ€§** | Discrete Logic Boundary (DLB) Score â†‘ | ç¦»æ•£åŸŸä¸­æ˜¯å¦è¿åç¢°æ’ã€é‡åŠ›ç­‰è§„åˆ™ï¼ˆè¶Šæ¥è¿‘ 100% è¶Šå¥½ï¼‰ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿æ–¹æ³• | å¹³å° | åˆ†è¾¨ç‡ | FPS | å¤‡æ³¨ |
|--------|-----|-------|-----|------|
| **Diamond [6]** | RTX 3090 | 64Ã—64 | 10.0 | æ‰©æ•£æ¨¡å‹ç”¨äº CS:GO åœºæ™¯ |
| **GameNGen [4]** | TPU v5 | 320Ã—240 | >20 | Google æå‡ºçš„å®æ—¶ç”Ÿæˆå¼•æ“ |
| **PGG Baseline*** | RTX 5090 | 256Ã—256 | 29.9 | å¼€æºå¹³å°è·³è·ƒç”Ÿæˆæ¨¡å‹ |

> æ³¨ï¼šæœ¬æ–‡æ–¹æ³•åœ¨æ›´é«˜åˆ†è¾¨ç‡ä¸‹ä»å®ç°æ›´ä¼˜æ€§èƒ½ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **æœ€å¤§åˆ†è¾¨ç‡æ”¯æŒ** | **720Ã—480**ï¼ˆSD æ ‡å‡†ï¼‰ |
| **è¿ç»­åŸŸï¼ˆMatrixï¼‰FPS** | **26.4 FPS** |
| **ç¦»æ•£åŸŸï¼ˆPGGï¼‰FPS** | **48.3 FPS** |
| **æ‘Šé”€æœ‰æ•ˆå»¶è¿Ÿï¼ˆå«æ¨æµ‹æ‰§è¡Œï¼‰** | **2.7 ms** |
| **å•å¸§ç‰©ç†å»¶è¿Ÿï¼ˆæ— ä¼˜åŒ–ï¼‰** | ~147 msï¼ˆä¸»è¦ç”± VAE è§£ç ä¸»å¯¼ï¼‰ |
| **VAE è§£ç åŠ é€Ÿæ¯”ï¼ˆOperator Fusionï¼‰** | **2.9Ã—**ï¼ˆä» 312.8ms â†’ 109.4msï¼‰ |

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœï¼ˆè§ Table IIIï¼‰
| æ–¹æ³• | åˆ†è¾¨ç‡ | FPS | Norm. Eff. (FPS/100TFLOPS) | Latency |
|------|--------|-----|----------------------------|--------|
| Diamond [6] | 64Ã—64 | 10.0 | 7.04 | 100 ms |
| GameNGen [4] | 320Ã—240 | >20 | 4.35 | ~50 ms |
| **Ours (Matrix-Cluster)** | **720Ã—480** | **26.4** | **0.44** | **38ms (2.7ms\*)** |

> âœ… å°½ç®¡å½’ä¸€åŒ–æ•ˆç‡è¾ƒä½ï¼ˆå› é€šä¿¡å¼€é”€ï¼‰ï¼Œä½†ç»å¯¹æ€§èƒ½æ˜¾è‘—è¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼Œä¸”è¾¾åˆ°å‰æ‰€æœªæœ‰çš„åˆ†è¾¨ç‡ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆè§ Table IV & Fig. 7ï¼‰
é€æ­¥æ·»åŠ ä¼˜åŒ–åçš„æ€§èƒ½æå‡è·¯å¾„å¦‚ä¸‹ï¼š

| é˜¶æ®µ | FPS | Speedup |
|------|-----|--------|
| Baselineï¼ˆå•å¡é¡ºåºæ‰§è¡Œï¼‰ | 2.1 | 1.0x |
| + Operator Fusion | 4.5 | 2.1x |
| + Ulysses (3:5) | 16.6 | 7.9x |
| + Optimal Ratio (5:3) | 19.4 | 9.2x |
| + Latent Extrapolation | 26.4 | **12.6x** |
| + Speculative Execution | 26.4 | æœ‰æ•ˆå»¶è¿Ÿé™è‡³ **2.7ms** |

> ğŸ”º **å…³é”®å‘ç°**ï¼šå¼‚æ„èµ„æºåˆ†é…å¸¦æ¥ **+16.9% ååå¢ç›Š**ï¼›æ½œåœ¨å¤–æ¨ alone æå‡ **+7.0 FPS**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **â€œMemory Wallâ€ æ˜¯é«˜åˆ†è¾¨ç‡ç”Ÿæˆçš„æ ¸å¿ƒç“¶é¢ˆ**ï¼Œå•çº¯ç®—æ³•ä¼˜åŒ–æ— æ³•è§£å†³ï¼Œå¿…é¡»è¿›è¡Œ **Hardware-Algorithm Co-Design**ã€‚
2. **World Model ä¸ Decoder å­˜åœ¨æ ¹æœ¬æ€§çš„èµ„æºé”™é…**ï¼šå‰è€…æ˜¯ compute-boundï¼Œåè€…æ˜¯ memory-boundï¼Œéœ€é‡‡ç”¨å¼‚æ„å¹¶è¡Œç­–ç•¥ã€‚
3. **é€šè¿‡ Operator Fusion å¯å¤§å¹…å‡å°‘ HBM è®¿é—®**ï¼Œå®ç° â€œZero-Copyâ€ æ•°æ®æµï¼Œæ˜¯æ‰“ç ´å†…å­˜å¢™çš„å…³é”®ã€‚
4. **æ—¶é—´å†—ä½™å¯ç”¨äºæ©ç›–è®¡ç®—å»¶è¿Ÿ**ï¼šManifold-Aware Latent Extrapolation å¯å®‰å…¨è·³è¿‡ 65% çš„ DiT è®¡ç®—è€Œä¸å½±å“è§†è§‰è¿è´¯æ€§ã€‚
5. **Speculative Prefetching + Change Blindness æ•ˆåº”** å¯æœ‰æ•ˆéšè—çªå‘ miss çš„è§†è§‰æŠ–åŠ¨ï¼Œç»´æŒæµç•…ä½“éªŒã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
1. **ä¾èµ–å¤§è§„æ¨¡åŠ é€Ÿå™¨é›†ç¾¤**ï¼šç›®å‰éœ€ 8 å¡ Ascend 910Cï¼Œé™åˆ¶äº†è¾¹ç¼˜éƒ¨ç½²ï¼ˆedge deploymentï¼‰çš„å¯èƒ½æ€§ã€‚
2. **Out-of-Distribution (OOD) åŠ¨ä½œå¯èƒ½å¯¼è‡´é€»è¾‘é”™è¯¯**ï¼šè™½ç„¶è®­ç»ƒåˆ†å¸ƒå†…ä¿æŒ 100% DLB æ­£ç¡®ç‡ï¼Œä½†åœ¨æœªè§è¿‡çš„åŠ¨ä½œç»„åˆä¸‹å¯èƒ½å‡ºç°â€œå¹»è§‰â€ï¼ˆå¦‚ç©¿å¢™ï¼‰ã€‚
3. **æœ€ä¼˜èµ„æºé…ç½®ä¾èµ–æ¨¡å‹è¶…å‚**ï¼ˆå¦‚ attention head æ•°ï¼‰ï¼Œéœ€è¦é’ˆå¯¹ä¸åŒæ¨¡å‹é‡æ–°è°ƒä¼˜ã€‚
4. **å°šæœªæ”¯æŒå¤šæ¨¡æ€è¾“å…¥**ï¼ˆå¦‚è¯­éŸ³ã€è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼‰ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **å¼€å‘ Hybrid Neuro-Symbolic Engine**ï¼šå¼•å…¥è½»é‡çº§ç¬¦å·é€»è¾‘å±‚ä½œä¸ºâ€œæ¸¸æˆè§„åˆ™ç›‘ç£å™¨â€ï¼Œå¼ºåˆ¶çº¦æŸå…³é”®è§„åˆ™ï¼ˆå¦‚ç¢°æ’ã€æ­»äº¡åˆ¤å®šï¼‰ã€‚
2. **æ¢ç´¢ Multi-modal Control Interfaces**ï¼šåˆ©ç”¨ Transformer çš„ cross-attention èƒ½åŠ›ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€æˆ–è¯­éŸ³å‘½ä»¤é©±åŠ¨æ¸¸æˆä¸–ç•Œã€‚
3. **æ¨åŠ¨ On-Device Quantization æŠ€æœ¯**ï¼šç ”ç©¶ 4-bit æƒé‡é‡åŒ– + æ¿€æ´»å¹³æ»‘æŠ€æœ¯ï¼Œä½¿æ¨¡å‹å¯åœ¨æ¶ˆè´¹çº§ NPU ä¸Šæœ¬åœ°è¿è¡Œã€‚
4. **è·¨å¹³å°ç¼–è¯‘å™¨æ”¯æŒ**ï¼šå°†è¯¥ååŒè®¾è®¡ç†å¿µç§»æ¤è‡³ OpenAI Triton ç­‰é€šç”¨åç«¯ï¼Œå®ç° NVIDIA/H100ã€AMD ç­‰å¤šå‚å•†å…¼å®¹ã€‚

---

## æ€»ç»“
æœ¬æ–‡é¦–æ¬¡å®ç°äº† **720Ã—480 åˆ†è¾¨ç‡ä¸‹çš„å®æ—¶ç”Ÿæˆå¼æ¸¸æˆå¼•æ“**ï¼Œé€šè¿‡ **ç¡¬ä»¶-ç®—æ³•ååŒè®¾è®¡** æˆåŠŸæ‰“ç ´äº†é•¿æœŸåˆ¶çº¦è¯¥é¢†åŸŸçš„â€œå†…å­˜å¢™â€ã€‚å…¶ä¸‰å¤§åˆ›æ–°â€”â€”**å¼‚æ„å¹¶è¡Œã€æ“ä½œèåˆã€æ½œåœ¨å¤–æ¨**â€”â€”å…±åŒæ”¯æ’‘èµ·é«˜è¾¾ **26.4 FPS** çš„æµç•…ä½“éªŒå’Œ **2.7 ms** çš„æä½æ„ŸçŸ¥å»¶è¿Ÿã€‚è¿™ä¸ä»…éªŒè¯äº†â€œç¥ç»ä¸–ç•Œæ¨¡å‹â€å¯ä»¥æ›¿ä»£ä¼ ç»Ÿå›¾å½¢å¼•æ“çš„æŠ€æœ¯å¯è¡Œæ€§ï¼Œä¹Ÿä¸ºä¸‹ä¸€ä»£æ²‰æµ¸å¼äº¤äº’å¨±ä¹å¥ å®šäº†åŸºç¡€ï¼š**ä¸–ç•Œä¸å†è¢«æ„å»ºï¼Œè€Œæ˜¯è¢«â€œæ¢¦è§â€**ã€‚

</details>

---

### 4. [Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States](https://arxiv.org/abs/2602.03708)

**Authors**: Ximing Dong, Shaowei Wang, Dayi Lin, Boyuan Chen, Ahmed E. Hassan  
**Category**: cs.CL  
**Published**: 2026-02-04  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2602.03708v1  

#### Abstract
Large Language Models (LLMs) achieve strong performance across many tasks but suffer from high inference latency due to autoregressive decoding. The issue is exacerbated in Large Reasoning Models (LRMs), which generate lengthy chains of thought. While speculative decoding accelerates inference by dr...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šBeyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ç°æœ‰çš„ **Speculative Decoding (SD)** æ–¹æ³•å¤§å¤šåœ¨ **token-level** ä¸Šè¿›è¡Œè‰ç¨¿ç”Ÿæˆä¸éªŒè¯ï¼Œå¿½ç•¥äº†è¯­ä¹‰ç­‰ä»·æ€§ï¼ˆsemantic equivalenceï¼‰â€”â€”å³ä¸åŒ token åºåˆ—å¯èƒ½è¡¨è¾¾ç›¸åŒå«ä¹‰ã€‚è¿™å¯¼è‡´å³ä½¿è¯­ä¹‰æ­£ç¡®ï¼Œåªè¦ token ä¸åŒ¹é…ä¹Ÿä¼šè¢«æ‹’ç»ï¼Œé€ æˆæ•ˆç‡ä½ä¸‹ã€‚

æ­¤å¤–ï¼Œé’ˆå¯¹ **Large Reasoning Models (LRMs)** çš„åºåˆ—çº§æ¨æµ‹æ–¹æ³•ï¼ˆå¦‚ SpecReasonã€Speculative Thinkingï¼‰ä¾èµ– **LLM-as-a-judge** æˆ–ç¡¬ç¼–ç ä¿¡å·ï¼ˆå¦‚â€œwaitâ€ï¼‰ï¼Œå­˜åœ¨åè§ä¸”ä¸å¯é ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šSemanticSpec
æå‡º **Semantic-Aware Speculative Decoding (SemanticSpec)**ï¼Œä¸€ç§å…¨æ–°çš„æ¨æµ‹è§£ç æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- å°†æ¨æµ‹å•ä½ä» **token-level** æå‡åˆ° **semantic sequence-level**ã€‚
- å¼•å…¥ **semantic probability estimation mechanism**ï¼Œé€šè¿‡æ¢æµ‹æ¨¡å‹å†…éƒ¨éšè—çŠ¶æ€ï¼ˆhidden statesï¼‰æ¥ä¼°è®¡ç”Ÿæˆç‰¹å®šè¯­ä¹‰æ„ä¹‰çš„æ¦‚ç‡ï¼Œè€Œéä»…çœ‹ token åˆ†å¸ƒã€‚

#### åˆ›æ–°æœºåˆ¶ï¼š
- **Semantic Probability Predictor**ï¼šè®­ç»ƒä¸€ä¸ªå›å½’æ¨¡å‹ï¼Œè¾“å…¥ä¸ºç›®æ ‡æ¨¡å‹åœ¨éªŒè¯æ—¶çš„å¤šå±‚ hidden statesï¼Œè¾“å‡ºä¸ºè¯¥åºåˆ—æ‰€è¡¨è¾¾è¯­ä¹‰çš„ç”Ÿæˆæ¦‚ç‡ã€‚
- **è·¨è¯­ä¹‰ä¸€è‡´æ€§éªŒè¯**ï¼šæ¯”è¾ƒè‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹å¯¹åŒä¸€è¯­ä¹‰çš„é¢„æµ‹æ¦‚ç‡ï¼Œå†³å®šæ˜¯å¦æ¥å—æ•´ä¸ªè¯­ä¹‰åºåˆ—ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿ Token-Level SD | Sequence-Level Baselines (e.g., SpecReason) | SemanticSpec |
|------|---------------------|--------------------------------------------|-------------|
| æ¨æµ‹ç²’åº¦ | Token | Sequenceï¼ˆè¡¨é¢å½¢å¼ï¼‰ | **Semantic Meaning** |
| éªŒè¯æ–¹å¼ | Greedy/top-k åŒ¹é… | LLMè¯„åˆ†æˆ–å…³é”®è¯è§¦å‘ | **åŸºäº internal states çš„è¯­ä¹‰æ¦‚ç‡å¯¹é½** |
| å¯¹è¯­ä¹‰ç­‰ä»·çš„æ”¯æŒ | âŒ å¿½ç•¥ | âš ï¸ ä¸ç¨³å®šï¼ˆä¾èµ–judgeï¼‰ | âœ… æ˜¾å¼å»ºæ¨¡ |
| æ•ˆç‡ä¸å‡†ç¡®æ€§å¹³è¡¡ | ä½æ¥å—ç‡ | é«˜é£é™©è¯¯åˆ¤ | **é«˜æ¥å—ç‡ + é«˜ä¿çœŸè¾“å‡º** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†ä¸Šè¯„ä¼°ï¼š
- **MATH-500**ï¼šæ•°å­¦é—®é¢˜æ±‚è§£ï¼ˆmulti-step deductionï¼‰
- **AIME24**, **AMC23**ï¼šç«èµ›çº§æ•°å­¦é¢˜
- **GPQA-D**ï¼šä¸“å®¶çº§ç§‘å­¦é—®ç­”ï¼ˆgraduate-level science questionsï¼‰

è¦†ç›–æ•°å­¦ä¸ç§‘å­¦é¢†åŸŸï¼Œå¼ºè°ƒå¤æ‚æ¨ç†èƒ½åŠ›ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **ç›®æ ‡æ¨¡å‹ï¼ˆTarget Modelï¼‰**ï¼š
  - `DeepSeekR1-32B`
  - `QwQ-32B`
- **è‰ç¨¿æ¨¡å‹ï¼ˆDraft Modelï¼‰**ï¼š
  - `DeepSeekR1-1.5B`ï¼ˆè½»é‡çº§åŒç³»åˆ—æ¨¡å‹ï¼‰
- **è®­ç»ƒæ•°æ®**ï¼š
  - ä½¿ç”¨ `SimpleScaling-S1K` æ•°æ®é›†è®­ç»ƒ semantic probability predictor
  - æŒ‰é¢†åŸŸåˆ’åˆ†ï¼šçº¦40ä¸‡æ ·æœ¬ç”¨äºç§‘å­¦ï¼ˆGPQA-Dï¼‰ï¼Œå…¶ä½™ç”¨äºæ•°å­¦ä»»åŠ¡
- **åºåˆ—åˆ†å‰²ç­–ç•¥**ï¼šé»˜è®¤ä½¿ç”¨ `\n\n` åˆ†å‰² reasoning stepsï¼›ä¹Ÿæµ‹è¯•äº†å¥æœ«æ ‡ç‚¹ `.?!` çš„ç»†ç²’åº¦åˆ‡åˆ†

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **Pass@1** | å•æ¬¡ç”Ÿæˆç­”æ¡ˆæ­£ç¡®çš„æ¯”ä¾‹ï¼Œè¡¡é‡å‡†ç¡®æ€§ |
| **Latency (s)** | æ€»æ¨ç†å»¶è¿Ÿï¼ˆå« drafting å’Œ verificationï¼‰ |
| **Token Per Second (TPS)** | ååé‡ï¼Œè¡¡é‡æ•ˆç‡ |
| **Speedup Ratio** | ç›¸å¯¹äºæ ‡å‡† decoding çš„åŠ é€Ÿæ¯” |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **SpecSampling** | Token-level SD | æ ‡å‡†æ¨æµ‹é‡‡æ ·ï¼Œæ— è¯­ä¹‰æ„ŸçŸ¥ |
| **SpecReason (Pan et al., 2025)** | Sequence-level | è‰æ‹Ÿ reasoning stepsï¼Œç”±ç›®æ ‡æ¨¡å‹æ‰“åˆ†éªŒè¯ |
| **Speculative Thinking (Yang et al., 2025)** | Sequence-level | åŸºäºâ€œwaitâ€ç­‰è¯è§¦å‘å¤§æ¨¡å‹ä»‹å…¥ |
| **Random / Last-hidden** | Ablation variants | ç”¨äºæ¶ˆèåˆ†æ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®
| æ¨¡å‹ç»„åˆ | åŠ é€Ÿæ¯” (Speedup) | TPS æå‡ | Pass@1 ä¸‹é™ |
|--------|------------------|----------|-------------|
| **DeepSeekR1-32B** | **2.7Ã—** | **+1.67Ã— vs SpecSampling** | ~4.9% â†“ |
| **QwQ-32B** | **2.1Ã—** | **+2.66Ã— vs SpecSampling** | ~3.9% â†“ |

> ğŸ’¡ åœ¨ä¿æŒæ¥è¿‘ç›®æ ‡æ¨¡å‹å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå®ç°æ˜¾è‘—åŠ é€Ÿã€‚

### ğŸ†š ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”
| å¯¹æ¯”é¡¹ | ç»“æœ |
|-------|------|
| **vs SpecSampling (token-level)** | 
| - å¹³å‡æå‡ Pass@1ï¼š**+17.3% (DeepSeek), +7.3% (QwQ)**  
| - å‡å°‘å»¶è¿Ÿï¼š**-40.2%, -45.3%**  
| - æ›´é«˜ TPSï¼šåˆ†åˆ«æå‡ **1.67Ã— å’Œ 2.66Ã—** |
| **vs SpecReason / Speculative Thinking** |
| - æ˜¾è‘—æ›´é«˜ Pass@1ï¼ˆå°¤å…¶åœ¨ AIME24 ä¸Šé«˜å‡ºè¶… 10%ï¼‰  
| - æ›´ä½å»¶è¿Ÿï¼Œæ›´ç¨³å®šè¡¨ç° |
| - Speculative Thinking å› ç›²ç›®æ¥å—è‰ç¨¿è€Œå‡†ç¡®ç‡æœ€ä½ |

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰
| å˜ä½“ | æè¿° | ç»“æœ |
|------|------|------|
| **Random** | è¯­ä¹‰æ¦‚ç‡éšæœºèµ‹å€¼ | æ‰€æœ‰æŒ‡æ ‡å¤§å¹…ä¸‹é™ â†’ è¡¨æ˜ predictor è‡³å…³é‡è¦ |
| **Last-hidden** | ä»…ç”¨æœ€åä¸€å±‚ hidden state | TPS æ›´é«˜ï¼ˆè®¡ç®—å¼€é”€å°ï¼‰ï¼Œä½† **Pass@1 æ˜æ˜¾æ›´ä½**ï¼ˆå¦‚ AIME24 ä¸Šä» 0.553â†’0.400ï¼‰ |

> âœ… å¤šå±‚ hidden states èšåˆèƒ½æ›´å¥½æ•æ‰è¯­ä¹‰ç½®ä¿¡åº¦ï¼Œè™½ç•¥æœ‰è®¡ç®—ä»£ä»·ï¼Œä½†æ˜¾è‘—æå‡å‡†ç¡®æ€§ã€‚

### ğŸ”„ æ³›åŒ–æ€§å®éªŒï¼ˆCross-domain Generalizationï¼‰
| è®¾ç½® | ç»“æœ |
|------|------|
| æ•°å­¦ä»»åŠ¡ç”¨ç§‘å­¦åŸŸè®­ç»ƒçš„ predictor | æ€§èƒ½è½»å¾®ä¸‹é™ï¼ˆå¦‚ AMC23: 0.892 â†’ 0.875ï¼‰ï¼Œä½†ä»æœ‰æ•ˆ |
| ç§‘å­¦ä»»åŠ¡ç”¨æ•°å­¦åŸŸè®­ç»ƒçš„ predictor | GPQA-D: 0.573 â†’ 0.552 |

> âœ… è¡¨æ˜ semantic probability predictor å…·å¤‡ä¸€å®šè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **è¯­ä¹‰å±‚é¢çš„æ¨æµ‹ä¼˜äº token/sequence-level**ï¼š
   - é€šè¿‡å»ºæ¨¡ **semantic probability**ï¼Œå¯æœ‰æ•ˆè¯†åˆ«è¯­ä¹‰ç­‰ä»·ä½†å½¢å¼ä¸åŒçš„åˆç†è¾“å‡ºï¼Œé¿å…æ— æ•ˆæ‹’ç»ã€‚
2. **LLM å†…éƒ¨çŠ¶æ€è•´å«è¯­ä¹‰ç½®ä¿¡åº¦ä¿¡æ¯**ï¼š
   - hidden statesï¼ˆå°¤å…¶æ˜¯å¤šå±‚èšåˆï¼‰èƒ½åæ˜ æ¨¡å‹å¯¹æŸè¯­ä¹‰çš„ç”Ÿæˆå€¾å‘ï¼Œå¯ç”¨äºé«˜æ•ˆè¯­ä¹‰éªŒè¯ã€‚
3. **SemanticSpec å®ç°æ•ˆç‡ä¸æ•ˆæœåŒèµ¢**ï¼š
   - åœ¨ä¸¤ä¸ªä¸»æµ LRM ä¸Šå®ç° **æœ€é«˜è¾¾ 2.7Ã— çš„ç«¯åˆ°ç«¯åŠ é€Ÿ**ï¼ŒåŒæ—¶ä¿æŒæ¥è¿‘åŸæ¨¡å‹çš„å‡†ç¡®ç‡ã€‚
4. **é€‚ç”¨äºé•¿é“¾æ¨ç†åœºæ™¯**ï¼š
   - ç‰¹åˆ«é€‚åˆ LRMs ä¸­çš„ Chain-of-Thought ç”Ÿæˆï¼Œå‡å°‘å†—ä½™é‡å¤æ¨ç†è·¯å¾„ã€‚

### âš ï¸ å±€é™æ€§
- å½“å‰è¯„ä¼°å±€é™äºä¸¤å¯¹å¼€æºæ¨¡å‹ï¼ˆDeepSeek/QwQ ç³»åˆ—ï¼‰å’Œå››ä¸ªåŸºå‡†ï¼Œæ³›åŒ–æ€§éœ€è¿›ä¸€æ­¥éªŒè¯ã€‚
- semantic probability predictor éœ€ç¦»çº¿è®­ç»ƒï¼Œå¢åŠ éƒ¨ç½²å¤æ‚æ€§ã€‚
- åºåˆ—åˆ‡åˆ†ç­–ç•¥ï¼ˆå¦‚ `\n\n`ï¼‰å½±å“æ€§èƒ½ï¼Œè‡ªåŠ¨è¯­ä¹‰è¾¹ç•Œæ£€æµ‹ä»æ˜¯æŒ‘æˆ˜ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **zero-shot æˆ– prompt-based semantic probability estimation**ï¼Œå‡å°‘è®­ç»ƒä¾èµ–ã€‚
- å°† SemanticSpec æ‰©å±•è‡³ **å¤šæ¨¡æ€æ¨¡å‹** æˆ– **å¯¹è¯ç³»ç»Ÿ**ã€‚
- ç»“åˆ **åŠ¨æ€åˆ‡åˆ†æœºåˆ¶** è‡ªåŠ¨è¯†åˆ«è¯­ä¹‰å•å…ƒè¾¹ç•Œã€‚
- ç ”ç©¶ **è½»é‡åŒ– predictor** è®¾è®¡ä»¥é™ä½æ¨ç†å¼€é”€ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> SemanticSpec é€šè¿‡â€œç†è§£è¯­ä¹‰â€è€Œéâ€œåŒ¹é…å­—é¢â€ï¼Œå®ç°äº†æ›´æ™ºèƒ½ã€é«˜æ•ˆçš„æ¨æµ‹è§£ç ï¼Œåœ¨ä¸ç‰ºç‰²å‡†ç¡®æ€§çš„å‰æä¸‹å¤§å¹…æå‡ LLM æ¨ç†é€Ÿåº¦ï¼Œä¸ºä¸‹ä¸€ä»£é«˜æ•ˆæ¨ç†ç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 5. [Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection](https://arxiv.org/abs/2602.03216)

**Authors**: Dongwon Jo, Beomseok Kang, Jiwon Song, Jae-Joon Kim  
**Category**: cs.CL  
**Published**: 2026-02-04  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.03216v1  

#### Abstract
The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely o...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šToken Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ï¼ˆlong-contextï¼‰æ—¶é¢ä¸´ **Attention æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦ $O(L^2)$** é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ **prefill é˜¶æ®µ**ï¼Œè¿™æˆä¸ºæ¨ç†æ•ˆç‡çš„ä¸»è¦ç“¶é¢ˆã€‚ç°æœ‰åŠ é€Ÿæ–¹æ³•å­˜åœ¨ä»¥ä¸‹ä¸è¶³ï¼š
- **ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰**ï¼šé€šå¸¸é‡‡ç”¨å—çº§ï¼ˆblock-levelï¼‰ç¨€ç–åŒ–ï¼Œæ— æ³•ç²¾ç»†å»é™¤æ— å…³ tokenã€‚
- **Token è’¸é¦/æ·˜æ±°ï¼ˆToken Evictionï¼‰**ï¼šå¦‚ FastKVã€GemFilter åœ¨æ—©æœŸå±‚æ°¸ä¹…ç§»é™¤éƒ¨åˆ† tokenï¼Œå¯¼è‡´åç»­å±‚æ— æ³•é‡æ–°è¯„ä¼°å…¶é‡è¦æ€§ï¼Œé€ æˆä¿¡æ¯ä¸¢å¤±ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šToken Sparse Attention
æå‡ºä¸€ç§è½»é‡çº§ã€åŠ¨æ€çš„ **token-level ç¨€ç–åŒ–æœºåˆ¶**ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯â€œ**å‹ç¼© â†’ æ³¨æ„åŠ›è®¡ç®— â†’ è§£å‹**â€ï¼ˆCompress-and-Decompressï¼‰ï¼Œå®ç° **å¯é€†çš„ token é€‰æ‹©**ã€‚

#### åˆ›æ–°ç‚¹ï¼š
1. **Interleaved Token Selectionï¼ˆäº¤é”™å¼ Token é€‰æ‹©ï¼‰**  
   - æ¯ä¸ª attention head å¯ç‹¬ç«‹é€‰æ‹©æœ€é‡è¦çš„ token å­é›†è¿›è¡Œè®¡ç®—ï¼ˆ$L' < L$ï¼‰ï¼Œå½¢æˆå‹ç¼©åçš„ $Q, K, V$ã€‚
   - æ³¨æ„åŠ›è¾“å‡ºåé€šè¿‡ **scatter æ“ä½œæ¢å¤åˆ°åŸå§‹åºåˆ—é•¿åº¦**ï¼Œæœªé€‰ä¸­çš„ä½ç½®ç½®é›¶ï¼Œä¿ç•™æ®‹å·®è·¯å¾„ä¸Šçš„åŸå§‹ä¿¡æ¯ã€‚
   - å…è®¸åç»­å±‚å†æ¬¡è€ƒè™‘ä¹‹å‰è¢«è·³è¿‡çš„ tokenï¼Œé¿å…äº†æ°¸ä¹…æ·˜æ±°å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚

2. **Dynamic Token Coverageï¼ˆåŠ¨æ€ç¨€ç–é¢„ç®—åˆ†é…ï¼‰**
   - ä¸å›ºå®šä¿ç•™æ¯”ä¾‹ï¼Œè€Œæ˜¯åŸºäº attention score åˆ†å¸ƒåŠ¨æ€å†³å®šåº”å‰ªæçš„ token æ•°é‡ã€‚
   - å¼•å…¥ **coverage threshold $T$**ï¼Œå‰ªå»ç´¯è®¡è´¡çŒ®å°äº $T$ çš„æœ€ä¸é‡è¦ token å°¾éƒ¨ï¼Œè§†ä¸ºâ€œattention noiseâ€ã€‚

3. **Sparse Layer Selectionï¼ˆç¨€ç–å±‚é€‰æ‹©æœºåˆ¶ï¼‰**
   - å¹¶éæ‰€æœ‰å±‚éƒ½é€‚åˆç¨€ç–åŒ–ã€‚å¼•å…¥ **Inter-Layer Representation Drift** æŒ‡æ ‡è¡¡é‡å±‚é—´è¡¨ç¤ºå˜åŒ–ã€‚
   - ä»…å¯¹è¡¨ç¤ºç¨³å®šçš„ä½æ¼‚ç§»å±‚åº”ç”¨ Token Sparse Attentionï¼Œæå‡é²æ£’æ€§ã€‚

4. **å¼ºå…¼å®¹æ€§è®¾è®¡**
   - å‹ç¼©åçš„ $Q, K, V$ ä»æ˜¯å¯†é›†å¼ é‡ï¼Œå®Œå…¨å…¼å®¹ **FlashAttention** å’Œå„ç±» **sparse attention kernels**ã€‚
   - å¯ä½œä¸ºæ’ä»¶æ— ç¼é›†æˆåˆ°å·²æœ‰æ–¹æ³•ä¸­ï¼ˆå¦‚ FlexPrefillã€Minferenceï¼‰ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ FastKV/GemFilterï¼‰ | Token Sparse Attention |
|------|-------------------------------|------------------------|
| **Token å¯æ¢å¤æ€§** | âŒ æ°¸ä¹…åˆ é™¤ï¼Œä¸å¯å†ç”¨ | âœ… é€šè¿‡ residual è·¯å¾„ä¿ç•™ |
| **Head-wise çµæ´»æ€§** | âŒ æ‰€æœ‰ head å…±äº«ç›¸åŒ token é›† | âœ… å„ head å¯é€‰ä¸åŒå­é›† |
| **Layer-wise åŠ¨æ€æ€§** | âŒ å›ºå®šæ—©æœŸå†³ç­– | âœ… æ¯å±‚åŠ¨æ€è°ƒæ•´ |
| **ä¸ç°æœ‰æ–¹æ³•å…³ç³»** | æ›¿ä»£æ–¹æ¡ˆ | âœ… **äº’è¡¥å¢å¼ºå‹æ¨¡å—** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†
- **RULER**ï¼šåˆæˆåŸºå‡†ï¼Œæµ‹è¯•é•¿ä¸Šä¸‹æ–‡ä¸‹çš„æ£€ç´¢ä¸å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œæ”¯æŒä» 4K åˆ° 128K çš„å¯æ§é•¿åº¦ã€‚
- **InfiniteBench**ï¼šå¤§è§„æ¨¡é•¿æ–‡æœ¬ç†è§£åŸºå‡†ï¼Œæ¶µç›–æ£€ç´¢ã€é—®ç­”ã€æ•°å­¦ã€ä»£ç ç­‰ä»»åŠ¡ï¼Œéƒ¨åˆ†ä»»åŠ¡è¶… 100K tokensã€‚
- ï¼ˆé™„å½•ä¸­è¿˜åŒ…å« **Needle-in-a-Haystack** å’Œ **LongBench** ç»“æœï¼‰

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹**ï¼š
  - `LLaMA-3.1-8B-Instruct`
  - `Mistral-Nemo-12B-Instruct`
- **ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š4K, 8K, 16K, ..., æœ€é«˜è‡³ **128K**
- **ç¡¬ä»¶å¹³å°**ï¼šå•å¼  NVIDIA A100 80GB GPU
- **è¯„ä¼°é˜¶æ®µ**ï¼šä»…åœ¨ **prefill é˜¶æ®µ** åº”ç”¨ä¼˜åŒ–ï¼Œdecode é˜¶æ®µä¿æŒæ ‡å‡† dense attentionã€‚

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
- **Accuracy**ï¼šä»»åŠ¡å‡†ç¡®ç‡ï¼ˆå¦‚ RULERã€InfiniteBench ä¸Šçš„å¹³å‡å¾—åˆ†ï¼‰
- **Speedup**ï¼šç›¸å¯¹äº FlashAttention çš„ **attention è®¡ç®—å»¶è¿ŸåŠ é€Ÿæ¯”**
- **Sparsity**ï¼šæ³¨æ„åŠ›å›¾ä¸­çš„å¹³å‡ç¨€ç–åº¦
- **Latency Breakdown**ï¼šåˆ†è§£ token scoringã€å‹ç¼©ã€æ³¨æ„åŠ›è®¡ç®—ã€è§£å‹ç­‰å„é˜¶æ®µè€—æ—¶

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æè¿° |
|------|------|------|
| **FlashAttention** | Dense Baseline | é«˜æ•ˆå†…å­˜è®¿é—®çš„å®Œæ•´ attention |
| **Minference** | Structured Sparse | ä½¿ç”¨ Vertical-Slash æ¨¡å¼çš„å—ç¨€ç– attention |
| **FlexPrefill** | Context-Aware Block-Sparse | åŠ¨æ€å—ç¨€ç–ï¼Œquery-aware pruning |
| **FastKV / GemFilter** | Token Eviction | æ—©æœŸå±‚é€‰æ‹©å¹¶æ°¸ä¹…å‹ç¼© token |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 & Figure 1ï¼‰

#### åœ¨ LLaMA-3.1-8B-Instruct ä¸Šçš„è¡¨ç°ï¼ˆRULER åŸºå‡†ï¼‰ï¼š

| æ–¹æ³• | Avg. Accuracy | 128K Speedup |
|------|---------------|-------------|
| FlashAttention | 87.01% | Ã—1.00 |
| + Token Sparse | **87.02%** | **Ã—1.36** |
| FlexPrefill | 87.27% | Ã—2.44 |
| + Token Sparse | **87.27%** | **Ã—2.76** |
| FlexPrefill + Token Sparse ($T=0.02$) | ~86.8% | **Ã—3.23** |

> âœ… **æœ€é«˜è¾¾ Ã—3.23 åŠ é€Ÿï¼Œç²¾åº¦ä¸‹é™ <1%**

#### åœ¨ Mistral-Nemo-12B-Instruct ä¸Šçš„ç»“æœï¼š
| æ–¹æ³• | Avg. Accuracy | 128K Speedup |
|------|---------------|-------------|
| FlashAttention | 67.60% | Ã—1.00 |
| + Token Sparse | 67.37% | Ã—1.22 |
| FlexPrefill | 68.16% | Ã—1.22 |
| + Token Sparse | 67.91% | **Ã—1.33** |

> âœ… æ˜¾è‘—æå‡å·²æœ‰æ–¹æ³•çš„æ•ˆç‡ï¼Œä¸”ç²¾åº¦æ³¢åŠ¨æå°ï¼ˆ<0.5%ï¼‰

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœ

#### âœ… **Dynamic vs. Fixed Sparsity**ï¼ˆTable 4ï¼‰
| æ–¹æ³• | Sparsity | Accuracy | Speedup |
|------|--------|---------|--------|
| Dynamic ($T=0.01$) | 67.36% | **86.84%** | Ã—1.51 |
| Fixed ($s=0.5$) | 74.95% | 85.43% | Ã—1.57 |

> ğŸ’¡ åŠ¨æ€åˆ†é…ç¨€ç–é¢„ç®—æ¯”å›ºå®šæ¯”ä¾‹æ›´é«˜æ•ˆï¼Œåœ¨ç›¸è¿‘é€Ÿåº¦ä¸‹ **é«˜å‡º 1.4% å‡†ç¡®ç‡**ã€‚

#### âœ… **vs. Token Eviction Methods**ï¼ˆTable 5ï¼‰
| æ–¹æ³• | RULER Avg. Acc. | Speedup |
|------|------------------|--------|
| FlashAttn | 87.01% | Ã—1.00 |
| GemFilter | 85.12% | Ã—1.53 |
| FastKV | 85.64% | Ã—1.50 |
| **Ours (TSA)** | **86.84%** | Ã—1.51 |

> âœ… åœ¨ç›¸ä¼¼åŠ é€Ÿæ¯”ä¸‹ï¼Œ**å‡†ç¡®ç‡æ˜¾è‘—ä¼˜äº token eviction æ–¹æ³•**ï¼ˆ+1.2~1.7%ï¼‰ã€‚

#### âœ… **Overhead Analysis**ï¼ˆFigure 6bï¼‰
- Token Sparse Attention çš„é¢å¤–å¼€é”€ï¼ˆtoken scoring + compress/decompressï¼‰åœ¨ 128K ä¸‹ä»…å æ€» attention å»¶è¿Ÿçš„ **8â€“11%**ã€‚
- å³ä½¿åœ¨ $T=0.05$ æç«¯ç¨€ç–ä¸‹ä»ä¿æŒé«˜æ•ˆã€‚

#### âœ… **Sparsity éšä¸Šä¸‹æ–‡å¢é•¿è€Œå¢åŠ **ï¼ˆTable 3ï¼‰
| Context Length | $T=0.01$ æ—¶å¹³å‡ Sparsity |
|----------------|--------------------------|
| 4K             | 28.02%                   |
| 8K             | 32.98%                   |
| 64K            | 47.32%                   |
| 128K           | **67.36%**               |

> è¡¨æ˜è¯¥æ–¹æ³•åœ¨ **è¶Šé•¿ä¸Šä¸‹æ–‡ä¸­å¢ç›Šè¶Šå¤§**ï¼Œç¬¦åˆé¢„æœŸã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **åŠ¨æ€ã€å¯é€†çš„ token çº§ç¨€ç–åŒ–æ˜¯å¯è¡Œä¸”é«˜æ•ˆçš„**ï¼šé€šè¿‡â€œå‹ç¼©-è§£å‹â€æœºåˆ¶ï¼Œæ—¢èƒ½å‡å°‘è®¡ç®—é‡ï¼Œåˆèƒ½ä¿ç•™æœªæ¥é‡è¯„æœºä¼šã€‚
2. **token é‡è¦æ€§å…·æœ‰ layer-wise å’Œ head-wise åŠ¨æ€æ€§**ï¼šå¼ºåˆ¶ç»Ÿä¸€ token é›†ä¼šæŸå®³è¡¨è¾¾èƒ½åŠ›ï¼›å…è®¸æ¯ head æ¯å±‚çµæ´»é€‰æ‹©æ›´åˆç†ã€‚
3. **ä¸ç°æœ‰ç¨€ç– attention æ–¹æ³•é«˜åº¦äº’è¡¥**ï¼šä¸æ˜¯æ›¿ä»£ï¼Œè€Œæ˜¯å¯å åŠ çš„â€œæ•ˆç‡æ”¾å¤§å™¨â€ï¼Œèƒ½è¿›ä¸€æ­¥æå‡ FlexPrefill ç­‰æ–¹æ³•çš„æ€§ä»·æ¯”ã€‚
4. **åœ¨ 128K ä¸Šå®ç°é«˜è¾¾ Ã—3.23 çš„ attention åŠ é€Ÿï¼Œç²¾åº¦æŸå¤± <1%**ï¼Œæ˜¾è‘—æ”¹å–„ accuracy-latency trade-offã€‚

### âš ï¸ å±€é™æ€§ï¼ˆLimitationï¼‰
- åœ¨çŸ­ä¸Šä¸‹æ–‡ï¼ˆå¦‚ <8Kï¼‰ä¸­æ”¶ç›Šè¾ƒå°ï¼Œå›  attention ä¸ä¸»å¯¼æ•´ä½“å»¶è¿Ÿã€‚
- è¿‡äºæ¿€è¿›çš„ token coverageï¼ˆå¦‚ $T > 0.05$ï¼‰å¯èƒ½å¯¼è‡´å…³é”®ä¿¡æ¯ä¸¢å¤±ã€‚
- å½“å‰ä»…åº”ç”¨äº prefill é˜¶æ®µï¼Œå°šæœªæ‰©å±•è‡³ decoding æˆ– KV cache ç®¡ç†ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- å°† Token Sparse Attention æ‰©å±•åˆ° **decoding é˜¶æ®µ**ï¼Œç”¨äº KV cache å‹ç¼©ã€‚
- æ¢ç´¢åœ¨ **vision-language models** ç­‰å¤šæ¨¡æ€åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
- è®¾è®¡æ›´æ™ºèƒ½çš„ coverage threshold è‡ªé€‚åº”ç­–ç•¥ï¼ˆä¾‹å¦‚åŸºäºè¾“å…¥éš¾åº¦ï¼‰ã€‚
- ç»“åˆé‡åŒ–æŠ€æœ¯ï¼ˆå¦‚ AWQã€GPTQï¼‰æ„å»ºç«¯åˆ°ç«¯é«˜æ•ˆæ¨ç†ç³»ç»Ÿã€‚

---

> ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **Token Sparse Attention æä¾›äº†ä¸€ç§çµæ´»ã€å¯é€†ã€å…¼å®¹æ€§å¼ºçš„ token-level ç¨€ç–åŒ–èŒƒå¼ï¼Œé€šè¿‡åŠ¨æ€äº¤é”™é€‰æ‹©æœºåˆ¶ï¼Œåœ¨å‡ ä¹æ— æŸç²¾åº¦çš„å‰æä¸‹å¤§å¹…æå‡é•¿ä¸Šä¸‹æ–‡ attention æ•ˆç‡ï¼Œæ˜¯å½“å‰ç¨€ç– attention æ–¹æ³•çš„é‡è¦è¡¥å……ã€‚**

</details>

---

### 6. [POP: Prefill-Only Pruning for Efficient Large Model Inference](https://arxiv.org/abs/2602.03295)

**Authors**: Junhui He, Zhihui Fu, Jun Wang, Qingan Li  
**Category**: cs.CL  
**Published**: 2026-02-04  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.03295v1  

#### Abstract
Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In th...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**POP: Prefill-Only Pruning for Efficient Large Model Inference**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ¨ç†æ—¶é¢ä¸´å·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œå°¤å…¶æ˜¯åœ¨ **prefill é˜¶æ®µ**ï¼ˆå¤„ç†è¾“å…¥åºåˆ—å¹¶ç”Ÿæˆ KV Cacheï¼‰ã€‚ç°æœ‰çš„ **structured pruning** æ–¹æ³•è™½ç„¶ç¡¬ä»¶å‹å¥½ï¼Œä½†åœ¨å¼€æ”¾ç”Ÿæˆä»»åŠ¡ä¸­å¸¸å¯¼è‡´ä¸¥é‡ç²¾åº¦ä¸‹é™ã€‚

ä½œè€…æŒ‡å‡ºï¼Œè¿™ç§å¤±è´¥æºäºâ€œ**stage-agnostic**â€ï¼ˆé˜¶æ®µæ— å…³ï¼‰çš„å‰ªæç­–ç•¥â€”â€”å³å¯¹ prefill å’Œ decode é˜¶æ®µé‡‡ç”¨ç›¸åŒçš„ç²¾ç®€æ¨¡å‹ç»“æ„ï¼Œå¿½ç•¥äº†äºŒè€…åŠŸèƒ½ä¸Šçš„ä¸å¯¹ç§°æ€§ã€‚

---

### ğŸš€ æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

æå‡º **Prefill-Only Pruning (POP)** â€”â€”ä¸€ç§**é˜¶æ®µæ„ŸçŸ¥**ï¼ˆstage-awareï¼‰çš„å‰ªæç­–ç•¥ï¼š

- **ä»…åœ¨ prefill é˜¶æ®µå‰ªææ·±å±‚ç½‘ç»œå±‚**ï¼Œå› ä¸ºè¿™äº›å±‚åœ¨ä¸Šä¸‹æ–‡ç¼–ç ä¸­æ˜¯å†—ä½™çš„ï¼›
- **decode é˜¶æ®µä¿ç•™å®Œæ•´æ¨¡å‹**ï¼Œä»¥ç¡®ä¿ next-token é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

ä¸ºæ­¤å¼•å…¥ä¸¤ä¸ªå…³é”®æŠ€æœ¯æœºåˆ¶ï¼š

1. **Virtual Gate Mechanism**  
   ç”¨äºé‡åŒ–æ¯ä¸€å±‚çš„é‡è¦æ€§ï¼Œé€šè¿‡æ¢¯åº¦ä¼°è®¡ç§»é™¤æŸå±‚åæŸå¤±çš„å˜åŒ–ï¼Œé¿å…å®é™…åˆ é™¤å†é‡è®­ã€‚

2. **Independent KV Projections**  
   å³ä½¿è·³è¿‡æŸå±‚çš„ Attention å’Œ FFN è®¡ç®—ï¼Œä»ç‹¬ç«‹æ‰§è¡Œå…¶ Key/Value æŠ•å½±ï¼Œä¿è¯ KV Cache å®Œæ•´æ€§ã€‚

3. **Boundary Handling**  
   å°†æœ€åä¸€ä¸ªè¾“å…¥ token çš„å¤„ç†åˆ’å½’ä¸º decode é˜¶æ®µï¼Œç”¨å®Œæ•´æ¨¡å‹é¢„æµ‹ç¬¬ä¸€ä¸ªè¾“å‡º tokenï¼Œé˜²æ­¢ç²¾åº¦ä¸‹é™ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ç°æœ‰ Structured Pruning | POP |
|------|--------------------------|-----|
| å‰ªæç­–ç•¥ | é˜¶æ®µæ— å…³ï¼ˆprefill & decode åŒæ­¥å‰ªæï¼‰ | **é˜¶æ®µæ„ŸçŸ¥**ï¼ˆä»… prefill å‰ªæï¼‰ |
| å‡†ç¡®ç‡ä¿æŒ | åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šå´©æºƒï¼ˆå¦‚ GSM8K ä¸‹é™è‡³ <1%ï¼‰ | æ¥è¿‘å…¨æ¨¡å‹æ€§èƒ½ï¼ˆ>95%ï¼‰ |
| ç¡¬ä»¶å…¼å®¹æ€§ | é«˜ï¼ˆç»“æ„åŒ–ï¼‰ | é«˜ï¼ˆç»“æ„åŒ–ï¼‰ |
| åŠ é€Ÿæ•ˆæœ | æœ‰é™ | æœ€é«˜è¾¾ **1.37Ã— prefill speedup** |
| æ— éœ€é‡è®­ç»ƒ | æ˜¯ | æ˜¯ |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼šåœ¨ä¸ç‰ºç‰² decode æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—åŠ é€Ÿ prefill é˜¶æ®µï¼Œçªç ´äº†ä¼ ç»Ÿ structured pruning ä¸­â€œç²¾åº¦-æ•ˆç‡â€çš„æƒè¡¡å›°å¢ƒã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

| ç±»å‹ | æ•°æ®é›† |
|------|--------|
| æ–‡æœ¬æ ¡å‡†ï¼ˆCalibrationï¼‰ | `WizardLM-V2-196K`ï¼ˆ200 æ ·æœ¬ï¼‰ |
| å¤šæ¨¡æ€æ ¡å‡† | `LLAVA-Instruct-150K`ï¼ˆ200 æ ·æœ¬ï¼‰ |
| è¯„æµ‹åŸºå‡† | `MMLU`, `HellaSwag`, `Winogrande`, `PIQA`, `GSM8K`, `HumanEval`, `MultiFieldQA`, `HotpotQA`, `MMMU`, `RealWorldQA`, `TextVQA`, `ScreenSpot` |

---

### âš™ï¸ å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

| é¡¹ç›® | è®¾ç½® |
|------|------|
| æ¨¡å‹ | Llama-3.1-8B-Instruct, Qwen3-VL-8B-Instruct, Gemma-3-12B-It |
| å‰ªææ¯”ä¾‹ | é»˜è®¤å‰ªå»æœ€å 1/3 å±‚ï¼ˆâ‰ˆ33%ï¼‰ |
| ç¡¬ä»¶ | NVIDIA A100 80GB GPUs |
| æ‰¹æ¬¡å¤§å° | 8ï¼ˆspeedup æµ‹è¯•ï¼‰ï¼Œ4ï¼ˆablationï¼‰ |
| è¯„ä¼°æŒ‡æ ‡ |  
- **Accuracy**: å„ä»»åŠ¡å¾—åˆ†ï¼ˆ0-shot æˆ– generationï¼‰  
- **Speedup**: Time-to-First-Token (TTFT) åŠ é€Ÿæ¯”  
- **FLOPs Reduction**ï¼ˆé—´æ¥è¡¡é‡ï¼‰ |

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **Wanda (30%)** | Unstructured Pruning | åŸºäºæƒé‡å¹…å€¼å’Œæ¿€æ´»å‰ªæï¼Œç²¾åº¦é«˜ä½†éœ€ç¨€ç–å†…æ ¸æ”¯æŒ |
| **SliceGPT (25%)** | Structured Pruning | åˆ é™¤æƒé‡çŸ©é˜µè¡Œåˆ—ï¼Œä¾èµ– PCA å˜æ¢ |
| **ShortGPT (25%)** | Structured Pruning | åŸºäºéšè—çŠ¶æ€ç›¸ä¼¼æ€§è¯†åˆ«å†—ä½™å±‚ |

> æ‰€æœ‰ baseline è°ƒæ•´è‡³ä¸ POP ç›¸å½“çš„ FLOPs å‡å°‘æ°´å¹³è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 & 2ï¼‰

#### âœ… å‡†ç¡®ç‡è¡¨ç°ï¼ˆAvg Scoreï¼‰

| æ–¹æ³• | Llama-3.1 | Qwen3-VL | Gemma-3 |
|------|-----------|----------|---------|
| Full Model | 70.19 | 74.00 | 63.07 |
| Wanda (30%) | 69.82 | 73.16 | 62.55 |
| SliceGPT (25%) | 34.88 | 32.82 | 18.12 |
| ShortGPT (25%) | â€” | â€” | 15.42 |
| **POP (33%)** | **68.47** | **73.16** | **62.95** |

> ğŸ’¡ **ç»“è®º**ï¼šPOP åœ¨å¤§å¹…å‰ªæä¸‹ä»æ¥è¿‘å…¨æ¨¡å‹ç²¾åº¦ï¼Œè¿œä¼˜äº structured baselineï¼Œåª²ç¾ unstructured æ–¹æ³•ã€‚

#### âš¡ æ¨ç†é€Ÿåº¦æå‡ï¼ˆTTFT Speedupï¼‰

| è¾“å…¥é•¿åº¦ | Llama-3.1 (POP) | Gemma-3 (POP) | Qwen3-VL (POP) |
|----------|------------------|----------------|------------------|
| 32 tokens | 1.22Ã— | 1.02Ã— | ~1.19Ã— |
| 2048 tokens | **1.36Ã—** | **1.37Ã—** | ~1.16Ã— |

> âœ… **é•¿åºåˆ—ä¼˜åŠ¿æ˜æ˜¾**ï¼šéšç€è¾“å…¥å¢é•¿ï¼ŒPOP çš„åŠ é€Ÿæ•ˆæœæ˜¾è‘—å¢å¼ºï¼Œé€‚ç”¨äº compute-bound åœºæ™¯ã€‚

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

ä½¿ç”¨ Qwen3-VL è¿›è¡ŒéªŒè¯ï¼ˆè§ Table 3 & 4ï¼‰ï¼š

| å˜ä½“ | GSM8K | HotpotQA | è¯´æ˜ |
|------|-------|----------|------|
| Full Model | 81.50 | 65.49 | åŸºçº¿ |
| POP (å®Œæ•´) | 80.21 | 63.13 | æ€§èƒ½å‡ ä¹æ— æŸ |
| Shallow Pruning | 0.15 | 0.00 | é”™è¯¯åœ°å‰ªæµ…å±‚ â†’ å®Œå…¨å¤±æ•ˆ |
| Interleaved Pruning | 56.48 | 6.81 | éæ·±å±‚å±‚å‰ªææ— æ•ˆ |
| w/o Indep. KV | 2.05 | 1.18 | ç¼ºå¤± KV â†’ æ— æ³•è®¿é—®å†å² |
| w/o Boundary | 77.33 | 11.45 | æœ€å token å‰ªæ â†’ æ˜¾è‘—é™çº§ |

> âœ… **è®¾è®¡å¿…è¦æ€§éªŒè¯**ï¼š
- å¿…é¡»å‰ª**æ·±å±‚**
- å¿…é¡»ä¿ç•™**ç‹¬ç«‹ KV æŠ•å½±**
- å¿…é¡»å¯ç”¨**è¾¹ç•Œå¤„ç†**

---

### ğŸ“ˆ å‰ªææ¯”ä¾‹æ•æ„Ÿæ€§åˆ†æï¼ˆTable 4ï¼‰

| å‰ªææ¯”ä¾‹ | Speedup | GSM8K | HotpotQA |
|---------|---------|--------|-----------|
| 20%     | 1.19Ã—   | 83.09  | 65.46     |
| 25%     | 1.25Ã—   | 82.34  | 65.81     |
| 33% (default) | **1.37Ã—** | 80.21  | 63.13     |
| 50%     | 1.67Ã—   | 78.54  | 34.69     |
| 60%     | 1.96Ã—   | 38.51  | 5.45      |

> âš ï¸ **å»ºè®®å‰ªæä¸Šé™ä¸º 50%**ï¼Œè¶…è¿‡å°†å¯¼è‡´æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œå°¤å…¶å½±å“é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **Deep layers are critical for decode, but redundant for prefill**  
   æ·±å±‚åœ¨ç½‘ç»œç”Ÿæˆé˜¶æ®µè‡³å…³é‡è¦ï¼Œä½†åœ¨ä¸Šä¸‹æ–‡ç¼–ç é˜¶æ®µè´¡çŒ®æå°ã€‚

2. **Stage-aware pruning breaks the accuracy-efficiency trade-off**  
   åŒºåˆ† prefill å’Œ decode é˜¶æ®µçš„åŠŸèƒ½å·®å¼‚ï¼Œå¯å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„å‰ªæã€‚

3. **POP enables up to 1.37Ã— prefill speedup with minimal accuracy loss**  
   åœ¨å¤šç§ LLM/VLM ä¸Šå‡æœ‰æ•ˆï¼Œä¸”æ— éœ€ retraining æˆ– special hardwareã€‚

4. **Attention mechanism exhibits functional robustness**  
   å³ä½¿éšè—çŠ¶æ€å‡ºç°æ¼‚ç§»ï¼ˆdriftï¼‰ï¼Œattention output ä»ä¿æŒé«˜ä¿çœŸï¼ˆcosine similarity > 0.96ï¼‰ï¼Œè§£é‡Šäº†ä¸ºä½•è·³è¿‡æ·±å±‚ä¸ä¼šå¯¼è‡´å´©æºƒã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

1. **ä¸å‡å°‘å³°å€¼æ˜¾å­˜å ç”¨ï¼ˆpeak VRAM usageï¼‰**  
   decode é˜¶æ®µä»éœ€åŠ è½½å®Œæ•´æ¨¡å‹ï¼Œå› æ­¤ä¸é€‚åˆ memory-bound åœºæ™¯ã€‚

2. **å½“å‰å®ç°åœ¨å•ä½“ç³»ç»Ÿä¸­è¿è¡Œ**  
   æœªé€‚é… disaggregated inference systemsï¼ˆå¦‚åˆ†ç¦» prefill/decoce å®ä¾‹ï¼‰ï¼Œé™åˆ¶é›†ç¾¤çº§ååä¼˜åŒ–æ½œåŠ›ã€‚

3. **é™æ€å‰ªæç­–ç•¥**  
   å½“å‰ä¸ºå›ºå®šæ¨¡å¼ï¼ˆå‰ªæœ€å 1/3 å±‚ï¼‰ï¼Œæœªæ¥å¯æ¢ç´¢åŠ¨æ€è‡ªé€‚åº”å‰ªæã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

- å°† POP é›†æˆåˆ° **disaggregated serving system**ï¼ˆå¦‚ DistServeï¼‰ä¸­ï¼Œå……åˆ†å‘æŒ¥é˜¶æ®µåˆ†ç¦»ä¼˜åŠ¿ã€‚
- æ¢ç´¢ **dynamic POP**ï¼šæ ¹æ®è¾“å…¥é•¿åº¦æˆ–å†…å®¹è‡ªåŠ¨è°ƒæ•´å‰ªææ·±åº¦ã€‚
- ç»“åˆå…¶ä»–ä¼˜åŒ–æŠ€æœ¯ï¼šå¦‚ **token pruning**, **sparse attention**, **quantization**ï¼Œå½¢æˆè”åˆåŠ é€Ÿæ–¹æ¡ˆã€‚
- æ‰©å±•è‡³æ›´å¤šæ¶æ„ï¼ˆå¦‚ encoder-decoder modelsï¼‰å’Œåº”ç”¨åœºæ™¯ï¼ˆå¦‚ RLHF æ¨ç†ï¼‰ã€‚

---

> **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **POP é€šè¿‡â€œåªåœ¨ prefill é˜¶æ®µå‰ªææ·±å±‚â€çš„ stage-aware ç­–ç•¥ï¼Œåœ¨å‡ ä¹ä¸å¤±æ•ˆçš„æƒ…å†µä¸‹å®ç°äº†é«˜è¾¾ 1.37Ã— çš„ prefill åŠ é€Ÿï¼Œä¸ºé«˜æ•ˆå¤§æ¨¡å‹æ¨ç†æä¾›äº†ç®€å•ã€å®ç”¨ä¸”å¯æ’æ‹”çš„æ–°èŒƒå¼ã€‚**

</details>

---

### 7. [MatGPTQ: Accurate and Efficient Post-Training Matryoshka Quantization](https://arxiv.org/abs/2602.03537)

**Authors**: Maximilian Kleinegger, Elvir Crn\v{c}evi\'c, Dan Alistarh  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.03537v1  

#### Abstract
Matryoshka Quantization (MatQuant) is a recent quantization approach showing that a single integer-quantized model can be served across multiple precisions, by slicing the most significant bits (MSB) at inference time. This enables a single checkpoint to cover a wide range of memory and latency budg...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMatGPTQ: Accurate and Efficient Post-Training Matryoshka Quantization

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
Matryoshka Quantization (MatQuant) æ˜¯ä¸€ç§æ–°å…´çš„é‡åŒ–æ–¹æ³•ï¼Œå…è®¸é€šè¿‡åœ¨æ¨ç†æ—¶â€œåˆ‡ç‰‡â€æœ€é«˜æœ‰æ•ˆä½ï¼ˆMSBï¼‰æ¥ä»ä¸€ä¸ªé«˜ç²¾åº¦æ•´æ•°é‡åŒ–æ¨¡å‹ä¸­ç”Ÿæˆå¤šä¸ªä½ç²¾åº¦å˜ä½“ï¼Œä»è€Œå®ç°**å•ä¸ªæ£€æŸ¥ç‚¹æ”¯æŒå¤šç²¾åº¦éƒ¨ç½²**ã€‚ç„¶è€Œï¼ŒåŸå§‹ MatQuant å­˜åœ¨ä»¥ä¸‹å…³é”®é™åˆ¶ï¼š
- ä¾èµ–æ˜‚è´µçš„ **Quantization-Aware Training (QAT)** æˆ– block-wise QATï¼ˆå¦‚ OmniQuantï¼‰ï¼Œè€Œéé«˜æ•ˆçš„ one-shot **Post-Training Quantization (PTQ)**ï¼›
- ç¼ºä¹å¼€æºå®ç°å’Œé«˜æ•ˆæ¨ç†å†…æ ¸ï¼ˆkernelï¼‰æ”¯æŒï¼›
- æœªæä¾›é’ˆå¯¹å¼‚æ„ï¼ˆéå‡åŒ€ï¼‰å±‚é—´æ¯”ç‰¹åˆ†é…çš„ä¼˜åŒ–ç­–ç•¥ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **MatGPTQ**ï¼Œé¦–ä¸ªé«˜æ•ˆä¸”å¼€æºçš„ **Post-Training Matryoshka Quantization** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **å¤šç²¾åº¦è”åˆä¼˜åŒ–ç›®æ ‡**ï¼šå°† Matryoshka é‡åŒ–å»ºæ¨¡ä¸ºä¸€ä¸ªæ–°çš„å¤šç²¾åº¦ GPTQ ç›®æ ‡å‡½æ•°ï¼Œå¼•å…¥ **bit-slicing** å’Œ **cross-bit error compensation**ï¼Œä½¿å¾—å•æ¬¡ PTQ è¿‡ç¨‹å³å¯ç”Ÿæˆä¸€ä¸ªèƒ½è¢«åˆ‡ç‰‡åˆ°å¤šç§ç›®æ ‡æ¯”ç‰¹å®½åº¦ï¼ˆå¦‚ {2,3,4,6,8}ï¼‰çš„â€œåµŒå¥—â€çˆ¶æ¨¡å‹ã€‚
- **å¼‚æ„æ¯”ç‰¹æœç´¢æœºåˆ¶**ï¼šé›†æˆåŸºäºè¿›åŒ–ç®—æ³•çš„ **EvoPress**ï¼Œåœ¨å›ºå®šå†…å­˜é¢„ç®—ä¸‹è‡ªåŠ¨æœç´¢æ¯å±‚æœ€ä¼˜çš„éå‡åŒ€æ¯”ç‰¹é…ç½®ï¼ˆMix-and-Matchï¼‰ï¼Œè¿›ä¸€æ­¥æå‡ç²¾åº¦-å‹ç¼©æƒè¡¡ã€‚
- **é«˜æ•ˆ CUDA å†…æ ¸å®ç°**ï¼šé¦–æ¬¡æä¾›äº†æ”¯æŒ 2â€“8 bit å¤šé‡åµŒå¥—ç²¾åº¦çš„ **GPU æ¨ç†å†…æ ¸**ï¼Œæ”¯æŒåŠ¨æ€åˆ‡ç‰‡å’Œæ··åˆç²¾åº¦æ‰§è¡Œï¼Œå¹¶å·²é›†æˆè‡³ vLLM ç­‰ä¸»æµæ¨ç†æ¡†æ¶ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | MatQuant (Nair et al., 2025) | MatGPTQ (æœ¬æ–‡) |
|------|----------------------------|----------------|
| **é‡åŒ–æ–¹å¼** | QAT / Block-wise QAT (OmniQuant) | One-shot PTQ (GPTQ-style) |
| **æ•ˆç‡** | é«˜è®¡ç®—æˆæœ¬ï¼Œéœ€å¾®è°ƒ | ä»…éœ€å°è§„æ¨¡æ ¡å‡†é›†ï¼Œå¿«é€Ÿå®Œæˆ |
| **å¼€æºä¸å¯ç”¨æ€§** | æ— å…¬å¼€ä»£ç /æ¨¡å‹/å†…æ ¸ | å®Œå…¨å¼€æºï¼Œå«è®­ç»ƒã€é‡åŒ–ã€æ¨ç†å…¨æµç¨‹ |
| **çµæ´»æ€§** | ä»…æ”¯æŒ FFN å±‚ï¼Œæœ‰é™æ¯”ç‰¹ç»„åˆ | æ”¯æŒ FFN + Attentionï¼Œä»»æ„æ¯”ç‰¹é›†åˆ |
| **å¼‚æ„é‡åŒ–** | ä»…æœ‰æ¦‚å¿µï¼Œæ— å®é™…æ–¹æ¡ˆ | æä¾› EvoPress æœç´¢ï¼Œå®ç° Pareto è¶…ä¼˜é…ç½® |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **æ ¡å‡†æ•°æ®é›†ï¼ˆCalibration Dataï¼‰**ï¼š
  - `FineWeb-Edu` æ•°æ®é›†ï¼Œä½¿ç”¨ 2,097,152 ä¸ª tokenï¼ˆ1024 ä¸ªæ ·æœ¬ Ã— åºåˆ—é•¿åº¦ 2048ï¼‰è¿›è¡Œ PTQ æ ¡å‡†ã€‚
- **è¯„ä¼°æ•°æ®é›†**ï¼š
  - **Perplexity**: `WikiText2` æµ‹è¯•é›†
  - **Zero-shot Accuracy**: `ARC-c`, `ARC-e`, `HellaSwag`, `PIQA`, `Winogrande`

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹**ï¼š
  - `LLaMA 3.1-8B`ï¼ˆInstruct / Baseï¼‰
  - `Qwen3-8B` / `Qwen3-14B`
  - `Phi-3-Medium`
- **ç›®æ ‡æ¯”ç‰¹å®½åº¦é›†åˆ**ï¼š`R = {3, 4, 8}`ï¼Œä¸»æ¯”ç‰¹å®½åº¦ `c = 8`
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **PPL (Perplexity)**ï¼šè¶Šä½è¶Šå¥½
  - **Task Average Accuracy (%)**ï¼šè¶Šé«˜è¶Šå¥½
  - **End-to-End Latency / Tokens per second**ï¼šè¡¡é‡æ¨ç†é€Ÿåº¦
- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA RTX A6000ï¼ŒCUDA 12.4

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **GPTQ**ï¼šæ ‡å‡† one-shot PTQ æ–¹æ³•ï¼Œåœ¨å„ç›®æ ‡æ¯”ç‰¹å®½åº¦ä¸Šç‹¬ç«‹é‡åŒ–ä½œä¸ºåŸºå‡†ã€‚
- **MatQuant (OmniQuant)**ï¼šåŸºäº block-wise QAT çš„ MatQuant å®ç°ï¼ˆä½œè€…å¤ç°ï¼‰ï¼Œä»…é‡åŒ– FFN å±‚ã€‚
- **MatGPTQ-EP**ï¼šMatGPTQ + EvoPress å¼‚æ„æœç´¢ç‰ˆæœ¬ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆä»¥ LLaMA 3.1-8B-Instruct ä¸ºä¾‹ï¼‰

| Bitwidth | Method       | PPL  | Task Avg (%) | Rel. Speedup |
|----------|--------------|------|---------------|-------------|
| 16       | FP16         | 7.23 | 74.00         | 1.00Ã—       |
| 8        | GPTQ         | 7.23 | 73.96         | ~2.0Ã—       |
| 8        | **MatGPTQ**  | 7.46 | **73.35**     | ~2.0Ã—       |
| 4        | GPTQ         | 7.84 | 72.65         | ~2.6Ã—       |
| 4        | **MatGPTQ**  | 7.82 | **72.62**     | ~2.6Ã—       |
| 3        | GPTQ         | 11.85| 64.57         | ~2.9Ã—       |
| 3        | **MatGPTQ**  | 10.16| **67.56**     | ~2.9Ã—       |
| 6 (interp)| GPTQ       | 7.23 | 73.28         | â€”           |
| 6 (interp)| **MatGPTQ** | 7.55 | **73.11**     | â€”           |

> æ³¨ï¼š3-bit ä¸‹ MatGPTQ ç›¸å¯¹ GPTQ **å¹³å‡æå‡ 1.34% å‡†ç¡®ç‡**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **vs. GPTQ**ï¼š
  - åœ¨ 4â€“8 bit åŒºé—´ï¼Œæ€§èƒ½å‡ ä¹æŒå¹³ï¼ˆå·®å¼‚ < 0.7%ï¼‰ï¼Œå¯è§†ä¸ºæ— æŸã€‚
  - åœ¨ **3-bit æç«¯å‹ç¼©åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äº GPTQ**ï¼ˆ+1.34% å¹³å‡å‡†ç¡®ç‡ï¼‰ï¼Œå½’å› äºå¤šç²¾åº¦è”åˆä¼˜åŒ–å¸¦æ¥çš„æ­£åˆ™åŒ–æ•ˆåº”ã€‚
  - æ’å€¼èƒ½åŠ›ä¼˜ç§€ï¼š6-bit åˆ‡ç‰‡æ¨¡å‹æ€§èƒ½æ¥è¿‘æ˜¾å¼è®­ç»ƒçš„ 6-bit GPTQ æ¨¡å‹ã€‚
- **vs. MatQuant (OmniQuant)**ï¼š
  - åœ¨ FFN-only è®¾ç½®ä¸‹ï¼ŒMatGPTQ **å…¨é¢è¶…è¶Š** MatQuant æ‰€æœ‰æ¯”ç‰¹å®½åº¦ä¸‹çš„è¡¨ç°ï¼ˆè§ Table 2ï¼‰ã€‚
  - ä¾‹å¦‚åœ¨ Gemma2 9B ä¸Šï¼Œ8-bit ä»»åŠ¡å¹³å‡å‡†ç¡®ç‡é«˜å‡º **0.4%**ï¼Œ3-bit é«˜å‡º **0.02%**ï¼Œä¸”æ— éœ€å¾®è°ƒã€‚
- **vs. å¼‚æ„æ–¹æ³•**ï¼š
  - **MatGPTQ-EP**ï¼ˆ+EvoPressï¼‰åœ¨å¹³å‡æ¯”ç‰¹å®½ 3 çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç‡ä»å¯è¾¾ç”šè‡³è¶…è¿‡å‡åŒ€ 3-bit GPTQï¼Œè¯æ˜å…¶ Pareto è¶…ä¼˜æ€§ã€‚

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰æŸå¤±æƒé‡ $ \lambda_r $ å½±å“ï¼ˆTable 12ï¼‰
- æµ‹è¯•ä¸åŒ $ \lambda_3:\lambda_4:\lambda_8 $ æƒé‡ç»„åˆï¼ˆå¦‚ (1,1,1), (1,0.5,0.1) ç­‰ï¼‰
- å‘ç° **uniform weighting ($\lambda_r=1$)** è¡¨ç°æœ€ä½³æˆ–æ¥è¿‘æœ€ä¼˜
- ç»“è®ºï¼šé™¤éç‰¹å®šåœºæ™¯å¼ºè°ƒæŸæ¯”ç‰¹ç²¾åº¦ï¼Œå¦åˆ™ç»Ÿä¸€åŠ æƒæ˜¯ç®€å•æœ‰æ•ˆçš„é»˜è®¤é€‰æ‹©ã€‚

#### ï¼ˆ2ï¼‰å¼‚æ„é‡åŒ–æ•ˆæœï¼ˆFigure 1 & Table 20ï¼‰
- ä½¿ç”¨ EvoPress å¯ç”Ÿæˆ **2.5â€“4 bit ä¹‹é—´çš„è¿ç»­æœ‰æ•ˆæ¨¡å‹**
- åœ¨ **2.5-bit** å³å¯è·å¾—å¯ç”¨æ¨¡å‹ï¼ˆTask Avg â‰ˆ 59.3%ï¼‰
- **3.0-bit MatGPTQ-EP** å‡†ç¡®ç‡ï¼ˆ68.58%ï¼‰**è¶…è¿‡** å‡åŒ€ 3-bit GPTQï¼ˆ64.57%ï¼‰å’Œ MatGPTQï¼ˆ67.56%ï¼‰
- æ˜¾ç¤º Mix-and-Match ç­–ç•¥èƒ½æœ‰æ•ˆç¼“è§£ä½æ¯”ç‰¹é‡åŒ–æŸå¤±ã€‚

#### ï¼ˆ3ï¼‰æ¨ç†æ€§èƒ½ï¼ˆTable 3 & Figure 4ï¼‰
| Bitwidth | Median Latency (ms) | Tokens/s | Rel. Speedup |
|----------|----------------------|----------|-------------|
| 16       | 23.53                | 42.5     | 1.00Ã—       |
| 4        | 9.13                 | 109.3    | 2.58Ã—       |
| 3        | 8.03                 | 124.4    | 2.93Ã—       |
| 2        | 7.24                 | 138.0    | **3.25Ã—**   |

- **3-bit æ¨ç†æ¥è¿‘ 3Ã— åŠ é€Ÿ**ï¼Œ2-bit è¾¾åˆ° **3.25Ã—**ï¼Œæ˜¾ç¤ºå®é™…éƒ¨ç½²ä»·å€¼ã€‚
- å†…æ ¸åœ¨ memory-bound åœºæ™¯ä¸‹æé€Ÿ **3â€“5.6Ã—**ï¼ˆAppendix Hï¼‰ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **MatGPTQ æˆåŠŸå®ç°äº†é«˜æ•ˆçš„ one-shot Matryoshka PTQ**ï¼Œæ— éœ€ QAT å³å¯ç”Ÿæˆé«˜è´¨é‡çš„å¤šç²¾åº¦åˆ‡ç‰‡æ¨¡å‹ã€‚
2. âœ… åœ¨ **é«˜æ¯”ç‰¹ï¼ˆ4â€“8 bitï¼‰ä¿æŒä¸ GPTQ ç›¸å½“ç²¾åº¦**ï¼Œåœ¨ **ä½æ¯”ç‰¹ï¼ˆ3 bitï¼‰æ˜¾è‘—ä¼˜äº GPTQ**ï¼ŒéªŒè¯äº†å¤šç²¾åº¦è”åˆä¼˜åŒ–çš„æ­£åˆ™åŒ–ä¼˜åŠ¿ã€‚
3. âœ… æ”¯æŒ **bit-width interpolation**ï¼ˆå¦‚ 6-bitï¼‰ï¼Œå³ä½¿æœªæ˜¾å¼ä¼˜åŒ–ä¹Ÿèƒ½ä¿æŒè‰¯å¥½æ€§èƒ½ã€‚
4. âœ… **MatGPTQ-EP** é€šè¿‡ EvoPress æœç´¢å®ç° **Pareto è¶…ä¼˜çš„å¼‚æ„é…ç½®**ï¼Œå¯åœ¨æ›´ä½å¹³å‡æ¯”ç‰¹ä¸‹è¾¾åˆ°æ›´é«˜ç²¾åº¦ã€‚
5. âœ… è‡ªç ” CUDA å†…æ ¸å¸¦æ¥ **é«˜è¾¾ 3.25Ã— çš„ç«¯åˆ°ç«¯æ¨ç†åŠ é€Ÿ**ï¼Œå…·å¤‡å®é™…éƒ¨ç½²å¯è¡Œæ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰ä»…æ”¯æŒ **æ•´æ•°é‡åŒ–**ï¼Œå°šæœªæ‰©å±•è‡³æµ®ç‚¹æ ¼å¼ï¼ˆå¦‚ FP8ï¼‰ï¼Œåè€…åœ¨ MSB åˆ‡ç‰‡æ—¶é¢ä¸´æŒ‡æ•°çº§æ•°å€¼å˜åŒ–æŒ‘æˆ˜ã€‚
- æä½æ¯”ç‰¹ï¼ˆå¦‚ 1-bitï¼‰ä»éš¾ä»¥ä¿è¯å¯ç”¨æ€§ï¼Œå½“å‰ GPTQ ç®—æ³•åœ¨æ ‡å‡† group size ä¸‹è¡¨ç°ä¸ä½³ã€‚
- åŠ¨æ€ token çº§æ¯”ç‰¹åˆ†é…å°è¯•å¤±è´¥ï¼ˆAppendix Cï¼‰ï¼Œè¡¨æ˜æ­¤ç±»è·¯ç”±éœ€åœ¨è®­ç»ƒé˜¶æ®µæ˜¾å¼å­¦ä¹ ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢å°† Matryoshka ç›®æ ‡ä¸æ›´é€‚åˆè¶…ä½ç²¾åº¦çš„æ–¹æ³•ç»“åˆï¼ˆå¦‚ SqueezeLLMã€Any-Precision LLMï¼‰ã€‚
- æ‰©å±•è‡³ **floating-point quantization**ï¼Œè§£å†³ MSB åˆ‡ç‰‡çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜ã€‚
- æ”¯æŒæ›´å¤š GPU æ¶æ„ï¼ˆå¦‚ Hopperã€Blackwellï¼‰ï¼Œå¹¶æ·±åº¦é›†æˆè‡³ vLLMã€TensorRT-LLM ç­‰ç”Ÿäº§çº§æ¨ç†å¼•æ“ã€‚
- ç ”ç©¶è®­ç»ƒæ—¶å¼•å…¥åŠ¨æ€æ¯”ç‰¹è·¯ç”±æœºåˆ¶ï¼Œä»¥å®ç°æ›´ç»†ç²’åº¦çš„è‡ªé€‚åº”æ¨ç†ã€‚

---

> **ä»£ç åœ°å€**ï¼š[https://github.com/IST-DASLab/MatGPTQ](https://github.com/IST-DASLab/MatGPTQ)  
> **ä¸€å¥è¯æ€»ç»“**ï¼šMatGPTQ å°† Matryoshka Quantization ä»æ˜‚è´µçš„ QAT èŒƒå¼è½¬å˜ä¸ºé«˜æ•ˆã€å¼€æºã€å®ç”¨çš„ PTQ æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†â€œå•æ¨¡å‹ã€å¤šç²¾åº¦â€éƒ¨ç½²çš„æ™®åŠã€‚

</details>

---

### 8. [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)

**Authors**: Yang Liu, Chuan Zhou, Yancheng Chen, Shuai Zhang, Xixun Lin, Xiaoqing Wang  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.01090v1  

#### Abstract
Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šHard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»„åˆä¼˜åŒ–ï¼ˆCombinatorial Optimization, COï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨¡å¼è¯†åˆ«å’Œåºåˆ—ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶æœ¬è´¨æ˜¯**æ— çº¦æŸçš„ç”Ÿæˆæ¨¡å‹**ï¼Œæ— æ³•ä¿è¯ç”Ÿæˆè§£æ»¡è¶³ç¡¬æ€§çº¦æŸæ¡ä»¶ï¼ˆå¦‚å®¹é‡é™åˆ¶ã€å“ˆå¯†é¡¿å›è·¯ç­‰ï¼‰ã€‚è¿™å¯¼è‡´ç°æœ‰æ–¹æ³•çš„å¯è¡Œæ€§ç‡ï¼ˆFeasibility Rateï¼‰å§‹ç»ˆä½äº100%ï¼Œä¸¥é‡é™åˆ¶äº†å…¶åœ¨ç‰©æµã€åˆ¶é€ ã€åº”æ€¥å“åº”ç­‰é«˜é£é™©åœºæ™¯ä¸­çš„å®é™…éƒ¨ç½²ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ï¼šFALCON**
æœ¬æ–‡æå‡ºäº† **FALCON**ï¼ˆFeasibility-Aware Language-based Combinatorial Optimization with Adaptive Inferenceï¼‰ï¼Œä¸€ä¸ªé¦–ä¸ªèƒ½æä¾›**100%å¯è¡Œæ€§ä¿è¯**çš„LLM-based COæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸‰å±‚ååŒæœºåˆ¶ï¼š

#### **(i) Grammar-Constrained Decodingï¼ˆè¯­æ³•çº¦æŸè§£ç ï¼‰**
- ä½¿ç”¨**ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•**ï¼ˆCFGï¼‰å¯¹æ¯ä¸ªCOé—®é¢˜å®šä¹‰è¾“å‡ºæ ¼å¼ï¼ˆå¦‚TSPè·¯å¾„ã€CVRPå¤šè·¯çº¿ç­‰ï¼‰ã€‚
- åœ¨è§£ç è¿‡ç¨‹ä¸­é€šè¿‡**ä¸‹æ¨è‡ªåŠ¨æœº**ï¼ˆPDAï¼‰åŠ¨æ€å±è”½éæ³•tokenï¼Œç¡®ä¿è¾“å‡º**è¯­æ³•æ­£ç¡®**ï¼ˆformat validityï¼‰ã€‚
- ä¾‹å¦‚ï¼šé˜²æ­¢èŠ‚ç‚¹ç´¢å¼•è¶Šç•Œã€æ‹¬å·ä¸åŒ¹é…ç­‰é—®é¢˜ã€‚

#### **(ii) Feasibility Repair Layerï¼ˆå¯è¡Œæ€§ä¿®å¤å±‚ï¼‰**
- è®¾è®¡**é—®é¢˜ç‰¹å®šçš„ä¿®å¤ç®—å­**ï¼ˆrepair operatorï¼‰ï¼Œå°†ä»»ä½•è¿åè¯­ä¹‰çº¦æŸçš„è§£è½¬æ¢ä¸ºå¯è¡Œè§£ã€‚
- æ»¡è¶³ä¸‰å¤§æ€§è´¨ï¼š
  1. **å¯è¡Œæ€§**ï¼ˆFeasibilityï¼‰ï¼šè¾“å‡ºå¿…ä¸ºå¯è¡Œè§£ï¼›
  2. **å¹‚ç­‰æ€§**ï¼ˆIdempotenceï¼‰ï¼šå·²å¯è¡Œè§£ä¸å˜ï¼›
  3. **æœ‰ç•Œå±€éƒ¨æ€§**ï¼ˆBounded Localityï¼‰ï¼šä¿®æ”¹å¹…åº¦ä¸è¿åç¨‹åº¦æˆæ­£æ¯”ã€‚
- ä¾‹å¦‚ï¼šCVRPä¸­æ‹†åˆ†è¶…è½½è·¯çº¿ï¼›MISä¸­ç§»é™¤ç›¸é‚»é¡¶ç‚¹ã€‚

#### **(iii) Adaptive Best-of-N Samplingï¼ˆè‡ªé€‚åº”é‡‡æ ·ï¼‰**
- åŠ¨æ€è°ƒæ•´é‡‡æ ·æ•°é‡ $N$ï¼ŒåŸºäº**è§£ä¸€è‡´æ€§**ï¼ˆconsistencyï¼‰ä¼°è®¡å®ä¾‹éš¾åº¦ã€‚
- ä½¿ç”¨**è´å¶æ–¯ç½®ä¿¡åº¦**å†³å®šæ˜¯å¦æå‰ç»ˆæ­¢ï¼Œé¿å…å¯¹ç®€å•å®ä¾‹è¿‡åº¦é‡‡æ ·ã€‚
- æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ç°æœ‰æ–¹æ³• | FALCON |
|------|--------|--------|
| å¯è¡Œæ€§ä¿è¯ | è½¯ç›®æ ‡ï¼Œä¸å¯æ§ï¼ˆé€šå¸¸<100%ï¼‰ | **ç†è®ºå¯è¯100%** |
| è§£è´¨é‡ | ä¾èµ–å¥–åŠ±å¡‘å½¢ï¼Œä¿¡å·ç¨€ç– | å¼•å…¥**BOPO**è®­ç»ƒï¼Œç›‘ç£æ›´å¯†é›† |
| æ¨ç†æ•ˆç‡ | å›ºå®šé‡‡æ ·æ•°ï¼Œèµ„æºæµªè´¹ | è‡ªé€‚åº”åˆ†é…è®¡ç®—èµ„æº |
| æ³›åŒ–æ€§ | éœ€äººå·¥æ ‡æ³¨åå¥½ | å®Œå…¨è‡ªåŠ¨åŒ–è®­ç»ƒ |

æ­¤å¤–ï¼Œæå‡º **BOPO**ï¼ˆBest-anchored Objective-guided Preference Optimizationï¼‰è®­ç»ƒç®—æ³•ï¼š
- åˆ©ç”¨â€œæœ€ä¼˜é”šå®šâ€ç­–ç•¥æ„å»ºåå¥½å¯¹ï¼›
- æŒ‰**ç›®æ ‡å€¼å·®è·**åŠ æƒæ¢¯åº¦æ›´æ–°ï¼Œä½¿å·®è·å¤§çš„æ ·æœ¬è´¡çŒ®æ›´å¤§ï¼›
- ç†è®ºè¯æ˜å…¶æ”¶æ•›é€Ÿç‡ä¸º $O(1/\sqrt{T})$ï¼Œä¼˜äºGRPOç­‰åŸºçº¿ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
åœ¨**ä¸ƒä¸ªNP-hard COé—®é¢˜**ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¦†ç›–ä¸‰å¤§é¢†åŸŸï¼š

| é—®é¢˜ç±»åˆ« | å…·ä½“é—®é¢˜ | è§„æ¨¡ | åˆ†å¸ƒ | å‚è€ƒæ±‚è§£å™¨ |
|---------|--------|------|------|-----------|
| **Routing** | TSP, CVRP, OP | 10â€“100èŠ‚ç‚¹ | Uniform, GM | LKH-3, COMPASS |
| **Graph** | MIS, MVC | 50â€“500èŠ‚ç‚¹ | ER, BA | Gurobi |
| **Scheduling** | PFSP, JSSP | 10â€“100ä½œä¸š / 6â€“30ä½œä¸š | Taillard | QIG, OR-Tools |

æ‰€æœ‰æ•°æ®é›†å‡ä¸ºåˆæˆç”Ÿæˆï¼Œè®­ç»ƒé›†50ä¸‡å®ä¾‹ï¼Œæµ‹è¯•é›†å„100ä¸ªã€‚

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**

#### **ä¸»å¹²æ¨¡å‹**
- åŸºç¡€æ¨¡å‹ï¼š`Qwen2.5-7B`
- å¾®è°ƒæ–¹å¼ï¼šLoRAï¼ˆr=64ï¼‰
- è®­ç»ƒä¸¤é˜¶æ®µï¼š
  1. **SFT**ï¼šç›‘ç£å¾®è°ƒäºä¸“å®¶è§£ï¼›
  2. **BOPO**ï¼šåŸºäºåå¥½ä¼˜åŒ–è¿›ä¸€æ­¥ç²¾ç‚¼ã€‚

#### **æ¨ç†é…ç½®**
- æœ€å°é‡‡æ ·æ•° $N_{\min}=8$
- æœ€å¤§é‡‡æ ·æ•° $N_{\max}=64$
- ç½®ä¿¡é˜ˆå€¼ $T=0.85$
- æ¸©åº¦ $T=0.7$

#### **è¯„ä¼°æŒ‡æ ‡**
| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **Feasibility (%)** | ç”Ÿæˆè§£æ»¡è¶³æ‰€æœ‰çº¦æŸçš„æ¯”ä¾‹ |
| **Optimality Gap (%)** | $(f(\hat{x}) - f(x^*)) / f(x^*) \times 100\%$ï¼Œç›¸å¯¹äºå‚è€ƒæœ€ä¼˜è§£ |
| **Inference Time (s)** | å•å®ä¾‹å¹³å‡æ¨ç†æ—¶é—´ |

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
å…±å››ç±»åŸºçº¿ï¼š
1. **é€šç”¨LLMé›¶æ ·æœ¬**ï¼šGPT-4o, Claude-3.5, Llama-3.3-70B, Qwen2.5-72B
2. **æ¨ç†å¢å¼ºLLM**ï¼šGPT-oç³»åˆ—, DeepSeek-R1
3. **LLMä¼˜åŒ–æ–¹æ³•**ï¼šOPRO, LMEA, PHP, SGE
4. **ç¥ç»COæ±‚è§£å™¨**ï¼šSFT-only, GRPO, LLMCoSolver

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ªTable 2ï¼‰**

| æ–¹æ³• | å¹³å‡å¯è¡Œæ€§ (%) | å¹³å‡Gap (%) | æ¨ç†æ—¶é—´ (s) |
|------|----------------|-------------|--------------|
| **FALCON (N=1)** | **100** | 1.89 | 5.8 |
| **FALCON (N=8)** | **100** | 0.95 | 10.4 |
| **FALCON (Adaptive)** | **100** | **0.92** | **6.3** |
| LLMCoSolver (N=8) | 94â€“100 | 1.03â€“10.94 | 9.8 |
| GRPO | 91â€“98 | 1.34â€“8.27 | 5.6 |
| GPT-4o | 6â€“88 | 11.70â€“212.00 | 5.3 |

> âœ… **FALCONåœ¨æ‰€æœ‰7ä¸ªé—®é¢˜ä¸Šå®ç°100%å¯è¡Œæ€§**  
> âœ… **è‡ªé€‚åº”ç‰ˆæœ¬ä»¥æ›´å°‘é‡‡æ ·è¾¾åˆ°æœ€ä½³æˆ–æ¬¡ä¼˜Gap**

### **ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ç»“æœ**
- **å¯è¡Œæ€§æ–¹é¢**ï¼š
  - é€šç”¨LLMå¯è¡Œæ€§æä½ï¼ˆå¦‚CVRPä»…6%ï¼‰ï¼Œå³ä½¿å¼ºæ¨¡å‹ï¼ˆClaude-3.5-Haikuï¼‰ä¹Ÿéš¾ç¨³å®šã€‚
  - LLMCoSolverè™½æ¥è¿‘100%ï¼Œä½†åœ¨MISä¸Šä»ä¸º94%ï¼ˆN=8ï¼‰ï¼Œ**æ— æ³•ç†è®ºä¿è¯**ã€‚
  - **FALCONé¦–æ¬¡å®ç°100%å¯è¡Œæ€§ä¸”å¯è¯æ˜**ã€‚

- **è§£è´¨é‡æ–¹é¢**ï¼š
  - FALCON(N=8)åœ¨5/7é—®é¢˜ä¸Šå–å¾—**æœ€ä¼˜Gap**ï¼Œå…¶ä½™ä¸ºç¬¬äºŒã€‚
  - è‡ªé€‚åº”é‡‡æ ·ä¸‹ï¼Œå¹³å‡Gapä»…æ¯”å›ºå®šN=8é«˜0.03%ï¼Œä½†èŠ‚çœ**55%æ¨ç†æ—¶é—´**ã€‚

- **å¯¹æ¯”ä¼ ç»Ÿå¯å‘å¼ï¼ˆTable 3ï¼‰**
  - è¿œè¶…Nearest Neighborã€FIFOç­‰ç®€å•è§„åˆ™ï¼›
  - ä¼˜äºACOã€OR-Toolsç­‰ç»å…¸æ±‚è§£å™¨ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡å®ä¾‹ä¸Šè¡¨ç°æ›´é²æ£’ã€‚

### **æ¶ˆèå®éªŒç»“æœï¼ˆTable 4 & Figure 2ï¼‰**

| é…ç½® | TSP Gap | CVRP Gap | å¯è¡Œæ€§ (%) |
|------|--------|----------|------------|
| **å®Œæ•´FALCON** | 0.92 | 3.52 | 100 |
| w/o Grammar | 0.95 | 3.67 | 100 |
| w/o Repair | 0.89 | 3.41 | 94 / 85 |
| w/o Adaptive | 0.91 | 3.48 | 100 |
| w/o BOPO | 1.85 | 5.89 | 100 |
| SFT Only | 2.30 | 6.02 | 89 / 59 |

#### **å…³é”®å‘ç°**ï¼š
- **ä¿®å¤å±‚æœ€å…³é”®**ï¼šç§»é™¤åå¯è¡Œæ€§éª¤é™ï¼ŒéªŒè¯å…¶ä¸å¯æ›¿ä»£æ€§ã€‚
- **BOPOæ˜¾è‘—æå‡æ€§èƒ½**ï¼šç›¸æ¯”GRPOï¼Œåœ¨CVRPä¸ŠGapé™ä½29%ï¼ŒMISå¯è¡Œæ€§æå‡12ä¸ªç™¾åˆ†ç‚¹ã€‚
- **è¯­æ³•çº¦æŸä½œç”¨è¾ƒå°ä½†å¿…è¦**ï¼šè™½å½±å“ä¸å¤§ï¼Œä½†ä¿éšœäº†è§£æç¨³å®šæ€§ã€‚
- **è‡ªé€‚åº”é‡‡æ ·é«˜æ•ˆ**ï¼šå‡å°‘çº¦58%é‡‡æ ·é‡ï¼ˆè§Table 5ï¼‰ï¼Œç»´æŒé«˜è´¨é‡ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **åˆ†ç¦»å¼æ¶æ„æœ‰æ•ˆ**ï¼šå°†â€œè¯­æ³•æœ‰æ•ˆæ€§â€ä¸â€œè¯­ä¹‰å¯è¡Œæ€§â€åˆ†ç¦»å¤„ç†ï¼Œåˆ†åˆ«ç”±**è¯­æ³•çº¦æŸè§£ç **å’Œ**ä¿®å¤å±‚**è´Ÿè´£ï¼Œæ—¢èƒ½ä¿è¯ä¸¥æ ¼å¯è¡Œæ€§ï¼Œåˆä¸å½±å“ç”Ÿæˆçµæ´»æ€§ã€‚
2. **100%å¯è¡Œæ€§å¯è¾¾æˆä¸”æ— ä»£ä»·**ï¼šé€šè¿‡ä¿®å¤æ“ä½œå¼•å…¥çš„è´¨é‡æŸå¤±æå°ï¼ˆå› BOPOè®­ç»ƒå‡ºçš„è§£æœ¬èº«é«˜åº¦å¯è¡Œï¼‰ï¼Œå®ç°äº†**å¯é æ€§ä¸æ€§èƒ½çš„åŒèµ¢**ã€‚
3. **BOPOä¼˜äºä¼ ç»Ÿåå¥½å­¦ä¹ **ï¼šç›®æ ‡æ„ŸçŸ¥åŠ æƒæœºåˆ¶æä¾›äº†æ›´å¯†é›†ã€æ›´æœ‰åŒºåˆ†åº¦çš„å­¦ä¹ ä¿¡å·ï¼Œå°¤å…¶åœ¨å¤æ‚çº¦æŸé—®é¢˜ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚
4. **è‡ªé€‚åº”é‡‡æ ·æå‡æ•ˆç‡**ï¼šåˆ©ç”¨è§£ä¸€è‡´æ€§ä½œä¸ºéš¾åº¦ä»£ç†ï¼Œè‡ªåŠ¨è°ƒèŠ‚è®¡ç®—é¢„ç®—ï¼Œå®ç°â€œæ˜“é¢˜å¿«è§£ï¼Œéš¾é¢˜æ·±æœâ€ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- **ä¿®å¤å¯èƒ½å¼•å…¥åå·®**ï¼šè™½ç„¶ç†è®ºä¸Šä¿®å¤ä¸ä¼šåŠ£åŒ–å¤ªå¤šï¼Œä½†åœ¨æç«¯æƒ…å†µä¸‹ï¼ˆå¦‚åˆå§‹è§£å®Œå…¨æ··ä¹±ï¼‰ï¼Œä¿®å¤åçš„è§£å¯èƒ½è¿œç¦»å…¨å±€æœ€ä¼˜ã€‚
- **ä¾èµ–è‰¯å¥½åˆå§‹åˆ†å¸ƒ**ï¼šè‹¥SFTé˜¶æ®µæœªèƒ½å­¦åˆ°åŸºæœ¬ç»“æ„ï¼Œä¿®å¤è´Ÿæ‹…è¿‡é‡ï¼Œå½±å“æ•ˆç‡ã€‚
- **æ‰©å±•æ€§æŒ‘æˆ˜**ï¼šå½“å‰ä¿®å¤ç®—å­ä¸ºæ‰‹å·¥è®¾è®¡ï¼Œéš¾ä»¥æ³›åŒ–åˆ°å…¨æ–°é—®é¢˜ç±»å‹ï¼›æœªæ¥éœ€æ¢ç´¢è‡ªåŠ¨æ„é€ ä¿®å¤é€»è¾‘çš„æ–¹æ³•ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **è‡ªåŠ¨åŒ–ä¿®å¤ç®—å­ç”Ÿæˆ**ï¼šç»“åˆç¨‹åºåˆæˆæŠ€æœ¯ï¼Œä»é—®é¢˜æè¿°è‡ªåŠ¨ç”Ÿæˆä¿®å¤å‡½æ•°ã€‚
2. **ç«¯åˆ°ç«¯å¯å¾®ä¿®å¤**ï¼šæ¢ç´¢å°†ä¿®å¤è¿‡ç¨‹åµŒå…¥æ¨¡å‹å†…éƒ¨ï¼Œå®ç°æ¢¯åº¦ä¼ æ’­ï¼Œè¿›ä¸€æ­¥ç¼©å°Gapã€‚
3. **å¤šç›®æ ‡ä¸éšæœºCO**ï¼šæ‰©å±•è‡³å¸¦ä¸ç¡®å®šæ€§çš„ç°å®åœºæ™¯ï¼ˆå¦‚éšæœºéœ€æ±‚CVRPï¼‰ã€‚
4. **çœŸå®ä¸–ç•Œéƒ¨ç½²éªŒè¯**ï¼šåœ¨å·¥ä¸šçº§ç‰©æµè°ƒåº¦ã€èŠ¯ç‰‡å¸ƒçº¿ç­‰ç³»ç»Ÿä¸­å®æµ‹æ•ˆæœã€‚

---

> ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> FALCONé¦–æ¬¡å®ç°äº†**ç†è®ºå¯è¯100%å¯è¡Œæ€§**çš„LLM-basedç»„åˆä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡**è¯­æ³•çº¦æŸ + ä¿®å¤å±‚ + è‡ªé€‚åº”é‡‡æ · + BOPOè®­ç»ƒ**å››é‡åˆ›æ–°ï¼Œåœ¨ä¿æŒç«äº‰åŠ›è§£è´¨é‡çš„åŒæ—¶ï¼Œè§£å†³äº†ç”Ÿæˆæ¨¡å‹ç”¨äºå…³é”®ç³»ç»Ÿçš„æ ¹æœ¬å¯ä¿¡æ€§é—®é¢˜ã€‚

</details>

---

### 9. [Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control](https://arxiv.org/abs/2602.02987)

**Authors**: Ruihan Lin, Zezhen Ding, Zean Han, Jiheng Zhang  
**Category**: cs.DC  
**Published**: 2026-02-04  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.02987v1  

#### Abstract
Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \emph{prefill} phase that processes...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šLarge-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
è¯¥è®ºæ–‡é’ˆå¯¹å¤§è§„æ¨¡ **Large Language Model (LLM)** æ¨ç†æœåŠ¡ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜â€”â€”**prefill-decode contention** å’Œ **workload heterogeneity**ï¼Œæå‡ºäº†ä¸€å¥—ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆã€‚

- **Prefill-Decode Contention**ï¼šLLMæ¨ç†åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š
  - **Prefill**ï¼šè®¡ç®—å¯†é›†å‹ï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥æç¤ºï¼ˆpromptï¼‰ã€‚
  - **Decode**ï¼šå†…å­˜å¸¦å®½å¯†é›†å‹ï¼Œé€ä¸ªç”Ÿæˆè¾“å‡ºtokenã€‚
  å½“ä¸¤è€…å…±äº«åŒä¸€GPUèµ„æºæ—¶ï¼Œprefillä»»åŠ¡ä¼šæ˜¾è‘—æ‹–æ…¢å…±å­˜çš„decodeä»»åŠ¡ï¼Œå½¢æˆèµ„æºç«äº‰ã€‚
  
- **Workload Heterogeneity**ï¼šä¸åŒåº”ç”¨ï¼ˆå¦‚æ‘˜è¦ã€åˆ›æ„å†™ä½œã€é—®ç­”ï¼‰å…·æœ‰å·®å¼‚å·¨å¤§çš„è¾“å…¥/è¾“å‡ºé•¿åº¦åˆ†å¸ƒï¼Œå¯¼è‡´èµ„æºåˆ©ç”¨ä¸å‡è¡¡ã€‚ä¾‹å¦‚ï¼š
  - æ‘˜è¦ä»»åŠ¡ï¼šé•¿è¾“å…¥ã€çŸ­è¾“å‡ºã€‚
  - åˆ›æ„å†™ä½œï¼šçŸ­è¾“å…¥ã€é•¿è¾“å‡ºã€‚

ä¼ ç»Ÿè°ƒåº¦ç­–ç•¥éš¾ä»¥åœ¨å¼‚æ„è´Ÿè½½ä¸‹å®ç°æœ€ä¼˜æ”¶ç›Šä¸æœåŠ¡è´¨é‡ï¼ˆSLIï¼‰çš„å¹³è¡¡ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•æˆ–æ–°æ€è·¯

è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäº **stochastic control** å’Œ **queueing network** çš„ç†è®ºæ¡†æ¶ï¼Œæ ¸å¿ƒæ˜¯ **gate-and-route æ§åˆ¶æ¶æ„**ï¼š

#### ï¼ˆ1ï¼‰å¤šç±»å¤šæœåŠ¡å™¨æ’é˜Ÿç½‘ç»œå»ºæ¨¡
- å°†æ¯ä¸ªGPUå»ºæ¨¡ä¸ºä¸¤ç§æ¨¡å¼ï¼š
  - **Mixed Mode**ï¼šè¿è¡Œä¸€ä¸ªprefillä»»åŠ¡ + æœ€å¤š $B-1$ ä¸ªdecodeä»»åŠ¡ã€‚
  - **Solo Mode**ï¼šä»…è¿è¡Œæœ€å¤š $B$ ä¸ªdecodeä»»åŠ¡ã€‚
- æœåŠ¡é€Ÿç‡æ˜¯**çŠ¶æ€ä¾èµ–**çš„ï¼šmixed modeä¸‹çš„decodeé€Ÿåº¦å› prefillçš„è®¡ç®—å‹åŠ›è€Œå˜æ…¢ã€‚

#### ï¼ˆ2ï¼‰æµä½“è¿‘ä¼¼ï¼ˆFluid Approximationï¼‰ä¸ç¨³æ€ä¼˜åŒ–
- åœ¨å¤§è§„æ¨¡GPUé›†ç¾¤ï¼ˆ$n \to \infty$ï¼‰ä¸‹ï¼Œé‡‡ç”¨**many-server fluid limit**åˆ†æç³»ç»Ÿè¡Œä¸ºã€‚
- å°†éšæœºç³»ç»Ÿæ”¶æ•›åˆ°ç¡®å®šæ€§æµä½“æ¨¡å‹ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ª**ç¨³æ€çº¿æ€§è§„åˆ’ï¼ˆSteady-State Linear Program, LPï¼‰**ã€‚
- è¯¥LPæ±‚è§£æœ€ä¼˜çš„ï¼š
  - GPUåœ¨mixed/soloæ¨¡å¼é—´çš„å®¹é‡åˆ†é…ã€‚
  - å„è¯·æ±‚ç±»åˆ«çš„prefillå ç”¨ç‡ç›®æ ‡ã€‚
  - decodeä»»åŠ¡çš„è·¯ç”±æ¯”ä¾‹ã€‚

#### ï¼ˆ3ï¼‰å¯å®æ–½çš„æ§åˆ¶ç­–ç•¥ï¼šGate-and-Route Policy
- **Prefill Gate**ï¼ˆå‡†å…¥é—¨æ§ï¼‰ï¼š
  - åŠ¨æ€è°ƒèŠ‚prefillä»»åŠ¡çš„å‡†å…¥ï¼Œç¡®ä¿å„ç±»åˆ«prefillçš„å®é™…å ç”¨ç‡é€¼è¿‘LPæ±‚å¾—çš„ç›®æ ‡å€¼ã€‚
  - é‡‡ç”¨è´Ÿåé¦ˆæœºåˆ¶ï¼šä¼˜å…ˆæ¥çº³å½“å‰å ç”¨ä½äºç›®æ ‡çš„ç±»åˆ«ã€‚
- **Decode Router**ï¼ˆè§£ç å¤´ç”±å™¨ï¼‰ï¼š
  - å†³å®šå®Œæˆprefillçš„ä»»åŠ¡åº”è¿›å…¥mixedè¿˜æ˜¯solo GPUè¿›è¡Œdecodeã€‚
  - åœ¨åŸºç¡€ç‰ˆæœ¬ä¸­é‡‡ç”¨work-conservingç­–ç•¥ï¼›åœ¨SLI-awareç‰ˆæœ¬ä¸­å¼•å…¥éšæœºåŒ–è·¯ç”±ä»¥ç²¾ç¡®åŒ¹é…ç›®æ ‡åˆ†å¸ƒã€‚

#### ï¼ˆ4ï¼‰æ”¯æŒå¤šç§è®¡è´¹æ¨¡å¼ä¸SLIçº¦æŸ
- æ”¯æŒ **bundled charging**ï¼ˆè¯·æ±‚å®Œæˆåç»Ÿä¸€æ”¶è´¹ï¼‰å’Œ **separate charging**ï¼ˆprefillå’Œdecodeåˆ†åˆ«è®¡è´¹ï¼‰ã€‚
- å¯å°† **Service Level Indicators (SLIs)** å¦‚å…¬å¹³æ€§ï¼ˆfairnessï¼‰ã€å»¶è¿Ÿï¼ˆlatencyï¼‰ä½œä¸ºçº¦æŸæˆ–æƒ©ç½šé¡¹çº³å…¥ä¼˜åŒ–ç›®æ ‡ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹é¢ | æœ¬æ–‡æ–¹æ³• | ä¼ ç»Ÿæ–¹æ³• |
|------|---------|--------|
| **ç†è®ºä¿è¯** | è¯æ˜äº†ç­–ç•¥åœ¨**many-GPUæé™ä¸‹çš„æ¸è¿‘æœ€ä¼˜æ€§**ï¼ˆasymptotic optimalityï¼‰ | å¤šä¸ºå¯å‘å¼è§„åˆ™ï¼Œç¼ºä¹ç†è®ºä¿éšœ |
| **èµ„æºåˆ©ç”¨ç‡** | æ˜¾å¼å»ºæ¨¡prefill-decode contentionï¼Œé¿å…decodeè¢«è¿‡åº¦å‹åˆ¶ | å¿½è§†çŠ¶æ€ä¾èµ–çš„æœåŠ¡é€Ÿç‡å˜åŒ– |
| **å¼‚æ„è´Ÿè½½å¤„ç†** | å¤šç±»è°ƒåº¦ï¼ŒåŠ¨æ€è°ƒæ•´å„ç±»åˆ«å‡†å…¥ï¼Œæœ€å¤§åŒ–token-based revenue | é™æ€ä¼˜å…ˆçº§ï¼ˆå¦‚FCFSã€SRTï¼‰ï¼Œæ— æ³•é€‚åº”æ··åˆè´Ÿè½½ |
| **SLIæ”¯æŒ** | å¯çµæ´»é›†æˆå…¬å¹³æ€§ã€å»¶è¿Ÿç­‰çº¦æŸï¼Œé‡åŒ–å…¶â€œå½±å­ä»·æ ¼â€ï¼ˆshadow priceï¼‰ | ç¼ºä¹ç³»ç»Ÿæ€§æƒè¡¡æœºåˆ¶ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **çœŸå®LLMæ¨ç†é…ç½®**ï¼šåŸºäº **Qwen-4B** å’Œ **Qwen-8B** æ¨¡å‹ï¼Œåœ¨ **NVIDIA A100-SXM4-40GB** GPUä¸Šæµ‹é‡è¿­ä»£æ—¶é—´ã€‚
- **å·¥ä½œè´Ÿè½½æ¨¡æ‹Ÿ**ï¼šæœªç›´æ¥ä½¿ç”¨çœŸå®ç”¨æˆ·æ—¥å¿—ï¼Œè€Œæ˜¯åŸºäºæ–‡çŒ®ï¼ˆå¦‚Sun et al. 2024, Zheng et al. 2024ï¼‰ç»Ÿè®¡ç‰¹å¾æ„é€ ä¸¤ç±»ä»£è¡¨æ€§ä»»åŠ¡ï¼š
  - **Class 0 (Decode-Heavy)**ï¼š`P=300`, `D=1000`ï¼ˆå¦‚ä»£ç ç”Ÿæˆï¼‰
  - **Class 1 (Prefill-Heavy)**ï¼š`P=3000`, `D=400`ï¼ˆå¦‚è®ºæ–‡æ‘˜è¦ï¼‰

### å®éªŒè®¾ç½®
- **ä»¿çœŸå¹³å°**ï¼šäº‹ä»¶é©±åŠ¨ï¼ˆevent-drivenï¼‰ç¦»æ•£äº‹ä»¶ä»¿çœŸã€‚
- **å‚æ•°æ ¡å‡†**ï¼š
  - é€šè¿‡å®æµ‹æ‹Ÿåˆå¾—åˆ°mixed iteration timeæ¨¡å‹ï¼š$T_{\text{mix}} = \alpha + \beta C$
    - Qwen-8B: $\alpha \approx 0.0174$, $\beta \approx 6.2 \times 10^{-5}$
    - Qwen-4B: $\alpha \approx 0.0152$, $\beta \approx 3.6 \times 10^{-5}$
  - Solo decodeé€Ÿç‡ï¼šQwen-8Bçº¦45.45 tokens/sã€‚
- **æ‰¹å¤§å°**ï¼š$B = 16$
- **GPUè§„æ¨¡**ï¼šæµ‹è¯• $n = 5, 20, 50, 200, 500$ï¼ŒéªŒè¯**scalability**ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **Per-GPU Revenue**ï¼šå•ä½GPUçš„å¹³å‡æ”¶ç›Šï¼ˆtoken-based pricingï¼‰ã€‚
- **Queue Lengths**ï¼šprefillå’Œdecodeé˜Ÿåˆ—é•¿åº¦ã€‚
- **Occupancy Convergence**ï¼šå®é™…èµ„æºå ç”¨æ˜¯å¦æ”¶æ•›è‡³æµä½“æ¨¡å‹é¢„æµ‹çš„ç›®æ ‡å€¼ã€‚
- **Shadow Price of SLIs**ï¼šä¸ºæ»¡è¶³æŸé¡¹SLIéœ€ç‰ºç‰²å¤šå°‘æ”¶ç›Šã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿åç§° | æè¿° |
|--------|------|
| **FI-WSP (First-Come-First-Served + Immediate)** | å·¥ä¸šç•Œæ ‡å‡†ï¼ˆå¦‚Sarathi-Serveï¼‰ï¼ŒFCFSå‡†å…¥ï¼Œdecodeç´§éšprefillåœ¨åŒä¸€slotæ‰§è¡Œ |
| **GI-WSP** | å¼•å…¥æœ¬æ–‡çš„Gateæ§åˆ¶ï¼Œä½†ä»é‡‡ç”¨Immediateæ‰§è¡Œæ¨¡å¼ |
| **GF-WSP** | ä½¿ç”¨Gateæ§åˆ¶ï¼Œä½†decodeè·¯ç”±é‡‡ç”¨FCFSï¼ˆégreedyï¼‰ |
| **FG-SP** | ç§»é™¤Gateæ§åˆ¶ï¼Œä»…ä¿ç•™é™æ€è§„åˆ’+Greedyè·¯ç”± |
| **OPT** | æµä½“æ¨¡å‹çš„ç†è®ºä¸Šé™ï¼ˆfluid optimumï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
- **æ”¶å…¥æ”¶æ•›æ€§**ï¼šéšç€GPUæ•°é‡å¢åŠ ï¼Œ**per-GPU revenue** è¶‹è¿‘äºæµä½“æœ€ä¼˜å€¼ $R^*$ã€‚
  - åœ¨ $n=500$ æ—¶ï¼Œå·²éå¸¸æ¥è¿‘ç†è®ºä¸Šé™ã€‚
- **é˜Ÿåˆ—ç¨³å®šæ€§**ï¼šprefillå’Œdecodeé˜Ÿåˆ—é•¿åº¦ç¨³å®šåœ¨æµä½“é¢„æµ‹æ°´å¹³é™„è¿‘ï¼Œæ— æŒç»­ç§¯å‹ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **GG-SPï¼ˆæœ¬æ–‡æ–¹æ³•ï¼‰æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿**ï¼š
  - ç›¸æ¯”å·¥ä¸šæ ‡å‡† **FI-WSP**ï¼Œ**æå‡å¯è¾¾30%ä»¥ä¸Š**ã€‚
  - å³ä½¿åªå¼•å…¥Gateï¼ˆGI-WSPï¼‰ï¼Œä¹Ÿæœ‰æ˜æ˜¾æ”¹è¿›ã€‚
  - ä½†å•ç‹¬ä½¿ç”¨é™æ€è§„åˆ’ï¼ˆFG-SPï¼‰åè€Œå¯èƒ½å› ç¼ºä¹æµé‡è°ƒæ§è€Œå¯¼è‡´å±€éƒ¨å¤±è¡¡ã€‚

> å›¾7æ˜¾ç¤ºï¼šGG-SPåœ¨å„ç§ç¡¬ä»¶å’Œè´Ÿè½½ç»„åˆä¸‹å‡å–å¾—æœ€é«˜ä¸”æœ€ç¨³å®šçš„æ”¶ç›Šã€‚

### æ¶ˆèå®éªŒç»“æœ
- **Gateçš„é‡è¦æ€§**ï¼šç§»é™¤Gateï¼ˆFG-SPï¼‰å¯¼è‡´æ”¶ç›Šå¤§å¹…ä¸‹é™ï¼Œè¯´æ˜**åŠ¨æ€å‡†å…¥æ§åˆ¶å¯¹é˜²æ­¢è¿‡è½½è‡³å…³é‡è¦**ã€‚
- **Routerçš„é‡è¦æ€§**ï¼šä½¿ç”¨FCFSè€ŒéGreedyè·¯ç”±ï¼ˆGF-WSPï¼‰ä¼šé™ä½æ•ˆç‡ï¼Œè¯´æ˜**decodeä¼˜å…ˆçš„work-conservingç­–ç•¥æ›´ä¼˜**ã€‚
- **é™æ€è§„åˆ’çš„ä½œç”¨**ï¼šç»“åˆGateä¸é™æ€è§„åˆ’æ‰èƒ½é‡Šæ”¾æœ€å¤§æ½œåŠ›ã€‚

### SLIæ•æ„Ÿæ€§åˆ†æï¼ˆPareto Frontierï¼‰
- **Prefill Fairness** çš„å½±å­ä»·æ ¼æé«˜ï¼šå¼ºåˆ¶å„ç±»åˆ«prefillå ç”¨å‡è¡¡ä¼šå¯¼è‡´æ˜¾è‘—æ”¶ç›ŠæŸå¤±ï¼ˆç»“æ„æ€§é”™é…ï¼‰ã€‚
- **Decode Fairness** çš„å½±å­ä»·æ ¼æä½ï¼šä¸€æ—¦è¯·æ±‚è¿›å…¥ç³»ç»Ÿï¼Œè°ƒæ•´å…¶decodeé€Ÿåº¦å¯¹æ€»æ”¶ç›Šå½±å“å°ã€‚
- **TPOT (Time Per Output Token)**ï¼šå½“ç›®æ ‡å»¶è¿Ÿæ¥è¿‘solo decodeæé™ï¼ˆ~0.022sï¼‰æ—¶ï¼Œæ”¶ç›Šæ€¥å‰§ä¸‹é™ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **Gate-and-Routeç­–ç•¥åœ¨å¤§è§„æ¨¡ä¸‹æ˜¯æ¸è¿‘æœ€ä¼˜çš„**ï¼šç†è®ºä¸ä»¿çœŸå®éªŒä¸€è‡´éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚
2. âœ… **Prefillé˜¶æ®µæ˜¯ç“¶é¢ˆ**ï¼šå¯¹å…¶æ–½åŠ ä¸¥æ ¼çš„å…¬å¹³æ€§çº¦æŸä»£ä»·é«˜æ˜‚ï¼Œåº”å…è®¸ç³»ç»Ÿæ ¹æ®æ”¶ç›Šæœ€ä¼˜åŸåˆ™åŠ¨æ€è°ƒæ•´ç±»åˆ«æ··åˆã€‚
3. âœ… **Decodeé˜¶æ®µæ›´çµæ´»**ï¼šå¯åœ¨ä¸å½±å“æ”¶ç›Šçš„å‰æä¸‹å®ç°è¾ƒé«˜å…¬å¹³æ€§ã€‚
4. âœ… **Separate Chargingå¯èƒ½å¯¼è‡´æ¿€åŠ±é”™ä½**ï¼šé¼“åŠ±è¿‡åº¦æ¥çº³prefillä»¥è·å–å³æ—¶æ”¶å…¥ï¼ŒåŠ å‰§ä¸‹æ¸¸decodeæ‹¥å¡ã€‚å»ºè®®**æŒ‰ç«¯åˆ°ç«¯å®Œæˆè®¡è´¹ï¼ˆbundled chargingï¼‰æ¥æŒ‡å¯¼è°ƒåº¦**ã€‚
5. âœ… **ç¡¬ä»¶å‡çº§å†³ç­–å¯é‡åŒ–**ï¼šé€šè¿‡åˆ†æ$\beta$ï¼ˆprefillè®¡ç®—æˆæœ¬ï¼‰å’Œ$B$ï¼ˆæ‰¹å¤§å°ï¼‰å¯¹æ”¶ç›Šçš„å½±å“ï¼Œå¯æŒ‡å¯¼æŠ•èµ„æ–¹å‘ï¼ˆå¦‚ä¼˜å…ˆé™ä½$\beta$ï¼‰ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å‡è®¾æŒ‡æ•°æœåŠ¡æ—¶é—´**ï¼šå®é™…decodeé•¿åº¦å¯èƒ½ä¸æœä»æŒ‡æ•°åˆ†å¸ƒï¼ˆå°½ç®¡å®éªŒè¡¨æ˜MLEæ‹Ÿåˆå°šå¯æ¥å—ï¼‰ã€‚
- **å¿½ç•¥å°¾éƒ¨å»¶è¿Ÿï¼ˆtail latencyï¼‰**ï¼šæµä½“æ¨¡å‹å…³æ³¨å¹³å‡è¡Œä¸ºï¼Œå¯¹p99å»¶è¿Ÿç­‰SLIç¼ºä¹ç›´æ¥æ§åˆ¶ã€‚
- **åŒæ„åŸºç¡€è®¾æ–½å‡è®¾**ï¼šæœªè€ƒè™‘å¤šä»£GPUæ··åˆéƒ¨ç½²åœºæ™¯ã€‚
- **é™æ€chunk size**ï¼šprefill chunkå¤§å°å›ºå®šï¼Œæœªè”åˆä¼˜åŒ–ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ¨å¹¿è‡³ä¸€èˆ¬æœåŠ¡æ—¶é—´åˆ†å¸ƒ**ï¼šä½¿ç”¨measure-valuedè¿‡ç¨‹å»ºæ¨¡ã€‚
2. **å‘å±•æ‰©æ•£è¿‘ä¼¼ï¼ˆdiffusion approximationï¼‰**ï¼šç”¨äºåˆ†æéšæœºæ³¢åŠ¨å’Œå°¾éƒ¨å»¶è¿Ÿã€‚
3. **å¼‚æ„åŸºç¡€è®¾æ–½è°ƒåº¦**ï¼šæ”¯æŒè·¨ä¸åŒå‹å·GPUçš„ååŒè°ƒåº¦ã€‚
4. **è”åˆä¼˜åŒ–chunk sizeä¸è°ƒåº¦ç­–ç•¥**ã€‚
5. **åœ¨çº¿å­¦ä¹ æ¡†æ¶**ï¼šåº”å¯¹æœªçŸ¥æˆ–åŠ¨æ€å˜åŒ–çš„å·¥ä½œè´Ÿè½½åˆ†å¸ƒã€‚

--- 

> **æ€»ç»“**ï¼šæœ¬æ–‡é¦–æ¬¡å°†**stochastic controlç†è®º**ç³»ç»Ÿåº”ç”¨äºå¤§è§„æ¨¡LLMæ¨ç†è°ƒåº¦ï¼Œæå‡ºäº†å…¼å…·**ç†è®ºæœ€ä¼˜æ€§**ä¸**å·¥ç¨‹å¯å®æ–½æ€§**çš„ **gate-and-route** æ¶æ„ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€å¯æ§ã€å•†ä¸šå‹å¥½çš„LLMæœåŠ¡å¹³å°æä¾›äº†åšå®åŸºç¡€ã€‚

</details>

---

### 10. [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)

**Authors**: Minhua Lin, Hanqing Lu, Zhan Shi, Bing He, Rui Mao, Zhiwei Zhang, Zongyu Wu, Xianfeng Tang, Hui Liu, Zhenwei Dai, Xiang Zhang, Suhang Wang, Benoit Dumoulin, Jian Pei  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.00359v1  

#### Abstract
As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but do...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠPosition: Agentic Evolution is the Path to Evolving LLMsã€‹æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜

å½“å‰çš„ **Large Language Models (LLMs)** åœ¨ä»é™æ€è®­ç»ƒç¯å¢ƒéƒ¨ç½²åˆ°å¼€æ”¾ã€åŠ¨æ€çš„çœŸå®ä¸–ç•Œåœºæ™¯æ—¶é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§æŒ‘æˆ˜ï¼š**train-deploy gap**ï¼ˆè®­ç»ƒ-éƒ¨ç½²ç¯å¢ƒå·®è·ï¼‰ã€‚  
- æ¨¡å‹åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒï¼Œæ— æ³•é¢„è§çœŸå®ç¯å¢ƒä¸­ä¸æ–­å˜åŒ–çš„APIã€æ¥å£ã€ç”¨æˆ·éœ€æ±‚å’Œåˆ†å¸ƒåç§»ã€‚
- ç°æœ‰çš„é€‚åº”æ–¹æ³•ï¼ˆå¦‚å‚æ•°å¾®è°ƒæˆ–å¯å‘å¼è®°å¿†ç§¯ç´¯ï¼‰ç¼ºä¹**æˆ˜ç•¥èƒ½åŠ¨æ€§ï¼ˆstrategic agencyï¼‰**ï¼Œå¯¼è‡´æ”¹è¿›ä¸å¯æŒç»­ã€ä¸å¯æ§ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

è®ºæ–‡æå‡º **Agentic Evolutionï¼ˆèƒ½åŠ¨æ¼”åŒ–ï¼‰** èŒƒå¼ï¼Œå¹¶æ„å»ºäº†é€šç”¨æ¡†æ¶ **A-Evolve**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> å°†æ¨¡å‹çš„éƒ¨ç½²æœŸè¿›åŒ–è¿‡ç¨‹æœ¬èº«è§†ä¸ºä¸€ä¸ªç”±â€œ**Evolver Agent**â€é©±åŠ¨çš„ã€ç›®æ ‡å¯¼å‘çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œè€Œéå›ºå®šæµæ°´çº¿ã€‚

#### åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š
- **æå‡º Agentic Evolution æ–°èŒƒå¼**ï¼šå°†æ¼”åŒ–ï¼ˆevolutionï¼‰ä»è¢«åŠ¨ã€é™æ€æ›´æ–°æå‡ä¸ºè‡ªä¸»ã€æœ‰ç›®æ ‡çš„å†³ç­–è¿‡ç¨‹ã€‚
- **å®šä¹‰ä¸‰å¤§åŸåˆ™**ï¼š
  1. **Goal-Oriented Principleï¼ˆç›®æ ‡å¯¼å‘ï¼‰**ï¼šä¸»åŠ¨è¯Šæ–­å¤±è´¥åŸå› ï¼Œå®šä½å¯ä¿®å¤ç»„ä»¶ã€‚
  2. **Autonomy Principleï¼ˆè‡ªä¸»æ€§ï¼‰**ï¼šå†³å®šä½•æ—¶æ›´æ–°ã€æ˜¯å¦æäº¤æ›´æ–°ï¼Œé¿å…ç›²ç›®ä¿®æ”¹ã€‚
  3. **Compositional Principleï¼ˆç»„åˆæ€§ï¼‰**ï¼šç”Ÿæˆæ¨¡å—åŒ–ã€å¯éªŒè¯çš„æŒä¹…åŒ–æ„ä»¶ï¼ˆå¦‚å·¥å…·ã€æŠ€èƒ½ã€æµ‹è¯•ï¼‰ã€‚
- **æå‡º Evolution-Scaling Hypothesisï¼ˆæ¼”åŒ–å¯æ‰©å±•æ€§å‡è®¾ï¼‰**ï¼š
  > æ¨¡å‹çš„é€‚åº”èƒ½åŠ›éšç€åˆ†é…ç»™æ¼”åŒ–è¿‡ç¨‹çš„ **evolution-time compute** å¢åŠ è€Œç³»ç»Ÿæ€§æå‡ï¼Œæ„æˆç»§ training-time å’Œ inference-time ä¹‹åçš„**ç¬¬ä¸‰æ¡æ‰©å±•è½´ï¼ˆscaling axisï¼‰**ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³•ç±»å‹ | å…¸å‹ä»£è¡¨ | ç¼ºé™· | A-Evolve çš„ä¼˜åŠ¿ |
|--------|--------|------|----------------|
| **Parametric Fine-tuning** | Test-time training, LoFiT | æ˜“å‘ç”Ÿ **catastrophic forgetting**ï¼Œæ›´æ–°ä¸é€æ˜ï¼Œéš¾å®¡è®¡ |
| **Non-parametric Heuristic** | APE, AWM, Promptbreeder | ä¾èµ–â€œappend-and-retrieveâ€ï¼Œå¯¼è‡´ **context saturation**ï¼Œæ”¹è¿›æ”¶ç›Šé€’å‡ | é€šè¿‡ç»“æ„åŒ–æ„ä»¶å’ŒéªŒè¯æœºåˆ¶å®ç°**æŒä¹…ã€å¯å¤ç”¨çš„èƒ½åŠ›æ²‰æ·€** |
| **A-Evolveï¼ˆæœ¬æ–‡ï¼‰** | â€” | â€” | æ”¯æŒ**æœ‰å› è¯Šæ–­ã€è‡ªä¸»å†³ç­–ã€æ¨¡å—åŒ–åˆæˆã€å®‰å…¨éªŒè¯**ï¼Œå®ç°å¯æŒç»­ã€å¯æ‰©å±•çš„å¼€æ”¾åŸŸé€‚åº” |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†

- **AppWorld** (Trivedi et al., 2024)ï¼šä¸€ä¸ªç”¨äºè¯„ä¼°å·¥å…·ä½¿ç”¨å‹æ™ºèƒ½ä½“çš„åŸºå‡†å¹³å°ã€‚
  - åŒ…å« 9 ä¸ªæ—¥å¸¸åº”ç”¨ï¼ˆå¦‚ Amazonã€Spotifyã€Venmoï¼‰å’Œ 457 ä¸ª APIã€‚
  - æ¨¡æ‹Ÿçº¦ 100 åè™šæ„ç”¨æˆ·çš„æ•°å­—æ´»åŠ¨ï¼Œæ”¯æŒå¤æ‚äº¤äº’ä»»åŠ¡ã€‚
  - ä»»åŠ¡éœ€è·¨å¤šä¸ªåº”ç”¨ã€å¤šæ­¥ API è°ƒç”¨å®Œæˆï¼Œå…·æœ‰çœŸå®ä¾èµ–å…³ç³»ã€‚
- **ä»»åŠ¡åˆ’åˆ†**ï¼š
  - è®­ç»ƒé›†ï¼š50 ä¸ªä»»åŠ¡ï¼ˆç”¨äºæ¼”åŒ–å­¦ä¹ ï¼‰
  - æµ‹è¯•é›†ï¼š50 ä¸ªä»»åŠ¡ï¼ˆæ¥è‡ª `test-normal` åˆ†å‰²ï¼‰

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### è¯„ä¼°åè®®
- å›ºå®š **solve-time compute é¢„ç®—**ï¼ˆæœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ã€æ­¥éª¤æ•°æˆ– token æ•°ï¼‰
- å›ºå®š **evolve-time compute é¢„ç®—**ï¼ˆæ¯ episode æœ€å¤§ token å’Œå·¥å…·è°ƒç”¨æ•°ï¼‰
- æ‰€æœ‰æ–¹æ³•åœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹è¿›è¡Œå…¬å¹³æ¯”è¾ƒ

#### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ | è¯´æ˜ |
|------|------|------|
| **TGC (Task Goal Completion)** | æˆåŠŸå®Œæˆçš„ä»»åŠ¡æ¯”ä¾‹ï¼ˆæ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ï¼‰ | è¡¡é‡æœ€ç»ˆæˆåŠŸç‡ |
| **APT (Average Passed Tests)** | æ¯ä¸ªä»»åŠ¡å¹³å‡é€šè¿‡çš„å•å…ƒæµ‹è¯•æ¯”ä¾‹ | è¡¡é‡æ¸è¿›å¼èƒ½åŠ›æå‡ï¼Œåæ˜ éƒ¨åˆ†æˆåŠŸ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿æ–¹æ³• | ç±»å‹ | æè¿° |
|---------|------|------|
| **Vanilla** | æ— æ¼”åŒ– | ç›´æ¥è°ƒç”¨ solverï¼Œæ— ä»»ä½•æŒä¹…åŒ–æ›´æ–° |
| **APE** (Zhou et al., 2022) | å¯å‘å¼æç¤ºæ¼”åŒ– | åŸºäºæœç´¢çš„ prompt è¿›åŒ–ï¼Œé€šè¿‡ä»»åŠ¡è¯„åˆ†é€‰æ‹©å€™é€‰æŒ‡ä»¤ |
| **AWM** (Wang et al., 2024) | ç»éªŒè®°å¿† | ä»å†å²è½¨è¿¹ä¸­æå–å¯é‡ç”¨çš„å·¥ä½œæµå¹¶å­˜å‚¨ |
| **A-Evolve** | æœ¬æ–‡æ–¹æ³• | åŸºäºèƒ½åŠ¨æ¼”åŒ–çš„æ¡†æ¶ï¼ŒåŒ…å«è¯Šæ–­ã€è§„åˆ’ã€æ›´æ–°ã€éªŒè¯å››é˜¶æ®µ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆTab. 1ï¼‰

| Method | Claude Haiku 4.5 (TGC / APT) | Gemini 3 Flash (TGC / APT) |
|--------|-------------------------------|----------------------------|
| Vanilla | 32% / 51.16 | 56% / 80.45 |
| APE     | 30% / 56.00 | 52% / 84.00 |
| AWM     | 46% / 65.76 | 52% / 87.75 |
| **A-Evolve** | **64% / 84.31** | **82% / 92.05** |

> âœ… **A-Evolve åœ¨æ‰€æœ‰ solver ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿**ï¼Œå°¤å…¶å¯¹å°æ¨¡å‹ï¼ˆå¦‚ Haiku 4.5ï¼‰æå‡å·¨å¤§ã€‚

#### å…³é”®å‘ç°ï¼š
- **èƒ½åŠ›æ”¾å¤§æ•ˆåº”ï¼ˆcapability multiplierï¼‰**ï¼šA-Evolve å¯å°†å¼±æ¨¡å‹ï¼ˆHaikuï¼‰çš„æ€§èƒ½æå‡è‡³æ¥è¿‘ç”šè‡³è¶…è¿‡æ›´å¼ºæ¨¡å‹ï¼ˆSonnetï¼‰çš„åŸå§‹æ°´å¹³ã€‚
- **ç¼©å°å®¹é‡å·®è·**ï¼šHaiku + A-Evolveï¼ˆ64% TGCï¼‰ > Vanilla Sonnetï¼ˆ42% TGCï¼‰ï¼Œè¡¨æ˜**ç¨‹åºæ€§èƒ½åŠ›æ¼”åŒ–å¯åª²ç¾æ¨¡å‹è§„æ¨¡æå‡**ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆFig. 3ï¼‰

æ¶ˆèäº† A-Evolve çš„å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

| å˜ä½“ | æè¿° | æ€§èƒ½å½±å“ |
|------|------|----------|
| **A-Evolve/D** | ç§»é™¤ **Diagnosis** | æ›´æ–°ç›²ç›®ï¼Œä»…åšè¡¨é¢ä¿®è¡¥ï¼Œæ€§èƒ½ä¸‹é™æ˜æ˜¾ |
| **A-Evolve/A** | ç§»é™¤ **Analysis Tools** | æ— æ³•è·¨ episode å‘ç°æ¨¡å¼ï¼Œä»…è§£å†³ä¸€æ¬¡æ€§é”™è¯¯ |
| **A-Evolve/P** | ç§»é™¤ **Planning** | æ— æ³•åè°ƒå¤šæ„ä»¶ä¾èµ–ï¼Œæ›´æ–°ä¸ä¸€è‡´ |
| **A-Evolve/V** | ç§»é™¤ **Verification** | æäº¤ç¼ºé™·æ„ä»¶ï¼ˆå¦‚è¯­æ³•é”™è¯¯å·¥å…·ï¼‰ï¼Œå¯¼è‡´**å›å½’å’Œæ±¡æŸ“ä¸Šä¸‹æ–‡**ï¼Œæ€§èƒ½æœ€å·® |

> ğŸ” **å…³é”®å‘ç°**ï¼šå®Œæ•´é—­ç¯ï¼ˆè¯Šæ–­ â†’ è§„åˆ’ â†’ æ›´æ–° â†’ éªŒè¯ï¼‰è‡³å…³é‡è¦ï¼Œå…¶ä¸­ **Verification æ˜¯ç¨³å®šå™¨**ï¼Œé˜²æ­¢èƒ½åŠ›é€€åŒ–ã€‚

---

### æ¼”åŒ–å¯æ‰©å±•æ€§åˆ†æï¼ˆFig. 4ï¼‰

#### ï¼ˆaï¼‰å¢åŠ  **evolution step**ï¼ˆå³ C_evolveï¼‰
- A-Evolve éš compute å¢åŠ **å•è°ƒæå‡**ï¼Œæœªè§é¥±å’Œã€‚
- AWM å¾ˆå¿«è¾¾åˆ°æ€§èƒ½å¤©èŠ±æ¿ã€‚
- æ”¯æŒ **Evolution-Scaling Hypothesis**ï¼šæ›´å¤šæ¼”åŒ–è®¡ç®—å¸¦æ¥æ›´é«˜é€‚åº”ä¸Šé™ã€‚

#### ï¼ˆbï¼‰å¢å¤§ **evolver æ¨¡å‹å°ºå¯¸**
| Evolver | TGC (Haiku Solver) | APT |
|--------|--------------------|-----|
| Haiku 4.5 | ~58% | ~80% |
| Sonnet 4.5 | ~64% | ~84% |
| Opus 4.5 | ~70% | ~88% |

> ğŸ“ˆ æ›´å¤§çš„ evolver èƒ½æ›´å‡†ç¡®è¯Šæ–­ã€ç”Ÿæˆæ›´é²æ£’æ„ä»¶ï¼Œ**éªŒè¯ evolver è‡ªèº«èƒ½åŠ›æ˜¯ C_evolve çš„å…³é”®ç»„æˆéƒ¨åˆ†**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. âœ… **Agentic Evolution æ˜¯çªç ´ train-deploy gap çš„å¿…ç„¶è·¯å¾„**ï¼š
   - ç°æœ‰é™æ€æ–¹æ³•ï¼ˆfine-tuning æˆ– memoryï¼‰æ— æ³•åº”å¯¹å¼€æ”¾ä¸–ç•Œçš„æŒç»­å˜åŒ–ã€‚
   - å¿…é¡»å¼•å…¥å…·æœ‰ç›®æ ‡å¯¼å‘ã€è‡ªä¸»æ€§å’Œç»„åˆæ€§çš„æ¼”åŒ–æœºåˆ¶ã€‚

2. âœ… **æ¼”åŒ–åº”è¢«è§†ä¸ºç¬¬ä¸‰æ¡æ‰©å±•è½´ï¼ˆscaling axisï¼‰**ï¼š
   - é™¤äº† training-time å’Œ inference-time computeï¼Œ**evolution-time compute** åŒæ ·å¯ç³»ç»Ÿæ€§æå‡æ¨¡å‹é€‚åº”èƒ½åŠ›ã€‚

3. âœ… **A-Evolve æ¡†æ¶æœ‰æ•ˆä¸”å¯æ‰©å±•**ï¼š
   - åœ¨ AppWorld ä¸Šæ˜¾è‘—ä¼˜äºå„ç±»åŸºçº¿ã€‚
   - æ”¯æŒä»å°æ¨¡å‹åˆ°å¤§æ¨¡å‹çš„ evolverï¼Œä¸”æ€§èƒ½éš compute å¢åŠ æŒç»­æå‡ã€‚

4. âœ… **éªŒè¯ï¼ˆVerificationï¼‰æ˜¯ä¿éšœé•¿æœŸç¨³å®šæ€§çš„å…³é”®**ï¼š
   - æ— éªŒè¯çš„æ¼”åŒ–ä¼šå¯¼è‡´â€œèƒ½åŠ›æ¼‚ç§»â€å’Œå›å½’ï¼Œç ´åå·²æœ‰åŠŸèƒ½ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§

- **è®¡ç®—å¼€é”€è¾ƒé«˜**ï¼šç»´æŠ¤ evolver agent å’Œæ‰§è¡ŒéªŒè¯éœ€è¦é¢å¤– computeï¼ŒçŸ­æœŸæ•ˆç‡ä½äºè½»é‡çº§å¯å‘å¼æ–¹æ³•ã€‚
- **ä¾èµ–é«˜è´¨é‡åé¦ˆä¿¡å·**ï¼šéœ€è¦ç¯å¢ƒæä¾›å¯è§‚æµ‹çš„é”™è¯¯ã€è½¨è¿¹å’Œå•å…ƒæµ‹è¯•ï¼Œå¦åˆ™éš¾ä»¥è¯Šæ–­ã€‚
- **å½“å‰å®ç°ä»åŸºäº LLM æ¨ç†**ï¼ševolver æœ¬èº«çš„å¯é æ€§å—é™äº LLM çš„å¹»è§‰å’Œæ¨ç†é”™è¯¯ã€‚
- **å°šæœªåœ¨çœŸå®ç”Ÿäº§ç¯å¢ƒå¤§è§„æ¨¡éªŒè¯**ï¼šå®éªŒåŸºäºæ¨¡æ‹Ÿç¯å¢ƒ AppWorldã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. **Benchmark è®¾è®¡**ï¼š
   - å¼€å‘ä¸“é—¨è¡¡é‡â€œæ¼”åŒ–èƒ½åŠ›â€çš„åŸºå‡†ï¼Œå…³æ³¨**æŒä¹…æ„ä»¶çš„å¤ç”¨ç‡ã€æ¼”åŒ–é€Ÿåº¦ã€æŠ—å¹²æ‰°èƒ½åŠ›**ç­‰ã€‚

2. **Framework ä¼˜åŒ–**ï¼š
   - æ”¹è¿›è¯Šæ–­ã€è§„åˆ’ã€éªŒè¯æ¨¡å—ï¼Œæå‡æ¼”åŒ–æ•ˆç‡å’Œå¯é æ€§ã€‚
   - æ¢ç´¢ multi-agent evolver åä½œæœºåˆ¶ã€‚

3. **ç†è®ºå»ºæ¨¡**ï¼š
   - å½¢å¼åŒ– agentic evolution ä¸ºç»„åˆç¨‹åºç©ºé—´ä¸Šçš„ä¼˜åŒ–é—®é¢˜ã€‚
   - å»ºç«‹ regret boundsï¼Œåˆ†æå…¶ç›¸å¯¹äºç†æƒ³ fine-tuning çš„æ”¶æ•›æ€§ã€‚

4. **å®‰å…¨æ€§ä¸å¯¹é½**ï¼š
   - åŠ å¼ºéªŒè¯æœºåˆ¶ï¼ˆå¦‚å½¢å¼åŒ–éªŒè¯ã€äººç±»å®¡æŸ¥é’©å­ï¼‰ã€‚
   - é˜²æ­¢ç›®æ ‡é”™ä½ï¼ˆmisaligned objectivesï¼‰å’Œèƒ½åŠ›æ»¥ç”¨ã€‚

---

> ğŸ’¡ **Impact Statement**ï¼š  
> Agentic Evolution ä¸ä»…æ˜¯ä¸€ç§æŠ€æœ¯æ”¹è¿›ï¼Œæ›´æ˜¯**èŒƒå¼è½¬å˜**â€”â€”ä»â€œé™æ€æ¨ç†â€è½¬å‘â€œæŒç»­è‡ªæˆ‘è¿›åŒ–â€ã€‚å®ƒæœ‰æœ›å¤§å¹…æå‡ AI ç³»ç»Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„**å¯é æ€§ã€éšç§æ€§ï¼ˆæœ¬åœ°æ¼”åŒ–ï¼‰å’Œå¯æŒç»­æ€§**ï¼Œæ˜¯é€šå¾€ AGI çš„å…³é”®ä¸€æ­¥ã€‚

</details>

---

### 11. [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)

**Authors**: Yue Zhong, Jiawen Kang, Yongju Tong, Hong-Ning Dai, Dong In Kim, Abbas Jamalipour, Shengli Xie  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.01131v1  

#### Abstract
With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous netw...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹**ä½ç©ºç»æµï¼ˆLow-Altitude Economy, LAEï¼‰**ä¸­æ— äººæœºï¼ˆUAVï¼‰ä½œä¸ºç©ºä¸­åŸºç«™æ—¶é¢ä¸´çš„èµ„æºåˆ†é…ä¸ç‰©ç†æ§åˆ¶ç¨³å®šæ€§ä¹‹é—´çš„çŸ›ç›¾é—®é¢˜ã€‚ä¼ ç»Ÿç½‘ç»œè®¾è®¡å¤šå…³æ³¨ååé‡æˆ–å»¶è¿Ÿä¼˜åŒ–ï¼Œå¿½è§†äº†é€šä¿¡å»¶è¿Ÿå¯¹æ§åˆ¶å›è·¯ç¨³å®šæ€§çš„ç›´æ¥å½±å“ã€‚åœ¨ç¾ååº”æ€¥ç­‰å®‰å…¨å…³é”®åœºæ™¯ä¸­ï¼Œè¿™ç§è„±èŠ‚å¯èƒ½å¯¼è‡´ä»»åŠ¡å¤±è´¥ã€‚

å…·ä½“æŒ‘æˆ˜åŒ…æ‹¬ï¼š
- é€šä¿¡èµ„æºï¼ˆå¸¦å®½ã€èƒ½é‡ï¼‰æœ‰é™ï¼›
- ç”¨æˆ·æœåŠ¡éœ€æ±‚å¼‚æ„ï¼ˆç´§æ€¥å‹ vs. é«˜é€šé‡å‹ï¼‰ï¼›
- æ§åˆ¶ç³»ç»Ÿå¯¹ç«¯åˆ°ç«¯å»¶è¿Ÿé«˜åº¦æ•æ„Ÿï¼›
- è¾¹ç¼˜è®¾å¤‡è®¡ç®—èƒ½åŠ›å—é™ï¼Œéš¾ä»¥éƒ¨ç½²å¤æ‚DRLæ¨¡å‹ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯

#### ï¼ˆ1ï¼‰æ„å»º **SCÂ³ é—­ç¯æ¡†æ¶ï¼ˆSensing-Communication-Computing-Control Closed-Loopï¼‰**
- å°† Sensingã€Communicationã€Computing å’Œ Control å››ä¸ªç¯èŠ‚å»ºæ¨¡ä¸ºä¸€ä¸ªç»Ÿä¸€çš„åŠ¨æ€åé¦ˆç³»ç»Ÿã€‚
- æ˜ç¡®é‡åŒ–äº†å„ç¯èŠ‚å»¶è¿Ÿï¼ˆå¦‚ $T_{\text{sense}}, T_{\text{comm}}, T_{\text{compute}}, T_{\text{control}}$ï¼‰å¦‚ä½•ç´¯ç§¯å¹¶å½±å“æ§åˆ¶çŠ¶æ€æ›´æ–°ã€‚

#### ï¼ˆ2ï¼‰å¼•å…¥ **Lyapunov Stability Theory è¿›è¡Œæ§åˆ¶-é€šä¿¡ååŒè®¾è®¡**
- åˆ©ç”¨ Lyapunov å‡½æ•°åˆ†æ UAV çŠ¶æ€è¯¯å·®æ¼”åŒ–è¿‡ç¨‹ï¼›
- æ¨å¯¼å‡ºæ»¡è¶³æ§åˆ¶ç¨³å®šæ€§çš„**æœ€å¤§å…è®¸é€šä¿¡å»¶è¿Ÿè¾¹ç•Œ**ï¼ˆå³ $D_{\text{req}} = (1-\eta_n)\epsilon_n$ï¼‰ï¼›
- å°†æŠ½è±¡çš„â€œç¨³å®šæ€§è¦æ±‚â€è½¬åŒ–ä¸ºå¯é‡åŒ–çš„**é€šä¿¡èµ„æºçº¦æŸæ¡ä»¶**ï¼Œå®ç°æ§åˆ¶ç†è®ºä¸ç½‘ç»œèµ„æºç®¡ç†çš„æ·±åº¦èåˆã€‚

#### ï¼ˆ3ï¼‰å»ºç«‹ **Stackelberg Game æ¨¡å‹è¿›è¡Œåˆ†å±‚èµ„æºè°ƒæ§**
- **é¢†å¯¼è€…ï¼ˆLeaderï¼‰**ï¼šUAV åŠ¨æ€å®šä»·å¸¦å®½èµ„æºï¼›
- **è¿½éšè€…ï¼ˆFollowerï¼‰**ï¼šç”¨æˆ·æ ¹æ®ä»·æ ¼å’ŒæœåŠ¡ç´§è¿«æ€§å†³å®šå¸¦å®½è¯·æ±‚ï¼›
- é€šè¿‡é€†å‘å½’çº³æ³•æ±‚è§£å‡è¡¡ï¼Œå®ç°è´Ÿè½½å‡è¡¡ä¸ç¨³å®šæ€§ä¿éšœçš„åŒé‡ç›®æ ‡ã€‚

#### ï¼ˆ4ï¼‰æå‡º **è½»é‡çº§ Pruning-based PPO ç®—æ³•**
- é’ˆå¯¹æ ‡å‡† DRL åœ¨è¾¹ç¼˜å¹³å°ä¸Šçš„é«˜è®¡ç®—å¼€é”€é—®é¢˜ï¼›
- å¼•å…¥ **dynamic structured pruning** æœºåˆ¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å‰ªæå†—ä½™ç¥ç»å…ƒï¼›
- æ˜¾è‘—å‹ç¼© Actor-Critic ç½‘ç»œè§„æ¨¡ï¼Œé™ä½æ¨ç†å»¶è¿Ÿä¸èƒ½è€—ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„ UAV å¹³å°ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | æœ¬æ–‡ä¼˜åŠ¿ |
|------|--------|
| **ç³»ç»Ÿå»ºæ¨¡** | è¶…è¶Šä¼ ç»Ÿ throughput-centric è®¾è®¡ï¼Œé¦–æ¬¡å°† Lyapunov ç¨³å®šæ€§åµŒå…¥ SCÂ³ é—­ç¯æ¡†æ¶ |
| **èµ„æºåˆ†é…æœºåˆ¶** | Stackelberg Game ç»“åˆç¨³å®šæ€§çº¦æŸï¼Œå®ç°â€œç´§æ€¥ä¼˜å…ˆ + ç¨³å®šè¿è¡Œâ€çš„æ¿€åŠ±å…¼å®¹æœºåˆ¶ |
| **ç®—æ³•æ•ˆç‡** | Pruning-based PPO å®ç°æ¨¡å‹å‹ç¼© >50%ï¼Œæ˜¾è‘—ä¼˜äºåŸå§‹ PPOã€Greedy å’Œ Random æ–¹æ³• |
| **å®ç”¨æ€§** | æ”¯æŒåœ¨çº¿å­¦ä¹ ä¸å¿«é€Ÿæ”¶æ•›ï¼Œé€‚åˆåŠ¨æ€å˜åŒ–çš„ LAE åœºæ™¯ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†ä¸ä»¿çœŸç¯å¢ƒ
- **æ— çœŸå®æ•°æ®é›†**ï¼Œé‡‡ç”¨**è‡ªå®šä¹‰æ•°å€¼ä»¿çœŸå¹³å°**æ¨¡æ‹Ÿ LAE åœºæ™¯ï¼›
- æ¨¡æ‹Ÿç¾åæ•‘æ´åœºæ™¯ï¼šåœ°é¢åŸºç¡€è®¾æ–½ç˜«ç—ªï¼ŒUAV éƒ¨ç½²ä¸ºç©ºä¸­åŸºç«™æ¢å¤é€šä¿¡ï¼›
- ç”¨æˆ·åˆ†å¸ƒéšæœºç”Ÿæˆï¼Œä¿¡é“æ¨¡å‹åŸºäºè‡ªç”±ç©ºé—´è·¯å¾„æŸè€—ï¼ˆFree-space Path Lossï¼‰ã€‚

---

### å®éªŒè®¾ç½®

| å‚æ•°ç±»åˆ« | è®¾ç½®å€¼ |
|--------|-------|
| UAV æ•°é‡ | 3 æˆ– å˜é‡ï¼ˆ3â€“12ï¼‰ |
| ç”¨æˆ·æ•°é‡ | 5â€“20 |
| é‡‡æ ·å‘¨æœŸ $\epsilon_n$ | 0.5 s |
| UAV è®¡ç®—èƒ½åŠ› $f_n$ | 1 GHz |
| å¸¦å®½æ€»é‡ $K_{\text{total}}$ | [15, 25] MHz |
| ç”¨æˆ·å‘å°„åŠŸç‡ | [100, 300] mW |
| æ•°æ®åŒ…å¤§å° $S_i$ | [40, 64] kbits |
| Lyapunov è¡°å‡ç‡ $p_n$ | 0.95 |
| æ§åˆ¶æ—¶é—´å¸¸æ•° $\tau_n$ | 0.005 s |
| å­¦ä¹ ç‡ | $1\times10^{-3}$ |
| æŠ˜æ‰£å› å­ $\gamma$ | 0.95 |
| éšè—å±‚ç»“æ„ | [256, 256] â†’ ç»å‰ªæåå¤§å¹…å‹ç¼© |

---

### è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | æè¿° |
|-----|------|
| **Test Reward / Utility** | UAV çš„é•¿æœŸç´¯è®¡æ•ˆç”¨ï¼Œåæ˜ èµ„æºæ”¶ç›Šä¸æˆæœ¬å¹³è¡¡ |
| **Convergence Speed** | ç®—æ³•è¾¾åˆ°ç¨³å®šç­–ç•¥æ‰€éœ€çš„è¿­ä»£æ¬¡æ•° |
| **Bandwidth Allocation Profile** | åˆ†æå¸¦å®½æ˜¯å¦å‘é«˜ä¼˜å…ˆçº§ç”¨æˆ·å€¾æ–œ |
| **Stability Compliance Rate** | å®é™…åˆ†é…å¸¦å®½æ˜¯å¦æ»¡è¶³ Lyapunov å¯¼å‡ºçš„æœ€å°éœ€æ±‚ |
| **Inference Latency & Model Size** | è¡¡é‡ç®—æ³•åœ¨è¾¹ç¼˜è®¾å¤‡çš„å¯è¡Œæ€§ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç‰¹ç‚¹ |
|------|------|
| **Standard PPO** | ä¸å«å‰ªæçš„æ ‡å‡†å¤šæ™ºèƒ½ä½“ PPO ç®—æ³• |
| **Greedy Algorithm** | UAV æŒ‰å›ºå®šé«˜ä»·å‡ºå”®èµ„æºï¼Œç”¨æˆ·æŒ‰éœ€ç”³è¯· |
| **Random Algorithm** | éšæœºå®šä»·ä¸éšæœºè¯·æ±‚ç­–ç•¥ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… å›¾3ï¼šæµ‹è¯•å¥–åŠ±å¯¹æ¯”ï¼ˆTest Reward Comparisonï¼‰
- **Pruning-based PPO** æœ€ç»ˆæ”¶æ•›å¥–åŠ±çº¦ä¸º **8.0**ï¼›
- Standard PPO æ”¶æ•›äºçº¦ **7.6**ï¼›
- Greedy â‰ˆ 4.0ï¼ŒRandom â‰ˆ 2.3ï¼›
- **æå‡å¹…åº¦**ï¼šç›¸æ¯”æ ‡å‡† PPO æå‡çº¦ **5.3%**ï¼Œç›¸æ¯” Greedy æå‡è¶…è¿‡ **100%**ã€‚

> ğŸ“Œ **è¯´æ˜**ï¼šå‰ªæä¸ä»…æœªæŸå®³æ€§èƒ½ï¼Œåè€Œå› æ­£åˆ™åŒ–æ•ˆæœæå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚

---

#### âœ… å›¾4ï¼šä¸åŒå‰ªæèµ·å§‹è½®æ¬¡çš„å½±å“
- æœ€ä¼˜è®¾ç½®ä¸º **pruning start epoch $t_0 = 50$**ï¼ˆæ—©æœŸå‰ªæï¼‰ï¼›
- æ”¶æ•›å¥–åŠ±æœ€é«˜ï¼ˆ~8.0ï¼‰ï¼Œä¸”æ–¹å·®æ›´å°ï¼›
- ä¸­æœŸå‰ªæï¼ˆ$t_0=200$ï¼‰è¡¨ç°æœ€å·®ï¼Œå¹²æ‰°ç­–ç•¥å½¢æˆï¼›
- å¯ç¤ºï¼š**early structural regularization æ›´æœ‰åˆ©äºç¨³å®šå­¦ä¹ **ã€‚

---

#### âœ… å›¾5ï¼šStackelberg ç­–ç•¥æ¼”åŒ–è½¨è¿¹
- UAV å®šä»·ç­–ç•¥ï¼ˆå›¾5aï¼‰åˆæœŸæ³¢åŠ¨å¤§ï¼ŒåæœŸæ”¶æ•›è‡³ç¨³å®šå€¼ï¼ˆçº¦ 3.0â€“4.5ï¼‰ï¼›
- ç”¨æˆ·å¸¦å®½è¯·æ±‚ï¼ˆå›¾5bï¼‰éšä»·æ ¼è°ƒæ•´è€ŒåŠ¨æ€å“åº”ï¼Œæœ€ç»ˆè¾¾æˆå‡è¡¡ï¼›
- éªŒè¯äº†ç®—æ³•èƒ½æœ‰æ•ˆé€¼è¿‘ **Stackelberg Equilibrium**ã€‚

---

#### âœ… å›¾6ï¼šä¸åŒèŠ‚ç‚¹å¯†åº¦ä¸‹çš„ç³»ç»Ÿæ€§èƒ½
- **ç”¨æˆ·æ•°å¢åŠ ï¼ˆUAV å›ºå®šä¸º3ï¼‰**ï¼š
  - UAV å¹³å‡å¥–åŠ±æŒç»­ä¸Šå‡ï¼ˆä» ~5 åˆ° ~17.9ï¼‰ï¼›
  - è¡¨æ˜æœºåˆ¶èƒ½æ¿€åŠ±æœåŠ¡æ›´å¤šç”¨æˆ·ï¼Œæå‡ç³»ç»Ÿåˆ©ç”¨ç‡ã€‚
- **UAV æ•°é‡å¢åŠ ï¼ˆç”¨æˆ·å›ºå®šä¸º15ï¼‰**ï¼š
  - å•ä¸ª UAV å¥–åŠ±ä¸‹é™ï¼ˆå¸‚åœºé¥±å’Œï¼Œç«äº‰åŠ å‰§ï¼‰ï¼›
  - ç¬¦åˆç»æµå­¦è§„å¾‹ï¼ŒéªŒè¯æ¨¡å‹åˆç†æ€§ã€‚

---

#### âœ… å›¾2ï¼šå¸¦å®½åˆ†é…åˆ†è§£åˆ†æ
- æ‰€æœ‰ç”¨æˆ·çš„å®é™…åˆ†é…å¸¦å®½å‡ **ä¸¥æ ¼é«˜äº Lyapunov æ¨å¯¼çš„æœ€å°éœ€æ±‚ï¼ˆMin Requirementï¼‰**ï¼›
- å¤šä½™éƒ¨åˆ†ç”¨äºæœ€å¤§åŒ–ä¸ªä½“æ•ˆç”¨ï¼ˆUtility Maximizationï¼‰ï¼›
- å®ç°äº†â€œ**ç¨³å®šæ€§ä¸ºåº•çº¿ï¼Œæ•ˆç”¨æœ€å¤§åŒ–ä¸ºç›®æ ‡**â€çš„è®¾è®¡ç†å¿µã€‚

---

### æ¶ˆèå®éªŒï¼ˆéšå«åœ¨å¯¹æ¯”ä¸­ï¼‰

è™½ç„¶æœªæ˜ç¡®åˆ—å‡ºæ¶ˆèå®éªŒè¡¨æ ¼ï¼Œä½†ä»ä»¥ä¸‹å¯¹æ¯”å¯è§†ä¸ºäº‹å®ä¸Šçš„æ¶ˆèåˆ†æï¼š

| æ¨¡å—ç§»é™¤/æ›¿æ¢ | æ€§èƒ½å½±å“ |
|--------------|---------|
| ç§»é™¤ Lyapunov çº¦æŸï¼ˆå³ä¸è€ƒè™‘ç¨³å®šæ€§ï¼‰ | ç³»ç»Ÿå¯èƒ½è¿›å…¥ä¸ç¨³å®šåŒºåŸŸï¼Œæ§åˆ¶å¤±æ•ˆé£é™©ä¸Šå‡ |
| æ›¿æ¢ä¸º Greedy/Random ç­–ç•¥ | å¥–åŠ±ä¸‹é™è¶… 50%ï¼Œæ— æ³•é€‚åº”åŠ¨æ€è´Ÿè½½ |
| ä¸ä½¿ç”¨ pruning | æ¨¡å‹å‚æ•°é‡å¤§ï¼Œæ¨ç†å»¶è¿Ÿé«˜ï¼Œä¸é€‚åˆè¾¹ç¼˜éƒ¨ç½² |
| ä½¿ç”¨ unstructured pruning | ç¡¬ä»¶åŠ é€Ÿå›°éš¾ï¼Œå®é™…åŠ é€Ÿæ¯”ä½ |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. âœ… **æ§åˆ¶ç¨³å®šæ€§å¯é€šè¿‡ Lyapunov ç†è®ºè½¬åŒ–ä¸ºé€šä¿¡èµ„æºçº¦æŸ**ï¼Œå®ç°äº†æ§åˆ¶å±‚ä¸é€šä¿¡å±‚çš„ç´§è€¦åˆè®¾è®¡ï¼›
2. âœ… **Stackelberg Game æ˜¯å»ºæ¨¡ UAV-User åˆ†å±‚äº¤äº’çš„æœ‰æ•ˆå·¥å…·**ï¼Œæ”¯æŒæ¿€åŠ±ç›¸å®¹çš„èµ„æºåˆ†é…ï¼›
3. âœ… **Pruning-based PPO åœ¨ä¿æŒç”šè‡³æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‹ç¼©æ¨¡å‹ä½“ç§¯**ï¼Œé€‚åˆéƒ¨ç½²äºèƒ½æºå—é™çš„ UAVï¼›
4. âœ… **æ—©æœŸç»“æ„åŒ–å‰ªæå…·æœ‰æ­£åˆ™åŒ–ä½œç”¨**ï¼Œæœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆå¹¶åŠ å¿«æ”¶æ•›ï¼›
5. âœ… æ‰€ææ–¹æ¡ˆåœ¨å¤šç§è´Ÿè½½æ¡ä»¶ä¸‹å‡èƒ½ç»´æŒç³»ç»Ÿç¨³å®šï¼Œå¹¶å®ç°é«˜æ•ˆèµ„æºåˆ©ç”¨ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§

| å±€é™æ€§ | è¯´æ˜ |
|-------|------|
| **ä¾èµ–ç²¾ç¡®çš„åŠ¨åŠ›å­¦å»ºæ¨¡** | éœ€è¦çŸ¥é“ UAV æ§åˆ¶çŸ©é˜µ $A, B$ï¼Œåœ¨å¤æ‚éçº¿æ€§åœºæ™¯ä¸‹å¯èƒ½ä¸å‡†ç¡® |
| **å‡è®¾çº¿æ€§åé¦ˆæ§åˆ¶å™¨** | å®é™…é£è¡Œæ§åˆ¶ç³»ç»Ÿå¯èƒ½æ›´å¤æ‚ï¼Œå­˜åœ¨éçº¿æ€§ä¸ä¸ç¡®å®šæ€§ |
| **é›†ä¸­å¼è®­ç»ƒï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ** | ä»éœ€ä¸€å®šç¨‹åº¦çš„ä¿¡æ¯å…±äº«ï¼Œéšç§ä¿æŠ¤æœ‰å¾…åŠ å¼º |
| **æœªè€ƒè™‘ç§»åŠ¨æ€§å»ºæ¨¡** | å½“å‰ focus åœ¨é™æ€æ‹“æ‰‘ï¼Œæœªæ¥éœ€æ‰©å±•è‡³åŠ¨æ€æ‹“æ‰‘å˜åŒ– |

---

### æœªæ¥å·¥ä½œæ–¹å‘

1. **è”åˆä¼˜åŒ–èƒ½é‡ä¸æ—¶å˜å¸¦å®½èµ„æº**ï¼Œç ”ç©¶å¤åˆ end-to-end latency æ³¢åŠ¨ä¸‹çš„é²æ£’æ§åˆ¶ï¼›
2. **æ·±åŒ–åŒå‘ co-design æœºåˆ¶**ï¼Œè®©æ§åˆ¶ç­–ç•¥åå‘å½±å“é€šä¿¡è°ƒåº¦ï¼ˆå¦‚ä¸»åŠ¨ä¸¢åŒ…å®¹å¿æ§åˆ¶ï¼‰ï¼›
3. **å¼•å…¥è”é‚¦å­¦ä¹ æˆ–å»ä¸­å¿ƒåŒ–è®­ç»ƒæœºåˆ¶**ï¼Œå¢å¼ºéšç§æ€§ä¸å¯æ‰©å±•æ€§ï¼›
4. **ç»“åˆè¯­ä¹‰é€šä¿¡ï¼ˆSemantic Communicationï¼‰**ï¼Œè¿›ä¸€æ­¥å‡å°‘ä¼ è¾“å¼€é”€ï¼›
5. **ç¡¬ä»¶åŸå‹éªŒè¯**ï¼šåœ¨çœŸå® UAV å¹³å°éƒ¨ç½² pruning-based PPOï¼Œæµ‹è¯•å®æ—¶æ€§èƒ½ã€‚

---

> ğŸ”š **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬è®ºæ–‡å¼€åˆ›æ€§åœ°å°† **Lyapunov ç¨³å®šæ€§ç†è®º**èå…¥ **SCÂ³ é—­ç¯èµ„æºç®¡ç†æ¡†æ¶**ï¼Œé€šè¿‡ **Stackelberg Game + Pruning-based PPO** å®ç°äº†å…¼é¡¾**æ§åˆ¶å¯é æ€§**ä¸**é€šä¿¡é«˜æ•ˆæ€§**çš„è½»é‡åŒ–æ™ºèƒ½å†³ç­–æ–¹æ¡ˆï¼Œä¸ºæœªæ¥ä½ç©ºç»æµä¸­çš„è‡ªä¸» UAV ç³»ç»Ÿæä¾›äº†åšå®çš„ç†è®ºä¸æŠ€æœ¯æ”¯æ’‘ã€‚

</details>

---

### 12. [Performance of Small Language Model Pretraining on FABRIC: An Empirical Study](https://arxiv.org/abs/2602.02632)

**Authors**: Praveen Rao  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.02632v1  

#### Abstract
Large language models (LLMs) require enormous computing power to pretrain on massive datasets. When limited datasets are available, smaller-sized LLMs are better choice to pretrain (on user-specified datasets) by following the scaling laws of LLMs. Using pretrained models, vector embeddings can be g...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šPerformance of Small Language Model Pretraining on FABRIC: An Empirical Study

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡èšç„¦äº**å­¦æœ¯ç ”ç©¶è€…åœ¨èµ„æºå—é™æ¡ä»¶ä¸‹é¢„è®­ç»ƒ Small Language Models (SLMs)** çš„å®é™…æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é¢„è®­ç»ƒé€šå¸¸éœ€è¦æ˜‚è´µçš„è®¡ç®—èµ„æºï¼ˆå¦‚æ•°åƒå¼ GPUï¼‰ï¼Œè¿™ä½¿å¾—å¤§å¤šæ•°å­¦æœ¯ç”¨æˆ·éš¾ä»¥å‚ä¸ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•åˆ©ç”¨è”é‚¦èµ„åŠ©çš„å®éªŒåŸºç¡€è®¾æ–½ **FABRIC**ï¼Œåœ¨æ— æˆæœ¬ã€ä½¿ç”¨æ¶ˆè´¹çº§GPUçš„æƒ…å†µä¸‹é«˜æ•ˆåœ°è¿›è¡Œ SLM é¢„è®­ç»ƒã€‚

### æå‡ºçš„æ–°æ–¹æ³•/æ–°æ€è·¯
- **ç³»ç»Ÿæ€§è¯„ä¼°å¤šç§å¹¶è¡ŒåŒ–ç­–ç•¥**ï¼šå…¨é¢æ¯”è¾ƒäº† **Data Parallelism**ã€**ZeRO2**ã€**Shard Parallelism**ï¼ˆä»… intra-operatorï¼‰ å’Œ **Pipeshard Parallelism**ï¼ˆç»“åˆ intra- å’Œ inter-operator/pipeline å¹¶è¡Œï¼‰åœ¨çœŸå®åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚
- **æå‡ºä¸€ç§è‡ªé€‚åº”é€‰æ‹©é¢„è®­ç»ƒæŠ€æœ¯çš„ç®—æ³•**ï¼ˆAlgorithm 1ï¼‰ï¼šåŸºäºå°è§„æ¨¡è¯•è¿è¡Œçš„ç»“æœï¼ŒåŠ¨æ€å†³å®šæœ€ä¼˜çš„å¹¶è¡Œç­–ç•¥å’ŒGPUé…ç½®ï¼Œä»¥å®ç°æœ€é«˜è®­ç»ƒæ€§èƒ½ã€æœ€çŸ­æ€»æ—¶é—´ï¼Œå¹¶å°½å¯èƒ½å‡å°‘GPUä½¿ç”¨æ•°é‡ã€‚
- **å¼ºè°ƒåœ°ç†åˆ†å¸ƒå¯¹é€šä¿¡å¼€é”€çš„å½±å“**ï¼šæ·±å…¥åˆ†æäº†å½“GPUè·¨å¤šä¸ªFABRICç«™ç‚¹ï¼ˆå³åœ°ç†ä¸Šåˆ†æ•£ï¼‰æ—¶ï¼Œç½‘ç»œå»¶è¿Ÿå¯¹ä¸åŒå¹¶è¡Œç­–ç•¥æ€§èƒ½çš„å½±å“ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **å®ç”¨æ€§æ›´å¼º**ï¼šä¸åŒäºå·¥ä¸šç•Œè¿½æ±‚æè‡´è§„æ¨¡çš„æ–¹æ³•ï¼Œæœ¬ç ”ç©¶æ›´å…³æ³¨**ç°å®å¯ç”¨èµ„æºä¸‹çš„æœ€ä½³å®è·µ**ï¼Œä¸ºå­¦æœ¯ç¤¾åŒºæä¾›äº†å¯å¤ç°ã€å¯æ“ä½œçš„æŒ‡å¯¼ã€‚
- **æ­ç¤ºäº† Pipeshard åœ¨é«˜å»¶è¿Ÿç¯å¢ƒä¸‹çš„é²æ£’æ€§**ï¼šå‘ç° Alpa çš„ Pipeshard ç­–ç•¥åœ¨è·¨ç«™ç‚¹è®­ç»ƒä¸­è¡¨ç°æœ€ä½³ï¼Œå°¤å…¶åœ¨ç½‘ç»œå»¶è¿Ÿè¾¾åˆ°æ•°åæ¯«ç§’æ—¶ä»èƒ½ä¿æŒè‰¯å¥½æ€§èƒ½ï¼Œè€Œå…¶ä»–æ–¹æ³•ï¼ˆå¦‚ Shardï¼‰å› é›†ä½“é€šä¿¡å¼€é”€å¤§è€Œä¸¥é‡é€€åŒ–ã€‚
- **æä¾›å†³ç­–æ”¯æŒå·¥å…·**ï¼šæå‡ºçš„ Algorithm 1 æ˜¯ä¸€ä¸ªå®ç”¨çš„å·¥ç¨‹æŒ‡å—ï¼Œå¸®åŠ©ç”¨æˆ·åœ¨å¤æ‚ç¯å¢ƒä¸­åšå‡ºæ˜æ™ºçš„æŠ€æœ¯é€‰å‹ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **Wikipedia æ•°æ®é›†**ï¼ˆæ¥è‡ª HuggingFaceï¼‰
- å…·ä½“æ–‡ä»¶ï¼š`20231101.ace` â€”â€” ä¸€ä¸ªé€‚ä¸­å¤§å°çš„ç»´åŸºç™¾ç§‘å¿«ç…§ï¼Œç”¨äºæµ‹è¯•é¢„è®­ç»ƒæ€§èƒ½ã€‚

### å®éªŒè®¾ç½®
- **å¹³å°**ï¼š**FABRIC** â€”â€” ç”±ç¾å›½å›½å®¶ç§‘å­¦åŸºé‡‘ä¼šï¼ˆNSFï¼‰èµ„åŠ©çš„å…¨å›½æ€§å¯ç¼–ç¨‹ç ”ç©¶åŸºç¡€è®¾æ–½ã€‚
- **ç¡¬ä»¶é…ç½®**ï¼š
  - æ„å»ºäº† **5 ç§ä¸åŒçš„ GPU é›†ç¾¤åˆ‡ç‰‡ï¼ˆsliceï¼‰**ï¼Œæ¯ç§åŒ…å«ä¸¤ä¸ª VMï¼Œæ¯ä¸ª VM æŒ‚è½½ 2 ä¸ª GPUã€‚
  - ä½¿ç”¨ä¸‰ç§ GPU ç±»å‹ï¼š
    - NVIDIA Quadro RTX 6000ï¼ˆRTXï¼Œ24GB VRAMï¼‰
    - NVIDIA Tesla T4ï¼ˆT4ï¼Œ16GB VRAMï¼‰
    - NVIDIA A30ï¼ˆAmpereï¼Œ24GB VRAMï¼‰
- **ç½‘ç»œè¿æ¥**ï¼š
  - åŒä¸€ç«™ç‚¹å†…ä½¿ç”¨ **L2Bridge**ï¼ˆä½å»¶è¿Ÿ ~0.1msï¼‰
  - è·¨ç«™ç‚¹ä½¿ç”¨ **L2STS**ï¼ˆLayer 2 Site-to-Site Connectionï¼‰ï¼Œå»¶è¿Ÿä» 20ms åˆ°è¶…è¿‡ 100ms ä¸ç­‰ã€‚
- **è½¯ä»¶æ ˆ**ï¼š
  - **Alpa**ï¼šç”¨äºç”Ÿæˆè‡ªåŠ¨ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œè®¡åˆ’ï¼ˆç‰¹åˆ«æ˜¯ Pipeshardï¼‰ã€‚
  - **Ray**ï¼šç”¨äºç®¡ç†é›†ç¾¤èµ„æºå’Œè°ƒåº¦ä»»åŠ¡ã€‚
  - CUDA 11.8, NCCL 2.15.1 ç­‰ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **æ€»å¢™é’Ÿæ—¶é—´ï¼ˆWall-clock Timeï¼‰**ï¼šå®Œæˆ 20 ä¸ª epoch æ‰€éœ€çš„æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ã€‚
- **å¹³å‡è®­ç»ƒæ€§èƒ½ï¼ˆAverage Training Performanceï¼‰**ï¼šä»¥ **TFLOP/s** è¡¡é‡çš„ååé‡ã€‚
- æˆåŠŸ/å¤±è´¥æ‰§è¡Œæƒ…å†µï¼ˆæ˜¯å¦å‡ºç° OOM é”™è¯¯ï¼‰ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | æè¿° |
|------|------|
| **Data** | ä¼ ç»Ÿæ•°æ®å¹¶è¡Œï¼Œæ¨¡å‹å¤åˆ¶åˆ°å„è®¾å¤‡ï¼Œå¤„ç†ä¸åŒæ•°æ®åˆ†ç‰‡ |
| **ZeRO2** | æ”¹è¿›çš„æ•°æ®å¹¶è¡Œï¼Œé€šè¿‡ä¼˜åŒ– optimizer stateã€gradient å’Œ activation å†…å­˜æ¥é™ä½æ˜¾å­˜å ç”¨ |
| **Shard** | Alpa çš„ intra-operator å¹¶è¡Œï¼ˆå¼ é‡åˆ‡åˆ†ï¼‰ï¼Œä¸åŒ…å« pipeline |
| **Pipeshard** | Alpa çš„æ··åˆå¹¶è¡Œç­–ç•¥ï¼Œç»“åˆ shard å’Œ pipeline å¹¶è¡Œ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table I åŠ Figuresï¼‰

| Technique | TACC-TACC (0.1ms) | UTAH-GPN (20.2ms) | UTAH-MASS (57.4ms) | BRIS-STAR (95.9ms) | GAT-AMST (103.0ms) |
|---------|-------------------|-------------------|--------------------|--------------------|---------------------|
| **Data** | 41 min | 136 min | 272 min | 199 min | 1,375 min |
| **ZeRO2** | 52 min | 295 min | 641 min | 363 min | 3,519 min |
| **Shard** | 82 min | 840 min | 1,808 min | 1,125 min | 5,400 min |
| **Pipeshard** | **29 min** | **57 min** | **86 min** | **96 min** | **100 min** |

> âœ… **Pipeshard åœ¨æ‰€æœ‰è·¨ç«™ç‚¹å®éªŒä¸­å‡å–å¾—æœ€å¿«è®­ç»ƒæ—¶é—´**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **åœ¨å•ç«™ç‚¹ï¼ˆTACC-TACCï¼‰**ï¼š
  - å½“æ¨¡å‹è¾ƒå°ï¼ˆgpt2mï¼‰ä¸”å¯æ”¾å…¥æ˜¾å­˜æ—¶ï¼Œ**Data** å’Œ **ZeRO2** æ€§èƒ½ä¼˜äº Pipeshardï¼ˆä¾‹å¦‚ Data è¾¾ 15.74 TFLOP/s vs Pipeshard 12.17 TFLOP/sï¼‰ã€‚
  - **Pipeshard åœ¨å•æœºç¯å¢ƒä¸‹å¹¶æ— ä¼˜åŠ¿**ï¼Œç”šè‡³å¯èƒ½æ›´æ…¢ã€‚
- **åœ¨è·¨ç«™ç‚¹ï¼ˆé«˜ç½‘ç»œå»¶è¿Ÿï¼‰**ï¼š
  - **Pipeshard æ˜æ˜¾èƒœå‡º**ï¼Œå…¶è®­ç»ƒæ—¶é—´è¿œä½äºå…¶ä»–æ–¹æ³•ï¼ˆå¦‚åœ¨ GAT-AMST ä¸Šä»…éœ€ 100 åˆ†é’Ÿï¼Œè€Œ Shard éœ€è¦ 5,400 åˆ†é’Ÿï¼‰ã€‚
  - **Shard å—é«˜å»¶è¿Ÿå½±å“æœ€å¤§**ï¼Œå› å…¶ä¾èµ–é¢‘ç¹çš„ GPU-GPU collective communicationï¼ˆé€šè¿‡ TCP/IP å®ç°ï¼Œæ•ˆç‡ä½ï¼‰ã€‚
  - **Data å’Œ ZeRO2 ä¹Ÿéšå»¶è¿Ÿå¢åŠ æ˜¾è‘—é€€åŒ–**ï¼Œä½†ä¸å¦‚ Shard ä¸¥é‡ã€‚
- **å†…å­˜æ–¹é¢**ï¼š
  - **Pipeshard å†…å­˜éœ€æ±‚æ›´é«˜**ï¼Œåœ¨å¼‚æ„ GPU æˆ–æ˜¾å­˜ä¸è¶³æ—¶å®¹æ˜“å¤±è´¥ï¼ˆå¦‚ gpt2L åœ¨ 2Ã—RTX ä¸Šæ— æ³•è¿è¡Œï¼‰ã€‚
  - **ZeRO2 æ›´èŠ‚çœå†…å­˜**ï¼Œåœ¨ Pipeshard å¤±è´¥æ—¶å¾€å¾€ä»èƒ½æˆåŠŸè¿è¡Œã€‚

### æ¶ˆèå®éªŒç»“æœ
è™½ç„¶æœªæ˜ç¡®æ ‡æ³¨â€œæ¶ˆèâ€ï¼Œä½†å®éªŒè®¾è®¡æœ¬è´¨ä¸Šæ˜¯å¯¹æ¯”ä¸åŒå¹¶è¡Œç­–ç•¥çš„æ•ˆæœï¼š
- **éªŒè¯äº† Pipeshard ä¸­ pipeline å¹¶è¡Œçš„ä»·å€¼**ï¼šå…¶ point-to-point é€šä¿¡æ¨¡å¼æ¯” Shard çš„ collective communication æ›´è€å—é«˜å»¶è¿Ÿã€‚
- **éªŒè¯äº†ç¡¬ä»¶åŒè´¨æ€§çš„é‡è¦æ€§**ï¼šå¼‚æ„ GPUï¼ˆå¦‚ RTX + T4ï¼‰ä¼šé™åˆ¶æŸäº›ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚
- **éªŒè¯äº†æœ¬åœ°è®­ç»ƒä¼˜å…ˆåŸåˆ™**ï¼šè‹¥æ¨¡å‹èƒ½åœ¨å•ä¸ª VM ä¸Šè¿è¡Œï¼Œé€šå¸¸æ¯”è·¨èŠ‚ç‚¹è®­ç»ƒæ›´å¿«ï¼ˆå³ä½¿ä½¿ç”¨æ›´å¤š GPUï¼‰ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Pipeshard æ˜¯åœ°ç†åˆ†å¸ƒè®­ç»ƒçš„æœ€ä½³é€‰æ‹©**ï¼š  
   åœ¨ FABRIC è¿™ç±»å…·æœ‰è¾ƒé«˜ç½‘ç»œå»¶è¿Ÿï¼ˆ10â€“100msï¼‰çš„åˆ†å¸ƒå¼ç¯å¢ƒä¸­ï¼ŒAlpa çš„ **Pipeshard** ç­–ç•¥ç”±äºå…¶å¯¹ **intra- å’Œ inter-operator parallelism çš„è”åˆä¼˜åŒ–**ï¼Œè¡¨ç°å‡ºæœ€å¼ºçš„é²æ£’æ€§å’Œæœ€é«˜çš„è®­ç»ƒæ•ˆç‡ã€‚

2. **ç½‘ç»œå»¶è¿Ÿä¸¥é‡å½±å“é€šä¿¡å¯†é›†å‹ç­–ç•¥**ï¼š  
   **Shard** å’Œ **Data Parallelism** å¯¹ç½‘ç»œå»¶è¿Ÿæä¸ºæ•æ„Ÿï¼Œéšç€å»¶è¿Ÿå¢åŠ ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ï¼›è€Œ **Pipeshard** çš„ pipeline ç»“æ„æœ‰æ•ˆç¼“è§£äº†è¿™ä¸€é—®é¢˜ã€‚

3. **å•èŠ‚ç‚¹è®­ç»ƒé€šå¸¸æ›´é«˜æ•ˆ**ï¼š  
   å¦‚æœæ¨¡å‹å¯ä»¥åœ¨å•ä¸ª VM çš„ GPU ä¸Šå®¹çº³ï¼Œä½¿ç”¨ **Data** æˆ– **ZeRO2** å¾€å¾€æ¯”è·¨èŠ‚ç‚¹ä½¿ç”¨ Pipeshard æ›´å¿«ï¼Œè¯´æ˜â€œæ›´å¤š GPU â‰  æ›´å¿«è®­ç»ƒâ€ã€‚

4. **å†…å­˜çº¦æŸæ˜¯å…³é”®ç“¶é¢ˆ**ï¼š  
   Pipeshard è™½ç„¶æ€§èƒ½å¥½ï¼Œä½†å†…å­˜æ¶ˆè€—æ›´å¤§ï¼Œåœ¨å¼‚æ„æˆ–å°æ˜¾å­˜è®¾å¤‡ä¸Šæ˜“å¤±è´¥ï¼›**ZeRO2 æ˜¯ä½å†…å­˜åœºæ™¯ä¸‹çš„å¯é å¤‡é€‰æ–¹æ¡ˆ**ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ– Alpa å’Œ Ray ç”Ÿæ€**ï¼šå®éªŒç»“æœé«˜åº¦ä¾èµ–äº Alpa è‡ªåŠ¨ç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’ï¼Œå…¶é€šç”¨æ€§å—é™äºè¯¥æ¡†æ¶çš„æ”¯æŒç¨‹åº¦ã€‚
- **ä»…æµ‹è¯• GPT-2 æ¨¡å‹**ï¼šç»“è®ºæ˜¯å¦é€‚ç”¨äºå…¶ä»–æ¶æ„ï¼ˆå¦‚ Llamaã€Mistralï¼‰å°šéœ€éªŒè¯ã€‚
- **FABRIC ç‰¹å®šç½‘ç»œç‰¹æ€§**ï¼šTCP/IP å°è£…çš„ NCCL é€šä¿¡å¯èƒ½æ”¾å¤§äº† collective communication çš„å¼€é”€ï¼Œå®é™…æƒ…å†µå¯èƒ½å›  RDMA ç­‰é«˜é€Ÿäº’è¿è€Œä¸åŒã€‚
- **æœªè€ƒè™‘å¤§è§„æ¨¡æ‰©å±•**ï¼šå®éªŒæœ€å¤šä½¿ç”¨ 4 ä¸ª GPUï¼Œæ›´å¤§è§„æ¨¡é›†ç¾¤çš„è¡Œä¸ºæœªçŸ¥ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å°†è¯¥æ–¹æ³•è®ºæ¨å¹¿è‡³æ›´å¤šç±»å‹çš„ SLMs å’Œæ›´å¤§è§„æ¨¡çš„é›†ç¾¤ã€‚
- æ¢ç´¢åœ¨ FABRIC ä¸Šéƒ¨ç½² RDMA æˆ–æ›´é«˜æ•ˆçš„é€šä¿¡åç«¯ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚
- å¼€å‘æ›´æ™ºèƒ½çš„ runtime è‡ªé€‚åº”è°ƒåº¦å™¨ï¼Œæ ¹æ®å®æ—¶è´Ÿè½½åŠ¨æ€åˆ‡æ¢å¹¶è¡Œç­–ç•¥ã€‚
- å°†é¢„è®­ç»ƒçš„ SLM åº”ç”¨äºæ„å»ºé¢†åŸŸç‰¹å®šçš„ **vector database**ï¼Œæ”¯æŒ RAG å’Œè¯­ä¹‰æœç´¢ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚

> ğŸ’¡ **æœ€ç»ˆç›®æ ‡**ï¼šé€šè¿‡æ°‘ä¸»åŒ– SLM é¢„è®­ç»ƒèƒ½åŠ›ï¼Œä½¿å­¦æœ¯ç ”ç©¶äººå‘˜èƒ½å¤Ÿåœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ã€æ•™è‚²ï¼‰æ„å»ºé«˜è´¨é‡çš„åµŒå…¥è¡¨ç¤ºï¼Œæ¨åŠ¨ç§‘å­¦å‘ç°ä¸æŠ€æœ¯åˆ›æ–°ã€‚

</details>

---

### 13. [NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference](https://arxiv.org/abs/2602.02988)

**Authors**: Jiangyong Yu, Xiaomeng Han, Xing Hu, Chen Xu, Zhe Jiang, Dawei Yang  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.02988v1  

#### Abstract
Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks, but their deployment is often constrained by substantial memory footprints and computational costs. While prior work has achieved significant progress in compressing and accelerating linear layers, no...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šNLI: Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´å·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œå°¤å…¶æ˜¯ **nonlinear operations**ï¼ˆå¦‚ SiLUã€Softmaxã€RMSNormï¼‰ä¾èµ–é«˜ç²¾åº¦æµ®ç‚¹è¿ç®—ï¼ˆå¦‚ FP32ï¼‰ï¼Œå¯¼è‡´ç¡¬ä»¶æ•ˆç‡ä½ä¸‹ã€‚å°½ç®¡çº¿æ€§å±‚å·²é€šè¿‡é‡åŒ–ï¼ˆå¦‚ W8A8ï¼‰æ˜¾è‘—åŠ é€Ÿï¼Œéçº¿æ€§å±‚ä»æˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚

æ­¤å¤–ï¼Œç°æœ‰è¿‘ä¼¼æ–¹æ³•ï¼ˆå¦‚ NN-LUTï¼‰å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- **ä¾èµ–æ•°æ®æ ¡å‡†ï¼ˆcalibration-dependentï¼‰**ï¼šæ³›åŒ–èƒ½åŠ›å·®ï¼Œå¯¹è®­ç»ƒèŒƒå›´å¤–çš„è¾“å…¥è¯¯å·®æ€¥å‰§ä¸Šå‡ï¼›
- **é€‚ç”¨èŒƒå›´çª„**ï¼šä»…é€‚ç”¨äºå°èŒƒå›´æ¿€æ´»å€¼ï¼ˆå¦‚ [-5,5]ï¼‰ï¼Œè€Œç°ä»£ LLMs çš„æ¿€æ´»å€¼å¯è¶…è¿‡ Â±100ï¼›
- **ç¡¬ä»¶ä¸å‹å¥½**ï¼šéœ€è¦å¤§é‡æ¯”è¾ƒå™¨ï¼ˆcomparatorsï¼‰ï¼Œèµ„æºæ¶ˆè€—å¤§ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šNLIï¼ˆNon-uniform Linear Interpolationï¼‰
æå‡ºä¸€ç§ **æ— éœ€æ ¡å‡†ã€åŠ¨æ€è§„åˆ’æœ€ä¼˜ã€ç¡¬ä»¶å‹å¥½çš„éçº¿æ€§å‡½æ•°è¿‘ä¼¼æ¡†æ¶**ï¼ŒåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š

1. **NLI-Algorithmï¼ˆè½¯ä»¶ç®—æ³•ï¼‰**
   - å°†éçº¿æ€§å‡½æ•°è¿‘ä¼¼è½¬åŒ–ä¸º **FP16 åŸŸä¸Šçš„éå‡åŒ€çº¿æ€§æ’å€¼ï¼ˆnon-uniform linear interpolationï¼‰**ï¼›
   - åˆ©ç”¨ **åŠ¨æ€è§„åˆ’ï¼ˆDynamic Programmingï¼‰** æ±‚è§£æœ€ä¼˜åˆ‡åˆ†ç‚¹ï¼ˆcutpointsï¼‰ï¼Œæœ€å°åŒ–å…¨å±€æ’å€¼è¯¯å·®ï¼›
   - éµå¾ª Bellman æœ€ä¼˜æ€§åŸç†ï¼Œåœ¨ $O(M \times N^2)$ æ—¶é—´å†…æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ï¼ˆ$M$: åˆ†æ®µæ•°ï¼Œ$N$: å€™é€‰ç‚¹æ•°ï¼‰ï¼›
   - æŸ¥æ‰¾è¡¨ï¼ˆLUTï¼‰å®Œå…¨ç”±ç›®æ ‡å‡½æ•° $f$ å’Œæ•°å€¼æ ¼å¼å†³å®šï¼Œ**æ— éœ€æ•°æ®æ ¡å‡†ï¼Œè·¨æ¨¡å‹/å±‚å¯å¤ç”¨**ã€‚

2. **NLI-Engineï¼ˆç¡¬ä»¶è®¾è®¡ï¼‰**
   - è®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„ã€å³æ’å³ç”¨çš„éçº¿æ€§è®¡ç®—å•å…ƒï¼›
   - å¼•å…¥ **ä¸¤çº§åœ°å€è½¬æ¢æœºåˆ¶ï¼ˆtwo-level address translationï¼‰**ï¼š
     - ç¬¬ä¸€çº§ç¡®å®šå®åŒºé—´ï¼ˆmacro-intervalï¼‰ï¼›
     - ç¬¬äºŒçº§é€šè¿‡ä¹˜æ³•ç¼©æ”¾å®šä½å­åŒºé—´ç´¢å¼•ï¼›
   - æ˜¾è‘—å‡å°‘æ¯”è¾ƒå™¨æ•°é‡ï¼ˆä» 259 â†’ 10ï¼‰ï¼Œæå‡èƒ½æ•ˆï¼›
   - æ”¯æŒæµæ°´çº¿è®¾è®¡ï¼Œç»´æŒé«˜ååã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | NN-LUT / å…¶ä»–æ–¹æ³• | NLIï¼ˆæœ¬æ–‡ï¼‰ |
|------|------------------|-----------|
| **æ˜¯å¦éœ€è¦æ ¡å‡†** | æ˜¯ï¼ˆä¾èµ–è®­ç»ƒæ•°æ®åˆ†å¸ƒï¼‰ | å¦ï¼ˆçº¯ç®—æ³•é©±åŠ¨ï¼‰ |
| **è¾“å…¥èŒƒå›´é€‚åº”æ€§** | çª„ï¼ˆå¦‚ [-5,5]ï¼‰ | å®½ï¼ˆæ”¯æŒ [-150,150]ï¼‰ |
| **è¯¯å·®æ§åˆ¶** | å¤–æ¨æ—¶è¯¯å·®çˆ†ç‚¸ | è¿‘æœºå™¨ç²¾åº¦ï¼Œæœ€åè¯¯å·® < $1.2\times10^{-3}$ |
| **ç¡¬ä»¶æ•ˆç‡** | é«˜æ¯”è¾ƒå™¨å¼€é”€ | æä½æ¯”è¾ƒå™¨éœ€æ±‚ï¼ˆä»…10ä¸ªï¼‰ |
| **é€šç”¨æ€§** | ç‰¹å®šå‡½æ•°ä¼˜åŒ– | æ”¯æŒå¤šç§å‡½æ•°ï¼ˆexp, rsqrt, SiLU, tanh ç­‰ï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†
- **è¯­è¨€æ¨¡å‹ä»»åŠ¡**ï¼š
  - **Wikitext-2**ï¼šç”¨äºè¯„ä¼° **Perplexityï¼ˆâ†“è¶Šä½è¶Šå¥½ï¼‰**
  - **Zero-shot benchmark suite**ï¼š
    - ARC-c/e, BoolQ, PIQA, HellaSwag, OBQA, LAMBADA, SIQA, WinoGrande
  - **ç»¼åˆè¯„æµ‹é›†**ï¼š
    - MMLUï¼ˆå¤šå­¦ç§‘çŸ¥è¯†ï¼‰
    - GSM8kï¼ˆæ•°å­¦æ¨ç†ï¼‰
    - HumanEvalï¼ˆä»£ç ç”Ÿæˆï¼‰

- **è§†è§‰æ¨¡å‹éªŒè¯æ³›åŒ–æ€§**ï¼š
  - ViT-Small, DETR, RT-DETR-L, YOLOv8-M

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹è¦†ç›–**ï¼š
  - LLMsï¼šLlama3-8B/70B, Qwen2.5-7B/32B, Qwen1.5-110B, Qwen3-8B/30B-A3B
  - æ‰€æœ‰æ¨¡å‹ä»¥ FP16 æ¨ç†è¿è¡Œï¼Œæ›¿æ¢åŸæœ‰éçº¿æ€§å±‚ä¸º NLI è¿‘ä¼¼æ¨¡å—
- **NLI å‚æ•°é…ç½®**ï¼š
  - æ€»åˆ‡åˆ†ç‚¹æ•°ï¼š259ï¼ˆå³ 2 + 8Ã—32 + 1ï¼‰
    - é¦–å°¾å„1ä¸ªè¾¹ç•Œç‚¹
    - ä¸­é—´8ä¸ªå®åŒºé—´ï¼Œæ¯ä¸ªå‡åŒ€åˆ’åˆ†ä¸º32ä¸ªå­åŒºé—´
  - åŠ¨æ€è§„åˆ’æœç´¢ä»…ä¼˜åŒ–11ä¸ªå®ç«¯ç‚¹ï¼Œå¤§å¹…é™ä½æœç´¢æ—¶é—´ï¼ˆ~610ç§’ vs ~17000ç§’ï¼‰

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| ç±»å‹ | æŒ‡æ ‡ |
|------|------|
| **å‡†ç¡®æ€§** | Perplexityï¼ˆâ†“ï¼‰ã€MMLUï¼ˆâ†‘ï¼‰ã€GSM8kï¼ˆâ†‘ï¼‰ã€HumanEvalï¼ˆâ†‘ï¼‰ã€mAP / Top-1 Accï¼ˆè§†è§‰ï¼‰ |
| **æ•ˆç‡** | Areaï¼ˆÎ¼mÂ²ï¼‰ã€Powerï¼ˆmWï¼‰ã€Throughputï¼ˆ1G ops/cycleï¼‰ã€Efficiency = Throughput / (Area Ã— Power) |
| **è¯¯å·®è´¨é‡** | Worst-case absolute error, Relative error, Error distribution visualization |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **FP32**ï¼šå…¨ç²¾åº¦åŸºå‡†
- **NN-LUT**ï¼šåŸºäºç¥ç»ç½‘ç»œæ‹Ÿåˆçš„ LUT æ–¹æ³•ï¼ˆYu et al., 2022ï¼‰
- **Uniform 259**ï¼šç­‰è·åˆ‡åˆ†ç‚¹
- **Curvature 259**ï¼šæŒ‰æ›²ç‡å¯†åº¦é‡‡æ ·
- **RI-LUT**ï¼šèŒƒå›´ä¸å˜è¿‘ä¼¼æ–¹æ³•ï¼ˆKim et al., 2023ï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

#### âœ… è½¯ä»¶å±‚é¢ï¼šå‡ ä¹æ— æŸç²¾åº¦
| æ¨¡å‹ | æ–¹æ³• | MMLU | GSM8k | Perplexity |
|------|------|-------|--------|------------|
| Llama3-8B | FP32 | 62.16 | 50.19 | 6.14 |
| | **NLI** | **62.14** | **50.49** | **6.14** |
| Qwen2.5-7B | FP32 | 70.56 | 44.28 | 7.46 |
| | **NLI** | **70.67** | **43.97** | **7.46** |
| Qwen3-30B-A3B | FP32 | 77.86 | 85.44 | 8.70 |
| | **NLI** | **77.88** | **85.39** | **8.70** |

> ğŸ’¡ **ç»“è®º**ï¼šNLI åœ¨æ‰€æœ‰æµ‹è¯•æ¨¡å‹ä¸Šå‡å®ç°ä¸ FP32 **å‡ ä¹ä¸€è‡´çš„æ€§èƒ½è¡¨ç°**ï¼Œä¸”æ˜¾è‘—ä¼˜äº NN-LUTï¼ˆåè€…åœ¨å¤šä¸ªæ¨¡å‹ä¸Šå´©æºƒï¼ŒPPL è¾¾ $10^4$ é‡çº§ï¼‰ã€‚

#### âŒ NN-LUT è¡¨ç°å´©å
- åœ¨ Qwen2.5-32B ä¸Šï¼ŒNN-LUT å¯¼è‡´ **Perplexity é£™å‡è‡³ 70,360**ï¼ˆæ­£å¸¸ä¸º 5.32ï¼‰ï¼Œè¯´æ˜å…¶æ— æ³•å¤„ç†å¤§èŒƒå›´æ¿€æ´»å€¼ï¼›
- å¤šé¡¹ä»»åŠ¡å‡†ç¡®ç‡ä¸‹é™è¶… 50%ï¼Œç”šè‡³è¶‹è¿‘äºéšæœºçŒœæµ‹ã€‚

#### ğŸ§ª æ¶ˆèå®éªŒï¼ˆAblation Studiesï¼‰

##### Ablation Iï¼šä¸¤é˜¶æ®µ vs å•é˜¶æ®µåˆ‡åˆ†ï¼ˆTable 2ï¼‰
| æ–¹æ³• | Cutpoints | MMLU | GSM8k |
|------|----------|-------|--------|
| NLI (2+8Ã—32+1) | 259 | 70.67 | 43.97 |
| Macro-only (DP, M=11) | 11 | 21.14 | 0 |

> â¤ ä½¿ç”¨ä»…11ä¸ªåˆ‡åˆ†ç‚¹ï¼ˆæ— å­åˆ’åˆ†ï¼‰ä¼šå¯¼è‡´ä¸¥é‡ç²¾åº¦æŸå¤±ï¼Œè¯æ˜ **å¾®è§‚å‡åŒ€åˆ’åˆ†çš„é‡è¦æ€§**ã€‚

##### Ablation IIï¼šç›´æ¥ DP vs ä¸¤é˜¶æ®µ DPï¼ˆTable 3ï¼‰
| æ–¹æ³• | Cutpoints | MMLU | Search Time (s) |
|------|----------|-------|------------------|
| NLI (2+8Ã—32+1) | 259 | 70.67 | 610 |
| Non-uniform 259 (full DP) | 259 | 70.65 | 17,000 |

> â¤ ç²¾åº¦ç›¸è¿‘ï¼Œä½† **æœç´¢æ—¶é—´å¿«çº¦28å€**ï¼Œä¸”æ›´åˆ©äºç¡¬ä»¶æ˜ å°„ã€‚

##### Ablation IIIï¼šä¸åŒåˆ‡åˆ†ç­–ç•¥å¯¹æ¯”ï¼ˆTable 4ï¼‰
| æ–¹æ³• | MMLU | GSM8k |
|------|------|--------|
| Uniform 259 | 29.1 | 18.13 |
| Curvature 259 | 65.74 | 32.58 |
| **NLI** | **70.65** | **43.97** |

> â¤ NLI æ˜¾è‘—ä¼˜äºå¯å‘å¼æ–¹æ³•ï¼Œå°¤å…¶åœ¨é«˜æ›²ç‡åŒºåŸŸä¿æŒé«˜ä¿çœŸã€‚

#### âš™ï¸ ç¡¬ä»¶å±‚é¢ï¼šæ•ˆç‡æå‡è¶… 4Ã—
| æ¨¡å— | Clock | Area (Î¼mÂ²) | Power (mW) | Throughput | Efficiency |
|------|-------|-------------|------------|------------|------------|
| NN-LUT | 1GHz | 23,238 | 46 | 1G | 0.94 |
| RI-LUT | 1GHz | 23,647 | 48 | 1G | 0.88 |
| **NLI** | 1GHz | **7,787** | **34** | 1G | **3.78** |

> âœ… **NLI æ•ˆç‡æ˜¯ NN-LUT çš„ 4.02Ã—ï¼Œæ˜¯ RI-LUT çš„ 4.29Ã—**
>
> âœ… é¢ç§¯èŠ‚çœ **68â€“69%**ï¼ŒåŠŸè€—é™ä½çº¦ **26â€“30%**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **NLI å®ç°äº†é«˜ç²¾åº¦ã€å…æ ¡å‡†ã€å¹¿é€‚ç”¨çš„éçº¿æ€§è¿‘ä¼¼**ï¼š
   - åœ¨ LLMs å’Œ Vision Models ä¸Šå‡æ— ç²¾åº¦æŸå¤±ï¼›
   - æ”¯æŒè¾“å…¥èŒƒå›´è¾¾ [-150,150]ï¼Œè¦†ç›– >99.9% çš„å®é™…æ¿€æ´»ï¼›
   - æœ€åç»å¯¹è¯¯å·®æ§åˆ¶åœ¨ $<1.5\times10^{-3}$ å†…ã€‚

2. **åŠ¨æ€è§„åˆ’å¯æœ‰æ•ˆæ±‚è§£å…¨å±€æœ€ä¼˜åˆ‡åˆ†ç‚¹**ï¼š
   - å°† cutpoint selection å»ºæ¨¡ä¸º DP é—®é¢˜ï¼Œåˆ©ç”¨æœ€ä¼˜å­ç»“æ„æ€§è´¨è·å¾—ç†è®ºæœ€ä¼˜è§£ï¼›
   - ç»“åˆä¸¤é˜¶æ®µè®¾è®¡ï¼ˆmacro + microï¼‰ï¼Œå…¼é¡¾ç²¾åº¦ä¸æ•ˆç‡ã€‚

3. **è½¯ç¡¬ååŒè®¾è®¡å¤§å¹…æå‡ç¡¬ä»¶æ•ˆç‡**ï¼š
   - ä¸¤çº§åœ°å€è½¬æ¢å°†æ¯”è¾ƒå™¨ä» 259 å‡å°‘åˆ° 10ï¼›
   - æµæ°´çº¿ç»“æ„ä¿è¯é«˜ååï¼›
   - åœ¨ SMIC 28nm å·¥è‰ºä¸‹éªŒè¯ï¼Œé¢ç§¯å’ŒåŠŸè€—ä¼˜åŠ¿æ˜¾è‘—ã€‚

### âš ï¸ å±€é™æ€§
- **å½“å‰ä»…æ”¯æŒ FP16 è¾“å…¥è¾“å‡º**ï¼Œæœªæ‰©å±•è‡³ INT8 æˆ–æ›´ä½æ¯”ç‰¹ï¼›
- **é¢„å®šä¹‰å®åŒºé—´ç»“æ„å¯èƒ½é™åˆ¶çµæ´»æ€§**ï¼Œè‹¥æŸäº›å‡½æ•°å±€éƒ¨å˜åŒ–å‰§çƒˆï¼Œå¯èƒ½éœ€è‡ªé€‚åº”åˆ’åˆ†ï¼›
- **åˆå§‹åŒ–ä¾èµ–ç¦»çº¿ DP æœç´¢**ï¼Œè™½ä¸€æ¬¡ç”Ÿæˆå¯ç”¨å¤šæ¬¡ï¼Œä½†ä»æœ‰ä¸€å®šè®¡ç®—æˆæœ¬ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³ **æ··åˆç²¾åº¦æ”¯æŒ**ï¼ˆå¦‚ INT8 + FP16ï¼‰ï¼›
- æ¢ç´¢ **åœ¨çº¿è‡ªé€‚åº”åˆ‡åˆ†æœºåˆ¶**ï¼Œåº”å¯¹æç«¯ outlierï¼›
- å°† NLI é›†æˆè¿›ä¸»æµ NPU æ¶æ„ï¼ˆå¦‚ Gemminiã€TPUï¼‰ï¼›
- åº”ç”¨äº MoE æ¶æ„ä¸­çš„ Sigmoid è·¯ç”±å‡½æ•°è¿‘ä¼¼ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> NLI æå‡ºäº†ä¸€ç§ **å…æ ¡å‡†ã€åŠ¨æ€è§„åˆ’æœ€ä¼˜ã€ç¡¬ä»¶é«˜æ•ˆ** çš„éçº¿æ€§å‡½æ•°è¿‘ä¼¼æ¡†æ¶ï¼Œè§£å†³äº† LLM æ¨ç†ä¸­éçº¿æ€§å±‚çš„æ•ˆç‡ç“¶é¢ˆï¼Œåœ¨ä¿æŒ **é›¶ç²¾åº¦æŸå¤±** çš„åŒæ—¶ï¼Œå®ç°äº† **è¶…è¿‡ 4Ã— çš„ç¡¬ä»¶æ•ˆç‡æå‡**ï¼Œå…·æœ‰æå¼ºçš„å®ç”¨æ€§å’Œæ¨å¹¿å‰æ™¯ã€‚

</details>

---

### 14. [CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs](https://arxiv.org/abs/2602.03048)

**Authors**: Zhiyuan Yao, Yi-Kai Zhang, Yuxin Chen, Yueqing Sun, Zishan Xu, Yu Yang, Tianhao Hu, Qi Gu, Hui Su, Xunliang Cai  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.03048v1  

#### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šCoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
åœ¨åŸºäº **Reinforcement Learning with Verifiable Rewards (RLVR)** çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè®­ç»ƒä¸­ï¼Œä¸»æµæ–¹æ³•å¦‚ **Group Relative Policy Optimization (GRPO)** é‡‡ç”¨**å›ºå®šçš„ rollout é¢„ç®—åˆ†é…ç­–ç•¥**ï¼ˆå³æ¯ä¸ªä»»åŠ¡æ ·æœ¬ç”Ÿæˆç›¸åŒæ•°é‡çš„å“åº”ï¼‰ï¼Œå¯¼è‡´èµ„æºåˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚ç®€å•ä»»åŠ¡å¯èƒ½è¢«è¿‡åº¦æ¢ç´¢ï¼Œè€Œå¤æ‚ä»»åŠ¡å› é¢„ç®—ä¸è¶³æœªèƒ½å……åˆ†å­¦ä¹ ã€‚

æ­¤å¤–ï¼Œç°æœ‰è‡ªé€‚åº”é¢„ç®—åˆ†é…æ–¹æ³•ï¼ˆå¦‚ Knapsack-RLï¼‰é€šå¸¸ä¾èµ–äºé™æ€æˆ–å¯å‘å¼çš„ä»·å€¼å‡½æ•°ï¼ˆä¾‹å¦‚åŸºäºå†å²é€šè¿‡ç‡ï¼‰ï¼Œ**å¿½ç•¥äº†æ¨¡å‹èƒ½åŠ›åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ¨æ€æ¼”åŒ–**ï¼Œæ— æ³•å®æ—¶è°ƒæ•´â€œæ¢ç´¢â€ï¼ˆexplorationï¼‰ä¸â€œåˆ©ç”¨â€ï¼ˆexploitationï¼‰ä¹‹é—´çš„æƒè¡¡ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡º **CoBA-RL (Capability-Oriented Budget Allocation for Reinforcement Learning)**ï¼Œä¸€ç§**åŸºäºæ¨¡å‹èƒ½åŠ›æ¼”åŒ–çš„åŠ¨æ€é¢„ç®—åˆ†é…ç®—æ³•**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

- **Capability-Oriented Value Function**ï¼š  
  è®¾è®¡äº†ä¸€ä¸ªä»¥ **Beta åˆ†å¸ƒå»ºæ¨¡çš„èƒ½åŠ›æ„ŸçŸ¥ä»·å€¼å‡½æ•°**ï¼Œè¯¥å‡½æ•°å°†æ¯ä¸ªä»»åŠ¡å®ä¾‹ $x_i$ æ˜ å°„ä¸ºå…¶æ½œåœ¨è®­ç»ƒå¢ç›Šã€‚å‡½æ•°å½¢çŠ¶ç”±å½“å‰è®­ç»ƒæ‰¹æ¬¡çš„**å…¨å±€å¤±è´¥ç‡ï¼ˆGlobal Failure Rateï¼‰** åŠ¨æ€è°ƒèŠ‚ï¼Œä»è€Œåæ˜ æ¨¡å‹å½“å‰èƒ½åŠ›æ°´å¹³ã€‚

- **åŠ¨æ€æ¢ç´¢-åˆ©ç”¨æƒè¡¡æœºåˆ¶**ï¼š  
  åœ¨è®­ç»ƒåˆæœŸï¼Œæ¨¡å‹èƒ½åŠ›å¼±ã€å¤±è´¥ç‡é«˜ï¼Œä»·å€¼å‡½æ•°å€¾å‘äºä¼˜å…ˆåˆ†é…èµ„æºç»™**é«˜é€šè¿‡ç‡ï¼ˆç®€å•ï¼‰ä»»åŠ¡**ï¼ˆexploitationï¼‰ï¼Œå¿«é€Ÿå»ºç«‹åŸºç¡€èƒ½åŠ›ï¼›éšç€æ¨¡å‹è¿›æ­¥ï¼Œå¤±è´¥ç‡ä¸‹é™ï¼Œå‡½æ•°è‡ªåŠ¨è½¬å‘**ä½é€šè¿‡ç‡ï¼ˆå›°éš¾ï¼‰ä»»åŠ¡**ï¼ˆexplorationï¼‰ï¼Œæ‹“å±•è§£é¢˜è¾¹ç•Œã€‚

- **é«˜æ•ˆåˆ†é…ä¼˜åŒ–ç®—æ³•ï¼ˆHeap-Based Greedy Strategyï¼‰**ï¼š  
  å°†é¢„ç®—åˆ†é…å½¢å¼åŒ–ä¸ºä¸€ä¸ªå¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶è®¾è®¡åŸºäºæœ€å¤§å †çš„è´ªå¿ƒç®—æ³•ï¼Œè¿­ä»£åœ°å°†é¢„ç®—å•å…ƒåˆ†é…ç»™è¾¹é™…æ”¶ç›Šæœ€é«˜çš„æ ·æœ¬ï¼Œå®ç°é«˜æ•ˆåœ¨çº¿åˆ†é…ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **åŠ¨æ€é€‚åº”æ€§**ï¼šé¦–æ¬¡æ˜¾å¼å»ºæ¨¡**æ¨¡å‹èƒ½åŠ›æ¼”åŒ–**å¯¹æ ·æœ¬è®­ç»ƒä»·å€¼çš„å½±å“ï¼Œè€Œéä¾èµ–é™æ€éš¾åº¦å‡è®¾ã€‚
- **æ›´é«˜çš„èµ„æºæ•ˆç‡**ï¼šç›¸æ¯”å‡åŒ€åˆ†é…ï¼ˆGRPOï¼‰å’Œé™æ€ç­–ç•¥ï¼Œèƒ½æ›´ç²¾å‡†è¯†åˆ«â€œé«˜è®­ç»ƒä»·å€¼â€æ ·æœ¬ï¼Œé¿å…è®¡ç®—æµªè´¹ã€‚
- **ç†è®ºä¿éšœ**ï¼šè¯æ˜äº†ä»·å€¼å‡½æ•°å…·æœ‰**é€’å‡è¾¹é™…æ”¶ç›Š**æ€§è´¨ï¼Œä¿è¯è´ªå¿ƒç­–ç•¥å¯å¾—æœ€ä¼˜è§£ã€‚
- **è®¡ç®—é«˜æ•ˆ**ï¼šå †ä¼˜åŒ–ç­–ç•¥æ—¶é—´å¤æ‚åº¦ä¸º $O(B_{\text{total}} \log M)$ï¼Œè¿œä¼˜äºåŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰ç­‰æ–¹æ³•ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡è®­ç»ƒæµæ°´çº¿ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **è®­ç»ƒæ•°æ®**ï¼š`DAPO-Math-17K` â€”â€” ä¸€ä¸ªå¹¿æ³›ç”¨äºæ•°å­¦æ¨ç†ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé›†ã€‚
- **è¯„ä¼°åŸºå‡†**ï¼ˆå…±5ä¸ªæŒ‘æˆ˜æ€§æ•°å­¦æ¨ç†è¯„æµ‹é›†ï¼‰ï¼š
  - `AIME24`, `AIME25`
  - `AMC23`
  - `MATH500`
  - `OLYMPIAD Bench`

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹**ï¼š
  - `Qwen2.5-7B-Base`, `Qwen2.5-7B-Instruct`
  - `Qwen3-1.7B-Base`, `Qwen3-4B-Base`
- **è®­ç»ƒé…ç½®**ï¼š
  - æ¯ä¸ªä»»åŠ¡åˆå§‹ rollout æ•°é‡ $G = 16$
  - æ€» rollout é¢„ç®—å—é™ï¼ˆå¦‚ $B_{\text{total}} = 8192$ï¼‰
  - ä½¿ç”¨ **Verl** æ¡†æ¶ + **SGLang** æ¨ç†å¼•æ“
  - ä¼˜åŒ–å™¨ï¼šAdamW ($lr = 1\times10^{-6}$)
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - ä¸»è¦æŒ‡æ ‡ï¼š`avg@16`ï¼ˆ16æ¬¡é‡‡æ ·ä¸­çš„å¹³å‡å‡†ç¡®ç‡ï¼‰
  - å…¶ä»–åˆ†æï¼šä»»åŠ¡éš¾åº¦è¿ç§»çŸ©é˜µã€æ”¶æ•›é€Ÿåº¦ã€èµ„æºæ•ˆç‡æ›²çº¿

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| **GRPO** | åŸºçº¿ | å›ºå®š rollout é¢„ç®—ï¼Œæ— è‡ªé€‚åº”æœºåˆ¶ |
| **Knapsack-RL** | è‡ªé€‚åº”åŸºçº¿ | åŸºäºç»å…¸èƒŒåŒ…é—®é¢˜å»ºæ¨¡ï¼Œæœ€å¤§åŒ–æ‰¹æ¬¡æ€»ä»·å€¼ï¼Œä½†ä½¿ç”¨é™æ€ä»·å€¼å‡½æ•° |
| **Static Strategies** | æ¶ˆèå¯¹æ¯” | å›ºå®šå‚æ•°çš„ Beta åˆ†å¸ƒï¼ˆå¦‚ $(\alpha,\beta)=(10.5,1.5)$ å¼ºè°ƒåˆ©ç”¨ï¼‰ |
| **Linear Step Decay** | å¯å‘å¼åŸºçº¿ | å‚æ•° $\alpha$ éšè®­ç»ƒæ­¥æ•°çº¿æ€§è¡°å‡ï¼Œé¢„è®¾è°ƒåº¦ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰
åœ¨ `Qwen2.5-7B-Instruct` ä¸Šçš„æ•´ä½“è¡¨ç°ï¼ˆAvg@16ï¼‰ï¼š

| Method        | AIME24 | AIME25 | AMC23  | MATH500 | OLYMPIAD | **Avg**  |
|---------------|--------|--------|--------|---------|----------|----------|
| GRPO          | 14.17  | 12.71  | 69.84  | 76.78   | 37.68    | **42.24** |
| Knapsack-RL   | 18.54  | 15.21  | 71.41  | 80.55   | 41.23    | **45.39** |
| **CoBA-RL (Ours)** | **18.96** | **18.33** | **73.12** | **80.30** | **43.19** | **46.78** |

- **ç›¸å¯¹ GRPO æå‡ +4.54%**ï¼Œæ˜¾è‘—è¶…è¶Š Knapsack-RL (+1.39%)ã€‚
- åœ¨ `AIME25` ä¸Šæå‡å°¤ä¸ºæ˜æ˜¾ï¼šä» 12.71% â†’ **18.33%**ï¼ˆ+5.62%ï¼‰ï¼Œè¡¨æ˜å¯¹éš¾é¢˜çš„æ¢ç´¢æ›´æœ‰æ•ˆã€‚

åœ¨å…¶ä»–æ¨¡å‹ä¸Šä¹Ÿä¸€è‡´é¢†å…ˆï¼š
- `Qwen2.5-7B-Base`: **47.43%** vs GRPO 43.68% (+3.75%)
- `Qwen3-4B-Base`: **49.46%** vs GRPO 44.72% (+4.74%)
- `Qwen3-1.7B-Base`: **35.75%** vs GRPO 31.61% (+4.14%)

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **å…¨é¢è¶…è¶Š GRPO å’Œ Knapsack-RL**ï¼šåœ¨æ‰€æœ‰æ¨¡å‹å’Œå¤§å¤šæ•°å­ä»»åŠ¡ä¸Šå‡å–å¾—æœ€ä½³æ€§èƒ½ã€‚
- **ä¼˜äºé™æ€ä¸å¯å‘å¼ç­–ç•¥**ï¼ˆTable 3ï¼‰ï¼š
  - CoBA-RL è¾¾åˆ° **46.78%**ï¼Œè¶…è¿‡æœ€ä½³é™æ€ç­–ç•¥ (45.21%) å’Œçº¿æ€§è¡°å‡ (45.39%)ã€‚
  - è¡¨æ˜**åŠ¨æ€é€‚åº”èƒ½åŠ›æ¼”åŒ–**æ¯”é¢„è®¾è§„åˆ™æ›´æœ‰æ•ˆã€‚

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰ä¸åŒé¢„ç®—ä¸‹çš„æ€§èƒ½ï¼ˆFigure 6ï¼‰
- å½“æ€»é¢„ç®—å—é™ä¸º `B_total=2048` æ—¶ï¼ŒCoBA-RL å‡†ç¡®ç‡è¾¾ **45.52%**ã€‚
- æ­¤æˆç»©**è¶…è¿‡ GRPO åœ¨åŒå€é¢„ç®—ï¼ˆ4096ï¼‰ä¸‹çš„è¡¨ç°ï¼ˆ42.78%ï¼‰**ï¼ŒéªŒè¯å…¶å“è¶Šçš„æ•°æ®ä¸è®¡ç®—æ•ˆç‡ã€‚

#### ï¼ˆ2ï¼‰è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æï¼ˆFigure 8ï¼‰
- å¯¹ Beta åˆ†å¸ƒå‚æ•°å’Œ $K=\alpha+\beta$ çš„å˜åŒ–è¡¨ç°å‡ºå¼ºé²æ£’æ€§ã€‚
- æœ€ä¼˜è®¾ç½® $K=11$ å–å¾—å³°å€¼æ€§èƒ½ **46.61%**ã€‚

#### ï¼ˆ3ï¼‰ä»»åŠ¡éš¾åº¦è¿ç§»åˆ†æï¼ˆFigure 7ï¼‰
- CoBA-RL åœ¨å„ç±»éš¾åº¦ä»»åŠ¡ä¸Šçš„è½¬åŒ–ç‡å‡æœ€é«˜ï¼š
  - **Hard ä»»åŠ¡**ï¼šè½¬åŒ–ç‡ **36.7%** vs GRPO (17.3%)ï¼Œæ¥è¿‘ç¿»å€ã€‚
  - **Medium ä»»åŠ¡**ï¼š**71.2%** vs GRPO (46.8%)ã€‚
  - **Easy ä»»åŠ¡ä¿ç•™ç‡**ï¼š**88.8%** vs GRPO (74.0%)ï¼Œæ˜¾ç¤ºå…¶åœ¨å·©å›ºå·²æœ‰çŸ¥è¯†æ–¹é¢åŒæ ·å‡ºè‰²ã€‚

#### ï¼ˆ4ï¼‰è¿è¡Œæ—¶æ•ˆç‡å¯¹æ¯”ï¼ˆTable 4ï¼‰
| æ–¹æ³• | åˆ†é…è€—æ—¶ï¼ˆç§’ï¼‰ | åŠ é€Ÿæ¯” |
|------|----------------|--------|
| Dynamic Programming (DP) | 115.05 | ~1Ã— |
| **Heap-Based Greedy (Ours)** | **0.124** | **~928Ã—** |

- å®ç°è¿‘ä¸‰ä¸ªæ•°é‡çº§çš„é€Ÿåº¦æå‡ï¼Œæ»¡è¶³åœ¨çº¿è®­ç»ƒéœ€æ±‚ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **è®­ç»ƒä»·å€¼æ˜¯åŠ¨æ€çš„**ï¼šæ ·æœ¬çš„â€œè®­ç»ƒä»·å€¼â€å¹¶éä»…ç”±å…¶å›ºæœ‰éš¾åº¦å†³å®šï¼Œè€Œæ˜¯**é«˜åº¦ä¾èµ–äºæ¨¡å‹å½“å‰çš„èƒ½åŠ›çŠ¶æ€**ã€‚
2. **æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼åº”æ˜¯â€œå…ˆåˆ©ç”¨åæ¢ç´¢â€**ï¼ˆExploit â†’ Exploreï¼‰ï¼š
   - åˆæœŸèšç„¦ç®€å•ä»»åŠ¡ä»¥å¿«é€Ÿæå‡åŸºç¡€èƒ½åŠ›ï¼›
   - åæœŸè½¬å‘å›°éš¾ä»»åŠ¡ä»¥æ‰©å±•è§£ç©ºé—´ã€‚
3. **é‡åŒ–è®­ç»ƒä»·å€¼å¹¶ä¼˜åŒ–é¢„ç®—åˆ†é…æ˜¯æå‡ LLM åè®­ç»ƒæ•ˆç‡çš„å…³é”®è·¯å¾„**ã€‚
4. CoBA-RL èƒ½**è‡ªä¸»æ ¡å‡†æ¢ç´¢-åˆ©ç”¨å¹³è¡¡**ï¼Œæ— éœ€äººå·¥è®¾è®¡è°ƒåº¦ç­–ç•¥ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- ä¾èµ–äº**äºŒå€¼å¥–åŠ±ä¿¡å·**ï¼ˆpass/failï¼‰ï¼Œéš¾ä»¥å¤„ç†ç»†ç²’åº¦åé¦ˆã€‚
- å½“å‰æ¡†æ¶ä¸»è¦é’ˆå¯¹æ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œæ˜¯å¦æ³›åŒ–è‡³å…¶ä»–é¢†åŸŸï¼ˆå¦‚ä»£ç ç”Ÿæˆã€å¯¹è¯ï¼‰éœ€è¿›ä¸€æ­¥éªŒè¯ã€‚
- Beta åˆ†å¸ƒå»ºæ¨¡è™½çµæ´»ï¼Œä½†ä»æ˜¯ä¸€ç§ç®€åŒ–å‡è®¾ï¼Œå¯èƒ½æ— æ³•æ•æ‰æ›´å¤æ‚çš„åå¥½æ¨¡å¼ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³å¤šç»´å¥–åŠ±ä¿¡å·ä¸‹çš„é¢„ç®—åˆ†é…ã€‚
- ç»“åˆ **curriculum learning** ä¸ CoBA-RLï¼Œå®ç°æ ·æœ¬é€‰æ‹©ä¸é¢„ç®—åˆ†é…çš„è”åˆä¼˜åŒ–ã€‚
- æ¢ç´¢æ›´å¤æ‚çš„ capability modeling æ–¹å¼ï¼ˆå¦‚å¼•å…¥éšå˜é‡æˆ–ç¥ç»ç½‘ç»œä¼°è®¡ï¼‰ã€‚
- åº”ç”¨äºæ›´å¤§è§„æ¨¡æ¨¡å‹ï¼ˆ>10Bï¼‰åŠçœŸå®ä¸–ç•Œ agentic ä»»åŠ¡ã€‚

---

> âœ… **ä¸€å¥è¯æ€»ç»“**ï¼š  
> CoBA-RL é€šè¿‡æ„å»ºä¸€ä¸ª**éšæ¨¡å‹èƒ½åŠ›æ¼”åŒ–çš„ Capability-Oriented Value Function**ï¼Œå®ç°äº† rollout é¢„ç®—çš„åŠ¨æ€ã€é«˜æ•ˆåˆ†é…ï¼Œåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Š GRPO å’Œ Knapsack-RLï¼Œæ­ç¤ºäº†â€œ**åŠ¨æ€æ„ŸçŸ¥èƒ½åŠ› + è‡ªé€‚åº”èµ„æºè°ƒåº¦**â€æ˜¯æå‡ LLM å¼ºåŒ–å­¦ä¹ æ•ˆç‡çš„æ ¸å¿ƒèŒƒå¼ã€‚

</details>

---

### 15. [Lookahead Path Likelihood Optimization for Diffusion LLMs](https://arxiv.org/abs/2602.03496)

**Authors**: Xuejie Liu, Yap Vit Chun, Yitao Liang, Anji Liu  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.03496v1  

#### Abstract
Diffusion Large Language Models (dLLMs) support arbitrary-order generation, yet their inference performance critically depends on the unmasking order. Existing strategies rely on heuristics that greedily optimize local confidence, offering limited guidance for identifying unmasking paths that are gl...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šLookahead Path Likelihood Optimization for Diffusion LLMs**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
Diffusion Large Language Models (dLLMs) è™½ç„¶æ”¯æŒä»»æ„é¡ºåºç”Ÿæˆï¼ˆarbitrary-order generationï¼‰ï¼Œå…¶æ¨ç†æ€§èƒ½å´é«˜åº¦ä¾èµ–äº **unmasking order**ï¼ˆå³ token çš„è§£ç é¡ºåºï¼‰ã€‚ç°æœ‰çš„ç­–ç•¥é€šå¸¸åŸºäºå¯å‘å¼è§„åˆ™ï¼ˆå¦‚é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜æˆ–ç†µæœ€ä½çš„ tokenï¼‰è¿›è¡Œå±€éƒ¨ä¼˜åŒ–ï¼Œç¼ºä¹å¯¹å…¨å±€ä¸€è‡´ä¸”é«˜ç²¾åº¦è·¯å¾„çš„æŒ‡å¯¼ï¼Œå¯¼è‡´æ¬¡ä¼˜è§£ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**
æœ¬æ–‡æå‡ºäº†ä»¥ä¸‹ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼š

- **Path Log-Likelihood (Path LL)**  
  å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è½¨è¿¹æ¡ä»¶ç›®æ ‡å‡½æ•°â€”â€”**Path LL**ï¼Œå®šä¹‰ä¸ºåœ¨ç‰¹å®š unmasking è·¯å¾„ä¸‹çš„è”åˆä¼¼ç„¶ã€‚å®éªŒè¯æ˜ï¼ŒPath LL ä¸ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡æœ‰å¼ºç›¸å…³æ€§ï¼Œæ˜¯è¡¡é‡ç”Ÿæˆè´¨é‡çš„å¯é ä»£ç†æŒ‡æ ‡ã€‚

- **POKE (Path-Optimistic K-step LL Estimator)**  
  æå‡ºä¸€ç§é«˜æ•ˆçš„å€¼ä¼°è®¡å™¨ POKEï¼Œç”¨äºé¢„æµ‹éƒ¨åˆ†è§£ç è½¨è¿¹åœ¨æœªæ¥å¯èƒ½è¾¾åˆ°çš„æœŸæœ› Path LLã€‚è¯¥ä¼°è®¡å™¨é€šè¿‡å¼•å…¥ç†µæ ¡æ­£é¡¹æ¥ç¼“è§£å¹¶è¡Œ rollout ä¸­çš„ç‹¬ç«‹æ€§åå·®ï¼Œå¹¶æä¾›ä¹è§‚ä¸Šç•Œä»¥é¿å…è¿‡æ—©å‰ªæé«˜è´¨é‡è·¯å¾„ã€‚

- **POKE-SMC (Sequential Monte Carlo-based Search Framework)**  
  å°† POKE é›†æˆåˆ° SMC æ¡†æ¶ä¸­ï¼Œæ„å»ºä¸€ä¸ªåŠ¨æ€æœç´¢æœºåˆ¶ï¼Œåœ¨æ¨ç†æ—¶è‡ªé€‚åº”åœ°è¯†åˆ«æœ€ä¼˜ unmasking è·¯å¾„ã€‚åˆ©ç”¨é‡è¦æ€§æƒé‡ç»“åˆå½“å‰ç´¯ç§¯ LL å’Œæœªæ¥ä¼°è®¡å€¼ï¼Œå®ç°ä»·å€¼å¼•å¯¼çš„ç²’å­ç­›é€‰ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **ç›®æ ‡è®¾è®¡** | ä½¿ç”¨ Path LL æ›¿ä»£ä¼ ç»Ÿ ELBO æˆ–ç†µå¯å‘å¼ï¼Œæ›´è´´è¿‘å®é™…ä»»åŠ¡æ€§èƒ½ |
| **æœç´¢æ•ˆç‡** | POKE å®ç°é«˜æ•ˆ lookahead è¯„ä¼°ï¼Œæ— éœ€è®­ç»ƒé¢å¤–çš„ä»·å€¼ç½‘ç»œ |
| **å…¨å±€ä¼˜åŒ–èƒ½åŠ›** | å…‹æœè´ªå©ªç­–ç•¥çš„çŸ­è§†é—®é¢˜ï¼Œæ¢ç´¢å…·æœ‰é«˜å…¨å±€æ½œåŠ›çš„è·¯å¾„ |
| **å¯æ‰©å±•æ€§** | åœ¨ä¿æŒåˆç†æ¨ç†å¼€é”€çš„åŒæ—¶æ˜¾è‘—æå‡å‡†ç¡®æ€§ï¼Œæ¨åŠ¨ accuracy-compute Pareto frontier |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
å®éªŒè¦†ç›– **6ä¸ªæ¨ç†ä»»åŠ¡**ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
- **GSM8K**ï¼šå°å­¦æ•°å­¦åº”ç”¨é¢˜ï¼Œæµ‹è¯•å¤šæ­¥æ¨ç†èƒ½åŠ›
- **MATH500**ï¼šé«˜ç­‰æ•°å­¦é—®é¢˜é›†åˆ
- **Countdown**ï¼šæ•°å€¼è®¡ç®—æŒ‘æˆ˜ä»»åŠ¡
- ï¼ˆå…¶ä»–æœªæ˜ç¡®å‘½åçš„ä»»åŠ¡ä¹ŸåŒ…å«åœ¨å†…ï¼Œå…±6é¡¹ï¼‰

### **å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡**
- **æ¨¡å‹åŸºç¡€**ï¼šåŸºäº LLaDA ç³»åˆ— dLLMsï¼ˆå¦‚ LLaDA-8B-Instructï¼‰
- **æ¨ç†é…ç½®**ï¼š
  - æ‰©æ•£æ­¥æ•° $ T = 256 $
  - åºåˆ—é•¿åº¦ 256 tokens
  - å¹¶è¡Œè§£ç å—å¤§å° 32
  - ä½¿ç”¨é›¶æ ·æœ¬è®¾å®šï¼ˆzero-shotï¼‰
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - ä¸»è¦ï¼š**Accuracy (%)**ï¼ˆç­”æ¡ˆæ­£ç¡®ç‡ï¼‰
  - è¾…åŠ©ï¼š**Latency (s)**ã€**accuracy-compute trade-off**
  - å†…éƒ¨éªŒè¯ï¼šPath LL ä¸æœ€ç»ˆå‡†ç¡®æ€§çš„ç›¸å…³æ€§åˆ†æ

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **PC-Sampler**ï¼šåŸºäºä½ç½®æ„ŸçŸ¥åç½®æ ¡å‡†çš„æ–¹æ³•
- **Majority Voting**ï¼šå¤šæ¬¡é‡‡æ ·å–å¤šæ•°æŠ•ç¥¨
- **E-SMC**ï¼šåŸºäºç†µçš„ SMC æ–¹æ³•
- **Random / Confidence-based Heuristics**ï¼šéšæœºæˆ–åŸºäºæœ€å¤§æ¦‚ç‡é€‰æ‹© unmasking é¡ºåº
- **LookUM (Lee et al., 2025)**ï¼šå­¦ä¹ å‹è·¯å¾„è§„åˆ’æ–¹æ³•ï¼ˆæ¶ˆèå®éªŒä¸­æåŠï¼‰

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
| æ–¹æ³• | Latency (s) | Accuracy (%) |
|------|-------------|--------------|
| PC-Sampler | 7.9 | 36.1 |
| Majority Voting | 9.5 / 18.0 / 32.6 | 34.0 / 36.0 / 36.9 |
| E-SMC | 9.5 / 18.0 / 32.6 | 35.4 / 38.3 / 39.8 |
| **POKE-SMC** | **8.9 / 22.1 / 34.7** | **36.4 / 42.3 / 40.8** |

> æ•°æ®æ¥è‡ª Table 10ï¼ˆCountdown ä»»åŠ¡ï¼‰ï¼Œæ˜¾ç¤º POKE-SMC åœ¨ç›¸è¿‘å»¶è¿Ÿä¸‹å–å¾—æ›´é«˜å‡†ç¡®ç‡ã€‚

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- åœ¨æ‰€æœ‰ 6 ä¸ªæ¨ç†ä»»åŠ¡ä¸Šï¼Œ**POKE-SMC ä¸€è‡´ä¼˜äºå¼ºåŸºçº¿**ã€‚
- ç›¸æ¯”å½“å‰å…ˆè¿›çš„ decoding-time scaling æ–¹æ³•ï¼Œå¹³å‡æå‡ **2%-3% ç»å¯¹å‡†ç¡®ç‡**ã€‚
- åœ¨é«˜åååœºæ™¯ï¼ˆæ¯æ­¥è§£ç ä¸¤ä¸ª tokenï¼‰ä¸‹ä»ä¿æŒé¢†å…ˆï¼Œåˆ†åˆ«åœ¨ä¸¤ä¸ª LLaDA æ¨¡å‹ä¸Šå®ç° **5.4% å’Œ 4.9% çš„å¹³å‡å¢ç›Š**ã€‚
- æ˜¾è‘—æ”¹è¿›äº† **accuracy-compute Pareto frontier**ï¼Œå³åœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹è·å¾—æ›´é«˜å‡†ç¡®ç‡ï¼Œæˆ–åœ¨ç›¸åŒå‡†ç¡®ç‡ä¸‹é™ä½è®¡ç®—æˆæœ¬ã€‚

### **æ¶ˆèå®éªŒç»“æœ**
- **POKE vs. Product LL**ï¼šæœ´ç´ ä¹˜ç§¯ä¼¼ç„¶ï¼ˆproduct LLï¼‰ä¸¥é‡ä½ä¼°çœŸå®è”åˆä¼¼ç„¶ï¼Œå°¤å…¶åœ¨å¤§å¹¶è¡Œåº¦ä¸‹ï¼›è€Œ POKE èƒ½æœ‰æ•ˆå¼¥åˆå·®è·ï¼ˆè§ Figure 2ï¼‰ã€‚
- **K å€¼å½±å“ï¼ˆlookahead horizonï¼‰**ï¼šå½“ $ K=16 $ï¼ˆå³æ¯ç»„ $|A|=16$ï¼‰æ—¶æ•ˆæœæœ€ä½³ï¼Œè¯´æ˜é€‚åº¦å‰ç»ä¼˜äºæç«¯çŸ­è§†æˆ–é•¿ç¨‹æ¨¡æ‹Ÿã€‚
- **resampling interval ä¸æ¸©åº¦æ•æ„Ÿæ€§åˆ†æ**ï¼šå‘¨æœŸæ€§é‡é‡‡æ ·å¯åœ¨æ•ˆç‡ä¸æ€§èƒ½é—´å–å¾—è‰¯å¥½å¹³è¡¡ï¼ˆè¯¦è§ Appendix Cï¼‰ã€‚
- **remasking æ¸©åº¦è®¾ç½®**ï¼šä½¿ç”¨ entropy-based remaskingï¼ˆæ¸©åº¦=1.0ï¼‰æœ‰åŠ©äºç»´æŒå¤šæ ·æ€§ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **Path LL æ˜¯ä¼˜ç§€çš„ç”Ÿæˆè´¨é‡ä»£ç†æŒ‡æ ‡**  
   å®éªŒè¡¨æ˜ï¼ŒPath LL ä¸ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡çš„ç›¸å…³æ€§è¿œé«˜äº ELBO æˆ–è¾¹é™…ç†µç­‰ä¼ ç»ŸæŒ‡æ ‡ï¼Œé€‚åˆä½œä¸º inference-time optimization çš„ç›®æ ‡ã€‚

2. **å±€éƒ¨æœ€ä¼˜ â‰  å…¨å±€æœ€ä¼˜**  
   è´ªå©ªç­–ç•¥è™½å¿«ï¼Œä½†å®¹æ˜“é™·å…¥ä½è´¨é‡è·¯å¾„ï¼›é€šè¿‡ lookahead å’Œå…¨å±€æœç´¢ï¼ˆå¦‚ SMC + POKEï¼‰ï¼Œå¯ä»¥å‘ç°æ›´å…·æ½œåŠ›çš„è§£ç é¡ºåºã€‚

3. **POKE å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„æœªæ¥ä»·å€¼ä¼°è®¡**  
   åˆ©ç”¨éšæœº rollout åŠ ç†µä¿®æ­£ï¼ŒPOKE åœ¨ä¸å¢åŠ è¿‡å¤šè®¡ç®—è´Ÿæ‹…çš„å‰æä¸‹æä¾›äº†å¯é çš„æœªæ¥ Path LL ä¸Šç•Œä¼°è®¡ã€‚

4. **POKE-SMC æ¨åŠ¨ accuracy-compute æƒè¡¡è¾¹ç•Œå‰ç§»**  
   åœ¨å¤šä¸ªä»»åŠ¡å’Œä¸åŒè®¡ç®—é¢„ç®—ä¸‹å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºé€šç”¨ inference-time scaling æ¡†æ¶çš„æ½œåŠ›ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- å½“å‰å®éªŒä»…é™äº **8B è§„æ¨¡æ¨¡å‹**ï¼ˆå¦‚ LLaDAï¼‰ï¼Œå°šæœªéªŒè¯åœ¨æ›´å¤§æ¨¡å‹ï¼ˆå¦‚ LLaDA2.0ï¼‰ä¸Šçš„å¯æ‰©å±•æ€§ã€‚
- **resampling ç­–ç•¥è¾ƒç®€å•**ï¼Œé‡‡ç”¨å›ºå®šå‘¨æœŸé‡é‡‡æ ·ï¼Œæœªå¼•å…¥åŠ¨æ€å›é€€æˆ–å†ç”Ÿæœºåˆ¶ã€‚
- ä¾èµ–äºå›ºå®šçš„æ‰©æ•£è°ƒåº¦å’Œ block sizeï¼Œçµæ´»æ€§æœ‰å¾…è¿›ä¸€æ­¥å¢å¼ºã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **æ‰©å±•è‡³æ›´å¤§è§„æ¨¡ dLLMs**  
   éªŒè¯ POKE-SMC åœ¨æ›´å¼º base model ä¸Šçš„æ•ˆæœï¼Œæ¢ç´¢æ˜¯å¦ä»èƒ½ç»´æŒå¢ç›Šã€‚
   
2. **å¼€å‘æ›´æ™ºèƒ½çš„ resampling ä¸ rollback æœºåˆ¶**  
   åˆ©ç”¨ Path LL çš„å…¨å±€å¯æ¯”æ€§ï¼Œè®¾è®¡è‡ªé€‚åº” rollback ç­–ç•¥ï¼Œå½“æ£€æµ‹åˆ°è·¯å¾„åç¦»é«˜æ¦‚ç‡åŒºåŸŸæ—¶ä¸»åŠ¨ä¿®æ­£ã€‚

3. **æ¢ç´¢éå•è°ƒè§£ç èŒƒå¼**  
   è¶…è¶Šä¼ ç»Ÿçš„â€œé€æ­¥ unmaskâ€æ¨¡å¼ï¼Œç ”ç©¶å…è®¸ remasking ä¸ rethinking çš„é—­ç¯æ¨ç†æµç¨‹ã€‚

4. **ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯èåˆ**  
   ç»“åˆ RL fine-tuning æˆ– prompt engineeringï¼Œå½¢æˆç«¯åˆ°ç«¯ä¼˜åŒ– pipelineã€‚

--- 

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬è®ºæ–‡æå‡º **Path LL** ä½œä¸º dLLMs è§£ç è·¯å¾„é€‰æ‹©çš„æ–°ç›®æ ‡ï¼Œå¹¶é€šè¿‡ **POKE-SMC** å®ç°é«˜æ•ˆçš„ lookahead æœç´¢ï¼Œåœ¨å‡ ä¹ä¸å¢åŠ æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹ï¼Œç³»ç»Ÿæ€§æå‡äº†ç”Ÿæˆå‡†ç¡®ç‡ï¼Œä¸º diffusion LLMs çš„æ¨ç†ä¼˜åŒ–å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚

</details>

---

### 16. [Quantization-Aware Regularizers for Deep Neural Networks Compression](https://arxiv.org/abs/2602.03614)

**Authors**: Dario Malchiodi, Mattia Ferraretto, Marco Frasca  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.03614v1  

#### Abstract
Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential...

---

### 17. [LLM-Inspired Pretrain-Then-Finetune for Small-Data, Large-Scale Optimization](https://arxiv.org/abs/2602.03690)

**Authors**: Zishi Zhang, Jinhui Han, Ming Hu, Yijie Peng  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.03690v1  

#### Abstract
We consider small-data, large-scale decision problems in which a firm must make many operational decisions simultaneously (e.g., across a large product portfolio) while observing only a few, potentially noisy, data points per instance. Inspired by the success of large language models (LLMs), we prop...

---

### 18. [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)

**Authors**: Ratul Ali  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.00053v1  

#### Abstract
Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency...

---

### 19. [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)

**Authors**: Fabian P. Kr\"uger, Andrea Hunklinger, Adrian Wolny, Tim J. Adler, Igor Tetko, Santiago David Villalba  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.00663v1  

#### Abstract
Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, su...

---

### 20. [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)

**Authors**: Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou, Xiaoyue Ma, Jianing Li, Yao Zhu  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.00815v1  

#### Abstract
Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-in...

---

### 21. [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)

**Authors**: Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang, Weijie Liu, Clive Bai, Kai Yang, Yangkun Chen, Saiyong Yang, Xiangyang Ji  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.01970v1  

#### Abstract
Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, cu...

---

### 22. [Efficient Algorithms for Partial Constraint Satisfaction Problems over Control-flow Graphs](https://arxiv.org/abs/2602.03588)

**Authors**: Xuran Cai, Amir Goharshady  
**Category**: cs.CL  
**Published**: 2026-02-04  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.03588v1  

#### Abstract
In this work, we focus on the Partial Constraint Satisfaction Problem (PCSP) over control-flow graphs (CFGs) of programs. PCSP serves as a generalization of the well-known Constraint Satisfaction Problem (CSP). In the CSP framework, we define a set of variables, a set of constraints, and a finite do...

---

### 23. [Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models](https://arxiv.org/abs/2602.03681)

**Authors**: Difan Deng, Andreas Bentzen Winje, Lukas Fehring, Marius Lindauer  
**Category**: cs.CL  
**Published**: 2026-02-04  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.03681v1  

#### Abstract
The quadratic computational complexity of softmax transformers has become a bottleneck in long-context scenarios. In contrast, linear attention model families provide a promising direction towards a more efficient sequential model. These linear attention models compress past KV values into a single ...

---

### 24. [ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization](https://arxiv.org/abs/2602.02597)

**Authors**: Hongyuan Su, Yu Zheng, Yong Li  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.02597v1  

#### Abstract
Large language models are transforming systems research by automating the discovery of performance-critical algorithms for computer systems. Despite plausible codes generated by LLMs, producing solutions that meet the stringent correctness and performance requirements of systems demands iterative op...

---

### 25. [WARP Logic Neural Networks](https://arxiv.org/abs/2602.03527)

**Authors**: Lino Gerlach, Thore Gerlach, Liv V{\aa}ge, Elliott Kauffman, Isobel Ojalvo  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.03527v1  

#### Abstract
Fast and efficient AI inference is increasingly important, and recent models that directly learn low-level logic operations have achieved state-of-the-art performance. However, existing logic neural networks incur high training costs, introduce redundancy or rely on approximate gradients, which limi...

---

### 26. [Sparse Training of Neural Networks based on Multilevel Mirror Descent](https://arxiv.org/abs/2602.03535)

**Authors**: Yannick Lunk, Sebastian J. Scott, Leon Bungert  
**Category**: cs.LG  
**Published**: 2026-02-04  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.03535v1  

#### Abstract
We introduce a dynamic sparse training algorithm based on linearized Bregman iterations / mirror descent that exploits the naturally incurred sparsity by alternating between periods of static and dynamic sparsity pattern updates. The key idea is to combine sparsity-inducing Bregman iterations with a...

---

### 27. [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)

**Authors**: Yiliu He, Tianle Li, Binghao Ji, Zhiyuan Liu, Di Huang  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.01082v1  

#### Abstract
Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large langu...

---

### 28. [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)

**Authors**: Yiming Dong, Kun Fu, Haoyu Li, Xinyuan Zhu, Yurou Liu, Lijing Shao, Jieping Ye, Zheng Wang  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.01103v1  

#### Abstract
Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severel...

---

### 29. [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)

**Authors**: Shaoxiong Yang, Junting Li, Mengyuan Zhang, Chao Li, Wei Liu, Jian Luan  
**Category**: cs.AI  
**Published**: 2026-02-04  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.01222v1  

#### Abstract
Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we...

---

### 30. [ROSA-Tuning: Enhancing Long-Context Modeling via Suffix Matching](https://arxiv.org/abs/2602.02499)

**Authors**: Yunao Zheng, Xiaojie Wang, Lei Ren, Wei Chen  
**Category**: cs.CL  
**Published**: 2026-02-04  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.02499v1  

#### Abstract
Long-context capability and computational efficiency are among the central challenges facing today's large language models. Existing efficient attention methods reduce computational complexity, but they typically suffer from a limited coverage of the model state. This paper proposes ROSA-Tuning, a r...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
