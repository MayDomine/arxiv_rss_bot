# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-06 06:37:15 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs](https://arxiv.org/abs/2602.05191)

**Authors**: Wentao Ni, Kangqi Zhang, Zhongming Yu, Oren Nelson, Mingu Lee, Hong Cai, Fatih Porikli, Jongryool Kim, Zhijian Liu, Jishen Zhao  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2602.05191v1  

#### Abstract
As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
åœ¨ **long-context LLMs** çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿ï¼ˆå¯è¾¾æ•°ä¸‡ç”šè‡³æ•°åä¸‡ tokensï¼‰ï¼Œæ ‡å‡†çš„ **full attention** æœºåˆ¶å› è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜è®¿é—®å¼€é”€å‘ˆå¹³æ–¹çº§å¢é•¿ï¼Œæˆä¸ºè§£ç é˜¶æ®µçš„ä¸»è¦ç“¶é¢ˆã€‚è™½ç„¶ **sparse attention** è¢«å¹¿æ³›ç”¨äºç¼“è§£è¯¥é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹ç¼ºé™·ï¼š

- **Fixed-budget top-k sparse attention**ï¼šé‡‡ç”¨å›ºå®šæ•°é‡çš„ token è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œæ— æ³•é€‚åº”ä¸åŒå±‚ã€å¤´å’Œè§£ç æ­¥ä¸­æ³¨æ„åŠ›åˆ†å¸ƒçš„å¼‚è´¨æ€§ï¼Œå¯¼è‡´é¢„ç®—ä¸è¶³æˆ–è¿‡åº¦ä¿å®ˆã€‚
- **Token-level top-p sparse attention**ï¼ˆå¦‚ Twilightï¼‰ï¼šè™½èƒ½è‡ªé€‚åº”ä¿ç•™ç›®æ ‡ attention massï¼ˆå¦‚ p=0.95ï¼‰ï¼Œä½†å…¶ä¼°è®¡è¿‡ç¨‹ä¾èµ–äº token çº§åˆ«çš„è¿‘ä¼¼è®¡ç®—ï¼Œä¸”é€‰æ‹©å¼€é”€éšä¸Šä¸‹æ–‡çº¿æ€§å¢é•¿ï¼Œæ•ˆç‡ä½ä¸‹ã€‚
- **Cluster-based sparse attention**ï¼ˆå¦‚ RetroInferï¼‰ï¼šé€šè¿‡èšç±»é™ä½ç¨€ç–æ³¨æ„åŠ›æˆæœ¬ï¼Œä½†ä»ä¾èµ–å›ºå®šé¢„ç®—è¿›è¡Œ cluster æˆ– token é€‰æ‹©ï¼Œç¼ºä¹å¯¹ attention mass çš„æ˜¾å¼æ§åˆ¶ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šDouble-P
ä½œè€…æå‡º **Double-P** â€”â€”ä¸€ç§**åˆ†å±‚çš„ top-p ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶**ï¼Œä»ä¸‰ä¸ªå±‚é¢è”åˆä¼˜åŒ–ï¼š

1. **Cluster-level top-p estimation**ï¼š  
   åœ¨èšç±»çº§åˆ«ä½¿ç”¨ **size-weighted centroids** å¿«é€Ÿä¼°ç®— attention massï¼Œå®ç°ç²—ç²’åº¦ã€ä½å¼€é”€çš„ top-p é¢„ç­›é€‰ã€‚

2. **Adaptive token-level refinement**ï¼š  
   å¼•å…¥ç¬¬äºŒé˜¶æ®µçš„ top-p æ§åˆ¶ï¼ŒåŠ¨æ€å†³å®šå“ªäº› cluster éœ€è¦ç²¾ç¡®è®¡ç®— token-level attentionï¼Œå…¶ä½™åˆ™ç”¨ centroid è¿‘ä¼¼ã€‚

3. **GPU-efficient kernel å®ç°**ï¼š  
   è®¾è®¡èåˆçš„ GPU kernelsï¼Œæœ€å°åŒ– selection overhead å¹¶æå‡æ•°æ®å±€éƒ¨æ€§ï¼Œæ”¯æŒé«˜æ•ˆç¨€ç– attention è®¡ç®—ã€‚

> ğŸ” **æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ¦‚ç‡é©±åŠ¨çš„ top-p åŸåˆ™ä¸ç³»ç»Ÿå±‚çº§ç»“æ„ç»“åˆï¼Œå®ç°â€œ**ç²—ä¼° + ç»†è°ƒ**â€çš„ä¸¤é˜¶æ®µè‡ªé€‚åº”ç¨€ç–åŒ–ã€‚

### âš–ï¸ ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | Double-P | ä¼ ç»Ÿæ–¹æ³• |
|------|----------|---------|
| **Accuracy Control** | æ˜¾å¼ä¿è¯ top-p attention massï¼Œè¯¯å·®æ¥è¿‘é›¶ | å›ºå®šé¢„ç®—æ˜“è¿åè´¨é‡çº¦æŸ |
| **Selection Overhead** | Cluster-level estimation å¤§å¹…é™ä½å¼€é”€ | Token-level estimation å¼€é”€é«˜ |
| **Sparse Attention Cost** | è‡ªé€‚åº”åˆ†é… token è®¡ç®—ï¼Œé¿å…å†—ä½™ | å›ºå®šé¢„ç®—å¯¼è‡´æµªè´¹æˆ–ä¸è¶³ |
| **ç«¯åˆ°ç«¯æ•ˆç‡** | å®ç°æ›´é«˜åŠ é€Ÿæ¯”ä¸”æ— ç²¾åº¦æŸå¤± | åŠ é€Ÿä»¥ç‰ºç‰²ç²¾åº¦ä¸ºä»£ä»· |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **RULER** (Hsieh et al., 2024)ï¼šç»¼åˆæ€§ benchmarkï¼ŒåŒ…å« 13 ä¸ªä»»åŠ¡ï¼Œä¸Šä¸‹æ–‡é•¿åº¦è¦†ç›– 4Kâ€“128Kã€‚
- **LongBench** (Bai et al., 2024)ï¼šåŒè¯­å¤šä»»åŠ¡ long-context åŸºå‡†ï¼Œå« 21 ä¸ªå­ä»»åŠ¡ï¼Œè¾“å…¥å¹³å‡é•¿åº¦ 5Kâ€“15Kã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹**ï¼šLLaMA-3.1-8B å’Œ Qwen3-8Bï¼Œå‡æ”¯æŒé•¿è¾¾ 128K ä¸Šä¸‹æ–‡ã€‚
- **ç¡¬ä»¶**ï¼šå• NVIDIA H100 PCIe GPUï¼ˆ80GBï¼‰ï¼ŒCUDA 12.8ï¼ŒPyTorch 2.8ã€‚
- **ä¸Šä¸‹æ–‡é•¿åº¦**ï¼šæµ‹è¯• 16Kã€32Kã€64Kã€‚
- **Batch size**ï¼š1ã€2ã€4ã€‚
- **ä¿ç•™ç­–ç•¥**ï¼šæ‰€æœ‰æ–¹æ³•å‡ä¿ç•™ sink tokensï¼ˆ4ä¸ªï¼‰å’Œ sliding window tokensï¼ˆ64ä¸ªï¼‰ã€‚

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
- **Accuracy**ï¼šä¸‹æ¸¸ä»»åŠ¡å¹³å‡å¾—åˆ†ï¼ˆvs. full attentionï¼‰ã€‚
- **Decoding Latency**ï¼šæ¯è¾“å‡º token çš„ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆmsï¼‰ã€‚
- **Attention Computation Cost**ï¼šself-attention å­æ¨¡å—è€—æ—¶åˆ†è§£ã€‚
- **Speedup**ï¼šç›¸å¯¹äº full attention å’Œ baseline æ–¹æ³•çš„åŠ é€Ÿæ¯”ã€‚

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **Full Attention** | å…¨æ³¨æ„åŠ› | ç²¾åº¦ä¸Šé™ï¼Œé€Ÿåº¦æœ€æ…¢ |
| **Quest** (Tang et al., 2024b) | Page-level top-k | åŸºäº key bounds çš„ page æ£€ç´¢ï¼Œå– 25% tokens |
| **RetroInfer** (Chen et al., 2025) | Cluster-based | å›ºå®šæ•°é‡ cluster æ£€ç´¢ + centroid è¿‘ä¼¼ |
| **Twilight** (Lin et al., 2025) | Token-level top-p | å›ºå®šé¢„ç®—ä¸‹è¿›è¡Œ top-p ä¼°è®¡ä¸å‰ªæ |
| **Quest + Twilight** | Hybrid | Quest æ£€ç´¢åæ¥ Twilight çš„ top-p å‰ªæ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆLLaMA-3.1-8Bï¼‰

#### âœ… å‡†ç¡®ç‡è¡¨ç°ï¼ˆRULERï¼‰
| Method | 16K | 32K | 64K | Avg. |
|--------|-----|-----|-----|------|
| Full Attention | 93.25 | 90.00 | 85.36 | 89.54 |
| Quest | 89.24 | 85.95 | 83.61 | 86.27 |
| RetroInfer | 91.56 | 88.12 | 83.77 | 87.82 |
| Quest + Twilight | 86.73 | 86.50 | 81.87 | 85.03 |
| **Double-P (Ours)** | **92.87** | **89.91** | **84.55** | **89.08** |
| â†‘ æå‡ï¼ˆvs best baselineï¼‰ | +1.31 | +1.79 | +0.78 | **+1.26** |

> ğŸ’¡ Double-P åœ¨æ‰€æœ‰é•¿åº¦ä¸Šè¾¾åˆ°æœ€é«˜ç²¾åº¦ï¼Œ**ç›¸æ¯” full attention ä»…ä¸‹é™ ~0.46 ç‚¹**ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»– sparse æ–¹æ³•ã€‚

#### âœ… LongBench å‡†ç¡®ç‡
| Method | LongBench Avg. |
|--------|----------------|
| Full Attention | 39.33 |
| Quest | 38.76 |
| RetroInfer | 39.03 |
| **Double-P (Ours)** | **39.06** |

> è¡¨æ˜ Double-P åœ¨çœŸå®åœºæ™¯ä»»åŠ¡ä¸­ä¹Ÿå…·å¤‡å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

#### â±ï¸ æ•ˆç‡ä¸å»¶è¿Ÿï¼ˆEnd-to-End Decodingï¼‰
- åœ¨ 32K ä¸Šä¸‹æ–‡ä¸‹ï¼Œ**Double-P è¾ƒ Quest åŠ é€Ÿ 1.11Ã—ï¼Œè¾ƒ RetroInfer åŠ é€Ÿ 1.26Ã—**ã€‚
- ç›¸æ¯” full attentionï¼Œ**æœ€é«˜å®ç° 2.23Ã— ç«¯åˆ°ç«¯åŠ é€Ÿ**ã€‚
- æ³¨æ„åŠ›è®¡ç®—é˜¶æ®µæœ€é«˜æé€Ÿè¾¾ **1.78Ã—**ï¼ˆvs RetroInferï¼‰å’Œ **1.74Ã—**ï¼ˆvs Quest-Twilightï¼‰ã€‚

#### ğŸ“‰ æ³¨æ„åŠ›å¼€é”€åˆ†è§£ï¼ˆè§ Figure 9ï¼‰
- **Twilight / Token-level top-p**ï¼šSpGEMV å’Œ top-p selection å æ® >60% æ³¨æ„åŠ›æ—¶é—´ã€‚
- **Double-P**ï¼šé€šè¿‡ cluster-level estimation æå¤§å‹ç¼© selection overheadï¼Œæœ€ç»ˆ sparse attention æˆæœ¬æ›´ä½ã€‚

### ğŸ” æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰
- åŒé˜ˆå€¼ `(p1, p2)` æ§åˆ¶ cluster-level å’Œ token-level çš„ top-p ä¿ç•™æ¯”ä¾‹ï¼š
  - `p1`ï¼šcluster ç²—ç­› threshold
  - `p2`ï¼štoken ç²¾ç»†è®¡ç®—åˆ†é… threshold
- å®éªŒè¡¨æ˜ï¼š
  - æ›´é«˜çš„ `p1`, `p2` â†’ æ›´é«˜ accuracyï¼Œä½†æ›´ä½æ•ˆç‡
  - æ¨èé…ç½®ï¼š`(p1=0.95, p2=0.7)` åœ¨ LLaMA-3.1-8B ä¸Šå–å¾—æœ€ä½³æƒè¡¡
- ç»“æœéªŒè¯äº†ä¸¤é˜¶æ®µè®¾è®¡çš„æœ‰æ•ˆæ€§å’Œå¯è°ƒèŠ‚æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Fixed top-k æ— æ³•æ»¡è¶³ top-p è´¨é‡è¦æ±‚**ï¼š  
   å›¾ 2 æ˜¾ç¤ºï¼Œå³ä½¿ä½¿ç”¨å¹³å‡é¢„ç®—ï¼ˆå¦‚ k=256ï¼‰ï¼Œä»æœ‰è¶…è¿‡ 20% çš„ attention heads è¿å p=0.95 çš„ mass çº¦æŸã€‚

2. **Token-level top-p estimation ä¸å¯é ä¸”æ˜‚è´µ**ï¼š  
   å›¾ 3 æ˜¾ç¤ºï¼Œåœ¨æ¨èå›ºå®šé¢„ç®—ä¸‹ï¼Œ**91.9% çš„æ ·æœ¬æœªèƒ½æ¢å¤è¶³å¤Ÿçš„ attention mass**ï¼›å›¾ 4 æ˜¾ç¤º estimation å¼€é”€ä¸»å¯¼æ€»å»¶è¿Ÿã€‚

3. **Double-P å®ç°å¸•ç´¯æ‰˜å‰æ²¿ä¼˜åŠ¿**ï¼š  
   å¦‚ Figure 1 æ‰€ç¤ºï¼ŒDouble-P åœ¨ accuracy-latency æ›²çº¿ä¸Šå½¢æˆ **Pareto frontier**ï¼ŒåŒæ—¶å®ç°é«˜ç²¾åº¦ä¸ä½å»¶è¿Ÿã€‚

4. **Hierarchical design æ˜¯å…³é”®**ï¼š  
   åˆ©ç”¨ cluster ç»“æ„è¿›è¡Œå¿«é€Ÿ mass ä¼°è®¡ï¼Œå¹¶åŸºäº error åˆ†å¸ƒåŠ¨æ€åˆ†é… token è®¡ç®—èµ„æºï¼Œæ˜¯å®ç°é«˜æ•ˆ top-p sparse attention çš„æœ‰æ•ˆè·¯å¾„ã€‚

### âš ï¸ å±€é™æ€§
- **èšç±»æœ¬èº«æœ‰å¼€é”€**ï¼šprefill é˜¶æ®µéœ€æ‰§è¡Œ k-means èšç±»ï¼Œå¯èƒ½å¢åŠ é¢„å¤„ç†æ—¶é—´ï¼ˆå°½ç®¡ä¸€æ¬¡å®Œæˆï¼‰ã€‚
- **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼šéœ€è¦è°ƒä¼˜ `p1`, `p2` ä»¥å¹³è¡¡ç²¾åº¦ä¸é€Ÿåº¦ï¼Œä¸åŒæ¨¡å‹/ä»»åŠ¡å¯èƒ½éœ€é‡æ–°æ ¡å‡†ã€‚
- **å‡è®¾ cluster å†…éƒ¨è¯­ä¹‰ä¸€è‡´æ€§**ï¼šè‹¥èšç±»æ•ˆæœå·®ï¼ˆå¦‚ key åˆ†å¸ƒæ··ä¹±ï¼‰ï¼Œcentroid è¿‘ä¼¼è¯¯å·®ä¼šå¢å¤§ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- å°† Double-P æ‰©å±•è‡³ **training-time sparse attention**ï¼Œæ¢ç´¢å¯è®­ç»ƒçš„ hierarchical routingã€‚
- ç»“åˆ **quantization** æˆ– **KV cache compression** æŠ€æœ¯è¿›ä¸€æ­¥é™ä½å†…å­˜å ç”¨ã€‚
- æ¢ç´¢æ›´è½»é‡åŒ–çš„ clustering æ–¹æ³•ï¼ˆå¦‚ online clusteringï¼‰ä»¥å‡å°‘ prefill å¼€é”€ã€‚
- åº”ç”¨äºæ›´å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ 70B+ï¼‰æˆ–å¤šæ¨¡æ€ long-context åœºæ™¯ã€‚

---

## æ€»ç»“
**Double-P** æå‡ºäº†ä¸€ç§å…¨æ–°çš„ **hierarchical top-p sparse attention** æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿ sparse attention åœ¨ accuracyã€selection overhead å’Œ computation cost ä¹‹é—´éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚å…¶å®éªŒç»“æœè¡¨æ˜ï¼š

> âœ… **å‡ ä¹æ— æŸç²¾åº¦**ï¼ˆnear-zero accuracy dropï¼‰  
> âœ… **æ˜¾è‘—é™ä½ attention å¼€é”€**ï¼ˆup to 1.78Ã—ï¼‰  
> âœ… **å®ç°é«˜è¾¾ 1.26Ã— ç«¯åˆ°ç«¯åŠ é€Ÿ**  
> âœ… **ä¼˜äºæ‰€æœ‰ state-of-the-art sparse attention æ–¹æ³•**

è¯¥å·¥ä½œä¸º long-context LLM çš„é«˜æ•ˆæ¨ç†æä¾›äº†**åŸåˆ™æ€§å¼ºã€å¯æ‰©å±•çš„åŸºç¡€æ¶æ„**ï¼Œæœ‰æœ›æ¨åŠ¨ retrieval-augmented generationã€document understanding å’Œ long-horizon reasoning ç­‰åº”ç”¨çš„å®é™…è½åœ°ã€‚

</details>

---

### 2. [TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference](https://arxiv.org/abs/2602.05145)

**Authors**: Jiyoung Park, Hankyu Jang, Changseok Song, Wookeun Jung  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2602.05145v1  

#### Abstract
Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptatio...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
- **Speculative Decoding** åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´æŒ‘æˆ˜ï¼šå…¶æ€§èƒ½é«˜åº¦ä¾èµ–äº *draft model* å’Œ *target model* ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼ˆalignmentï¼‰ï¼Œè€ŒçœŸå®åœºæ™¯ä¸­çš„æ¨ç†å·¥ä½œè´Ÿè½½ï¼ˆworkloadï¼‰æ˜¯åŠ¨æ€å˜åŒ–çš„ï¼ˆnon-stationaryï¼‰ï¼Œå¯¼è‡´ draft-target alignment éšæ—¶é—´é€€åŒ–ï¼Œacceptance rate ä¸‹é™ï¼Œä»è€Œå‰Šå¼±åŠ é€Ÿæ•ˆæœã€‚
- ç°æœ‰åœ¨çº¿è®­ç»ƒ draft model çš„æ–¹æ³•å­˜åœ¨æ˜¾è‘—å¼€é”€ï¼šéœ€è¦é‡æ–°è¿è¡Œ target model æ¨ç†ä»¥ç”Ÿæˆè®­ç»ƒä¿¡å·ï¼ˆhidden statesï¼‰ï¼Œå¸¦æ¥é¢å¤–è®¡ç®—å’Œå­˜å‚¨è´Ÿæ‹…ï¼Œä¸”éš¾ä»¥é›†æˆåˆ°é«˜æ€§èƒ½æ¨ç†ç³»ç»Ÿä¸­ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹
TIDE æ˜¯ä¸€ä¸ª **serving-engine-native** çš„è‡ªé€‚åº” speculative decoding æ¡†æ¶ï¼Œæ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### âœ… **é›¶å¼€é”€è®­ç»ƒä¿¡å·ç”Ÿæˆï¼ˆZero-overhead Training Signal Generationï¼‰**
- åˆ©ç”¨ speculative decoding è¿‡ç¨‹ä¸­ target model å·²ç»è®¡ç®—å‡ºçš„ä¸­é—´ **hidden states** ä½œä¸ºè®­ç»ƒä¿¡å·ï¼Œç›´æ¥å¤ç”¨è¿™äº›åœ¨ prefill å’Œ verification é˜¶æ®µäº§ç”Ÿçš„å‰¯äº§å“ï¼Œæ— éœ€é‡æ–°åŠ è½½æˆ–è¿è¡Œ target modelã€‚
- å®ç°äº†çœŸæ­£çš„â€œé›¶é¢å¤–å¼€é”€â€è®­ç»ƒæ•°æ®é‡‡é›†ã€‚

#### âœ… **å¢é‡å¼åœ¨çº¿ draft æ¨¡å‹é€‚åº”ï¼ˆTemporal Incremental Draft Adaptationï¼‰**
- åŸºäºçŸ­æœŸæ—¶é—´å±€éƒ¨æ€§ï¼ˆtemporal localityï¼‰ï¼ŒæŒç»­åˆ©ç”¨æœ€æ–°æ¨ç†è¡Œä¸ºå¾®è°ƒ draft modelï¼Œç»´æŒå…¶ä¸å½“å‰ workload çš„å¯¹é½ã€‚
- æ”¯æŒå®æ—¶ã€æ¸è¿›å¼çš„æ¨¡å‹æ›´æ–°ï¼Œé€‚åº”åˆ†å¸ƒæ¼‚ç§»ï¼ˆdistribution shiftï¼‰ã€‚

#### âœ… **è‡ªé€‚åº”è¿è¡Œæ—¶æ§åˆ¶æœºåˆ¶ï¼ˆAdaptive Runtime Controlï¼‰**
- **Adaptive Drafter**ï¼šåŸºäº batch size å’Œ acceptance length åŠ¨æ€å†³å®šæ˜¯å¦å¯ç”¨ speculative decodingï¼Œé¿å…åœ¨ compute-bound åœºæ™¯ä¸‹æ— æ•ˆæ¨æµ‹ã€‚
- **Selective Training**ï¼šé€šè¿‡ç›‘æ§ acceptance rate çš„çŸ­æœŸ/é•¿æœŸç§»åŠ¨å¹³å‡å€¼ï¼ˆEMAï¼‰ï¼Œåˆ¤æ–­æ˜¯å¦å‘ç”Ÿåˆ†å¸ƒåç§»ï¼Œå¹¶æ®æ­¤è§¦å‘æˆ–æš‚åœè®­ç»ƒä¿¡å·æ”¶é›†ï¼Œé˜²æ­¢èµ„æºæµªè´¹å’Œè¿‡æ‹Ÿåˆã€‚

#### âœ… **å¼‚æ„ GPU èµ„æºè§£è€¦åˆ©ç”¨ï¼ˆHeterogeneous GPU Utilizationï¼‰**
- å°† **inference serving** ä¸ **draft model training** è§£è€¦ï¼Œåˆ†åˆ«éƒ¨ç½²åœ¨ä¸åŒç±»å‹çš„ GPU ä¸Šï¼š
  - Inference ä½¿ç”¨é«˜æ€§èƒ½ NVIDIA H100ï¼ˆé€‚åˆä½å»¶è¿Ÿæ¨ç†ï¼‰
  - Training ä½¿ç”¨æˆæœ¬æ›´ä½çš„ AMD Instinct MI250ï¼ˆé€‚åˆé«˜ååè®­ç»ƒï¼‰
- å……åˆ†åˆ©ç”¨æ•°æ®ä¸­å¿ƒå¼‚æ„é›†ç¾¤çš„ç¡¬ä»¶å¤šæ ·æ€§ï¼Œæå‡æ•´ä½“èµ„æºåˆ©ç”¨ç‡ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
ä½¿ç”¨å¤šä¸ªé¢†åŸŸçš„çœŸå®ä¸–ç•Œæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼š
- **ShareGPT**ï¼šå¼€æ”¾å¯¹è¯ä»»åŠ¡ï¼ˆconversationalï¼‰
- **Science**ï¼ˆCAMEL-AIï¼‰ï¼šç§‘å­¦æ–‡æœ¬ç”Ÿæˆï¼ˆbiology, chemistry, physicsï¼‰
- **NuminaMath**ï¼šæ•°å­¦æ¨ç†ï¼ˆmathematical reasoningï¼‰
- **EvolCodeAlpaca**ï¼šä»£ç ç”Ÿæˆï¼ˆcode generationï¼‰
- å¤šè¯­è¨€ Alpaca æ•°æ®é›†ï¼ˆKorean, Arabic, Chinese, Frenchï¼‰ï¼šç”¨äºæµ‹è¯•è·¨è¯­è¨€åˆ†å¸ƒåç§»ä¸‹çš„é²æ£’æ€§

### å®éªŒè®¾ç½®
- **ç›®æ ‡æ¨¡å‹ï¼ˆTarget Modelsï¼‰**ï¼š
  - `gpt-oss-120b`ï¼ˆ120Bï¼‰
  - `Qwen3-235B-A22B`ï¼ˆ235Bï¼‰
  - `Llama-4-Scout-17B-16E`ï¼ˆ17Bï¼‰
  - `Llama-3.3-70B-Instruct`ï¼ˆ70Bï¼‰
- **Draft Model æ¶æ„**ï¼šé‡‡ç”¨ **EAGLE-3**ï¼Œä»…å«å•å±‚ decoder + LM headï¼Œè½»é‡é«˜æ•ˆã€‚
- **å€™é€‰ token æ•°é‡ï¼ˆcandidate tokens Î³ï¼‰**ï¼šå›ºå®šä¸º 3ï¼ˆç»æ¶ˆèå®éªŒè¯æ˜æœ€ä¼˜ï¼‰ã€‚
- **ç¡¬ä»¶é…ç½®**ï¼š
  - Inferenceï¼šNVIDIA H100 Ã—8
  - Trainingï¼šAMD Instinct MI250 Ã—4
- **å®ç°åŸºç¡€**ï¼š
  - Inference Engineï¼šåŸºäº **SGLang**
  - Training Engineï¼šåŸºäº **SpecForge**

### è¯„ä¼°æŒ‡æ ‡
- **Throughput**ï¼ˆtokens/secï¼‰ï¼šä¸»è¦ç«¯åˆ°ç«¯æ€§èƒ½æŒ‡æ ‡
- **Acceptance Length / Rate**ï¼šè¡¡é‡ draft-target alignment è´¨é‡
- **Training Time & Storage Overhead**ï¼šè¯„ä¼°è®­ç»ƒæ•ˆç‡
- **Speedup**ï¼šç›¸å¯¹äº vanilla autoregressive decoding æˆ– baseline æ–¹æ³•çš„åŠ é€Ÿæ¯”

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | æè¿° |
|------|------|
| **SpecForge Offline Training** | ç¦»çº¿é¢„ç”Ÿæˆæ‰€æœ‰ hidden states å¹¶å­˜ç›˜ï¼Œå†ç”¨äºè®­ç»ƒï¼›å­˜å‚¨å¼€é”€å¤§ |
| **SpecForge Online Training** | æ¯æ¬¡è®­ç»ƒæ—¶é‡æ–°è¿è¡Œ target model è·å– hidden statesï¼›è®¡ç®—å¼€é”€å¤§ |
| **TIDE-default** | å§‹ç»ˆå¼€å¯ speculative decodingï¼Œæ— è‡ªé€‚åº”æ§åˆ¶ |
| **All-Inference Baseline** | æ‰€æœ‰ GPU ä»…ç”¨äºæ¨ç†ï¼Œå…³é—­ speculation |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### ğŸ”¹ ååæå‡ï¼ˆThroughput Improvementï¼‰
- TIDE åœ¨å¤šæ•° workload ä¸Šå®ç°äº† **æœ€é«˜è¾¾ 1.15Ã— çš„ååæå‡**ï¼ˆç›¸æ¯”é™æ€ speculative decodingï¼‰ã€‚
- åœ¨ **Science** æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å¤§æ”¶ç›Šï¼ˆ1.22Ã— ç›¸å¯¹äº all-inference baselineï¼‰ã€‚
- ShareGPT æå‡æœ‰é™ï¼ˆçº¦ 1.08Ã—ï¼‰ï¼Œå› å…¶é«˜ç†µã€éç»“æ„åŒ–å¯¹è¯ç‰¹æ€§ä¸é€‚åˆ speculationã€‚

#### ğŸ”¹ è®­ç»ƒæ•ˆç‡ä¼˜åŠ¿
| æ–¹æ³• | Prefill æ—¶é—´ | è®­ç»ƒæ—¶é—´ | æ€»è€—æ—¶ | åŠ é€Ÿæ¯” |
|------|------------|--------|-------|--------|
| SpecForge Offline | 6.16 hr | 9.16 hr | 15.32 hr | 1.00Ã— |
| SpecForge Online | 18.48 hr | 9.16 hr | 27.64 hr | 0.55Ã— |
| **TIDE** | â€” | **9.16 hr** | **9.16 hr** | **1.67Ã—** |

> âœ… TIDE æ¶ˆé™¤äº† prefill å¼€é”€ï¼Œè®­ç»ƒæ€»æ—¶é—´å‡å°‘ **1.67Ã—**ï¼Œä¸”æ— éœ€é‡å¤åŠ è½½ target modelã€‚

#### ğŸ”¹ å­˜å‚¨å¼€é”€å¤§å¹…é™ä½
| Target Model | SpecForge Offline | TIDE |
|-------------|------------------|------|
| gpt-oss-120b | 4.66 TB | **0.19 TB** |
| Qwen3-235B-A22B | 19.89 TB | **0.82 TB** |
| Llama-3.3-70B-Instruct | 46.40 TB | **1.92 TB** |

> âœ… TIDE ä»…ç¼“å­˜æ´»è·ƒ batch çš„ hidden statesï¼Œå­˜å‚¨éœ€æ±‚ä¸‹é™ **24â€“25 å€**ã€‚

#### ğŸ”¹ è‡ªé€‚åº”æ§åˆ¶æœ‰æ•ˆæ€§
- **TIDE-adaptive** åœ¨åˆ†å¸ƒåˆ‡æ¢ï¼ˆå¦‚éŸ©â†’é˜¿â†’ä¸­â†’æ³•ï¼‰æœŸé—´èƒ½åŠæ—¶ç¦ç”¨ speculationï¼Œé¿å…ä½ acceptance å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚
- ç›¸æ¯”ä¹‹ä¸‹ï¼Œ**TIDE-default** å‡ºç°æ˜æ˜¾ throughput æ³¢åŠ¨å’Œæ€§èƒ½å›é€€ã€‚

#### ğŸ”¹ å¼‚æ„ GPU åˆ†é…æ”¶ç›Š
| GPU ç±»å‹ | Inference Throughput (ç›¸å¯¹ MI250) | Training Throughput (ç›¸å¯¹ MI250) |
|---------|-------------------------------|------------------------------|
| MI250 | 1.00Ã— | 1.00Ã— |
| MI300X | 4.42Ã— | 1.77Ã— |
| H100 | **6.76Ã—** | **2.44Ã—** |

> âš ï¸ æ¨ç†æ€§èƒ½å·®è·è¿œå¤§äºè®­ç»ƒæ€§èƒ½å·®è· â†’ æ›´é€‚åˆå°†ä½ç«¯ GPUï¼ˆå¦‚ MI250ï¼‰ç”¨äº draft trainingã€‚

- åœ¨ H100:MI250 = 4:1 é…ç½®ä¸‹ï¼ŒTIDE å®ç° **æœ€é«˜ 1.26Ã— çš„ç›¸å¯¹ååæå‡**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Speculative decoding çš„å®ç”¨æ€§å¯é€šè¿‡è¿è¡Œæ—¶è‡ªé€‚åº”æ˜¾è‘—å¢å¼º**ï¼šTIDE æˆåŠŸå°† speculative decoding ä»â€œé™æ€ä¼˜åŒ–æŠ€æœ¯â€è½¬å˜ä¸ºâ€œåŠ¨æ€è‡ªè¿›åŒ–ç³»ç»Ÿâ€ï¼Œé€‚ç”¨äºç”Ÿäº§çº§åŠ¨æ€ workloadã€‚
2. **zero-overhead è®­ç»ƒä¿¡å·å¤ç”¨æ˜¯å¯è¡Œä¸”é«˜æ•ˆçš„**ï¼šåˆ©ç”¨æ¨ç†è¿‡ç¨‹ä¸­çš„ hidden states å¯å®Œå…¨æ›¿ä»£æ˜‚è´µçš„ re-computationï¼Œæå¤§æå‡è®­ç»ƒæ•ˆç‡ã€‚
3. **è‡ªé€‚åº”æ§åˆ¶è‡³å…³é‡è¦**ï¼šç›²ç›®æŒç»­ speculation æˆ– training ä¼šå¯¼è‡´èµ„æºæµªè´¹ç”šè‡³æ€§èƒ½ä¸‹é™ï¼›åŸºäº acceptance rate å’Œ batch size çš„åŠ¨æ€å†³ç­–æœºåˆ¶å¯æœ‰æ•ˆè§„é¿æ­¤é—®é¢˜ã€‚
4. **å¼‚æ„ GPU å¯è¢«é«˜æ•ˆåˆ©ç”¨**ï¼šé€šè¿‡è§£è€¦ inference ä¸ trainingï¼Œå¯åœ¨ä¸å½±å“ä¸»æ¨ç†æ€§èƒ½çš„å‰æä¸‹ï¼Œåˆ©ç”¨ä½æˆæœ¬ GPU å®Œæˆ draft model æ›´æ–°ï¼Œæå‡æ•´ä½“ ROIã€‚

### å±€é™æ€§
- å½“å‰æ¡†æ¶ä¾èµ–äºç‰¹å®š speculative decoding æŠ€æœ¯ï¼ˆå¦‚ EAGLE-3ï¼‰ï¼Œè¯¥ç±» draft model éœ€åŸºäº target model çš„ hidden states è¿›è¡Œé¢„æµ‹ã€‚
- å¯¹äºæé«˜ç†µã€å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚ ShareGPTï¼‰ï¼Œspeculative decoding æœ¬èº«å¢ç›Šæœ‰é™ï¼Œé™åˆ¶äº† TIDE çš„ä¸Šé™ã€‚
- è‡ªé€‚åº”æ§åˆ¶æ¨¡å‹åœ¨æŸäº›æ¨¡å‹æ¶æ„ï¼ˆå¦‚ Llamaï¼‰ä¸Šé¢„æµ‹è¯¯å·®è¾ƒé«˜ï¼ˆè¾¾ 25%ï¼‰ï¼Œè¯´æ˜ draft model å¼€é”€å‡è®¾ä¸æ€»æ˜¯æˆç«‹ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•æ”¯æŒæ›´å¤šç±»å‹çš„ draft model æ¶æ„ã€‚
- æ¢ç´¢æ›´ç²¾ç»†çš„åˆ†å¸ƒåç§»æ£€æµ‹æœºåˆ¶ï¼ˆå¦‚åŸºäº prompt embedding çš„èšç±»ï¼‰ã€‚
- ç»“åˆ prompt cachingã€KV cache sharing ç­‰å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ï¼Œæ„å»ºç»Ÿä¸€çš„é«˜æ•ˆ LLM serving stackã€‚
- ç ”ç©¶åœ¨è¾¹ç¼˜è®¾å¤‡æˆ–æ··åˆäº‘ç¯å¢ƒä¸­éƒ¨ç½² TIDE çš„å¯è¡Œæ€§ã€‚

--- 

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **TIDE å°† speculative decoding å‡çº§ä¸ºä¸€ä¸ªå…·å¤‡è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›çš„æœåŠ¡å†…åµŒç³»ç»Ÿï¼Œé€šè¿‡é›¶å¼€é”€ä¿¡å·å¤ç”¨ã€è‡ªé€‚åº”æ§åˆ¶ä¸å¼‚æ„èµ„æºè°ƒåº¦ï¼Œåœ¨çœŸå®åŠ¨æ€è´Ÿè½½ä¸‹å®ç°äº†å¯æŒç»­çš„æ¨ç†åŠ é€Ÿã€‚**

</details>

---

### 3. [OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale](https://arxiv.org/abs/2602.05711)

**Authors**: Jingze Shi, Zhangyang Peng, Yizhang Zhu, Yifan Wu, Guang Liu, Yuyu Luo  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.05711v1  

#### Abstract
Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-design...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale â€”â€” æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ç°æœ‰çš„ **Mixture-of-Experts (MoE)** æ¶æ„åœ¨**ä¸“å®¶ç²’åº¦**ä¸Šé¢ä¸´æ ¹æœ¬æ€§æƒè¡¡ï¼š
- **ç²—ç²’åº¦ MoE**ï¼ˆå¦‚ DeepSeekMoEï¼‰ï¼šç¡¬ä»¶æ•ˆç‡é«˜ï¼Œä½†æ¿€æ´»ä¸ç²¾ç¡®ï¼Œå­˜åœ¨å†—ä½™è®¡ç®—ã€‚
- **ç»†ç²’åº¦ MoE**ï¼ˆå¦‚ PEERï¼‰ï¼šå‚æ•°åˆ©ç”¨ç‡é«˜ã€è·¯ç”±çµæ´»ï¼Œä½†å› å†…å­˜è®¿é—®åˆ†æ•£å¯¼è‡´ä¸¥é‡çš„ **memory-bound** é—®é¢˜ï¼Œæ¨ç†å»¶è¿Ÿæé«˜ã€‚

OmniMoE çš„ç›®æ ‡æ˜¯ï¼š**åœ¨ä¿æŒç»†ç²’åº¦ MoE é«˜å‚æ•°æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°ç²—ç²’åº¦ MoE çš„ç¡¬ä»¶æ‰§è¡Œæ•ˆç‡**ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

OmniMoE æ˜¯ä¸€ä¸ª **system-algorithm co-designed** æ¡†æ¶ï¼Œé€šè¿‡ä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯ååŒè§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼š

#### ï¼ˆ1ï¼‰Atomic Experts + Dynamic Expert Assembly (DEA)
- å°†ä¸“å®¶ç²’åº¦æ¨å‘é€»è¾‘æé™ï¼šæ¯ä¸ª **Atomic Expert** ä»…ç”±ä¸€å¯¹å‘é‡ $(w_{\text{in}}, w_{\text{out}})$ å‚æ•°åŒ–ï¼Œæ„æˆæœ€å°å¯è·¯ç”±å•å…ƒã€‚
- å¼•å…¥ **Dynamic Expert Assembly (DEA)**ï¼šå¯¹æ¯ä¸ª token åŠ¨æ€æ£€ç´¢å¹¶ç»„åˆå¤šä¸ªåŸå­ä¸“å®¶ï¼Œå½¢æˆ token-specific çš„é«˜æ•ˆéçº¿æ€§å˜æ¢ã€‚
- ä¼˜åŠ¿ï¼šæå¤§æå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ä¸é•¿å°¾çŸ¥è¯†å»ºæ¨¡èƒ½åŠ›ï¼ŒåŒæ—¶é¿å…å›ºå®šå¤§å— FFN çš„å†—ä½™æ¿€æ´»ã€‚

#### ï¼ˆ2ï¼‰Cartesian Product Router
- ä¼ ç»Ÿ Top-K è·¯ç”±åœ¨ç™¾ä¸‡çº§ä¸“å®¶ä¸‹å¼€é”€ä¸º $O(Nd)$ï¼Œä¸å¯æ‰©å±•ã€‚
- æå‡ºå°†ä¸€ç»´ä¸“å®¶ç´¢å¼•ç©ºé—´åˆ†è§£ä¸ºäºŒç»´ç½‘æ ¼ï¼ˆ$N_r \times N_c$, $N = N_r N_c$ï¼‰ï¼Œé€šè¿‡ä¸¤ä¸ªä½ç»´æŠ•å½±ï¼ˆè¡Œ/åˆ—ï¼‰è”åˆç”Ÿæˆè·¯ç”±å¾—åˆ†ã€‚
- è·¯ç”±å¤æ‚åº¦ä» $O(N)$ é™è‡³ $O(\sqrt{N})$ï¼Œæ˜¾è‘—é™ä½è®¡ç®—ä¸å­˜å‚¨æˆæœ¬ã€‚

#### ï¼ˆ3ï¼‰Expert-Centric Scheduling
- ä¼ ç»Ÿ **token-centric** æ‰§è¡Œå¯¼è‡´éšæœºã€ç¢ç‰‡åŒ–çš„ HBM è®¿é—®ã€‚
- æå‡º **expert-centric** æ‰§è¡ŒèŒƒå¼ï¼š
  - æ”¶é›†æ‰€æœ‰ token çš„è·¯ç”±è¯·æ±‚ï¼›
  - æŒ‰æ¿€æ´»ä¸“å®¶åˆ†ç»„å¹¶é‡æ’åºï¼›
  - åˆå¹¶ç›¸åŒä¸“å®¶çš„è¾“å…¥ï¼Œè½¬æ¢ä¸º **Grouped GEMM** æ“ä½œã€‚
- æ•ˆæœï¼šå°†ç¨€ç–ã€éšæœºè®¿å­˜è½¬åŒ–ä¸ºå¯†é›†çŸ©é˜µè¿ç®—ï¼Œå¤§å¹…æå‡ GPU åˆ©ç”¨ç‡ä¸å†…å­˜å¸¦å®½æ•ˆç‡ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ç²—ç²’åº¦ MoEï¼ˆå¦‚ DeepSeekMoEï¼‰ | ç»†ç²’åº¦ MoEï¼ˆå¦‚ PEERï¼‰ | **OmniMoEï¼ˆæœ¬æ–‡ï¼‰** |
|------|-------------------------------|------------------------|-----------------------|
| å‚æ•°æ•ˆç‡ | ä½ï¼ˆå¤§å—æ¿€æ´»ï¼‰ | é«˜ï¼ˆç²¾ç»†æ§åˆ¶ï¼‰ | âœ… æé«˜ï¼ˆåŸå­çº§ï¼‰ |
| è·¯ç”±ç²¾åº¦ | ä½ | ä¸­é«˜ | âœ… æé«˜ï¼ˆåŠ¨æ€ç»„è£…ï¼‰ |
| ç¡¬ä»¶æ•ˆç‡ | âœ… é«˜ï¼ˆdense matmulï¼‰ | ä½ï¼ˆscatter accessï¼‰ | âœ… é«˜ï¼ˆGrouped GEMMï¼‰ |
| å¯æ‰©å±•æ€§ | æœ‰é™ï¼ˆç¦»æ•£å¢é•¿ï¼‰ | é«˜ | âœ… æé«˜ï¼ˆæ”¯æŒç™¾ä¸‡ä¸“å®¶ï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **é¢„è®­ç»ƒæ•°æ®**ï¼šSmolLMCorpusï¼ŒåŒ…å« 400 äº¿ tokenï¼Œæ¶µç›– Webã€æ•™ç§‘ä¹¦ã€ä»£ç ã€æ•°å­¦ç­‰é¢†åŸŸã€‚
- **åˆ†è¯å™¨**ï¼šNeoX tokenizerï¼ˆvocab size = 128,256ï¼‰ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹è§„æ¨¡**ï¼šä¸»å®éªŒé‡‡ç”¨ **6.4B æ€»å‚æ•° / 1.7B æ¿€æ´»å‚æ•°** çš„ MoE æ¨¡å‹ï¼Œå¯¹æ¯”ä¸åŒ FFN ç»“æ„ã€‚
- **éª¨å¹²ç½‘ç»œä¸€è‡´**ï¼šæ‰€æœ‰æ¨¡å‹å…±äº«ç›¸åŒçš„ Transformer backboneï¼ˆdepth, width, attention configï¼‰ï¼Œä»…æ›¿æ¢ FFN æ¨¡å—ã€‚
- **è®­ç»ƒç­–ç•¥**ï¼š
  - ä½¿ç”¨ AdamW + WSD å­¦ä¹ ç‡è°ƒåº¦ï¼›
  - éµå¾ª Chinchilla æœ€ä¼˜è®¡ç®—åˆ†é…åŸåˆ™ï¼›
  - åœ¨å•èŠ‚ç‚¹ 8Ã— NVIDIA A100 ä¸Šè¿›è¡Œæ¨ç†è¯„æµ‹ã€‚

---

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
| ç±»åˆ« | æŒ‡æ ‡ |
|------|------|
| **ä¸‹æ¸¸æ€§èƒ½** | Zero-shot accuracy on 7 benchmarksï¼š<br>- MMLUï¼ˆå¤šä»»åŠ¡çŸ¥è¯†ï¼‰<br>- TriviaQAï¼ˆäº‹å®å›å¿†ï¼‰<br>- ARCï¼ˆç§‘å­¦æ¨ç†ï¼‰<br>- PIQAï¼ˆç‰©ç†å¸¸è¯†ï¼‰<br>- HellaSwagï¼ˆå¸¸è¯†æ¨æ–­ï¼‰<br>- OBQAï¼ˆå¼€æ”¾é—®ç­”ï¼‰<br>- Winograndeï¼ˆå…±æŒ‡æ¶ˆè§£ï¼‰ |
| **ç³»ç»Ÿæ•ˆç‡** | - æ¨ç†å»¶è¿Ÿï¼ˆlatencyï¼‰<br>- å³°å€¼æ˜¾å­˜å ç”¨ï¼ˆpeak memoryï¼‰<br>- Perplexityï¼ˆè¯­è¨€å»ºæ¨¡è´¨é‡ï¼‰ |
| **å¯æ‰©å±•æ€§** | åˆ†å¸ƒå¼è®­ç»ƒé€šä¿¡å¼€é”€åˆ†æï¼ˆExpert Parallelismï¼‰ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **Dense** | å…¨æ¿€æ´» MLP | åŸºå‡†æ¨¡å‹ |
| **Gshard** | ç²—ç²’åº¦ MoE | Top-K è·¯ç”±ï¼Œç»å…¸æ¶æ„ |
| **DeepSeekMoE** | ç²—ç²’åº¦ MoE + å…±äº«ä¸“å®¶ | å½“å‰ä¸»æµæ–¹æ¡ˆ |
| **PKM** | ç»†ç²’åº¦ MoE | åŸºäº Product Key Memory |
| **PEER** | ç»†ç²’åº¦ MoE | ç™¾ä¸‡çº§è½»é‡ä¸“å®¶ï¼Œå½“å‰ SOTA ç»†ç²’åº¦åŸºçº¿ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰ä¸‹æ¸¸ä»»åŠ¡å¹³å‡å‡†ç¡®ç‡ï¼ˆZero-shot Accuracyï¼‰
| æ¨¡å‹ | å¹³å‡ ACC |
|------|---------|
| Dense | 45.9 |
| DeepSeekMoE | 50.2 |
| PEER | 48.9 |
| **OmniMoEï¼ˆoursï¼‰** | **50.9** âœ… |

> OmniMoE åœ¨ **7 é¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€é«˜å¹³å‡ç²¾åº¦**ï¼Œä¼˜äºæœ€å¼ºç²—ç²’åº¦ï¼ˆ+0.7ï¼‰å’Œç»†ç²’åº¦ï¼ˆ+2.0ï¼‰åŸºçº¿ã€‚

#### ï¼ˆ2ï¼‰æ¨ç†å»¶è¿Ÿï¼ˆ4096 tokens è¾“å…¥ï¼‰
| æ¨¡å‹ | æ¨ç†å»¶è¿Ÿ |
|------|----------|
| PEER | 73 ms |
| DeepSeekMoE | 102 ms |
| **OmniMoEï¼ˆoursï¼‰** | **6.7 ms** âœ… |

> **å®ç° 10.9Ã— é€Ÿåº¦æå‡ï¼ˆvs PEERï¼‰**ï¼Œä¸”æ˜¾å­˜å ç”¨ä¸ç²—ç²’åº¦ MoE ç›¸å½“ã€‚

#### ï¼ˆ3ï¼‰Scaling Law è¡¨ç°
- åœ¨ç›¸åŒè®­ç»ƒ FLOPs ä¸‹ï¼ŒOmniMoE å®ç°æœ€ä½ validation perplexityã€‚
- æ§åˆ¶æ¿€æ´»å‚æ•°é¢„ç®—æ—¶ï¼Œä»è¡¨ç°æœ€ä¼˜ï¼Œè¯´æ˜å…¶ **å…¼å…·è®¡ç®—æ•ˆç‡ä¸å‚æ•°æ•ˆç‡**ã€‚

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

| æ¶ˆèé…ç½® | Latencyâ†‘ | Memoryâ†‘ | PPLâ†‘ | Reasoningâ†“ | Knowledgeâ†“ | Expert Usageâ†“ | Unevennessâ†‘ |
|----------|----------|---------|------|------------|-------------|----------------|--------------|
| Full Model | 1.0x | 1.0x | 1.0x | 1.0x | 1.0x | 100% | 0.24 |
| w/o Shared Dense MLP | 0.86x | 0.98x | 1.2x | 0.79x | 0.91x | 100% | 0.27 |
| w/o Cartesian Router | 30.6x | 337.5x | 1.4x | 0.79x | 0.66x | 4% | 0.77 |
| w/o Expert-Centric Sched | 24.8x | 417.7x | 1.0x | 1.0x | 1.0x | 100% | 0.24 |

> - **ç§»é™¤ Cartesian Router** å¯¼è‡´è·¯ç”±å´©æºƒï¼Œä¸“å®¶åˆ©ç”¨ç‡éª¤é™ï¼Œè´Ÿè½½æä¸å‡è¡¡ã€‚
> - **ç§»é™¤ Expert-Centric Scheduling** å¯¼è‡´å»¶è¿Ÿé£™å‡ 24.8 å€ï¼ŒéªŒè¯å…¶å¯¹ç³»ç»Ÿæ•ˆç‡çš„æ ¸å¿ƒä½œç”¨ã€‚
> - **å…±äº« MLP** è™½è½»å¾®å¢åŠ å¼€é”€ï¼Œä½†æ˜¾è‘—æå‡æ³›åŒ–ä¸æ¨ç†èƒ½åŠ›ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **ç»†ç²’åº¦ MoE å¯ä»¥æ—¢å¿«åˆå‡†**ï¼šé€šè¿‡ç®—æ³•-ç³»ç»ŸååŒè®¾è®¡ï¼ŒOmniMoE æˆåŠŸæ‰“ç ´â€œç»†ç²’åº¦=æ…¢â€çš„å›ºæœ‰è®¤çŸ¥ã€‚
2. **Atomic Expert + DEA** å®ç°äº†æè‡´çš„å‚æ•°æ§åˆ¶ä¸è¡¨è¾¾åŠ›ï¼Œç‰¹åˆ«é€‚åˆé•¿å°¾çŸ¥è¯†æ£€ç´¢ã€‚
3. **Cartesian Product Router** ä½¿ç™¾ä¸‡çº§ä¸“å®¶è·¯ç”±å˜å¾—å¯è¡Œï¼Œè§£å†³äº†å¤§è§„æ¨¡è·¯ç”±çš„æŒ‡æ•°çº§å¼€é”€é—®é¢˜ã€‚
4. **Expert-Centric Scheduling** æ˜¯æ€§èƒ½é£è·ƒçš„å…³é”®ï¼šå°† memory-bound è½¬ä¸º compute-boundï¼Œé‡Šæ”¾ GPU ç¡¬ä»¶æ½œåŠ›ã€‚
5. **æ··åˆæ¶æ„æ›´ä¼˜**ï¼šå…±äº« dense MLP å¤„ç†é€šç”¨è¯­ä¹‰ï¼Œè·¯ç”±åˆ†æ”¯ä¸“æ³¨ç‰¹åŒ–çŸ¥è¯†ï¼ŒäºŒè€…äº’è¡¥ã€‚

---

### âš ï¸ å±€é™æ€§
- å½“å‰å®ç°ä¾èµ–é«˜åº¦å®šåˆ¶åŒ–çš„ Triton å†…æ ¸ï¼Œåœ¨é€šç”¨ç¡¬ä»¶ä¸Šçš„éƒ¨ç½²é—¨æ§›è¾ƒé«˜ã€‚
- å¯¹è¶…å¤§è§„æ¨¡ä¸“å®¶æ± ï¼ˆ>1Mï¼‰çš„åˆå§‹åŒ–ä¸è®­ç»ƒç¨³å®šæ€§ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚
- åŠ¨æ€ç»„è£…æœºåˆ¶å¯èƒ½å¸¦æ¥é¢å¤–å…ƒæ•°æ®ç®¡ç†å¼€é”€ï¼Œåœ¨æç«¯ç¨€ç–åœºæ™¯ä¸‹éœ€ä¼˜åŒ–ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **æ›´é«˜æ•ˆçš„ Cartesian åˆ†è§£æ–¹å¼**ï¼ˆå¦‚æ›´é«˜ç»´ä¹˜ç§¯ç»“æ„ï¼‰ã€‚
- å°† OmniMoE æ€è·¯æ¨å¹¿è‡³ **CNNã€Vision Transformer ç­‰éè¯­è¨€æ¨¡å‹**ã€‚
- ç ”ç©¶ **åœ¨çº¿ä¸“å®¶å¢åˆ æœºåˆ¶**ï¼Œæ”¯æŒåŠ¨æ€æ‰©å±•ä¸“å®¶æ± ã€‚
- å¼€å‘ **è‡ªåŠ¨ç¼–è¯‘å·¥å…·é“¾**ï¼Œé™ä½ Expert-Centric Scheduling çš„å·¥ç¨‹å¤æ‚åº¦ã€‚

---

> ğŸ”— **å¼€æºåœ°å€**ï¼š[https://github.com/flash-algo/omni-moe](https://github.com/flash-algo/omni-moe)

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼šOmniMoE é€šè¿‡ **Atomic Experts + Cartesian Router + Expert-Centric Scheduling** çš„ä¸‰ä½ä¸€ä½“è®¾è®¡ï¼Œé¦–æ¬¡å®ç°äº† **å¤§è§„æ¨¡ã€é«˜ç²¾åº¦ã€é«˜é€Ÿåº¦** çš„ç»†ç²’åº¦ MoEï¼Œä¸ºä¸‹ä¸€ä»£é«˜æ•ˆå¤§æ¨¡å‹æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 4. [DFlash: Block Diffusion for Flash Speculative Decoding](https://arxiv.org/abs/2602.06036)

**Authors**: Jian Chen, Yesheng Liang, Zhijian Liu  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2602.06036v1  

#### Abstract
Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the targ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šDFlash: Block Diffusion for Flash Speculative Decoding**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶é‡‡ç”¨è‡ªå›å½’ï¼ˆautoregressiveï¼‰ç”Ÿæˆæ–¹å¼ï¼Œé€ä¸ª token ç”Ÿæˆè¾“å‡ºï¼Œå¯¼è‡´**é«˜å»¶è¿Ÿã€ä½ GPU åˆ©ç”¨ç‡**ï¼Œå°¤å…¶åœ¨é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚ Chain-of-Thought æ¨ç†ï¼‰ä¸­æˆä¸ºç“¶é¢ˆã€‚

ç°æœ‰ **Speculative Decoding** æ–¹æ³•é€šè¿‡ä¸€ä¸ªè½»é‡çº§ draft model é¢„æµ‹å¤šä¸ª tokenï¼Œå†ç”±ç›®æ ‡æ¨¡å‹å¹¶è¡ŒéªŒè¯ï¼Œè™½èƒ½åŠ é€Ÿï¼Œä½†ä»ä¾èµ–**è‡ªå›å½’ drafting**ï¼Œé™åˆ¶äº†å®é™…åŠ é€Ÿæ•ˆæœï¼ˆé€šå¸¸ä»… 2â€“3Ã—ï¼‰ã€‚è€Œæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰è™½æ”¯æŒå¹¶è¡Œç”Ÿæˆï¼Œä½†å•ç‹¬ä½¿ç”¨æ—¶ç”Ÿæˆè´¨é‡ä½äºè‡ªå›å½’æ¨¡å‹ï¼Œä¸”å¤šæ­¥å»å™ªè¿‡ç¨‹æ‹–æ…¢é€Ÿåº¦ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
æœ¬æ–‡æå‡º **DFlash**ï¼Œä¸€ç§åŸºäº**å—æ‰©æ•£ï¼ˆblock diffusionï¼‰çš„ speculative decoding æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- ä½¿ç”¨ä¸€ä¸ª**è½»é‡çº§çš„ block diffusion æ¨¡å‹**ä½œä¸º draft modelï¼Œåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­å¹¶è¡Œç”Ÿæˆå¤šä¸ª tokenã€‚
- **åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„éšè—ç‰¹å¾ï¼ˆhidden featuresï¼‰ä½œä¸ºä¸Šä¸‹æ–‡æ¡ä»¶**ï¼ŒæŒ‡å¯¼ draft model ç”Ÿæˆæ›´é«˜è´¨é‡çš„å€™é€‰åºåˆ—ã€‚
- é€šè¿‡ **KV Injection** æœºåˆ¶å°†ç›®æ ‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç‰¹å¾æ³¨å…¥ draft model çš„æ¯ä¸€å±‚ Key-Value ç¼“å­˜ä¸­ï¼Œå®ç°å¼ºè€Œç¨³å®šçš„æ¡ä»¶æ§åˆ¶ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| å¯¹æ¯”ç»´åº¦ | ä¼ ç»Ÿè‡ªå›å½’ draftï¼ˆå¦‚ EAGLE-3ï¼‰ | DFlash |
|---------|-------------------------------|--------|
| **Drafting æ–¹å¼** | è‡ªå›å½’ï¼Œä¸²è¡Œç”Ÿæˆ | å¹¶è¡Œç”Ÿæˆï¼Œå•æ¬¡å‰å‘ä¼ æ’­ |
| **Drafting å»¶è¿Ÿ** | éšé¢„æµ‹ token æ•°çº¿æ€§å¢é•¿ | å‡ ä¹ä¸éš block size å¢åŠ  |
| **æ¨¡å‹å®¹é‡** | å—é™äºæµ…å±‚ç»“æ„ï¼ˆå¦‚ 1 å±‚ï¼‰ | å¯ä½¿ç”¨æ›´æ·±ã€è¡¨è¾¾èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹ |
| **æ¥å—ç‡ï¼ˆacceptance rateï¼‰** | è¾ƒä½ï¼Œæ˜“å› é”™è¯¯ç´¯ç§¯ä¸­æ–­ | æ›´é«˜ï¼Œå¾—ç›Šäºç›®æ ‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡å¼•å¯¼ |
| **å®é™…åŠ é€Ÿæ¯”** | ~2â€“3Ã— | **>6Ã—** |

DFlash å°† diffusion æ¨¡å‹çš„è§’è‰²ä»â€œç‹¬ç«‹ç”Ÿæˆå™¨â€è½¬å˜ä¸ºâ€œé«˜æ•ˆå¹¶è¡Œ draft adapterâ€ï¼Œå……åˆ†å‘æŒ¥å…¶å¹¶è¡Œä¼˜åŠ¿ï¼ŒåŒæ—¶è§„é¿å…¶ç”Ÿæˆè´¨é‡çŸ­æ¿ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **æ•°å­¦ä»»åŠ¡**ï¼šGSM8Kã€MATH-500ã€AIME25
- **ä»£ç ä»»åŠ¡**ï¼šHumanEvalã€MBPPã€LiveCodeBenchï¼ˆLCBï¼‰
- **å¯¹è¯ä»»åŠ¡**ï¼šMT-Benchã€Alpaca
- **è®­ç»ƒæ•°æ®**ï¼šçº¦ 800K æ ·æœ¬ï¼Œæ¥è‡ª NVIDIA Nemotron Post-Training Dataset V2 å’Œ CodeAlpacaï¼Œå“åº”ç”±ç›®æ ‡æ¨¡å‹ç”Ÿæˆä»¥ä¿è¯å¯¹é½ã€‚

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**
- **ç›®æ ‡æ¨¡å‹**ï¼šQwen3-4Bã€Qwen3-8Bã€Qwen3-Coder-30B-A3B-Instructã€LLaMA-3.1-8B-Instruct
- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA H200 / B200 GPU
- **æ¨ç†æ¡†æ¶**ï¼šTransformersã€SGLangï¼ˆä½¿ç”¨ FA4 åç«¯ï¼‰
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **å¹³å‡æ¥å—é•¿åº¦ï¼ˆAverage Acceptance Length, Tï¼‰**
  - **ç«¯åˆ°ç«¯è§£ç åŠ é€Ÿæ¯”ï¼ˆEnd-to-end Speedupï¼‰**
  - **ååé‡ï¼ˆThroughput, tok/sï¼‰**

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **Baseline**ï¼šæ ‡å‡†è‡ªå›å½’è§£ç 
- **EAGLE-3**ï¼šå½“å‰æœ€å…ˆè¿›çš„ speculative decoding æ–¹æ³•ï¼Œä½¿ç”¨è‡ªå›å½’ draft tree
  - å¯¹æ¯”ä¸¤ç§é…ç½®ï¼štree size = 16ï¼ˆå…¬å¹³æ¯”è¾ƒï¼‰ã€tree size = 60ï¼ˆæœ€å¤§åŒ–æ¥å—é•¿åº¦ï¼‰

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
| æ¨¡å‹ | æ–¹æ³• | å¹³å‡åŠ é€Ÿæ¯” | æœ€é«˜åŠ é€Ÿæ¯” |
|------|------|------------|------------|
| Qwen3-8B | DFlash | **~4.9Ã—**ï¼ˆgreedyï¼‰<br>~4.1Ã—ï¼ˆtemp=1ï¼‰ | **6.1Ã—**ï¼ˆå›¾1ï¼‰ |
| Qwen3-4B | DFlash | ~4.8Ã— | â€” |
| LLaMA-3.1-8B | DFlash | ~2.8Ã—ï¼ˆHumanEvalï¼‰ | â€” |

> **æ³¨**ï¼šDFlash åœ¨ Qwen3-8B ä¸Šå®ç°äº† **è¶…è¿‡ 6Ã— çš„ lossless åŠ é€Ÿ**ï¼Œæ˜¯ EAGLE-3 çš„ **2.5Ã— ä»¥ä¸Š**ã€‚

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- åœ¨æ‰€æœ‰ä»»åŠ¡ï¼ˆæ•°å­¦ã€ä»£ç ã€å¯¹è¯ï¼‰ä¸Šï¼ŒDFlash æ˜¾è‘—ä¼˜äº EAGLE-3ï¼š
  - **æ¥å—é•¿åº¦ï¼ˆTï¼‰æå‡ 2â€“3 å€**ï¼ˆå¦‚ EAGLE-3 Tâ‰ˆ3.5ï¼ŒDFlash Tâ‰ˆ6.5ï¼‰
  - **åŠ é€Ÿæ¯”é«˜å‡º 2.2â€“2.5Ã—**
- å³ä½¿ EAGLE-3 ä½¿ç”¨æ›´å¤§çš„ draft treeï¼ˆ60 tokensï¼‰ï¼ŒDFlash ä»ä»¥æ›´ä½çš„éªŒè¯å¼€é”€å®ç°æ›´é«˜æˆ–ç›¸å½“çš„æ¥å—é•¿åº¦ã€‚

### **æ¶ˆèå®éªŒç»“æœ**
#### **(1) æ˜¯å¦ä½¿ç”¨ç›®æ ‡æ¨¡å‹ä¸Šä¸‹æ–‡ç‰¹å¾**
- **æ— ä¸Šä¸‹æ–‡ç‰¹å¾**ï¼ˆTable 8ï¼‰ï¼š5 å±‚ diffusion draft model ä»…å®ç° ~2.8â€“3.7Ã— åŠ é€Ÿï¼Œè¿œä½äºå®Œæ•´ç‰ˆ DFlashï¼ˆ>4.7Ã—ï¼‰
- **ç»“è®º**ï¼šç›®æ ‡æ¨¡å‹çš„éšè—ç‰¹å¾å¯¹é«˜è´¨é‡ drafting è‡³å…³é‡è¦ã€‚

#### **(2) Draft Model å±‚æ•°å½±å“**ï¼ˆTable 5ï¼‰
- 3 å±‚ â†’ 5 å±‚ï¼šæ¥å—é•¿åº¦æ˜¾è‘—æå‡ï¼ˆTâ†‘ï¼‰ï¼ŒåŠ é€Ÿæ¯”æé«˜
- 5 å±‚ â†’ 8 å±‚ï¼šæ¥å—é•¿åº¦ç»§ç»­ä¸Šå‡ï¼Œä½† drafting å»¶è¿Ÿå¢åŠ ï¼Œ**æ•´ä½“åŠ é€Ÿæ¯”ä¸‹é™**
- **ç»“è®º**ï¼š**5 å±‚ä¸ºæœ€ä¼˜å¹³è¡¡ç‚¹**

#### **(3) ç›®æ ‡æ¨¡å‹éšè—å±‚æ•°é‡**ï¼ˆTable 6ï¼‰
- ä½¿ç”¨ 5 å±‚éšè—ç‰¹å¾ vs 3 å±‚ï¼šæ¥å—é•¿åº¦å’ŒåŠ é€Ÿæ¯”å‡æå‡
- **ç»“è®º**ï¼šæ›´å¤šå±‚ç‰¹å¾æä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯

#### **(4) è®­ç»ƒä¸æ¨ç† block size åŒ¹é…æ€§**ï¼ˆTable 7ï¼‰
- **å¤§è®­ç»ƒ block â†’ å°æ¨ç† block**ï¼šæ³›åŒ–è‰¯å¥½ï¼ˆ16â†’8 æ•ˆæœæ¥è¿‘ 8â†’8ï¼‰
- **å°è®­ç»ƒ block â†’ å¤§æ¨ç† block**ï¼šæ€§èƒ½ä¸‹é™æ˜æ˜¾
- **ç»“è®º**ï¼šå¯è®­ç»ƒå¤§ block æ¨¡å‹ï¼Œç”¨äºåŠ¨æ€è°ƒåº¦ä¸åŒ block sizeï¼Œæå‡éƒ¨ç½²çµæ´»æ€§

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **Diffusion æ¨¡å‹é€‚åˆä½œä¸º speculative decoding çš„ draft model**ï¼Œè€Œéç‹¬ç«‹ç”Ÿæˆå™¨ã€‚
2. **ç›®æ ‡æ¨¡å‹çš„éšè—ç‰¹å¾è•´å«æœªæ¥ token ä¿¡æ¯**ï¼Œå¯ç”¨äºæ˜¾è‘—æå‡ draft è´¨é‡ã€‚
3. **KV Injection æ˜¯æœ‰æ•ˆä¸”å¯æ‰©å±•çš„æ¡ä»¶æ³¨å…¥æœºåˆ¶**ï¼Œæ”¯æŒæ·±å±‚ draft model çš„ç¨³å®šè®­ç»ƒä¸æ¨ç†ã€‚
4. **å¹¶è¡Œ drafting + æ¡ä»¶å¼•å¯¼** å¯çªç ´ä¼ ç»Ÿ speculative decoding çš„åŠ é€Ÿå¤©èŠ±æ¿ï¼Œå®ç° >6Ã— lossless åŠ é€Ÿã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- å½“å‰ DFlash draft model ä»éœ€ä¸ç‰¹å®šç›®æ ‡æ¨¡å‹å¯¹é½è®­ç»ƒï¼Œ**ç¼ºä¹é€šç”¨æ€§**ã€‚
- ä¾èµ–ç›®æ ‡æ¨¡å‹çš„éšè—å±‚è¾“å‡ºï¼Œå¯èƒ½å—é™äºæŸäº›é—­æºæ¨¡å‹çš„ API æ¥å£ã€‚
- åœ¨æé«˜å¹¶å‘åœºæ™¯ä¸‹ï¼Œå¤§ block size å¯èƒ½å¢åŠ éªŒè¯æˆæœ¬ï¼Œéœ€åŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- **è‡ªé€‚åº” block size è°ƒåº¦**ï¼šæ ¹æ®è´Ÿè½½åŠ¨æ€é€‰æ‹©æœ€ä¼˜ block sizeã€‚
- **é€šç”¨ draft model**ï¼šè®­ç»ƒå¯é€‚é…å¤šç§ç›®æ ‡æ¨¡å‹çš„ diffusion drafterã€‚
- **å‡å°‘è®­ç»ƒå¼€é”€**ï¼šæ¢ç´¢æ›´é«˜æ•ˆçš„ç¦»çº¿ç‰¹å¾ç¼“å­˜ä¸è’¸é¦æ–¹æ³•ã€‚
- **æ‰©å±•è‡³å¤šæ¨¡æ€**ï¼šå°† block diffusion drafting åº”ç”¨äºè§†è§‰ã€è¯­éŸ³ç­‰ç”Ÿæˆä»»åŠ¡ã€‚

---

> **æ€»ç»“**ï¼šDFlash é‡æ–°å®šä¹‰äº† diffusion æ¨¡å‹åœ¨ LLM æ¨ç†ä¸­çš„è§’è‰²ï¼Œé€šè¿‡â€œ**å¹¶è¡Œ drafting + ç›®æ ‡æ¨¡å‹å¼•å¯¼**â€çš„èŒƒå¼ï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„æ¨ç†åŠ é€Ÿï¼Œä¸ºé«˜æ•ˆã€ä½æˆæœ¬çš„å¤§æ¨¡å‹éƒ¨ç½²æä¾›äº†æ–°è·¯å¾„ã€‚

</details>

---

### 5. [TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation](https://arxiv.org/abs/2602.04929)

**Authors**: Junhan Kim, Yeo Jeong Park, Seungwoo Son, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon Jeon  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2602.04929v1  

#### Abstract
The rapid growth of large language models (LLMs) has heightened the importance of post-training quantization (PTQ) for reducing memory and computation costs. Among PTQ methods, GPTQ has gained significant attention for its efficiency, enabling billion-scale LLMs to be quantized within a few GPU hour...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éƒ¨ç½²æ—¶é¢ä¸´é«˜å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬çš„é—®é¢˜ï¼Œ**Post-Training Quantization (PTQ)** æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„é«˜æ•ˆ PTQ æ–¹æ³•å¦‚ **GPTQ** å› ä¸ºå‡è®¾å±‚é—´ç‹¬ç«‹ï¼Œåœ¨ä½æ¯”ç‰¹ï¼ˆå¦‚ INT2ï¼‰é‡åŒ–ä¸‹ç²¾åº¦ä¸¥é‡ä¸‹é™ã€‚è€Œæ›´ç²¾ç¡®çš„æ–¹æ³• **BoA** è™½ç„¶é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æ¨¡å—å†…çš„è·¨å±‚ä¾èµ–æå‡äº†ç²¾åº¦ï¼Œä½†å…¶é€é€šé“ï¼ˆout-channelï¼‰ä¸²è¡Œé‡åŒ–å¯¼è‡´æ•ˆç‡æä½ã€‚

æœ¬æ–‡æ—¨åœ¨è§£å†³ **BoA æ–¹æ³•åœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ•ˆç‡ä½ä¸‹** çš„æ ¸å¿ƒçŸ›ç›¾ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡ºäº† **TURBOBOA**ï¼Œä¸€ç§æ— éœ€åå‘ä¼ æ’­çš„ PTQ ç®—æ³•ï¼Œé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯å®ç°å¯¹ BoA çš„åŠ é€Ÿä¸å¢å¼ºï¼š

1. **å¤šé€šé“è”åˆé‡åŒ–ï¼ˆJoint Quantization of Multiple Out-Channelsï¼‰**  
   - æ”¹å˜ BoA é€é€šé“ä¸²è¡Œé‡åŒ–çš„æ¨¡å¼ï¼Œæ”¹ä¸º**åŒæ—¶é‡åŒ–å¤šä¸ª N ä¸ª out-channels**ã€‚
   - æå‡ºä¸€ä¸ª**é—­å¼è¯¯å·®è¡¥å¿è§„åˆ™**ï¼ˆclosed-form error compensation ruleï¼‰ï¼Œåœ¨å‡å°‘ä¸²è¡Œæ“ä½œçš„åŒæ—¶æ˜¾å¼å»ºæ¨¡é€šé“é—´çš„ä¾èµ–å…³ç³»ã€‚
   - **æ•ˆæœ**ï¼šå°†ä¸²è¡Œæ“ä½œæ•°ä» `d_out` å‡å°‘åˆ° `d_out / N`ï¼Œå¸¦æ¥æ˜¾è‘—åŠ é€Ÿã€‚

2. **å‰åºé‡åŒ–å±‚è¯¯å·®è¡¥å¿ï¼ˆError Compensation for Pre-quantized Layersï¼‰**  
   - å¼•å…¥å¯¹**ç”±å…ˆå‰é‡åŒ–å±‚ä¼ æ’­è€Œæ¥è¾“å…¥åå·®ï¼ˆAXï¼‰æ‰€å¼•èµ·çš„è¾“å‡ºå¤±çœŸ**çš„æ˜¾å¼è¡¥å¿ã€‚
   - å°† `GWAX` é¡¹çº³å…¥è¯¯å·®é‡å»ºç›®æ ‡ï¼Œä½¿é‡åŒ–æ¨¡å‹èƒ½æ›´å¿ å®å¤ç°å…¨ç²¾åº¦ï¼ˆFPï¼‰æ¨¡å‹çš„è¡Œä¸ºã€‚

3. **è‡ªé€‚åº”ç½‘æ ¼é€‰æ‹©ä¸åæ ‡ä¸‹é™ç²¾ç‚¼ï¼ˆAdaptive Grid Selection with CD-based Refinementï¼‰**  
   - **åŠ¨æ€ç½‘æ ¼è®¡ç®—**ï¼šåœ¨æ¯æ¬¡é‡åŒ–å‰é‡æ–°è®¡ç®—é‡åŒ–ç½‘æ ¼ï¼Œç¡®ä¿ä¸å·²æ›´æ–°çš„æƒé‡å¯¹é½ã€‚
   - **ç½‘æ ¼ç²¾ç‚¼**ï¼šåœ¨æ•´æ•°é‡åŒ–å®Œæˆåï¼Œå†»ç»“ `W_int`ï¼Œä»…é€šè¿‡**åæ ‡ä¸‹é™ï¼ˆCoordinate Descent, CDï¼‰** ä¼˜åŒ– scale å‘é‡ `s`ï¼Œä»¥è¿›ä¸€æ­¥æœ€å°åŒ–æ³¨æ„åŠ›é‡å»ºè¯¯å·®ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **é€Ÿåº¦**ï¼šç›¸æ¯” BoAï¼Œå®ç°äº†**è¶…è¿‡ä¸‰å€çš„åŠ é€Ÿ**ï¼ˆTable 2ï¼‰ï¼Œå°¤å…¶åœ¨å¤§æ¨¡å‹ï¼ˆå¦‚ 70Bï¼‰ä¸ŠèŠ‚çœè¿‘ 10-12 å°æ—¶ã€‚
- **ç²¾åº¦**ï¼šä¸ä»…æ²¡æœ‰å› å¹¶è¡ŒåŒ–æŸå¤±ç²¾åº¦ï¼Œåè€Œé€šè¿‡è¯¯å·®è¡¥å¿å’Œç½‘æ ¼ä¼˜åŒ–ï¼Œ**å…¨é¢è¶…è¶Š BoA å’Œå…¶ä»– SOTA æ–¹æ³•**ã€‚
- **å…¼å®¹æ€§**ï¼šå¯ä¸ **QuaRotã€SpinQuantã€OSTQuant** ç­‰ outlier suppression æŠ€æœ¯æ— ç¼ç»“åˆï¼Œè¾¾åˆ°å½“å‰æœ€ä½³ï¼ˆstate-of-the-artï¼‰æ€§èƒ½ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **æ ¡å‡†æ•°æ®ï¼ˆCalibration Dataï¼‰**ï¼šç”¨äºé‡åŒ–è¿‡ç¨‹çš„ 128 æ¡é•¿åº¦ä¸º 2048 çš„åºåˆ—ï¼Œé‡‡æ ·è‡ª **WikiText-2 (Wiki2)** æ•°æ®é›†ã€‚
- **æµ‹è¯•æ•°æ®ï¼ˆTest Setsï¼‰**ï¼š
  - **Wiki2** å’Œ **C4** æ•°æ®é›†ä¸Šçš„ **Perplexity (PPL)**ã€‚
  - åœ¨ **8 ä¸ªé›¶æ ·æœ¬å¸¸è¯†æ¨ç†ä»»åŠ¡**ï¼ˆå¦‚ ARC, BoolQ, HellaSwag, PIQA, Winogrande ç­‰ï¼‰ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹**ï¼šLlama3.2-1B, Llama3.2-3B, Llama3-8B, Llama3.1-70B, Llama2-7B, Llama2-13Bã€‚
- **ç¡¬ä»¶**ï¼šNVIDIA H100 GPUs (80GB)ï¼Œ70B æ¨¡å‹ä½¿ç”¨åŒå¡ã€‚
- **é‡åŒ–é…ç½®**ï¼š
  - **Weight-only**ï¼šINT2, INT3ã€‚
  - **Weight-Activation**ï¼šW2A4KV4, W2A4KV16ï¼ˆæƒé‡ 2-bitï¼Œæ¿€æ´» 4-bitï¼ŒKV cache 4 æˆ– 16-bitï¼‰ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Perplexity (PPL)**ï¼šè¶Šä½è¶Šå¥½ã€‚
  - **Zero-shot Accuracy**ï¼šè¶Šé«˜è¶Šå¥½ã€‚
  - **é‡åŒ–æ—¶é—´**ï¼šè¡¡é‡æ•ˆç‡ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **GPTQ**ï¼šé«˜æ•ˆçš„åŸºå‡†æ–¹æ³•ï¼Œä½†å¿½ç•¥è·¨å±‚ä¾èµ–ã€‚
- **BoA**ï¼šTURBOBOA çš„ç›´æ¥å‰èº«ï¼Œç²¾åº¦é«˜ä½†é€Ÿåº¦æ…¢ã€‚
- **Transformation-based Methods**ï¼š
  - **QuaRot**ï¼šä½¿ç”¨å“ˆè¾¾ç›æ—‹è½¬æŠ‘åˆ¶ outlierã€‚
  - **SpinQuant** å’Œ **OSTQuant**ï¼šå­¦ä¹ æ—‹è½¬çŸ©é˜µè¿›è¡Œå˜æ¢ã€‚
  - **DuQuant**, **OmniQuant**ï¼šå…¶ä»– outlier suppression æ–¹æ³•ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ

#### Weight-only Quantization (INT2, ç»“åˆ QuaRot)
| Model | Method | Wiki2 PPL | C4 PPL | Zero-shot Acc (%) |
| :--- | :--- | :--- | :--- | :--- |
| Llama3.2-1B | BoA | 40.86 | 107.9 | 38.67 |
| | **TURBOBOA** | **33.33** | **85.55** | **40.31** |
| Llama3.2-3B | BoA | 33.40 | 79.21 | 43.86 |
| | **TURBOBOA** | **24.10** | **54.20** | **45.85** |

> **ç»“è®º**ï¼šTURBOBOA åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äº BoAã€‚

#### Weight-Activation Quantization (W2A4KV16, ç»“åˆ SpinQuant)
| Model | Method | Wiki2 PPL | Zero-shot Acc (%) |
| :--- | :--- | :--- | :--- |
| Llama3.2-1B | BoA | 59.95 | 36.56 |
| | **TURBOBOA** | **49.74** | **38.28** |
| Llama3-8B | BoA | 17.31 | 44.53 |
| | **TURBOBOA** | **15.43** | **49.22** |

> **ç»“è®º**ï¼šTURBOBOA åœ¨æ›´ä¸¥æ ¼çš„é‡åŒ–è®¾ç½®ä¸‹ä»ä¿æŒé¢†å…ˆã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### 1. å¤šé€šé“è”åˆé‡åŒ–ï¼ˆF1ï¼‰
- **N=1**ï¼ˆå³ BoAï¼‰ï¼šLlama3.1-70B è€—æ—¶ **16.99 å°æ—¶**ã€‚
- **N=16**ï¼šè€—æ—¶é™è‡³ **5.636 å°æ—¶**ï¼Œ**åŠ é€Ÿè¶…è¿‡ 3 å€**ã€‚
- **ç²¾åº¦å½±å“**ï¼šå³ä½¿ N=64ï¼Œç²¾åº¦ä¸‹é™ä¹Ÿâ€œnegligibleâ€ï¼Œè¯æ˜äº†æ–¹æ³•çš„é²æ£’æ€§ã€‚

#### 2. æ€§èƒ½å¢å¼ºç‰¹æ€§ï¼ˆF2 & F3ï¼‰
åœ¨ Llama3.2-1B (INT2) ä¸Šçš„æ¶ˆèï¼ˆTable 3ï¼‰ï¼š
- **Baseline (BoA, N=16)**ï¼šWiki2 PPL = 41.85
- **+ F2 (è¯¯å·®è¡¥å¿)**ï¼šPPL â†’ 37.15
- **+ F3 (è‡ªé€‚åº”ç½‘æ ¼)**ï¼šPPL â†’ 39.45
- **+ F2 + F3 (TURBOBOA)**ï¼šPPL â†’ **33.33**

> **ç»“è®º**ï¼šä¸¤é¡¹å¢å¼ºç‰¹æ€§å‡æœ‰æ•ˆï¼Œä¸”å…·æœ‰äº’è¡¥æ€§ã€‚

#### 3. CD è¿­ä»£æ¬¡æ•°
- å®éªŒè¡¨æ˜ï¼Œ**1 æ¬¡ CD è¿­ä»£**å³å¯æ•è·ç»å¤§éƒ¨åˆ†æ€§èƒ½å¢ç›Šï¼Œæ›´å¤šè¿­ä»£æ”¶ç›Šé€’å‡ã€‚å› æ­¤ä¸»å®éªŒé‡‡ç”¨ `n_iter=1`ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å¹¶è¡ŒåŒ–ä¸ç²¾åº¦å¯å…¼å¾—**ï¼šé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é—­å¼è¯¯å·®è¡¥å¿ï¼Œå¯ä»¥å®‰å…¨åœ°å¹¶è¡Œé‡åŒ–å¤šä¸ª out-channelsï¼Œä»è€Œ**æ‰“ç ´ BoA çš„ä¸²è¡Œç“¶é¢ˆ**ï¼Œå®ç°è¶…è¿‡ 3 å€çš„åŠ é€Ÿã€‚
2. **è¯¯å·®ä¼ æ’­æ˜¯å…³é”®**ï¼šæ˜¾å¼è¡¥å¿æ¥è‡ªå‰åºé‡åŒ–å±‚çš„è¯¯å·® (`GWAX`) å¯¹ç»´æŒæ·±å±‚ç½‘ç»œçš„ç²¾åº¦è‡³å…³é‡è¦ã€‚
3. **åŠ¨æ€ç½‘æ ¼æå‡å¯¹é½**ï¼šé™æ€é‡åŒ–ç½‘æ ¼ä¼šå› æƒé‡æ›´æ–°è€Œå¤±é…ï¼Œ**è‡ªé€‚åº”é‡è®¡ç®—å’Œ CD ç²¾ç‚¼**èƒ½æœ‰æ•ˆç¼“è§£æ­¤é—®é¢˜ï¼Œå°¤å…¶åœ¨ä½æ¯”ç‰¹ä¸‹ã€‚
4. **SOTA æ€§èƒ½**ï¼šTURBOBOA ä¸ä»…æ˜¯ BoA çš„åŠ é€Ÿç‰ˆï¼Œæ›´æ˜¯å…¶å¢å¼ºç‰ˆï¼Œåœ¨ **weight-only** å’Œ **weight-activation** é‡åŒ–ä¸­å‡è¾¾åˆ°å½“å‰æœ€ä½³æ°´å¹³ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ç†è®ºåˆ†æä¸è¶³**ï¼šè™½ç„¶å®éªŒæ˜¾ç¤ºå¤§ N ä¸‹æ€§èƒ½ç¨³å®šï¼Œä½†ç¼ºä¹å¯¹ `N` ä¸æœ€ç»ˆè¯¯å·®ä¹‹é—´å…³ç³»çš„ä¸¥æ ¼ç†è®ºç•Œã€‚
- **ä¾èµ–ç‰¹å®š Hessian**ï¼šå½“å‰æ–¹æ³•åŸºäº BoA æ¨å¯¼çš„ attention-aware Hessianï¼Œå…¶é€šç”¨æ€§å—é™äºæ­¤ç±» Hessian çš„å¯ç”¨æ€§ã€‚
- **é¢å¤–å¼€é”€**ï¼šF2 ç‰¹æ€§éœ€è¦ä¸€æ¬¡é¢å¤–çš„ FP æ¨¡å‹å‰å‘ä¼ æ’­æ¥è®¡ç®— `AX`ï¼Œå°½ç®¡æ˜¯ä¸€æ¬¡æ€§çš„ï¼Œä½†ä»å¢åŠ å›ºå®šå¼€é”€ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- ä¸º `N` çš„é€‰æ‹©æä¾›**ç†è®ºè¯¯å·®è¾¹ç•Œåˆ†æ**ã€‚
- æ¢ç´¢æ›´é€šç”¨æˆ–æ›´ç²¾ç¡®çš„ **Hessian è¿‘ä¼¼æ–¹æ³•**ï¼Œå¹¶å°†å…¶é›†æˆåˆ° TURBOBOA æ¡†æ¶ä¸­ã€‚
- å°†è¯¥æ¡†æ¶æ‰©å±•åˆ°**é Transformer æ¶æ„**æˆ–å…¶ä»–ç±»å‹çš„æ¨¡å‹å‹ç¼©ä»»åŠ¡ã€‚
- è¿›ä¸€æ­¥ä¼˜åŒ– CD ç²¾ç‚¼æ­¥éª¤ï¼Œæ¢ç´¢æ›´é«˜æ•ˆçš„éæ¢¯åº¦ä¼˜åŒ–ç­–ç•¥ã€‚

> **ä»£ç åœ°å€**ï¼šhttps://github.com/SamsungLabs/TurboBoA

</details>

---

### 6. [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)

**Authors**: Yinyi Luo, Yiqiao Jin, Weichen Yu, Mengqi Zhang, Srijan Kumar, Xiaoxiao Li, Weijie Xu, Xin Chen, Jindong Wang  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.03955v1  

#### Abstract
While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weigh...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šAgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent**

---

## **1. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systems, MASï¼‰é€šè¿‡è¾©è®ºã€æ‰¹åˆ¤å’Œå…±è¯†ç­‰äº¤äº’æœºåˆ¶ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå…¶**é«˜è®¡ç®—æˆæœ¬**ï¼ˆinference latencyï¼‰å’Œ**é”™è¯¯ä¼ æ’­é£é™©**ï¼ˆerror propagationï¼‰é™åˆ¶äº†å®é™…éƒ¨ç½²ã€‚æ­¤å¤–ï¼Œè®¸å¤šç°æœ‰æ–¹æ³•ä»…æ¨¡ä»¿æœ€ç»ˆç­”æ¡ˆæˆ–æµ…å±‚äº¤äº’ç—•è¿¹ï¼Œæœªèƒ½çœŸæ­£å†…åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„**è¿­ä»£å†²çªä¸ä¿®æ­£åŠ¨æ€**ã€‚

æœ¬æ–‡æå‡ºä¸€ä¸ªæ ¹æœ¬æ€§é—®é¢˜ï¼š  
> **èƒ½å¦å°†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¨ç†ä¼˜åŠ¿â€œè’¸é¦â€åˆ°å•ä¸ªæ¨¡å‹ä¸­ï¼Œä½¿å…¶åœ¨ä¿æŒé«˜æ•ˆæ¨ç†çš„åŒæ—¶å…·å¤‡å¤šæ™ºèƒ½ä½“çš„é›†ä½“æ™ºæ…§ï¼Ÿ**

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯**
ä½œè€…æå‡ºäº† **AgentArk**ï¼Œä¸€ç§å…¨æ–°çš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„**æ¨ç†åŠ¨æ€**ï¼ˆreasoning dynamicsï¼‰é€šè¿‡è®­ç»ƒè¿‡ç¨‹â€œè’¸é¦â€è¿›å•ä¸ª LLM çš„æƒé‡ä¸­ï¼Œä»è€Œå®ç°ï¼š

- å°†æµ‹è¯•æ—¶çš„æ˜¾å¼äº¤äº’è½¬åŒ–ä¸ºæ¨¡å‹å†…éƒ¨éšå«çš„èƒ½åŠ›ï¼›
- å•ä¸€ agent åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­å³å¯æ¨¡æ‹Ÿå¤š agent çš„è¾©è¯æ¨ç†è¿‡ç¨‹ã€‚

#### **ä¸‰å¤§åˆ†å±‚è’¸é¦ç­–ç•¥**
1. **Reasoning-Enhanced SFT (RSFT)**  
   åœ¨ç›‘ç£å¾®è°ƒä¸­åŒæ—¶ä½¿ç”¨æœ€ç»ˆç­”æ¡ˆå’Œå®Œæ•´çš„æ¨ç†è½¨è¿¹ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œæå‡æ¨¡å‹ç”Ÿæˆè¿è´¯ä¸­é—´æ­¥éª¤çš„èƒ½åŠ›ã€‚

2. **Reasoning Trajectory-based Data Augmentation (DA)**  
   ä»å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­æå–å¤šæ ·åŒ–çš„æ­£ç¡®æ¨ç†è·¯å¾„ï¼Œå¹¶åˆ©ç”¨å¤§å®¹é‡æ•™å¸ˆæ¨¡å‹è¿›è¡Œå¤šæ ·åŒ–æŠ½å–ï¼Œå¢å¼ºå­¦ç”Ÿæ¨¡å‹å¯¹å¤šç§é€»è¾‘ç­–ç•¥çš„å­¦ä¹ èƒ½åŠ›ã€‚

3. **Process-Aware Distillation (PAD)**  
   å¼•å…¥ **Process Reward Model (PRM)** å¯¹æ¯ä¸€æ­¥æ¨ç†è¿›è¡Œç»†ç²’åº¦å¥–åŠ±å»ºæ¨¡ï¼Œå¹¶ç»“åˆ **Group Relative Policy Optimization (GRPO)** è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œä½¿æ¨¡å‹å­¦ä¼šè‡ªæˆ‘æ£€æŸ¥ã€åˆ†è§£ä¸çº é”™ã€‚

> âœ… **æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ¨ç†è´Ÿæ‹…ä»æ¨ç†é˜¶æ®µè½¬ç§»åˆ°è®­ç»ƒé˜¶æ®µï¼Œå®ç°â€œç¦»çº¿åä½œï¼Œåœ¨çº¿é«˜æ•ˆâ€ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | AgentArk | ä¼ ç»Ÿæ–¹æ³• |
|------|---------|--------|
| **æ•ˆç‡** | æ¨ç†ä»…éœ€å•æ¬¡ç”Ÿæˆï¼Œä½å»¶è¿Ÿ | å¤šè½®äº¤äº’ï¼Œè®¡ç®—å¼€é”€å¤§ |
| **æ³›åŒ–æ€§** | è’¸é¦çš„æ˜¯æ¨ç†æ¨¡å¼è€Œéç‰¹å®šç»“æ„ | ä¾èµ–é¢„å®šä¹‰è§’è‰²/åè®®ï¼Œéš¾ä»¥è¿ç§» |
| **é²æ£’æ€§** | å­¦ä¼šè¯†åˆ«å¹¶çº æ­£é”™è¯¯ï¼ŒæŠ—å¹²æ‰°å¼º | æ˜“å—ç¾¤ä½“åè§æˆ–å¹»è§‰æ”¾å¤§å½±å“ |
| **é€šç”¨æ€§** | æ¡†æ¶ä¸ä¾èµ–å…·ä½“ MAS ç®—æ³•ï¼ˆå¦‚ debate/critiqueï¼‰ | å¤šæ•°æ–¹æ³•ç»‘å®šç‰¹å®šäº¤äº’èŒƒå¼ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
| æ•°æ®é›† | ç±»å‹ | æè¿° |
|-------|------|------|
| **GSM8K** | æ•°å­¦æ¨ç† | å°å­¦æ•°å­¦åº”ç”¨é¢˜ï¼Œå¼ºè°ƒå¤šæ­¥ç®—æœ¯æ¨ç† |
| **MATH** | æ•°å­¦æ¨ç† | é«˜éš¾åº¦æ•°å­¦é—®é¢˜ï¼Œæ¶µç›–ä»£æ•°ã€å‡ ä½•ç­‰ |
| **MetaMathQA (MMQA)** | å¢å¼ºæ•°å­¦æ¨ç† | è‡ªåŠ¨æ‰©å¢çš„æ•°å­¦é—®ç­”æ•°æ®é›†ï¼Œå¤šæ ·æ€§æ›´é«˜ |
| **MedMCQA** | åŒ»ç–—é¢†åŸŸé—®ç­” | åŒ»å­¦è€ƒè¯•é£æ ¼é€‰æ‹©é¢˜ï¼Œè€ƒå¯Ÿä¸“ä¸šçŸ¥è¯† |
| **HotpotQA / QASPER / QMSum** | OOD æ³›åŒ–è¯„ä¼° | å¤šè·³æ¨ç†ã€é•¿æ–‡æœ¬ç†è§£ã€ä¼šè®®æ‘˜è¦ï¼Œç”¨äºé›¶æ ·æœ¬è¿ç§»æµ‹è¯• |

### **å®éªŒè®¾ç½®**
- **æ•™å¸ˆæ¨¡å‹**ï¼š`Qwen3-32B`, `Gemma3-27B-it`, `Qwen3-8B`
- **å­¦ç”Ÿæ¨¡å‹**ï¼š`Qwen3-8B`, `Qwen3-1.7B`, `Qwen3-0.6B`, `Llama3-8B`, `Gemma-7B`
- **è’¸é¦æ–¹å¼**ï¼šè·¨å®¶æ—ï¼ˆcross-familyï¼‰ã€åŒå®¶æ—ï¼ˆsame-familyï¼‰ã€å¤§å°æ¨¡å‹é—´ distillation
- **ä»£ç†æ•°é‡**ï¼šé»˜è®¤ 5 agentsï¼Œæ‰©å±•è‡³ 10 å’Œ 20 ä»¥ç ”ç©¶è§„æ¨¡æ•ˆåº”
- **è®­ç»ƒæ•°æ®é‡**ï¼šä»æ•°åƒåˆ°æ•°åä¸‡æ ·æœ¬ï¼Œåˆ†ææ•°æ®è´¨é‡ vs æ•°é‡çš„å½±å“

### **è¯„ä¼°æŒ‡æ ‡**
| æŒ‡æ ‡ç±»å‹ | å…·ä½“æŒ‡æ ‡ |
|--------|--------|
| **ä¸»æŒ‡æ ‡** | å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ |
| **æ¨ç†è´¨é‡åˆ†æ** | Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰ã€Step Decompositionã€Intermediate Verificationã€Error Localizationã€Reasoning Coherenceï¼ˆç”± InternLM-2.5-20b è‡ªåŠ¨è¯„åˆ†ï¼‰ |
| **é²æ£’æ€§è¯„ä¼°** | TruthfulQA ä¸Šçš„ BLEUã€ROUGEã€BERTScore |
| **æ³›åŒ–èƒ½åŠ›** | åœ¨æœªè§è¿‡çš„ OOD æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆHotpotQA ç­‰ï¼‰ |

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| æ–¹æ³• | æè¿° |
|------|------|
| **Single Agent** | åŸå§‹åŸºç¡€æ¨¡å‹ï¼Œæ— ä»»ä½•è’¸é¦ |
| **Vanilla Multi-Agent Debate** | å¤š agent å®æ—¶åä½œæ¨ç†ï¼ˆé«˜æˆæœ¬åŸºå‡†ï¼‰ |
| **Standard SFT** | ä»…ç”¨è¾“å…¥-è¾“å‡ºå¯¹å¾®è°ƒï¼ˆæ— æ¨ç†é“¾ï¼‰ |
| **RSFT / DA / PAD** | ä¸‰ç§è’¸é¦ç­–ç•¥å•ç‹¬åŠç»„åˆä½¿ç”¨ |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
| æ–¹æ³• | å¹³å‡æå‡ï¼ˆvs å• agentï¼‰ | æ¥è¿‘å¤š agent æ€§èƒ½ |
|------|--------------------------|--------------------|
| **AgentArk æ•´ä½“** | **+4.8%** | ä»…ç•¥ä½äºåŸå§‹ MASï¼Œä½†æ¨ç†é€Ÿåº¦å¿«æ•°å€ |
| **PAD æœ€ä¼˜è¡¨ç°** | æœ€é«˜è¾¾ **+8.9%**ï¼ˆå¦‚ Qwen3-32B â†’ Qwen3-8B on GSM8Kï¼‰ | è¾¾åˆ°å¤š agent çš„ 95%+ æ€§èƒ½ |
| **å°æ¨¡å‹æ”¶ç›Šæ›´æ˜¾è‘—** | å¦‚ `Qwen3-0.6B` åœ¨ PAD ä¸‹å‡†ç¡®ç‡ä» 41.93 â†’ 44.61ï¼ˆâ†‘2.68ï¼‰ | è¡¨æ˜è’¸é¦æœ‰æ•ˆç¼“è§£å®¹é‡ç“¶é¢ˆ |

> ğŸ“Š å›¾3æ˜¾ç¤ºï¼šæ‰€æœ‰è’¸é¦æ–¹æ³•å‡æœ‰æå‡ï¼Œå…¶ä¸­ **PAD æœ€ç¨³å®šä¸”æ•ˆæœæœ€å¼º**ã€‚

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
| å‘ç° | è¯´æ˜ |
|------|------|
| âœ… **PAD æ˜¾è‘—ä¼˜äº RSFT å’Œ DA** | åœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å–å¾—æœ€ä½³æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚ MetaMathQAï¼‰ä¸Šä¼˜åŠ¿æ˜æ˜¾ |
| âš ï¸ **RSFT å’Œ DA æå‡ä¸ç¨³å®š** | æœ‰æ—¶ç”šè‡³ä¸å¦‚ baselineï¼Œè¡¨æ˜å•çº¯å¢åŠ æ¨ç†é“¾æˆ–æ•°æ®é‡ä¸è¶³ä»¥ä¿è¯æ”¶ç›Š |
| ğŸ” **æ–¹æ³•å¯ç»„åˆä½¿ç”¨** | RSFT+DA æˆ– PAD+DA å¯è¿›ä¸€æ­¥å°å¹…æå‡æ€§èƒ½ï¼ˆè§ Table 7ï¼‰ï¼Œè¯æ˜ç­–ç•¥å…¼å®¹æ€§å¼º |

### **æ¶ˆèå®éªŒç»“æœ**
#### **(1) PRM å®¹é‡æ¯”å­¦ç”Ÿæ¨¡å‹æ›´é‡è¦**
- ä½¿ç”¨æ›´å¤§ PRMï¼ˆå¦‚ 8Bï¼‰å³ä½¿ç›‘ç£å°æ¨¡å‹ï¼ˆ0.6Bï¼‰ï¼Œä¹Ÿèƒ½å¸¦æ¥æ˜¾è‘—æå‡ï¼›
- è‹¥ PRM å¤ªå°ï¼ˆ0.6Bï¼‰ï¼Œå³ä½¿å­¦ç”Ÿæ˜¯ 8B æ¨¡å‹ä¹Ÿéš¾æœ‰æ”¹è¿›ã€‚
> ğŸ‘‰ ç»“è®ºï¼š**é«˜è´¨é‡çš„è¿‡ç¨‹ç›‘ç£ä¿¡å·æ˜¯æˆåŠŸçš„å…³é”®**ã€‚

#### **(2) å­¦ç”Ÿæ¨¡å‹å®¹é‡å†³å®šä¸Šé™**
- å°æ¨¡å‹ï¼ˆå¦‚ 0.6Bï¼‰åœ¨æ•™å¸ˆè¶…è¿‡ä¸€å®šè§„æ¨¡åå‡ºç°æ€§èƒ½é¥±å’Œç”šè‡³ä¸‹é™ï¼›
- å¤§æ¨¡å‹ï¼ˆå¦‚ 8Bï¼‰èƒ½æŒç»­å—ç›Šäºæ›´å¤š agent å’Œæ›´ä¸°å¯Œæ•°æ®ã€‚
> ğŸ‘‰ ç»“è®ºï¼š**è’¸é¦å¢ç›Šå—é™äºå­¦ç”Ÿæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›**ã€‚

#### **(3) æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡**
- å•çº¯æ‰©å¤§è®­ç»ƒæ•°æ®å¹¶æœªå¸¦æ¥å•è°ƒæå‡ï¼Œåè€Œå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆæˆ–å™ªå£°å¹²æ‰°ï¼›
- PAD å› æœ‰ PRM è¿‡æ»¤é«˜è´¨é‡æ¨ç†æ­¥éª¤ï¼Œè¡¨ç°æœ€ç¨³å¥ã€‚
> ğŸ‘‰ ç»“è®ºï¼š**é«˜ä¿¡å™ªæ¯”çš„æ¨ç†è½¨è¿¹æ¯”æµ·é‡æ•°æ®æ›´é‡è¦**ã€‚

#### **(4) è·¨æ¨¡æ€è’¸é¦åˆæ­¥éªŒè¯**
- å°†æ–‡æœ¬æ¨ç†èƒ½åŠ›è’¸é¦åˆ° **Multimodal LLMs (MLLMs)**ï¼ˆå¦‚ Qwen2.5-VLï¼‰ä¸­ä»æœ‰æ•ˆï¼›
- å°½ç®¡ç»å¯¹å¢ç›Šè¾ƒå°ï¼Œä½† PAD ä¾ç„¶æœ€ä¼˜ã€‚
> ğŸ‘‰ æ”¯æŒâ€œæ¨ç†èƒ½åŠ›å…·æœ‰æ¨¡æ€æ— å…³æ€§â€çš„å‡è®¾ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **å•ä¸ª LLM å¯ä»¥å†…åŒ–å¤šæ™ºèƒ½ä½“çš„æ¨ç†èƒ½åŠ›**  
   é€šè¿‡åˆç†çš„è’¸é¦è®¾è®¡ï¼Œå• agent å¯åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­å±•ç°å‡ºç±»ä¼¼å¤š agent çš„è‡ªæˆ‘åæ€ã€é”™è¯¯æ£€æµ‹ä¸ä¿®æ­£è¡Œä¸ºã€‚

2. âœ… **è¿‡ç¨‹æ„ŸçŸ¥è’¸é¦ï¼ˆPADï¼‰æ˜¯æœ€æœ‰æ•ˆçš„ç­–ç•¥**  
   PRM + GRPO æ¶æ„ä¸ä»…èƒ½æé«˜å‡†ç¡®æ€§ï¼Œè¿˜èƒ½æ”¹å–„æ¨ç†è¡Œä¸ºæœ¬èº«ï¼ˆå¦‚åˆ†è§£ã€è‡ªæ£€ã€çº é”™ï¼‰ã€‚

3. âœ… **æ¨ç†è´¨é‡ä¼˜äºæ•°é‡ï¼Œç›‘ç£ä¿¡å·çš„è´¨é‡è‡³å…³é‡è¦**  
   é«˜è´¨é‡çš„ä¿®æ­£è½¨è¿¹ï¼ˆcorrective trajectoriesï¼‰æ¯”å¤§é‡æ™®é€šè½¨è¿¹æ›´æœ‰ä»·å€¼ã€‚

4. âœ… **è’¸é¦å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§**  
   åœ¨ OOD ä»»åŠ¡ï¼ˆå¦‚å¤šè·³é—®ç­”ã€æ‘˜è¦ï¼‰å’Œ TruthfulQA ä¸Šå‡è¡¨ç°æ›´ä¼˜ï¼Œè¯´æ˜å­¦åˆ°çš„æ˜¯é€šç”¨æ¨ç†èƒ½åŠ›è€Œéè¿‡æ‹Ÿåˆã€‚

5. âœ… **é€‚ç”¨äºè·¨æ¶æ„ã€è·¨è§„æ¨¡ã€è·¨æ¨¡æ€åœºæ™¯**  
   ä¸é™äºç‰¹å®šæ¨¡å‹å®¶æ—æˆ–æ¨¡æ€ï¼Œå…·å¤‡è¾ƒå¼ºé€šç”¨æ€§ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
| å±€é™ | è¯´æ˜ |
|------|------|
| **ä¾èµ–é«˜è´¨é‡æ•™å¸ˆæ•°æ®** | è‹¥æ•™å¸ˆæ¨¡å‹æœ¬èº«å­˜åœ¨ç³»ç»Ÿæ€§åå·®æˆ–é”™è¯¯ï¼Œå¯èƒ½è¢«è’¸é¦è¿›å­¦ç”Ÿæ¨¡å‹ |
| **è®­ç»ƒæˆæœ¬è¾ƒé«˜** | ç‰¹åˆ«æ˜¯ PAD ä¸­ PRM è®­ç»ƒå’Œ GRPO ä¼˜åŒ–éœ€è¦è¾ƒå¤š GPU æ—¶é—´ï¼ˆçº¦ 20 å°æ—¶ on H100ï¼‰ |
| **å½“å‰èšç„¦æ¨ç†ä»»åŠ¡** | æœªæ¶‰åŠå·¥å…·è°ƒç”¨ï¼ˆtool useï¼‰ã€è®°å¿†ç®¡ç†ï¼ˆmemoryï¼‰ç­‰å…¶ä»– agent åŠŸèƒ½ |
| **æœªæ¢ç´¢éè¾©è®ºç±» MAS** | å½“å‰ä¸»è¦åŸºäº debate æœºåˆ¶ï¼Œæ˜¯å¦é€‚ç”¨äº reflection/collaboration ç­‰å¾…éªŒè¯ |

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **è‡ªé€‚åº”è’¸é¦ç­–ç•¥**  
   æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€é€‰æ‹©è’¸é¦å¼ºåº¦æˆ–è½¨è¿¹ç±»å‹ã€‚

2. **æ¨¡å—åŒ– PRM è®¾è®¡**  
   æ„å»ºé’ˆå¯¹ä¸åŒæ¨ç†ç»„ä»¶ï¼ˆå¦‚åˆ†è§£ã€éªŒè¯ã€å½’çº³ï¼‰çš„ä¸“ä¸šåŒ– PRMã€‚

3. **æ‰©å±•è‡³çœŸå®ä¸–ç•Œåœºæ™¯**  
   åº”ç”¨äºå®‰å…¨å…³é”®å†³ç­–æ”¯æŒã€è‡ªåŠ¨åŒ–ç¼–ç¨‹åŠ©æ‰‹ã€æ•™è‚²è¾…å¯¼ç­‰å®é™…åº”ç”¨ã€‚

4. **è½»é‡åŒ–éƒ¨ç½²ä¼˜åŒ–**  
   æ¢ç´¢å¦‚ä½•å°†è’¸é¦åçš„æ¨¡å‹å‹ç¼©è‡³è¾¹ç¼˜è®¾å¤‡è¿è¡Œã€‚

5. **å¤šæ¨¡æ€è”åˆè’¸é¦**  
   åŒæ—¶è’¸é¦è§†è§‰-è¯­è¨€è”åˆæ¨ç†èƒ½åŠ›ï¼Œæ„å»ºçœŸæ­£çš„é€šç”¨æ™ºèƒ½ä½“ã€‚

---

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **AgentArk æˆåŠŸå®ç°äº†â€œæŠŠä¸€ç¾¤èªæ˜äººçš„è®¨è®ºè¿‡ç¨‹ï¼Œå˜æˆä¸€ä¸ªäººè„‘å­é‡Œçš„æ€è€ƒä¹ æƒ¯â€ï¼Œä¸ºé«˜æ•ˆã€é²æ£’ã€å¯æ‰©å±•çš„ LLM æ¨ç†æä¾›äº†æ–°èŒƒå¼ã€‚**

</details>

---

### 7. [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)

**Authors**: Yansong Ning, Jun Fang, Naiqiang Tan, Hao Liu  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.04284v1  

#### Abstract
Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šAgent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰çš„ **LLM Agent** åœ¨å¤šè½®ä¸ç¯å¢ƒäº¤äº’è¿‡ç¨‹ä¸­ï¼Œé€šå¸¸ä¼šç”Ÿæˆå†—ä½™çš„ **Thought**ï¼ˆæ¨ç†è¿‡ç¨‹ï¼‰å¹¶ç´¯ç§¯å¤§é‡å†å² **Observation**ï¼ˆç¯å¢ƒåé¦ˆï¼‰ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡é•¿åº¦æ€¥å‰§å¢é•¿ï¼Œæ˜¾è‘—é™ä½æ¨ç†æ•ˆç‡ï¼ˆtoken cost é«˜ï¼‰ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

ç°æœ‰æ–¹æ³•å¦‚ **Thought Management (TM)**ã€**Observation Management (OM)** å’Œ **Thought&Observation Management (TOM)** å¤šé‡‡ç”¨å…¨å±€å‹ç¼©æˆ–é™æ€å‰ªæç­–ç•¥ï¼ˆå¦‚å›ºå®šçª—å£ã€LLM summarizationï¼‰ï¼Œå¿½ç•¥äº†ä¸åŒäº¤äº’è½®æ¬¡ä¸­ **Thought å¿…è¦æ€§** å’Œ **Observation å®ç”¨æ€§** çš„åŠ¨æ€å˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™æˆ–ä¼˜åŒ–ä¸è¶³ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡º **Agent-Omit**ï¼Œä¸€ä¸ªç»Ÿä¸€çš„è®­ç»ƒæ¡†æ¶ï¼Œä½¿ LLM Agent èƒ½å¤Ÿ**è‡ªé€‚åº”åœ°çœç•¥å†—ä½™çš„ Thought å’Œ Observation**ï¼Œä»è€Œæå‡æ•ˆç‡è€Œä¸ç‰ºç‰²æ•ˆæœã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
- **ç»Ÿä¸€çš„åˆ†ææ¡†æ¶**ï¼šé¦–æ¬¡é€šè¿‡å®šé‡å®éªŒè¯æ˜ï¼ŒThought å’Œ Observation å¯¹ä»»åŠ¡æˆåŠŸçš„å½±å“æ˜¯**è½®æ¬¡ä¾èµ–**ï¼ˆturn-dependentï¼‰çš„ï¼Œå¹¶éæ‰€æœ‰è½®æ¬¡éƒ½åŒç­‰é‡è¦ã€‚
- **è‡ªé€‚åº”çœç•¥æœºåˆ¶**ï¼šAgent å¯ä»¥åœ¨è¿è¡Œæ—¶åŠ¨æ€å†³å®šæ˜¯å¦çœç•¥æŸä¸€è½®çš„ Thought æˆ–æŒ‡å®šå†å²è½®æ¬¡çš„ Observationã€‚
- **ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼**ï¼š
  1. **Omission Behavior Synthesisï¼ˆå†·å¯åŠ¨å¾®è°ƒï¼‰**ï¼šæ„å»ºå•è½®ä¸å¤šè½®çœç•¥æ ·æœ¬ï¼Œæ•™ä¼šæ¨¡å‹å¦‚ä½•æ‰§è¡Œçœç•¥è¡Œä¸ºã€‚
  2. **Omit-Aware Agentic Reinforcement Learningï¼ˆçœç•¥æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼‰**ï¼š
     - å¼•å…¥ **Dual Sampling Strategy**ï¼ˆå…¨è½¨è¿¹ vs. å±€éƒ¨è½¨è¿¹é‡‡æ ·ï¼‰ï¼Œè§£å†³â€œçœç•¥åæ— æ³•å­¦ä¹ â€çš„ä¸Šä¸‹æ–‡æ–­è£‚é—®é¢˜ã€‚
     - è®¾è®¡ **Omission Reward**ï¼Œæ˜¾å¼é¼“åŠ± token èŠ‚çœï¼ŒåŒæ—¶ä¿è¯ä»»åŠ¡æ­£ç¡®æ€§ä¼˜å…ˆã€‚
- **ç†è®ºä¿éšœ**ï¼šè¯æ˜æ‰€å­¦çœç•¥ç­–ç•¥çš„åå·®ç”± KL æ•£åº¦ä¸Šç•Œçº¦æŸï¼Œç¡®ä¿æ”¶æ•›æ€§ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ç°æœ‰æ–¹æ³• | Agent-Omit |
|------|--------|----------|
| **çœç•¥ç²’åº¦** | å…¨å±€/é™æ€ï¼ˆå¦‚æ»‘åŠ¨çª—å£ï¼‰ | **ç»†ç²’åº¦ã€è‡ªé€‚åº”ã€è½®æ¬¡çº§å†³ç­–** |
| **è®­ç»ƒæ–¹å¼** | å¯å‘å¼è§„åˆ™æˆ–ç‹¬ç«‹å‹ç¼©æ¨¡å— | **ç«¯åˆ°ç«¯è”åˆè®­ç»ƒï¼Œç­–ç•¥å¯å­¦ä¹ ** |
| **ä¸Šä¸‹æ–‡ç®¡ç†** | æ˜“ä¸¢å¤±å…³é”®ä¿¡æ¯æˆ–è¿‡åº¦ä¿ç•™å™ªå£° | **æ™ºèƒ½è¯†åˆ«å†—ä½™ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§** |
| **æ•ˆç‡-æ•ˆæœæƒè¡¡** | å¾€å¾€ç‰ºç‰²å‡†ç¡®æ€§æ¢æ•ˆç‡ | **å®ç°æ›´ä¼˜çš„ effectiveness-efficiency trade-off** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
åœ¨äº”ä¸ªå¤šæ ·åŒ–åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¦†ç›–å¤šç§ä»»åŠ¡ç±»å‹ï¼š

| æ•°æ®é›† | ç±»å‹ | ä»»åŠ¡æè¿° |
|-------|------|---------|
| **DeepSearch** | Information Search | ä½¿ç”¨æœç´¢å¼•æ“å›ç­”çŸ¥è¯†å¯†é›†å‹é—®é¢˜ |
| **WebShop** | Web Navigation | åœ¨ç”µå•†ç½‘ç«™ä¸­å¯¼èˆªå¹¶å®Œæˆè´­ä¹°ä»»åŠ¡ |
| **TextCraft** | Digital Game | æ–‡æœ¬ç‰ˆ Minecraftï¼Œéœ€è§„åˆ’åˆæˆè·¯å¾„ |
| **BabyAI** | Embodied Control | ç½‘æ ¼ä¸–ç•Œä¸­éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤å¯¼èˆª |
| **SciWorld** | Scientific Discovery | è¿›è¡Œç§‘å­¦å®éªŒè®¾è®¡ä¸å‡è®¾æ£€éªŒ |

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### ä¸»è¦è¯„ä¼°æŒ‡æ ‡ï¼š
- **Pass@1**ï¼šä»»åŠ¡æˆåŠŸç‡ï¼ˆä¸»æŒ‡æ ‡ï¼‰
- **Avg Tok. â†“**ï¼šå¹³å‡æ€» token æ¶ˆè€—é‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- **Effectiveness-Efficiency Trade-off**ï¼šç»¼åˆè¡¡é‡å‡†ç¡®ç‡ä¸æˆæœ¬

#### æ¨¡å‹é…ç½®ï¼š
- **Backbone**ï¼šQwen3-4B / Qwen3-8B
- **è®­ç»ƒæµç¨‹**ï¼š
  - **SFT é˜¶æ®µ**ï¼šä½¿ç”¨åˆæˆçš„ 2â€“4K å†·å¯åŠ¨æ•°æ®è¿›è¡Œå…¨å‚æ•°å¾®è°ƒ
  - **RL é˜¶æ®µ**ï¼šåŸºäº **Group Relative Policy Optimization (GRPO)** è¿›è¡Œ agentic RL è®­ç»ƒ
- **æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š32K tokens

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

#### å¯¹æ¯”ä¸¤ç±»åŸºçº¿ï¼š

##### ï¼ˆ1ï¼‰å‰æ²¿ LLM Agentï¼ˆFrontier LLM Agentsï¼‰
- DeepSeek-R1-0528, DeepSeek-V3.2
- OpenAI o3/o4-mini
- Qwen3 ç³»åˆ—å¤§æ¨¡å‹ï¼ˆ235B, 80B, 32Bï¼‰

##### ï¼ˆ2ï¼‰é«˜æ•ˆ Agent æ„å»ºæ–¹æ³•ï¼ˆEfficient Agent Methodsï¼‰
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **Thought Management** | Thinking-Retention, DEPO, ToolLight |
| **Observation Management** | Observation-Mask, DeepMiner |
| **TOM** | MEM-Agent, ReSum |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 2 & 3ï¼‰

#### âœ… ä¸å‰æ²¿ LLM Agent å¯¹æ¯”ï¼ˆAgent-Omit-8B-RL è¡¨ç°çªå‡ºï¼‰ï¼š

| æ¨¡å‹ | DeepSearch (Pass@1) | DeepSearch (Tok.) | WebShop (Pass@1) | WebShop (Tok.) |
|------|---------------------|------------------|------------------|---------------|
| DeepSeek-R1-0528 | 25.25 | 6,412 | 19.37 | 11,308 |
| Qwen3-32B | 19.00 | 6,640 | 11.31 | 11,872 |
| **Agent-Omit-8B-RL** | **26.56** | **4,356** | **23.57** | **8,764** |

> ğŸ’¡ ç»“è®ºï¼š**Agent-Omit-8B-RL ä¸ä»…åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šæ‰€æœ‰é—­æº/å¼€æºå¤§æ¨¡å‹ï¼Œä¸” token æ¶ˆè€—æ˜¾è‘—æ›´ä½**ã€‚

#### âœ… ä¸é«˜æ•ˆ Agent æ–¹æ³•å¯¹æ¯”ï¼ˆå…¨é¢é¢†å…ˆï¼‰ï¼š

| æ–¹æ³• | DeepSearch (Pass@1) | DeepSearch (Tok.) | WebShop (Pass@1) | WebShop (Tok.) |
|------|---------------------|------------------|------------------|---------------|
| Qwen3-8B Base | 17.75 | 8,281 | 6.93 | 16,741 |
| ReSum | 22.28 | 5,724 | 17.80 | 9,251 |
| **Agent-Omit-8B-RL** | **24.56** | **4,356** | **23.57** | **8,764** |

> ğŸ’¡ ç»“è®ºï¼š**åœ¨ç›¸åŒ backbone ä¸‹ï¼ŒAgent-Omit å®ç°æœ€é«˜å‡†ç¡®ç‡ + æœ€ä½ token æˆæœ¬ï¼Œè¾¾æˆæœ€ä¼˜ trade-off**ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

åœ¨ WebShop ä¸Šå¯¹ Agent-Omit-8B-RL è¿›è¡Œæ¶ˆèï¼ˆè§ Figure 5ï¼‰ï¼š

| å˜ä½“ | Pass@1 | Avg Tok. | åˆ†æ |
|------|--------|---------|------|
| å®Œæ•´æ¨¡å‹ï¼ˆAgent-Omit-RLï¼‰ | 23.57 | 8,764 | æ€§èƒ½æœ€ä½³ |
| ç§»é™¤ Omission Reward (w/o OR) | ~22.0 | >9K | ç¼ºå°‘æ¿€åŠ±ï¼Œtoken å‡å°‘ä¸æ˜æ˜¾ |
| ç§»é™¤ Partial Trajectory (w/o PT) | ~21.5 | ~9.5K | å­¦ä¹ ä¿¡å·å¼±ï¼Œç­–ç•¥éš¾æ”¶æ•› |
| ç§»é™¤ Full Trajectory (w/o FT) | ~22.0 | ~9K | ç¼ºå°‘æ•´ä½“ä»»åŠ¡ç›‘ç£ |
| ä»… SFT é˜¶æ®µ | 14.43 | 11,376 | æ˜¾è‘—åŠ£äº RL ç‰ˆæœ¬ |

> ğŸ” å‘ç°ï¼š
- **SFT + RL åŒé˜¶æ®µç¼ºä¸€ä¸å¯**ï¼ŒRL æ˜¯æ€§èƒ½è·ƒå‡çš„å…³é”®ã€‚
- **Partial Trajectory é‡‡æ ·** æ¯” Full æ›´é‡è¦ï¼Œè§£å†³äº†â€œçœç•¥åæ— æ³•å­¦ä¹ â€çš„æ ¹æœ¬éš¾é¢˜ã€‚
- **Omission Reward** æ˜¯é©±åŠ¨ token å‹ç¼©çš„æ ¸å¿ƒåŠ¨åŠ›ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Thought å’Œ Observation çš„ä»·å€¼éšè½®æ¬¡å˜åŒ–**ï¼šåˆæœŸç”¨äºè§„åˆ’ï¼Œæœ«æœŸç”¨äºæ€»ç»“ï¼Œä¸­é—´è½®æ¬¡å¸¸ä¸ºå†—ä½™ã€‚
2. **é€‰æ‹©æ€§çœç•¥å¯è¡Œä¸”æœ‰æ•ˆ**ï¼šé€‚å½“çœç•¥ä¸­é—´è½®æ¬¡çš„ Thought/Observation å¯å¤§å¹…å‡å°‘ token å¼€é”€ï¼Œ**ä¸æŸå®³ç”šè‡³æå‡æ€§èƒ½**ã€‚
3. **è‡ªé€‚åº”ç­–ç•¥ä¼˜äºé™æ€è§„åˆ™**ï¼šå¯å‘å¼æ–¹æ³•ï¼ˆå¦‚æ»‘åŠ¨çª—å£ï¼‰æ˜“è¯¯åˆ å…³é”®ä¿¡æ¯ï¼›è€Œå¯å­¦ä¹ çš„çœç•¥ç­–ç•¥èƒ½åŠ¨æ€åˆ¤æ–­ä½•æ—¶è¯¥çœç•¥ã€‚
4. **Agent-Omit å®ç°å°æ¨¡å‹èµ¶è¶…å¤§æ¨¡å‹**ï¼šåŸºäº Qwen3-8B çš„ Agent-Omit-RL åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¿‡æ•°åäº¿ä¹ƒè‡³æ•°ç™¾äº¿å‚æ•°çš„å‰æ²¿æ¨¡å‹ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–é«˜è´¨é‡å†·å¯åŠ¨æ•°æ®åˆæˆ**ï¼šè‡ªåŠ¨è¯†åˆ«â€œå¯çœç•¥è½®æ¬¡â€éœ€è¦ rollout åˆ†æï¼Œè®¡ç®—å¼€é”€è¾ƒé«˜ã€‚
- **ç›®å‰ä»…é€‚ç”¨äºæ–‡æœ¬å‹äº¤äº’ç¯å¢ƒ**ï¼šæ‰©å±•è‡³è§†è§‰æˆ–å¤šæ¨¡æ€ Agent å°šæœªéªŒè¯ã€‚
- **å¯¹å¤æ‚é•¿æœŸä¾èµ–ä»»åŠ¡ä»å¯èƒ½è¯¯åˆ å…³é”®ä¸Šä¸‹æ–‡**ï¼šå°½ç®¡æœ‰ç†è®ºè¾¹ç•Œï¼Œä½†åœ¨æç«¯æƒ…å†µä¸‹ä»å­˜åœ¨é£é™©ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **å°† omission data synthesis æ‰©å±•è‡³é¢„è®­ç»ƒé˜¶æ®µ**ï¼Œæ„å»ºâ€œå¤©ç”Ÿé«˜æ•ˆâ€çš„ Agentã€‚
2. **åº”ç”¨äºæ›´å¤§è§„æ¨¡çš„ LLM**ï¼ˆå¦‚ Qwen3-72B æˆ–ä»¥ä¸Šï¼‰ï¼Œæ¢ç´¢æé™æ€§èƒ½ã€‚
3. **ç»“åˆ memory-augmented architecture**ï¼Œè¿›ä¸€æ­¥å¢å¼ºä¸Šä¸‹æ–‡ç®¡ç†èƒ½åŠ›ã€‚
4. **æ¢ç´¢å¤šæ¨¡æ€çœç•¥æœºåˆ¶**ï¼Œæ”¯æŒå›¾åƒã€éŸ³é¢‘ç­‰æ¨¡æ€çš„è‡ªé€‚åº”è¿‡æ»¤ã€‚

---

> ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **Agent-Omit é€šè¿‡â€œå¯å­¦ä¹ çš„è‡ªé€‚åº”çœç•¥â€æœºåˆ¶ï¼Œåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹æ˜¾è‘—æå‡äº† LLM Agent çš„æ¨ç†æ•ˆç‡ï¼Œä¸ºæ„å»ºè½»é‡ã€é«˜é€Ÿã€é«˜å‡†çš„æ™ºèƒ½ä½“æä¾›äº†æ–°èŒƒå¼ã€‚**

</details>

---

### 8. [RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference](https://arxiv.org/abs/2602.05853)

**Authors**: Siran Liu, Guoxia Wang, Sa Wang, Jinle Zeng, HaoYang Xie, Siyu Lou, JiaBin Yang, DianHai Yu, Haifeng Wang, Chao Yang  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.05853v1  

#### Abstract
The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating que...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference â€”â€” æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶é¢ä¸´ **attention æœºåˆ¶çš„äºŒæ¬¡è®¡ç®—å¤æ‚åº¦ $O(L^2)$** é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚ã€éš¾ä»¥éƒ¨ç½²ã€‚è™½ç„¶å·²æœ‰åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›ï¼ˆdynamic sparse attentionï¼‰æ–¹æ³•å°è¯•é€šè¿‡è‡ªé€‚åº”é€‰æ‹©é‡è¦ attention åˆ†æ•°æ¥é™ä½è®¡ç®—é‡ï¼Œä½†è¿™äº›æ–¹æ³•æ™®éå­˜åœ¨ä»¥ä¸‹æƒè¡¡ï¼ˆtrade-offsï¼‰ï¼š

- éœ€è¦é¢„è®­ç»ƒæˆ–ç¦»çº¿æ¨¡å¼æœç´¢ï¼ˆpreprocessingï¼‰
- ç¼ºä¹å…¨å±€è¯„ä¼°èƒ½åŠ›ï¼ˆglobal evaluationï¼‰ï¼Œæ˜“é—æ¼è¿œè·ç¦»ä¾èµ–
- è¿åæŸ¥è¯¢ç‹¬ç«‹æ€§ï¼ˆquery independenceï¼‰ï¼Œä¸åŒ query çš„æ³¨æ„åŠ›åˆ†å¸ƒç›¸äº’å¹²æ‰°
- æ¨¡å¼çµæ´»æ€§å·®æˆ–è®¡ç®—å¼€é”€é«˜

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **RRAttention**ï¼Œä¸€ç§å…¨æ–°çš„åŠ¨æ€å—ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯ **Per-Head Round-Robinï¼ˆå¤´è½®è½¬ï¼‰é‡‡æ ·ç­–ç•¥**ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- åœ¨æ¯ä¸ª **stride** å†…ï¼Œä¸ºä¸åŒçš„ attention head è½®æµé€‰æ‹©ä¸åŒçš„ query ä½ç½®è¿›è¡Œé‡‡æ ·ã€‚
- æ‰€æœ‰ head åˆä½œå®Œæˆå¯¹æ•´ä¸ªåºåˆ—çš„è¦†ç›–ï¼Œå®ç° **stride-level èšåˆ** å’Œ **block-level ç¨€ç–è®¡ç®—**ã€‚
- ç»“åˆ **adaptive Top-T block selection** åŠ¨æ€ä¿ç•™æœ€é‡è¦çš„ attention å—ã€‚

#### å…·ä½“æµç¨‹ä¸‰é˜¶æ®µï¼š
1. **Query Sampling with Head Round-Robin Strategy**  
   æ¯ä¸ª head åœ¨ stride å†…æŒ‰å…¬å¼ $P(i,h) = iS + (S-1 - (h \mod S))$ é€‰å–ä¸€ä¸ªä»£è¡¨æ€§çš„ queryï¼Œç¡®ä¿æ‰€æœ‰ä½ç½®éƒ½èƒ½è¢«é‡‡æ ·åˆ°ã€‚

2. **Stride-level Importance Estimation**  
   å°† key å‘é‡åœ¨ stride ç²’åº¦èšåˆï¼Œä¸é‡‡æ ·çš„ query è®¡ç®— attention å¾—åˆ†ï¼Œæ˜¾è‘—é™ä½å¤æ‚åº¦è‡³ $O(L^2/S^2)$ã€‚

3. **Block-level Selection via Top-T Thresholding**  
   å°† stride-level åˆ†æ•°èšåˆä¸º block-levelï¼Œä¿ç•™ç´¯è®¡é‡è¦æ€§è¶…è¿‡é˜ˆå€¼ $T$ çš„ key blocksï¼Œå¹¶å¼ºåˆ¶ä¿ç•™æœ€åä¸€ä¸ª query block ä»¥ä¿éšœç”Ÿæˆè´¨é‡ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | RRAttention | FlexPrefill | XAttention | SeerAttention | MInference |
|------|-------------|-----------|----------|---------------|------------|
| **æ— éœ€é¢„å¤„ç†ï¼ˆPreprocessing-freeï¼‰** | âœ… | âœ… | âœ… | âŒ | âŒ |
| **æ”¯æŒå…¨å±€è¯„ä¼°ï¼ˆGlobal Evaluationï¼‰** | âœ… | âŒ | âœ… | âœ… | âŒ |
| **ä¿æŒæŸ¥è¯¢ç‹¬ç«‹æ€§ï¼ˆQuery Independenceï¼‰** | âœ… | âŒ | âŒ | âœ… | âœ… |
| **æ¨¡å¼æ— å…³ï¼ˆPattern-agnosticï¼‰** | âœ… | âŒ | âœ… | âœ… | âŒ |
| **é«˜æ•ˆ softmax ç²’åº¦ï¼ˆStride-levelï¼‰** | âœ… | âŒï¼ˆtokenï¼‰ | âœ… | âŒï¼ˆblockï¼‰ | âŒï¼ˆtokenï¼‰ |

ğŸ‘‰ **RRAttention æ˜¯å”¯ä¸€åŒæ—¶æ»¡è¶³äº”å¤§ç†æƒ³å±æ€§çš„æ–¹æ³•**ï¼Œå®ç°äº†ç²¾åº¦ä¸æ•ˆç‡çš„æœ€ä½³å¹³è¡¡ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

| ç±»åˆ« | æ•°æ®é›† | æè¿° |
|------|--------|------|
| **è‡ªç„¶è¯­è¨€ç†è§£** | [HELMET](https://arxiv.org/abs/2412.15115) | åŒ…å«7å¤§ç±»ä»»åŠ¡ï¼š<br>- æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰<br>- å¼•ç”¨ç”Ÿæˆï¼ˆCiteï¼‰<br>- å¤šè·³æ¨ç†ï¼ˆICLï¼‰<br>- é•¿æ–‡æ¡£é—®ç­”ï¼ˆLongQAï¼‰<br>- æ®µè½é‡æ’åºï¼ˆRerankï¼‰<br>- æ‘˜è¦ï¼ˆSummarizationï¼‰<br>- åˆæˆå›å¿†ï¼ˆRecallï¼‰ |
| **å¤šæ¨¡æ€è§†é¢‘ç†è§£** | [Video-MME](https://arxiv.org/abs/2405.21075) | åŒ…å«900ä¸ªè§†é¢‘ã€2700é“å¤šé€‰é¢˜ï¼Œæ¶µç›–æ„ŸçŸ¥ã€æ¨ç†ã€ä¿¡æ¯æ•´åˆç­‰12ç§ä»»åŠ¡ç±»å‹ï¼Œæµ‹è¯•æ¨¡å‹è·¨å¸§æ—¶ç©ºå»ºæ¨¡èƒ½åŠ› |

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

- **æ¨¡å‹**ï¼š
  - LLaMA-3.1-8B-Instructï¼ˆæ”¯æŒ 128K ä¸Šä¸‹æ–‡ï¼‰
  - Qwen2.5-7B-Instructï¼ˆæ”¯æŒ 128Kï¼‰
  - Qwen2-VL-7B-Instructï¼ˆç”¨äº Video-MMEï¼‰
  - åç»­æ‰©å±•è‡³ Yi-9B-200K å’Œ Qwen3-30B-A3B-Instruct éªŒè¯æ³›åŒ–æ€§

- **ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š8K, 16K, 32K, 64K, 128K tokens

- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **å¹³å‡å‡†ç¡®ç‡ï¼ˆAvg. Scoreï¼‰**
  - **ç¨€ç–åº¦ï¼ˆSparsityï¼‰**ï¼šè·³è¿‡çš„ attention block æ¯”ä¾‹
  - **ç«¯åˆ°ç«¯æ¨ç†é€Ÿåº¦ï¼ˆFPS / Timeï¼‰**
  - **æ¨¡å¼æœç´¢å¼€é”€ï¼ˆPattern Search Overheadï¼‰**

- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA H100 GPU

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| æ–¹æ³• | ç‰¹ç‚¹ |
|------|------|
| **FlashAttention** | å¯†é›† attention åŸºçº¿ï¼Œä½œä¸ºæ€§èƒ½ä¸Šé™å‚è€ƒ |
| **FlexPrefill** | åŸºäºæœ€åä¸€ä¸ª query block å‘ç°å‚ç›´/æ–œçº¿æ¨¡å¼ï¼Œä½¿ç”¨ JS æ•£åº¦åˆ¤æ–­å¯é æ€§ |
| **XAttention** | æŠ—å¯¹è§’çº¿é‡‡æ · + stride-level èšåˆï¼Œæ”¯æŒå…¨å±€è¯„ä¼°ä½†è¿å query independence |
| **RRAttention (Ours)** | å¤´è½®è½¬é‡‡æ · + stride-level é‡è¦æ€§ä¼°è®¡ + Top-T block é€‰æ‹© |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… **åœ¨ HELMET ä¸Šçš„è¡¨ç°ï¼ˆLLaMA & Qwenï¼‰**

| æ¨¡å‹ | æ–¹æ³• | å¹³å‡å¾—åˆ† | ç¨€ç–åº¦ | æ¢å¤ FullAttention æ€§èƒ½ |
|------|------|---------|--------|------------------------|
| LLaMA | RRAttention ($\gamma=0.95$) | **56.24** | 48.68% | **99.7%** |
| LLaMA | XAttention ($T=0.95$) | 55.74 | 47.50% | 98.8% |
| LLaMA | FlexPrefill ($\gamma=0.99$) | 56.02 | 38.63% | 99.3% |

> ğŸ”º åœ¨æ›´é«˜ç¨€ç–åº¦ä¸‹ä»å–å¾—æœ€é«˜ç²¾åº¦ï¼Œè¯´æ˜ **pattern å‘ç°æ›´æœ‰æ•ˆ**ã€‚

#### âœ… **åœ¨ Video-MME ä¸Šçš„è¡¨ç°ï¼ˆQwen2-VLï¼‰**

| è®¾ç½® | æ–¹æ³• | å¹³å‡å¾—åˆ† | ç¨€ç–åº¦ |
|------|------|---------|--------|
| 1fps | RRAttention ($T=0.95$) | **64.30** | 34.70% |
| 1fps | XAttention ($T=0.95$) | 64.10 | 37.50% |
| 0.5fps + subs | RRAttention ($T=0.95$) | **67.70** | 37.40% |

> ğŸ‘‰ åœ¨è§†é¢‘è¿™ç±»éœ€è¦æ•æ‰å¤æ‚æ—¶ç©ºå…³ç³»çš„ä»»åŠ¡ä¸­ï¼Œ**å…¨å±€è¯„ä¼°èƒ½åŠ›å°¤ä¸ºé‡è¦**ï¼ŒRRAttention è¡¨ç°æ›´ä¼˜ã€‚

#### âœ… **è¿è¡Œæ—¶æ•ˆç‡ï¼ˆSpeedupï¼‰**

- åœ¨ **128K ä¸Šä¸‹æ–‡** ä¸‹ï¼š
  - RRAttention å®ç° **2.4Ã— ç«¯åˆ°ç«¯åŠ é€Ÿ**
  - æ¨¡å¼æœç´¢æ—¶é—´æ¯” XAttention å‡å°‘ **18.2%**
  - å³ä½¿åœ¨ 8K ä¸Šä¸‹æ–‡ä¹Ÿä¿æŒæ˜¾è‘—ä¼˜åŠ¿

> ğŸ’¡ å›¾ 3 æ˜¾ç¤ºï¼Œéšç€ä¸Šä¸‹æ–‡å¢é•¿ï¼ŒRRAttention çš„ pattern search å¼€é”€å æ¯”ç¨³å®šåœ¨ ~19%ï¼Œè¯æ˜å…¶å¯æ‰©å±•æ€§å¼ºã€‚

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

| å¯¹æ¯”ç»´åº¦ | RRAttention vs Baselines |
|----------|-------------------------|
| **å‡†ç¡®æ€§** | åœ¨æ‰€æœ‰æ¨¡å‹å’Œä»»åŠ¡ä¸Šå‡ä¼˜äº FlexPrefill å’Œ XAttentionï¼Œå°¤å…¶åœ¨ Recallã€LongQA ç­‰éœ€å…¨å±€ç†è§£çš„ä»»åŠ¡ä¸­é¢†å…ˆæ˜æ˜¾ |
| **ç¨€ç–åº¦-ç²¾åº¦æƒè¡¡** | åœ¨ç›¸åŒç¨€ç–åº¦ä¸‹ç²¾åº¦æ›´é«˜ï¼›åœ¨ç›¸åŒç²¾åº¦ä¸‹å¯è¾¾åˆ°æ›´é«˜ç¨€ç–åº¦ |
| **æ•ˆç‡** | æ¨¡å¼æœç´¢æ›´å¿«ï¼Œæ€»æ¨ç†å»¶è¿Ÿæ›´ä½ï¼Œé€‚åˆå®é™…éƒ¨ç½² |

### **æ¶ˆèå®éªŒç»“æœ**

#### ï¼ˆ1ï¼‰**æœ€åä¸€å—ä¿æŠ¤æœºåˆ¶ï¼ˆLast Query Block Protectionï¼‰**
- åŠ å…¥è¯¥æœºåˆ¶åï¼ŒXAttention æå‡æœ‰é™ï¼ˆ55.74 â†’ 55.92ï¼‰
- RRAttention æœ¬èº«å·²å…·å¤‡æ›´å¼º pattern å‘ç°èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥åŠ å…¥åå¯è¾¾ 56.24
- è¯´æ˜ **æœ‰æ•ˆçš„ pattern discovery æ¯”å•çº¯ä¿æŠ¤æ›´é‡è¦**

#### ï¼ˆ2ï¼‰**è½®è½¬ç­–ç•¥å¯¹æ¯”ï¼ˆHead-RR vs Layer-RR vs Hybrid-RRï¼‰**
- **Head-RR** è¡¨ç°æœ€ä¼˜ä¸”æœ€ç¨³å®š
- Layer-RR å’Œ Hybrid-RR åœ¨éƒ¨åˆ†é•¿åº¦ä¸‹é™
- è¯æ˜ **è·¨ head çš„å¤šæ ·åŒ–é‡‡æ ·æ˜¯å…³é”®è®¾è®¡**

#### ï¼ˆ3ï¼‰**æ­¥é•¿ï¼ˆStride Sizeï¼‰å½±å“**
- å½“ $S \leq 16$ æ—¶æ€§èƒ½ç¨³å®š
- $S=32$ æ—¶å› ç²’åº¦è¿‡ç²—å¯¼è‡´æ€§èƒ½ä¸‹é™
- æ¨èä½¿ç”¨ $S=8$ æˆ– $S=16$

#### ï¼ˆ4ï¼‰**å—é€‰æ‹©å‡†ç¡®æ€§åˆ†æï¼ˆBlock Selection Accuracyï¼‰**
- RRAttention åœ¨ **precision** ä¸Šç³»ç»Ÿæ€§é«˜äº XAttentionï¼ˆ+0.52% avgï¼‰
- Recall ç›¸å½“ï¼ˆä»…ä½ 0.3%ï¼‰
- F1 åˆ†æ•°å…¨é¢é¢†å…ˆï¼ˆ+0.25~+0.77 ptsï¼‰
- è¯´æ˜å…¶é‡è¦å—è¯†åˆ«æ›´ç²¾å‡†ï¼Œè¯¯æŠ¥æ›´å°‘

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **RRAttention æˆåŠŸæ‰“ç ´äº†åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›ä¸­çš„â€œä¸å¯èƒ½ä¸‰è§’â€**ï¼Œé¦–æ¬¡å®ç°äº†ï¼š
   - æ— éœ€é¢„å¤„ç†
   - æ”¯æŒå…¨å±€è¯„ä¼°
   - ä¿æŒæŸ¥è¯¢ç‹¬ç«‹æ€§
   - æ¨¡å¼æ— å…³
   - é«˜æ•ˆ stride-level è®¡ç®—  
   ğŸ‘‰ **äº”é¡¹ç†æƒ³ç‰¹æ€§å…¼å¤‡**

2. **Head Round-Robin é‡‡æ ·æ˜¯ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„æœºåˆ¶**ï¼š
   - ä¿è¯äº†æ¯ä¸ªä½ç½®éƒ½æœ‰æœºä¼šè¢«é‡‡æ ·
   - è‡ªç„¶åœ°ä¸ vertical/slash ç­‰å¸¸è§ attention pattern å¯¹é½
   - ä¸å¼•å…¥é¢å¤–å‚æ•°æˆ–è®­ç»ƒæˆæœ¬

3. **åœ¨å¤šç§ä»»åŠ¡å’Œæ¨¡å‹ä¸Šå…·æœ‰å¼ºæ³›åŒ–æ€§**ï¼š
   - åœ¨ LLaMAã€Qwenã€Yiã€Qwen3 ç­‰ä¸åŒæ¶æ„ä¸Šå‡è¡¨ç°é¢†å…ˆ
   - åœ¨æ–‡æœ¬ã€è§†é¢‘ç­‰å¤šæ¨¡æ€åœºæ™¯ä¸­å‡æœ‰æ•ˆ

4. **ç¨€ç–ä¸ç­‰äºæŸå¤±**ï¼š
   - RRAttention åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³ç•¥å¾®è¶…è¶Š Full Attentionï¼ˆå¦‚ Yi-9B ä¸Š 46.54 > 46.04ï¼‰ï¼Œè¡¨æ˜åˆç†çš„ç¨€ç–å¯èƒ½èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨ï¼Œè¿‡æ»¤å™ªå£°ä¸Šä¸‹æ–‡ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

- **æç«¯é…ç½®ä¸‹çš„è¦†ç›–é—®é¢˜**ï¼šå½“ stride size $S$ å¤§äº attention head æ•°é‡æ—¶ï¼Œæ— æ³•ä¿è¯æ¯ position éƒ½è¢«é‡‡æ ·ï¼Œå¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚
- **å½“å‰ä»…åº”ç”¨äº Prefill é˜¶æ®µ**ï¼šæœªæ¢ç´¢åœ¨ decoding é˜¶æ®µçš„åº”ç”¨ï¼Œé™åˆ¶äº†å¯¹ KV Cache çš„è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
- **ä¾èµ– stride/block åˆ’åˆ†**ï¼šè¶…å‚é€‰æ‹©ï¼ˆå¦‚ $S$, $B$ï¼‰ä¼šå½±å“æ€§èƒ½ï¼Œéœ€è°ƒä¼˜ã€‚

> âš ï¸ ä½†ä½œè€…æŒ‡å‡ºï¼Œè¿™äº›é—®é¢˜åœ¨å®è·µä¸­å¾ˆå°‘å‡ºç°ï¼Œæ¨èçš„ $S=8$ æˆ– $16$ å¯é¿å…ä¸Šè¿°è¾¹ç•Œæƒ…å†µã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **å·¥ç¨‹ä¼˜åŒ–**ï¼š
   - è¿ç§»åˆ° FlashAttention-3ï¼Œåˆ©ç”¨ warp specialization ç­‰ç‰¹æ€§è¿›ä¸€æ­¥æå‡ååã€‚

2. **è®­ç»ƒæ„ŸçŸ¥ç¨€ç–ï¼ˆTraining-Aware Sparse Attentionï¼‰**ï¼š
   - åœ¨è®­ç»ƒé˜¶æ®µå­¦ä¹ ç¨€ç–æ¨¡å¼ï¼ˆå¦‚é€šè¿‡ distillationï¼‰ï¼Œæ¶ˆé™¤æ¨ç†æ—¶ pattern search å¼€é”€ã€‚

3. **æ‰©å±•è‡³ Decoding é˜¶æ®µ**ï¼š
   - å°† RR æœºåˆ¶ç”¨äºç”Ÿæˆè¿‡ç¨‹ï¼Œå‡å°‘ KV Cache å†™å…¥å’Œè¯»å–å¸¦å®½ï¼Œé™ä½ per-token latencyã€‚

4. **ä¸å…¶ä»–æŠ€æœ¯ç»“åˆ**ï¼š
   - ä¸ PagedAttentionã€KV Cache Quantization ç­‰æ­£äº¤æŠ€æœ¯è”åˆä½¿ç”¨ï¼Œæ„å»ºå…¨æ ˆé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†ç³»ç»Ÿã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **RRAttention é€šè¿‡å·§å¦™çš„ Per-Head Round-Robin é‡‡æ ·ï¼Œåœ¨æ— éœ€é¢„å¤„ç†çš„å‰æä¸‹å®ç°äº†é«˜æ•ˆã€å‡†ç¡®ã€å…¨å±€ã€ç‹¬ç«‹çš„åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›ï¼Œæ˜¯ç›®å‰ç»¼åˆæ€§èƒ½æœ€å¼ºçš„ long-context inference æ–¹æ¡ˆä¹‹ä¸€ã€‚**

</details>

---

### 9. [DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs](https://arxiv.org/abs/2602.05992)

**Authors**: Lizhuo Luo, Shenggui Li, Yonggang Wen, Tianwei Zhang  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.05992v1  

#### Abstract
Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. How...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDSB: Dynamic Sliding Block Scheduling for Diffusion LLMs

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰åœ¨ **diffusion large language models (dLLMs)** ä¸­å¹¿æ³›ä½¿ç”¨çš„**å›ºå®šå—è°ƒåº¦ç­–ç•¥ï¼ˆnaive block schedulingï¼‰** å­˜åœ¨ä»¥ä¸‹å…³é”®ç¼ºé™·ï¼š
- **è¯­ä¹‰ä¸æ„ŸçŸ¥**ï¼šè°ƒåº¦ç­–ç•¥æ˜¯é¢„å®šä¹‰ä¸”é™æ€çš„ï¼Œå¿½ç•¥äº†ä¸åŒä½ç½®çš„è¯­ä¹‰éš¾åº¦å·®å¼‚ã€‚
- **æ¬¡ä¼˜æƒè¡¡**ï¼šå¼ºåˆ¶åœ¨ä½ç½®ä¿¡åº¦ä½ç½®æå‰è§£ç ï¼ˆpremature commitmentï¼‰ï¼ŒåŒæ—¶å»¶è¿Ÿé«˜ç½®ä¿¡åº¦ä½†ä½äºå—è¾¹ç•Œå¤–çš„ä½ç½®ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™å’Œå¹¶è¡Œæ•ˆç‡é™ä½ã€‚

è¿™ç§â€œä¸€åˆ€åˆ‡â€çš„æ–¹å¼é™åˆ¶äº† dLLMs åœ¨è´¨é‡å’Œæ¨ç†é€Ÿåº¦ä¹‹é—´çš„å¹³è¡¡ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡º **Dynamic Sliding Block (DSB)**ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„åŠ¨æ€å—è°ƒåº¦æœºåˆ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³åŒ…æ‹¬ï¼š

- **æ»‘åŠ¨ä¸”åŠ¨æ€å¤§å°çš„æ´»è·ƒå—ï¼ˆsliding block with dynamic sizeï¼‰**ï¼š
  - æ´»è·ƒå—çš„å·¦è¾¹ç•Œæ ¹æ®å·²è§£ç ä½ç½®åŠ¨æ€å‰ç§»ï¼›
  - å³è¾¹ç•Œæ ¹æ®æœªè§£ç  token æ•°é‡è‡ªé€‚åº”æ‰©å±•ï¼Œç¡®ä¿è‡³å°‘ä¿ç•™ä¸€å®šæ•°é‡å¾…å¤„ç† tokenï¼›
  - æœ€å¤§å—å°ºå¯¸å¯è®¾ä¸ºä¸Šé™ï¼ˆ`Smax`ï¼‰ï¼Œå®ç°å¯¹å› æœæ€§å’Œå¹¶è¡Œæ€§çš„çµæ´»æ§åˆ¶ã€‚

æ­¤å¤–ï¼Œä¸ºåº”å¯¹ DSB å¼•å…¥çš„ KV-cache ä¸ç¨³å®šæ€§ï¼Œæå‡ºäº†é…å¥—çš„ **DSB Cache** æœºåˆ¶ï¼š
- åœ¨æ´»è·ƒå—ä¹‹å‰ç»´æŠ¤ä¸€ä¸ª**å‰ç¼€çª—å£ï¼ˆprefix windowï¼‰**ï¼Œè¯¥çª—å£ä¸æ´»è·ƒå—ä¸€èµ·æ¯æ­¥åˆ·æ–°ï¼›
- ç¼“å­˜å…¶ä½™éæ´»è·ƒåŒºåŸŸçš„ KV çŠ¶æ€ä»¥å¤ç”¨ï¼›
- å®šæœŸæ‰§è¡Œå…¨å±€åˆ·æ–°ä»¥åŒæ­¥çŠ¶æ€ã€‚

> âœ… **å®Œå…¨ training-free**ï¼šä¸¤ç§æ–¹æ³•å‡æ— éœ€é¢å¤–è®­ç»ƒï¼Œé€‚ç”¨äºç°æˆæ¨¡å‹éƒ¨ç½²ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **è´¨é‡æå‡** | é¿å…è¿‡æ—©æäº¤ä¸ç¡®å®š tokenï¼Œå…è®¸æ›´å¯é çš„ä¸Šä¸‹æ–‡ç§¯ç´¯ï¼Œå‡å°‘é”™è¯¯ä¼ æ’­ã€‚ |
| **æ•ˆç‡æå‡** | æ›´åŠæ—¶åœ°é‡Šæ”¾é«˜ç½®ä¿¡åº¦ tokenï¼Œæé«˜æœ‰æ•ˆå¹¶è¡Œåº¦ï¼ŒåŠ å¿«æ”¶æ•›ã€‚ |
| **ç¼“å­˜å…¼å®¹æ€§** | DSB Cache æ˜¾è‘—ç¼“è§£å› å—ç§»åŠ¨å¸¦æ¥çš„ KV çŠ¶æ€ä¸ç¨³å®šé—®é¢˜ï¼Œé¿å…é¢‘ç¹å¤±æ•ˆå’Œé‡è®¡ç®—ã€‚ |
| **é€šç”¨æ€§å¼º** | æ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„æˆ–é‡æ–°è®­ç»ƒï¼Œå¯å³æ’å³ç”¨åˆ°å¤šç§ dLLM æ¶æ„ä¸­ã€‚ |

ç›¸æ¯”å¦‚ WeDLM ç­‰éœ€é€šè¿‡è®­ç»ƒå­¦ä¹ è°ƒåº¦çš„æ–¹æ³•ï¼ŒDSB å®ç°äº†**æ›´å¼ºçš„ semi-autoregressive æ¨ç†èŒƒå¼**ï¼ŒåŒæ—¶ä¿æŒé›¶è®­ç»ƒæˆæœ¬ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
æ¶µç›–å¤šä¸ªå…¸å‹ä»»åŠ¡é¢†åŸŸï¼Œå…±äº”ä¸ª benchmarkï¼š
- **GSM8K**ï¼ˆ5-shotï¼‰ï¼šæ•°å­¦æ¨ç†
- **MATH**ï¼ˆ4-shotï¼‰ï¼šå¤æ‚æ•°å­¦é—®é¢˜
- **HumanEval**ï¼ˆ0-shotï¼‰ï¼šä»£ç ç”Ÿæˆèƒ½åŠ›
- **MBPP**ï¼ˆ3-shotï¼‰ï¼šPython ç¼–ç¨‹ä»»åŠ¡
- **BBH**ï¼ˆ3-shotï¼‰ï¼šç»¼åˆæ¨ç†æŒ‘æˆ˜

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹**ï¼š
  - `LLaDA-8B-Instruct`, `LLaDA-1.5`
  - `Dream-v0-Base-7B`, `Dream-v0-Instruct-7B`
- **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA H200 140G GPU
- **ç”Ÿæˆé•¿åº¦**ï¼šç»Ÿä¸€è®¾ä¸º 256
- **åˆå§‹å—å¤§å° `Sinit`**ï¼š32
- **æœ€å¤§å—å¤§å° `Smax`**ï¼šåˆ†ä¸ºå¸¸é‡ï¼ˆDSB const.ï¼‰å’Œæ— ç•Œï¼ˆDSB greedyï¼‰
- **æœ€å°å‰ç¼€çª—å£é•¿åº¦ `lpmin`**ï¼šLLaDA ç³»åˆ—ä¸º 24ï¼ŒDream ç³»åˆ—ä¸º 4

#### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **Accuracy** | è¡¡é‡ç”Ÿæˆè´¨é‡ï¼ˆæ­£ç¡®ç‡ï¼‰ |
| **Tokens Per Second (TPS)** | è¡¡é‡æ¨ç†ååé‡ï¼Œåæ˜ æ•ˆç‡ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
ä»ä¸‰ä¸ªç»´åº¦è¿›è¡Œæ¯”è¾ƒï¼š

| ç±»åˆ« | åŸºçº¿æ–¹æ³• |
|-------|----------|
| **Decoding Strategy** | - Vanilla Top-1 Sampling<br>- Confidence-aware Parallel Decoding (from Fast-dLLM) |
| **Block Scheduling** | - Naive Block Schedulingï¼ˆå›ºå®šåˆ†å—ï¼‰ |
| **KV Caching** | - Dual Cacheï¼ˆç¼“å­˜æ‰€æœ‰éå½“å‰å—ä½ç½®ï¼Œå‘¨æœŸæ›´æ–°ï¼‰ |

ä¸»è¦å¯¹æ¯”ç»„åˆåŒ…æ‹¬ï¼š
- `Vanilla + None + Naive`
- `Confidence + None + Naive`
- `Confidence + Dual + Naive`
- `Confidence + DSB + DSB`

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ‘˜å½•è‡ª Table 1ï¼‰

#### åœ¨ LLaDA-8B-Instruct ä¸Šçš„è¡¨ç°ï¼ˆéƒ¨åˆ†ä»£è¡¨æ€§ç»“æœï¼‰ï¼š

| æ–¹æ³• | GSM8K Acc. | TPS | HumanEval Acc. | TPS |
|------|------------|-----|----------------|-----|
| Naive Block + Dual Cache | 77.40 | 92.26 | 37.80 | 100.6 |
| **DSB (const.) + DSB Cache** | **80.14** â†‘ | **98.10** â†‘ | **37.80** | **105.3** â†‘ |
| **DSB (greedy) + DSB Cache** | **80.29** â†‘ | **99.61** â†‘ | **39.63** â†‘ | **107.7** â†‘ |

> ğŸ”º åœ¨ GSM8K ä¸Šå‡†ç¡®ç‡æœ€é«˜è¾¾ **80.29**ï¼Œè¿œè¶… baseline çš„ 77.40ï¼›TPS æå‡æ˜¾è‘—ã€‚

#### åœ¨ Dream-v0-Instruct-7B ä¸Šçš„è¡¨ç°ï¼š

| æ–¹æ³• | GSM8K Acc. | TPS |
|------|------------|-----|
| Naive Block + Dual Cache | 67.32 | 72.18 |
| **DSB (greedy) + DSB Cache** | **73.08** â†‘ | **75.27** â†‘ |

> å³ä½¿åœ¨ AR-initialized çš„ Dream æ¨¡å‹ä¸Šï¼ŒDSB ä»å¸¦æ¥æ˜æ˜¾å¢ç›Šã€‚

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **åœ¨å‡ ä¹æ‰€æœ‰æ¨¡å‹å’Œä»»åŠ¡ä¸Šï¼ŒDSB + DSB Cache å‡ä¼˜äºæ‰€æœ‰ baseline**ï¼›
- ç›¸æ¯” naive block è°ƒåº¦ï¼š
  - å¹³å‡æå‡ **~2â€“3% å‡†ç¡®ç‡**
  - ååé‡ï¼ˆTPSï¼‰æå‡ **5â€“15%**
- ç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¼ºé€»è¾‘è¿è´¯æ€§çš„ä»»åŠ¡ï¼ˆå¦‚ GSM8Kã€MATHï¼‰ä¸­ï¼Œè´¨é‡å¢ç›Šæ›´ä¸ºæ˜¾è‘—ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰DSB Cache çš„æœ‰æ•ˆæ€§ï¼ˆTable 2ï¼‰
å°† DSB Cache æ›¿æ¢ä¸ºæ™®é€š Dual Cache å¯¼è‡´ä¸¥é‡é€€åŒ–ï¼š

| æ–¹æ³• | GSM8K Acc. | TPS |
|------|------------|-----|
| DSB (const.) + Dual Cache | 76.42 â†“ | 78.93 â†“ |
| **DSB (const.) + DSB Cache** | **80.14** â†‘ | **98.10** â†‘ |

> â—è¯´æ˜ï¼š**å‰ç¼€çª—å£åˆ·æ–°æœºåˆ¶è‡³å…³é‡è¦**ï¼Œå¦åˆ™è¾¹ç•Œ KV çŠ¶æ€ä¸ç¨³å®šä¼šç ´åç”Ÿæˆè´¨é‡ã€‚

#### ï¼ˆ2ï¼‰åˆå§‹å—é•¿åº¦ `Sinit` å½±å“ï¼ˆFigure 4ï¼‰
- DSB å¯¹ `Sinit` æ›´é²æ£’ï¼›
- è¿‡å¤§çš„ `Sinit`ï¼ˆå¦‚ 64ï¼‰å‰Šå¼±å› æœçº¦æŸï¼Œè½»å¾®å½±å“ç²¾åº¦ï¼›
- DSB åœ¨å„ç§è®¾ç½®ä¸‹å§‹ç»ˆä¼˜äº naive blockã€‚

#### ï¼ˆ3ï¼‰ç”Ÿæˆé•¿åº¦å½±å“ï¼ˆFigure 5ï¼‰
- éšç€ç”Ÿæˆé•¿åº¦å¢åŠ ï¼ˆ128 â†’ 1024ï¼‰ï¼ŒDSB ä¾ç„¶ç»´æŒé«˜è´¨é‡ä¸é«˜ TPSï¼›
- è¡¨ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

#### ï¼ˆ4ï¼‰æœ€å¤§å—é•¿åº¦ `Smax` ä¸æœ€å°çª—å£é•¿åº¦ `lpmin`ï¼ˆFigures 6 & 7ï¼‰
- `Smax` å¢å¤§ä¼šæå‡ TPS ä½†å¯èƒ½ç‰ºç‰²å‡†ç¡®æ€§ï¼ˆå°¤å…¶åœ¨ LLaDA ä¸Šï¼‰ï¼›
- `lpmin` å­˜åœ¨æœ€ä¼˜å€¼ï¼ˆLLaDA: 24, Dream: 4ï¼‰ï¼Œè¿‡å¤§åè€Œé™ä½æ•ˆç‡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å›ºå®šå—è°ƒåº¦æ˜¯ç“¶é¢ˆ**ï¼šå…¶æ— è§†è¯­ä¹‰éš¾åº¦ï¼Œé€ æˆè´¨é‡ä¸æ•ˆç‡åŒè¾“ã€‚
2. **DSB å®ç°äº†æ›´æ™ºèƒ½çš„ semi-autoregressive æ¨ç†**ï¼š
   - åŠ¨æ€è°ƒæ•´å—èŒƒå›´ï¼Œè®©é«˜ç½®ä¿¡ token å°½æ—©é‡Šæ”¾ï¼Œä½ç½®ä¿¡ token å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡ã€‚
3. **DSB Cache æ˜¯é«˜æ•ˆæ¨ç†çš„å…³é”®æ”¯æ’‘**ï¼š
   - é€šè¿‡ prefix window è®¾è®¡è§£å†³äº†æ»‘åŠ¨è¿‡ç¨‹ä¸­ KV çŠ¶æ€ä¸ç¨³å®šçš„é—®é¢˜ã€‚
4. **æ•´ä½“æ–¹æ¡ˆæ˜¾è‘—æå‡äº† quality-speed trade-off**ï¼š
   - åœ¨å¤šä¸ªæ¨¡å‹å’Œä»»åŠ¡ä¸Šä¸€è‡´å–å¾— SOTA çº§æ”¹è¿›ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–ç½®ä¿¡åº¦ä¼°è®¡**ï¼šDSB çš„æ•ˆæœå—é™äºæ¨¡å‹è¾“å‡ºçš„ confidence å¯é æ€§ï¼›
- **æç«¯é•¿æ–‡æœ¬åœºæ™¯æœªå……åˆ†éªŒè¯**ï¼šè™½ç„¶ Figure 5 æ˜¾ç¤ºè‰¯å¥½æ‰©å±•æ€§ï¼Œä½†åœ¨ >1024 token åœºæ™¯ä¸‹çš„è¡¨ç°æœ‰å¾…è¿›ä¸€æ­¥æµ‹è¯•ï¼›
- **å¯¹æŸäº›æ¨¡å‹å¢ç›Šè¾ƒå°**ï¼šä¾‹å¦‚ Dream ç³»åˆ—ç”±äºé‡‡ç”¨ AR åˆå§‹åŒ–è®­ç»ƒï¼Œæå‡ä¸å¦‚ LLaDA æ˜æ˜¾ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
- å°† DSB æ€è·¯èå…¥ **pretraining æˆ– post-training** é˜¶æ®µï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°é€‚é…åŠ¨æ€è°ƒåº¦ï¼›
- ç»“åˆå…¶ä»–åŠ é€ŸæŠ€æœ¯ï¼ˆå¦‚ early stoppingã€distillationï¼‰æ„å»ºç«¯åˆ°ç«¯ä¼˜åŒ– pipelineï¼›
- æ¢ç´¢åŸºäº learned confidence æˆ– uncertainty calibration çš„å¢å¼ºç‰ˆ DSBï¼›
- æ‰©å±•è‡³å¤šæ¨¡æ€ diffusion æ¨¡å‹ä¸­çš„è·¨æ¨¡æ€å—è°ƒåº¦ã€‚

---

> ğŸ“¦ **å¼€æºä¿¡æ¯**ï¼šä»£ç å·²å‘å¸ƒäº GitHub â†’ [https://github.com/lizhuo-luo/DSB](https://github.com/lizhuo-luo/DSB)

</details>

---

### 10. [Stochastic hierarchical data-driven optimization: application to plasma-surface kinetics](https://arxiv.org/abs/2602.04975)

**Authors**: Jos\'e Afonso, Vasco Guerra, Pedro Viegas  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.04975v1  

#### Abstract
This work introduces a stochastic hierarchical optimization framework inspired by Sloppy Model theory for the efficient calibration of physical models. Central to this method is the use of a reduced Hessian approximation, which identifies and targets the stiff parameter subspace using minimal simula...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Stochastic Hierarchical Data-Driven Optimization: Application to Plasma-Surface Kinetics*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

è¯¥è®ºæ–‡é’ˆå¯¹**å¤æ‚ç‰©ç†ç³»ç»Ÿå»ºæ¨¡ä¸­çš„é«˜ç»´ã€ç—…æ€ï¼ˆill-conditionedï¼‰é€†é—®é¢˜ä¼˜åŒ–æŒ‘æˆ˜**ï¼Œç‰¹åˆ«æ˜¯åœ¨**è®¡ç®—ä»£ä»·é«˜æ˜‚çš„ä»¿çœŸå™¨**ï¼ˆsimulatorï¼‰ç¯å¢ƒä¸‹ï¼Œå¦‚ä½•åœ¨æœ‰é™çš„æ•°æ®å’Œæ¨¡æ‹Ÿè°ƒç”¨æ¬¡æ•°ä¸‹é«˜æ•ˆåœ°æ ¡å‡†æ¨¡å‹å‚æ•°ã€‚

å…·ä½“åº”ç”¨åœºæ™¯æ˜¯**ç­‰ç¦»å­ä½“-è¡¨é¢ç›¸äº’ä½œç”¨**ï¼ˆplasma-surface interactionsï¼‰ï¼Œå…¶ä¸­è¡¨é¢ååº”å‚æ•°ï¼ˆå¦‚å¸é™„ç³»æ•°ã€èƒ½å’ï¼‰é«˜åº¦ä¸ç¡®å®šï¼Œä¸”åŠ¨åŠ›å­¦æ¨¡æ‹Ÿéå¸¸è€—æ—¶ï¼Œä½¿å¾—ä¼ ç»Ÿçš„å…¨å±€æœç´¢æ–¹æ³•ï¼ˆå¦‚DEã€CMA-ESï¼‰æ•ˆç‡ä½ä¸‹ç”šè‡³ä¸å¯è¡Œã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æå‡ºäº†ä¸€ç§**å—â€œSloppy Modelâ€ç†è®ºå¯å‘çš„éšæœºåˆ†å±‚ä¼˜åŒ–æ¡†æ¶**ï¼ˆStochastic Hierarchical Optimization Frameworkï¼‰ï¼Œå…¶æ ¸å¿ƒæ€æƒ³åŒ…æ‹¬ï¼š

- **å‡ ä½•æ„ŸçŸ¥çš„å‚æ•°ç©ºé—´åˆ†è§£**ï¼šåŸºäº Gauss-Newton Hessian çš„ç‰¹å¾å€¼è°±åˆ†æï¼Œå°†å‚æ•°ç©ºé—´åˆ’åˆ†ä¸º**åˆšæ€§å­ç©ºé—´**ï¼ˆstiff subspaceï¼‰å’Œ**æ¾æ•£å­ç©ºé—´**ï¼ˆsloppy subspaceï¼‰ã€‚å‰è€…ä¸»å¯¼æ¨¡å‹è¡Œä¸ºï¼Œåè€…å¯¹è¾“å‡ºå½±å“å¾®å¼±ã€‚
- **åˆ†å±‚ä¼˜åŒ–ç­–ç•¥**ï¼š
  - é¦–å…ˆåœ¨åˆšæ€§å­ç©ºé—´è¿›è¡Œä¼˜åŒ–ï¼Œå¿«é€Ÿæ”¶æ•›åˆ°ä½èƒ½é‡æµå½¢ï¼ˆvalley floorï¼‰ï¼›
  - å†åœ¨æ¾æ•£å­ç©ºé—´è¿›è¡Œç²¾ç»†åŒ–è°ƒæ•´ã€‚
- **éšæœºä½ç§© Hessian è¿‘ä¼¼**ï¼ˆReduced Hessian Proxyï¼‰ï¼š
  - ä¸ç›´æ¥è®¡ç®—å®Œæ•´çš„ $ n \times n $ Hessian çŸ©é˜µï¼ˆéœ€ $ n+1 $ æ¬¡æ¨¡æ‹Ÿï¼‰ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªéšæœºä½ç»´å­ç©ºé—´ $ \Omega \in \mathbb{R}^{n \times k} $ï¼ˆ$ k \ll n $ï¼‰æ¥éšå¼ä¼°è®¡ä¸»å¯¼æ›²ç‡ã€‚
  - æ„é€ å°çŸ©é˜µ $ H_{\text{small}} = \Omega^T H_{\text{GN}} \Omega $ï¼Œä»…éœ€ $ k+1 $ æ¬¡æ¨¡æ‹Ÿå³å¯æå–åˆšæ€§æ–¹å‘ï¼Œæå¤§æå‡æ ·æœ¬æ•ˆç‡ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ä¼˜åŠ¿ç»´åº¦ | è¯´æ˜ |
|--------|------|
| **æ ·æœ¬æ•ˆç‡**ï¼ˆSample Efficiencyï¼‰ | æ˜¾è‘—å‡å°‘æ‰€éœ€çš„æ¨¡æ‹Ÿè°ƒç”¨æ¬¡æ•°ï¼Œåœ¨æ—©æœŸè¿­ä»£ä¸­å¿«é€Ÿä¸‹é™æŸå¤±å‡½æ•°ï¼Œå°¤å…¶é€‚ç”¨äºæ˜‚è´µä»¿çœŸå™¨ã€‚ |
| **æ— éœ€ä»£ç†æ¨¡å‹**ï¼ˆNo Surrogate Neededï¼‰ | ç›´æ¥æŸ¥è¯¢çœŸå®ä»¿çœŸå™¨ï¼Œé¿å…äº†é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ç­‰ä»£ç†æ¨¡å‹å¯èƒ½å¼•å…¥çš„è™šå‡æå°å€¼é—®é¢˜ã€‚ |
| **å¯æ‰©å±•æ€§**ï¼ˆScalabilityï¼‰ | éšæœº Hessian æ–¹æ³•å°†è®¡ç®—æˆæœ¬ä» $ O(n) $ é™è‡³ $ O(k) $ï¼Œé€‚ç”¨äºé«˜ç»´å‚æ•°ç©ºé—´ã€‚ |
| **å‡ ä½•é²æ£’æ€§** | ä¸»åŠ¨åˆ©ç”¨ç›®æ ‡å‡½æ•°çš„å„å‘å¼‚æ€§ç»“æ„ï¼ˆanisotropic landscapeï¼‰ï¼Œæœ‰æ•ˆåº”å¯¹ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å¤„ç†çš„â€œç»†é•¿å±±è°·â€é—®é¢˜ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- **å®éªŒæ•°æ®æ¥æº**ï¼šæ¥è‡ªæ–‡çŒ® [18â€“20] åŠæœªå‘è¡¨æµ‹é‡ï¼ˆç”± LPP æä¾›ï¼‰ã€‚
- **æ•°æ®è§„æ¨¡**ï¼šå…± **225 ä¸ªç¨³æ€å®éªŒæ¡ä»¶**ã€‚
- **å˜é‡èŒƒå›´**ï¼š
  - æ°”å‹ï¼š0.2 â€“ 10 Torr
  - æ”¾ç”µç”µæµï¼š10 â€“ 40 mA
  - å£æ¸©ï¼š-20Â°C â€“ 50Â°C
  - æ°”ä½“æ··åˆç‰©ï¼šOâ‚‚ / COâ‚‚ï¼Œæ€»æµé‡ 7.4 sccm
- **è§‚æµ‹é‡**ï¼ˆMacroscopic Observableï¼‰ï¼šåŸå­æ°§çš„æœ‰æ•ˆå¤åˆæ¦‚ç‡ $ y_o $

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### **ä¼˜åŒ–ä»»åŠ¡**
- ä¼˜åŒ– **29 ä¸ªä¸ç¡®å®šæ€§æœ€é«˜çš„åŠ¨åŠ›å­¦å‚æ•°**ï¼ŒåŒ…æ‹¬ï¼š
  - èƒ½å’ $ E_a $
  - æŒ‡å‰å› å­ $ k_0 $
  - ç‰©ç†å¸é™„ç‰©ç§çš„è„±é™„é¢‘ç‡å‚æ•°ï¼ˆA, B, Eï¼‰
- å‚æ•°å½’ä¸€åŒ–è‡³ [0,1] åŒºé—´ã€‚

#### **è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†**
- æ•°æ®é›†æŒ‰ **80%/20% åˆ†å‰²**ï¼ˆ180 è®­ç»ƒ / 45 æµ‹è¯•ï¼‰ï¼Œé‡‡ç”¨åŸºäºå‹åŠ›-æ¸©åº¦åˆ†å¸ƒçš„**åˆ†å±‚éšæœºæŠ½æ ·**ï¼ˆstratified samplingï¼‰ç¡®ä¿è¦†ç›–å‡åŒ€ã€‚
- æ‰€æœ‰ä¼˜åŒ–ä»…ä½¿ç”¨è®­ç»ƒé›†ï¼Œæµ‹è¯•é›†ç”¨äºéªŒè¯æ³›åŒ–èƒ½åŠ›ã€‚

#### **è¯„ä¼°æŒ‡æ ‡**
- **è®­ç»ƒæŸå¤±** $ \mathcal{L}_{\text{train}} $ å’Œ **æµ‹è¯•æŸå¤±** $ \mathcal{L}_{\text{test}} $ï¼šåŸºäºåŠ æƒæ®‹å·®å¹³æ–¹å’Œï¼ˆEq. 2ï¼‰ã€‚
- **æ ·æœ¬æ•ˆç‡**ï¼šä»¥**ç´¯è®¡æ¨¡æ‹Ÿè°ƒç”¨æ¬¡æ•°**ï¼ˆcumulative simulator callsï¼‰ä¸ºæ¨ªè½´ï¼Œè¡¡é‡æ”¶æ•›é€Ÿåº¦ã€‚
- **æ³›åŒ–èƒ½åŠ›**ï¼šæ¯”è¾ƒè®­ç»ƒä¸æµ‹è¯•æŸå¤±è½¨è¿¹æ˜¯å¦ä¸€è‡´ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- **å‚æ•°ç¨³å®šæ€§**ï¼šé€šè¿‡ 5 æŠ˜äº¤å‰éªŒè¯è¯„ä¼°ä¼˜åŒ–å‚æ•°çš„ä¸€è‡´æ€§ã€‚

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

é€‰å–äº”ç±»ä»£è¡¨æ€§ç®—æ³•ä½œä¸º baselineï¼š

| æ–¹æ³• | ç±»å‹ | è¯´æ˜ |
|-----|------|------|
| **DE** (Differential Evolution) | å…¨å±€æ¢ç´¢ | ç§ç¾¤å‹å¯å‘å¼ï¼Œæ“…é•¿å…¨å±€æœç´¢ä½†å¯¹ç—…æ€åœ°å½¢æ•æ„Ÿ |
| **CMA-ES** | è‡ªé€‚åº”æ¼”åŒ– | å­¦ä¹ åæ–¹å·®çŸ©é˜µä»¥é€‚åº”åœ°å½¢ï¼Œé€‚åˆéçƒå½¢è°· |
| **TRF** (Trust Region Reflective) | å±€éƒ¨ä¼˜åŒ– | Levenberg-Marquardt å˜ä½“ï¼Œä½¿ç”¨æœ‰é™å·®åˆ†ä¼°è®¡é›…å¯æ¯” |
| **Powellâ€™s Method** | æ— æ¢¯åº¦å±€éƒ¨ | æ²¿å…±è½­æ–¹å‘çº¿æœç´¢ï¼Œä¸ä¾èµ–æ¢¯åº¦ |
| **GP** (Gaussian Process) | ä»£ç†æ¨¡å‹ | ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æ„å»ºå“åº”é¢ä»£ç† |

æ­¤å¤–ï¼Œæœ¬æ–‡æ–¹æ³•åŒ…å«ä¸¤ä¸ªå˜ä½“ï¼š
- **Hierarchical Exact**ï¼šä½¿ç”¨å®Œæ•´ Hessian
- **Hierarchical Stochastic**ï¼šä½¿ç”¨éšæœºä½ç§© Hessianï¼ˆ$ k=3,5,10,18 $ï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| æœ€ç»ˆæµ‹è¯•æŸå¤± $ \mathcal{L}_{\text{test}} $ | ä¸ TRF ç›¸å½“ï¼Œçº¦ **0.054 â€“ 0.087**ï¼ˆ5æ¬¡ç‹¬ç«‹è¿è¡Œï¼‰ |
| å…¨å±€é¢„æµ‹ç›¸å…³æ€§ $ R^2 $ | è¾¾åˆ° **0.736**ï¼ˆå›¾3aï¼‰ |
| æ”¶æ•›æ‰€éœ€æœ€å¤§è¿­ä»£æ•° | è®¾å®šä¸º 50 æ¬¡ï¼ˆæ¯æ¬¡å¯¹åº”è‹¥å¹²æ¨¡æ‹Ÿè°ƒç”¨ï¼‰ |
| éšæœºå­ç©ºé—´å¤§å° $ k $ | å³ä½¿ $ k=3 $ æˆ– $ 5 $ ä¹Ÿèƒ½å®ç°å¿«é€Ÿæ”¶æ•› |

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

#### âœ… **æ ·æœ¬æ•ˆç‡æ˜¾è‘—ä¼˜äºå¤šæ•°åŸºçº¿**
- åœ¨**ç›¸åŒæ¨¡æ‹Ÿè°ƒç”¨æ¬¡æ•°ä¸‹**ï¼ŒHierarchical æ–¹æ³•ï¼ˆå°¤å…¶æ˜¯ Stochastic ç‰ˆæœ¬ï¼‰**è®­ç»ƒæŸå¤±ä¸‹é™æœ€å¿«**ï¼ˆå›¾2aï¼‰ã€‚
- **è¿œè¶… DEã€CMA-ESã€Powell å’Œ GP**ï¼Œä»… TRF åœ¨åæœŸæ¥è¿‘å…¶æ€§èƒ½ï¼Œä½†å‰æœŸæ”¶æ•›æ›´æ…¢ã€‚

#### âœ… **éšæœº Hessian ç­–ç•¥é«˜æ•ˆä¸”é²æ£’**
- å›¾2(b) æ˜¾ç¤ºï¼šå³ä½¿ä½¿ç”¨æå°çš„ $ k=3 $ æˆ– $ 5 $ï¼Œ**Stochastic æ–¹æ³•åœ¨å‰ 20 æ¬¡è¿­ä»£ä¸­è¡¨ç°ä¼˜äº Exact æ–¹æ³•**ã€‚
- è¡¨æ˜ï¼š**ä¸»å¯¼åˆšæ€§æ–¹å‘å¯é€šè¿‡æä½ç§©è¿‘ä¼¼å‡†ç¡®æ•æ‰**ï¼ŒéªŒè¯äº†æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚

#### âœ… **æ³›åŒ–èƒ½åŠ›å¼ºï¼Œæ— è¿‡æ‹Ÿåˆ**
- å›¾2(c) æ˜¾ç¤ºï¼šHierarchical ä¸ TRF çš„è®­ç»ƒ/æµ‹è¯•æŸå¤±æ›²çº¿é«˜åº¦é‡åˆï¼Œè¡¨æ˜ä¼˜åŒ–ç»“æœå…·æœ‰è‰¯å¥½çš„å¤–æ¨èƒ½åŠ›ã€‚
- å¯¹æ¯”ä¹‹ä¸‹ï¼ŒæŸäº›åŸºçº¿å¯èƒ½å‡ºç°è®­ç»ƒ/æµ‹è¯•åˆ†ç¦»è¶‹åŠ¿ã€‚

#### âœ… **å‚æ•°ä¼°è®¡ç¨³å®šå¯é **
- 5 æ¬¡ç‹¬ç«‹è¿è¡Œå¾—åˆ°çš„ä¼˜åŒ–å‚æ•°å…·æœ‰ä¸€è‡´ä¸­ä½æ•°ï¼Œä¸” Hessian å¯¼å‡ºçš„ä¸ç¡®å®šæ€§åŒºé—´åˆç†ï¼ˆå›¾4ï¼‰ã€‚
- æˆåŠŸè¯†åˆ«å‡ºçœŸæ­£â€œåˆšæ€§â€çš„ç‰©ç†å‚æ•°ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**

è™½ç„¶æœªæ˜ç¡®æ ‡æ³¨â€œablation studyâ€ï¼Œä½†ä»¥ä¸‹è®¾è®¡æ„æˆå®è´¨ä¸Šçš„æ¶ˆèåˆ†æï¼š

| æ¯”è¾ƒé¡¹ | å‘ç° |
|-------|------|
| **Exact vs. Stochastic Hessian** | Stochastic ç‰ˆæœ¬åœ¨æ›´å°‘æ¨¡æ‹Ÿè°ƒç”¨ä¸‹è¾¾åˆ°åŒç­‰ç”šè‡³æ›´å¥½åˆæœŸæ€§èƒ½ï¼Œè¯æ˜ä½ç§©è¿‘ä¼¼æœ‰æ•ˆã€‚ |
| **ä¸åŒ $ k $ å€¼çš„å½±å“**ï¼ˆ$ k=3,5,10,18 $ï¼‰ | å° $ k $ å·²è¶³å¤Ÿæ•æ‰ä¸»å¯¼æ›²ç‡ï¼Œæ”¯æŒæ–¹æ³•åœ¨æ›´é«˜ç»´é—®é¢˜ä¸­çš„åº”ç”¨ã€‚ |
| **Hessian ç‰¹å¾è°±åˆ†æ**ï¼ˆå›¾2dï¼‰ | ç‰¹å¾å€¼å‘ˆæŒ‡æ•°è¡°å‡ï¼Œè¯å®æ¨¡å‹å…·æœ‰å…¸å‹â€œSloppyâ€ç»“æ„ï¼Œä¸ºåˆ†å±‚ä¼˜åŒ–æä¾›ç†è®ºä¾æ®ã€‚ |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **è®ºæ–‡çš„ä¸»è¦å‘ç°**

1. **ç‰©ç†æ¨¡å‹å¤©ç„¶å­˜åœ¨â€œSloppyâ€ç»“æ„**ï¼š
   - Hessian ç‰¹å¾å€¼è°±æ˜¾ç¤ºæ˜æ˜¾çš„åˆšæ€§-æ¾æ•£åˆ†ç¦»ï¼ˆå›¾2dï¼‰ï¼Œæ„å‘³ç€åªéœ€å°‘æ•°ç»„åˆå‚æ•°å³å¯å†³å®šç³»ç»Ÿè¡Œä¸ºã€‚

2. **åˆšæ€§å‚æ•°å¯è¢«å®éªŒæ•°æ®æœ‰æ•ˆçº¦æŸ**ï¼š
   - é€šè¿‡ Hessian ä¸ç¡®å®šæ€§åˆ†æï¼Œç¡®è®¤ä»¥ä¸‹å‚æ•°ä¸ºâ€œåˆšæ€§â€ï¼š
     - **è„±é™„é¢‘ç‡å‚æ•°**ï¼ˆA, B, Eï¼‰â€”â€”æ§åˆ¶ OF, Oâ‚‚F, COF è„±é™„
     - **CO åŒ–å­¦å¸é™„ç›¸å…³çš„æŒ‡å‰å› å­**ï¼ˆå¦‚ $ k_{0.32}, k_{0.37}, k_{0.39} $ï¼‰
     - **äºšç¨³æ€ç”Ÿæˆ/ç ´åçš„èƒ½é‡**ï¼ˆ$ E_c, E_{\min} $ï¼‰
   - å…¶ä½™å‚æ•°ï¼ˆå¦‚éƒ¨åˆ†ç‰©ç†å¸é™„ååº”ï¼‰åˆ™é«˜åº¦ä¸ç¡®å®šï¼Œå±ç»“æ„æ€§â€œæ¾æ•£â€ã€‚

3. **ä¼˜åŒ–æ–¹æ³•å…¼å…·é«˜æ•ˆæ€§ä¸ç‰©ç†å¯è§£é‡Šæ€§**ï¼š
   - ä¸ä»…æå‡äº†æ ¡å‡†æ•ˆç‡ï¼Œè¿˜æä¾›äº†**åŸºäºå‡ ä½•çš„å‚æ•°é‡è¦æ€§æ’åº**ï¼Œè¾…åŠ©ç‰©ç†æœºåˆ¶ç†è§£ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

| å±€é™æ€§ | è¯´æ˜ |
|--------|------|
| **å±€éƒ¨æœ€ä¼˜é£é™©** | æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯å±€éƒ¨ä¼˜åŒ–ï¼Œä¸èƒ½ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜ $ \theta_{\text{global}} $ã€‚ä½†åœ¨ Sloppy æ¨¡å‹ä¸­ï¼Œè¿‘ä¼˜è§£åŒºåŸŸ $ S_\epsilon $ é€šå¸¸å¾ˆå¤§ï¼Œå®é™…å½±å“è¾ƒå°ã€‚ |
| **ä¾èµ–åˆå§‹çŒœæµ‹** | æ€§èƒ½å—åˆå§‹å‚æ•° $ \theta^{(0)} $ å½±å“ï¼Œè‹¥è¿œç¦»åˆç†åŒºåŸŸå¯èƒ½é™·å…¥ä¸è‰¯å±€éƒ¨æå°ã€‚ |
| **Hessian æ­£åˆ™åŒ–éœ€æ±‚** | æ•°å€¼ä¸ç¨³å®šæ—¶éœ€å¼•å…¥ Tikhonov æ­£åˆ™åŒ–ï¼ˆ$ \lambda_{\text{reg}} $ï¼‰ï¼Œå¢åŠ è°ƒå‚è´Ÿæ‹…ã€‚ |
| **å®ç°å¤æ‚åº¦è¾ƒé«˜** | ç›¸æ¯”æ ‡å‡†ä¼˜åŒ–å™¨ï¼Œéœ€è‡ªè¡Œå®ç°åˆ†å±‚é€»è¾‘ä¸å‡ ä½•æ›´æ–°æœºåˆ¶ã€‚ |

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **æ·±å…¥ç‰©ç†æœºåˆ¶ç ”ç©¶**ï¼š
   - åˆ©ç”¨ä¼˜åŒ–åçš„æ¨¡å‹è¿›ä¸€æ­¥åˆ†æåŸå­æ°§åœ¨ Pyrex è¡¨é¢çš„å¤åˆæœºç†ï¼Œç‰¹åˆ«æ˜¯ CO å¯¹æ´»æ€§ä½ç‚¹çš„é’åŒ–æ•ˆåº”ã€‚

2. **å¤–æ¨èƒ½åŠ›è¯„ä¼°**ï¼š
   - æ¢ç´¢æ¨¡å‹åœ¨è®­ç»ƒåŸŸä¹‹å¤–çš„æ“ä½œæ¡ä»¶ä¸‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œæ£€éªŒå…¶å¤–æ¨è¾¹ç•Œã€‚

3. **æ–¹æ³•é€šç”¨åŒ–ä¸è‡ªåŠ¨åŒ–**ï¼š
   - å°†è¯¥æ¡†æ¶æ¨å¹¿è‡³å…¶ä»–å¤æ‚ååº”ç½‘ç»œï¼ˆå¦‚ç”ŸåŒ–ç³»ç»Ÿã€ç‡ƒçƒ§åŒ–å­¦ï¼‰ï¼Œå‘å±•è‡ªåŠ¨è¯†åˆ«åˆšæ€§å­ç©ºé—´çš„å·¥å…·é“¾ã€‚

4. **ç»“åˆä¸»åŠ¨å­¦ä¹ **ï¼š
   - è”åˆå®éªŒè®¾è®¡ï¼ˆactive learningï¼‰ï¼ŒæŒ‡å¯¼ä¸‹ä¸€æ­¥æœ€ä¿¡æ¯ä¸°å¯Œçš„å®éªŒæµ‹é‡ï¼Œæœ€å¤§åŒ–å‚æ•°çº¦æŸåŠ›ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡æå‡ºäº†ä¸€ç§**å‡ ä½•æ„ŸçŸ¥ã€æ ·æœ¬é«˜æ•ˆçš„åˆ†å±‚ä¼˜åŒ–æ¡†æ¶**ï¼Œé€šè¿‡**éšæœºä½ç§© Hessian è¿‘ä¼¼**ç²¾å‡†å®šä½ Sloppy æ¨¡å‹ä¸­çš„åˆšæ€§æ–¹å‘ï¼Œåœ¨**ç­‰ç¦»å­ä½“è¡¨é¢åŠ¨åŠ›å­¦å‚æ•°æ ¡å‡†**ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ä»…æå‡äº†ä¼˜åŒ–æ•ˆç‡ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸ç‰©ç†ä¸€è‡´æ€§ã€‚

</details>

---

### 11. [Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance](https://arxiv.org/abs/2602.05774)

**Authors**: Xiandong Zou, Jianshu Li, Jing Huang, Pan Zhou  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.05774v1  

#### Abstract
Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šVariational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
æœ¬æ–‡é’ˆå¯¹ **speculative decoding** ä¸­å­˜åœ¨çš„ **training-decoding distributional discrepancy**ï¼ˆè®­ç»ƒ-è§£ç åˆ†å¸ƒä¸ä¸€è‡´ï¼‰é—®é¢˜æå‡ºè§£å†³æ–¹æ¡ˆã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒ draft model æ—¶é€šå¸¸é‡‡ç”¨ **token-level cross-entropy loss**ï¼Œä¼˜åŒ–å•ä¸€è´ªå©ªè·¯å¾„çš„ token é¢„æµ‹å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨å®é™…æ¨ç†ä¸­ï¼Œdecoding è¿‡ç¨‹ä¾èµ–äº **multi-path sampling** å’Œ **path-level ranking** æ¥é€‰æ‹©å¯è¢« target model æ¥å—çš„ draft è·¯å¾„ã€‚

è¿™ç§â€œè®­ç»ƒçœ‹ tokenï¼Œè§£ç çœ‹è·¯å¾„â€çš„é”™é…å¯¼è‡´ï¼š
- è®­ç»ƒç›®æ ‡ä¸å®é™…åŠ é€Ÿç›®æ ‡è„±èŠ‚ï¼›
- draft model å­¦åˆ°çš„åˆ†å¸ƒæ— æ³•æœ‰æ•ˆæ”¯æŒå¤šè·¯å¾„éªŒè¯æœºåˆ¶ï¼›
- æœ€ç»ˆé™åˆ¶äº† speculative decoding å¯å®ç°çš„ **speedup** æ•ˆæœã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä½œè€…æå‡ºäº† **Variational Speculative Decoding (VSD)**ï¼Œå°† draft model çš„è®­ç»ƒé‡æ–°å»ºæ¨¡ä¸ºä¸€ä¸ª **variational inference** é—®é¢˜ï¼š

- å°† draft path è§†ä¸º **latent proposal**ï¼›
- ç›®æ ‡æ˜¯æœ€å¤§åŒ– target model å¯¹ draft path çš„ **acceptance probability**ï¼›
- æ¨å¯¼å‡ºä¸€ä¸ª **Evidence Lower Bound (ELBO)** ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼š
  
  $$
  \mathcal{L}_{\text{VSD}} = \mathbb{E}_{z \sim q_\theta}[\log K(x,z)] - D_{\text{KL}}(q_\theta(z|x) \| p_e(z|x))
  $$

  å…¶ä¸­ï¼š
  - $ K(x,z) $ æ˜¯æ•´æ¡è·¯å¾„è¢«æ¥å—çš„æ¦‚ç‡ï¼ˆpath-level validityï¼‰ï¼›
  - ç¬¬ä¸€é¡¹é¼“åŠ±ç”Ÿæˆé«˜æ¥å—ç‡çš„è·¯å¾„ï¼›
  - ç¬¬äºŒé¡¹ä½œä¸ºæ­£åˆ™é¡¹ï¼Œä½¿ draft åˆ†å¸ƒè´´è¿‘ target model çš„è¾“å‡ºåˆ†å¸ƒã€‚

ä¸ºäº†ä¼˜åŒ–è¯¥ç›®æ ‡ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäº **Expectation-Maximization (EM)** æ¡†æ¶çš„ç®—æ³•ï¼š
- **E-step**ï¼šä½¿ç”¨ **MCMC + oracle filtering** ä»â€œè¢« target æ¥å—çš„é«˜è´¨é‡è·¯å¾„â€åéªŒåˆ†å¸ƒä¸­é‡‡æ ·ï¼›
- **M-step**ï¼šå¯¹è¿™äº›æ ·æœ¬è¿›è¡ŒåŠ æƒæœ€å¤§ä¼¼ç„¶æ›´æ–°ã€‚

å¹¶å¼•å…¥ä¸¤ä¸ªå…³é”®æŠ€æœ¯æå‡ç¨³å®šæ€§ä¸æ•ˆç‡ï¼š
- **Adaptive Rejection Weighting (ARW)**ï¼šåŠ¨æ€è°ƒæ•´ rejected samples çš„æ¢¯åº¦æƒé‡ï¼Œé™ä½æ–¹å·®ï¼›
- **Confidence-Aware Regularization (CAR)**ï¼šæƒ©ç½šæ¨¡å‹å¯¹é”™è¯¯è·¯å¾„èµ‹äºˆè¿‡é«˜ç½®ä¿¡åº¦çš„è¡Œä¸ºï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´åŒ¹é… decoding ç›®æ ‡**ï¼šç›´æ¥ä¼˜åŒ–è·¯å¾„çº§æ¥å—ç‡è€Œé token çº§ä¼¼ç„¶ï¼›
- **ç†è®ºä¿éšœæ›´å¼º**ï¼šè¯æ˜äº†æœ€å¤§åŒ– VSD ç›®æ ‡ç­‰ä»·äºæå‡æœŸæœ›æ¥å—é•¿åº¦çš„ä¸‹ç•Œï¼›
- **å…¼å®¹æ€§å¼º**ï¼šå¯ä½œä¸ºæ’ä»¶å¼æ¨¡å—é›†æˆåˆ° EAGLE-3ã€MSDã€ViSpec ç­‰ä¸»æµæ¡†æ¶ä¸­ï¼›
- **æ€§èƒ½æ˜¾è‘—æå‡**ï¼šåœ¨å¤šä¸ª LLM å’Œ MLLM ä¸Šå‡å–å¾— SOTA åŠ é€Ÿæ•ˆæœã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
#### å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®éªŒï¼š
- **ShareGPT**ï¼šç”¨äºè®­ç»ƒé™¤ DeepSeek-R1 å¤–çš„æ‰€æœ‰ draft modelsï¼›
- **OpenThoughts-114k-math**ï¼šç”¨äºè®­ç»ƒ DeepSeek-R1-Distill-LLaMA-8Bã€‚

#### å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰å®éªŒï¼š
- **ShareGPT** ä¸ **LLaVA-Instruct-150K**ï¼šè”åˆç”¨äºè®­ç»ƒ LLaVA ç³»åˆ—çš„ draft modelsã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹è§„æ¨¡**ï¼š
  - LLMs: LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-13B, DeepSeek-R1-8Bï¼›
  - MLLMs: LLaVA-1.5-7B / 13Bã€‚
- **ç¡¬ä»¶å¹³å°**ï¼š
  - LLM å®éªŒï¼šå•å¼  RTX PRO 6000 GPUï¼›
  - MLLM å®éªŒï¼šL-40S GPUã€‚
- **æ¸©åº¦è®¾ç½®**ï¼š$ T \in \{0, 1\} $ï¼Œåˆ†åˆ«å¯¹åº” greedy ä¸ stochastic decodingã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼ˆå‡ä¸ºæ•ˆç‡æŒ‡æ ‡ï¼Œå›  VSD ä¿æŒ lossless æ€§è´¨ï¼‰ï¼š
  - **Speedup Ratio (SR)**ï¼šç›¸å¯¹äº vanilla autoregressive decoding çš„ wall-clock æ—¶é—´åŠ é€Ÿæ¯”ï¼›
  - **Acceptance Length (T)**ï¼šæ¯æ¬¡ draft-verify cycle å¹³å‡è¢«æ¥å—çš„ token æ•°é‡ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **åŸºç¡€ baseline**ï¼š
  - Vanilla autoregressive decoding (SR = 1.00Ã—)
- **Speculative decoding æ–¹æ³•**ï¼š
  - SPS, PLD, Lookahead, Medusa
  - EAGLE, EAGLE-2, EAGLE-3
  - HASS, GRIFFIN, MSD, ViSpec

æ‰€æœ‰ baseline å‡ä½¿ç”¨å®˜æ–¹å‘å¸ƒæ¨¡å‹æˆ–ä»£ç ï¼Œåœ¨ç›¸åŒç¡¬ä»¶ä¸Šå¤ç°ä»¥ä¿è¯å…¬å¹³æ€§ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
#### åœ¨ LLM ä¸Šçš„è¡¨ç°ï¼ˆå¹³å‡æå‡ï¼‰ï¼š
| æŒ‡æ ‡ | ç›¸æ¯” EAGLE-3 æå‡ |
|------|------------------|
| **Speedup Ratio (SR)** | **+9.6%**ï¼ˆgreedy, T=0ï¼‰<br>+7.3%ï¼ˆstochastic, T=1ï¼‰ |
| **Acceptance Length (T)** | **+7.6%** |

å…·ä½“æ¡ˆä¾‹ï¼š
- åœ¨ **HumanEval** ä¸Šï¼ŒDeepSeek-R1-8B + VSD è¾¾åˆ° **14.2% SR æå‡**ï¼›
- åœ¨ **MT-Bench** ä¸Šï¼ŒVicuna-13B + VSD å®ç° **13.6% SR æå‡**ã€‚

#### åœ¨ MLLM ä¸Šçš„è¡¨ç°ï¼ˆLLaVA-1.5-13Bï¼‰ï¼š
| æŒ‡æ ‡ | ç›¸æ¯” MSD | ç›¸æ¯” ViSpec |
|------|--------|-----------|
| **SR (T=0)** | **+10.1%** | **+8.5%** |
| **T (T=0)** | **+11.9%** | **+8.8%** |
| **SR (T=1)** | **+7.8%** | **+6.8%** |

> è¡¨æ˜ VSD åœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­åŒæ ·æœ‰æ•ˆï¼Œå°¤å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡å¦‚ TextVQAã€ChartQA ä¸Šè¡¨ç°çªå‡ºã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- VSD åœ¨æ‰€æœ‰æµ‹è¯•æ¨¡å‹ã€ä»»åŠ¡å’Œæ¸©åº¦è®¾ç½®ä¸‹å‡ä¼˜äºæ‰€æœ‰ baselineï¼›
- å³ä½¿åœ¨å·²é«˜åº¦ä¼˜åŒ–çš„ EAGLE-3 åŸºç¡€ä¸Šï¼ŒVSD ä»èƒ½å¸¦æ¥æ˜¾è‘—å¢ç›Šï¼›
- åœ¨ MLLM åœºæ™¯ä¸­ï¼ŒVSD æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§è§£å†³ draft training-decoding ä¸ä¸€è‡´é—®é¢˜çš„æ–¹æ³•ï¼Œè¶…è¶Šä¸“ä¸º VLM è®¾è®¡çš„ ViSpecã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰
åœ¨ `LLaMA-3-Instruct-8B` ä¸Šè¿›è¡Œæ¶ˆèåˆ†æï¼ˆTab. 4ï¼‰ï¼š

| è®¾ç½® | SR (avg) â†‘ | T (avg) â†‘ | è¯´æ˜ |
|------|------------|----------|------|
| Baseline (EAGLE-3) | 3.97 | 6.58 | â€” |
| + S=40 (æ›´å¤š latent proposals) | 4.06 | 6.73 | æ›´å¤šæ ·æœ¬æå‡è·¯å¾„æ¢ç´¢èƒ½åŠ› |
| + ARW | 4.15 | 6.88 | æ˜¾è‘—é™ä½æ¢¯åº¦æ–¹å·® |
| + CAR | **4.22** | **7.00** | æŠ‘åˆ¶ overconfident é”™è¯¯ï¼Œè¿›ä¸€æ­¥æå‡ |

æ­¤å¤–ï¼Œå¢åŠ  latent proposal æ•°é‡ $ S $ ä» 10 åˆ° 40 æŒç»­å¸¦æ¥æ€§èƒ½å¢ç›Šï¼Œè¡¨æ˜ VSD èƒ½æœ‰æ•ˆåˆ©ç”¨æ›´å¤šé‡‡æ ·è·¯å¾„ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **è®­ç»ƒ-è§£ç ä¸ä¸€è‡´æ˜¯ speculative decoding çš„æ ¹æœ¬ç“¶é¢ˆ**ï¼š
   - token-level è®­ç»ƒæ— æ³•åæ˜  multi-path decoding çš„çœŸå®éœ€æ±‚ï¼›
   - å®éªŒæ˜¾ç¤ºä»…çº¦ 36% çš„ greedy è·¯å¾„æœ€ç»ˆè¢«æ¥å—ã€‚

2. **VSD æˆåŠŸæ¡¥æ¥äº†è¿™ä¸€å·®è·**ï¼š
   - é€šè¿‡ variational inference æ¡†æ¶ï¼Œä½¿ draft model å­¦ä¹  target model å®é™…â€œæ„¿æ„æ¥å—â€çš„è·¯å¾„åˆ†å¸ƒï¼›
   - ç†è®ºè¯æ˜æœ€ä¼˜è§£å³ä¸ºç›®æ ‡æ¨¡å‹ä¸‹çš„ valid-path posteriorã€‚

3. **VSD å…·æœ‰å¼ºæ³›åŒ–æ€§å’Œå…¼å®¹æ€§**ï¼š
   - å¯æ— ç¼é›†æˆè‡³ EAGLE-3ã€MSDã€ViSpecã€GRIFFINã€HASS ç­‰å¤šç§æ¡†æ¶ï¼›
   - åœ¨ LLM ä¸ MLLM ä¸Šå‡å–å¾—ä¸€è‡´ä¸”æ˜¾è‘—çš„åŠ é€Ÿæ”¶ç›Šã€‚

4. **è·¯å¾„çº§ä¼˜åŒ–ä¼˜äº token çº§ä¼˜åŒ–**ï¼š
   - å¼•å…¥ path-level utility å’Œ rejection-aware learning æœºåˆ¶ï¼Œæ˜¾è‘—æå‡é•¿åºåˆ—æ¥å—ç‡ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **è®¡ç®—å¼€é”€è¾ƒé«˜**ï¼šå—é™äº MCMC é‡‡æ ·æˆæœ¬ï¼Œå½“å‰æœ€å¤šä½¿ç”¨ $ S=40 $ ä¸ª latent proposalsï¼›
- **éš¾ä»¥æ‰©å±•åˆ°æ›´å¤§è§„æ¨¡**ï¼šå—é™äº GPU å†…å­˜ä¸è®­ç»ƒæ—¶é—´ï¼Œæš‚æœªåœ¨è¶…å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ 70B+ï¼‰ä¸Šå…¨é¢éªŒè¯ï¼›
- **ç›®å‰é›†ä¸­åœ¨æ–‡æœ¬ä¸å›¾åƒæ¨¡æ€**ï¼šå°šæœªæ‹“å±•è‡³è¯­éŸ³ã€è§†é¢‘ç­‰å…¶ä»–æ¨¡æ€ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±• latent proposal æ•°é‡ $ S $ï¼Œæ¢ç´¢æ›´é«˜æ•ˆçš„é‡‡æ ·ç­–ç•¥ï¼ˆå¦‚ importance samplingï¼‰ï¼›
- å°† VSD åº”ç”¨äº **speech**, **video**, **multimodal planning** ç­‰æ–°å…´åœºæ™¯ï¼›
- ç»“åˆ **reinforcement learning** è¿›ä¸€æ­¥ä¼˜åŒ–è·¯å¾„çº§ rewardï¼›
- æ¢ç´¢ **on-the-fly adaptation** æœºåˆ¶ï¼Œä½¿ draft policy åŠ¨æ€é€‚åº”ä¸åŒè¾“å…¥åˆ†å¸ƒã€‚

--- 

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> VSD é€šè¿‡å°† speculative decoding çš„ draft training é‡æ„ä¸º **variational inference over latent paths**ï¼Œé¦–æ¬¡å®ç°äº†è®­ç»ƒç›®æ ‡ä¸è§£ç ç›®æ ‡çš„ä¸€è‡´æ€§ï¼Œåœ¨ç†è®ºä¸Šæ›´ä¼˜ã€å®è·µä¸­æ›´å¿«ï¼Œæˆä¸ºå½“å‰ speculative decoding é¢†åŸŸçš„æ–°èŒƒå¼ã€‚

</details>

---

### 12. [Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better](https://arxiv.org/abs/2602.05393)

**Authors**: Ji Zhao, Yufei Gu, Shitong Shao, Xun Zhou, Liang Xiang, Zeke Xie  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.05393v1  

#### Abstract
As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computati...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šLate-to-Early Training: LET LLMs Learn Earlier, So Faster and Better

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬è®ºæ–‡é’ˆå¯¹å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢„è®­ç»ƒè¿‡ç¨‹è®¡ç®—æˆæœ¬æé«˜ã€è€—æ—¶é•¿çš„é—®é¢˜ï¼Œæå‡ºä¸€ä¸ªæ–°é¢–ä¸”è¢«å¿½è§†çš„å®é™…æŒ‘æˆ˜ï¼š  
**èƒ½å¦åˆ©ç”¨ç¤¾åŒºä¸­å·²æœ‰çš„ã€ç»è¿‡å¤§é‡è®¡ç®—èµ„æºè®­ç»ƒçš„å°å‹å¼€æºé¢„è®­ç»ƒæ¨¡å‹ï¼ˆsmall pretrained modelsï¼‰ï¼Œæ¥åŠ é€Ÿæ›´å¤§è§„æ¨¡ LLM çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Ÿ**

ä¼ ç»ŸçŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillation, KDï¼‰é€šå¸¸ä¾èµ–æ›´å¤§çš„æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ï¼Œè¿™åœ¨å®è·µä¸­éš¾ä»¥å®ç°ï¼Œå¹¶å¯èƒ½é™åˆ¶å­¦ç”Ÿçš„æœ€ç»ˆæ€§èƒ½ä¸Šé™ã€‚è€Œæœ¬æ–‡æ¢ç´¢çš„æ˜¯â€œåå‘â€è·¯å¾„â€”â€”ç”¨æ›´å°ä½†å·²å……åˆ†è®­ç»ƒçš„æ¨¡å‹å»å¼•å¯¼æ›´å¤§æ¨¡å‹çš„å­¦ä¹ ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ï¼šLate-to-Early Training (LET)
ä½œè€…æå‡ºäº† **Late-to-Early Training (LET)** èŒƒå¼ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
> åˆ©ç”¨ä¸€ä¸ªå·²å®Œæˆè®­ç»ƒçš„å°å‹æ¨¡å‹ $ T $ï¼ˆä»£è¡¨â€œåæœŸé˜¶æ®µâ€çš„çŸ¥è¯†ï¼‰æ¥æŒ‡å¯¼å¤§å‹ç›®æ ‡æ¨¡å‹ $ M $ åœ¨**æ—©æœŸè®­ç»ƒæ­¥éª¤**å’Œ**è¾ƒæµ…å±‚ç½‘ç»œ**ä¸­çš„è¡¨ç¤ºå­¦ä¹ ã€‚

è¯¥èŒƒå¼åŒ…å«ä¸¤ä¸ªå…³é”®æœºåˆ¶ï¼š

- **Late-to-Early-Step Learning**ï¼šåœ¨è®­ç»ƒåˆæœŸå¼•å…¥å°å‹æ•™å¸ˆæ¨¡å‹ $ T $ è¿›è¡Œè¡¨ç¤ºå¯¹é½ï¼Œéšç€è®­ç»ƒè¿›è¡Œé€æ¸å‡å°‘å…¶å½±å“ï¼ˆé€šè¿‡è¡°å‡æƒé‡ $\lambda$ è‡³é›¶ï¼‰ï¼Œä½¿æ¨¡å‹æœ€ç»ˆå›å½’æ ‡å‡†è¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚
- **Late-to-Early-Layer Learning**ï¼šå°†æ•™å¸ˆæ¨¡å‹ $ T $ çš„**æœ€åä¸€å±‚**ï¼ˆlate layerï¼‰è¡¨ç¤ºï¼Œç”¨äºå¯¹é½å­¦ç”Ÿæ¨¡å‹ $ M $ çš„**æ—©æœŸå±‚**ï¼ˆearly layerï¼‰è¡¨ç¤ºã€‚

è¿™ç§è®¾è®¡å…è®¸åç»­æ·±å±‚ç½‘ç»œè‡ªç„¶åœ°é€‚åº”å¹¶ç²¾ç‚¼æ¥è‡ªæ•™å¸ˆçš„ä¿¡æ¯ï¼Œä»è€Œæå‡å­¦ä¹ æ•ˆç‡ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | å±€é™æ€§ | LET çš„ä¼˜åŠ¿ |
|------|--------|-----------|
| **ä¼ ç»Ÿ KD** | æ•™å¸ˆå¿…é¡»å¤§äºå­¦ç”Ÿï¼Œå†…å­˜å¼€é”€å¤§ï¼›å­¦ç”Ÿéš¾è¶…è¶Šæ•™å¸ˆ | å¯ä½¿ç”¨è¿œå°äºç›®æ ‡æ¨¡å‹ï¼ˆå¦‚ 10Ã— å°ï¼‰çš„æ•™å¸ˆæ¨¡å‹ï¼ŒèŠ‚çœèµ„æºä¸”ä¸ç‰ºç‰²æ€§èƒ½ |
| **SALT / Rawat et al. [65]** | æ•™å¸ˆä¸å­¦ç”Ÿå°ºå¯¸å·®è·æœ‰é™ï¼ˆä»… ~1.87Ã—ï¼‰ï¼Œä»è¾ƒå¤§ | æ”¯æŒé«˜è¾¾ 10Ã— å‚æ•°å·®è·ï¼Œæ›´å…·å®ç”¨æ€§ |
| **æ¨¡å‹å¢é•¿ç­–ç•¥ï¼ˆModel Growthï¼‰** | éœ€ä¿®æ”¹æ¶æ„ï¼ˆæ·±åº¦/å®½åº¦æ‰©å±•ï¼‰ï¼Œå¤æ‚åº¦é«˜ | å®Œå…¨æ¶æ„æ— å…³ï¼ˆarchitecture-agnosticï¼‰ï¼Œæ— éœ€ç»“æ„è°ƒæ•´ |

æ­¤å¤–ï¼ŒLET ä¸ä¾èµ–ç‰¹å®šæ•°æ®é¢„å¤„ç†æˆ–å¤æ‚çš„åœ¨çº¿é€‰æ‹©æœºåˆ¶ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰è®­ç»ƒæµç¨‹ä¸­ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- ä¸»è¦é¢„è®­ç»ƒæ•°æ®ï¼š**The Pile**ï¼ˆçº¦ 825GB è‹±æ–‡æ–‡æœ¬ï¼Œ22 ä¸ªæ¥æºï¼‰
- å®éªŒä½¿ç”¨å…¶ä¸­çº¦ **200 äº¿ tokensï¼ˆ20Bï¼‰** è¿›è¡Œè®­ç»ƒ
- ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°åŸºäº 9 ä¸ªå…¬å¼€åŸºå‡†ï¼ˆè§ä¸‹è¡¨ï¼‰

### æ¨¡å‹æ¶æ„
- ç›®æ ‡æ¨¡å‹ $ M $ï¼šåŸºäº **LLaMA æ¶æ„**ï¼Œé‡‡ç”¨ RMSNorm å’Œ SwiGLU æ¿€æ´»å‡½æ•°
- æ•™å¸ˆæ¨¡å‹ $ T $ï¼šæ¥è‡ªå¤šä¸ªç³»åˆ—ï¼ŒåŒ…æ‹¬ï¼š
  - **OPT**ï¼ˆå¦‚ OPT-125Mï¼‰
  - **Pythia**ï¼ˆå¦‚ Pythia-160Mï¼‰
  - **SmolLM**ï¼ˆå¦‚ SmolLM-135M / SmolLM-1.7Bï¼‰
- å®éªŒè§„æ¨¡è¦†ç›– **1.4Bã€3Bã€7B** å‚æ•°çº§åˆ«

### å®éªŒè®¾ç½®
- æ€» batch size: 2048
- åºåˆ—é•¿åº¦: 1280 tokens
- ä½¿ç”¨ **BF16** ç²¾åº¦
- ä¼˜åŒ–å™¨: AdamWï¼Œcosine å­¦ä¹ ç‡è°ƒåº¦
- åˆå§‹æŠ•å½±æŸå¤±æƒé‡ $\lambda_0 = 0.1$ï¼Œåœ¨æŒ‡å®šæ­¥æ•° $S_{\text{stop}}$ åçº¿æ€§è¡°å‡è‡³ 0

### è¯„ä¼°æŒ‡æ ‡
- **ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½**ï¼š9 ä¸ªä»»åŠ¡ä¸Šçš„å¹³å‡ one-shot å‡†ç¡®ç‡
  - åŒ…æ‹¬ HellaSwag (HS), Winogrande (Wino.), LAMBADA (LAMB), OpenBookQA (OBQA), ARC-e/c, PIQA, SciQ, BoolQ
- **è¯­è¨€å»ºæ¨¡èƒ½åŠ›**ï¼šThe Pile æµ‹è¯•é›†ä¸Šçš„ **Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰**
- **è®­ç»ƒæ•ˆç‡**ï¼šè¾¾åˆ°ç›¸åŒæ€§èƒ½æ‰€éœ€çš„ token æ•°é‡ï¼ˆå³æ”¶æ•›é€Ÿåº¦ï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿æ–¹æ³• | æè¿° |
|---------|------|
| **Baseline** | æ ‡å‡†å› æœè¯­è¨€å»ºæ¨¡ï¼ˆCausal LMï¼‰è®­ç»ƒ |
| **RKD (Reverse Knowledge Distillation)** | ä½¿ç”¨å°æ¨¡å‹ä½œä¸ºæ•™å¸ˆçš„ä¼ ç»Ÿ KD æ–¹æ³• |
| **SALT** | Rawat et al. [65] æå‡ºçš„æ–¹æ³•ï¼Œç»“åˆ KD ä¸ä¸¤é˜¶æ®µè®­ç»ƒ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 å’Œ Figure 1ï¼‰

#### âœ… åœ¨ 1.4B æ¨¡å‹ä¸Šï¼ˆä½¿ç”¨ SmolLM-135M ä½œä¸ºæ•™å¸ˆï¼Œå‚æ•°å° 10Ã—ï¼‰ï¼š
- **å¹³å‡ä¸‹æ¸¸å‡†ç¡®ç‡**ï¼š
  - Baseline: **41.6%**
  - LET: **43.6%**ï¼ˆâ†‘ **+2.0pp**ï¼‰
- **è®­ç»ƒåŠ é€Ÿæ¯”**ï¼š
  - LET ä»…éœ€ **67% çš„è®­ç»ƒæ­¥æ•°**å³å¯è¶…è¿‡ Baseline çš„æœ€ç»ˆæ€§èƒ½ â†’ å®ç° **æœ€é«˜è¾¾ 1.6Ã— çš„åŠ é€Ÿ**
- å³ä¾¿ä½¿ç”¨æ¯”ç›®æ ‡å° 10Ã— çš„æ•™å¸ˆæ¨¡å‹ï¼ŒLET ä¾ç„¶æ˜¾è‘—ä¼˜äºåŸºçº¿

#### âœ… åœ¨ 7B æ¨¡å‹ä¸Šï¼ˆä½¿ç”¨ SmolLM-1.7B ä½œä¸ºæ•™å¸ˆï¼‰ï¼š
- **å¹³å‡ä¸‹æ¸¸å‡†ç¡®ç‡**ï¼š
  - Baseline: **43.3%**
  - LET: **45.5%**ï¼ˆâ†‘ **+2.2pp**ï¼‰
- åŒæ ·è§‚å¯Ÿåˆ°æ˜æ˜¾åŠ é€Ÿè¶‹åŠ¿ï¼Œåœ¨æ›´æ—©é˜¶æ®µå°±è¶…è¶Š Baseline

> ğŸ” **ç‰¹åˆ«å‘ç°**ï¼šLET-1.4B çš„æ€§èƒ½ç”šè‡³è¶…è¿‡äº† **Baseline-3B**ï¼Œè¯´æ˜ LET æ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™åœºæ™¯ã€‚

---

### ä¸å…¶ä»–æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | 1.4B å¹³å‡ Acc (%) | æ˜¯å¦ä¼˜äº Baseline |
|------|------------------|------------------|
| Baseline | 41.6 | â€” |
| RKD | 41.4 | âŒï¼ˆç•¥å·®ï¼‰ |
| SALT+ | 42.9 | âœ…ï¼ˆ+1.3ppï¼‰ |
| **LET** | **43.6** | âœ…âœ…ï¼ˆ**+2.0pp**ï¼‰ |

- **RKD è¡¨ç°ä¸ä½³**ï¼šå½“æ•™å¸ˆè¿œå°äºå­¦ç”Ÿæ—¶ï¼Œä¼ ç»Ÿ KD ä¼šæŸå®³æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ SciQ ç­‰ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æ˜æ˜¾ä¸‹é™
- **LET æ˜¾è‘—ä¼˜äº SALT**ï¼šåœ¨ç›¸åŒè®¾ç½®ä¸‹ï¼ŒLET å®ç°æ›´é«˜æ€§èƒ½å¢ç›Š

---

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰ä¸åŒå±‚å¯¹é½ç­–ç•¥æ¯”è¾ƒï¼ˆFigure 3 & 4ï¼‰
æµ‹è¯•å…­ç§å¯¹é½æ–¹å¼ï¼ˆL2E, L2M, L2L, M2E, M2M, M2Lï¼‰ï¼š
- **æœ€ä½³é…ç½®ä¸º L2E**ï¼ˆTeacher æœ€åä¸€å±‚ â†’ Student æ—©æœŸå±‚ï¼‰
- ä½¿ç”¨ä¸­é—´å±‚è¡¨ç¤ºï¼ˆM2Xï¼‰æ•ˆæœæ™®éè¾ƒå·®
- L2E åœ¨è®­ç»ƒç»“æŸåä¿æŒç¨³å®šä½å›°æƒ‘åº¦ï¼Œå…¶ä»–ç­–ç•¥å‡ºç°åå¼¹ â†’ è¡¨æ˜ L2E æ›´é²æ£’

#### ï¼ˆ2ï¼‰è¶…å‚æ•° $\lambda$ å½±å“ï¼ˆFigure 5 & 6ï¼‰
- $\lambda = 0.1$ ä¸ºæœ€ä¼˜å€¼
  - $\lambda < 0.1$ï¼šå¯¹é½ä¸è¶³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ•™å¸ˆä¿¡å·
  - $\lambda > 0.1$ï¼šè¿‡åº¦å¯¹é½ï¼ŒæŠ‘åˆ¶ä»æ•°æ®ä¸­å­¦ä¹ çš„èƒ½åŠ›
- Cosine similarity åˆ†ææ˜¾ç¤ºï¼šå³ä½¿ $\lambda$ å¾ˆå°ä¹Ÿèƒ½å»ºç«‹æœ‰æ•ˆå¯¹é½

#### ï¼ˆ3ï¼‰ä¸åŒæ•™å¸ˆæ¨¡å‹çš„å½±å“
- ä½¿ç”¨ **SmolLM** ä½œä¸ºæ•™å¸ˆæ•ˆæœæœ€å¥½ï¼Œä¼˜äº OPT å’Œ Pythia
- ä½¿ç”¨ **LLaMA 3.2-1B** ä½œä¸º 7B æ¨¡å‹çš„æ•™å¸ˆä¹Ÿå–å¾—æ˜¾è‘—åŠ é€Ÿï¼ŒéªŒè¯æ³›åŒ–æ€§

#### ï¼ˆ4ï¼‰å¤±è´¥æ¨¡å¼åˆ†æï¼ˆTable 6ï¼‰
- å½“ä½¿ç”¨ **GPT-2**ï¼ˆè®­ç»ƒæ•°æ®æˆªæ­¢äº 2017 å¹´ï¼‰ä½œä¸ºæ•™å¸ˆæ—¶ï¼ŒLET è¡¨ç°ä¸å¦‚ Baseline
- åŸå› æ¨æµ‹ï¼šGPT-2 çš„è®­ç»ƒæ•°æ®è´¨é‡è¾ƒä½ï¼Œå¯¼è‡´å…¶è¡¨ç¤ºç¼ºä¹ç°ä»£è¯­ä¹‰ä¿¡æ¯
- ä½†ä»ä¼˜äº RKD â†’ è¡¨æ˜ LET å¯¹ä½è´¨æ•™å¸ˆå…·æœ‰æ›´å¼ºé²æ£’æ€§

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **å°å‹é¢„è®­ç»ƒæ¨¡å‹å¯æœ‰æ•ˆåŠ é€Ÿå¤§æ¨¡å‹è®­ç»ƒ**ï¼šå³ä½¿æ•™å¸ˆæ¨¡å‹æ¯”å­¦ç”Ÿå° 10 å€ï¼Œä¹Ÿèƒ½æä¾›æœ‰ä»·å€¼çš„è¡¨ç¤ºå¼•å¯¼ã€‚
2. âœ… **Late-to-Early-Layer æ˜¯å…³é”®æœºåˆ¶**ï¼šå°†æ•™å¸ˆçš„â€œæ™šæœŸâ€çŸ¥è¯†æ³¨å…¥å­¦ç”Ÿçš„â€œæ—©æœŸâ€å±‚ï¼Œèƒ½æœ€æœ‰æ•ˆåœ°ä¿ƒè¿›å¿«é€Ÿæ”¶æ•›å’Œæ³›åŒ–ã€‚
3. âœ… **LET å®ç°æ›´å¿«æ›´å¥½çš„è®­ç»ƒ**ï¼šä¸ä»…åŠ å¿«æ”¶æ•›ï¼ˆ**1.6Ã— åŠ é€Ÿ**ï¼‰ï¼Œè¿˜æå‡æœ€ç»ˆæ€§èƒ½ï¼ˆ**+2% ç»å¯¹å‡†ç¡®ç‡æå‡**ï¼‰ã€‚
4. âœ… **æ–¹æ³•å…·å¤‡å¼ºé²æ£’æ€§å’Œé€šç”¨æ€§**ï¼š
   - è·¨ä¸åŒè¯æ±‡è¡¨ï¼ˆSmolLM/OPT/Pythiaï¼‰å‡æœ‰æ•ˆï¼ˆFigure 2ï¼‰
   - è·¨ä¸åŒæ¨¡å‹å®¶æ—å’Œæ¶æ„å‡é€‚ç”¨ï¼ˆTable 3 æ˜¾ç¤ºå¼‚æ„ç»“æ„ä»æœ‰æ•ˆï¼‰
   - å¯æ¨å¹¿è‡³é NLP ä»»åŠ¡ï¼ˆå¦‚æ—¶é—´åºåˆ—åˆ†ç±»ï¼ŒTable 2ï¼‰

---

### æ–¹æ³•çš„å±€é™æ€§
1. âš ï¸ **ååé‡ç•¥æœ‰ä¸‹é™**ï¼š
   - å› éœ€å‰å‘ä¼ æ’­æ•™å¸ˆæ¨¡å‹ï¼ŒLET çš„æ¯ç§’å¤„ç† token æ•°ç•¥ä½äº Baselineï¼ˆçº¦ 98.4% throughput ratioï¼‰
   - ä½†åœ¨å¤§è§„æ¨¡ batch ä¸‹ï¼Œå³°å€¼ VRAM å ç”¨æ›´ä½ï¼ˆLET < SALT/RKDï¼‰ï¼Œå…·å¤‡æ›´å¥½æ‰©å±•æ½œåŠ›
2. âš ï¸ **ä¾èµ–æ•™å¸ˆæ¨¡å‹çš„è´¨é‡**ï¼š
   - è‹¥æ•™å¸ˆæ¨¡å‹è®­ç»ƒæ•°æ®é™ˆæ—§æˆ–è´¨é‡å·®ï¼ˆå¦‚ GPT-2ï¼‰ï¼Œåˆ™æ— æ³•å¸¦æ¥å¢ç›Š
3. âš ï¸ **å½“å‰å®éªŒè§„æ¨¡æœ‰é™**ï¼š
   - å®éªŒé›†ä¸­åœ¨ 1.4Bâ€“7B æ¨¡å‹ï¼Œå°šæœªéªŒè¯åœ¨ 70B+ è¶…å¤§è§„æ¨¡ä¸‹çš„è¡¨ç°

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³æ›´å¤§æ¨¡å‹å’Œæ›´å¤šæ•°æ®**ï¼šéªŒè¯ LET åœ¨ç™¾Bçº§æ¨¡å‹ï¼ˆå¦‚ Llama-3 70Bï¼‰å’Œä¸‡äº¿ token æ•°æ®ä¸Šçš„å¯æ‰©å±•æ€§ã€‚
2. **åŠ¨æ€é€‰æ‹©æ•™å¸ˆæ¨¡å‹**ï¼šç ”ç©¶å¦‚ä½•è‡ªåŠ¨æŒ‘é€‰æœ€é€‚åˆå½“å‰è®­ç»ƒé˜¶æ®µçš„æ•™å¸ˆæ¨¡å‹ã€‚
3. **å¤šæ•™å¸ˆèåˆæœºåˆ¶**ï¼šæ¢ç´¢åŒæ—¶åˆ©ç”¨å¤šä¸ªå°å‹æ¨¡å‹çš„çŸ¥è¯†ä»¥è¿›ä¸€æ­¥å¢å¼ºè¡¨ç¤ºå¤šæ ·æ€§ã€‚
4. **ç†è®ºæ·±åŒ–**ï¼šè¿›ä¸€æ­¥åˆ†æä¸ºä½• L2E ç»“æ„èƒ½å¸¦æ¥æ›´å¹³æ»‘çš„ä¼˜åŒ–æ™¯è§‚ï¼ˆæ–‡ä¸­ Hessian åˆ†æä»…ä¸ºåˆæ­¥è§£é‡Šï¼‰ã€‚

---

## æ€»ç»“
> **LET å¼€è¾Ÿäº†ä¸€æ¡é«˜æ•ˆå¤ç”¨å·²æœ‰å°å‹å¼€æºæ¨¡å‹çš„æ–°è·¯å¾„ï¼Œä½¿å¾—â€œåæ¥è€…å±…ä¸Šâ€æˆä¸ºå¯èƒ½â€”â€”è®©å¤§æ¨¡å‹åœ¨è®­ç»ƒæ—©æœŸå°±èƒ½â€œç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šâ€ï¼Œä»è€Œå­¦å¾—æ›´å¿«ã€æ›´å¥½ã€‚**

è¯¥æ–¹æ³•ç®€å•ã€é€šç”¨ã€æ— éœ€æ¶æ„æ”¹åŠ¨ï¼Œæœ‰æœ›æˆä¸ºä¸‹ä¸€ä»£ LLM é¢„è®­ç»ƒçš„æ ‡å‡†ç»„ä»¶ä¹‹ä¸€ã€‚

</details>

---

### 13. [Multi-Token Prediction via Self-Distillation](https://arxiv.org/abs/2602.06019)

**Authors**: John Kirchenbauer, Abhimanyu Hans, Brian Bartoldson, Micah Goldblum, Ashwinee Panda, Tom Goldstein  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.06019v1  

#### Abstract
Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single ne...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMulti-Token Prediction via Self-Distillation

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
ç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œå¦‚ **speculative decoding**ï¼Œä¾èµ–äºè®­ç»ƒè¾…åŠ©çš„â€œæ¨æµ‹å™¨â€ï¼ˆspeculatorï¼‰æ¨¡å‹ï¼Œå¹¶æ„å»ºå¤æ‚çš„æ¨ç†æµæ°´çº¿ã€‚è¿™äº›æ–¹æ³•è™½ç„¶èƒ½æå‡ç”Ÿæˆé€Ÿåº¦ï¼Œä½†å¢åŠ äº†ç³»ç»Ÿå¤æ‚æ€§å’Œéƒ¨ç½²æˆæœ¬ã€‚

æ­¤å¤–ï¼Œä¼ ç»Ÿçš„ **multi-token prediction (MTP)** æ–¹æ³•é€šå¸¸åŸºäºæ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œç›´æ¥å°†æ¨¡å‹è¾“å‡ºä¸çœŸå®æ–‡æœ¬ï¼ˆground truthï¼‰å¯¹é½ã€‚è¿™ç§æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ï¼šå®ƒåªå»ºæ¨¡æ¯ä¸ªä½ç½®çš„è¾¹ç¼˜åˆ†å¸ƒï¼ˆmarginal distributionï¼‰ï¼Œè€Œå¿½ç•¥äº†å¤šä¸ªtokenä¹‹é—´çš„è”åˆåˆ†å¸ƒï¼ˆjoint distributionï¼‰ï¼Œå¯¼è‡´åŒæ—¶é¢„æµ‹å¤šä¸ªtokenæ—¶å¯èƒ½å‡ºç°è¯­æ³•ä¸è¿è´¯æˆ–è¯­ä¹‰ä¸åˆç†çš„é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œâ€œå–‚ç†ŠçŒ«è‚‰â€æˆ–â€œå–‚ç‹®å­ç«¹å­â€ï¼‰ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº **self-distillation** å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯å‘çš„æ–°å‹ MTP è®­ç»ƒèŒƒå¼ï¼Œç§°ä¸º **"Student Forced" Online MTP**ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- **åœ¨çº¿è’¸é¦ç›®æ ‡ï¼ˆOnline Distillation Objectiveï¼‰**ï¼š  
  ä¸å†ä½¿ç”¨ ground truth ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè€Œæ˜¯è®©ä¸€ä¸ªå¼ºå¤§çš„é¢„è®­ç»ƒ **teacher model**ï¼ˆå³æ ‡å‡†çš„è‡ªå›å½’ NTP æ¨¡å‹ï¼‰æ¥è¯„åˆ¤å­¦ç”Ÿæ¨¡å‹ï¼ˆstudent MTP modelï¼‰ç”Ÿæˆçš„å¤štokenåºåˆ—çš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­é¢„æµ‹ $k$ ä¸ªtokenï¼Œç„¶å teacher model å¯¹è¯¥å®Œæ•´åºåˆ—æ‰“åˆ†ï¼ˆè®¡ç®—å…¶ä¼¼ç„¶ï¼‰ï¼Œä»è€Œæä¾›ä¸€ä¸ªâ€œon-policyâ€çš„å¥–åŠ±ä¿¡å·ã€‚

- **ç¡®å®šæ€§é‡‡æ ·ä¸ä½ç†µä¼˜åŒ–**ï¼š  
  å­¦ç”Ÿæ¨¡å‹ä½¿ç”¨ `argmax` ç¡®å®šæ€§åœ°ç”Ÿæˆtokenåºåˆ—ï¼ˆrolloutï¼‰ï¼Œé¿å…éšæœºé‡‡æ ·å¸¦æ¥çš„ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡æœ€å°åŒ– teacher åˆ†å¸ƒä¸å­¦ç”Ÿåˆ†å¸ƒä¹‹é—´çš„ KL æ•£åº¦ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹è¾“å‡ºé«˜ç½®ä¿¡åº¦ã€ä½ç†µçš„é¢„æµ‹ï¼Œä»è€Œä¿è¯ç”Ÿæˆçš„å¤štokenåºåˆ—åœ¨è¯­ä¹‰å’Œè¯­æ³•ä¸Šçš„è¿è´¯æ€§ã€‚

- **æ— éœ€é¢å¤–æ¨¡å—æˆ–éªŒè¯æœºåˆ¶**ï¼š  
  æœ€ç»ˆçš„ MTP æ¨¡å‹å®Œå…¨ä¿ç•™åŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„å®ç°æ–¹å¼ï¼Œ**æ— éœ€å¼•å…¥ä»»ä½•è¾…åŠ© verifier æˆ–ä¿®æ”¹æ¨ç†ä»£ç **ï¼Œå³å¯ç‹¬ç«‹å®Œæˆå¿«é€Ÿè§£ç ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | æœ¬æ–‡æ–¹æ³• | Speculative Decoding | ä¼ ç»Ÿ MTP |
|------|--------|---------------------|----------|
| **ç³»ç»Ÿå¤æ‚åº¦** | æä½ï¼ˆå•æ¨¡å‹ï¼Œæ—  verifierï¼‰ | é«˜ï¼ˆéœ€è®­ç»ƒ speculator + verifierï¼‰ | ä½ |
| **è®­ç»ƒæ•ˆç‡** | é«˜æ•ˆçš„åœ¨çº¿è’¸é¦ | æ— éœ€è®­ç»ƒä¸»æ¨¡å‹ï¼Œä½†éœ€é¢å¤– speculator | ç®€å•ä½†æ•ˆæœå·® |
| **ç”Ÿæˆè´¨é‡** | é«˜ï¼ˆç”± teacher ä¿è¯è¿è´¯æ€§ï¼‰ | é«˜ï¼ˆverifier ä¿éšœï¼‰ | ä½ï¼ˆå¿½ç•¥è”åˆåˆ†å¸ƒï¼‰ |
| **éƒ¨ç½²ä¾¿æ·æ€§** | å¯ç›´æ¥éƒ¨ç½²ï¼Œå…¼å®¹ç°æœ‰æ¡†æ¶ | éœ€å®šåˆ¶æ¨ç†å¼•æ“ | å¯ç›´æ¥éƒ¨ç½² |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼šä»¥æç®€æ¶æ„å®ç°äº†æ¥è¿‘ speculative decoding çš„åŠ é€Ÿæ•ˆæœï¼Œä¸”æ— éœ€å¤æ‚çš„å¤šæ¨¡å‹åä½œã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
- **è®­ç»ƒæ•°æ®**ï¼š
  - ä¸»è¦ä½¿ç”¨ **MetaMathQA**ï¼šä¸€ä¸ªåˆæˆçš„å°å­¦æ•°å­¦ï¼ˆgrade school mathï¼‰æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹ã€‚
  - å¯¹æ¯”å®éªŒä¸­ä¹Ÿå°è¯•äº†æ›´é€šç”¨çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† **Magpie-Pro-300k-v0.1** å’Œ **Magpie-Reasoning-150k**ã€‚

- **è¯„ä¼°æ•°æ®é›†**ï¼š
  - **GSM8K**ï¼šå°å­¦æ•°å­¦é¢˜åŸºå‡†ï¼Œç”¨äºè¡¡é‡æ¨ç†èƒ½åŠ›ã€‚
  - **AIME25**ï¼šé«˜çº§æ•°å­¦ç«èµ›é¢˜ã€‚
  - **GPQA**ï¼šç ”ç©¶ç”Ÿçº§é—®ç­”ï¼Œè€ƒå¯Ÿæ·±åº¦çŸ¥è¯†ã€‚
  - **BBH (Big-Bench Hard)**ï¼šå¤æ‚æŒ‡ä»¤ç†è§£ä»»åŠ¡ã€‚
  - **IFEVAL**ï¼šæŒ‡ä»¤éµå¾ªç²¾ç¡®æ€§æµ‹è¯•ã€‚
  - **CNN DailyMail**ï¼šæ‘˜è¦ç”Ÿæˆä»»åŠ¡ï¼Œè¯„ä¼°å¼€æ”¾ç”Ÿæˆèƒ½åŠ›ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **æ¨¡å‹åŸºç¡€**ï¼š
  - ä¸¤ä¸ªä¸»æµæ¨¡å‹ä½œä¸ºèµ·ç‚¹ï¼š
    - `Llama-3.1-8B-MagpieAlign-SFT-v0.1`ï¼ˆç®€ç§° L3.1-8B-Magpieï¼‰
    - `Qwen3-4B-Instruct-2507`ï¼ˆç®€ç§° Qwen3-4B-Inst-2507ï¼‰

- **è®­ç»ƒç­–ç•¥**ï¼š
  - ä½¿ç”¨ **blocked causal attention** å’Œ **MTP mask tokens** å®ç°å¤štokenå¹¶è¡Œé¢„æµ‹ã€‚
  - åœ¨è®­ç»ƒä¸­éšæœºåŒ– MTP åŒºåŸŸçš„ä½ç½®å’Œé•¿åº¦ $k \in [2, 16]$ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚
  - Teacher æ¨¡å‹å‚æ•°å†»ç»“ï¼Œä»…è®­ç»ƒ Student æ¨¡å‹ã€‚
  - ä½¿ç”¨ **hard teacher**ï¼ˆteacher è¾“å‡ºä¸º argmaxï¼Œå½¢æˆ delta åˆ†å¸ƒï¼‰ä»¥æ›´å¿«é™ä½å­¦ç”Ÿæ¨¡å‹ç†µã€‚

- **æ¨ç†ç­–ç•¥**ï¼š
  - **Static Decoding**ï¼šå›ºå®šæ¯æ¬¡é¢„æµ‹ $k$ ä¸ª tokenã€‚
  - **Confidence-Adaptive (ConfAdapt)**ï¼šåŠ¨æ€è°ƒæ•´ $k$ï¼Œä»…å½“æ‰€æœ‰é¢„æµ‹ token çš„æœ€å¤§æ¦‚ç‡è¶…è¿‡é˜ˆå€¼ $\tau$ æ—¶æ‰æ¥å—ã€‚ä¾‹å¦‚ $\tau=0.9$ è¡¨ç¤º 90% ç½®ä¿¡åº¦ã€‚

- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Accuracy**ï¼šåˆ†ç±»/æ•°å­¦ä»»åŠ¡çš„å‡†ç¡®ç‡ã€‚
  - **Acceleration Factor (Effective k)**ï¼šå¹³å‡æ¯æ­¥ç”Ÿæˆçš„ token æ•°ï¼Œåæ˜ æ¨ç†é€Ÿåº¦ã€‚
  - **ROUGE-L**ï¼šæ‘˜è¦ä»»åŠ¡çš„ç”Ÿæˆè´¨é‡ã€‚
  - æ‰€æœ‰ç»“æœæŠ¥å‘Šå‡å«æ ‡å‡†è¯¯å·®ï¼ˆStd. Errorï¼‰ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Baseline**ï¼šåŸå§‹æ¨¡å‹åœ¨ $k=1$ ä¸‹çš„æ ‡å‡†è‡ªå›å½’è§£ç ã€‚
- **Static MTP**ï¼šå›ºå®š $k$ çš„å¤štokené¢„æµ‹ã€‚
- **ConfAdapt MTP**ï¼šæœ¬æ–‡æå‡ºçš„åŠ¨æ€ç½®ä¿¡åº¦é€‚åº”ç­–ç•¥ã€‚
- é—´æ¥å¯¹æ¯” **speculative decoding** çš„ç†è®ºåŠ é€Ÿä¸Šé™ï¼ˆæ–‡ä¸­æŒ‡å‡ºå…¶å®ç”¨æ€§å—æ‰¹å¤„ç†ç­‰ç°å®å› ç´ é™åˆ¶ï¼‰ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆä»¥ GSM8K ä¸ºä¾‹ï¼‰

#### L3.1-8B-Magpie æ¨¡å‹
| æ–¹æ³• | Accuracy (%) | Acceleration (Ã—) | Drop vs Baseline |
|------|--------------|------------------|------------------|
| Baseline ($k=1$) | 66.0 Â± 1.3 | 1Ã— | â€” |
| Static $k=5$ | 41.5 Â± 1.4 | 5Ã— | -24.5% |
| **ConfAdapt ($\tau=0.9$)** | **64.1 Â± 1.3** | **3.3Ã—** | **<3%** |

> âœ… åœ¨ç²¾åº¦å‡ ä¹ä¸å˜ï¼ˆä»…ä¸‹é™çº¦ 3%ï¼‰çš„æƒ…å†µä¸‹å®ç° **3.3å€åŠ é€Ÿ**ã€‚

#### Qwen3-4B-Inst-2507 æ¨¡å‹
| æ–¹æ³• | Accuracy (%) | Acceleration (Ã—) | Drop vs Baseline |
|------|--------------|------------------|------------------|
| Baseline ($k=1$) | 89.1 Â± 0.9 | 1Ã— | â€” |
| Static $k=5$ | 28.9 Â± 1.2 | 5Ã— | -60.2% |
| **ConfAdapt ($\tau=0.9$)** | **83.6 Â± 1.0** | **3.1Ã—** | **~7%** |

> âœ… å®ç° **3å€åŠ é€Ÿ**ï¼Œç²¾åº¦ä¸‹é™æ§åˆ¶åœ¨ **7%ä»¥å†…**ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **ConfAdapt æ˜¾è‘—ä¼˜äº Static MTP**ï¼š  
  å›ºå®š $k$ å¯¼è‡´ç²¾åº¦æ€¥å‰§ä¸‹é™ï¼Œè€Œ ConfAdapt èƒ½è‡ªåŠ¨å¹³è¡¡â€œæ˜“â€ä¸â€œéš¾â€tokenï¼Œä¿æŒé«˜è´¨é‡è¾“å‡ºã€‚
- **pareto-optimal trade-off**ï¼š  
  åœ¨ accuracy-speed æ›²çº¿ä¸Šï¼ŒConfAdapt æ–¹æ³•å§‹ç»ˆå¤„äºå‰æ²¿ï¼Œå®ç°æœ€ä¼˜æƒè¡¡ã€‚
- **è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›å¼º**ï¼š  
  å°½ç®¡åªåœ¨ MetaMathQA ä¸Šè®­ç»ƒï¼Œä½†åœ¨ GPQAã€BBH ç­‰ä»»åŠ¡ä¸Šä»è¡¨ç°å‡ºè‰¯å¥½è¿ç§»èƒ½åŠ›ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰
| å˜ä½“ | ç»“æœåˆ†æ |
|------|---------|
| **Ground Truth Supervision (GT Suprv.)** | ä½¿ç”¨çœŸå®æ ‡ç­¾è®­ç»ƒ MTPï¼Œæ€§èƒ½è¿œå·®äº online distillationï¼Œè¯´æ˜ teacher åˆ¤æ–­æ›´æœ‰æ•ˆã€‚ |
| **Bidirectional Attention (BDA)** | å…è®¸ MTP åŒºåŸŸå†…åŒå‘æ³¨æ„åŠ›ï¼Œæœªå¸¦æ¥æ˜æ˜¾æå‡ï¼Œè¯´æ˜å› æœç»“æ„å·²è¶³å¤Ÿã€‚ |
| **Soft Teacher (softmax logits)** | ä½¿ç”¨ soft teacher è¾“å‡ºï¼ˆé argmaxï¼‰æ•ˆæœä¸å¦‚ hard teacherï¼Œåè€…æ›´èƒ½æ¨åŠ¨ä½ç†µæ”¶æ•›ã€‚ |
| **Static $k=16$** | å›ºå®šæœ€å¤§ $k$ å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œæ€§èƒ½ä¸‹é™ï¼Œè¯æ˜éšæœºåŒ– $k$ æ›´ä¼˜ã€‚ |
| **Prefix NTP Loss** | åœ¨é MTP åŒºåŸŸæ·»åŠ é¢å¤– NTP æŸå¤±åè€Œé™ä½æ€§èƒ½ï¼Œè¯´æ˜å•ä¸€ç›®æ ‡æ›´å¹²å‡€é«˜æ•ˆã€‚ |

> âœ… éªŒè¯äº†æœ¬æ–‡è®¾è®¡é€‰æ‹©çš„åˆç†æ€§ï¼š**online distillation + hard teacher + randomized k + no auxiliary loss** æ˜¯æœ€ä½³ç»„åˆã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **MTP å¯ä»¥é€šè¿‡ self-distillation å®ç°é«˜è´¨é‡åŠ é€Ÿ**ï¼š  
   åˆ©ç”¨ teacher model å¯¹å®Œæ•´ token åºåˆ—çš„è¯„åˆ†ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œèƒ½æœ‰æ•ˆå¼•å¯¼å­¦ç”Ÿæ¨¡å‹å­¦ä¹ åˆ°åˆç†çš„è”åˆåˆ†å¸ƒï¼Œå…‹æœä¼ ç»Ÿ MTP çš„è¿è´¯æ€§éš¾é¢˜ã€‚

2. **ConfAdapt ç­–ç•¥å®ç°äº†æ™ºèƒ½åŠ é€Ÿ**ï¼š  
   æ¨¡å‹èƒ½è‡ªåŠ¨è¯†åˆ«â€œç®€å•â€ä¸Šä¸‹æ–‡å¹¶æ‰¹é‡è¾“å‡ºå¤šä¸ª tokenï¼Œè€Œåœ¨å›°éš¾éƒ¨åˆ†é€€å›å•æ­¥è§£ç ï¼Œå®ç°äº† **2â€“5Ã— çš„å®é™…åŠ é€Ÿ**ï¼Œä¸”ç²¾åº¦æŸå¤±æå°ã€‚

3. **æ— éœ€å¤æ‚å·¥ç¨‹å³å¯éƒ¨ç½²é«˜é€Ÿæ¨ç†**ï¼š  
   è¯¥æ–¹æ³•æœ€ç»ˆæ¨¡å‹ä¸åŸæ¨¡å‹å®Œå…¨å…¼å®¹ï¼Œ**æ— éœ€ verifierã€æ— éœ€ä¿®æ”¹æ¨ç†å¼•æ“**ï¼Œæå¤§é™ä½äº†éƒ¨ç½²é—¨æ§›ã€‚

4. **ä½ç†µè¾“å‡ºä¸ç”Ÿæˆè´¨é‡å¼ºç›¸å…³**ï¼š  
   å®éªŒè¡¨æ˜ï¼Œé«˜ç½®ä¿¡åº¦ï¼ˆä½ç†µï¼‰é¢„æµ‹å¾€å¾€å¯¹åº”æ­£ç¡®çš„æ¨ç†è·¯å¾„ï¼Œè¿™ä¸ºæœªæ¥è®¾è®¡æ›´é«˜æ•ˆçš„é‡‡æ ·ç­–ç•¥æä¾›äº†ä¾æ®ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **è®­ç»ƒä¾èµ–å¼º teacher**ï¼šéœ€è¦ä¸€ä¸ªé«˜è´¨é‡çš„ teacher model æ¥æä¾›å¯é åé¦ˆã€‚
- **ç›®å‰ä»…é€‚ç”¨äºç‰¹å®šé¢†åŸŸå¾®è°ƒ**ï¼šå½“å‰å®éªŒé›†ä¸­åœ¨æ•°å­¦æ¨ç†ï¼Œå¼€æ”¾åŸŸç”Ÿæˆï¼ˆå¦‚ CNN DMï¼‰åŠ é€Ÿæœ‰é™ã€‚
- **æœªå®Œå…¨æ¶ˆé™¤ä¸ç¡®å®šæ€§**ï¼šConfAdapt ä»æ˜¯ deterministic çš„ï¼Œç¼ºä¹å¤šæ ·æ€§ï¼Œä¸é€‚åˆåˆ›æ„ç”Ÿæˆã€‚
- **æç«¯é•¿åºåˆ—é¢„æµ‹ä»æœ‰æŒ‘æˆ˜**ï¼šå°½ç®¡é¿å…äº† â€œthe the theâ€¦â€ é€€åŒ–ï¼Œä½†è¶…é•¿è·¨åº¦é¢„æµ‹ä»å—é™ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **è¿›ä¸€æ­¥é™ä½è¾“å‡ºç†µ**ï¼šæ¢ç´¢æ˜¾å¼ç†µæœ€å°åŒ–ç›®æ ‡ï¼Œä½¿æ¨¡å‹èƒ½åœ¨æ›´é«˜ $k$ ä¸‹ç¨³å®šè¾“å‡ºã€‚
2. **å¼•å…¥å¯æ§éšæœºæ€§**ï¼šåœ¨ $k=1$ æ—¶å…è®¸ä» softmax é‡‡æ ·ï¼Œå¢åŠ ç”Ÿæˆå¤šæ ·æ€§ã€‚
3. **Self-Speculative Decoding**ï¼šç”¨ MTP æ¨¡å‹è‡ªèº«ä½œä¸º speculatorï¼Œåœ¨åç»­æ­¥éª¤ä¸­éªŒè¯è‡ªå·±çš„é¢„æµ‹ï¼Œå®ç° lossless åŠ é€Ÿã€‚
4. **è½»é‡åŒ– MTP æ¶æ„**ï¼šæ¢ç´¢æ·»åŠ å°å‹é¢„æµ‹å¤´ï¼ˆå¦‚ Medusaï¼‰è€Œé mask tokenï¼Œæå‡æ•ˆç‡ã€‚
5. **Long-Horizon RL ä¼˜åŒ–**ï¼šé‡‡ç”¨å®Œæ•´çš„ rollout + reward è®¾è®¡ï¼Œè¿›è¡Œç«¯åˆ°ç«¯ç­–ç•¥ä¼˜åŒ–ï¼Œå¯èƒ½è·å¾—æ›´å¤§çªç ´ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡æå‡ºä¸€ç§ç®€æ´é«˜æ•ˆçš„ **self-distillation MTP æ–¹æ³•**ï¼Œé€šè¿‡ teacher model çš„åœ¨çº¿è¯„åˆ¤æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹å­¦ä¹ é«˜è´¨é‡çš„å¤štokenç”Ÿæˆï¼Œåœ¨ **GSM8K ä¸Šå®ç° >3Ã— æ¨ç†åŠ é€Ÿä¸”ç²¾åº¦æŸå¤± <5%**ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿ MTP å¹¶åª²ç¾ speculative decodingï¼ŒåŒæ—¶å…·å¤‡æé«˜çš„éƒ¨ç½²å‹å¥½æ€§ã€‚

</details>

---

### 14. [Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics](https://arxiv.org/abs/2602.04928)

**Authors**: Ruizhe Zhong, Jiesong Lian, Xiaoyue Mi, Zixiang Zhou, Yuan Zhou, Qinglin Lu, Junchi Yan  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.04928v1  

#### Abstract
While online Reinforcement Learning has emerged as a crucial technique for aligning flow matching models with human preferences, current approaches are hindered by inefficient exploration during training rollouts. Relying on undirected stochasticity and sparse outcome rewards, these methods struggle...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šEuphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰åŸºäº **Reinforcement Learning (RL)** å¯¹ **Flow Matching** æ¨¡å‹è¿›è¡Œå¯¹é½è®­ç»ƒçš„æ–¹æ³•ï¼ˆå¦‚ Flow-GRPOã€DanceGRPOï¼‰å­˜åœ¨ä»¥ä¸‹ç“¶é¢ˆï¼š
- **æ¢ç´¢æ•ˆç‡ä½ä¸‹**ï¼šä¾èµ–æ— å¯¼å‘çš„éšæœºå™ªå£°ï¼ˆundirected stochasticityï¼‰åœ¨éšç©ºé—´ä¸­æ¢ç´¢ï¼Œéš¾ä»¥æœ‰æ•ˆå‘ç°é«˜å¥–åŠ±æ ·æœ¬ã€‚
- **åé¦ˆç¨€ç–**ï¼šä»…åœ¨å®Œæ•´è§†é¢‘ç”Ÿæˆåè·å¾— outcome rewardï¼Œç¼ºä¹ä¸­é—´è¿‡ç¨‹çš„å¯†é›†ç›‘ç£ä¿¡å·ã€‚
- **è®­ç»ƒæ”¶æ•›æ…¢**ï¼šç”±äºä½æ•ˆæ¢ç´¢ï¼Œå¯¼è‡´ä¼˜åŒ–è¿‡ç¨‹æ•°æ®åˆ©ç”¨ç‡ä½ã€æ”¶æ•›é€Ÿåº¦ç¼“æ…¢ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ ¸å¿ƒæ€æƒ³
ä½œè€…æå‡º **Euphonium**ï¼Œä¸€ç§é€šè¿‡ **Process Reward Gradient (PRG)** å¼•å¯¼éšæœºåŠ¨åŠ›å­¦çš„æ–°å‹æ¡†æ¶ï¼Œå®ç°å¯¹è§†é¢‘ç”Ÿæˆæµç¨‹çš„ä¸»åŠ¨å¼•å¯¼ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰**Guided Exploration via Process Reward Gradient**
- å°†é‡‡æ ·è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªç†è®ºä¸¥è°¨çš„ **Stochastic Differential Equation (SDE)**ï¼Œæ˜¾å¼åœ°å°† **Process Reward Model (PRM)** çš„æ¢¯åº¦æ³¨å…¥ flow drift ä¸­ï¼š
  $$
  dX_t = \left[u_\theta(X_t,t) - \frac{\epsilon_t}{\beta}\nabla_x r_p(X_t,t)\right]dt + \sqrt{2\epsilon_t}dW_t
  $$
- åˆ©ç”¨ **Non-Equilibrium Transport Sampling (NETS)** å½¢å¼åŒ–æ¨å¯¼è¯¥ SDEï¼Œä½¿æ¯ä¸€æ­¥ç”Ÿæˆéƒ½å—åˆ°è¿‡ç¨‹å¥–åŠ±æ¢¯åº¦çš„â€œæ‹‰åŠ›â€ï¼Œå‘é«˜å¥–åŠ±åŒºåŸŸä¸»åŠ¨æ¨è¿›ã€‚
- è¯¥è®¾è®¡å®ç°äº† **dense, step-by-step steering**ï¼Œæ˜¾è‘—æå‡æ¢ç´¢æ•ˆç‡ã€‚

#### ï¼ˆ2ï¼‰**Dual-Reward Optimization**
- å¼•å…¥åŒå¥–åŠ±æœºåˆ¶ï¼š
  - **Latent PRM**ï¼šæä¾›ä¸­é—´æ­¥éª¤çš„è´¨é‡è¯„ä¼°ï¼Œç”¨äºé«˜æ•ˆä¿¡ç”¨åˆ†é…ï¼ˆcredit assignmentï¼‰ã€‚
  - **Pixel-level ORM (Outcome Reward Model)**ï¼šè¯„ä¼°æœ€ç»ˆè§†é¢‘è´¨é‡ï¼Œç¡®ä¿è§†è§‰ä¿çœŸåº¦ã€‚
- åœ¨ GRPO æ¡†æ¶ä¸‹èåˆä¸¤è€…ä¼˜åŠ¿ï¼Œå…¼é¡¾è¿‡ç¨‹æ§åˆ¶ä¸ç»“æœè´¨é‡ã€‚

#### ï¼ˆ3ï¼‰**Reward-Gradient-Free Inference via Policy Distillation**
- ä¸ºé¿å…æ¨ç†æ—¶éœ€åŠ è½½å¤–éƒ¨ PRM å¯¼è‡´ç³»ç»Ÿå¤æ‚æ€§å’Œå†…å­˜å¼€é”€å¢åŠ ï¼Œæå‡º **Policy Distillation Objective**ï¼š
  - å°†å¸¦å¥–åŠ±å¼•å¯¼çš„è½¨è¿¹è§†ä¸ºâ€œæ•™å¸ˆâ€è¡Œä¸ºã€‚
  - è®­ç»ƒ flow networkï¼ˆå­¦ç”Ÿï¼‰å»æ‹Ÿåˆè¿™äº›è¢«å¼•å¯¼çš„è½¨è¿¹ï¼Œå³ä½¿å…¶è‡ªèº«ä¸è®¡ç®— $\nabla_x r_p$ã€‚
- æœ€ç»ˆéƒ¨ç½²æ¨¡å‹å¯å®Œå…¨è„±ç¦» PRMï¼Œä¿æŒä¸åŸºç¡€ç”Ÿæˆå™¨ç›¸åŒçš„æ¨ç†æ¨¡å¼ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç‰¹æ€§ | Euphonium | Flow-GRPO / DanceGRPO |
|------|-----------|------------------------|
| æ¢ç´¢æ–¹å¼ | ä¸»åŠ¨å¼•å¯¼ï¼ˆreward gradient guidanceï¼‰ | æ— å¯¼å‘éšæœºæ¢ç´¢ |
| å¥–åŠ±å¯†åº¦ | å¯†é›†è¿‡ç¨‹å¥–åŠ± + ç»“æœå¥–åŠ± | ä»…ç»“æœå¥–åŠ±ï¼ˆç¨€ç–ï¼‰ |
| æ”¶æ•›é€Ÿåº¦ | å¿« 1.66Ã— | è¾ƒæ…¢ |
| æ¨ç†ä¾èµ– | æ— éœ€ PRMï¼ˆdistilledï¼‰ | ä¸ä¾èµ– |
| ç†è®ºç»Ÿä¸€æ€§ | åŒ…å«ç°æœ‰æ–¹æ³•ä½œä¸ºç‰¹ä¾‹ï¼ˆå½“ reward â†’ 0ï¼‰ | æ—  |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **Reward Model è®­ç»ƒæ•°æ®**ï¼š
  - æ„å»ºåŒ…å« **200,000 æ¡è§†é¢‘æ ·æœ¬**çš„æ•°æ®é›†ï¼Œæ¥è‡ª **20,000 ä¸ªå”¯ä¸€æ–‡æœ¬ prompt**ã€‚
  - ä½¿ç”¨ pointwise äºŒåˆ†ç±»æ ‡æ³¨ï¼ˆæ­£/è´Ÿï¼‰ï¼Œå¹¶æ„é€ æˆ pair-wise æ•°æ®ä»¥è®­ç»ƒåå¥½æ¨¡å‹ã€‚
- **GRPO è®­ç»ƒæ•°æ®**ï¼š
  - ä½¿ç”¨ **10,000 ä¸ªç‹¬ç«‹ prompts**ï¼ˆä¸¥æ ¼æ’é™¤ reward model è®­ç»ƒé›†ä¸­å‡ºç°è¿‡çš„ï¼‰ï¼Œæ¥æºäº DanceGRPO å’Œå†…éƒ¨äººåƒç±»æ•°æ®æºã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **ä¸»å¹²æ¨¡å‹**ï¼šåŸºäºå¼€æº **HunyuanVideo-14B**ï¼ˆDiT æ¶æ„ï¼‰ã€‚
- **Process Reward Model (PRM)**ï¼š
  - è½»é‡çº§ DiTï¼ˆ8 å±‚ï¼‰ï¼Œç›´æ¥ä½œç”¨äº VAE latent spaceã€‚
  - è¾“å…¥ï¼šlatent $x_t$ã€æ—¶é—´æ­¥ $t$ã€text embedding $c$ã€‚
  - å†»ç»“ VAE å’Œ text encoderã€‚
- **Outcome Reward Model (ORM)**ï¼š
  - åŸºäº InternVL3-1B å¾®è°ƒï¼Œç”¨äºåƒç´ ç©ºé—´è´¨é‡è¯„åˆ†ã€‚
- **è®­ç»ƒç›®æ ‡**ï¼š
  - PRM ä½¿ç”¨ **Bradley-Terry æ¨¡å‹** è¿›è¡Œ pair-wise æ’åºå­¦ä¹ ã€‚
  - Flow network ä½¿ç”¨ **Group Relative Policy Optimization (GRPO)** æ›´æ–°ï¼Œç»“åˆ dual-reward advantagesã€‚

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
- ä¸»è¦è¯„æµ‹åŸºå‡†ï¼š**VBench2**ï¼ˆæ¶µç›–äº”å¤§ç»´åº¦ï¼‰ï¼š
  - Creativity
  - Commonsense
  - Controllability
  - Human Fidelity
  - Physics
- æŠ¥å‘Š **Total Score** åŠå„å­é¡¹å¾—åˆ†ã€‚
- é¢å¤–åˆ†æè®­ç»ƒæ•ˆç‡ï¼ˆconvergence speedï¼‰ã€è®¡ç®—å¼€é”€ï¼ˆlatency/memoryï¼‰ã€‚

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Base Model**ï¼šåŸå§‹é¢„è®­ç»ƒ HunyuanVideoã€‚
- **Flow-GRPO**ï¼šé¦–ä¸ªå°† GRPO åº”ç”¨äº flow matching çš„æ–¹æ³•ã€‚
- **DanceGRPO**ï¼šå¼•å…¥å…±äº«å™ªå£°ç­–ç•¥æ”¹è¿›ç­–ç•¥è¯„ä¼°ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆVBench2 æ€»åˆ†ï¼‰
| Method | Total Score |
|--------|------------|
| Base Model | 51.09 |
| Flow-GRPO | 51.52 |
| DanceGRPO | 51.85 |
| **Euphonium (Ours)** | **54.24** âœ… |

> **æå‡å¹…åº¦**ï¼šç›¸æ¯”æœ€å¼ºåŸºçº¿ DanceGRPO æå‡ **+2.39 pts**ï¼Œç»å¯¹é¢†å…ˆã€‚

### ğŸ† å„ç»´åº¦è¡¨ç°ï¼ˆTop-1 è¡¨ç°ï¼‰
Euphonium åœ¨ **4/5 ä¸ªç»´åº¦ä¸Šæ’åç¬¬ä¸€**ï¼š
- **Commonsense**: 67.17 â†’ æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•
- **Controllability**: 26.88
- **Human Fidelity**: 88.91
- **Physics**: 46.84
- **Creativity**: 41.42ï¼ˆä»…æ¬¡äº Flow-GRPO çš„ 42.42ï¼Œä½†ä»å…·ç«äº‰åŠ›ï¼‰

### â±ï¸ è®­ç»ƒæ•ˆç‡
- **æ”¶æ•›é€Ÿåº¦å¿« 1.66Ã—**ï¼šè¾¾åˆ°åŒç­‰æ€§èƒ½æ‰€éœ€è®­ç»ƒæ­¥æ•°å‡å°‘ 40%ã€‚
- å›¾å½¢åŒ–å±•ç¤ºï¼ˆFigure 1ï¼‰æ˜¾ç¤º Euphonium æ›´å¿«é€¼è¿‘æœ€ä¼˜ rewardã€‚

### ğŸ’° è®¡ç®—å¼€é”€åˆ†æï¼ˆTable 2ï¼‰
| æŒ‡æ ‡ | Baseline | w/ RGG | å¼€é”€ |
|------|---------|--------|-----|
| å•æ­¥å»¶è¿Ÿ (ms/step) | 1005.6 | 1030.1 | **+2.4%** |
| å³°å€¼æ˜¾å­˜ (VRAM) | 32.9 GB | 35.7 GB | **+8.5%** |

> è½»é‡ PRM è®¾è®¡å¸¦æ¥æä½é¢å¤–å¼€é”€ï¼Œå…·å¤‡å®é™…åº”ç”¨ä»·å€¼ã€‚

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

#### ï¼ˆ1ï¼‰ç§»é™¤ä¸»åŠ¨å¼•å¯¼ï¼ˆw/o Active Steeringï¼‰
| è®¾ç½® | VBench2 Total |
|------|---------------|
| Full Euphonium | 54.24 |
| w/o Reward Gradient Guidance | 53.61 â†“ |

> è¯´æ˜ **reward gradient æ˜¯æ€§èƒ½å¢ç›Šçš„å…³é”®æ¥æº**ã€‚

#### ï¼ˆ2ï¼‰åŒå¥–åŠ±æ¶ˆè
| è®¾ç½® | VBench2 Total |
|------|---------------|
| w/o PRM Advantage | 53.95 |
| w/o ORM Advantage | 53.59 â†“ |

> ORM å¯¹è§†è§‰ä¿çœŸè‡³å…³é‡è¦ï¼›PRM æä¾›ç²¾ç»†è¿‡ç¨‹æ§åˆ¶ã€‚

#### ï¼ˆ3ï¼‰Reward Guidance å‚æ•°æ•æ„Ÿæ€§
- **æŒ‡å¯¼å¼ºåº¦ $\lambda$**ï¼ˆå¯¹åº” reward gradient ç¼©æ”¾ç³»æ•°ï¼‰ï¼š
  | $\lambda$ | Score |
  |----------|-------|
  | 0.01 | 53.61 |
  | **0.1** | **54.24** âœ… |
  | 1.0 | 52.86 |

- **æ—¶é—´æ¿€æ´»çª—å£**ï¼ˆä½•æ—¶æ–½åŠ å¼•å¯¼ï¼‰ï¼š
  | çª—å£ | æ­¥éª¤èŒƒå›´ | Score |
  |------|--------|-------|
  | å…¨ç¨‹ (0â‰¤tâ‰¤1) | 0â†’16 | 53.64 |
  | ååŠæ®µ (0.5â‰¤tâ‰¤1) | 8â†’16 | **54.24** âœ… |
  | æ™šæœŸ (0.75â‰¤tâ‰¤1) | 12â†’16 | 54.14 |

> **ååŠæ®µå¼•å¯¼æ•ˆæœæœ€ä½³**ï¼šé¿å¼€æ—©æœŸç»“æ„å½¢æˆé˜¶æ®µæ‰°åŠ¨ï¼Œä¿ç•™è¶³å¤Ÿ refine æ—¶é—´ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **ä¸»åŠ¨å¼•å¯¼ä¼˜äºè¢«åŠ¨æ¢ç´¢**ï¼š
   - æ³¨å…¥ **Process Reward Gradient** æ˜¾è‘—æå‡æ¢ç´¢æ•ˆç‡ï¼Œæ˜¯åŠ é€Ÿæ”¶æ•›çš„æ ¸å¿ƒæœºåˆ¶ã€‚
2. **åŒå¥–åŠ±ååŒå¢æ•ˆ**ï¼š
   - **Latent PRM** æä¾›é«˜æ•ˆè¿‡ç¨‹ç›‘ç£ï¼Œ**Pixel ORM** ä¿è¯æœ€ç»ˆè´¨é‡ï¼ŒäºŒè€…ç¼ºä¸€ä¸å¯ã€‚
3. **Policy Distillation å®ç°é«˜æ•ˆéƒ¨ç½²**ï¼š
   - æˆåŠŸå°†å¤–éƒ¨å¼•å¯¼ä¿¡å·å†…åŒ–åˆ° flow network æƒé‡ä¸­ï¼Œå®ç° **zero-shot reward-gradient-free inference**ã€‚
4. **ç†è®ºç»Ÿä¸€æ€§**ï¼š
   - Euphonium æ¡†æ¶åœ¨æ•°å­¦ä¸Šç»Ÿä¸€äº† Flow-GRPOã€DanceGRPO ç­‰æ–¹æ³•ï¼ˆå½“ reward term vanish æ—¶é€€åŒ–ä¸ºå…¶å½¢å¼ï¼‰ã€‚

### âš ï¸ æ–¹æ³•å±€é™æ€§
1. **Latent PRM çš„æ³›åŒ–èƒ½åŠ›æœ‰é™**ï¼š
   - å½“å‰ PRM ç´§è€¦åˆäºç‰¹å®š VAE çš„ latent spaceï¼Œéš¾ä»¥è·¨æ¶æ„è¿ç§»ã€‚
2. **Latent Space Reward çš„å¯é æ€§æŒ‘æˆ˜**ï¼š
   - å°½ç®¡ PRM åœ¨å¤šä¸ªå™ªå£°çº§åˆ«ä¿æŒ >70% å‡†ç¡®ç‡ï¼ˆFigure 3ï¼‰ï¼Œä½†å…¶é²æ£’æ€§ä»å¼±äº pixel-space ORMã€‚
3. **ä¾èµ–é«˜è´¨é‡åå¥½æ•°æ®**ï¼š
   - æ–¹æ³•æ€§èƒ½å—é™äº PRM å’Œ ORM çš„è®­ç»ƒæ•°æ®è´¨é‡å’Œæ ‡æ³¨ä¸€è‡´æ€§ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **Representation Autoencoders (RAE)** æˆ–å›ºå®šè§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ DINOv2ï¼‰æ„å»ºé€šç”¨ latent spaceï¼Œå‘å±• **backbone-agnostic PRM**ã€‚
- æ‰©å±•è‡³å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚éŸ³é¢‘-è§†é¢‘è”åˆç”Ÿæˆï¼‰ä¸­çš„è¿‡ç¨‹æ§åˆ¶ã€‚
- ç ”ç©¶æ›´é«˜æ•ˆçš„ distillation ç­–ç•¥ï¼Œè¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹è§„æ¨¡ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **Euphonium é€šè¿‡å¼•å…¥ process reward gradient guided SDE å®ç°äº†å¯¹ flow matching è§†é¢‘ç”Ÿæˆçš„â€œä¸»åŠ¨å¯¼èˆªâ€ï¼Œåœ¨ä¸ç‰ºç‰²æ¨ç†æ•ˆç‡çš„å‰æä¸‹ï¼Œå¤§å¹…æå‡äº†å¯¹é½è´¨é‡å’Œè®­ç»ƒé€Ÿåº¦ï¼Œä¸ºå¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ post-training alignment æä¾›äº†æ–°èŒƒå¼ã€‚**

</details>

---

### 15. [E-Globe: Scalable $\epsilon$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching](https://arxiv.org/abs/2602.05068)

**Authors**: Wenting Li, Saif R. Kazi, Russell Bent, Duo Zhou, Huan Zhang  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.05068v1  

#### Abstract
Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# E-Globe: Scalable $\epsilon$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching  
**æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç¥ç»ç½‘ç»œåœ¨å®‰å…¨å…³é”®é¢†åŸŸï¼ˆå¦‚ç”µåŠ›ç³»ç»Ÿï¼‰çš„åº”ç”¨å—é™äºå…¶é²æ£’æ€§éªŒè¯çš„å›°éš¾ã€‚ç°æœ‰çš„ **neural network verification (VNN)** æ–¹æ³•é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- **Complete verifiers**ï¼ˆå¦‚åŸºäº MIP çš„æ–¹æ³•ï¼‰è™½ç„¶èƒ½æä¾›å½¢å¼åŒ–ä¿è¯ï¼Œä½†è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§å‹ç½‘ç»œã€‚
- **Incomplete verifiers**ï¼ˆå¦‚åŸºäºæ¾å¼›çš„æ–¹æ³•æˆ– PGD æ”»å‡»ï¼‰è¦ä¹ˆæ— æ³•é‡åŒ–æœ€ä¼˜æ€§é—´éš™ï¼ˆoptimality gapï¼‰ï¼Œè¦ä¹ˆå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œå¯¼è‡´ä¸Šç•Œè¿‡æ¾ã€‚

æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€ **scalability-completeness trade-off**ï¼Œæå‡ºä¸€ç§é«˜æ•ˆä¸”æ¥è¿‘å…¨å±€æœ€ä¼˜çš„éªŒè¯æ¡†æ¶ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

ä½œè€…æå‡ºäº† **E-Globe** â€”â€” ä¸€ä¸ªåŸºäº **branch-and-bound (BaB)** æ¡†æ¶çš„æ··åˆéªŒè¯å™¨ï¼Œé€šè¿‡åŒå‘ç´§ç¼©ä¸Šä¸‹ç•Œå®ç° $\epsilon$-global verificationã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š

1. **NLP-CC ä¸Šç•Œæ±‚è§£ï¼ˆTight Upper Boundingï¼‰**
   - å°†æ¯ä¸ª ReLU å±‚ç²¾ç¡®å»ºæ¨¡ä¸º **complementarity constraints (CC)**ï¼Œæ„å»ºä¸€ä¸ªéçº¿æ€§è§„åˆ’é—®é¢˜ï¼ˆNLP-CCï¼‰ã€‚
   - è¯¥æ¨¡å‹æ˜¯åŸ ReLU ç½‘ç»œçš„**ç²¾ç¡®é‡æ„**ï¼ˆexact reformulationï¼‰ï¼Œä»»ä½•å¯è¡Œè§£éƒ½å¯¹åº”ä¸€ä¸ªæœ‰æ•ˆçš„ç½‘ç»œè¾“å‡ºï¼Œä»è€Œäº§ç”Ÿ**æœ‰æ•ˆä¸Šç•Œ** $u \geq f^*$ã€‚
   - å¯ç”Ÿæˆå…·ä½“åä¾‹ï¼ˆcounterexampleï¼‰ï¼Œæ”¯æŒæ—©æœŸç»ˆæ­¢ï¼ˆearly stoppingï¼‰ã€‚

2. **Warm-started NLP with Low-Rank KKT Updates**
   - åœ¨ BaB è¿‡ç¨‹ä¸­å¤ç”¨çˆ¶èŠ‚ç‚¹çš„ primal-dual è§£ä½œä¸º warm-startã€‚
   - åˆ©ç”¨åˆ†æ”¯ä»…æ”¹å˜å°‘é‡ç¥ç»å…ƒçŠ¶æ€çš„ç‰¹ç‚¹ï¼Œå¯¹ KKT ç³»ç»Ÿè¿›è¡Œ**ä½ç§©æ›´æ–°**ï¼ˆrank â‰¤ 4ï¼‰ï¼Œæ˜¾è‘—åŠ é€Ÿå­é—®é¢˜æ±‚è§£ã€‚
   - å®è·µä¸­å¸¦æ¥ **2â€“5Ã— åŠ é€Ÿ**ã€‚

3. **Pattern-Aligned Strong Branching**
   - å¼•å…¥æ¥è‡ª NLP-CC è§£çš„æ¿€æ´»æ¨¡å¼ï¼ˆactivation patternï¼‰æŒ‡å¯¼åˆ†æ”¯ç­–ç•¥ã€‚
   - åˆ†æ”¯è¯„åˆ†å‡½æ•°èåˆä¼ ç»Ÿ filtered smart branching å¾—åˆ†ä¸å½“å‰æ¿€æ´»æ¨¡å¼çš„ä¸€è‡´æ€§å¾—åˆ†ï¼š
     $$
     s_a(C_i) = s(C_i) + \lambda \cdot m(\alpha(C_i), \alpha_{\text{NLP}})
     $$
   - æ›´æœ‰æ•ˆåœ°å¼•å¯¼æœç´¢å‘æœ€å¯èƒ½åŒ…å«å…¨å±€æœ€ä¼˜çš„åŒºåŸŸã€‚

4. **$\epsilon$-Global Verification Criterion**
   - å½“ä¸Šä¸‹ç•Œå·®è· $u - l \leq \epsilon$ æ—¶åœæ­¢ï¼Œè¿”å› $\epsilon$-optimal certificateã€‚
   - è‹¥ $u < 0$ï¼Œç«‹å³åˆ¤å®šä¸º **Unsafe**ï¼›è‹¥ $l > 0$ï¼Œåˆ¤å®šä¸º **Safe**ã€‚
   - æ‰€æœ‰ç»“æœå‡é™„å¸¦å¯è§£é‡Šçš„å®‰å…¨è£•åº¦ï¼ˆsafety marginï¼‰ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | E-Globe | MIP-Based | PGD | Relaxation-Based |
|------|--------|----------|-----|------------------|
| ä¸Šç•Œè´¨é‡ | âœ… æç´§ï¼ˆæ¥è¿‘å…¨å±€æœ€ä¼˜ï¼‰ | âŒ æ±‚è§£æ…¢ã€éš¾æ”¶æ•› | âŒ æ¾æ•£ã€å¸¸å¤±è´¥ | ä¸é€‚ç”¨ |
| ä¸‹ç•Œè´¨é‡ | âœ… ç”± B-CROWN ä¿éšœ | âœ… å…¨å±€æœ€ä¼˜ | âŒ æ— ä¸‹ç•Œ | âœ… å¯é ä½†è¾ƒæ¾ |
| å¯æ‰©å±•æ€§ | âœ… å¤šé¡¹å¼æ—¶é—´è¶‹åŠ¿ | âŒ æŒ‡æ•°å¢é•¿ï¼ˆéš binary varsï¼‰ | âœ… å¿« | âœ… å¿« |
| å®é™…æ•ˆç‡ | âœ… æ•°é‡çº§å¿«äº MIP | âŒ è¶…æ—¶å¸¸è§ | âœ… å¿«ä½†ä¸å¯é  | âœ… å¿«ä½† gap å¤§ |
| è¾“å‡ºä¿¡æ¯ | âœ… å®‰å…¨/ä¸å®‰å…¨ + $\epsilon$-gap | âœ… å½¢å¼åŒ–è¯æ˜ | âŒ ä»…åä¾‹æˆ–å¤±è´¥ | âŒ ä»…ä¸‹ç•Œ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **MNIST**ï¼šè¾“å…¥ç»´åº¦ 784ï¼Œä½¿ç”¨ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼ˆNoSoftmaxNetï¼Œæ¯å±‚ 50 ç¥ç»å…ƒï¼‰
- **CIFAR-10**ï¼šè¾“å…¥ç»´åº¦ 3072ï¼Œä¸¤å±‚å…¨è¿æ¥ ReLU MLPï¼ˆ256 â†’ 256ï¼‰

> æ‰€æœ‰æ¨¡å‹å‡ä¸ºæ ‡å‡†è®­ç»ƒï¼Œæœªä½¿ç”¨ç‰¹æ®Šæ­£åˆ™åŒ–ä»¥ä¿æŒé€šç”¨æ€§ã€‚

---

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### è¯„ä¼°åœºæ™¯
- è¾“å…¥æ‰°åŠ¨èŒƒå›´ï¼š$\ell_\infty$ çƒï¼ŒåŠå¾„ $\delta \in \{0.01, 0.03, 0.1\}$
- éªŒè¯ç›®æ ‡ï¼šæœ€å°åˆ†ç±» margin æ˜¯å¦éè´Ÿï¼ˆå³ robustnessï¼‰

#### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ | è¯´æ˜ |
|------|------|------|
| $\Delta_\delta = |u - f^*|$ æˆ– $|l - f^*|$ | ç»å¯¹è¯¯å·® | è¡¡é‡ bound tightness |
| $\Delta_\delta / |f^*|$ | ç›¸å¯¹è¯¯å·® | å½’ä¸€åŒ–æ¯”è¾ƒ |
| ä¸Šç•Œå®šç‡ $\phi(\%)$ | æˆåŠŸè·å¾—ä¸Šç•Œçš„æ¡ˆä¾‹æ¯”ä¾‹ | è¡¡é‡æ”»å‡»æˆåŠŸç‡ |
| Runtime (s) | å•æ¬¡æ±‚è§£è€—æ—¶ | æ•ˆç‡å¯¹æ¯” |
| $|I_0|$ | ä¸æ»¡è¶³ä¸¥æ ¼äº’è¡¥æ€§çš„ç¥ç»å…ƒæ•°é‡ | è¡¡é‡ NLP-CC è§£çš„è´¨é‡ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»å‹ | æ–¹æ³• |
|------|------|
| **Lower Bound Propagation** | CROWN-IBP, CROWN, $\alpha$-CROWN |
| **Upper Bound (Attack)** | PGD |
| **Complete Verifier** | MIPï¼ˆGurobi + a-CROWN åˆå§‹åŒ–ï¼‰ |
| **Hybrid Baseline** | B-CROWN + standard FSB branching |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### è¡¨ 1ï¼šMNIST ($\delta=0.1$)ï¼Œå¹³å‡è¡¨ç°ï¼ˆ10 å›¾åƒï¼‰
| Method | $\Delta_{0.1}$ (rel) | Time (s) |
|--------|-----------------------|-----------|
| CROWN-IBP | 9.69Ã— | 0.0029 |
| CROWN | 5.75Ã— | 0.0022 |
| $\alpha$-CROWN | 5.02Ã— | 0.1119 |
| **E-Globe$_u$** | **0.039Ã—** | **1.3624** |

> â¤ E-Globe$_u$ ä¸Šç•Œç›¸å¯¹è¯¯å·®ä»…ä¸º 3.9%ï¼Œè¿œä¼˜äºæ‰€æœ‰æ¾å¼›æ–¹æ³•ï¼ˆ>500% å·®è·ï¼‰

#### è¡¨ 2ï¼šMNIST ($\delta=0.01$)
| Method | $\Delta_{0.01}$ (rel) | Time (s) |
|--------|------------------------|-----------|
| $\alpha$-CROWN | 0.0681Ã— | 0.096 |
| **E-Globe$_u$** | **4.04e-5Ã— (~0)** | **0.3808** |

> â¤ åœ¨å°æ‰°åŠ¨ä¸‹å‡ ä¹è¾¾åˆ°ç†è®ºæœ€ä¼˜ä¸Šç•Œ

#### è¡¨ 3 & 4ï¼šCIFAR-10 ($\delta=0.01, 0.03$)
| Method | $\phi(\%)$ | $\Delta_\delta$ (abs) |
|--------|------------|------------------------|
| PGD | 21â€“42% | 0.17â€“0.204 |
| **E-Globe$_u$** | **100%** | **0.0005â€“0.003** |

> â¤ PGD ç»å¸¸æ‰¾ä¸åˆ°åä¾‹ï¼ˆä¸Šç•Œå®šç‡ä½ï¼‰ï¼Œè€Œ E-Globe$_u$ å§‹ç»ˆæˆåŠŸä¸”ä¸Šç•Œæç´§

---

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ç»“æœ

#### âœ… å¯¹æ¯” MIP
- **è¿è¡Œæ—¶é—´**ï¼šå½“ binary variables > 120 æ—¶ï¼ŒMIP å¹³å‡æ¯” E-Globe$_u$ æ…¢ **ä¸¤ä¸ªæ•°é‡çº§**ã€‚
- **å¯æ‰©å±•æ€§**ï¼šMIP æ—¶é—´éš binary vars æŒ‡æ•°å¢é•¿ï¼›E-Globe$_u$ æ˜¾ç¤ºè¿‘ä¼¼å¤šé¡¹å¼è¶‹åŠ¿ã€‚
- **å…¸å‹æ¡ˆä¾‹**ï¼šæŸ CIFAR-10 å®ä¾‹ MIP è€—æ—¶ >2000 ç§’ï¼ŒE-Globe åœ¨ $\epsilon=0.1$ ä¸‹ä»…éœ€ ~20 ç§’ï¼Œæé€Ÿ **>100Ã—**ã€‚

#### âœ… å¯¹æ¯” PGD
- PGD ä¸Šç•Œå¹³å‡æ¯”çœŸå® $f^*$ é«˜å‡º 0.2 ä»¥ä¸Šï¼Œè€Œ E-Globe$_u$ æ§åˆ¶åœ¨ 0.005 å†…ã€‚
- PGD åœ¨æ— åä¾‹æ—¶æ— æ³•æä¾›ä¸Šç•Œï¼ˆ$\phi < 50\%$ï¼‰ï¼ŒE-Globe$_u$ å§‹ç»ˆè¿”å›æœ‰æ•ˆä¸Šç•Œã€‚

#### âœ… æ¶ˆèå®éªŒç»“æœ

##### (1) Warm-start åŠ é€Ÿæ•ˆæœï¼ˆå›¾ 7ï¼‰
- ä½¿ç”¨ warm-start åï¼Œæ¯æ¬¡ NLP-CC æ±‚è§£æ—¶é—´ä¸‹é™ **2â€“5Ã—**ã€‚
- ç‰¹åˆ«æ˜¯åœ¨åç»­åˆ†æ”¯ä¸­ï¼Œwarm-start å‡ ä¹ä½¿ Newton æ­¥éª¤æ”¶æ•›äºå‡ æ­¥ä¹‹å†…ã€‚

##### (2) Pattern-Aligned Branchingï¼ˆå›¾ 9ï¼‰
- å¼•å…¥ $\lambda > 0$ åï¼Œlower bound ä¸Šå‡é€Ÿåº¦æ˜æ˜¾åŠ å¿«ã€‚
- æœ€ä½³å‚æ•° $\lambda = 0.1$ï¼Œåœ¨çº¦ 500 è½®åå°† gap ç¼©å°è‡³ baseline çš„ä¸€åŠã€‚

##### (3) GPU Batch åŠ é€Ÿï¼ˆå›¾ 10ï¼‰
- ä½¿ç”¨ GPU æ‰¹å¤„ç†æ‰§è¡Œ B-CROWNï¼Œlower bound æ”¶æ•›é€Ÿåº¦å¤§å¹…æå‡ã€‚
- åœ¨ç›¸åŒåˆ†è£‚æ¬¡æ•°ä¸‹ï¼ŒGPU ç‰ˆæœ¬æ¯” CPU å¿« **3â€“5Ã—**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **NLP-CC æ˜¯é«˜è´¨é‡ä¸Šç•Œçš„æœ‰æ•ˆæ¥æº**ï¼š
   - å°½ç®¡æ˜¯éå‡¸ä¼˜åŒ–ï¼Œä½†å®è·µä¸­ NLP-CC è¿”å›çš„å±€éƒ¨æœ€ä¼˜å€¼éå¸¸æ¥è¿‘å…¨å±€æœ€ä¼˜ï¼ˆ$|I_0| < 5$ï¼Œé€šå¸¸è¿œå°äº binary vars æ•°é‡ï¼‰ã€‚
   - æä¾›äº†æ¯” PGD æ›´å¯é ã€æ›´ç´§çš„ä¸Šç•Œã€‚

2. **Pattern Guidance æ˜¾è‘—æå‡ BaB æ•ˆç‡**ï¼š
   - æ¥è‡ª NLP-CC çš„ activation pattern æ˜¯é¢„æµ‹ worst-case region çš„å¼ºä¿¡å·ã€‚
   - pattern-aligned branching èƒ½æ›´æ—©åœ°èšç„¦äºå…³é”®å­ç©ºé—´ï¼Œå‡å°‘æ— æ•ˆæ¢ç´¢ã€‚

3. **Warm-start + Low-Rank Update æå¤§é™ä½ NLP å¼€é”€**ï¼š
   - åˆ†æ”¯é—´æ¨¡å‹å˜åŒ–ç¨€ç–ï¼Œé€‚åˆå¢é‡æ±‚è§£ã€‚
   - æ˜¯å®ç°å®æ—¶ BaB æµç¨‹çš„å…³é”®å·¥ç¨‹ä¼˜åŒ–ã€‚

4. **E-Globe å®ç°äº†è‰¯å¥½çš„ç²¾åº¦-æ•ˆç‡æƒè¡¡**ï¼š
   - åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å¯åœ¨æ•°ç§’å†…å®ŒæˆéªŒè¯ã€‚
   - å¯¹äºéš¾ä»¥å®Œå…¨éªŒè¯çš„å®ä¾‹ï¼Œä»èƒ½æä¾› tight $\epsilon$-certificate å’Œå®‰å…¨è£•åº¦ä¼°è®¡ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
1. **ç†è®ºä¸Šå±äº incomplete verifier**ï¼š
   - è™½ç„¶å®è·µä¸­å‡ ä¹æ€»èƒ½æ”¶æ•›ï¼Œä½†ä¸èƒ½ä¿è¯å¯¹æ‰€æœ‰å®ä¾‹å®ŒæˆéªŒè¯ã€‚
2. **ä¾èµ– NLP æ±‚è§£å™¨ç¨³å®šæ€§**ï¼š
   - IPOPT ç­‰æ±‚è§£å™¨å¯èƒ½å› æ•°å€¼é—®é¢˜å¤±è´¥ï¼Œéœ€ careful tuningã€‚
3. **ç›®å‰ä¸»è¦é€‚ç”¨äº ReLU ç½‘ç»œ**ï¼š
   - å¯¹ Swishã€GeLU ç­‰éåˆ†æ®µçº¿æ€§æ¿€æ´»çš„æ”¯æŒæœ‰é™ï¼ˆå°½ç®¡æ–‡ä¸­æåˆ°å¯æ‰©å±•ï¼‰ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **é›†æˆæ›´å¼ºçš„ local solver**ï¼š
   - æ¢ç´¢ç»“åˆ SQPã€trust-region æ–¹æ³•è¿›ä¸€æ­¥æé«˜ NLP-CC æ±‚è§£é²æ£’æ€§ã€‚
2. **æ›´å¤§è§„æ¨¡ç½‘ç»œéªŒè¯**ï¼š
   - å°† E-Globe æ‰©å±•è‡³ CNNã€ResNet ç­‰ç»“æ„ï¼Œå¹¶ç ”ç©¶åˆ†å¸ƒå¼ BaBã€‚
3. **real-time verification pipeline**ï¼š
   - ç»“åˆ early stopping ä¸ adaptive $\epsilon$ï¼Œç”¨äºåœ¨çº¿å®‰å…¨ç›‘æ§ã€‚
4. **äº¤äº’å¼ verification framework**ï¼š
   - åˆ©ç”¨ human-in-the-loop æä¾›å…ˆéªŒ patternï¼Œè¿›ä¸€æ­¥åŠ é€Ÿ branchingã€‚

---

> âœ… **æ€»ä½“è¯„ä»·**ï¼šE-Globe æˆåŠŸå¼¥åˆäº† complete ä¸ incomplete verification ä¹‹é—´çš„é¸¿æ²Ÿï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®ç°äº†å‰æ‰€æœªæœ‰çš„å¯æ‰©å±•æ€§ï¼Œæ˜¯è¿ˆå‘å¤§è§„æ¨¡ç¥ç»ç½‘ç»œå½¢å¼åŒ–éªŒè¯çš„é‡è¦ä¸€æ­¥ã€‚

</details>

---

### 16. [CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers](https://arxiv.org/abs/2602.05243)

**Authors**: Boxiang Zhang, Baijian Yang  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.05243v1  

#### Abstract
Vision Transformers achieve strong accuracy but incur high compute and memory cost. Structured pruning can reduce inference cost, but most methods rely on retraining or multi-stage optimization. These requirements limit post-training deployment. We propose \textbf{CORP}, a closed-form one-shot struc...

---

### 17. [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)

**Authors**: Xiaolin Hu, Hang Yuan, Xinzhu Sang, Binbin Yan, Zhou Yu, Cong Huang, Kai Chen  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.04913v1  

#### Abstract
Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent ...

---

### 18. [Learning, Solving and Optimizing PDEs with TensorGalerkin: an efficient high-performance Galerkin assembly algorithm](https://arxiv.org/abs/2602.05052)

**Authors**: Shizheng Wen, Mingyuan Chi, Tianwei Yu, Ben Moseley, Mike Yan Michelis, Pu Ren, Hao Sun, Siddhartha Mishra  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.05052v1  

#### Abstract
We present a unified algorithmic framework for the numerical solution, constrained optimization, and physics-informed learning of PDEs with a variational structure. Our framework is based on a Galerkin discretization of the underlying variational forms, and its high efficiency stems from a novel hig...

---

### 19. [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)

**Authors**: Xiaofeng Lin, Sirou Zhu, Yilei Chen, Mingyu Chen, Hejian Sang, Ioannis Paschalidis, Zhipeng Wang, Aldo Pacchiano, Xuezhou Zhang  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.04089v1  

#### Abstract
Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction,...

---

### 20. [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)

**Authors**: Zhentao Tang, Yuqi Cui, Shixiong Kai, Wenqian Zhao, Ke Ye, Xing Li, Anxin Tian, Zehua Pei, Hui-Ling Zhen, Shoubo Hu, Xiaoguang Li, Yunhe Wang, Mingxuan Yuan  
**Category**: cs.AI  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.04496v1  

#### Abstract
Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidenc...

---

### 21. [FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters](https://arxiv.org/abs/2602.05235)

**Authors**: Zhilin Liang, Yuxiang Wang, Zimu Zhou, Hainan Zhang, Boyi Liu, Yongxin Tong  
**Category**: cs.CL  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.05235v1  

#### Abstract
Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. Thi...

---

### 22. [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)

**Authors**: Seonghye Cho, Jaemin Han, Hyunjin Kim, Euisoo Jung, Jae-Gil Lee  
**Category**: cs.DC  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.05754v1  

#### Abstract
Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulti...

---

### 23. [SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel](https://arxiv.org/abs/2602.04915)

**Authors**: Jose Miguel Luna, Taha Bouhsine, Krzysztof Choromanski  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.04915v1  

#### Abstract
We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interaction...

---

### 24. [Transolver-3: Scaling Up Transformer Solvers to Industrial-Scale Geometries](https://arxiv.org/abs/2602.04940)

**Authors**: Hang Zhou, Haixu Wu, Haonan Shangguan, Yuezhou Ma, Huikun Weng, Jianmin Wang, Mingsheng Long  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.04940v1  

#### Abstract
Deep learning has emerged as a transformative tool for the neural surrogate modeling of partial differential equations (PDEs), known as neural PDE solvers. However, scaling these solvers to industrial-scale geometries with over $10^8$ cells remains a fundamental challenge due to the prohibitive memo...

---

### 25. [SpectraKAN: Conditioning Spectral Operators](https://arxiv.org/abs/2602.05187)

**Authors**: Chun-Wun Cheng, Carola-Bibiane Sch\"onlieb, Angelica I. Aviles-Rivero  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.05187v1  

#### Abstract
Spectral neural operators, particularly Fourier Neural Operators (FNO), are a powerful framework for learning solution operators of partial differential equations (PDEs) due to their efficient global mixing in the frequency domain. However, existing spectral operators rely on static Fourier kernels ...

---

### 26. [Path-Guided Flow Matching for Dataset Distillation](https://arxiv.org/abs/2602.05616)

**Authors**: Xuhui Li, Zhengquan Luo, Xiwei Liu, Yongqiang Yu, Zhiqiang Xu  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.05616v1  

#### Abstract
Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sam...

---

### 27. [Exact Recovery in the Data Block Model](https://arxiv.org/abs/2602.05852)

**Authors**: Amir R. Asadi, Akbar Davoodi, Ramin Javadi, Farzad Parvaresh  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.05852v1  

#### Abstract
Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact r...

---

### 28. [Regularized Calibration with Successive Rounding for Post-Training Quantization](https://arxiv.org/abs/2602.05902)

**Authors**: Seohyeon Cha, Huancheng Chen, Dongjun Kim, Haoran Zhang, Kevin Chan, Gustavo de Veciana, Haris Vikalo  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.05902v1  

#### Abstract
Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained wei...

---

### 29. [Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training](https://arxiv.org/abs/2602.05933)

**Authors**: Zhenghao Xu, Qin Lu, Changlong Yu, Tuo Zhao  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.05933v1  

#### Abstract
Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable pa...

---

### 30. [Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference](https://arxiv.org/abs/2602.06029)

**Authors**: Yingke Li, Anjali Parashar, Enlu Zhou, Chuchu Fan  
**Category**: cs.LG  
**Published**: 2026-02-06  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.06029v1  

#### Abstract
Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and e...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
