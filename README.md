# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-09-15 12:52:22 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective](https://arxiv.org/abs/2509.10371)

**Authors**: Seokjin Go, Joongun Park, Spandan More, Hanjiang Wu, Irene Wang, Aaron Jezghani, Tushar Krishna, Divya Mahajan  
**Category**: cs.DC  
**Published**: 2025-09-15  
**Score**: 14.5

arXiv:2509.10371v1 Announce Type: new 
Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training workloads far beyond the limits of single-node analysis, demanding a deeper understanding of how these models behave across large-scale, multi-GPU systems. In this paper, we present...

---

### 2. [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)

**Authors**: Jordan Sassoon, Michal Szczepanski, Martyna Poreba  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 10.5

arXiv:2509.10334v1 Announce Type: cross 
Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic segmentation, yet their deployment on resource-constrained devices remains limited due to their high memory footprint and computational cost. Quantization offers an effecti...

---

### 3. [UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs](https://arxiv.org/abs/2406.18173)

**Authors**: Wenhao Li, Mingbao Lin, Yunshan Zhong, Shuicheng Yan, Rongrong Ji  
**Category**: cs.CL  
**Published**: 2025-09-15  
**Score**: 9.0

arXiv:2406.18173v3 Announce Type: replace 
Abstract: Managing long texts is challenging for large language models (LLMs) due to limited context window sizes. This study introduces UIO-LLMs, an unbiased incremental optimization approach for memory-enhanced transformers under long-context settings. We...

---

### 4. [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/abs/2505.19634)

**Authors**: Zili Wang, Tianyu Zhang, Haoli Bai, Lu Hou, Xianzhi Yu, Wulong Liu, Shiming Xiang, Lei Zhu  
**Category**: cs.CL  
**Published**: 2025-09-15  
**Score**: 9.0

arXiv:2505.19634v4 Announce Type: replace 
Abstract: Test-Time Scaling (TTS) has proven effective in improving the performance of Large Language Models (LLMs) during inference. However, existing research has overlooked the efficiency of TTS from a latency-sensitive perspective. Through a latency-awa...

---

### 5. [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)

**Authors**: Ke Wang, Yiming Qin, Nikolaos Dimitriadis, Alessandro Favero, Pascal Frossard  
**Category**: cs.CL  
**Published**: 2025-09-15  
**Score**: 8.5

arXiv:2506.07899v3 Announce Type: replace 
Abstract: Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably-without retraining or forgetting previous information-remains a major ch...

---

### 6. [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)

**Authors**: Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, Feiyu Chen  
**Category**: cs.LG  
**Published**: 2025-09-15  
**Score**: 8.5

arXiv:2509.10396v1 Announce Type: new 
Abstract: Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL ...

---

### 7. [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)

**Authors**: Hossein Yazdanjouei, Arash Mansouri, Mohammad Shokouhifar  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 8.0

arXiv:2509.09750v1 Announce Type: cross 
Abstract: This study proposes a semi-supervised co-training framework for object detection in densely packed retail environments, where limited labeled data and complex conditions pose major challenges. The framework combines Faster R-CNN (utilizing a ResNet ...

---

### 8. [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)

**Authors**: Omar Erak, Omar Alhussein, Hatem Abou-Zeid, Mehdi Bennis, Sami Muhaidat  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 8.0

arXiv:2509.09955v1 Announce Type: cross 
Abstract: Large-scale transformers are central to modern semantic communication, yet their high computational and communication costs hinder deployment on resource-constrained edge devices. This paper introduces a training-free framework for adaptive token me...

---

### 9. [LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing](https://arxiv.org/abs/2508.05672)

**Authors**: Yao Zhao, Yantian Ding, Zhiyue Zhang, Dapeng Yao, Yanxun Xu  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 8.0

arXiv:2508.05672v2 Announce Type: replace-cross 
Abstract: Retrieval Augmented Generation (RAG) systems often struggle with domain-specific knowledge due to performance deterioration of pre-trained embeddings and prohibitive computational costs of large language model (LLM)-based retrievers. While f...

---

### 10. [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)

**Authors**: Shiwei Li, Qunwei Li, Haozhao Wang, Ruixuan Li, Jianbin Lin, Wenliang Zhong  
**Category**: cs.DC  
**Published**: 2025-09-15  
**Score**: 8.0

arXiv:2509.10161v1 Announce Type: cross 
Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm that enables collaborative model training without sharing local data. Despite its advantages, FL suffers from substantial communication overhead, which can affect training ...

---

### 11. [QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads](https://arxiv.org/abs/2505.07531)

**Authors**: Muhammad Ahmad, Khurram Mazher, Saqib Akram, Ahmad Tameem, Saad Bin Nasir  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 7.5

arXiv:2505.07531v2 Announce Type: replace 
Abstract: We present QuantX: a tailored suite of recipes for LLM and VLM quantization. It is capable of quantizing down to 3-bit resolutions with minimal loss in performance. The quantization strategies in QuantX take into account hardware-specific constrai...

---

### 12. [Modelling the 5G Energy Consumption using Real-world Data: Energy Fingerprint is All You Need](https://arxiv.org/abs/2406.16929)

**Authors**: Tingwei Chen, Yantao Wang, Hanzhi Chen, Zijian Zhao, Xinhao Li, Nicola Piovesan, Guangxu Zhu, Qingjiang Shi  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 7.5

arXiv:2406.16929v2 Announce Type: replace-cross 
Abstract: The introduction of 5G technology has revolutionized communications, enabling unprecedented capacity, connectivity, and ultra-fast, reliable communications. However, this leap has led to a substantial increase in energy consumption, presenti...

---

### 13. [Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation](https://arxiv.org/abs/2508.08765)

**Authors**: Andrea Montibeller, Dasara Shullani, Daniele Baracchi, Alessandro Piva, Giulia Boato  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 7.5

arXiv:2508.08765v2 Announce Type: replace-cross 
Abstract: The growing presence of AI-generated videos on social networks poses new challenges for deepfake detection, as detectors trained under controlled conditions often fail to generalize to real-world scenarios. A key factor behind this gap is th...

---

### 14. [MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining](https://arxiv.org/abs/2509.06806)

**Authors**: Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 7.5

arXiv:2509.06806v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonst...

---

### 15. [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)

**Authors**: Yixiao Zhou, Ziyu Zhao, Dongzhou Cheng, zhiliang wu, Jie Gui, Yi Yang, Fei Wu, Yu Cheng, Hehe Fan  
**Category**: cs.CL  
**Published**: 2025-09-15  
**Score**: 7.5

arXiv:2509.10377v1 Announce Type: new 
Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large language models (LLMs) due to their computational efficiency. However, though only a few experts are activated for each token, SMoE still requires loading all expert parameters, l...

---

### 16. [K2-Think: A Parameter-Efficient Reasoning System](https://arxiv.org/abs/2509.07604)

**Authors**: Zhoujun Cheng, Richard Fan, Shibo Hao, Taylor W. Killian, Haonan Li, Suqi Sun, Hector Ren, Alexander Moreno, Daqian Zhang, Tianjun Zhong, Yuxin Xiong, Yuanzhe Hu, Yutao Xie, Xudong Han, Yuqi Wang, Varad Pimpalkhute, Yonghao Zhuang, Aaryamonvikram Singh, Xuezhi Liang, Anze Xie, Jianshu She, Desai Fan, Chengqian Gao, Liqun Ma, Mikhail Yurochkin, John Maggs, Xuezhe Ma, Guowei He, Zhiting Hu, Zhengzhong Liu, Eric P. Xing  
**Category**: cs.LG  
**Published**: 2025-09-15  
**Score**: 7.5

arXiv:2509.07604v2 Announce Type: replace 
Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance with a 32B parameter model, matching or surpassing much larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system shows that smaller mod...

---

### 17. [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)

**Authors**: Leen Daher, Zhaobo Wang, Malcolm Mielle  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 7.0

arXiv:2509.09747v1 Announce Type: cross 
Abstract: Cross-modal transfer learning is used to improve multi-modal classification models (e.g., for human activity recognition in human-robot collaboration). However, existing methods require paired sensor data at both training and inference, limiting dep...

---

### 18. [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)

**Authors**: Jenny Y. Huang, Mehul Damani, Yousef El-Kurdi, Ramon Astudillo, Wei Sun  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 7.0

arXiv:2509.09864v1 Announce Type: cross 
Abstract: Inference-time scaling has emerged as a powerful way to improve large language model (LLM) performance by generating multiple candidate responses and selecting among them. However, existing work on dynamic allocation for test-time compute typically ...

---

### 19. [AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning](https://arxiv.org/abs/2505.24298)

**Authors**: Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 7.0

arXiv:2505.24298v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has become a dominant paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training sy...

---

### 20. [Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments](https://arxiv.org/abs/2508.08791)

**Authors**: Junjie Ye, Changhao Jiang, Zhengyin Du, Yufei Xu, Xuesong Yao, Zhiheng Xi, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huang, Jiecao Chen  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 7.0

arXiv:2508.08791v2 Announce Type: replace-cross 
Abstract: Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use...

---

### 21. [Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification](https://arxiv.org/abs/2505.13204)

**Authors**: Jikai Wang, Zhenxu Tian, Juntao Li, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Min Zhang  
**Category**: cs.CL  
**Published**: 2025-09-15  
**Score**: 7.0

arXiv:2505.13204v2 Announce Type: replace 
Abstract: Recent works have revealed the great potential of speculative decoding in accelerating the autoregressive generation process of large language models. The success of these methods relies on the alignment between draft candidates and the sampled ou...

---

### 22. [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)

**Authors**: Roshini Pulishetty, Mani Kishan Ghantasala, Keerthy Kaushik Dasoju, Niti Mangwani, Vishal Garimella, Aditya Mate, Somya Chatterjee, Yue Kang, Ehi Nosakhare, Sadid Hasan, Soundar Srinivasan  
**Category**: cs.LG  
**Published**: 2025-09-15  
**Score**: 7.0

arXiv:2509.09782v1 Announce Type: new 
Abstract: The proliferation of large language models (LLMs) with varying computational costs and performance profiles presents a critical challenge for scalable, cost-effective deployment in real-world applications. We introduce a unified routing framework that...

---

### 23. [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)

**Authors**: Mohammad Hasan Narimani, Mostafa Tavassolipour  
**Category**: cs.LG  
**Published**: 2025-09-15  
**Score**: 7.0

arXiv:2509.10041v1 Announce Type: new 
Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model training across decentralized devices, such as smartphones, balancing enhanced predictive performance with the protection of user privacy in sensitive areas like Internet of...

---

### 24. [FedFitTech: A Baseline in Federated Learning for Fitness Tracking](https://arxiv.org/abs/2506.16840)

**Authors**: Zeyneddin Oz, Shreyas Korde, Marius Bock, Kristof Van Laerhoven  
**Category**: cs.LG  
**Published**: 2025-09-15  
**Score**: 7.0

arXiv:2506.16840v2 Announce Type: replace 
Abstract: The rapid evolution of sensors and resource-efficient machine learning models has spurred the widespread adoption of wearable fitness tracking devices. Equipped with inertial sensors, such devices can continuously capture physical movements for fi...

---

### 25. [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)

**Authors**: Jiajun Shen, Yufei Jin, Yi He, Xingquan Zhu  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 6.5

arXiv:2509.09843v1 Announce Type: cross 
Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous graphs. We argue that the heterogeneity in node types, nodal features, and local neighborhood topology poses significant challenges for ensemble learning, particularly in acc...

---

### 26. [HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control](https://arxiv.org/abs/2506.14391)

**Authors**: Yaqiao Zhu, Hongkai Wen, Geyong Min, Man Luo  
**Category**: cs.AI  
**Published**: 2025-09-15  
**Score**: 6.5

arXiv:2506.14391v2 Announce Type: replace-cross 
Abstract: Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers ...

---

### 27. [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)

**Authors**: Francisco Javier Esono Nkulu Andong, Qi Min  
**Category**: cs.LG  
**Published**: 2025-09-15  
**Score**: 6.5

arXiv:2509.10163v1 Announce Type: new 
Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge environments, efficient resource management under stringent privacy, mobility, and energy constraints becomes critical. This paper introduces a novel Federated Multi-Agent Rei...

---

### 28. [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)

**Authors**: Benjamin Holzschuh, Georg Kohl, Florian Redinger, Nils Thuerey  
**Category**: cs.LG  
**Published**: 2025-09-15  
**Score**: 6.5

arXiv:2509.10186v1 Announce Type: new 
Abstract: We present a scalable framework for learning deterministic and probabilistic neural surrogates for high-resolution 3D physics simulations. We introduce a hybrid CNN-Transformer backbone architecture targeted for 3D physics simulations, which significa...

---

### 29. [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)

**Authors**: Rupert Mitchell, Kristian Kersting  
**Category**: cs.LG  
**Published**: 2025-09-15  
**Score**: 6.5

arXiv:2509.10406v1 Announce Type: new 
Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of softmax attention that combines semantic clustering with multipole expansions from computational physics. Our method addresses the quadratic computational complexity of tran...

---

### 30. [Efficient transformer adaptation for analog in-memory computing via low-rank adapters](https://arxiv.org/abs/2411.17367)

**Authors**: Chen Li, Elena Ferro, Corey Lammie, Manuel Le Gallo, Irem Boybat, Bipin Rajendran  
**Category**: cs.LG  
**Published**: 2025-09-15  
**Score**: 6.5

arXiv:2411.17367v3 Announce Type: replace-cross 
Abstract: Analog In-Memory Computing (AIMC) offers a promising solution to the von Neumann bottleneck. However, deploying transformer models on AIMC remains challenging due to their inherent need for flexibility and adaptability across diverse tasks. ...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
