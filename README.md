# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-10-29 12:55:35 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Efficient Low Rank Attention for Long-Context Inference in Large Language Models](https://arxiv.org/abs/2510.23649)

**Authors**: Tenghui Li, Guoxu Zhou, Xuyang Zhao, Yuning Qiu, Qibin Zhao  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 10.5  
**Type**: cross  
**ArXiv ID**: 2510.23649v1  

As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or subopt...

---

### 2. [REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving](https://arxiv.org/abs/2506.01374)

**Authors**: Sujun Tang, Christopher Priebe, Rohan Mahapatra, Lianhui Qin, Hadi Esmaeilzadeh  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 9.5  
**Type**: replace-cross  
**ArXiv ID**: 2506.01374v2  

While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers strugg...

---

### 3. [Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way](https://arxiv.org/abs/2510.24605)

**Authors**: Yicun Yang, Cong Wang, Shaobo Wang, Zichen Wen, Biqing Qi, Hanlin Xu, Linfeng Zhang  
**Category**: cs.CL  
**Published**: 2025-10-29  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2510.24605v1  

Diffusion-based large language models (dLLMs) have exhibited substantial potential for parallel text generation, which may enable more efficient generation compared to autoregressive models. However, current dLLMs suffer from fixed generation lengths, which indicates the generation lengths of dLLMs ...

---

### 4. [Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs](https://arxiv.org/abs/2510.24606)

**Authors**: Siheng Xiong, Joe Zou, Faramarz Fekri, Yae Jee Cho  
**Category**: cs.CL  
**Published**: 2025-10-29  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2510.24606v1  

The quadratic cost of attention hinders the scalability of long-context LLMs, especially in resource-constrained settings. Existing static sparse methods such as sliding windows or global tokens utilizes the sparsity of attention to reduce the cost of attention, but poorly adapts to the content-depe...

---

### 5. [All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level Spiking Neural Networks](https://arxiv.org/abs/2510.24637)

**Authors**: Andrea Castagnetti, Alain Pegatoquet, Beno\^it Miramond  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2510.24637v1  

Spiking Neural Networks (SNNs) are one of the most promising bio-inspired neural networks models and have drawn increasing attention in recent years. The event-driven communication mechanism of SNNs allows for sparse and theoretically low-power operations on dedicated neuromorphic hardware. However,...

---

### 6. [PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM](https://arxiv.org/abs/2509.24085)

**Authors**: Ju-Hyung Lee, Yanqing Lu, Klaus Doppler  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2509.24085v2  

We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a framework for cooperative cross-layer optimization in device-to-device (D2D) communication. Building on our previous work on single-device on-device LLMs, PEARL extends the paradigm by leveraging both publisher and subscriber state...

---

### 7. [GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training](https://arxiv.org/abs/2509.24494)

**Authors**: Hongcheng Wang, Yinuo Huang, Sukai Wang, Guanghui Ren, Hao Dong  
**Category**: cs.CL  
**Published**: 2025-10-29  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2509.24494v2  

Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coup...

---

### 8. [JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model](https://arxiv.org/abs/2505.17257)

**Authors**: Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, Benjamin Wild  
**Category**: cs.LG  
**Published**: 2025-10-29  
**Score**: 9.0  
**Type**: replace  
**ArXiv ID**: 2505.17257v4  

Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-ran...

---

### 9. [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/abs/2502.01068)

**Authors**: Dongwon Jo, Jiwon Song, Yulhwa Kim, Jae-Joon Kim  
**Category**: cs.CL  
**Published**: 2025-10-29  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2502.01068v4  

While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefil...

---

### 10. [Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits](https://arxiv.org/abs/2004.06231)

**Authors**: Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp, Guy Van den Broeck, Kristian Kersting, Zoubin Ghahramani  
**Category**: cs.LG  
**Published**: 2025-10-29  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2004.06231v2  

Probabilistic circuits (PCs) are a promising avenue for probabilistic modeling, as they permit a wide range of exact and efficient inference routines. Recent ``deep-learning-style'' implementations of PCs strive for a better scalability, but are still difficult to train on real-world data, due to th...

---

### 11. [Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor Decompositions and Deep Unrolling](https://arxiv.org/abs/2409.11529)

**Authors**: Lukas Schynol, Marius Pesavento  
**Category**: cs.LG  
**Published**: 2025-10-29  
**Score**: 8.5  
**Type**: replace  
**ArXiv ID**: 2409.11529v3  

Anomaly detection (AD) is increasingly recognized as a key component for ensuring the resilience of future communication systems. While deep learning has shown state-of-the-art AD performance, its application in critical systems is hindered by concerns regarding training data efficiency, domain adap...

---

### 12. [Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion](https://arxiv.org/abs/2510.24390)

**Authors**: Xianjun Gao, Jianchun Liu, Hongli Xu, Liusheng Huang  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2510.24390v1  

The integration of Large Language Models (LLMs) into real-time Web applications, such as AI-powered search and conversational agents, presents a fundamental Web infrastructure challenge: reconciling the demand for high-quality, complex reasoning with the stringent low-latency and high-throughput req...

---

### 13. [STree: Speculative Tree Decoding for Hybrid State-Space Models](https://arxiv.org/abs/2505.14969)

**Authors**: Yangchao Wu, Zongyue Qin, Alex Wong, Stefano Soatto  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.14969v2  

Speculative decoding is a technique to leverage hardware concurrency in order to enable multiple steps of token generation in a single forward pass, thus improving the efficiency of large-scale autoregressive (AR) Transformer models. State-space models (SSMs) are already more efficient than AR Trans...

---

### 14. [PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models](https://arxiv.org/abs/2509.16989)

**Authors**: He Xiao, Runming Yang, Qingyao Yang, Wendong Xu, Zhen Li, Yupeng Su, Zhengwu Liu, Hongxia Yang, Ngai Wong  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2509.16989v2  

Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and model expressiveness. While existing ultra-low-bit PTQ methods rely on binary approximations or complex compensation ...

---

### 15. [SALS: Sparse Attention in Latent Space for KV cache Compression](https://arxiv.org/abs/2510.24273)

**Authors**: Junlin Mu, Hantao Huang, Jihang Zhang, Minghui Yu, Tao Wang, Yidong Li  
**Category**: cs.LG  
**Published**: 2025-10-29  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2510.24273v1  

Large Language Models capable of handling extended contexts are in high demand, yet their inference remains challenging due to substantial Key-Value cache size and high memory bandwidth requirements. Previous research has demonstrated that KV cache exhibits low-rank characteristics within the hidden...

---

### 16. [SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs](https://arxiv.org/abs/2510.24021)

**Authors**: Haiduo Huang, Jiangcheng Song, Yadong Zhang, Pengju Ren  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.24021v1  

Knowledge Distillation (KD) has become a cornerstone technique for compressing Large Language Models (LLMs) into smaller, more efficient student models. However, conventional KD approaches typically apply the distillation loss uniformly across all tokens, regardless of the teacher's confidence. This...

---

### 17. [Trajectory Design for UAV-Based Low-Altitude Wireless Networks in Unknown Environments: A Digital Twin-Assisted TD3 Approach](https://arxiv.org/abs/2510.24255)

**Authors**: Jihao Luo, Zesong Fei, Xinyi Wang, Le Zhao, Yuanhao Cui, Guangxu Zhu, Dusit Niyato  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2510.24255v1  

Unmanned aerial vehicles (UAVs) are emerging as key enablers for low-altitude wireless network (LAWN), particularly when terrestrial networks are unavailable. In such scenarios, the environmental topology is typically unknown; hence, designing efficient and safe UAV trajectories is essential yet cha...

---

### 18. [Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting](https://arxiv.org/abs/2510.23656)

**Authors**: Fuqiang Liu, Weiping Ding, Luis Miranda-Moreno, Lijun Sun  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.23656v1  

Deep neural networks (DNNs) play a significant role in an increasing body of research on traffic forecasting due to their effectively capturing spatiotemporal patterns embedded in traffic data. A general assumption of training the said forecasting models via mean squared error estimation is that the...

---

### 19. [FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point Arithmetic](https://arxiv.org/abs/2510.24061)

**Authors**: Kanghyun Choi, Hyeyoon Lee, SunJong Park, Dain Kwon, Jinho Lee  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.24061v1  

Low-bit floating-point (FP) formats, such as FP8, provide significant acceleration and memory savings in model training thanks to native hardware support on modern GPUs and NPUs. However, we analyze that FP8 quantization offers speedup primarily for large-dimensional matrix multiplications, while in...

---

### 20. [PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling](https://arxiv.org/abs/2510.24235)

**Authors**: Ai Jian, Jingqing Ruan, Xing Ma, Dailin Li, QianLin Zhou, Ke Zeng, Xunliang Cai  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: cross  
**ArXiv ID**: 2510.24235v1  

Reward models (RMs) are central to reinforcement learning from human feedback (RLHF), providing the critical supervision signals that align large language models (LLMs) with human preferences. While generative reward models (GRMs) offer greater interpretability than traditional scalar RMs, current t...

---

### 21. [Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay](https://arxiv.org/abs/2506.05316)

**Authors**: Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2506.05316v3  

Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this...

---

### 22. [TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs](https://arxiv.org/abs/2510.15545)

**Authors**: Sibo Xiao, Jinyuan Fu, Zhongle Xie, Lidan Shou  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.15545v2  

Accelerating the inference of large language models (LLMs) has been a critical challenge in generative AI. Speculative decoding (SD) substantially improves LLM inference efficiency. However, its utility is limited by a fundamental constraint: the draft and target models must share the same vocabular...

---

### 23. [CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known Document Corpora](https://arxiv.org/abs/2510.21729)

**Authors**: Nathan Paull  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: replace-cross  
**ArXiv ID**: 2510.21729v2  

Dense embedding models have become critical for modern information retrieval, particularly in RAG pipelines, but their performance often degrades when applied to specialized corpora outside their pre-training distribution. To address thi we introduce CustomIR, a framework for unsupervised adaptation...

---

### 24. [BitSkip: An Empirical Analysis of Quantization and Early Exit Composition](https://arxiv.org/abs/2510.23766)

**Authors**: Ramshankar Bhuvaneswaran, Handan Liu  
**Category**: cs.CL  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2510.23766v1  

The pursuit of efficient Large Language Models (LLMs) has led to increasingly complex techniques like extreme quantization and dynamic routing. While individual benefits of these methods are well-documented, their compositional effects remain poorly understood. This paper introduces BitSkip, a hybri...

---

### 25. [MinatoLoader: Accelerating Machine Learning Training Through Efficient Data Preprocessing](https://arxiv.org/abs/2509.10712)

**Authors**: Rahma Nouaji, Stella Bitchebe, Ricardo Macedo, Oana Balmau  
**Category**: cs.DC  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2509.10712v2  

Data loaders are used by Machine Learning (ML) frameworks like PyTorch and TensorFlow to apply transformations to data before feeding it into the accelerator. This operation is called data preprocessing. Data preprocessing plays an important role in the ML training workflow because if it is ineffici...

---

### 26. [Efficient Global-Local Fusion Sampling for Physics-Informed Neural Networks](https://arxiv.org/abs/2510.24026)

**Authors**: Jiaqi Luo, Shixin Xu, Zhouwang Yang  
**Category**: cs.LG  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2510.24026v1  

The accuracy of Physics-Informed Neural Networks (PINNs) critically depends on the placement of collocation points, as the PDE loss is approximated through sampling over the solution domain. Global sampling ensures stability by covering the entire domain but requires many samples and is computationa...

---

### 27. [Federated Structured Sparse PCA for Anomaly Detection in IoT Networks](https://arxiv.org/abs/2503.23981)

**Authors**: Chenyi Huang, Xianchao Xiu  
**Category**: cs.LG  
**Published**: 2025-10-29  
**Score**: 7.0  
**Type**: replace  
**ArXiv ID**: 2503.23981v3  

Although federated learning has gained prominence as a privacy-preserving framework tailored for distributed Internet of Things (IoT) environments, current federated principal component analysis (PCA) methods lack integration of sparsity, a critical feature for robust anomaly detection. To address t...

---

### 28. [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://arxiv.org/abs/2510.23691)

**Authors**: Zihao Wang, Xujing Li, Yining Ye, Junjie Fang, Haoming Wang, Longxiang Liu, Shihao Liang, Junting Lu, Zhiyong Wu, Jiazhan Feng, Wanjun Zhong, Zili Li, Yu Wang, Yu Miao, Bo Zhou, Yuanfan Li, Hao Wang, Zhongkai Zhao, Faming Wu, Zhengxuan Jiang, Weihao Tan, Heyuan Yao, Shi Yan, Xiangyang Li, Yitao Liang, Yujia Qin, Guang Shi  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2510.23691v1  

We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simul...

---

### 29. [Bridging Function Approximation and Device Physics via Negative Differential Resistance Networks](https://arxiv.org/abs/2510.23638)

**Authors**: Songyuan Li, Teng Wang, Jinrong Tang, Ruiqi Liu, Yuyao Lu, Feng Xu, Bin Gao, Xiangwei Zhu  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2510.23638v1  

Achieving fully analog neural computation requires hardware that can natively implement both linear and nonlinear operations with high efficiency. While analogue matrix-vector multiplication has advanced via compute-in-memory architectures, nonlinear activation functions remain a bottleneck, often r...

---

### 30. [Scalable GPU-Based Integrity Verification for Large Machine Learning Models](https://arxiv.org/abs/2510.23938)

**Authors**: Marcin Spoczynski, Marcela S. Melara  
**Category**: cs.AI  
**Published**: 2025-10-29  
**Score**: 6.5  
**Type**: cross  
**ArXiv ID**: 2510.23938v1  

We present a security framework that strengthens distributed machine learning by standardizing integrity protections across CPU and GPU platforms and significantly reducing verification overheads. Our approach co-locates integrity verification directly with large ML model execution on GPU accelerato...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
