# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-11-13 12:56:00 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Mixture-of-Channels: Exploiting Sparse FFNs for Efficient LLMs Pre-Training and Inference](https://arxiv.org/abs/2511.09323)

**Authors**: Tong Wu, Yutong He, Bin Wang, Kun Yuan  
**Category**: cs.LG  
**Published**: 2025-11-13  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2511.09323v1  

Large language models (LLMs) have demonstrated remarkable success across diverse artificial intelligence tasks, driven by scaling laws that correlate model size and training data with performance improvements. However, this scaling paradigm incurs substantial memory overhead, creating significant ch...

---

### 2. [Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning](https://arxiv.org/abs/2505.11570)

**Authors**: Chongyang Tan, Ruoqi Wen, Rongpeng Li, Zhifeng Zhao, Ekram Hossain, Honggang Zhang  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 11.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.11570v2  

Federated Learning (FL) enables distributed model training across edge devices in a privacy-friendly manner. However, its efficiency heavily depends on effective device selection and high-dimensional resource allocation in dynamic and heterogeneous wireless environments. Conventional methods demand ...

---

### 3. [Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity](https://arxiv.org/abs/2505.14884)

**Authors**: Susav Shrestha, Brad Settlemyer, Nikoli Dryden, Narasimha Reddy  
**Category**: cs.LG  
**Published**: 2025-11-13  
**Score**: 10.5  
**Type**: replace  
**ArXiv ID**: 2505.14884v3  

Accelerating large language model (LLM) inference is critical for real-world deployments requiring high throughput and low latency. Contextual sparsity, where each token dynamically activates only a small subset of the model parameters, shows promise but does not scale to large batch sizes due to un...

---

### 4. [Motif 2 12.7B technical report](https://arxiv.org/abs/2511.07464)

**Authors**: Junghwan Lim, Sungmin Lee, Dongseok Kim, Taehyun Kim, Eunhwan Park, Jeesoo Lee, Jeongdoo Lee, Junhyeok Lee, Wai Ting Cheung, Dahye Choi, Jaeheui Her, Jaeyeon Huh, Hanbin Jung, Changjin Kang, Beomgyu Kim, Minjae Kim, Taewhan Kim, Youngrok Kim, Hyukjin Kweon, Haesol Lee, Kungyu Lee, Dongpin Oh, Yeongjae Park, Bokki Ryu, Dongjoo Weon  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 10.0  
**Type**: cross  
**ArXiv ID**: 2511.07464v1  

We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compu...

---

### 5. [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)

**Authors**: Dinghong Song, Yuan Feng, Yiwei Wang, Shangye Chen, Cyril Guyot, Filip Blagojevic, Hyeran Jeon, Pengfei Su, Dong Li  
**Category**: cs.CL  
**Published**: 2025-11-13  
**Score**: 10.0  
**Type**: replace  
**ArXiv ID**: 2510.25979v3  

Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model en...

---

### 6. [LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication](https://arxiv.org/abs/2511.09557)

**Authors**: Prajwal Singhania, Siddharth Singh, Lannie Dalton Hough, Akarsh Srivastava, Harshitha Menon, Charles Fredrick Jekel, Abhinav Bhatele  
**Category**: cs.DC  
**Published**: 2025-11-13  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2511.09557v1  

As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node di...

---

### 7. [FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks](https://arxiv.org/abs/2511.09025)

**Authors**: Tianao Xiang, Mingjian Zhi, Yuanguo Bi, Lin Cai, Yuhao Chen  
**Category**: cs.LG  
**Published**: 2025-11-13  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2511.09025v1  

Large Language Models (LLMs) have impressive data fusion and reasoning capabilities for autonomous driving (AD). However, training LLMs for AD faces significant challenges including high computation transmission costs, and privacy concerns associated with sensitive driving data. Federated Learning (...

---

### 8. [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)

**Authors**: Genglin Wang, Liekang Zeng, Bufang Yang, Kaiwei Liu, Guoliang Xing, Chumin Sun, Li Zhou, Jie Sun, Zhenyu Yan  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 9.5  
**Type**: cross  
**ArXiv ID**: 2511.07423v1  

Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially t...

---

### 9. [A Distributed Training Architecture For Combinatorial Optimization](https://arxiv.org/abs/2511.09261)

**Authors**: Yuyao Long  
**Category**: cs.LG  
**Published**: 2025-11-13  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2511.09261v1  

In recent years, graph neural networks (GNNs) have been widely applied in tackling combinatorial optimization problems. However, existing methods still suffer from limited accuracy when addressing that on complex graphs and exhibit poor scalability, since full training requires loading the whole adj...

---

### 10. [Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views](https://arxiv.org/abs/2511.07813)

**Authors**: Haida Feng, Hao Wei, Zewen Xu, Haolin Wang, Chade Li, Yihong Wu  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 9.0  
**Type**: cross  
**ArXiv ID**: 2511.07813v1  

Recently, large language models (LLMs) have been explored widely for 3D scene understanding. Among them, training-free approaches are gaining attention for their flexibility and generalization over training-based methods. However, they typically struggle with accuracy and efficiency in practical dep...

---

### 11. [MA-GTS: A Multi-Agent Framework for Solving Complex Graph Problems in Real-World Applications](https://arxiv.org/abs/2502.18540)

**Authors**: Zike Yuan, Ming Liu, Hui Wang, Bing Qin  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2502.18540v2  

Graph-theoretic problems arise in real-world applications like logistics, communication networks, and traffic optimization. These problems are often complex, noisy, and irregular, posing challenges for traditional algorithms. Large language models (LLMs) offer potential solutions but face challenges...

---

### 12. [FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models](https://arxiv.org/abs/2505.15683)

**Authors**: Zishuai Zhang, Hainan zhang, Weihua Li, Qinnan zhang, jin Dong, Yongxin Tong, Zhiming Zheng  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.15683v3  

Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offlo...

---

### 13. [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)

**Authors**: Bingning Huang, Tu Nguyen, Matthieu Zimmer  
**Category**: cs.CL  
**Published**: 2025-11-13  
**Score**: 9.0  
**Type**: replace-cross  
**ArXiv ID**: 2509.09284v2  

Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS-derived trajectories-traditionally use...

---

### 14. [FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing](https://arxiv.org/abs/2511.07665)

**Authors**: Yuzhe Fu, Changchun Zhou, Hancheng Ye, Bowen Duan, Qiyu Huang, Chiyue Wei, Cong Guo, Hai "Helen'' Li, Yiran Chen  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 8.5  
**Type**: cross  
**ArXiv ID**: 2511.07665v1  

Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve ...

---

### 15. [A Remarkably Efficient Paradigm to Multimodal Large Language Models for Sequential Recommendation](https://arxiv.org/abs/2511.05885)

**Authors**: Qiyong Zhong, Jiajie Su, Ming Yang, Yunshan Ma, Xiaolin Zheng, Chaochao Chen  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 8.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.05885v2  

Sequential recommendations (SR) predict users' future interactions based on their historical behavior. The rise of Large Language Models (LLMs) has brought powerful generative and reasoning capabilities, significantly enhancing SR performance, while Multimodal LLMs (MLLMs) further extend this by int...

---

### 16. [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)

**Authors**: Zihao Ding, Mufeng Zhu, Yao Liu  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.07426v1  

Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual info...

---

### 17. [How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference](https://arxiv.org/abs/2505.09598)

**Authors**: Nidhal Jegham, Marwan Abdelatti, Chan Young Koh, Lassad Elmoubarki, Abdeltawab Hendawi  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2505.09598v5  

This paper introduces an infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models in commercial datacenters. The framework combines public API performance data with company-specific environmental multipliers and statis...

---

### 18. [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)

**Authors**: Ke Li, Di Wang, Ting Wang, Fuyu Dong, Yiming Zhang, Luyao Zhang, Xiangyu Wang, Shaofeng Li, Quan Wang  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 8.0  
**Type**: replace-cross  
**ArXiv ID**: 2509.18711v2  

Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage ...

---

### 19. [TransactionGPT](https://arxiv.org/abs/2511.08939)

**Authors**: Yingtong Dou, Zhimeng Jiang, Tianyi Zhang, Mingzhi Hu, Zhichao Xu, Shubham Jain, Uday Singh Saini, Xiran Fan, Jiarui Sun, Menghai Pan, Junpeng Wang, Xin Dai, Liang Wang, Chin-Chia Michael Yeh, Yujie Fan, Vineeth Rakesh, Huiyuan Chen, Mangesh Bendre, Zhongfang Zhuang, Xiaoting Li, Prince Aboagye, Vivian Lai, Minghua Xu, Hao Yang, Yiwei Cai, Mahashweta Das, Yuzhong Chen  
**Category**: cs.CL  
**Published**: 2025-11-13  
**Score**: 8.0  
**Type**: cross  
**ArXiv ID**: 2511.08939v1  

We present TransactionGPT (TGPT), a foundation model for consumer transaction data within one of world's largest payment networks. TGPT is designed to understand and generate transaction trajectories while simultaneously supporting a variety of downstream prediction and classification tasks. We intr...

---

### 20. [Global Optimization on Graph-Structured Data via Gaussian Processes with Spectral Representations](https://arxiv.org/abs/2511.07734)

**Authors**: Shu Hong, Yongsheng Mei, Mahdi Imani, Tian Lan  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.07734v1  

Bayesian optimization (BO) is a powerful framework for optimizing expensive black-box objectives, yet extending it to graph-structured domains remains challenging due to the discrete and combinatorial nature of graphs. Existing approaches often rely on either full graph topology-impractical for larg...

---

### 21. [OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition](https://arxiv.org/abs/2511.08133)

**Authors**: Lixu Sun, Nurmemet Yolwas, Wushour Silamu  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.08133v1  

Scene Text Recognition (STR) remains challenging due to real-world complexities, where decoupled visual-linguistic optimization in existing frameworks amplifies error propagation through cross-modal misalignment. Visual encoders exhibit attention bias toward background distractors, while decoders su...

---

### 22. [Dual-Kernel Graph Community Contrastive Learning](https://arxiv.org/abs/2511.08287)

**Authors**: Xiang Chen, Kun Yue, Wenjie Liu, Zhenyu Zhang, Liang Duan  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.08287v1  

Graph Contrastive Learning (GCL) has emerged as a powerful paradigm for training Graph Neural Networks (GNNs) in the absence of task-specific labels. However, its scalability on large-scale graphs is hindered by the intensive message passing mechanism of GNN and the quadratic computational complexit...

---

### 23. [Glia: A Human-Inspired AI for Automated Systems Design and Optimization](https://arxiv.org/abs/2510.27176)

**Authors**: Pouya Hamadanian, Pantea Karimi, Arash Nasr-Esfahany, Kimia Noorbakhsh, Joseph Chandler, Ali ParandehGheibi, Mohammad Alizadeh, Hari Balakrishnan  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2510.27176v2  

Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reason...

---

### 24. [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)

**Authors**: Shijie Zhou, Viet Dac Lai, Hao Tan, Jihyung Kil, Wanrong Zhu, Changyou Chen, Ruiyi Zhang  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 7.5  
**Type**: replace-cross  
**ArXiv ID**: 2511.00810v2  

Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet direc...

---

### 25. [Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion](https://arxiv.org/abs/2511.08653)

**Authors**: Kaleem Ullah Qasim, Jiashu Zhang  
**Category**: cs.CL  
**Published**: 2025-11-13  
**Score**: 7.5  
**Type**: cross  
**ArXiv ID**: 2511.08653v1  

Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours...

---

### 26. [Selective Sinkhorn Routing for Improved Sparse Mixture of Experts](https://arxiv.org/abs/2511.08972)

**Authors**: Duc Anh Nguyen, Huu Binh Ta, Nhuan Le Duc, Tan M. Nguyen, Toan Tran  
**Category**: cs.LG  
**Published**: 2025-11-13  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2511.08972v1  

Sparse Mixture-of-Experts (SMoE) has gained prominence as a scalable and computationally efficient architecture, enabling significant growth in model capacity without incurring additional inference costs. However, existing SMoE models often rely on auxiliary losses (e.g., z-loss, load balancing) and...

---

### 27. [Evolutionary Policy Optimization](https://arxiv.org/abs/2503.19037)

**Authors**: Jianren Wang, Yifan Su, Abhinav Gupta, Deepak Pathak  
**Category**: cs.LG  
**Published**: 2025-11-13  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2503.19037v3  

On-policy reinforcement learning (RL) algorithms are widely used for their strong asymptotic performance and training stability, but they struggle to scale with larger batch sizes, as additional parallel environments yield redundant data due to limited policy-induced diversity. In contrast, Evolutio...

---

### 28. [Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving](https://arxiv.org/abs/2511.06029)

**Authors**: Hui Zeng, Daming Zhao, Pengfei Yang, WenXuan Hou, Tianyang Zheng, Hui Li, Weiye Ji, Jidong Zhai  
**Category**: cs.LG  
**Published**: 2025-11-13  
**Score**: 7.5  
**Type**: replace  
**ArXiv ID**: 2511.06029v2  

Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they...

---

### 29. [Alignment-Aware Quantization for LLM Safety](https://arxiv.org/abs/2511.07842)

**Authors**: Sunghyun Wee, Suyoung Kim, Hyeonjin Kim, Kyomin Hwang, Nojun Kwak  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.07842v1  

Safety and efficiency are both important factors when deploying large language models(LLMs). LLMs are trained to follow human alignment for safety, and post training quantization(PTQ) is applied afterward for efficiency. However, these two objectives are often in conflict, revealing a fundamental fl...

---

### 30. [Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models](https://arxiv.org/abs/2511.07979)

**Authors**: Wenhan Yu, Xinbo Lin, Lanxin Ni, Jinhua Cheng, Lei Sha  
**Category**: cs.AI  
**Published**: 2025-11-13  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2511.07979v1  

Large language models (LLMs) have demonstrated strong reasoning abilities across specialized domains, motivating research into their application to legal reasoning. However, existing legal benchmarks often conflate factual recall with genuine inference, fragment the reasoning process, and overlook t...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
