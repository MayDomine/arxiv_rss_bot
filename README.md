# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-08-26 12:55:13 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Experts](https://arxiv.org/abs/2508.17515)

**Authors**: Kyrylo Yemets, Mykola Lukashchuk, Ivan Izonin  
**Category**: cs.LG  
**Published**: 2025-08-26  
**Score**: 6.5

arXiv:2508.17515v1 Announce Type: new 
Abstract: Accurate univariate forecasting remains a pressing need in real-world systems, such as energy markets, hydrology, retail demand, and IoT monitoring, where signals are often intermittent and horizons span both short- and long-term. While transformers a...

---

### 2. [Dynamic Sparse Attention on Mobile SoCs](https://arxiv.org/abs/2508.16703)

**Authors**: Wangsong Yin, Daliang Xu, Mengwei Xu, Gang Huang, Xuanzhe Liu  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 6.0

arXiv:2508.16703v1 Announce Type: cross 
Abstract: On-device running Large Language Models (LLMs) is nowadays a critical enabler towards preserving user privacy. We observe that the attention operator falls back from the special-purpose NPU to the general-purpose CPU/GPU because of quantization sens...

---

### 3. [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)

**Authors**: Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 6.0

arXiv:2508.06259v3 Announce Type: replace-cross 
Abstract: Current multimodal large language models (MLLMs) still face significant challenges in complex visual tasks (e.g., spatial understanding, fine-grained perception). Prior methods have tried to incorporate visual reasoning, however, they fail t...

---

### 4. [Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel](https://arxiv.org/abs/2508.18224)

**Authors**: Ran Yan, Youhe Jiang, Binhang Yuan  
**Category**: cs.DC  
**Published**: 2025-08-26  
**Score**: 6.0

arXiv:2508.18224v1 Announce Type: new 
Abstract: Recent progress in sparse attention mechanisms has demonstrated strong potential for reducing the computational cost of long-context training and inference in large language models (LLMs). Native Sparse Attention (NSA), a state-of-the-art approach, in...

---

### 5. [MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models](https://arxiv.org/abs/2508.17467)

**Authors**: Krishna Teja Chitty-Venkata, Sylvia Howland, Golara Azar, Daria Soboleva, Natalia Vassilieva, Siddhisanket Raskar, Murali Emani, Venkatram Vishwanath  
**Category**: cs.LG  
**Published**: 2025-08-26  
**Score**: 6.0

arXiv:2508.17467v1 Announce Type: new 
Abstract: Mixture of Experts (MoE) models have enabled the scaling of Large Language Models (LLMs) and Vision Language Models (VLMs) by achieving massive parameter counts while maintaining computational efficiency. However, MoEs introduce several inference-time...

---

### 6. [BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens](https://arxiv.org/abs/2508.17196)

**Authors**: Hao Wen, Xinrui Wu, Yi Sun, Feifei Zhang, Liye Chen, Jie Wang, Yunxin Liu, Ya-Qin Zhang, Yuanchun Li  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 5.5

arXiv:2508.17196v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have leveraged increased test-time computation to enhance reasoning capabilities, a strategy that, while effective, incurs significant latency and resource costs, limiting their applicability in re...

---

### 7. [HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation](https://arxiv.org/abs/2508.18118)

**Authors**: Junyi Chen, Lu Chi, Siliang Xu, Shiwei Ran, Bingyue Peng, Zehuan Yuan  
**Category**: cs.CL  
**Published**: 2025-08-26  
**Score**: 5.5

arXiv:2508.18118v1 Announce Type: cross 
Abstract: AI-generated content technologies are widely used in content creation. However, current AIGC systems rely heavily on creators' inspiration, rarely generating truly user-personalized content. In real-world applications such as online advertising, a s...

---

### 8. [WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling](https://arxiv.org/abs/2508.16676)

**Authors**: Jiacheng Li, Jianchao Tan, Zhidong Yang, Pingwei Sun, Feiye Huo, Jiayu Qin, Yerui Sun, Yuchen Xie, Xunliang Cai, Xiangyu Zhang, Maoxin He, Guangming Tan, Weile Jia, Tong Zhao  
**Category**: cs.CL  
**Published**: 2025-08-26  
**Score**: 5.0

arXiv:2508.16676v1 Announce Type: cross 
Abstract: Transformer architecture gradually dominates the LLM field. Recent advances in training optimization for Transformer-based large language models (LLMs) primarily focus on architectural modifications or optimizer adjustments. However, these approache...

---

### 9. [AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations](https://arxiv.org/abs/2508.17320)

**Authors**: Yifei Yao, Mengnan Du  
**Category**: cs.LG  
**Published**: 2025-08-26  
**Score**: 5.0

arXiv:2508.17320v1 Announce Type: new 
Abstract: Understanding the internal representations of large language models (LLMs) remains a central challenge for interpretability research. Sparse autoencoders (SAEs) offer a promising solution by decomposing activations into interpretable features, but exi...

---

### 10. [EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention](https://arxiv.org/abs/2508.16771)

**Authors**: Yifan Zhang, Chen Huang, Yueke Zhang, Jiahao Zhang, Toby Jia-Jun Li, Collin McMillan, Kevin Leach, Yu Huang  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.5

arXiv:2508.16771v1 Announce Type: cross 
Abstract: Code language models (so-called CodeLLMs) are now commonplace in software development. As a general rule, CodeLLMs are trained by dividing training examples into input tokens and then learn importance of those tokens in a process called machine atte...

---

### 11. [DiscussLLM: Teaching Large Language Models When to Speak](https://arxiv.org/abs/2508.18167)

**Authors**: Deep Anil Patel, Iain Melvin, Christopher Malon, Martin Renqiang Min  
**Category**: cs.CL  
**Published**: 2025-08-26  
**Score**: 4.5

arXiv:2508.18167v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like text, yet they largely operate as reactive agents, responding only when directly prompted. This passivity creates an "awareness gap," lim...

---

### 12. [AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training](https://arxiv.org/abs/2508.16647)

**Authors**: Boran Zhao, Hetian Liu, Zihang Yuan, Li Zhu, Fan Yang, Lina Xie Tian Xia, Wenzhe Zhao, Pengju Ren  
**Category**: cs.LG  
**Published**: 2025-08-26  
**Score**: 4.5

arXiv:2508.16647v1 Announce Type: new 
Abstract: Training deep neural networks (DNNs) directly on edge devices has attracted increasing attention, as it offers promising solutions to challenges such as domain adaptation and privacy preservation. However, conventional DNN training typically requires ...

---

### 13. [Deep Learning with Self-Attention and Enhanced Preprocessing for Precise Diagnosis of Acute Lymphoblastic Leukemia from Bone Marrow Smears in Hemato-Oncology](https://arxiv.org/abs/2508.17216)

**Authors**: Md. Maruf, Md. Mahbubul Haque, Bishowjit Paul  
**Category**: cs.LG  
**Published**: 2025-08-26  
**Score**: 4.5

arXiv:2508.17216v1 Announce Type: cross 
Abstract: Acute lymphoblastic leukemia (ALL) is a prevalent hematological malignancy in both pediatric and adult populations. Early and accurate detection with precise subtyping is essential for guiding therapy. Conventional workflows are complex, time-consum...

---

### 14. [Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets](https://arxiv.org/abs/2508.17391)

**Authors**: Nikolaos Pavlidis, Vasilis Perifanis, Symeon Symeonidis, Pavlos S. Efraimidis  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.17391v1 Announce Type: new 
Abstract: Large Language Models (LLMs), originally developed for natural language processing (NLP), have demonstrated the potential to generalize across modalities and domains. With their in-context learning (ICL) capabilities, LLMs can perform predictive tasks...

---

### 15. [TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis](https://arxiv.org/abs/2508.17565)

**Authors**: Feng Tian, Flora D. Salim, Hao Xue  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.17565v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have enabled powerful agent-based applications in finance, particularly for sentiment analysis, financial report comprehension, and stock forecasting. However, existing systems often lack inter-agent...

---

### 16. [Bridging Foundation Models and Efficient Architectures: A Modular Brain Imaging Framework with Local Masking and Pretrained Representation Learning](https://arxiv.org/abs/2508.16597)

**Authors**: Yanwen Wang, Xinglin Zhao, Yijin Song, Xiaobo Liu, Yanrong Hao, Rui Cao, Xin Wen  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.16597v1 Announce Type: cross 
Abstract: Functional connectivity (FC) derived from resting-state fMRI plays a critical role in personalized predictions such as age and cognitive performance. However, applying foundation models(FM) to fMRI data remains challenging due to its high dimensiona...

---

### 17. [Systematic Characterization of LLM Quantization: A Performance, Energy, and Quality Perspective](https://arxiv.org/abs/2508.16712)

**Authors**: Tianyao Shi, Yi Ding  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.16712v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their heavy resource demands make quantization-reducing precision to lower-bit formats-critical for efficient serving. While many quantization methods...

---

### 18. [Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model](https://arxiv.org/abs/2508.16947)

**Authors**: Fan Ding, Xuewen Luo, Hwa Hui Tew, Ruturaj Reddy, Xikun Wang, Junn Yong Loo  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.16947v1 Announce Type: cross 
Abstract: Recent advances in motion planning for autonomous driving have led to models capable of generating high-quality trajectories. However, most existing planners tend to fix their policy after supervised training, leading to consistent but rigid driving...

---

### 19. [Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data](https://arxiv.org/abs/2508.18244)

**Authors**: Chu-Cheng Lin, Daiyi Peng, Yifeng Lu, Ming Zhang, Eugene Ie  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.18244v1 Announce Type: cross 
Abstract: Reliably composing Large Language Models (LLMs) for complex, multi-step workflows remains a significant challenge. The dominant paradigm-optimizing discrete prompts in a pipeline-is notoriously brittle and struggles to enforce the formal compliance ...

---

### 20. [Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline](https://arxiv.org/abs/2507.21886)

**Authors**: Stefanos Gkikas, Ioannis Kyprakis, Manolis Tsiknakis  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2507.21886v5 Announce Type: replace 
Abstract: Pain is a complex condition that affects a large portion of the population. Accurate and consistent evaluation is essential for individuals experiencing pain and supports the development of effective and advanced management strategies. Automatic p...

---

### 21. [Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models](https://arxiv.org/abs/2502.05945)

**Authors**: Paul Darm, Annalisa Riccardi  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2502.05945v3 Announce Type: replace-cross 
Abstract: Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safet...

---

### 22. [ICQuant: Index Coding enables Low-bit LLM Quantization](https://arxiv.org/abs/2505.00850)

**Authors**: Xinlin Li, Osama Hanna, Christina Fragouli, Suhas Diggavi  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2505.00850v2 Announce Type: replace-cross 
Abstract: The rapid deployment of Large Language Models (LLMs) highlights the need for efficient low-bit post-training quantization (PTQ), due to their high memory costs. A key challenge in weight quantization is the presence of outliers, which inflat...

---

### 23. [Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate](https://arxiv.org/abs/2505.19525)

**Authors**: Liangwei Nathan Zheng, Wei Emma Zhang, Mingyu Guo, Miao Xu, Olaf Maennel, Weitong Chen  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2505.19525v2 Announce Type: replace-cross 
Abstract: Effectively managing missing modalities is a fundamental challenge in real-world multimodal learning scenarios, where data incompleteness often results from systematic collection errors or sensor failures. Sparse Mixture-of-Experts (SMoE) ar...

---

### 24. [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)

**Authors**: Bing Han, Feifei Zhao, Dongcheng Zhao, Guobin Shen, Ping Wu, Yu Shi, Yi Zeng  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.09190v3 Announce Type: replace-cross 
Abstract: Fine-tuning as service injects domain-specific knowledge into large language models (LLMs), while challenging the original alignment mechanisms and introducing safety risks. A series of defense strategies have been proposed for the alignment...

---

### 25. [CURE: Critical-Token-Guided Re-Concatenation for Entropy-Collapse Prevention](https://arxiv.org/abs/2508.11016)

**Authors**: Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming Yang, Minghui Qiu, Jing Yang  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.11016v2 Announce Type: replace-cross 
Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have driven the emergence of more sophisticated cognitive behaviors in large language models (LLMs), thereby enhancing their reasoning capabilities. However, in prior RLVR...

---

### 26. [High-Throughput Low-Cost Segmentation of Brightfield Microscopy Live Cell Images](https://arxiv.org/abs/2508.14106)

**Authors**: Surajit Das, Gourav Roy, Pavel Zun  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.14106v2 Announce Type: replace-cross 
Abstract: Live cell culture is crucial in biomedical studies for analyzing cell properties and dynamics in vitro. This study focuses on segmenting unstained live cells imaged with bright-field microscopy. While many segmentation approaches exist for m...

---

### 27. [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill and Decode Inference](https://arxiv.org/abs/2508.15881)

**Authors**: Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang  
**Category**: cs.AI  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.15881v2 Announce Type: replace-cross 
Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across mult...

---

### 28. [KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF](https://arxiv.org/abs/2508.17000)

**Authors**: Jason R Brown, Lennie Wells, Edward James Young, Sergio Bacallado  
**Category**: cs.CL  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.17000v1 Announce Type: new 
Abstract: Proximal Policy Optimisation (PPO) is an established and effective policy gradient algorithm used for Language Model Reinforcement Learning from Human Feedback (LM-RLHF). PPO performs well empirically but has a heuristic motivation and handles the KL-...

---

### 29. [ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models](https://arxiv.org/abs/2508.17892)

**Authors**: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li  
**Category**: cs.CL  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.17892v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memor...

---

### 30. [DPad: Efficient Diffusion Language Models with Suffix Dropout](https://arxiv.org/abs/2508.14148)

**Authors**: Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai "Helen" Li, Yiran Chen  
**Category**: cs.CL  
**Published**: 2025-08-26  
**Score**: 4.0

arXiv:2508.14148v2 Announce Type: replace 
Abstract: Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative Decoding

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
