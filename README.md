# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2026-01-19 05:57:12 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Mugi: Value Level Parallelism For Efficient LLMs](https://arxiv.org/abs/2601.10823)

**Authors**: Daniel Price, Prabhu Vellaisamy, John Shen, Di Wu  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2601.10823v1  

#### Abstract
Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM...

---

### 2. [HOSL: Hybrid-Order Split Learning for Memory-Constrained Edge Training](https://arxiv.org/abs/2601.10940)

**Authors**: Aakriti, Zhe Li, Dandan Liang, Chao Huang, Rui Li, Haibo Yang  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.10940v1  

#### Abstract
Split learning (SL) enables collaborative training of large language models (LLMs) between resource-constrained edge devices and compute-rich servers by partitioning model computation across the network boundary. However, existing SL systems predominantly rely on first-order (FO) optimization, which...

---

### 3. [Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs](https://arxiv.org/abs/2601.10801)

**Authors**: Alberto Coppi, Ema Puljak, Lorenzo Borella, Daniel Jaschke, Enrique Rico, Maurizio Pierini, Jacopo Pazzini, Andrea Triossi, Simone Montangero  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.10801v1  

#### Abstract
We present a systematic study of Tensor Network (TN) models $\unicode{x2013}$ Matrix Product States (MPS) and Tree Tensor Networks (TTN) $\unicode{x2013}$ for real-time jet tagging in high-energy physics, with a focus on low-latency deployment on Field Programmable Gate Arrays (FPGAs). Motivated by ...

---

### 4. [BYOL: Bring Your Own Language Into LLMs](https://arxiv.org/abs/2601.10804)

**Authors**: Syed Waqas Zamir, Wassim Hamidouche, Boulbaba Ben Amor, Luana Marotti, Inbal Becker-Reshef, Juan Lavista Ferres  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.10804v1  

#### Abstract
Large Language Models (LLMs) exhibit strong multilingual capabilities, yet remain fundamentally constrained by the severe imbalance in global language resources. While over 7,000 languages are spoken worldwide, only a small subset (fewer than 100) has sufficient digital presence to meaningfully infl...

---

### 5. [One LLM to Train Them All: Multi-Task Learning Framework for Fact-Checking](https://arxiv.org/abs/2601.11293)

**Authors**: Malin Astrid Larsson, Harald Fosen Grunnaleite, Vinay Setty  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.11293v1  

#### Abstract
Large language models (LLMs) are reshaping automated fact-checking (AFC) by enabling unified, end-to-end verification pipelines rather than isolated components. While large proprietary models achieve strong performance, their closed weights, complexity, and high costs limit sustainability. Fine-tuni...

---

### 6. [Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering](https://arxiv.org/abs/2601.11487)

**Authors**: Paulo S\'ergio Almeida  
**Category**: cs.DC  
**Published**: 2026-01-19  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.11487v1  

#### Abstract
Message delivery respecting causal ordering (causal delivery) is one of the most classic and widely useful abstraction for inter-process communication in a distributed system. Most approaches tag messages with causality information and buffer them at the receiver until they can be safely delivered. ...

---

### 7. [Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration](https://arxiv.org/abs/2601.10973)

**Authors**: Zain ul Abdeen, Waris Gill, Ming Jin  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.10973v1  

#### Abstract
Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential d...

---

### 8. [Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.11401)

**Authors**: Ahmed Rashwan, Keith Briggs, Chris Budd, Lisa Kreusser  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.11401v1  

#### Abstract
Credit assignment is a core challenge in multi-agent reinforcement learning (MARL), especially in large-scale systems with structured, local interactions. Graph-based Markov decision processes (GMDPs) capture such settings via an influence graph, but standard critics are poorly aligned with this str...

---

### 9. [Extractive summarization on a CMOS Ising machine](https://arxiv.org/abs/2601.11491)

**Authors**: Ziqing Zeng, Abhimanyu Kumar, Chris H. Kim, Ulya R. Karpuzcu, Sachin S. Sapatnekar  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.11491v1  

#### Abstract
Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU inf...

---

### 10. [AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts](https://arxiv.org/abs/2601.11044)

**Authors**: Keyu Li, Junhao Shi, Yang Xiao, Mohan Jiang, Jie Sun, Yunze Wu, Shijie Xia, Xiaojie Cai, Tianze Xu, Weiye Si, Wenjie Li, Dequan Wang, Pengfei Liu  
**Category**: cs.AI  
**Published**: 2026-01-19  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.11044v1  

#### Abstract
Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on hum...

---

### 11. [Budget-Aware Anytime Reasoning with LLM-Synthesized Preference Data](https://arxiv.org/abs/2601.11038)

**Authors**: Xuanming Zhang, Shwan Ashrafi, Aziza Mirsaidova, Amir Rezaeian, Miguel Ballesteros, Lydia B. Chilton, Zhou Yu, Dan Roth  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.11038v1  

#### Abstract
We study the reasoning behavior of large language models (LLMs) under limited computation budgets. In such settings, producing useful partial solutions quickly is often more practical than exhaustive reasoning, which incurs high inference costs. Many real-world tasks, such as trip planning, require ...

---

### 12. [Theoretically and Practically Efficient Resistance Distance Computation on Large Graphs](https://arxiv.org/abs/2601.11159)

**Authors**: Yichun Yang, Longlong Lin, Rong-Hua Li, Meihao Liao, Guoren Wang  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.11159v1  

#### Abstract
The computation of resistance distance is pivotal in a wide range of graph analysis applications, including graph clustering, link prediction, and graph neural networks. Despite its foundational importance, efficient algorithms for computing resistance distances on large graphs are still lacking. Ex...

---

### 13. [FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning](https://arxiv.org/abs/2601.11311)

**Authors**: Zhihan Yang, Jiaqi Wei, Xiang Zhang, Haoyu Dong, Yiwen Wang, Xiaoke Guo, Pengkun Zhang, Yiwei Xu, Chenyu You  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.11311v1  

#### Abstract
Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in the...

---

### 14. [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)

**Authors**: Emma Hart, Bas Peters, Julianne Chung, Matthias Chung  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.11397v1  

#### Abstract
This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mapping...

---

### 15. [Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics](https://arxiv.org/abs/2601.11012)

**Authors**: Jiahao Wang, Shuangjia Zheng  
**Category**: cs.AI  
**Published**: 2026-01-19  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.11012v1  

#### Abstract
The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we prop...

---

### 16. [DialDefer: A Framework for Detecting and Mitigating LLM Dialogic Deference](https://arxiv.org/abs/2601.10896)

**Authors**: Parisa Rabbani, Priyam Sahoo, Ruben Mathew, Aishee Mondal, Harshita Ketharaman, Nimet Beyza Bozdag, Dilek Hakkani-T\"ur  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.10896v1  

#### Abstract
LLMs are increasingly used as third-party judges, yet their reliability when evaluating speakers in dialogue remains poorly understood. We show that LLMs judge identical claims differently depending on framing: the same content elicits different verdicts when presented as a statement to verify ("Is ...

---

### 17. [CoG: Controllable Graph Reasoning via Relational Blueprints and Failure-Aware Refinement over Knowledge Graphs](https://arxiv.org/abs/2601.11047)

**Authors**: Yuanxiang Liu, Songze Li, Xiaoke Guo, Zhaoyan Gong, Qifei Zhang, Huajun Chen, Wen Zhang  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.11047v1  

#### Abstract
Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities but often grapple with reliability challenges like hallucinations. While Knowledge Graphs (KGs) offer explicit grounding, existing paradigms of KG-augmented LLMs typically exhibit cognitive rigidity--applying homogeneou...

---

### 18. [Membership Inference on LLMs in the Wild](https://arxiv.org/abs/2601.11314)

**Authors**: Jiatong Yi, Yanyang Li  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.11314v1  

#### Abstract
Membership Inference Attacks (MIAs) act as a crucial auditing tool for the opaque training data of Large Language Models (LLMs). However, existing techniques predominantly rely on inaccessible model internals (e.g., logits) or suffer from poor generalization across domains in strict black-box settin...

---

### 19. [Reward Modeling for Scientific Writing Evaluation](https://arxiv.org/abs/2601.11374)

**Authors**: Furkan \c{S}ahinu\c{c}, Subhabrata Dutta, Iryna Gurevych  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.11374v1  

#### Abstract
Scientific writing is an expert-domain task that demands deep domain knowledge, task-specific requirements and reasoning capabilities that leverage the domain knowledge to satisfy the task specifications. While scientific text generation has been widely studied, its evaluation remains a challenging ...

---

### 20. [FAConvLSTM: Factorized-Attention ConvLSTM for Efficient Feature Extraction in Multivariate Climate Data](https://arxiv.org/abs/2601.10914)

**Authors**: Francis Ndikum Nji, Jianwu Wang  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.10914v1  

#### Abstract
Learning physically meaningful spatiotemporal representations from high-resolution multivariate Earth observation data is challenging due to strong local dynamics, long-range teleconnections, multi-scale interactions, and nonstationarity. While ConvLSTM2D is a commonly used baseline, its dense convo...

---

### 21. [GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling](https://arxiv.org/abs/2601.11161)

**Authors**: Pascal Schlachter, Bin Yang  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.11161v1  

#### Abstract
Unsupervised domain adaptation tackles the problem that domain shifts between training and test data impair the performance of neural networks in many real-world applications. Thereby, in realistic scenarios, the source data may no longer be available during adaptation, and the label space of the ta...

---

### 22. [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)

**Authors**: Percy Jardine  
**Category**: cs.AI  
**Published**: 2026-01-19  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.10738v1  

#### Abstract
Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to un...

---

### 23. [BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search](https://arxiv.org/abs/2601.11037)

**Authors**: Shiyu Liu, Yongjing Yin, Jianhao Yan, Yunbo Tang, Qinggang Zhang, Bei Li, Xin Chen, Jingang Wang, Xunliang Cai, Jinsong Su  
**Category**: cs.AI  
**Published**: 2026-01-19  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.11037v1  

#### Abstract
RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recogniz...

---

### 24. [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)

**Authors**: Yohai Trabelsi, Guojun Xiong, Fentabil Getnet, St\'ephane Verguet, Milind Tambe  
**Category**: cs.AI  
**Published**: 2026-01-19  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.11479v1  

#### Abstract
Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder ...

---

### 25. [T$^\star$: Progressive Block Scaling for MDM Through Trajectory Aware RL](https://arxiv.org/abs/2601.11214)

**Authors**: Hanchen Xia, Baoyou Chen, Yutang Ge, Guojiang Zhao, Siyu Zhu  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.11214v1  

#### Abstract
We present T$^\star$, a simple \textsc{TraceRL}-based training curriculum for progressive block-size scaling in masked diffusion language models (MDMs). Starting from an AR-initialized small-block MDM, T$^\star$~transitions smoothly to larger blocks, enabling higher-parallelism decoding with minimal...

---

### 26. [Latent Dynamics Graph Convolutional Networks for model order reduction of parameterized time-dependent PDEs](https://arxiv.org/abs/2601.11259)

**Authors**: Lorenzo Tomada, Federico Pichi, Gianluigi Rozza  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.11259v1  

#### Abstract
Graph Neural Networks (GNNs) are emerging as powerful tools for nonlinear Model Order Reduction (MOR) of time-dependent parameterized Partial Differential Equations (PDEs). However, existing methodologies struggle to combine geometric inductive biases with interpretable latent behavior, overlooking ...

---

### 27. [QUPID: A Partitioned Quantum Neural Network for Anomaly Detection in Smart Grid](https://arxiv.org/abs/2601.11500)

**Authors**: Hoang M. Ngo, Tre' R. Jeter, Jung Taek Seo, My T. Thai  
**Category**: cs.LG  
**Published**: 2026-01-19  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.11500v1  

#### Abstract
Smart grid infrastructures have revolutionized energy distribution, but their day-to-day operations require robust anomaly detection methods to counter risks associated with cyber-physical threats and system faults potentially caused by natural disasters, equipment malfunctions, and cyber attacks. C...

---

### 28. [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2601.11002)

**Authors**: Qianen Zhang, Zeyu Yang, Satoshi Nakamura  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.11002v1  

#### Abstract
Simultaneous Machine Translation (SiMT) requires high-quality translations under strict real-time constraints, which traditional policies with only READ/WRITE actions cannot fully address. We extend the action space of SiMT with four adaptive actions: Sentence_Cut, Drop, Partial_Summarization and Pr...

---

### 29. [From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models](https://arxiv.org/abs/2601.11020)

**Authors**: Youmi Ma, Naoaki Okazaki  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.11020v1  

#### Abstract
Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether r...

---

### 30. [DOREMI: Optimizing Long Tail Predictions in Document-Level Relation Extraction](https://arxiv.org/abs/2601.11190)

**Authors**: Laura Menotti, Stefano Marchesin, Gianmaria Silvello  
**Category**: cs.CL  
**Published**: 2026-01-19  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.11190v1  

#### Abstract
Document-Level Relation Extraction (DocRE) presents significant challenges due to its reliance on cross-sentence context and the long-tail distribution of relation types, where many relations have scarce training examples. In this work, we introduce DOcument-level Relation Extraction optiMizing the ...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
