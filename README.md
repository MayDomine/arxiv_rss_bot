# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-09-24 12:51:43 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)

**Authors**: Javed I. Khan an Henry Uwabor Moye  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 13.5

arXiv:2509.18114v1 Announce Type: new 
Abstract: Autoregressive inference in large transformer-based language models (LLMs) presents significant challenges for runtime efficiency, particularly during the decode phase where load imbalance across GPU shards can cause throughput degradation and latency...

---

### 2. [Cross-Attention Speculative Decoding](https://arxiv.org/abs/2505.24544)

**Authors**: Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 10.5

arXiv:2505.24544v3 Announce Type: replace-cross 
Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly...

---

### 3. [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.11815)

**Authors**: Haiduo Huang, Fuwei Yang, Zhenhua Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 10.5

arXiv:2509.11815v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a powerful way to accelerate autoregressive large language models (LLMs), but directly porting it to vision-language models (VLMs) faces unique systems constraints: the prefill stage is dominated by visual tokens whos...

---

### 4. [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)

**Authors**: Wonjun Bang, Jongseok Park, Hongseung Yu, Kyungmin Bin, Kyunghan Lee  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 10.5

arXiv:2509.18172v1 Announce Type: new 
Abstract: With the advent of large language models (LLMs), numerous Post-Training Quantization (PTQ) strategies have been proposed to alleviate deployment barriers created by their enormous parameter counts. Quantization achieves compression by limiting the num...

---

### 5. [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)

**Authors**: Yuxuan Cai, Xiaozhuan Liang, Xinghua Wang, Jin Ma, Haijin Liang, Jinwen Luo, Xinyu Zuo, Lisheng Duan, Yuyang Yin, Xi Chen  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 10.5

arXiv:2509.18362v1 Announce Type: new 
Abstract: As large language models (LLMs) become increasingly powerful, the sequential nature of autoregressive generation creates a fundamental throughput bottleneck that limits the practical deployment. While Multi-Token Prediction (MTP) has demonstrated rema...

---

### 6. [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)

**Authors**: Przemys{\l}aw Spyra  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 9.5

arXiv:2509.19063v1 Announce Type: new 
Abstract: The rising computational and energy demands of deep neural networks (DNNs), driven largely by backpropagation (BP), challenge sustainable AI development. This paper rigorously investigates three BP-free training methods: the Forward-Forward (FF), Casc...

---

### 7. [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)

**Authors**: Sharan Sahu, Martin T. Wells  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 9.5

arXiv:2509.19104v1 Announce Type: new 
Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for aligning Large Language Models (LLMs) with human intent. However, existing offline RLHF approaches suffer from overoptimization, where models overfit to reward misspecification a...

---

### 8. [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)

**Authors**: Alexandre Pich\'e, Ehsan Kamaloo, Rafael Pardinas, Dzmitry Bahdanau  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 9.5

arXiv:2509.19128v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs). However, effectively scaling these RL methods presents significant challenges, primarily due to the difficulty in maintaining h...

---

### 9. [LightCode: Compiling LLM Inference for Photonic-Electronic Systems](https://arxiv.org/abs/2509.16443)

**Authors**: Ryan Tomich, Zhizhen Zhong, Dirk Englund  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 9.0

arXiv:2509.16443v1 Announce Type: cross 
Abstract: The growing demand for low-latency, energy-efficient inference in large language models (LLMs) has catalyzed interest in heterogeneous architectures. While GPUs remain dominant, they are poorly suited for integration with emerging domain-specific ac...

---

### 10. [Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers](https://arxiv.org/abs/2509.17533)

**Authors**: Anastasios Fanariotis, Theofanis Orphanoudakis, Vasilis Fotopoulos  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 9.0

arXiv:2509.17533v1 Announce Type: cross 
Abstract: The deployment of machine learning (ML) models on microcontrollers (MCUs) is constrained by strict energy, latency, and memory requirements, particularly in battery-operated and real-time edge devices. While software-level optimizations such as quan...

---

### 11. [Confidence-gated training for efficient early-exit neural networks](https://arxiv.org/abs/2509.17885)

**Authors**: Saad Mokssit, Ouassim Karrakchou, Alejandro Mousist, Mounir Ghogho  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 9.0

arXiv:2509.17885v1 Announce Type: cross 
Abstract: Early-exit neural networks reduce inference cost by enabling confident predictions at intermediate layers. However, joint training often leads to gradient interference, with deeper classifiers dominating optimization. We propose Confidence-Gated Tra...

---

### 12. [An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation](https://arxiv.org/abs/2508.17007)

**Authors**: Riad Hassan, M. Rubaiyat Hossain Mondal, Sheikh Iqbal Ahamed, Fahad Mostafa, Md Mostafijur Rahman  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 9.0

arXiv:2508.17007v2 Announce Type: replace-cross 
Abstract: Proper segmentation of organs-at-risk is important for radiation therapy, surgical planning, and diagnostic decision-making in medical image analysis. While deep learning-based segmentation architectures have made significant progress, they ...

---

### 13. [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)

**Authors**: Pei-Shuo Wang, Jian-Jia Chen, Chun-Che Yang, Chi-Chih Chang, Ning-Chi Huang, Mohamed S. Abdelfattah, Kai-Chiang Wu  
**Category**: cs.CL  
**Published**: 2025-09-24  
**Score**: 9.0

arXiv:2509.18344v1 Announce Type: new 
Abstract: The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and...

---

### 14. [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)

**Authors**: Yuzhen Zhou, Jiajun Li, Yusheng Su, Gowtham Ramesh, Zilin Zhu, Xiang Long, Chenyang Zhao, Jin Pan, Xiaodong Yu, Ze Wang, Kangrui Du, Jialian Wu, Ximeng Sun, Jiang Liu, Qiaolin Yu, Hao Chen, Zicheng Liu, Emad Barsoum  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 9.0

arXiv:2509.18521v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale pre-trained language models (LLMs). Successive generations, including GPT-o series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale RL training to en...

---

### 15. [MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE](https://arxiv.org/abs/2509.17238)

**Authors**: Soheil Zibakhsh, Mohammad Samragh, Kumari Nishu, Lauren Hannah, Arnav Kundu, Minsik Cho  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 8.5

arXiv:2509.17238v1 Announce Type: new 
Abstract: The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quali...

---

### 16. [Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding](https://arxiv.org/abs/2509.18085)

**Authors**: Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee, Christopher Lott, Fatih Porikli  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 8.5

arXiv:2509.18085v1 Announce Type: cross 
Abstract: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at mu...

---

### 17. [A Scalable Multi-Robot Framework for Decentralized and Asynchronous Perception-Action-Communication Loops](https://arxiv.org/abs/2309.10164)

**Authors**: Saurav Agarwal, Frederic Vatnsdal, Romina Garcia Camargo, Vijay Kumar, Alejandro Ribeiro  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 8.5

arXiv:2309.10164v2 Announce Type: replace-cross 
Abstract: Collaboration in large robot swarms to achieve a common global objective is a challenging problem in large environments due to limited sensing and communication capabilities. The robots must execute a Perception-Action-Communication (PAC) lo...

---

### 18. [PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference](https://arxiv.org/abs/2509.04467)

**Authors**: Hao Zhang, Mengsi Lyu, Zhuo Chen, Xingrun Xing, Yulong Ao, Yonghua Lin  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 8.5

arXiv:2509.04467v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However...

---

### 19. [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)

**Authors**: Hengbo Xiao, Jingyuan Fan, Xin Tong, Jingzhao Zhang, Chao Lu, Guannan He  
**Category**: cs.CL  
**Published**: 2025-09-24  
**Score**: 8.5

arXiv:2509.18169v1 Announce Type: cross 
Abstract: Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. ...

---

### 20. [Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism](https://arxiv.org/abs/2506.01979)

**Authors**: Yuhao Shen, Junyi Shen, Quan Kong, Tianyu Liu, Yao Lu, Cong Wang  
**Category**: cs.DC  
**Published**: 2025-09-24  
**Score**: 8.5

arXiv:2506.01979v2 Announce Type: replace 
Abstract: Speculative decoding (SD) has emerged as a promising technique to accelerate LLM inference by employing a small draft model to propose draft tokens in advance, and validating them in parallel with the large target model. However, the existing SD m...

---

### 21. [Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark](https://arxiv.org/abs/2509.17894)

**Authors**: Siu Hang Ho, Prasad Ganesan, Nguyen Duong, Daniel Schlabig  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 8.5

arXiv:2509.17894v2 Announce Type: replace 
Abstract: Efficient inference is a critical challenge in deep generative modeling, particularly as diffusion models grow in capacity and complexity. While increased complexity often improves accuracy, it raises compute costs, latency, and memory requirement...

---

### 22. [PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models](https://arxiv.org/abs/2509.16989)

**Authors**: He Xiao, Runming Yang, Qingyao Yang, Wendong Xu, Zheng Li, Yupeng Su, Zhengwu Liu, Hongxia Yang, Ngai Wong  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 8.0

arXiv:2509.16989v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and model expressiveness. While existing ultra-low-bit PTQ methods rel...

---

### 23. [SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models](https://arxiv.org/abs/2509.14269)

**Authors**: Jianbin Zhang, Yulin Zhu, Wai Lun Lo, Richard Tai-Chiu Hsung, Harris Sik-Ho Tsang, Kai Zhou  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 8.0

arXiv:2509.14269v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved great success in medical question answering and clinical decision-making, promoting the efficiency and popularization of the personalized virtual doctor in society. However, the traditional fine-tun...

---

### 24. [TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation](https://arxiv.org/abs/2509.15666)

**Authors**: Yongsheng Feng, Yuetonghui Xu, Jiehui Luo, Hongjia Liu, Xiaobing Li, Feng Yu, Wei Li  
**Category**: cs.AI  
**Published**: 2025-09-24  
**Score**: 8.0

arXiv:2509.15666v2 Announce Type: replace-cross 
Abstract: Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly ...

---

### 25. [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)

**Authors**: Zeyu Liu, Souvik Kundu, Lianghao Jiang, Anni Li, Srikanth Ronanki, Sravan Bodapati, Gourav Datta, Peter A. Beerel  
**Category**: cs.CL  
**Published**: 2025-09-24  
**Score**: 8.0

arXiv:2509.18467v1 Announce Type: new 
Abstract: Although transformer architectures have achieved state-of-the-art performance across diverse domains, their quadratic computational complexity with respect to sequence length remains a significant bottleneck, particularly for latency-sensitive long-co...

---

### 26. [Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner](https://arxiv.org/abs/2508.15044)

**Authors**: Bolian Li, Yanran Wu, Xinyu Luo, Ruqi Zhang  
**Category**: cs.CL  
**Published**: 2025-09-24  
**Score**: 8.0

arXiv:2508.15044v3 Announce Type: replace 
Abstract: Aligning large language models (LLMs) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance LLM sa...

---

### 27. [ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding](https://arxiv.org/abs/2509.15235)

**Authors**: Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen  
**Category**: cs.CL  
**Published**: 2025-09-24  
**Score**: 8.0

arXiv:2509.15235v3 Announce Type: replace-cross 
Abstract: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (...

---

### 28. [Phantora: Maximizing Code Reuse in Simulation-based Machine Learning System Performance Estimation](https://arxiv.org/abs/2505.01616)

**Authors**: Jianxing Qin, Jingrong Chen, Xinhao Kong, Yongji Wu, Tianjun Yuan, Liang Luo, Zhaodong Wang, Ying Zhang, Tingjun Chen, Alvin R. Lebeck, Danyang Zhuo  
**Category**: cs.DC  
**Published**: 2025-09-24  
**Score**: 8.0

arXiv:2505.01616v2 Announce Type: replace 
Abstract: Modern machine learning (ML) training workloads place substantial demands on both computational and communication resources. Consequently, accurate performance estimation has become increasingly critical for guiding system design decisions, such a...

---

### 29. [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)

**Authors**: Zihan Dong, Xinyu Fan, Zixiang Tang, Yunqing Li  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 8.0

arXiv:2509.18230v1 Announce Type: cross 
Abstract: Controlling desktop applications via software remains a fundamental yet under-served problem. Existing multi-modal large language models (MLLMs) ingest screenshots and task instructions to generate keystrokes and mouse events, but they suffer from p...

---

### 30. [An Efficient Self-Supervised Framework for Long-Sequence EEG Modeling](https://arxiv.org/abs/2502.17873)

**Authors**: Jiazhen Hong, Geoffrey Mackellar, Soheila Ghane  
**Category**: cs.LG  
**Published**: 2025-09-24  
**Score**: 8.0

arXiv:2502.17873v2 Announce Type: replace 
Abstract: Electroencephalogram (EEG) signals generally exhibit low signal-to-noise ratio (SNR) and high inter-subject variability, making generalization across subjects and domains challenging. Recent advances in deep learning, particularly self-supervised ...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
