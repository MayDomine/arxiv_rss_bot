# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-13 06:44:36 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm](https://arxiv.org/abs/2602.11543)

**Authors**: Jinrui Zhang, Chaodong Xiao, Aoqi Wu, Xindong Zhang, Lei Zhang  
**Category**: cs.CL  
**Published**: 2026-02-13  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2602.11543v1  

#### Abstract
Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each n...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒé«˜åº¦ä¾èµ–é›†ä¸­å¼é«˜æ€§èƒ½è®¡ç®—é›†ç¾¤ï¼Œéœ€è¦æ•°åƒå¼ é«˜æ˜¾å­˜GPUï¼ˆå¦‚H100/A100ï¼‰ä»¥åŠä½å»¶è¿Ÿã€é«˜å¸¦å®½äº’è”ï¼ˆå¦‚RDMAï¼‰ã€‚è¿™å¯¼è‡´è®­ç»ƒæˆæœ¬æé«˜ï¼Œé™åˆ¶äº†å¤§å¤šæ•°ç ”ç©¶è€…å‚ä¸LLMç ”å‘çš„èƒ½åŠ›ã€‚å°½ç®¡å·²æœ‰å»ä¸­å¿ƒåŒ–è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚DiLiCoã€Photonï¼‰ï¼Œä½†å®ƒä»¬ä»éœ€åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šå­˜å‚¨å’Œæ›´æ–°**å®Œæ•´æ¨¡å‹å‚æ•°**ï¼Œå› æ­¤æ— æ³•çªç ´å•ä¸ªGPUçš„æ˜¾å­˜ç“¶é¢ˆã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šSPESï¼ˆSparse Expert Synchronizationï¼‰
æœ¬æ–‡æå‡ºäº†ä¸€ç§**å†…å­˜é«˜æ•ˆã€å»ä¸­å¿ƒåŒ–çš„MoE LLMé¢„è®­ç»ƒæ¡†æ¶â€”â€”SPES**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- **ä¸“å®¶åˆ†ç‰‡è®­ç»ƒ**ï¼šå°†Mixture-of-Expertsï¼ˆMoEï¼‰æ¨¡å‹ä¸­çš„ä¸åŒexpertåˆ†é…ç»™ä¸åŒçš„åˆ†å¸ƒå¼èŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»…è´Ÿè´£è®­ç»ƒè‡ªå·±è¢«åˆ†é…çš„ä¸€ç»„expertï¼Œå…¶ä½™expertä¿æŒå†»ç»“ã€‚
- **ç¨€ç–åŒæ­¥æœºåˆ¶**ï¼šå„èŠ‚ç‚¹åªåœ¨æœ¬åœ°æ›´æ–°åï¼Œå‘å‚æ•°æœåŠ¡å™¨ä¸Šä¼ **å·²æ›´æ–°çš„sharedæ¨¡å—å’Œassigned expertsçš„æƒé‡**ï¼Œé¿å…å…¨é‡å‚æ•°ä¼ è¾“ã€‚
- **ä¸“å®¶åˆå¹¶çƒ­å¯åŠ¨ç­–ç•¥ï¼ˆExpert-Merging Warm-Upï¼‰**ï¼šåœ¨è®­ç»ƒåˆæœŸï¼Œå‘¨æœŸæ€§åœ°å°†ç›¸ä¼¼çš„expertè¿›è¡ŒåŠ æƒå¹³å‡èåˆï¼Œä»¥æå‡tokenåˆ©ç”¨ç‡å¹¶åŠ é€ŸçŸ¥è¯†å…±äº«ï¼Œå¸®åŠ©æ¨¡å‹å¿«é€Ÿå»ºç«‹åŸºç¡€èƒ½åŠ›ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿé›†ä¸­å¼/DiLiCo/Photon | SPES |
|------|--------------------------|------|
| æ˜¾å­˜å ç”¨ | é«˜ï¼ˆéœ€å­˜å‚¨å…¨éƒ¨optimizer statesã€gradientsï¼‰ | **æ˜¾è‘—é™ä½**ï¼ˆä»…ç»´æŠ¤assigned expertçš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰ |
| é€šä¿¡å¼€é”€ | é«˜ï¼ˆæ¯è½®éœ€åŒæ­¥å…¨éƒ¨æ¨¡å‹å‚æ•°ï¼‰ | **å¤§å¹…å‡å°‘**ï¼ˆä»…åŒæ­¥æ›´æ–°éƒ¨åˆ†ï¼‰ |
| å¯æ‰©å±•æ€§ | ä¾èµ–é«˜é€Ÿäº’è”ï¼Œéš¾ä»¥è·¨å¹¿åŸŸç½‘éƒ¨ç½² | æ”¯æŒå¼±è¿æ¥ã€å¼‚æ„è®¾å¤‡ï¼ˆå¦‚äº’è”ç½‘ä¸Šçš„ç‹¬ç«‹GPUï¼‰ |
| ç¡¬ä»¶é—¨æ§› | å¿…é¡»ä½¿ç”¨é«˜æ˜¾å­˜GPUï¼ˆå¦‚80GB H100ï¼‰ | å¯è¿è¡Œäºå¸¸è§48GB GPUï¼ˆå¦‚L40Sï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **ä»é›¶è®­ç»ƒï¼ˆTraining from Scratchï¼‰**
  - `Ultra-FineWeb`ï¼šé«˜è´¨é‡è‹±æ–‡ç½‘é¡µè¯­æ–™
  - `SlimPajama`ï¼šå»é‡åçš„å¤šæºæ–‡æœ¬é›†åˆ
  - `OLMo-Mix-1124` å­é›†ï¼šåŒ…å« `arXiv`, `OpenWebMath`, `StarCoder`, `pes2o` ç­‰é¢†åŸŸä¸“ç”¨æ•°æ®
- **è¿ç§»è®­ç»ƒï¼ˆUpcyclingï¼‰**
  - `Nemotron Pretraining Dataset`ï¼šä¸“ä¸ºæ•°å­¦ã€ä»£ç å’Œæ¨ç†ä»»åŠ¡è®¾è®¡çš„å¤§è§„æ¨¡è¯­æ–™

### å®éªŒè®¾ç½®
| æ¨¡å‹è§„æ¨¡ | èŠ‚ç‚¹æ•° | å•èŠ‚ç‚¹GPUé…ç½® | æ€»è®­ç»ƒTokené‡ |
|---------|--------|---------------|----------------|
| 2B MoE | 16     | 1Ã—NVIDIA L40S (48GB) | 500B |
| 7B MoE | 4      | 8Ã—A800 (é€šè¿‡NVLinkäº’è”) | 500B |
| 9B MoE* | -      | åˆå§‹åŒ–è‡ª Qwen3-1.7B-Base | 400B |

> æ³¨ï¼š9Bæ¨¡å‹é‡‡ç”¨â€œupcyclingâ€æ–¹å¼ï¼Œå³ä»dense checkpointå¤åˆ¶FFNå±‚å¹¶æ³¨å…¥å™ªå£°ç”Ÿæˆåˆå§‹MoEç»“æ„ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **é€šç”¨èƒ½åŠ›åŸºå‡†**ï¼š
  - `MMLU`ï¼ˆ5-shotï¼‰ã€`C-Eval`ï¼ˆ0-shotï¼‰
- **å¸¸è¯†æ¨ç†åŸºå‡†**ï¼š
  - `ARC(e/c)`ã€`PIQA`ã€`SciQ`ã€`OBQA`ã€`BoolQ`ã€`SIQA`ã€`WinoGrande`
- **å…¶ä»–è¡¥å……æŒ‡æ ‡**ï¼š
  - æ¯GPUæ˜¾å­˜å ç”¨ï¼ˆTraining GPU Memoryï¼‰
  - æ¯è½®ä¸Šè¡Œé€šä¿¡é‡ï¼ˆUplink Volume per Roundï¼‰
  - ååé‡ï¼ˆtokens/s/GPUï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Centralized Training**ï¼šæ ‡å‡†é›†ä¸­å¼FSDP/Data Parallelè®­ç»ƒ
- **DiLiCo**ï¼šä»£è¡¨æ€§çš„å»ä¸­å¿ƒåŒ–é¢„è®­ç»ƒæ–¹æ³•ï¼ˆåŸºäºFedAvgï¼‰
- å…¶ä»–å¼€æºLLMä½œä¸ºå¤–éƒ¨å‚è€ƒï¼ˆå¦‚TinyLlamaã€Pythiaã€Qwenç³»åˆ—ç­‰ï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§Table 3ï¼‰
| æ–¹æ³• | å‚æ•°é‡ | è®­ç»ƒToken | ARC(e) | ARC(c) | PIQA | SciQ | BoolQ | Avg |
|------|--------|------------|--------|--------|------|--------|--------|-----|
| SPES-2B | 0.8B/2.1B | 500B | 63.8 | 35.3 | 69.3 | 85.0 | 61.4 | â€” |
| SPES-7B | 1.6B/7.3B | 500B | 72.1 | 43.8 | 74.7 | 89.9 | 62.7 | â€” |
| SPES-9B* | 3.1B/9.4B | 400B | **81.5** | **57.3** | **78.9** | **95.3** | **77.3** | â€” |

> âœ… **SPES-9Båœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°SOTAçº§åˆ«è¡¨ç°ï¼Œç”šè‡³ä¼˜äºè®¸å¤šæ›´å¤§è§„æ¨¡æˆ–æ›´é•¿è®­ç»ƒæ—¶é—´çš„æ¨¡å‹**

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ï¼ˆTable 1 & Figure 3ï¼‰
| æŒ‡æ ‡ | Centralized | DiLiCo | SPES |
|------|-------------|--------|------|
| 2Bæ¨¡å‹å•å¡æ˜¾å­˜å ç”¨ | >55GB âŒ | ~55GB âŒ | **~35GB** âœ… |
| 7Bæ¨¡å‹æ¯è½®ä¸Šè¡Œé€šä¿¡é‡ | 28.6 GB | 28.6 GB | **9.8 GB** â†“65% |
| 2Bæ¨¡å‹é€šä¿¡æˆæœ¬ | â€” | â€” | â†“33.3% |
| è®­ç»ƒååï¼ˆtokens/s/GPUï¼‰ | 3.79k | â€” | 3.67k ï¼ˆæ¥è¿‘ï¼‰ |

> ğŸ” åœ¨ä»…æœ‰16å°48GB L40S GPUä¸”é€šè¿‡æ™®é€šä»¥å¤ªç½‘è¿æ¥çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸå®Œæˆ2B MoEæ¨¡å‹è®­ç»ƒï¼Œè€ŒåŒç±»æ–¹æ³•å› æ˜¾å­˜ä¸è¶³æ— æ³•è¿è¡Œã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰ä¸“å®¶åˆå¹¶çƒ­å¯åŠ¨æ•ˆæœï¼ˆTable 4ï¼‰
| æ–¹æ³• | ARC(e) | SciQ | BoolQ | Avg Score |
|------|--------|--------|--------|-----------|
| w/o merging | 52.8 | 75.9 | 58.0 | 50.5 |
| w/ merging | 52.1 | 77.8 | 60.4 | **51.3** â†‘ |

> ğŸ’¡ ä¸“å®¶åˆå¹¶ç­–ç•¥æœ‰æ•ˆæå‡äº†tokenåˆ©ç”¨ç‡ï¼Œåœ¨`BoolQ`å’Œ`SciQ`ä¸Šæœ‰æ˜æ˜¾å¢ç›Šã€‚

#### ï¼ˆ2ï¼‰åŒæ­¥é¢‘ç‡å½±å“ï¼ˆFigure A2ï¼‰
- å½“æœ¬åœ°è®­ç»ƒæ­¥æ•° $ H $ ä»50å¢åŠ åˆ°400æ—¶ï¼Œæ€§èƒ½ä¸‹é™ã€‚
- è¡¨æ˜**é¢‘ç¹åŒæ­¥æœ‰åŠ©äºç»´æŒå…¨å±€ä¸€è‡´æ€§**ï¼Œå°¤å…¶åœ¨å¸¦å®½å—é™ç¯å¢ƒä¸‹æ›´ä¸ºé‡è¦ã€‚

#### ï¼ˆ3ï¼‰èŠ‚ç‚¹æ•°é‡æ‰©å±•æ€§ï¼ˆTable A5ï¼‰
| èŠ‚ç‚¹æ•° | Avg Score |
|-------|-----------|
| 2     | 50.6      |
| 4     | 49.7      |
| 8     | 49.5      |

> ğŸ“ˆ å°½ç®¡éšç€èŠ‚ç‚¹å¢å¤šç•¥æœ‰ä¸‹é™ï¼Œä½†æ•´ä½“æ€§èƒ½ç¨³å®šï¼ŒéªŒè¯äº†SPESè‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **SPESå®ç°äº†çœŸæ­£æ„ä¹‰ä¸Šçš„ä½èµ„æºLLMé¢„è®­ç»ƒ**ï¼šé¦–æ¬¡è¯æ˜å¯åœ¨ä»…16å—48GBæ¶ˆè´¹çº§GPUä¸Šã€é€šè¿‡äº’è”ç½‘è¿æ¥å®Œæˆ2B MoEæ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒã€‚
2. **æ˜¾å­˜ä¸é€šä¿¡åŒé‡ä¼˜åŒ–**ï¼šç›¸æ¯”DiLiCoç­‰å»ä¸­å¿ƒåŒ–æ–¹æ³•ï¼ŒSPESå°†å•å¡æ˜¾å­˜éœ€æ±‚é™ä½è¿‘40%ï¼Œé€šä¿¡å¼€é”€å‡å°‘33%-65%ã€‚
3. **æ€§èƒ½ä¸å¦¥å**ï¼šåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹ï¼ŒSPESè®­ç»ƒå‡ºçš„æ¨¡å‹æ€§èƒ½åª²ç¾ç”šè‡³è¶…è¶Šé›†ä¸­å¼è®­ç»ƒç»“æœã€‚
4. **æ”¯æŒçµæ´»æ‰©å±•ä¸è¿ç§»å­¦ä¹ **ï¼šä¸ä»…èƒ½ä»å¤´è®­ç»ƒ7Bæ¨¡å‹ï¼Œè¿˜èƒ½æˆåŠŸâ€œå‡çº§â€ä¸€ä¸ªdense checkpointè‡³9B MoEæ¨¡å‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å®ç”¨æ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æœ€å¤§éªŒè¯è§„æ¨¡ä¸º9Bå‚æ•°ï¼Œå°šæœªæµ‹è¯•è¶…å¤§è§„æ¨¡ï¼ˆå¦‚70B+ï¼‰ä¸‹çš„æ”¶æ•›ç¨³å®šæ€§ã€‚
- æ‰€æœ‰å®éªŒå‡åŸºäºè¯­è¨€ç†è§£ä»»åŠ¡ï¼Œæœªæ¢ç´¢multimodalæˆ–generativeä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚
- ä¸“å®¶åˆ†é…ç­–ç•¥ç›®å‰ä¸ºé™æ€åˆ’åˆ†ï¼Œæœªè€ƒè™‘åŠ¨æ€è´Ÿè½½å‡è¡¡æˆ–routingä¼˜åŒ–ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- éªŒè¯SPESåœ¨æ›´å¤§æ¨¡å‹ï¼ˆ>10Bï¼‰å’Œæ›´é•¿è®­ç»ƒå‘¨æœŸä¸‹çš„å¯æ‰©å±•æ€§ã€‚
- æ¢ç´¢å…¶åœ¨**å¤šæ¨¡æ€å»ºæ¨¡**ã€**å¼ºåŒ–å­¦ä¹ å¯¹é½**ç­‰åœºæ™¯çš„åº”ç”¨æ½œåŠ›ã€‚
- ç»“åˆæ›´å…ˆè¿›çš„optimizerï¼ˆå¦‚Des-LoCï¼‰æˆ–æ¶æ„è®¾è®¡è¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚
- æ„å»ºå¼€æ”¾ç¤¾åŒºé©±åŠ¨çš„å»ä¸­å¿ƒåŒ–è®­ç»ƒç”Ÿæ€ï¼Œæ¨åŠ¨å…¨æ°‘å‚ä¸LLMç ”å‘ã€‚

> ğŸš€ **æ€»ä½“è€Œè¨€ï¼ŒSPESä¸ºæ‰“ç ´LLMè®­ç»ƒå£å’æä¾›äº†åˆ‡å®å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ï¼Œæœ‰æœ›ä¿ƒè¿›æ›´åŠ åŒ…å®¹å’Œå¹³ç­‰çš„äººå·¥æ™ºèƒ½ç ”ç©¶æ ¼å±€ã€‚**

</details>

---

### 2. [Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration](https://arxiv.org/abs/2602.11937)

**Authors**: Akhiad Bercovich, Nir Ailon, Vladimir Anisimov, Tomer Asida, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Roi Koren, Itay Levy, Zach Moshe, Pavlo Molchanov, Najeeb Nabwani, Mostofa Patwari, Omri Puny, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, Ran El-Yaniv  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2602.11937v1  

#### Abstract
Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šExtending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰é¢å‘ **reasoning** çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ç”Ÿæˆæ›´é•¿çš„æ¨ç†é“¾ï¼ˆreasoning tracesï¼‰æ¥æå‡ç­”æ¡ˆè´¨é‡ï¼Œä½†è¿™æ˜¾è‘—å¢åŠ äº†æœåŠ¡æˆæœ¬ï¼ˆserving costï¼‰ã€‚ä¸»è¦åŸå› åœ¨äºï¼š
- è‡ªå›å½’æ¨ç†ä¸­ï¼Œ**self-attention** çš„è®¡ç®—å¤æ‚åº¦éšåºåˆ—é•¿åº¦å¢é•¿è€Œä¸Šå‡ï¼›
- **KV-cache** çš„å†…å­˜å ç”¨å’Œå¸¦å®½éœ€æ±‚éš token æ•°çº¿æ€§å¢åŠ ï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡ï¼ˆå¦‚ 128Kï¼‰åœºæ™¯ä¸‹æˆä¸ºä¸»è¦ç“¶é¢ˆï¼›
- æ¨ç†è¿‡ç¨‹ä¸­çš„ token æ•°é‡å˜åŒ–ä½¿å¾—ä¼ ç»Ÿçš„ per-token ååé‡ï¼ˆtok/sï¼‰å’Œå»¶è¿Ÿï¼ˆms/tokenï¼‰æ— æ³•å‡†ç¡®åæ˜ ç«¯åˆ°ç«¯æ•ˆç‡ã€‚

å› æ­¤ï¼Œå¦‚ä½•åœ¨ä¸ç‰ºç‰²æ¨ç†èƒ½åŠ›çš„å‰æä¸‹ï¼Œä¼˜åŒ– **MoE æ¶æ„** å’Œ **é•¿ä¸Šä¸‹æ–‡æ³¨æ„åŠ›æœºåˆ¶** çš„æ¨ç†æ•ˆç‡ï¼Œæˆä¸ºä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

---

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡ºå°† **Puzzle** è¿™ä¸€åè®­ç»ƒç¥ç»æ¶æ„æœç´¢ï¼ˆpost-training NASï¼‰æ¡†æ¶æ‰©å±•è‡³æ”¯æŒ **MoE å±‚** å’Œ **é•¿ä¸Šä¸‹æ–‡æ¨ç†** çš„åœºæ™¯ï¼Œå¹¶åº”ç”¨äº `gpt-oss-120B` æ¨¡å‹ï¼Œæ„å»ºå‡ºè½»é‡åŒ–ã€é«˜æ€§èƒ½çš„è¡ç”Ÿæ¨¡å‹ `gpt-oss-puzzle-88B`ã€‚

#### ä¸»è¦æŠ€æœ¯ç»„åˆåŒ…æ‹¬ï¼š
- **å¼‚æ„ MoE ä¸“å®¶å‰ªæï¼ˆHeterogeneous MoE Expert Pruningï¼‰**  
  åœ¨ä¸åŒå±‚é‡‡ç”¨ä¸åŒçš„ä¸“å®¶æ•°é‡è¿›è¡Œå‰ªæï¼ŒåŸºäºæ¿€æ´»ç›¸ä¼¼æ€§ï¼ˆactivation-based replace-1-block scoringï¼‰è¯„ä¼°æ¯å±‚é‡è¦æ€§ï¼Œå®ç°ç»†ç²’åº¦å‹ç¼©ã€‚
  
- **é€‰æ‹©æ€§çª—å£æ³¨æ„åŠ›ï¼ˆSelective Window Attentionï¼‰**  
  å°†éƒ¨åˆ†å…¨å±€æ³¨æ„åŠ›å±‚æ›¿æ¢ä¸º **window attention**ï¼ˆçª—å£å¤§å° 8192ï¼‰ï¼Œä»¥å¤§å¹…å‡å°‘ KV-cache å ç”¨ã€‚å…³é”®åˆ›æ–°æ˜¯ä½¿ç”¨ **AA-LCR**ï¼ˆArtificial Analysis Long-Context Reasoningï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸‹é™ä½œä¸ºè¯„åˆ†ä¿¡å·ï¼Œè€Œéä¼ ç»Ÿæ¿€æ´»å·®å¼‚ï¼Œä»è€Œæ›´å¥½ä¿ç•™é•¿ç¨‹ä¾èµ–èƒ½åŠ›ã€‚

- **FP8 KV-Cache é‡åŒ– + æ ¡å‡†å°ºåº¦ï¼ˆCalibrated Scalesï¼‰**  
  å¼•å…¥ FP8 é‡åŒ–å¹¶ä½¿ç”¨æœ€å¤§å€¼æ ¡å‡†ï¼ˆmax calibrationï¼‰ç¡®å®šç¼©æ”¾å› å­ï¼Œä¸”å°†ç¼©æ”¾å› å­å‘ä¸Šå–æ•´ä¸º 2 çš„å¹‚æ¬¡ï¼Œå…¼é¡¾ç²¾åº¦ä¸ç¡¬ä»¶å‹å¥½æ€§ã€‚

- **å¼ºåŒ–å­¦ä¹ å¾®è°ƒ + Checkpoint å¹³å‡**  
  è®­ç»ƒä¸¤ä¸ª RL å˜ä½“ï¼šä¸€ä¸ªä¸“æ³¨é«˜æ¨ç†åŠªåŠ›ï¼ˆhigh-effortï¼‰ä»¥æå‡å‡†ç¡®æ€§ï¼Œå¦ä¸€ä¸ªé‡‡ç”¨æ··åˆæ•°æ®ä»¥æ§åˆ¶ç”Ÿæˆé•¿åº¦ã€‚æœ€ç»ˆé€šè¿‡ **checkpoint averaging** èåˆä¸¤è€…ä¼˜åŠ¿ï¼Œåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶æŠ‘åˆ¶è¿‡åº¦ç”Ÿæˆã€‚

- **è¯·æ±‚çº§æ•ˆç‡è¯„ä¼°æŒ‡æ ‡ï¼ˆRequest-Level Efficiencyï¼‰**  
  æå‡ºæ–°çš„è¯„ä¼°èŒƒå¼ï¼š`ç›¸å¯¹è¯·æ±‚é€Ÿç‡ï¼ˆrelative request rateï¼‰ = æœ€å¤§ååé‡ / å¹³å‡ç”Ÿæˆ token æ•°`ï¼Œå½’ä¸€åŒ–åå½¢æˆ **accuracy-speed frontier**ï¼Œæ›´çœŸå®åæ˜ å®é™…éƒ¨ç½²è¡¨ç°ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **æ¶æ„çµæ´»æ€§** | æ”¯æŒå¼‚æ„ MoE å‰ªæå’Œæ··åˆæ³¨æ„åŠ›ç»“æ„ï¼Œä¼˜äºç»Ÿä¸€å‰ªææˆ–å…¨é‡æ›¿æ¢ç­–ç•¥ |
| **é•¿ä¸Šä¸‹æ–‡é€‚é…æ€§** | ä½¿ç”¨ä»»åŠ¡æ„ŸçŸ¥çš„ AA-LCR è¯„åˆ†æŒ‡å¯¼ window attention æ›¿æ¢ï¼Œä¼˜äºä»…ä¾èµ–å±€éƒ¨æ¿€æ´»åŒ¹é…çš„æ–¹æ³• |
| **KV-cache ä¼˜åŒ–** | FP8 + calibrated scales å®ç° ~2Ã— KV å®¹é‡æå‡ï¼Œä¼˜äºæ—  scale æˆ–é™æ€ scale æ–¹æ¡ˆ |
| **æ•ˆç‡-è´¨é‡å¹³è¡¡** | è¯·æ±‚çº§æŒ‡æ ‡æ­ç¤ºçœŸå®æ”¶ç›Šï¼Œé¿å…â€œååç¿»å€ä½† trace åŠ å€â€çš„è™šå‡ä¼˜åŒ– |
| **ç«¯åˆ°ç«¯æ€§èƒ½** | åœ¨å¤šä¸ªç»´åº¦å…¨é¢è¶…è¶ŠåŸºçº¿ï¼Œå°¤å…¶åœ¨å•å¡åœºæ™¯ä¸‹è¡¨ç°çªå‡º |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **è®­ç»ƒä¸è’¸é¦æ•°æ®**ï¼šåŸºäº `nvidia/Llama-Nemotron-Post-Training-Dataset` æ„å»ºçš„ **LNPT-gpt-oss** æ•°æ®é›†ï¼Œæ¶µç›–æ•°å­¦ã€ä»£ç ã€é€šç”¨æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡ã€‚
- **å“åº”ç”Ÿæˆæ–¹å¼**ï¼šä½¿ç”¨çˆ¶æ¨¡å‹ `gpt-oss-120B` å¯¹æç¤ºè¯åˆ†åˆ«åœ¨ **high** å’Œ **medium reasoning effort** ä¸‹ç”Ÿæˆå“åº”ï¼Œç”¨äºåç»­ KD å’Œ RLã€‚
- **è¯„åˆ†ä¸“ç”¨æ•°æ®**ï¼š
  - MoE ä¸“å®¶è´¡çŒ®è¯„åˆ†ï¼š12K æ¡ 8K é•¿åº¦æ‰“åŒ…åºåˆ—ï¼›
  - Replace-1-block è¯„åˆ†ï¼š128 æ¡ 32K é•¿åº¦åºåˆ—ï¼›
  - AA-LCR ç”¨äºæ³¨æ„åŠ›å±‚å¯æ›¿æ¢æ€§è¯„ä¼°ã€‚

---

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### ç¡¬ä»¶å¹³å°
- ä¸»è¦æµ‹è¯•ç¯å¢ƒï¼š**8Ã—H100 80GB HBM3 èŠ‚ç‚¹**
- è¡¥å……æµ‹è¯•ï¼š**å•å¼  H100 GPU**

#### æ¨ç†åœºæ™¯
| åœºæ™¯ | è¾“å…¥/è¾“å‡ºé•¿åº¦ | ç‰¹ç‚¹ |
|------|----------------|------|
| çŸ­ä¸Šä¸‹æ–‡ | 4K / 4K | MoE è®¡ç®—ä¸»å¯¼ |
| é•¿ä¸Šä¸‹æ–‡ | 64K / 64K æˆ– 128K | KV-cache å†…å­˜ä¸å¸¦å®½ä¸»å¯¼ |

#### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Per-token throughput (tok/s)** | æ¯ç§’å¤„ç† token æ•°ï¼Œè¡¡é‡æ¶æ„æ•ˆç‡ |
| **Decode latency (ITL)** | å•ä¸ª token è§£ç å»¶è¿Ÿ |
| **Suite-average accuracy (%)** | å¤šä¸ªåŸºå‡†ä»»åŠ¡å¹³å‡å¾—åˆ† |
| **Relative request rate** | è¯·æ±‚çº§æ•ˆç‡æŒ‡æ ‡ï¼š`max throughput / avg gen tokens`ï¼Œå½’ä¸€åŒ–äºåŸºçº¿ |
| **Effort Length Ratio** | é«˜/ä½æ¨ç†åŠªåŠ›ä¸‹çš„ç”Ÿæˆé•¿åº¦æ¯”ï¼Œè¡¡é‡å¯æ§æ€§ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Parent Model**ï¼š`gpt-oss-120B`ï¼ˆBF16 / FP8 KVï¼‰
- **Compressed Derivative**ï¼š`HyperNova-60B`ï¼ˆç¬¬ä¸‰æ–¹å‹ç¼©ç‰ˆæœ¬ï¼‰
- **æœ¬å·¥ä½œæ¨¡å‹**ï¼š`gpt-oss-puzzle-88B`ï¼ˆKV BF16 / FP8ï¼‰

æ‰€æœ‰æ¨¡å‹å‡åœ¨ç›¸åŒæ¡ä»¶ä¸‹æµ‹è¯•ï¼ˆvLLM v0.11.2ï¼‰ï¼Œè€ƒè™‘æœ€ä¼˜ TP å’Œ batch size é…ç½®ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆ8Ã—H100 èŠ‚ç‚¹ï¼‰

| åœºæ™¯ | æ¨¡å‹ | ååé‡ (tok/s) | Speedup |
|------|------|----------------|---------|
| 4K/4K | gpt-oss-120B | 29.6K | 1.00Ã— |
|        | gpt-oss-puzzle-88B | **36.1K** | **1.22Ã—** |
| 64K/64K | gpt-oss-120B | 5.7K | 1.00Ã— |
|         | gpt-oss-puzzle-88B | **9.3K** | **1.63Ã—** |

> âœ… è¾¾æˆé¢„è®¾ç›®æ ‡ï¼šçŸ­ä¸Šä¸‹æ–‡ 1.2Ã—ï¼Œé•¿ä¸Šä¸‹æ–‡ 1.6Ã— ååæå‡

---

### å•å¡æ€§èƒ½ï¼ˆH100 GPUï¼‰

| åœºæ™¯ | æ¨¡å‹ | ååé‡ (tok/s) | Speedup |
|------|------|----------------|---------|
| 4K/4K | gpt-oss-120B | 0.3K | 1.00Ã— |
|        | gpt-oss-puzzle-88B | **0.8K** | **2.44Ã—** |
| 64K/64K | gpt-oss-120B | 0.3K | 1.00Ã— |
|         | gpt-oss-puzzle-88B | **0.8K** | **2.82Ã—** |

> ğŸ’¡ å•å¡å¢ç›Šæ›´é«˜ï¼Œå› åŸæ¨¡å‹æ˜“å— batch size é™åˆ¶ï¼Œè€Œä¼˜åŒ–åé‡Šæ”¾å†…å­˜æ”¯æŒæ›´å¤§ batchã€‚

---

### å‡†ç¡®ç‡ä¿ç•™æƒ…å†µï¼ˆsuite-average, KV FP8ï¼‰

| æ¨ç†åŠªåŠ› | gpt-oss-120B | gpt-oss-puzzle-88B | Retention |
|----------|---------------|--------------------|-----------|
| High | 58.19% | **58.67%** | **100.8%** |
| Medium | 52.89% | **54.93%** | **103.9%** |
| Low | 44.71% | **48.38%** | **108.2%** |

> ğŸ¯ åœ¨ä»…ä¿ç•™çº¦ 73% å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå®ç°äº† **è·¨æ¨ç†åŠªåŠ›æ°´å¹³çš„å‡†ç¡®ç‡æŒå¹³ç”šè‡³åè¶…**

---

### è¯·æ±‚çº§æ•ˆç‡ï¼ˆRelative Request Rateï¼‰

ä» Figure 1 å¯è§ï¼Œ`gpt-oss-puzzle-88B` åœ¨æ•´ä¸ª **accuracy-speed frontier** ä¸Šä¼˜äº `gpt-oss-120B` å’Œ `HyperNova-60B`ï¼š
- æœ€é«˜è¾¾åˆ° **1.29Ã— æ›´é«˜çš„è¯·æ±‚çº§æ•ˆç‡**
- å°½ç®¡ `HyperNova-60B` æœ‰è¾ƒé«˜ååï¼Œä½†ç”±äºç”Ÿæˆ token æ•°æ˜¾è‘—å¢åŠ ï¼Œå…¶è¯·æ±‚çº§æ•ˆç‡è¢«æŠµæ¶ˆ

---

### æ¶ˆèå®éªŒç»“æœ

#### (1) æ³¨æ„åŠ›å±‚è¯„åˆ†æ–¹å¼å¯¹æ¯”ï¼ˆAppendix Cï¼‰
| æ›¿æ¢ç­–ç•¥ | å…¨å±€å±‚æ•° | çª—å£å¤§å° | MMLU-Pro | GPQA | AIME25 | SciCode | AALCR |
|--------|--------|--------|--------|--------|--------|--------|--------|
| Activation-MSE | 9 | 8192 | 78.32 | 68.69 | 79.17 | 41.12 | 8.00 |
| **AA-LCR-based** | 9 | 8192 | **78.92** | **73.42** | **88.12** | **40.24** | **14.00** |

> âœ… ä½¿ç”¨ AA-LCR ä½œä¸ºè¯„åˆ†ä¿¡å·èƒ½æ›´å¥½åœ°ä¿ç•™é•¿ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼ˆAALCR æå‡æ˜¾è‘—ï¼‰

#### (2) KV é‡åŒ–ç¼©æ”¾å› å­å½±å“ï¼ˆTable 4ï¼‰
| è®¾ç½® | High-effort Accuracy |
|------|------------------------|
| No Scales | 58.14% |
| **With Calibrated Scales** | **58.67%** |

> âœ… æ˜¾è‘—æå‡ç²¾åº¦ï¼ŒéªŒè¯ scale æ ¡å‡†çš„é‡è¦æ€§

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Puzzle æ¡†æ¶å¯æˆåŠŸæ‰©å±•è‡³ MoE + é•¿ä¸Šä¸‹æ–‡åœºæ™¯**  
   é€šè¿‡å¼‚æ„å‰ªæã€ä»»åŠ¡æ„ŸçŸ¥è¯„åˆ†ã€FP8 é‡åŒ–ç­‰æ‰‹æ®µï¼Œå¯åœ¨å¤§å¹…é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶ç»´æŒç”šè‡³æå‡æ¨¡å‹è´¨é‡ã€‚

2. **per-token ååä¸æ˜¯å”¯ä¸€æ ‡å‡†ï¼Œå¿…é¡»ç»“åˆç”Ÿæˆé•¿åº¦çœ‹è¯·æ±‚çº§æ•ˆç‡**  
   â€œååç¿»å€ä½†æ¨ç†å˜é•¿ä¸¤å€â€ç­‰äºé›¶æ”¶ç›Šï¼›æœ¬æ–‡æå‡ºçš„ **request-level efficiency** æ˜¯æ›´åˆç†çš„è¯„ä¼°èŒƒå¼ã€‚

3. **å¤šç§ä¼˜åŒ–æ‰‹æ®µååŒä½œç”¨æ•ˆæœæ˜¾è‘—**  
   MoE å‰ªæ + window attention + FP8 KV + RL å¾®è°ƒå…±åŒè´¡çŒ®æœ€ç»ˆæ€§èƒ½çªç ´ã€‚

4. **AA-LCR æ˜¯æœ‰æ•ˆçš„ long-context æ•æ„Ÿè¯„åˆ†å™¨**  
   ç›¸æ¯”æ¿€æ´» MSEï¼Œå®ƒæ›´èƒ½è¯†åˆ«å¯¹é•¿ç¨‹æ¨ç†è‡³å…³é‡è¦çš„æ³¨æ„åŠ›å±‚ï¼Œé˜²æ­¢å…³é”®å±‚è¢«é”™è¯¯æ›¿æ¢ã€‚

5. **checkpoint averaging æ˜¯å¹³è¡¡ accuracy ä¸ verbosity çš„æœ‰æ•ˆç­–ç•¥**  
   æˆåŠŸèåˆâ€œé«˜å‡†ç¡®â€ä¸â€œä½ç”Ÿæˆé•¿åº¦â€ä¸¤ç§æç«¯ç­–ç•¥çš„ä¼˜ç‚¹ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–é«˜è´¨é‡è¯„åˆ†æ•°æ®é›†**ï¼šéœ€è¦ä¸çˆ¶æ¨¡å‹åˆ†å¸ƒä¸€è‡´çš„è®­ç»ƒæ•°æ®æ¥è¿›è¡Œå¯é çš„é‡è¦æ€§ä¼°è®¡ã€‚
- **æœç´¢ç©ºé—´å—é™**ï¼šç›®å‰ä»…æ¢ç´¢ MoE ä¸“å®¶æ•°å’Œ attention ç±»å‹ï¼Œæœªæ¶‰åŠ FFN ç»“æ„æˆ–å…¶ä»–ç¨€ç–åŒ–æ–¹å¼ã€‚
- **é•¿ä¸Šä¸‹æ–‡è¯„åˆ†å¼€é”€è¾ƒå¤§**ï¼šAA-LCR æµ‹è¯•æœ¬èº«è€—æ—¶ï¼Œéš¾ä»¥å¤§è§„æ¨¡éå†ã€‚
- **æœªå¼€æ”¾å®Œæ•´è®­ç»ƒç»†èŠ‚**ï¼šå¦‚ RL é˜¶æ®µçš„å…·ä½“å¥–åŠ±å‡½æ•°è®¾è®¡æœªå®Œå…¨æŠ«éœ²ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±• Puzzle è‡³æ›´å¤šæ–°å‹æ¨¡å—ï¼ˆå¦‚ Mambaã€MQA/GQA ç­‰ï¼‰
- æ¢ç´¢åŠ¨æ€æ¨ç†è·¯å¾„é€‰æ‹©ï¼ˆdynamic early exitingï¼‰ä¸ Puzzle çš„ç»“åˆ
- å¼€å‘æ›´é«˜æ•ˆçš„ long-context proxy task ç”¨äºå¿«é€Ÿè¯„åˆ†
- å°†è¯¥æµç¨‹è‡ªåŠ¨åŒ–å¹¶é›†æˆè¿›æ¨ç†å¼•æ“ï¼ˆå¦‚ TensorRT-LLMï¼‰
- æ¨åŠ¨ç¤¾åŒºå»ºç«‹æ ‡å‡†åŒ–çš„ **request-level efficiency benchmark**

--- 

> ğŸ”š æ€»ç»“ï¼šæœ¬æ–‡å±•ç¤ºäº† **post-training NAS** åœ¨ç°ä»£æ¨ç†å‹ LLM ä¸Šçš„å·¨å¤§æ½œåŠ›ï¼Œ`gpt-oss-puzzle-88B` ä¸ä»…å®ç°äº†é«˜è¾¾ **2.82Ã— å•å¡åååŠ é€Ÿ**ï¼Œè¿˜åœ¨å¤šæ•°ä»»åŠ¡ä¸Š **åŒ¹é…æˆ–è¶…è¿‡åŸå§‹ 120B æ¨¡å‹çš„è¡¨ç°**ï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²æä¾›äº†å®ç”¨èŒƒä¾‹ã€‚

</details>

---

### 3. [T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization](https://arxiv.org/abs/2602.12262)

**Authors**: Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang, Vladimir Pavlovic, Dimitris N. Metaxas  
**Category**: cs.CL  
**Published**: 2026-02-13  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.12262v1  

#### Abstract
Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substan...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šT3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
**Diffusion Large Language Models (DLLMs)** è™½ç„¶æ”¯æŒå¹¶è¡Œè§£ç ä»¥æå‡ç”Ÿæˆé€Ÿåº¦ï¼Œä½†åœ¨å®é™…æ¨ç†ä¸­ä»éœ€å¤§é‡å»å™ªæ­¥éª¤ï¼ˆdiffusion stepsï¼‰ï¼Œå¯¼è‡´å»¶è¿Ÿè¾ƒé«˜ã€‚å½“å¼ºè¡Œå‡å°‘æ­¥æ•°æ—¶ï¼Œç”Ÿæˆè´¨é‡ä¼šæ˜¾è‘—ä¸‹é™ã€‚è¿™ä¸€çŸ›ç›¾é™åˆ¶äº†å…¶åœ¨å®æ—¶åœºæ™¯ï¼ˆå¦‚å¯¹è¯ç³»ç»Ÿã€è¾¹ç¼˜è®¾å¤‡ï¼‰ä¸­çš„åº”ç”¨ã€‚

æ­¤å¤–ï¼Œç°æœ‰ few-step æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š
- **Mean-field approximation error**ï¼šç”±äº token-wise factorized å‚æ•°åŒ–å‡è®¾ï¼Œåœ¨å°‘æ­¥è§£ç ä¸‹è¯¯å·®éšæ­¥æ•°å‡å°‘è€Œå¢å¤§ã€‚
- **Train-inference distribution mismatch**ï¼šè®­ç»ƒæ—¶ä½¿ç”¨éšæœºæ©ç ï¼ˆrandom maskingï¼‰ï¼Œè€Œæ¨ç†æ—¶é‡‡ç”¨åŸºäºç½®ä¿¡åº¦çš„ééšæœºè°ƒåº¦ï¼ˆconfidence-based schedulesï¼‰ï¼Œå¯¼è‡´ä¸­é—´çŠ¶æ€åˆ†å¸ƒä¸ä¸€è‡´ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡º **T3D (Trajectory Self-Distillation via DDO)**ï¼Œä¸€ç§é€šè¿‡**è½¨è¿¹è‡ªè’¸é¦**ï¼ˆtrajectory self-distillationï¼‰ç»“åˆ**ç›´æ¥åˆ¤åˆ«ä¼˜åŒ–**ï¼ˆDirect Discriminative Optimization, DDOï¼‰æ¥æå‡å°‘æ­¥è§£ç æ€§èƒ½çš„æ–°æ¡†æ¶ã€‚å…¶ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ä¸ºï¼š

1. **Trajectory Self-Distillation**  
   åˆ©ç”¨é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹è‡ªèº«åœ¨ç›®æ ‡è§£ç ç­–ç•¥ä¸‹çš„å®Œæ•´ç”Ÿæˆè½¨è¿¹ï¼ˆä»å…¨æ©ç åˆ°å¹²å‡€åºåˆ—çš„ä¸­é—´çŠ¶æ€åºåˆ—ï¼‰ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œå¯¹å°‘é‡æ­¥çš„å­¦ç”Ÿæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¿™ä½¿å¾—å­¦ç”Ÿæ¨¡å‹åœ¨ç­–ç•¥ä¸Šï¼ˆon-policyï¼‰å­¦ä¹ çœŸå®çš„æ¨ç†è·¯å¾„ï¼Œç¼“è§£äº† train-inference åˆ†å¸ƒåç§»ã€‚

2. **Direct Discriminative Optimization (DDO)**  
   å¼•å…¥ä¸€ç§ GAN å¯å‘å¼çš„åå‘ KLï¼ˆreverse-KLï¼‰é£æ ¼ç›®æ ‡å‡½æ•°ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„å‰å‘ KLï¼ˆforward-KLï¼‰åŒ¹é…ã€‚DDO é¼“åŠ±â€œæ¨¡å¼å¯»æ±‚â€ï¼ˆmode-seekingï¼‰è¡Œä¸ºï¼Œä½¿å­¦ç”Ÿæ›´å…³æ³¨é«˜æ¦‚ç‡çš„æ•™å¸ˆç”Ÿæˆè·¯å¾„ï¼Œé¿å…å› å¤šæ¨¡æ€åéªŒå¯¼è‡´çš„é¢„æµ‹å¹³æ»‘åŒ–ï¼ˆover-smoothingï¼‰é—®é¢˜ã€‚

3. **Path Consistency Regularization**  
   å¯¹æ—©æœŸè§£ç çš„ token æ–½åŠ æ›´é«˜çš„æŸå¤±æƒé‡ï¼Œä»¥æŠ‘åˆ¶é”™è¯¯ä¼ æ’­ã€‚å…¬å¼ä¸­å®šä¹‰æ¯ token çš„è§£ç æ­¥æ•° $ \tau_i $ï¼Œåˆ™æƒé‡ä¸ºï¼š
   $$
   w_i = \frac{B - \tau_i + 1}{B}
   $$
   å…¶ä¸­ $ B $ æ˜¯æ€»æ­¥æ•°é¢„ç®—ã€‚

æœ€ç»ˆç›®æ ‡å‡½æ•°ä¸ºï¼š
$$
\mathcal{L}_{T3D}(\theta) = \mathcal{L}_{\text{traj-DDO}}(\theta) + \lambda \mathcal{L}_{\text{path}}(\theta)
$$

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ— éœ€é¢å¤–æ ‡æ³¨æ•°æ®**ï¼šå®Œå…¨åŸºäºæ•™å¸ˆæ¨¡å‹è‡ªç”Ÿæˆçš„è½¨è¿¹è¿›è¡Œè’¸é¦ï¼Œå®ç°æ— ç›‘ç£ few-step å¾®è°ƒã€‚
- **æ›´å¼ºçš„ç”Ÿæˆä¸€è‡´æ€§**ï¼šé€šè¿‡è½¨è¿¹çº§ç›‘ç£é™ä½æ¡ä»¶ä¾èµ–å»ºæ¨¡è¯¯å·®ï¼ˆconditional total correlationï¼‰ï¼Œç†è®ºè¯æ˜å…¶ç­‰ä»·äºå¹¿ä¹‰çš„ Discrete Rectified Flowã€‚
- **æ›´é²æ£’çš„å°‘æ­¥è§£ç èƒ½åŠ›**ï¼šåœ¨æç«¯å‹ç¼©ï¼ˆå¦‚ 1â€“2 æ­¥å®Œæˆä¸€ä¸ª blockï¼‰ä¸‹ä»èƒ½ä¿æŒé«˜è´¨é‡ç”Ÿæˆï¼Œæ˜¾è‘—ç¼©å°ä¸ full-step è§£ç ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
#### è®­ç»ƒæ•°æ®é›†ï¼ˆç”¨äº self-distillationï¼‰
- **æ•°å­¦æ¨ç†ä»»åŠ¡**ï¼š`MATH` training setï¼ˆHendrycks et al., 2021ï¼‰
- **ä»£ç ç”Ÿæˆä»»åŠ¡**ï¼š`PrimeIntellect` datasetï¼ˆJaghouar et al., 2024ï¼‰

#### åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ˆevaluation benchmarksï¼‰
- **MATH500**ï¼šé«˜ä¸­åŠç«èµ›çº§åˆ«æ•°å­¦é¢˜
- **GSM8K**ï¼šå°å­¦ç®—æœ¯æ–‡å­—é¢˜
- **MBPP**ï¼šPython ç¼–ç¨‹ä»»åŠ¡
- **HumanEval**ï¼šå‡½æ•°çº§ä»£ç ç”ŸæˆæŒ‘æˆ˜

è¿™äº›ä»»åŠ¡å‡éœ€è¦å¤šæ­¥æ¨ç†æˆ–è¿­ä»£ä¿®æ­£ï¼Œå¯¹è§£ç è´¨é‡æ•æ„Ÿï¼Œé€‚åˆè¯„ä¼°å°‘æ­¥ç”Ÿæˆçš„ç¨³å¥æ€§ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
#### æ¨¡å‹æ¶æ„
æ‰€æœ‰å®éªŒåŸºäº **SDAR-family models**ï¼ˆCheng et al., 2025ï¼‰ï¼š
- `SDAR-1.7B-Chat`
- `SDAR-4B-Chat`

SDAR æ˜¯ä¸€ç§å—å¼æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆblock-based diffusion LMï¼‰ï¼Œåœ¨å—å†…ä½¿ç”¨ diffusion è§£ç ï¼Œåœ¨å—é—´ä¿æŒè‡ªå›å½’ï¼Œå½¢æˆ semi-autoregressive ç”Ÿæˆæµç¨‹ã€‚

#### å°‘æ­¥è§£ç é…ç½®
- **Block Size**: 4 æˆ– 8
- **Tokens Per Step (TokPS)**: æ§åˆ¶æ¯ä¸ª diffusion step è§£ç å¤šå°‘ token
  - å¦‚ Block Size=4, TokPS=2 â†’ 2 æ­¥å®Œæˆä¸€ä¸ª block
  - TokPS è¶Šå¤§ï¼Œå‹ç¼©è¶Šæ¿€è¿›

#### åŠ¨æ€è§£ç å®éªŒï¼ˆdynamic decodingï¼‰
- ä½¿ç”¨ confidence threshold = 0.9 è‡ªé€‚åº”å†³å®šæ¯æ­¥è§£ç  token æ•°é‡
- æŠ¥å‘Šååé‡ï¼ˆTPSï¼‰ã€å»¶è¿Ÿï¼ˆlatencyï¼‰ã€å¹³å‡æ­¥æ•°ã€å‡†ç¡®ç‡ï¼ˆaccï¼‰

#### è¯„ä¼°æŒ‡æ ‡
- **Accuracy**ï¼šå„ benchmark ä¸Šçš„ä»»åŠ¡æ­£ç¡®ç‡
- **Throughput (TPS)**ï¼štokens per second
- **Latency**ï¼šå•æ ·æœ¬å“åº”æ—¶é—´
- **Average Decoding Steps**ï¼šåŠ¨æ€è§£ç ä¸‹çš„å¹³å‡å»å™ªæ­¥æ•°

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | æè¿° |
|------|------|
| **Original Model** | æœªå¾®è°ƒçš„åŸå§‹ SDAR æ¨¡å‹ |
| **SFT** | åœ¨çœŸå®æ•°æ®ä¸Šç›‘ç£å¾®è°ƒï¼ˆsupervised fine-tuningï¼‰ |
| **ReDi** (Yoo et al., 2025) | æ ‡å‡†è‡ªè’¸é¦æ–¹æ³•ï¼Œå¯¹åº” marginal matching |
| **dParallel** (Chen et al., 2025) | æœ€å¤§åŒ–ä»å…¨æ©ç åˆ°è¾“å‡ºçš„è½¬ç§»æ¦‚ç‡ |
| **Naive TD** | ä»…ä½¿ç”¨ forward-KL çš„è½¨è¿¹è’¸é¦ï¼ˆEqn. 5ï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1ï¼‰

#### åœ¨ `SDAR-1.7B-Chat` ä¸Šï¼ˆBlock Size=8, TokPS=4ï¼‰
| Method | MATH500 â†‘ | GSM8K â†‘ | MBPP â†‘ | HumanEval â†‘ | Avg Imp (%) |
|--------|------------|---------|--------|-------------|--------------|
| ReDi | 15.00 | 32.45 | 3.40 | 5.49 | 135.95 â†“ |
| dParallel | 22.80 | 45.26 | 10.20 | 12.20 | 128.09 â†“ |
| **T3D (Ours)** | **25.60** | **42.91** | **9.40** | **15.24** | **124.79 â†‘** |

> âœ… T3D åœ¨æœ€æ¿€è¿›è®¾ç½®ä¸‹ä»ä¿æŒç¨³å®šï¼Œä¼˜äºæ‰€æœ‰ baselineã€‚

#### åœ¨ `SDAR-4B-Chat` ä¸Šï¼ˆBlock Size=4, TokPS=4ï¼‰
| Method | MATH500 â†‘ | GSM8K â†‘ | MBPP â†‘ | HumanEval â†‘ | Avg Imp (%) |
|--------|------------|---------|--------|-------------|--------------|
| ReDi | 25.40 | 53.30 | 5.00 | 7.32 | 0.65 â†“ |
| dParallel | 34.20 | 45.94 | 13.20 | 20.73 | 39.05 â†“ |
| **T3D (Ours)** | **47.80** | **69.90** | **22.60** | **23.78** | **85.02 â†‘** |

> âœ… T3D å®ç°å¤§å¹…é¢†å…ˆï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡ï¼ˆMATH500ï¼‰ä¸Šæå‡æ˜æ˜¾ã€‚

---

### ä¿ç•™å…¨æ­¥è§£ç æ€§èƒ½ï¼ˆTable 2ï¼‰
å°† few-step å¾®è°ƒåçš„æ¨¡å‹æ¢å¤ä¸º full-step è§£ç ï¼ˆstatic decoding, 1 token/stepï¼‰ï¼Œè¯„ä¼°æ˜¯å¦é—å¿˜åŸå§‹æ‰©æ•£èƒ½åŠ›ï¼š

| Model | Method | MATH500 (Full-step Acc) |
|-------|--------|--------------------------|
| SDAR-1.7B | Original | 59.40 |
| SDAR-1.7B | ReDi | 47.00 â†“ |
| SDAR-1.7B | dParallel | 0.40 â†“â†“ |
| SDAR-1.7B | **T3D (Ours)** | **56.80** â‰ˆ Original |
| SDAR-4B | Original | 68.00 |
| SDAR-4B | **T3D (Ours)** | **70.00** > Original âœ… |

> ğŸ” å‘ç°ï¼šT3D å‡ ä¹å®Œå…¨ä¿ç•™ç”šè‡³ç•¥å¾®è¶…è¶ŠåŸæ¨¡å‹çš„ full-step æ€§èƒ½ï¼Œè¯´æ˜å…¶æœªè¿‡æ‹Ÿåˆäºå°‘æ­¥è§£ç ã€‚

---

### åŠ¨æ€è§£ç æ€§èƒ½ï¼ˆTable 3ï¼‰
å°½ç®¡ T3D åœ¨é™æ€æ­¥æ•°ä¸‹è®­ç»ƒï¼Œä½†åœ¨åŠ¨æ€è§£ç ä¸­ä¾ç„¶è¡¨ç°ä¼˜å¼‚ï¼š

| Benchmark | Method | Accuracy â†‘ | Latency â†“ | TPS â†‘ |
|-----------|--------|------------|-----------|--------|
| MATH500 | Original | 39.00 | 1.10 | 657.72 |
| MATH500 | **T3D** | **49.40** | **0.66** | **791.23** |
| GSM8K | **T3D** | **72.40** | **0.37** | **843.05** |
| MBPP | **T3D** | 23.60 | 0.19 | 313.18 |
| HumanEval | **T3D** | **29.27** | **0.26** | **222.68** |

> âœ… T3D åœ¨åŠ¨æ€è§£ç ä¸­å®ç°æ›´é«˜å‡†ç¡®ç‡ + æ›´ä½å»¶è¿Ÿ + æ›´é«˜ååï¼ŒéªŒè¯å…¶æ³›åŒ–èƒ½åŠ›ã€‚

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### D.1ï¼šæ­£åˆ™åŒ–ç³»æ•° $ \lambda $ çš„å½±å“ï¼ˆTable 4ï¼‰
- $ \lambda = 0.2 $ åœ¨å¤šæ•°é…ç½®ä¸‹è¾¾åˆ°æœ€ä¼˜å¹³è¡¡
- è¿‡å°ï¼ˆ0.05ï¼‰æ— æ³•æœ‰æ•ˆçº¦æŸï¼›è¿‡å¤§ï¼ˆ0.5ï¼‰å¯èƒ½æŸå®³æ€§èƒ½
- é»˜è®¤è®¾ä¸º **0.2**

#### D.2ï¼šç»„ä»¶å¯¹ full-step æ€§èƒ½çš„å½±å“ï¼ˆTable 5ï¼‰
| ç»„ä»¶ç»„åˆ | MATH500 Acc |
|---------|-------------|
| Naive TD | 22.00 â†“ |
| Naive TD + Lpath | 58.00 |
| Naive TD + Random Tokens | 57.40 |
| **DDO + Lpath + Random** | **69.00** âœ… |

> ğŸ” å‘ç°ï¼š**DDO + Path Consistency + Random Token Mixing** ä¸‰è€…ååŒæ˜¯æˆåŠŸçš„å…³é”®ã€‚

#### D.3ï¼šfew-step ç”Ÿæˆæ¶ˆèï¼ˆTable 6 & 7ï¼‰
- DDO æ˜¾è‘—ä¼˜äº Naive TDï¼Œå°¤å…¶æ˜¯åœ¨å¼ºå‹ç¼©ï¼ˆ2 steps/blockï¼‰ä¸‹
- Random initialization å¯¹ DDO è‡³å…³é‡è¦ï¼ˆæå‡ç¨³å®šæ€§ï¼‰
- Path consistency æå‡é²æ£’æ€§ï¼Œå°¤å…¶åœ¨ä¸­ç­‰å‹ç¼©ä¸‹æ•ˆæœæ˜¾è‘—

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **è½¨è¿¹è‡ªè’¸é¦å¯æœ‰æ•ˆç¼“è§£ train-inference åˆ†å¸ƒåç§»**ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹åœ¨çœŸå®æ¨ç†è·¯å¾„ä¸Šå­¦ä¹ ï¼Œæå‡å°‘æ­¥è§£ç æ•ˆç‡ã€‚
2. **DDO çš„ reverse-KL ç‰¹æ€§æ›´é€‚åˆå¤„ç†é«˜åº¦å¤šæ¨¡æ€çš„å»å™ªåéªŒ**ï¼Œé¿å… forward-KL å¯¼è‡´çš„é¢„æµ‹æ¨¡ç³Šé—®é¢˜ã€‚
3. **T3D èƒ½åœ¨æç«¯å°‘æ­¥è®¾ç½®ä¸‹ä¿æŒé«˜æ€§èƒ½ï¼Œå¹¶å‡ ä¹æ— æŸåœ°ä¿ç•™ full-step è§£ç èƒ½åŠ›**ï¼Œå®ç°äº†â€œä¸¤å…¨å…¶ç¾â€ã€‚
4. **è¯¥æ–¹æ³•å…¼å®¹åŠ¨æ€è§£ç ç­–ç•¥**ï¼Œåœ¨å®è·µä¸­å±•ç°å‡ºæ›´é«˜çš„ååä¸æ›´ä½å»¶è¿Ÿã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ–¹æ³•ä¾èµ–äºé«˜è´¨é‡æ•™å¸ˆè½¨è¿¹ç”Ÿæˆï¼Œè‹¥æ•™å¸ˆæœ¬èº«ä¸ç¨³å®šï¼Œä¼šå½±å“è’¸é¦æ•ˆæœã€‚
- DDO çš„ä¼˜åŒ–è¿‡ç¨‹å¯èƒ½è¾ƒæ•æ„Ÿï¼Œéœ€ careful tuning of $ \lambda $ å’Œ reference model æ›´æ–°é¢‘ç‡ã€‚
- å®éªŒé›†ä¸­åœ¨æ•°å­¦ä¸ä»£ç ä»»åŠ¡ï¼Œé€šç”¨æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚å¯¹è¯ã€æ‘˜è¦ï¼‰ä¸Šçš„è¡¨ç°æœ‰å¾…è¿›ä¸€æ­¥éªŒè¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢å°† T3D æ‰©å±•è‡³å…¶ä»–ç±»å‹çš„ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ã€å¯¹è¯ï¼‰ã€‚
- ç»“åˆå¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–åŠ¨æ€è§£ç ç­–ç•¥ã€‚
- è®¾è®¡æ›´é«˜æ•ˆçš„ reference model æ›´æ–°æœºåˆ¶ï¼ˆå¦‚ exponential moving averageï¼‰ã€‚
- å°† T3D ä¸å…¶ä»–åŠ é€ŸæŠ€æœ¯ï¼ˆå¦‚ KV Cache for DLLMsï¼‰é›†æˆï¼Œæ„å»ºç«¯åˆ°ç«¯é«˜æ•ˆæ¨ç†ç³»ç»Ÿã€‚

---

> ğŸŒŸ **æ€»ä½“è¯„ä»·**ï¼šT3D æä¾›äº†ä¸€ä¸ªç®€æ´è€Œå¼ºå¤§çš„æ¡†æ¶ï¼Œæ¨åŠ¨äº† **practical few-step DLLMs** çš„å‘å±•ï¼Œä¸ºå®ç°é«˜é€Ÿã€é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆæä¾›äº†åšå®åŸºç¡€ã€‚  
> æºç å·²å¼€æºï¼š[https://github.com/Tyrion58/T3D](https://github.com/Tyrion58/T3D)

</details>

---

### 4. [RL over Commodity Networks: Overcoming the Bandwidth Barrier with Lossless Sparse Deltas](https://arxiv.org/abs/2602.11456)

**Authors**: Chaoyi Ruan, Geng Luo, Xinyi Wan, Long Zhao, Qinghe Wang, Jiaan Zhu, Duling Xu, Guanbin Xu, Dehui Wei, Xiang Liu, Cheng Li, Haifeng Sun, Congcong Miao, Jialin Li  
**Category**: cs.DC  
**Published**: 2026-02-13  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2602.11456v1  

#### Abstract
LLM post-training with reinforcement learning (RL) requires frequent synchronization of large model parameters between the trainer and distributed rollout actors. High-throughput RL post-training therefore relies on dedicated RDMA HPC clusters, an infrastructure cost most organizations cannot absorb...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*RL over Commodity Networks: Overcoming the Bandwidth Barrier with Lossless Sparse Deltas*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒåé˜¶æ®µå¹¿æ³›é‡‡ç”¨ **Reinforcement Learning (RL)** è¿›è¡Œå¯¹é½ä¼˜åŒ–ï¼ˆå¦‚ RLHFï¼‰ã€‚è¿™ç±»è®­ç»ƒä¾èµ–äº **Trainer** å’Œå¤šä¸ªåˆ†å¸ƒå¼ **Rollout Actors** ä¹‹é—´çš„é¢‘ç¹å‚æ•°åŒæ­¥ã€‚ä¼ ç»Ÿç³»ç»Ÿä¾èµ–é«˜å¸¦å®½ã€ä½å»¶è¿Ÿçš„ **RDMA HPC é›†ç¾¤** æ¥å®ç°é«˜æ•ˆçš„å…¨æƒé‡å¹¿æ’­ï¼ˆfull-weight broadcastï¼‰ï¼Œä½†è¿™ç§åŸºç¡€è®¾æ–½æˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥è¢«å­¦æœ¯æœºæ„æˆ–åˆåˆ›å…¬å¸è·å–ã€‚

å½“å°è¯•å°† RL è®­ç»ƒéƒ¨ç½²åˆ°ç”±æ ‡å‡†ä»¥å¤ªç½‘æˆ–å¹¿åŸŸç½‘ï¼ˆWANï¼‰è¿æ¥çš„â€œæ¾æ•£è€¦åˆâ€GPU èµ„æºï¼ˆå¦‚è·¨äº‘ã€è·¨åŒºåŸŸã€è·¨å®éªŒå®¤ï¼‰æ—¶ï¼Œç½‘ç»œå¸¦å®½æˆä¸ºä¸¥é‡ç“¶é¢ˆã€‚ä¾‹å¦‚ï¼Œåœ¨ 1 Gbps çš„é“¾è·¯ä¸ŠåŒæ­¥ä¸€ä¸ª 8B æ¨¡å‹å¯èƒ½éœ€è¦è¶…è¿‡ 100 ç§’ï¼Œè¿œè¶…ç”Ÿæˆ rollouts æ‰€éœ€æ—¶é—´ï¼ˆé€šå¸¸å‡ åç§’ï¼‰ï¼Œå¯¼è‡´ GPU åˆ©ç”¨ç‡æä½ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **SparrowRL** â€”â€” ä¸€ç§ä¸“ä¸ºå•†å“åŒ–ç½‘ç»œï¼ˆcommodity networksï¼‰è®¾è®¡çš„é«˜æ€§èƒ½ RL è®­ç»ƒç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ RL å¾®è°ƒè¿‡ç¨‹ä¸­å‚æ•°æ›´æ–°çš„**é«˜åº¦ç¨€ç–æ€§**ã€‚

#### ä¸»è¦åˆ›æ–°æœºåˆ¶ï¼š
1. **Lossless Sparse Delta Checkpointsï¼ˆæ— æŸç¨€ç–å¢é‡æ£€æŸ¥ç‚¹ï¼‰**
   - è§‚å¯Ÿåˆ° RL æ¯æ­¥ä»…çº¦ **1% çš„å‚æ•°å…ƒç´ å‘ç”Ÿå˜åŒ–**ã€‚
   - ä¸å†ä¼ è¾“å®Œæ•´æ¨¡å‹æƒé‡ï¼Œè€Œæ˜¯æå–å¹¶ä¼ è¾“**ç¨€ç–çš„å¢é‡ï¼ˆsparse deltaï¼‰**ï¼ŒåªåŒ…å«å˜åŒ–çš„å‚æ•°åŠå…¶ç²¾ç¡®å€¼ã€‚
   - é‡‡ç”¨ **delta-encoded variable-length indexing** å¯¹ç´¢å¼•è¿›è¡Œå‹ç¼©ï¼Œè¿›ä¸€æ­¥å‡å°‘å…ƒæ•°æ®å¼€é”€ã€‚
   - **å®Œå…¨æ— æŸï¼ˆlosslessï¼‰**ï¼Œä¸è¿›è¡Œé‡åŒ–æˆ–ä¸¢å¼ƒï¼Œä¿è¯æ¨¡å‹ç²¾åº¦ä¸å˜ã€‚

2. **Streaming Delta Transfer Protocolï¼ˆæµå¼å¢é‡ä¼ è¾“åè®®ï¼‰**
   - å°†ç¨€ç–å¢é‡åˆ’åˆ†ä¸ºå¤šä¸ªæ®µï¼ˆsegmentsï¼‰ï¼Œé€šè¿‡å¤šè·¯å¹¶è¡Œ TCP æµï¼ˆmulti-streamï¼‰ä¼ è¾“ï¼Œæå‡å¸¦å®½åˆ©ç”¨ç‡å¹¶é™ä½å°¾éƒ¨å»¶è¿Ÿã€‚
   - å®ç° **cut-through forwarding**ï¼šåœ¨ Trainer æå–å¢é‡çš„åŒæ—¶å°±å¼€å§‹ä¼ è¾“å·²ç¼–ç çš„éƒ¨åˆ†ï¼Œå®ç°æå–ä¸ä¼ è¾“çš„æµæ°´çº¿é‡å ã€‚
   - å¼•å…¥ **Relay-based fanout**ï¼šæ¯ä¸ªåŒºåŸŸæŒ‡å®šä¸€ä¸ª Relay èŠ‚ç‚¹æ¥æ”¶å¢é‡ï¼Œå¹¶åœ¨åŒºåŸŸå†…è½¬å‘ç»™å…¶ä»– Actorsï¼Œé¿å…è·¨åŒºåŸŸå¤šæ¬¡ä¼ è¾“ã€‚

3. **Heterogeneity-Aware Scheduling + Lease-Based Fault Tolerance**
   - **å¼‚æ„æ„ŸçŸ¥è°ƒåº¦**ï¼šæ ¹æ®å„ Actor çš„å®é™…ååé‡å’Œç½‘ç»œå¸¦å®½åŠ¨æ€åˆ†é…ä»»åŠ¡æ‰¹æ¬¡å¤§å°ï¼Œç¡®ä¿æ‰€æœ‰ Actor åœ¨ä¸€æ­¥ç­–ç•¥æ»åå†…å®Œæˆã€‚
   - **åŸºäºç§Ÿçº¦çš„å®¹é”™**ï¼šé€šè¿‡æ—¶é—´é™å®šçš„ç§Ÿçº¦ç®¡ç†ä»»åŠ¡ï¼Œè‡ªåŠ¨æ£€æµ‹å¤±è´¥èŠ‚ç‚¹å¹¶é‡æ–°åˆ†é…ä»»åŠ¡ï¼Œæ— éœ€å…¨å±€åŒæ­¥å±éšœã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **æ€§èƒ½** | åœ¨ WAN ä¸Šå®ç°é«˜è¾¾ **2.4â€“9.5Ã— çš„ååé‡æå‡**ï¼Œæ¥è¿‘ç†æƒ³ RDMA å•æ•°æ®ä¸­å¿ƒæ€§èƒ½ï¼ˆå·®è·ç¼©å°è‡³ **8.91% å†…**ï¼‰ã€‚ |
| **æˆæœ¬** | åˆ©ç”¨æŒ‰éœ€ã€è·¨äº‘çš„ commodity GPUï¼Œå®ç° **1.21â€“1.59Ã— æ›´é«˜çš„ tokens/$**ï¼Œæ˜¾è‘—ä¼˜äºé¢„ç•™ RDMA é›†ç¾¤ã€‚ |
| **å¯è®¿é—®æ€§** | ä½¿ä¸å…·å¤‡ä¸“ç”¨ HPC è®¾æ–½çš„ç ”ç©¶è€…ä¹Ÿèƒ½é«˜æ•ˆè¿›è¡Œå¤§è§„æ¨¡ RL è®­ç»ƒã€‚ |
| **å‡†ç¡®æ€§** | ä¿æŒ **bit-exact æ›´æ–°**ï¼Œæ— ä¿¡æ¯æŸå¤±ï¼Œä¸å½±å“æ¨¡å‹è´¨é‡ã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- **Hendrycks MATH**ï¼šæ•°å­¦æ¨ç†åŸºå‡†ã€‚
- **GSM8K**ï¼šå°å­¦æ•°å­¦åº”ç”¨é¢˜æ•°æ®é›†ã€‚
- **DeepScaleR**ï¼šç”¨äºè¯„ä¼° LLM æ¨ç†èƒ½åŠ›çš„åˆæˆæ•°æ®é›†ã€‚

### **å®éªŒè®¾ç½®**

- **æ¨¡å‹**ï¼šQwen3 ç³»åˆ—æ¨¡å‹ï¼ˆ4B, 8B, 14B å‚æ•°è§„æ¨¡ï¼‰ã€‚
- **è®­ç»ƒå™¨ï¼ˆTrainerï¼‰**ï¼šåœ¨ç¾å›½éƒ¨ç½²ï¼Œä½¿ç”¨ H100 GPUï¼ˆSXM5ï¼‰ï¼Œé€šè¿‡ FSDP2 è¿›è¡Œåˆ†å¸ƒå¼ä¼˜åŒ–ã€‚
- **Rollout Actors**ï¼šåˆ†å¸ƒåœ¨åŠ æ‹¿å¤§ã€æ—¥æœ¬ã€è·å…°ã€å†°å²›ç­‰åœ°ï¼Œä½¿ç”¨ A100 æˆ– L40 GPUï¼Œé€šè¿‡ vLLM è¿›è¡Œæ¨ç†ã€‚
- **ç½‘ç»œç¯å¢ƒ**ï¼š
  - å®é™…è·¨äº‘ WAN é“¾æ¥ï¼Œå¸¦å®½æ³¢åŠ¨äº **500 Mbps è‡³ 1 Gbps**ã€‚
  - ä½¿ç”¨ `tc` å·¥å…·æ¨¡æ‹Ÿä» **250 Mbps åˆ° 10 Gbps** çš„ä¸åŒå¸¦å®½æ¡ä»¶ã€‚
  - åœ°ç†åˆ†å¸ƒæ‰©å±•è‡³æœ€å¤š **4 ä¸ªæ•°æ®ä¸­å¿ƒï¼ˆDCï¼‰**ã€‚

### **è¯„ä¼°æŒ‡æ ‡**

| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **Throughput** | æ•´ä¸ªç³»ç»Ÿçš„å¹³å‡æ¯ç§’å¤„ç† token æ•°ï¼ˆtokens/sï¼‰ã€‚ |
| **Training Step Time** | å•ä¸ªä¼˜åŒ–æ­¥éª¤çš„å¢™é’Ÿæ—¶é—´ï¼ˆwall-clock durationï¼‰ï¼ŒåŒ…æ‹¬åŒæ­¥ã€ç”Ÿæˆå’Œè®¡ç®—æ¢¯åº¦ã€‚ |
| **Tokens per Dollar (tokens/$)** | æˆæœ¬æ•ˆç‡æŒ‡æ ‡ï¼Œè¡¡é‡å•ä½ç¾å…ƒæ‰€èƒ½å¤„ç†çš„ token æ•°é‡ã€‚ |
| **Gap to Ideal Performance** | ä¸ç†æƒ³å•æ•°æ®ä¸­å¿ƒ RDMA åŸºçº¿çš„ååé‡å·®è·ç™¾åˆ†æ¯”ã€‚ |

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

| åŸºçº¿æ–¹æ³• | æè¿° |
|---------|------|
| **Ideal-SingleDC** | ç†æƒ³åŒ–çš„å•æ•°æ®ä¸­å¿ƒ RDMA åŸºçº¿ï¼ˆ800 Gbps + NVLinkï¼‰ï¼Œä½œä¸ºæ€§èƒ½ä¸Šé™ã€‚ |
| **PrimeRL-Full** | åœ¨åœ°ç†åˆ†å¸ƒå¼ç¯å¢ƒä¸‹å¹¿æ’­å®Œæ•´æ¨¡å‹æƒé‡ã€‚ |
| **PrimeRL-MultiStream** | å¹¿æ’­å®Œæ•´æƒé‡ï¼Œä½†ä½¿ç”¨å¤šæµä¼ è¾“æå‡å¸¦å®½åˆ©ç”¨ç‡ã€‚ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

| æ¨¡å‹ | æ–¹æ³• | ååé‡ (tokens/s) | æ­¥éª¤æ—¶é—´ (s) | Tokens/$ (Ã—10â¶) |
|------|------|------------------|--------------|------------------|
| Qwen3-8B | SparrowRL | ~15.9k | ~12.6 | 3.60 |
| Qwen3-8B | Ideal-SingleDC | ~16.5k | ~11.8 | 2.99 |
| Qwen3-14B | SparrowRL | ~14.0k | ~17.1 | 2.12 |
| Qwen3-14B | Ideal-SingleDC | ~14.8k | ~16.2 | 1.33 |

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

- **ååé‡æå‡**ï¼š
  - ç›¸æ¯” **PrimeRL-Full**ï¼ŒSparrowRL åœ¨ Qwen3-4B ä¸Šæå‡ **2.4â€“3.7Ã—**ï¼Œåœ¨ Qwen3-14B ä¸Šæå‡ **7.7â€“9.5Ã—**ã€‚
  - ç›¸æ¯” **PrimeRL-MultiStream**ï¼Œä»é«˜å‡º **1.5â€“6.0Ã—**ã€‚
- **æ­¥éª¤æ—¶é—´å¤§å¹…ç¼©çŸ­**ï¼š
  - PrimeRL-Full åœ¨ Qwen3-14B ä¸Šæ­¥éª¤æ—¶é—´è¶…è¿‡ **500 ç§’**ï¼Œè€Œ SparrowRL ä»…ä¸º **~17 ç§’**ã€‚
- **æ¥è¿‘ç†æƒ³æ€§èƒ½**ï¼š
  - SparrowRL çš„ååé‡ä¸ Ideal-SingleDC çš„å·®è·ä»…ä¸º **1.31%â€“8.91%**ï¼Œè€Œ PrimeRL-Full çš„å·®è·é«˜è¾¾ **59.0%â€“90.3%**ã€‚
- **æˆæœ¬æ•ˆç‡æ˜¾è‘—æé«˜**ï¼š
  - SparrowRL å®ç° **1.21Ã— (8B)** å’Œ **1.59Ã— (14B)** æ›´é«˜çš„ tokens/$ï¼Œå³ä½¿ä¸ç†æƒ³ RDMA é›†ç¾¤ç›¸æ¯”ä¹Ÿæ›´å…·ç»æµæ€§ã€‚

### **æ¶ˆèå®éªŒç»“æœ**

| ä¼˜åŒ–é¡¹ | æ•ˆæœ |
|--------|------|
| **ç¨€ç–ç¼–ç ï¼ˆSparse Encodingï¼‰** | å°† Qwen3-8B çš„æ¯æ­¥ä¼ è¾“è´Ÿè½½ä» **15.6 GB** å‡å°‘åˆ° **202 MB**ï¼ˆ**79Ã— å‹ç¼©**ï¼‰ã€‚ |
| **uint8 delta ç¼–ç ** | ç›¸æ¯” int32/64 ç´¢å¼•ç¼–ç ï¼Œè¿›ä¸€æ­¥å°† payload ä» **414 MB â†’ 202 MB**ï¼Œä¼ è¾“æ—¶é—´ä» **9.22s â†’ 4.71s**ã€‚ |
| **Multi-Stream ä¼ è¾“** | åœ¨å•æµåŸºç¡€ä¸Šè¿›ä¸€æ­¥å°†ä¼ è¾“æ—¶é—´ä» **4.71s â†’ 2.90s**ï¼Œæå‡å¸¦å®½åˆ©ç”¨ç‡ã€‚ |
| **Relay-based Distribution** | åœ¨è·¨åŒºåŸŸéƒ¨ç½²ä¸­ï¼Œååé‡æå‡ **4.4%â€“13.9%**ï¼Œå°¤å…¶åœ¨é«˜å»¶è¿Ÿåœºæ™¯ä¸‹æ›´æ˜æ˜¾ã€‚ |
| **Heterogeneity-Aware Scheduling** | åœ¨æ··åˆ A100/L40 çš„å¼‚æ„ç¯å¢ƒä¸­ï¼Œç›¸æ¯”å‡åŒ€åˆ†é…ï¼Œç«¯åˆ°ç«¯ååé‡æå‡ **26.4%â€“35.5%**ã€‚ |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **RL å‚æ•°æ›´æ–°å…·æœ‰é«˜åº¦ç¨€ç–æ€§**ï¼šæ¯æ­¥ä»…æœ‰çº¦ **1% çš„å‚æ•°å…ƒç´ å‘ç”Ÿå˜åŒ–**ï¼Œè¿™ä¸€ç‰¹æ€§åœ¨å¤šç§æ¨¡å‹ï¼ˆQwen, Llama, GLMï¼‰å’Œç®—æ³•ï¼ˆGRPO, RLOO, OPOï¼‰ä¸­æ™®éå­˜åœ¨ä¸”ç¨³å®šã€‚
2. **ç¨€ç–å¢é‡ä¼ è¾“å¯çªç ´å¸¦å®½ç“¶é¢ˆ**ï¼šé€šè¿‡ä¼ è¾“ sparse delta è€Œé full weightsï¼Œå¯åœ¨ **1â€“10 Gbps çš„ commodity WAN** ä¸Šå®ç°æ¥è¿‘ RDMA é›†ç¾¤çš„è®­ç»ƒæ•ˆç‡ã€‚
3. **ç³»ç»Ÿçº§ååŒè®¾è®¡è‡³å…³é‡è¦**ï¼šç»“åˆ **sparse checkpointing + streaming transfer + heterogeneity-aware scheduling + lease-based fault tolerance** å¯æœ‰æ•ˆåº”å¯¹æ¾æ•£è€¦åˆç¯å¢ƒä¸‹çš„æŒ‘æˆ˜ã€‚
4. **æˆæœ¬æ•ˆç›Šæ˜¾è‘—**ï¼šåˆ©ç”¨è·¨äº‘æŒ‰éœ€ GPUï¼ŒSparrowRL åœ¨ç›¸ä¼¼ååé‡ä¸‹æä¾› **1.21â€“1.59Ã— æ›´é«˜çš„ tokens/$**ï¼Œæå¤§é™ä½äº† RL è®­ç»ƒé—¨æ§›ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**

- **ä¾èµ– Trainer çš„é«˜ç®—åŠ›**ï¼šTrainer éœ€è¦åœ¨é«˜æ€§èƒ½é›†ç¾¤ä¸Šè¿è¡Œä»¥å¿«é€Ÿå®Œæˆä¼˜åŒ–å’Œç¨€ç–å¢é‡æå–ã€‚
- **CPU å¼€é”€**ï¼šç¨€ç–å¢é‡çš„æå–å’Œç¼–ç å¸¦æ¥é¢å¤–çš„ CPU å¼€é”€ï¼Œè™½è¢«é‡å éšè—ä½†ä»å­˜åœ¨ã€‚
- **ä¸é€‚ç”¨äºå…¨é‡æ¢¯åº¦èšåˆåœºæ™¯**ï¼šæœ¬æ–‡èšç„¦äº **policy synchronization from Trainer to Actors**ï¼Œæœªè§£å†³è·¨åŒºåŸŸæ¢¯åº¦èšåˆé—®é¢˜ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**

- å°† SparrowRL ä¸åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯ï¼ˆå¦‚ ZeROã€pipeline parallelismï¼‰ç»“åˆï¼Œæ”¯æŒæ›´å¤§è§„æ¨¡çš„è·¨åŒºåŸŸè”åˆè®­ç»ƒã€‚
- æ¢ç´¢åœ¨ **decentralized RL** æ¶æ„ä¸­è¿›ä¸€æ­¥ä¼˜åŒ–é€šä¿¡æ¨¡å¼ã€‚
- æ”¯æŒæ›´å¤šç±»å‹çš„ç¨€ç–ç»“æ„ï¼ˆå¦‚ structured sparsityï¼‰ä»¥è¿›ä¸€æ­¥å‹ç¼©ã€‚
- æ‰©å±•è‡³å…¶ä»–éœ€è¦é«˜é¢‘å‚æ•°åŒæ­¥çš„è®­ç»ƒèŒƒå¼ï¼ˆå¦‚ continual learningï¼‰ã€‚

--- 

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> SparrowRL é€šè¿‡æ´å¯Ÿå¹¶åˆ©ç”¨ RL å¾®è°ƒä¸­çš„**ç»†ç²’åº¦å‚æ•°ç¨€ç–æ€§**ï¼Œå®ç°äº†åœ¨ä½æˆæœ¬ã€ä½å¸¦å®½çš„å•†å“åŒ–ç½‘ç»œä¸Šè¿›è¡Œé«˜æ•ˆã€å‡†ç¡®çš„å¤§è§„æ¨¡ RL è®­ç»ƒï¼Œ**æ‰“ç ´äº†å¯¹æ˜‚è´µ RDMA é›†ç¾¤çš„ä¾èµ–**ï¼Œä¸ºæ›´å¹¿æ³›çš„ç¤¾åŒºæ‰“å¼€äº†å‰æ²¿æ¨¡å‹ç ”ç©¶çš„å¤§é—¨ã€‚

</details>

---

### 5. [TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents](https://arxiv.org/abs/2602.11767)

**Authors**: Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Holger Boche  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.11767v1  

#### Abstract
Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this r...

---

### 6. [Learn from Your Mistakes: Self-Correcting Masked Diffusion Models](https://arxiv.org/abs/2602.11590)

**Authors**: Yair Schiff, Omer Belhasin, Roy Uziel, Guanghan Wang, Marianne Arriola, Gilad Turok, Michael Elad, Volodymyr Kuleshov  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2602.11590v1  

#### Abstract
Masked diffusion models (MDMs) have emerged as a promising alternative to autoregressive models, enabling parallel token generation while achieving competitive performance. Despite these advantages, MDMs face a fundamental limitation: once tokens are unmasked, they remain fixed, leading to error acc...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Learn from Your Mistakes: Self-Correcting Masked Diffusion Models*

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
**Masked Diffusion Models (MDMs)** è™½ç„¶åœ¨ç¦»æ•£æ•°æ®ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶æ”¯æŒå¹¶è¡Œ token ç”Ÿæˆï¼Œä½†å­˜åœ¨ä¸€ä¸ªæ ¹æœ¬æ€§ç¼ºé™·ï¼šä¸€æ—¦æŸä¸ª token è¢«â€œunmaskâ€ï¼ˆè§£ç ï¼‰ï¼Œå®ƒåœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­å°±å›ºå®šä¸å˜ã€‚è¿™å¯¼è‡´æ—©æœŸè§£ç é”™è¯¯æ— æ³•è¢«ä¿®æ­£ï¼Œéšç€ç”Ÿæˆè¿›è¡Œï¼Œé”™è¯¯ä¸æ–­ç´¯ç§¯ï¼Œæœ€ç»ˆé€ æˆæ ·æœ¬è´¨é‡ä¸‹é™ï¼ˆdistributional driftï¼‰ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šProgressive Self-Correction (ProSeCo)
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º **Progressive Self-Correction (ProSeCo)** çš„æ–°æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- **è®­ç»ƒä¸€ä¸ªæ—¢èƒ½è§£ç åˆèƒ½çº é”™çš„ç»Ÿä¸€æ¨¡å‹**ï¼šé€šè¿‡å°† MDM è§£ç å™¨è‡ªèº«çš„è¾“å‡ºè§†ä¸ºä¸€ç§â€œå™ªå£°â€ç‰ˆæœ¬çš„æ•°æ®ï¼Œè®­ç»ƒåŒä¸€ä¸ªæ¨¡å‹å»â€œçº æ­£â€è¿™äº›ç”±è‡ªèº«äº§ç”Ÿçš„æ½œåœ¨é”™è¯¯ã€‚
- **å¼•å…¥è‡ªçº æ­£æŸå¤± (Self-Correcting Loss)**ï¼šåœ¨æ ‡å‡† MDM çš„å˜åˆ†ç›®æ ‡å‡½æ•°åŸºç¡€ä¸Šï¼Œå¢åŠ äº†ä¸€ä¸ªç®€å•çš„äº¤å‰ç†µï¼ˆcross-entropyï¼‰æŸå¤±é¡¹ã€‚è¯¥æŸå¤±é¡¹çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹å­¦ä¼šä»å…¶è‡ªèº«è§£ç å™¨ï¼ˆ`xÎ¸`ï¼‰çš„è¾“å‡ºä¸­æ¢å¤å‡ºçœŸå®çš„å¹²å‡€æ•°æ®ã€‚
- **æ¨ç†æ—¶äº¤é”™æ‰§è¡Œè§£ç ä¸çº é”™æ­¥éª¤**ï¼šåœ¨é‡‡æ ·é˜¶æ®µï¼Œé™¤äº†æ ‡å‡†çš„ unmasking æ­¥éª¤å¤–ï¼Œè¿˜å‘¨æœŸæ€§åœ°æ’å…¥â€œcorrector loopâ€ã€‚åœ¨è¿™äº›å¾ªç¯ä¸­ï¼Œæ¨¡å‹ä»¥å½“å‰å·²ç”Ÿæˆçš„å®Œæ•´åºåˆ—ä¸ºè¾“å…¥ï¼Œå°è¯•æ›´æ–°å…¶ä¸­å¯èƒ½å­˜åœ¨çš„é”™è¯¯ tokenã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **ç®€å•é«˜æ•ˆ**ï¼šProSeCo å¯¹ç°æœ‰çš„ MDM è®­ç»ƒå’Œé‡‡æ ·ç®—æ³•ä»…éœ€æå°çš„ä¿®æ”¹ï¼ˆå¦‚å¢åŠ ä¸€ä¸ªæŸå¤±é¡¹å’Œä¸€ä¸ªé‡‡æ ·å¾ªç¯ï¼‰ï¼Œæ˜“äºå®ç°ã€‚
- **è´¨é‡-æ•ˆç‡æƒè¡¡æ›´ä¼˜**ï¼šèƒ½å¤Ÿä»¥æ›´å¿«çš„é€Ÿåº¦ï¼ˆå‡å°‘ 2-3 å€çš„ NFEsï¼‰è¾¾åˆ°ä¸æ ‡å‡† MDM ç›¸åŒç”šè‡³æ›´é«˜çš„æ ·æœ¬è´¨é‡ã€‚
- **æ”¯æŒæ¨ç†æ—¶è®¡ç®—æ‰©å±• (Inference-time Compute Scaling)**ï¼šé€šè¿‡å¢åŠ çº é”™å¾ªç¯çš„é¢‘ç‡å’Œæ­¥æ•°ï¼Œå¯ä»¥åœ¨æµ‹è¯•æ—¶æŠ•å…¥æ›´å¤šè®¡ç®—èµ„æºæ¥è¿›ä¸€æ­¥æå‡æ ·æœ¬è´¨é‡ï¼Œè¿™æ˜¯æ ‡å‡† MDM æ‰€ä¸å…·å¤‡çš„èƒ½åŠ›ã€‚
- **é€šç”¨æ€§å¼º**ï¼šé€‚ç”¨äºæ¡ä»¶å’Œéæ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œåœ¨ä»£ç ã€æ•°å­¦ã€åˆ†å­è®¾è®¡å’Œæ— æ¡ä»¶æ–‡æœ¬ç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸå‡è¡¨ç°å‡ºè‰²ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **ä»£ç ä¸æ•°å­¦ä»»åŠ¡**ï¼šå¯¹ LLaDA-Base 8B æ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½¿ç”¨äº† `rStar-Coder` å’Œ `OpenMathInstruct-2` æ•°æ®é›†ã€‚
- **ä¸‹æ¸¸è¯„æµ‹åŸºå‡†**ï¼š
  - **ä»£ç **ï¼š`HumanEval`, `MBPP`
  - **æ•°å­¦**ï¼š`GSM8K`, `Minerva`
- **å¼•å¯¼åˆ†å­è®¾è®¡ (Guided Molecule Design)**ï¼šåœ¨ `QM9` æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œä½¿ç”¨ SMILES å­—ç¬¦ä¸²è¡¨ç¤ºåˆ†å­ã€‚
- **æ— æ¡ä»¶æ–‡æœ¬ç”Ÿæˆ (Unconditional Text Generation)**ï¼šåœ¨ `OpenWebText (OWT)` æ•°æ®é›†ä¸Šä»å¤´è®­ç»ƒæ¨¡å‹ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **ä»£ç /æ•°å­¦ä»»åŠ¡**ï¼šPass@1 å‡†ç¡®ç‡ã€‚
  - **åˆ†å­è®¾è®¡**ï¼šç”Ÿæˆçš„æœ‰æ•ˆï¼ˆvalidï¼‰ã€å”¯ä¸€ï¼ˆuniqueï¼‰ã€æ–°é¢–ï¼ˆnovelï¼‰åˆ†å­æ•°é‡ï¼Œä»¥åŠæ–°é¢–åˆ†å­çš„å¹³å‡å±æ€§å€¼ï¼ˆå¦‚ç¯æ•° ring count æˆ–è¯ç‰©ç›¸ä¼¼æ€§ QEDï¼‰ã€‚
  - **æ— æ¡ä»¶ç”Ÿæˆ**ï¼šMAUVE åˆ†æ•°ï¼ˆè¡¡é‡ä¸äººç±»æ–‡æœ¬çš„åˆ†å¸ƒå·®è·ï¼‰ã€ç”Ÿæˆå›°æƒ‘åº¦ï¼ˆGen. PPLï¼Œç”¨ GPT2-Large è¯„ä¼°ï¼‰å’Œåºåˆ—ç†µï¼ˆè¡¡é‡å¤šæ ·æ€§ï¼‰ã€‚
- **å…³é”®å˜é‡**ï¼šé€šè¿‡è°ƒæ•´ `unmasking steps (T)`ã€`corrector frequency (w)` å’Œ `corrector steps per loop (S)` æ¥æ§åˆ¶è®¡ç®—é¢„ç®—å’Œæ¢ç´¢è´¨é‡-æ•ˆç‡æƒè¡¡ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Off-the-shelf æ¨¡å‹**ï¼š`Llama3.1-Instruct`, `LLaDA1.5`, `LLaDA-Instruct`ã€‚
- **MDM åŸºçº¿**ï¼š`LLaDA-Base` + Vanilla SFTã€‚
- **å¸¦å…¶ä»–çº é”™æœºåˆ¶çš„ MDM**ï¼š`+ReMDM`, `+PRISM`ã€‚
- **å…¶ä»–ç”ŸæˆèŒƒå¼**ï¼šæ ‡å‡† `Autoregressive (AR)` æ¨¡å‹ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ
1. **å…¨é¢è¶…è¶ŠåŸºçº¿**ï¼š
   - åœ¨æ‰€æœ‰å››ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆHumanEval, MBPP, GSM8K, Minervaï¼‰ä¸Šï¼Œ**ProSeCo SFT** æ¨¡å‹å‡æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºäº MDM çš„åŸºçº¿æ¨¡å‹ã€‚
   - å³ä½¿ä¸ä½¿ç”¨ ProSeCo çš„é‡‡æ ·ç®—æ³•ï¼Œä»…ç”¨ ProSeCo ç›®æ ‡å‡½æ•°è®­ç»ƒçš„æ¨¡å‹ä¹Ÿå·²ä¼˜äºæ ‡å‡† MDM è®­ç»ƒçš„æ¨¡å‹ã€‚

2. **å“è¶Šçš„è´¨é‡-æ•ˆç‡æƒè¡¡**ï¼š
   - **é€Ÿåº¦æå‡**ï¼šé€šè¿‡å¢åŠ å¹¶è¡Œè§£ç ï¼ˆå‡å°‘ unmasking stepsï¼‰å¹¶è¾…ä»¥çº é”™ï¼ŒProSeCo å¯ä»¥å®ç° **~2-3 å€çš„åŠ é€Ÿ**ï¼ˆNFEs å‡å°‘ï¼‰ï¼ŒåŒæ—¶ä¿æŒç”šè‡³è¶…è¿‡æ ‡å‡† MDM çš„å‡†ç¡®ç‡ã€‚
   - **è´¨é‡æå‡**ï¼šé€šè¿‡å¢åŠ æ¨ç†æ—¶çš„è®¡ç®—é‡ï¼ˆæ›´å¤šçš„çº é”™æ­¥éª¤ï¼‰ï¼ŒProSeCo å¯ä»¥åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°æœ€é«˜è¾¾ **~1.3 å€çš„å‡†ç¡®ç‡æå‡**ã€‚
   - ä¾‹å¦‚ï¼Œåœ¨ HumanEval ä¸Šï¼ŒProSeCo (Sampling) è¾¾åˆ°äº† **62.20%** çš„ Pass@1ï¼Œè¿œè¶…åŸºçº¿ SFT çš„ 48.17% å’Œ Llama3.1-Instruct çš„ 58.54%ã€‚

3. **å¼•å¯¼ç”Ÿæˆä¸­çš„å¸•ç´¯æ‰˜å‰æ²¿ä¼˜åŒ–**ï¼š
   - åœ¨åˆ†å­è®¾è®¡ä»»åŠ¡ä¸­ï¼ŒProSeCo æˆåŠŸåœ°åœ¨â€œæ–°é¢–æ€§â€å’Œâ€œå±æ€§æœ€å¤§åŒ–â€ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œæ˜¾è‘—æ‰©å±•äº†å¸•ç´¯æ‰˜å‰æ²¿ï¼Œå°¤å…¶æ˜¯åœ¨æœ€å¤§åŒ–â€œç¯æ•°â€æ—¶æ•ˆæœå°¤ä¸ºçªå‡ºã€‚

4. **æ— æ¡ä»¶ç”Ÿæˆçš„ä¼˜è¶Šæ€§**ï¼š
   - åœ¨ OWT ä¸Šçš„æ— æ¡ä»¶ç”Ÿæˆä¸­ï¼ŒProSeCo åœ¨ MAUVE å’Œç”Ÿæˆå›°æƒ‘åº¦ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äº MDLMã€ReMDM å’Œ PRISM ç­‰åŸºçº¿ã€‚
   - ä¾‹å¦‚ï¼Œåœ¨ T=256 æ­¥æ—¶ï¼ŒProSeCo çš„ MAUVE è¾¾åˆ° **0.419**ï¼Œè€Œ PRISM ä¸º 0.294ï¼ŒReMDM ä¸º 0.216ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **çº é”™é¢„ç®—é…ç½®**ï¼šå®éªŒè¡¨æ˜ï¼Œå¯¹äºå¿«é€Ÿé‡‡æ ·æ¨¡å¼ï¼ˆé«˜å¹¶è¡Œåº¦ï¼‰ï¼Œéœ€è¦æ›´é¢‘ç¹çš„çº é”™ï¼ˆ`w` è¾ƒå°ï¼‰æ¥æŠµæ¶ˆè´¨é‡ä¸‹é™ï¼›è€Œå¯¹äºä½å¹¶è¡Œåº¦ï¼Œå¢åŠ  `S` å’Œé™ä½ `w`ï¼ˆå³å¢åŠ çº é”™å¼ºåº¦ `S/w`ï¼‰å‡ ä¹æ€»èƒ½å¸¦æ¥æ€§èƒ½æå‡ã€‚
- **çº é”™å¼ºåº¦ `p=S/w` æ˜¯å…³é”®**ï¼š
  - **Fast æ¨¡å¼ (`p < 1`)**ï¼šç¨€ç–çº é”™ï¼Œä¼˜å…ˆé€Ÿåº¦ï¼Œå¯å®ç° 2-3 å€åŠ é€Ÿä¸”ç²¾åº¦ä¸é™ã€‚
  - **Balanced æ¨¡å¼ (`p â‰ˆ 1`)**ï¼šå¹³è¡¡è´¨é‡å’Œæ•ˆç‡ã€‚
  - **Max æ¨¡å¼ (`p > 1`)**ï¼šå¯†é›†çº é”™ï¼Œè¿½æ±‚æœ€é«˜è´¨é‡ï¼Œé€šè¿‡æ¨ç†æ—¶è®¡ç®—æ‰©å±•å®ç°æ€§èƒ½ä¸Šé™çªç ´ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
- **é”™è¯¯çº æ­£æ˜¯æå‡ MDM æ€§èƒ½çš„å…³é”®**ï¼šé€šè¿‡è®© MDM å­¦ä¼šâ€œè‡ªæˆ‘çº æ­£â€ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³å›  token å›ºå®šè€Œå¯¼è‡´çš„é”™è¯¯ç´¯ç§¯é—®é¢˜ã€‚
- **ProSeCo æ¶æ„éå¸¸æœ‰æ•ˆ**ï¼šæå‡ºçš„è®­ç»ƒå’Œé‡‡æ ·æ¡†æ¶ç®€å•ã€é€šç”¨ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸Šå–å¾—ä¸€è‡´ä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
- **å®ç°äº†æ–°çš„èƒ½åŠ›â€”â€”æ¨ç†æ—¶è®¡ç®—æ‰©å±•**ï¼šProSeCo é¦–æ¬¡å…è®¸åœ¨æ¨ç†æ—¶é€šè¿‡å¢åŠ çº é”™æ­¥éª¤æ¥åŠ¨æ€æå‡ç”Ÿæˆè´¨é‡ï¼Œä¸ºæ¨¡å‹éƒ¨ç½²æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **è®­ç»ƒæˆæœ¬å¢åŠ **ï¼šç”±äºåœ¨è®­ç»ƒæ—¶éœ€è¦é¢å¤–çš„å‰å‘ä¼ æ’­æ¥è®¡ç®—çº æ­£æŸå¤±ï¼Œå¢åŠ äº†çº¦ä¸€å€çš„è®­ç»ƒè®¡ç®—å¼€é”€ï¼Œç›¸æ¯”çº¯æ¨ç†æ—¶çº é”™çš„æ–¹æ³•ï¼ˆå¦‚ ReMDMï¼‰ä»£ä»·æ›´é«˜ã€‚
- **ä¾èµ–äºåŸºç¡€æ¶æ„**ï¼šè™½ç„¶è®ºæ–‡å¼ºè°ƒå…¶é€šç”¨æ€§ï¼Œä½†å…¶æœ‰æ•ˆæ€§å»ºç«‹åœ¨æ ‡å‡† Transformer æ¶æ„ä¹‹ä¸Šï¼Œå¯¹å…¶ä»–éª¨å¹²ç½‘ç»œçš„é€‚é…æ€§æœªå……åˆ†éªŒè¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **åˆ†ç¦»è§£ç å™¨ä¸çº æ­£å™¨**ï¼šæ¢ç´¢è§£è€¦ `denoiser` å’Œ `corrector` çš„æƒé‡ï¼Œæˆ–ä½¿ç”¨å®Œå…¨ç‹¬ç«‹çš„ç¥ç»ç½‘ç»œï¼Œä»¥ç ”ç©¶ä¸¤è€…åŠŸèƒ½çš„ä¸“ä¸šåŒ–æ˜¯å¦èƒ½å¸¦æ¥è¿›ä¸€æ­¥æ”¶ç›Šã€‚
- **æ›´å¤æ‚çš„çº é”™è°ƒåº¦ç­–ç•¥**ï¼šè®¾è®¡æ›´æ™ºèƒ½çš„æ–¹æ¡ˆæ¥è”åˆè°ƒåº¦è§£ç å’Œçº é”™æ­¥éª¤ï¼Œä¾‹å¦‚åŸºäºç½®ä¿¡åº¦åŠ¨æ€å†³å®šä½•æ—¶çº é”™ï¼Œè€Œéå›ºå®šçš„é¢‘ç‡ `w`ã€‚
- **æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€**ï¼šå°† ProSeCo æ¡†æ¶åº”ç”¨äºå›¾åƒã€éŸ³é¢‘ç­‰å…¶ä»–é¢†åŸŸçš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€‚

</details>

---

### 7. [Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization](https://arxiv.org/abs/2602.11351)

**Authors**: Yihang Yao, Zhepeng Cen, Haohong Lin, Shiqi Liu, Zuxin Liu, Jiacheng Zhu, Zhang-Wei Hong, Laixi Shi, Ding Zhao  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.11351v1  

#### Abstract
Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently eme...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šPushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹**Proactive LLM Agents**åœ¨å¤šè½®äº¤äº’ä»»åŠ¡ä¸­é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜â€”â€”**ä»»åŠ¡æ€§èƒ½ä¸ç”¨æˆ·å‚ä¸åº¦ä¹‹é—´çš„æƒè¡¡ï¼ˆtrade-offï¼‰**ã€‚å…·ä½“è€Œè¨€ï¼š
- ä¸ºäº†æå‡æœ€ç»ˆä»»åŠ¡å®Œæˆè´¨é‡ï¼Œä»£ç†ï¼ˆagentï¼‰å€¾å‘äºé¢‘ç¹å‘ç”¨æˆ·è¯·æ±‚åé¦ˆï¼ˆå¦‚éªŒè¯ç­”æ¡ˆï¼‰ï¼Œè¿™è™½ç„¶èƒ½æé«˜å‡†ç¡®æ€§ï¼Œä½†ä¼šæ˜¾è‘—å¢åŠ ç”¨æˆ·çš„è´Ÿæ‹…ï¼Œé™ä½æ»¡æ„åº¦ã€‚
- åä¹‹ï¼Œè‹¥é™åˆ¶ç”¨æˆ·äº¤äº’æ¬¡æ•°ï¼Œåˆ™å¯èƒ½å¯¼è‡´ä¿¡æ¯è·å–ä¸è¶³ï¼Œå½±å“æ¨ç†èƒ½åŠ›ã€‚

è¿™ä¸€çŸ›ç›¾æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMulti-Objective Optimization, MOOï¼‰é—®é¢˜**ï¼Œå³å¦‚ä½•åœ¨æœ€å¤§åŒ–ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶æœ€å°åŒ–ç”¨æˆ·å‚ä¸æˆæœ¬ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šBAO (Behavioral Agentic Optimization)
ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä½œè€…æå‡ºäº† **BAO** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†**è¡Œä¸ºå»ºæ¨¡**ï¼ˆbehavior modelingï¼‰æ·±åº¦é›†æˆåˆ° Agentic RL ä¸­ï¼Œé€šè¿‡ä¸¤ä¸ªå…³é”®æœºåˆ¶å®ç°å¸•ç´¯æ‰˜å‰æ²¿ï¼ˆPareto Frontierï¼‰çš„å‰ç§»ï¼š

1. **Behavior Enhancement (è¡Œä¸ºå¢å¼º)**  
   åœ¨ SFT é˜¶æ®µï¼Œé€šè¿‡æç¤ºä¸€ä¸ªå¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚ GPT-4oï¼‰ï¼Œæ˜¾å¼åœ°ç”ŸæˆåŒ…å«ç‰¹å®š**å¤šè½®è¡Œä¸ºæ¨¡å¼**çš„æ•°æ®ï¼Œç”¨äºå¾®è°ƒå­¦ç”Ÿæ¨¡å‹ã€‚è¿™äº›è¡Œä¸ºåŒ…æ‹¬ï¼š
   - **Retrospective Reasoning (å›æº¯æ€§æ¨ç†)**ï¼šç»´æŠ¤è®°å¿†ã€ç®¡ç†å‡è®¾ã€åœ¨æ”¶åˆ°å¦å®šåé¦ˆæ—¶ä¿®æ­£é”™è¯¯å‡è®¾ï¼ˆHypothesis Refinementï¼‰ã€‚
   - **Prospective Planning (å‰ç»æ€§è§„åˆ’)**ï¼šåŠ¨æ€è°ƒåº¦ï¼ˆDynamic Schedulingï¼‰äº¤äº’é¢„ç®—ã€æˆ˜ç•¥æ€§æé—®ï¼ˆStrategical Queryingï¼‰ä»¥é«˜æ•ˆå‡å°‘ä¸ç¡®å®šæ€§ã€‚

2. **Behavior-Regularized RL (è¡Œä¸ºæ­£åˆ™åŒ–å¼ºåŒ–å­¦ä¹ )**  
   åœ¨ RL é˜¶æ®µï¼Œå¼•å…¥åŸºäºå›åˆï¼ˆturn-levelï¼‰çš„å¥–åŠ±å¡‘å½¢ï¼ˆreward shapingï¼‰æ¥æŠ‘åˆ¶ä½æ•ˆè¡Œä¸ºï¼š
   - **Information-Seeking Regularization**ï¼šæƒ©ç½šåœ¨æ²¡æœ‰è·å¾—ç¯å¢ƒä¿¡æ¯å¢ç›Šçš„æƒ…å†µä¸‹è¿ç»­å‘ç”¨æˆ·æé—®ã€‚
   - **Over-Thinking Regularization**ï¼šæƒ©ç½šå› è¿‡åº¦æ€è€ƒè€Œæå‰è€—å°½äº¤äº’é¢„ç®—çš„è½¨è¿¹ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **è¶…è¶Šè¢«åŠ¨ä¸è¿‡åº¦ä¸»åŠ¨**ï¼šBAO ä¸æ˜¯ç®€å•åœ°åœ¨â€œå®Œå…¨è¢«åŠ¨â€å’Œâ€œè¿‡åº¦ä¾èµ–ç”¨æˆ·â€ä¹‹é—´æŠ˜è¡·ï¼Œè€Œæ˜¯é€šè¿‡å¢å¼ºæ™ºèƒ½ä½“çš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æ›´è‡ªä¸»ã€æ›´é«˜æ•ˆåœ°å®Œæˆä»»åŠ¡ã€‚
- **æ¨åŠ¨å¸•ç´¯æ‰˜å‰æ²¿**ï¼šå®éªŒè¯æ˜ï¼ŒBAO èƒ½å¤Ÿæ‰¾åˆ°æ¯”ç°æœ‰æ–¹æ³•æ›´ä¼˜çš„æƒè¡¡è§£ï¼Œåœ¨åŒç­‰ç”¨æˆ·å‚ä¸ä¸‹å–å¾—æ›´é«˜æ€§èƒ½ï¼Œæˆ–åœ¨åŒç­‰æ€§èƒ½ä¸‹å¤§å¹…å‡å°‘ç”¨æˆ·å¹²é¢„ã€‚
- **ç¼“è§£ Reward Hacking**ï¼šç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼ŒBAO äº§ç”Ÿçš„ç­–ç•¥æ›´ç¨³å¥ï¼Œä¸æ˜“é€šè¿‡ç”Ÿæˆå†—é•¿ã€æ··ä¹±çš„å›ç­”æ¥æ¬ºéª—åŸºäº LLM çš„å¥–åŠ±æ¨¡å‹ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
å®éªŒåŸºäº **UserRL benchmark suite** ä¸­çš„ä¸‰ä¸ªå¤šè½® Proactive Agent ä»»åŠ¡ï¼š
- **Function-Gym**ï¼šæ¨æ–­ä¸€ä¸ªç”±å››ä¸ªå˜é‡æ„æˆçš„æœªçŸ¥æ•°å­¦å‡½æ•°ã€‚ä»£ç†å¯é€šè¿‡ `Action` æŸ¥è¯¢ä»»æ„è¾“å…¥çš„è¾“å‡ºï¼Œå¹¶é€šè¿‡ `Answer` æäº¤å¯¹æµ‹è¯•ç”¨ä¾‹çš„é¢„æµ‹ã€‚ç¯å¢ƒåé¦ˆå®Œå…¨åŸºäºè§„åˆ™ï¼Œæ— æ¨¡æ‹Ÿç”¨æˆ·ã€‚
- **Telepathy-Gym**ï¼šæ¨æ–­ä¸€ä¸ªéšè—çš„ç›®æ ‡å®ä½“ã€‚ä»£ç†å¯å‘ç¯å¢ƒï¼ˆLLM æ¨¡æ‹Ÿï¼‰æå‡ºâ€œæ˜¯/å¦â€é—®é¢˜ï¼ˆ`Action`ï¼‰ï¼Œå¹¶æäº¤å€™é€‰ç­”æ¡ˆï¼ˆ`Answer`ï¼‰ä»¥è·å–åé¦ˆã€‚å­˜åœ¨è®­ç»ƒ-è¯„ä¼°åˆ†å¸ƒåç§»ï¼ˆtraining-evaluation distribution shiftï¼‰ã€‚
- **Turtle-Gym**ï¼šè§£å¼€ä¸€ä¸ªâ€œä¹Œé¾Ÿæ±¤â€å¼çš„è°œé¢˜ï¼Œæ­ç¤ºæ•…äº‹èƒŒåçš„éšè—è½¬æŠ˜ã€‚ä»£ç†é€šè¿‡ `Action` æé—®ï¼Œé€šè¿‡ `Answer` æäº¤è§£é‡Šã€‚å¥–åŠ±ç”± LLM è¯„ä¼°å™¨æ ¹æ®è¦†ç›–çš„å…³é”®ç‚¹è®¡ç®—ï¼Œä¸”æ¯ä¸ªå…³é”®ç‚¹ä»…é¦–æ¬¡æ­£ç¡®æåŠæ‰å¾—åˆ†ã€‚

æ‰€æœ‰ä»»åŠ¡å‡è®¾å®šæœ€å¤§äº¤äº’è½®æ•°ï¼ˆinteraction budgetï¼‰ä¸º 15 è½®ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **åŸºç¡€æ¨¡å‹**ï¼šä½¿ç”¨ Qwen3 ç³»åˆ—æ¨¡å‹ï¼ˆ1.7B å’Œ 4B å‚æ•°é‡ï¼‰ä½œä¸ºåŸºç¡€ã€‚
- **è®­ç»ƒæµç¨‹**ï¼š
  1. **SFT (Supervised Fine-Tuning)**ï¼šä½¿ç”¨è¡Œä¸ºå¢å¼ºåçš„æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒã€‚
  2. **RL (Reinforcement Learning)**ï¼šé‡‡ç”¨ GRPO ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨ turn-level ä¼˜åŠ¿ä¼°è®¡ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Pass@U-k**ï¼šå…è®¸æœ€å¤š k æ¬¡ç”¨æˆ·å‚ä¸åŠ¨ä½œï¼ˆ`Answer`ï¼‰æ—¶çš„æˆåŠŸç‡ã€‚è¡¡é‡æ—©æœŸå‡†ç¡®æ€§å’Œä»é”™è¯¯ä¸­æ¢å¤çš„èƒ½åŠ›ã€‚
  - **Score (Unshaped Cumulative Reward)**ï¼šæœªç»è¿‡å¡‘å½¢çš„ç´¯ç§¯å¥–åŠ±ï¼Œåæ˜ æœ€ç»ˆä»»åŠ¡æ€§èƒ½ã€‚
  - **User Involvement Rate (UR)**ï¼šè½¨è¿¹ä¸­ç”¨æˆ·å‚ä¸åŠ¨ä½œï¼ˆ`Answer`ï¼‰çš„æ¯”ä¾‹ã€‚è¶Šä½è¶Šå¥½ï¼Œä»£è¡¨ç”¨æˆ·è´Ÿæ‹…è¶Šå°ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **å•†ä¸šé—­æºæ¨¡å‹**ï¼šGemini ç³»åˆ—ã€GPT ç³»åˆ—ï¼ˆå¦‚ GPT-4o, GPT-5ï¼‰ã€‚
- **å¼€æºåŸºçº¿æ¨¡å‹**ï¼šæœªç»å¾®è°ƒçš„ Qwen3 åŸå§‹æ¨¡å‹ï¼ˆRaw Modelsï¼‰ã€‚
- **RL åŸºçº¿æ–¹æ³•**ï¼š**UserRL** (Qian et al., 2025c)ï¼Œè¿™æ˜¯å½“å‰æœ€å…ˆè¿›çš„ Proactive Agent RL è®­ç»ƒæ¡†æ¶ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ
å®éªŒç»“æœï¼ˆè§åŸæ–‡ Table 1ï¼‰è¡¨æ˜ï¼Œ**BAO æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•**ï¼š

| æŒ‡æ ‡ | BAO vs. UserRL | BAO vs. å•†ä¸šæ¨¡å‹ |
| :--- | :--- | :--- |
| **Pass@U-1 / Pass@U-2** | **å…¨é¢é¢†å…ˆ**ã€‚ä¾‹å¦‚åœ¨ Function-Gym ä¸Šï¼ŒBAO-4B çš„ Pass@U-1 è¾¾åˆ° 0.2692ï¼Œè¿œè¶… UserRL-4B çš„ 0.2436ï¼›Pass@U-2 æ›´æ˜¯ä» 0.4103 æå‡è‡³ 0.5354ã€‚ | **ç›¸å½“ç”šè‡³æ›´å¥½**ã€‚åœ¨ Function-Gym ä¸Šï¼ŒBAO-4B çš„ Pass@U-2 (0.5354) è¶…è¿‡ GPT-4o (0.5854)ï¼Œæ¥è¿‘ GPT-5-0807 (0.6923)ã€‚ |
| **Score (ä»»åŠ¡æ€§èƒ½)** | **æ˜¾è‘—æ›´é«˜**ã€‚BAO-4B åœ¨ Function-Gym ä¸Šçš„åˆ†æ•°è¾¾åˆ° 0.6923ï¼Œè¿œé«˜äº UserRL-4B çš„ 0.5256ã€‚ | **å…·æœ‰ç«äº‰åŠ›**ã€‚BAO-4B åœ¨ Function-Gym ä¸Šçš„åˆ†æ•°è¶…è¿‡ GPT-4o (0.1410) å’Œ GPT-4o-mini (0.0385)ã€‚ |
| **User Involvement Rate (UR)** | **å¤§å¹…é™ä½**ã€‚BAO-4B åœ¨ Function-Gym ä¸Šçš„ UR ä»…ä¸º 0.2148ï¼Œè€Œ UserRL-4B é«˜è¾¾ 0.3758ã€‚ | **æ›´ä½**ã€‚BAO-4B çš„ UR (0.2148) è¿œä½äº GPT-4o (0.2248) å’Œ GPT-4o-mini (0.1767)ã€‚ |

**æ ¸å¿ƒç»“è®º**ï¼šBAO åœ¨**ä¸ç‰ºç‰²ç”šè‡³æå‡ä»»åŠ¡æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹ç”¨æˆ·çš„ä¾èµ–**ï¼Œå®ç°äº†æ›´ä¼˜çš„å¸•ç´¯æ‰˜æƒè¡¡ã€‚

### æ¶ˆèå®éªŒç»“æœ (Ablation Studies)
æ¶ˆèå®éªŒéªŒè¯äº† BAO å„ç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼š

1. **ç§»é™¤ Behavior Regularization (w/o BR)**ï¼š
   - å¯¼è‡´ **UR æ€¥å‰§ä¸Šå‡**ï¼ˆå¦‚ BAO-4B w/o BR çš„ UR ä» 0.2148 å‡è‡³ 0.3755ï¼‰ã€‚
   - è¡¨æ˜æ­£åˆ™åŒ–å¯¹äºæŠ‘åˆ¶ä»£ç†è¿‡åº¦ä¾èµ–ç”¨æˆ·éªŒè¯è‡³å…³é‡è¦ã€‚

2. **ç§»é™¤ Behavior Enhancement (w/o BE)**ï¼š
   - å¯¼è‡´ **Pass@U-k å’Œ Score ä¸‹é™**ï¼ˆå¦‚ BAO-4B w/o BE çš„ Pass@U-2 ä» 0.5354 é™è‡³ 0.3718ï¼‰ã€‚
   - è¡¨æ˜è¡Œä¸ºå¢å¼ºå¯¹äºæ³¨å…¥æœ‰æ•ˆçš„å¤šè½®æ¨ç†èƒ½åŠ›æ˜¯å¿…è¦çš„ã€‚

3. **Retrospective vs. Prospective Behaviors**ï¼š
   - **Retrospective Reasoning** å¯¹æå‡æœ€ç»ˆä¸Šé™æ€§èƒ½ï¼ˆScoreï¼‰æ›´é‡è¦ã€‚
   - **Prospective Planning** å¯¹æå‡æ—©æœŸæˆåŠŸç‡ï¼ˆPass@U-1ï¼‰æ›´æœ‰æ•ˆã€‚
   - ä¸¤è€…ç»“åˆæ‰èƒ½å®ç°**å¹³è¡¡ä¸”å…¨é¢çš„æ€§èƒ½æå‡**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **Proactive Agent è®­ç»ƒæœ¬è´¨æ˜¯ MOO é—®é¢˜**ï¼šå¿…é¡»åŒæ—¶ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½å’Œç”¨æˆ·å‚ä¸æˆæœ¬ï¼Œå•çº¯è°ƒæ•´å¥–åŠ±æƒé‡æ— æ³•æœ‰æ•ˆè§£å†³æ­¤é—®é¢˜ã€‚
2. **å¤šè½®è¡Œä¸ºæ˜¯å…³é”®**ï¼š**Retrospective Reasoning** å’Œ **Prospective Planning** æ˜¯è¿æ¥å†å²ä¿¡æ¯ä¸æœªæ¥å†³ç­–çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œæ˜¯å®ç°é«˜æ•ˆã€è‡ªä¸»æ¨ç†çš„åŸºç¡€ã€‚
3. **BAO æˆåŠŸæ¨åŠ¨äº†å¸•ç´¯æ‰˜å‰æ²¿**ï¼šé€šè¿‡ **Behavior Enhancement** æ³¨å…¥æ™ºèƒ½è¡Œä¸ºï¼Œå¹¶é€šè¿‡ **Behavior-Regularized RL** æŠ‘åˆ¶ä½æ•ˆæ¨¡å¼ï¼ŒBAO èƒ½å¤Ÿè®­ç»ƒå‡ºæ€§èƒ½æ›´å¼ºã€æ›´å°‘æ‰“æ‰°ç”¨æˆ·çš„ Proactive Agentsã€‚
4. **BAO å…·æœ‰é²æ£’æ€§**ï¼šåœ¨å­˜åœ¨ sim-to-real gap çš„ Turtle-Gym ä»»åŠ¡ä¸Šï¼ŒBAO æœ‰æ•ˆç¼“è§£äº† Reward Hacking é—®é¢˜ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–é«˜è´¨é‡æ•™å¸ˆæ¨¡å‹**ï¼šè¡Œä¸ºå¢å¼ºé˜¶æ®µéœ€è¦ GPT-4o ç­‰å¼ºå¤§æ¨¡å‹æ¥ç”Ÿæˆè¡Œä¸ºç¤ºèŒƒæ•°æ®ï¼Œå¢åŠ äº†è®­ç»ƒæˆæœ¬ã€‚
- **è¯­è¨€æ¨¡æ€é™åˆ¶**ï¼šå½“å‰å·¥ä½œä¸“æ³¨äºçº¯æ–‡æœ¬ä»£ç†ï¼Œæœªæ¶‰åŠè§†è§‰ç­‰å¤šæ¨¡æ€åœºæ™¯ã€‚
- **ä»»åŠ¡èŒƒå›´**ï¼šå®éªŒé›†ä¸­åœ¨ UserRL benchmark çš„ç‰¹å®šä»»åŠ¡ä¸Šï¼Œå…¶é€šç”¨æ€§æœ‰å¾…åœ¨æ›´å¹¿æ³›çš„åº”ç”¨ä¸­éªŒè¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å°† BAO æ¡†æ¶æ‰©å±•åˆ° **Multimodal Agent** çš„è®­ç»ƒä¸­ã€‚
- æ¢ç´¢æ›´é«˜æ•ˆçš„ã€æ— éœ€å¼ºæ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºå¼•å¯¼æ–¹æ³•ã€‚
- ç ”ç©¶å¦‚ä½•å°† BAO åº”ç”¨äºæ›´å¤æ‚çš„ç°å®ä¸–ç•Œåº”ç”¨åœºæ™¯ï¼Œå¦‚ä¸ªæ€§åŒ–åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å®¢æœç­‰ã€‚

</details>

---

### 8. [Predicting LLM Output Length via Entropy-Guided Representations](https://arxiv.org/abs/2602.11812)

**Authors**: Huanyi Xie, Yubin Chen, Liangyu Wang, Lijie Hu, Di Wang  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.11812v1  

#### Abstract
The long-tailed distribution of sequence lengths in LLM serving and reinforcement learning (RL) sampling causes significant computational waste due to excessive padding in batched inference. Existing methods rely on auxiliary models for static length prediction, but they incur high overhead, general...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šPredicting LLM Output Length via Entropy-Guided Representations

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æœåŠ¡å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é‡‡æ ·ä¸­ï¼Œè¾“å‡ºåºåˆ—é•¿åº¦å‘ˆç°**é•¿å°¾åˆ†å¸ƒ**ï¼Œå¯¼è‡´æ‰¹å¤„ç†ï¼ˆbatchingï¼‰æ—¶å› å¯¹é½è€Œäº§ç”Ÿå¤§é‡æ— ç”¨çš„å¡«å……ï¼ˆpaddingï¼‰ï¼Œé€ æˆæ˜¾è‘—çš„è®¡ç®—èµ„æºæµªè´¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ç‹¬ç«‹çš„è¾…åŠ©æ¨¡å‹è¿›è¡Œé™æ€é•¿åº¦é¢„æµ‹ï¼Œå­˜åœ¨ä»¥ä¸‹ä¸‰å¤§ç¼ºé™·ï¼š
- **é«˜å¼€é”€**ï¼šéœ€é¢å¤–è®­ç»ƒå’Œéƒ¨ç½²ä¸€ä¸ªæ¨¡å‹ï¼›
- **æ³›åŒ–èƒ½åŠ›å·®**ï¼šåœ¨é•¿åºåˆ—ã€å¤æ‚æ¨ç†ç­‰åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ï¼›
- **æ— æ³•åº”å¯¹éšæœºæ€§**ï¼šåœ¨â€œä¸€å¯¹å¤šâ€ï¼ˆone-to-manyï¼‰çš„éšæœºé‡‡æ ·åœºæ™¯ï¼ˆå¦‚RLï¼‰ä¸­å¤±æ•ˆã€‚

### æå‡ºçš„æ–°æ–¹æ³•
æœ¬æ–‡æå‡ºä¸€ç§è½»é‡çº§ã€é«˜æ•ˆçš„æ¡†æ¶ï¼Œç›´æ¥å¤ç”¨ä¸»æ¨¡å‹å†…éƒ¨çš„éšè—çŠ¶æ€ï¼ˆhidden statesï¼‰è¿›è¡Œé•¿åº¦é¢„æµ‹ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

#### (1) Entropy-Guided Token Pooling (EGTP)
- **æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨tokençš„**ç†µå€¼**ä½œä¸ºæ³¨æ„åŠ›æƒé‡ï¼Œå¯¹è¾“å…¥promptçš„éšè—çŠ¶æ€è¿›è¡ŒåŠ æƒæ± åŒ–ã€‚
- **åŠ¨æœº**ï¼šé«˜ç†µtokenä»£è¡¨æ¨¡å‹ä¸ç¡®å®šæ€§é«˜ï¼Œå¾€å¾€å¯¹åº”æ›´å…³é”®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¹é¢„æµ‹ç”Ÿæˆé•¿åº¦æ›´å…·æŒ‡ç¤ºæ€§ã€‚
- **ä¼˜åŠ¿**ï¼šæ— éœ€é¢å¤–æ¨¡å‹ï¼Œä»…å¢åŠ ä¸€ä¸ªè½»é‡å›å½’å¤´ï¼Œè®¡ç®—å¼€é”€æä½ã€‚

#### (2) Progressive Length Prediction (PLP)
- **æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨æ¯ä¸ªè§£ç æ­¥åŠ¨æ€æ›´æ–°å‰©ä½™é•¿åº¦é¢„æµ‹ã€‚
- **æœºåˆ¶**ï¼šç»“åˆå·²ç”Ÿæˆtokençš„éšè—çŠ¶æ€ä¸åŸå§‹promptè¡¨ç¤ºï¼Œé€æ­¥ç»†åŒ–é¢„æµ‹ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šä¸“ä¸ºRLç­‰**åŠ¨æ€ã€éšæœºé‡‡æ ·**ç¯å¢ƒè®¾è®¡ï¼Œå…‹æœé™æ€é¢„æµ‹çš„å±€é™ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | æœ¬æ–‡æ–¹æ³• |
|------|--------|---------|
| å¼€é”€ | é«˜ï¼ˆéœ€é¢å¤–æ¨¡å‹ï¼‰ | æä½ï¼ˆå¤ç”¨å·²æœ‰æ¿€æ´»ï¼‰ |
| æ³›åŒ–æ€§ | å·®ï¼ˆä¾èµ–ç®€å•æ•°æ®é›†ï¼‰ | å¼ºï¼ˆé€‚ç”¨äºé•¿åºåˆ—ã€CoTã€RLï¼‰ |
| åŠ¨æ€é€‚åº”æ€§ | æ—  | æ”¯æŒï¼ˆé€šè¿‡PLPï¼‰ |
| éƒ¨ç½²å¤æ‚åº¦ | é«˜ | ä½ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
æ„å»ºå¹¶å‘å¸ƒäº†é¦–ä¸ªç»¼åˆæ€§é•¿åº¦é¢„æµ‹åŸºå‡† **ForeLen**ï¼ŒåŒ…å«ä¸¤å¤§åœºæ™¯ï¼š

#### (1) é•¿åºåˆ—ä¸å¤æ‚æ¨ç†ç”Ÿæˆï¼ˆLong-Sequence & Reasoningï¼‰
- **æ¥æº**ï¼šLongBenchã€ZeroSCROLLSã€IFEval
- **ä»»åŠ¡ç±»å‹**ï¼šé•¿æ–‡æœ¬é—®ç­”ã€æ‘˜è¦ã€å¤æ‚æŒ‡ä»¤éµå¾ª
- **æ¨¡å‹**ï¼šQwen2.5ã€Llama3.2ã€DeepSeek-R1-Distill

#### (2) åŠ¨æ€RLé‡‡æ ·ï¼ˆDynamic RL Samplingï¼‰
- **æ¥æº**ï¼šçœŸå®GRPOè®­ç»ƒæµç¨‹
- **æ•°æ®é›†**ï¼šCRUXEvalã€GSM8Kã€LiveCodeBenchã€MATHã€MBPPã€MMLU-STEM
- **ç­–ç•¥**ï¼šæ¯æ¡promptè¿›è¡ŒK=4çš„åˆ†ç»„é‡‡æ ·ï¼Œè®°å½•å¤šä¸ªå€™é€‰å“åº”åŠå…¶é•¿åº¦

> âœ… æ€»æ ·æœ¬æ•°ï¼š23,556ï¼ˆè®­ç»ƒ14,134ï¼ŒéªŒè¯/æµ‹è¯•å„4,711ï¼‰

### å®éªŒè®¾ç½®
- **ç¡¬ä»¶**ï¼š1Ã—V100 GPU, 10æ ¸CPU, 64GBå†…å­˜
- **ä¼˜åŒ–å™¨**ï¼šAdamWï¼Œå­¦ä¹ ç‡ 2e-5ï¼Œæœ€å¤§10è½®ï¼Œbatch size=16
- **éšæœºç§å­**ï¼š42ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
- **è½¯æ ‡ç­¾è®¾è®¡**ï¼šå°†è¿ç»­é•¿åº¦ç¦»æ•£ä¸ºK=20ä¸ªbinï¼Œä½¿ç”¨è·ç¦»åæ¯”æ¦‚ç‡æ„é€ è½¯æ ‡ç­¾
- **æŸå¤±å‡½æ•°**ï¼šè”åˆæŸå¤± $ \mathcal{L} = \lambda \cdot \mathcal{L}_{CE} + (1-\lambda) \cdot \mathcal{L}_{MSE} $ï¼Œ$\lambda=0.95$

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **MAE**ï¼ˆMean Absolute Errorï¼‰ | é¢„æµ‹é•¿åº¦ä¸çœŸå®é•¿åº¦çš„å¹³å‡ç»å¯¹è¯¯å·® |
| **Throughput** â†‘ | å•ä½æ—¶é—´å®Œæˆçš„ä»»åŠ¡æ•° |
| **Job Completion Time (JCT)** â†“ | å¹³å‡ä»»åŠ¡å®Œæˆæ—¶é—´ |
| **Padding Ratio** â†“ | å¡«å……éƒ¨åˆ†å æ€»åˆ†é…èµ„æºçš„æ¯”ä¾‹ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æ¨¡å‹ |
|------|------|------|
| SSJF-Reg / SSJF-MC | å›å½’ / åˆ†ç±» | BERT |
| S3 | åˆ†ç±» | DistilBERT |
| PiA | æŒ‡ä»¤å¾®è°ƒ | Vicuna |
| TPV | çº¿æ€§å›å½’ | DeepSeek-R1-Distill å†…éƒ¨çŠ¶æ€ |
| TRAIL | MLPåˆ†ç±»å™¨ | LlamaåµŒå…¥ |
| LTR-C | åˆ†ç±» | OPT |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆMAE â†“ï¼‰

#### åœ¨ **ForeLen** ä¸Šçš„å¹³å‡MAEè¡¨ç°ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ï¼š

| æ¨¡å‹ | EGTP (Ours) | æœ€ä½³åŸºçº¿ (TRAIL) | æå‡å¹…åº¦ |
|------|-------------|------------------|----------|
| Qwen2.5 3B | **110.75** | 146.42 | â†“24.4% |
| Qwen2.5 7B | **103.47** | 137.96 | â†“25.0% |
| Llama3.2 1B | **105.08** | 151.80 | â†“30.8% |
| Llama3.2 3B | **101.53** | 157.88 | â†“35.7% |

> ğŸ“Œ **æ€»ä½“æå‡**ï¼šç›¸æ¯”æœ€å¼ºåŸºçº¿TRAILï¼Œ**å¹³å‡MAEé™ä½29.16%**ï¼›ç›¸æ¯”å¹¿æ³›ä½¿ç”¨çš„SSJF-Regï¼Œ**é™ä½55.09%**

#### åœ¨æ ‡å‡† **LMSYS** åŸºå‡†ä¸Šçš„è¡¨ç°ï¼š
- GPT-4ï¼šEGTP MAE=**87.32** vs. ç¬¬äºŒå116.91ï¼ˆâ†“25.3%ï¼‰
- Claude-2ï¼šEGTP MAE=**68.33** vs. ç¬¬äºŒå91.32ï¼ˆâ†“25.2%ï¼‰

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- åœ¨æ‰€æœ‰æ¨¡å‹å’Œåœºæ™¯ä¸‹ï¼Œ**EGTPå…¨é¢ä¸”æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿**
- ç‰¹åˆ«æ˜¯åœ¨**é•¿åºåˆ—**å’Œ**RLé‡‡æ ·**ä»»åŠ¡ä¸Šä¼˜åŠ¿æ›´å¤§ï¼Œè¯´æ˜å…¶å¯¹å¤æ‚å’ŒåŠ¨æ€åœºæ™¯æ›´å¼ºçš„é€‚åº”èƒ½åŠ›
- **PiA**ï¼ˆæŒ‡ä»¤é¢„æµ‹è‡ªèº«é•¿åº¦ï¼‰è¡¨ç°è¾ƒå·®ï¼Œè¡¨æ˜LLMéš¾ä»¥å‡†ç¡®è‡ªçŸ¥è¾“å‡ºé•¿åº¦

### æ¶ˆèå®éªŒç»“æœ

#### (1) ä¸åŒPoolingæ–¹å¼å¯¹æ¯”ï¼ˆQwen2.5 7Bï¼‰

| Pooling æ–¹æ³• | LongSeq | Reasoning | RL | Avg MAE |
|--------------|---------|-----------|----|---------|
| **EGTP (Ours)** | **81.60** | 133.57 | 95.24 | **103.47** |
| Max Pooling | 122.74 | 137.88 | 98.46 | 119.69 |
| Average Pooling | 173.85 | 142.40 | 149.92 | 155.39 |
| Last Token Pooling | 135.44 | 139.09 | 105.39 | 126.64 |

> âœ… **EGTPæ˜¾è‘—ä¼˜äºä¼ ç»ŸPoolingæ–¹æ³•**ï¼Œå°¤å…¶åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸Šå·®è·å·¨å¤§

#### (2) æŸå¤±å‡½æ•°å˜ä½“å¯¹æ¯”ï¼ˆQwen2.5-7Bï¼‰

| æ–¹æ³• | LongSeq | Reasoning | RL |
|------|---------|-----------|-----|
| **EGTP (Ours)** | **83.65** | **139.14** | **92.78** |
| Log-scale Regression | 135.73 | 136.09 | 208.02 |
| Label Smoothing | 158.44 | 144.93 | 268.88 |
| Label Noise | 119.05 | 217.44 | 201.77 |

> âœ… è½¯æ ‡ç­¾+è”åˆæŸå¤±è®¾è®¡æœ‰æ•ˆï¼Œå°¤å…¶åœ¨RLä»»åŠ¡ä¸ŠMAEé™ä½è¶…è¿‡100

#### (3) è®¡ç®—æ•ˆç‡åˆ†æï¼ˆé¢„æµ‹å»¶è¿Ÿä¸æ˜¾å­˜ï¼‰

| æ–¹æ³• | Avg Time (ms) | Avg VRAM (MB) |
|------|---------------|----------------|
| **EGTP (Ours)** | **~0.66** | **~6** |
| SSJFç³»åˆ— | 3.9â€“4.2 | ~270 |
| TRAIL | ~2.6 | ~268 |
| PiA | ~60.6 | â€” |

> âœ… EGTPå»¶è¿Ÿ**ä¸è¶³1ms**ï¼Œæ˜¾å­˜å¼€é”€ä»…**6MBå·¦å³**ï¼Œå‡ ä¹æ— é¢å¤–è´Ÿæ‹…

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **LLMå†…éƒ¨çŠ¶æ€è•´å«é•¿åº¦ä¿¡å·**ï¼šä¸»æ¨¡å‹çš„éšè—çŠ¶æ€å·²éšå«è¶³å¤Ÿä¿¡æ¯ç”¨äºé•¿åº¦é¢„æµ‹ï¼Œæ— éœ€å¤–éƒ¨è¾…åŠ©æ¨¡å‹ã€‚
2. **Tokenç†µæ˜¯å…³é”®ç‰¹å¾**ï¼šé«˜ç†µtokenæ›´èƒ½åæ˜ ç”Ÿæˆå¤æ‚åº¦ï¼ŒåŸºäºç†µåŠ æƒçš„æ± åŒ–æ˜¾è‘—æå‡é¢„æµ‹ç²¾åº¦ã€‚
3. **åŠ¨æ€é¢„æµ‹è‡³å…³é‡è¦**ï¼šåœ¨RLç­‰éšæœºé‡‡æ ·åœºæ™¯ä¸­ï¼Œ**PLP**é€šè¿‡é€æ­¥æ›´æ–°é¢„æµ‹ï¼Œèƒ½æœ‰æ•ˆåº”å¯¹â€œä¸€å¯¹å¤šâ€çš„é•¿åº¦å˜å¼‚ã€‚
4. **ç«¯åˆ°ç«¯æ”¶ç›Šæ˜¾è‘—**ï¼šé›†æˆEGTPçš„è°ƒåº¦å™¨å¯å°†**padding ratioé™ä½è‡³0.09â€“0.18**ï¼Œååé‡æå‡ï¼ŒJCTå‡åŠã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ–¹æ³•ä¾èµ–äºprefillé˜¶æ®µçš„æœ€ç»ˆå±‚éšè—çŠ¶æ€ï¼Œæœªæ¢ç´¢ä¸­é—´å±‚æˆ–å¤šå±‚èåˆï¼›
- PLPè™½æ”¯æŒåŠ¨æ€é¢„æµ‹ï¼Œä½†ä»å‡è®¾å‰©ä½™é•¿åº¦å¯ç”±å½“å‰çŠ¶æ€ä¼°è®¡ï¼Œæç«¯è·¯å¾„å¯èƒ½ä»åå·®è¾ƒå¤§ï¼›
- å¯¹è¶…é•¿åºåˆ—ï¼ˆ>17kï¼‰çš„æ³›åŒ–è™½ä¼˜äºåŸºçº¿ï¼Œä½†ä»å­˜åœ¨è¯¯å·®ç´¯ç§¯é£é™©ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢**å¤šç²’åº¦è¡¨ç¤ºèåˆ**ï¼ˆå¦‚layer-wise attentionï¼‰ä»¥è¿›ä¸€æ­¥æå‡é²æ£’æ€§ï¼›
- å°†PLPä¸**early stopping**æˆ–**speculative decoding**ç»“åˆï¼Œå®ç°æ›´ç»†ç²’åº¦æ§åˆ¶ï¼›
- æ‰©å±•è‡³**å¤šæ¨¡æ€ç”Ÿæˆ**ï¼ˆå¦‚å›¾æ–‡ã€éŸ³è§†é¢‘ï¼‰çš„é•¿åº¦/æ—¶é•¿é¢„æµ‹ï¼›
- æ„å»ºæ›´å¤§è§„æ¨¡çš„**åŠ¨æ€é•¿åº¦é¢„æµ‹Benchmark**ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸå‘å±•ã€‚

---

> ğŸ”— **ä»£ç ä¸æ•°æ®å…¬å¼€**ï¼š  
> GitHub: [https://github.com/xiehuanyi/LP_Bench](https://github.com/xiehuanyi/LP_Bench)  
> Dataset: [https://huggingface.co/datasets/abinzzz/ForeLen](https://huggingface.co/datasets/abinzzz/ForeLen)

</details>

---

### 9. [Few-Shot Design Optimization by Exploiting Auxiliary Information](https://arxiv.org/abs/2602.12112)

**Authors**: Arjun Mani, Carl Vondrick, Richard Zemel  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2602.12112v1  

#### Abstract
Many real-world design problems involve optimizing an expensive black-box function $f(x)$, such as hardware design or drug discovery. Bayesian Optimization has emerged as a sample-efficient framework for this problem. However, the basic setting considered by these methods is simplified compared to r...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šFew-Shot Design Optimization by Exploiting Auxiliary Information**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
è¯¥è®ºæ–‡é’ˆå¯¹ç°å®ä¸–ç•Œä¸­**è®¾è®¡ä¼˜åŒ–**ï¼ˆdesign optimizationï¼‰ä»»åŠ¡ä¸­å­˜åœ¨çš„ä¸¤ä¸ªå…³é”®é™åˆ¶ï¼š
- ä¼ ç»Ÿ **Bayesian Optimization (BayesOpt)** ä»…åˆ©ç”¨æ ‡é‡å¥–åŠ± $ f(x) $ è¿›è¡Œé»‘ç›’ä¼˜åŒ–ï¼Œå¿½ç•¥äº†å®éªŒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸°å¯Œè¾…åŠ©ä¿¡æ¯ï¼ˆauxiliary informationï¼‰ã€‚
- ç°å®ä¸­çš„è®¾è®¡ä»»åŠ¡å¾€å¾€ä¼šäº§ç”Ÿé«˜ç»´ã€ç»†ç²’åº¦çš„è§‚æµ‹æ•°æ®ï¼ˆå¦‚ä¼ æ„Ÿå™¨æ—¶é—´åºåˆ—ã€å­¦ä¹ æ›²çº¿ç­‰ï¼‰ï¼Œè¿™äº›ä¿¡æ¯å¯¹ç†è§£â€œä¸ºä½•æŸä¸ªè®¾è®¡æˆåŠŸ/å¤±è´¥â€è‡³å…³é‡è¦ã€‚

ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–è®¾å®šï¼šåœ¨æ¯æ¬¡è¯„ä¼°è®¾è®¡ $ x $ æ—¶ï¼Œä¸ä»…è·å¾—æ€§èƒ½æŒ‡æ ‡ $ f(x) $ï¼Œè¿˜è·å¾—ä¸€ä¸ªæ½œåœ¨é«˜ç»´çš„**è¾…åŠ©ä¿¡æ¯** $ h(x) $ï¼Œä¾‹å¦‚æœºå™¨äººæŠ“å–è¿‡ç¨‹ä¸­çš„è§¦è§‰åé¦ˆæˆ–ç¥ç»ç½‘ç»œè®­ç»ƒçš„å­¦ä¹ æ›²çº¿ã€‚

æ­¤å¤–ï¼Œå‡è®¾å­˜åœ¨ä¸€ä¸ªæ¥è‡ªç›¸åŒä»»åŠ¡æ—çš„å†å²ä»»åŠ¡é›†åˆï¼Œå¯ç”¨äºåŠ é€Ÿæ–°ä»»åŠ¡çš„ä¼˜åŒ–ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
ä½œè€…æå‡ºä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„**Few-Shot é¢„æµ‹æ¨¡å‹** $ \mathcal{P}_\theta(\cdot|C, x) $ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- åˆ©ç”¨ä¸€ä¸ªåŒ…å«å°‘é‡å·²è¯„ä¼°è®¾è®¡çš„å°å‹ä¸Šä¸‹æ–‡é›† $ C = \{(x_i, f(x_i), h(x_i))\} $ æ¥é¢„æµ‹æœªè§è®¾è®¡ $ x $ çš„æ€§èƒ½ $ f(x) $ã€‚
- æ¨¡å‹é€šè¿‡åœ¨å¤§é‡å†å²ä»»åŠ¡ä¸Šè¿›è¡Œå…ƒå­¦ä¹ è®­ç»ƒï¼Œå­¦ä¼šå¦‚ä½•æœ‰æ•ˆè¡¨ç¤ºå’Œåˆ©ç”¨ $ h(x) $ æ¥æå‡å¯¹æ–°ä»»åŠ¡çš„ few-shot é¢„æµ‹èƒ½åŠ›ã€‚
- é‡‡ç”¨ **Transformer-based æ¶æ„**ï¼ˆå— Neural Processes å¯å‘ï¼‰ï¼Œå®ç°å¯¹ä¸Šä¸‹æ–‡ä¸å˜æ€§å’Œç›®æ ‡-ä¸Šä¸‹æ–‡æ³¨æ„åŠ›æœºåˆ¶çš„æ”¯æŒã€‚

> âœ… **å…³é”®åˆ›æ–°**ï¼šå°†è¾…åŠ©ä¿¡æ¯ $ h(x) $ æ˜¾å¼åœ°èå…¥ä¸Šä¸‹æ–‡ä¸­ï¼Œå¹¶è®©æ¨¡å‹å­¦ä¹ å¦‚ä½•ä» $ h(x) $ ä¸­æå–æœ‰åŠ©äºä¼˜åŒ–çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ç®€å•åœ°å»ºæ¨¡ $ f(x) $ ä¸ $ x $ çš„å…³ç³»ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| å¯¹æ¯”ç»´åº¦ | æœ¬æ–‡æ–¹æ³• | ç°æœ‰æ–¹æ³• |
|--------|--------|--------|
| **æ˜¯å¦ä½¿ç”¨ $ h(x) $** | âœ… æ˜¯ï¼Œæ˜¾å¼åˆ©ç”¨ | âŒ å¤šæ•°ä¸ä½¿ç”¨ï¼›å°‘æ•°ï¼ˆå¦‚ Composite BayesOptï¼‰å—é™äº $ f(x)=g(h(x)) $ ç»“æ„ |
| **æ˜¯å¦æ”¯æŒå¤šä»»åŠ¡è¿ç§»** | âœ… æ˜¯ï¼Œåˆ©ç”¨ä»»åŠ¡å†å²å­¦ä¹ é€šç”¨è¡¨ç¤º | âš ï¸ éƒ¨åˆ†æ”¯æŒï¼Œä½†é€šå¸¸åªè¿ç§» $ f(x) $ ç›¸å…³ç‰¹å¾ |
| **æ¨¡å‹çµæ´»æ€§** | âœ… ç¥ç»ç½‘ç»œï¼Œå¯å¤„ç†å¤æ‚ã€å¼‚æ„ã€é«˜ç»´ $ h(x) $ | âŒ GP ç±»æ–¹æ³•è®¡ç®—æ˜‚è´µï¼Œéš¾ä»¥æ‰©å±•åˆ°é«˜ç»´ $ h(x) $ |
| **è®­ç»ƒ-æ¨ç†ä¸€è‡´æ€§** | âœ… æ¨¡å‹å†»ç»“éƒ¨ç½²ï¼Œæ— éœ€åœ¨çº¿å¾®è°ƒ | âš ï¸ ä¸€äº›æ–¹æ³•éœ€è¿­ä»£é‡è®­ç»ƒ |

> ğŸ’¡ å› æ­¤ï¼Œæœ¬æ–¹æ³•èƒ½æ›´é«˜æ•ˆåœ°åˆ©ç”¨ä¿¡æ¯ä¸°å¯Œçš„å®éªŒåé¦ˆï¼Œåœ¨æå°‘é‡è¯•éªŒä¸‹å¿«é€Ÿæ‰¾åˆ°é«˜è´¨é‡è®¾è®¡æ–¹æ¡ˆã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**

#### ï¼ˆ1ï¼‰**Gripper Design Benchmarkï¼ˆæ–°æ„å»ºï¼‰**
- **ä»»åŠ¡**ï¼šä¸ºä¸åŒ 3D ç‰©ä½“è®¾è®¡æœ€ä¼˜æœºæ¢°æ‰‹æŠ“æ‰‹å½¢çŠ¶ã€‚
- **$ x $**: æŠ“æ‰‹å‡ ä½•å‚æ•°ï¼ˆ21ç»´ Bezier æ›²é¢æ§åˆ¶ç‚¹ + åˆå§‹é«˜åº¦ï¼‰
- **$ f(x) $**: æœ€å¤§æŠ—æ‰°åŠ›ï¼ˆgrasp stabilityï¼‰ï¼Œå•ä½ Nï¼Œæœ€å¤§çº¦ 6.0N
- **$ h(x) $**: è§¦è§‰åé¦ˆï¼ŒåŒ…æ‹¬ä¸¤ä¸ª 16Ã—16 è§¦è§‰å›¾ï¼ˆtactile mapsï¼‰ã€å…¶ä»–é¢æ¥è§¦è¯»æ•°ã€ç³»ç»ŸçŠ¶æ€ï¼ˆä½ç½®ã€é€Ÿåº¦ã€æ–½åŠ åŠ›ç­‰ï¼‰ï¼ŒæŒ‰æ—¶é—´æ­¥è®°å½• â†’ å½¢æˆæ—¶é—´åºåˆ—
- **è§„æ¨¡**ï¼š**997ä¸ªç‰©ä½“ Ã— å¹³å‡ ~4300æ¬¡è¯„ä¼° = æ€»è®¡ 428ä¸‡æ¡æ•°æ®**
- **æ¥æº**ï¼šåŸºäº **ShapeNet** é‡‡æ ·ç‰©ä½“ï¼Œåœ¨ **MuJoCo** ä¸­ä»¿çœŸç”Ÿæˆ
- **åˆ’åˆ†**ï¼šè®­ç»ƒé›†ï¼ˆ772ï¼‰ã€éªŒè¯é›†ï¼ˆ75ï¼‰ã€æµ‹è¯•é›†ï¼ˆ150ï¼‰

> ğŸ” è¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€çœŸå®æ„Ÿå¼ºã€æŒ‘æˆ˜æ€§çš„ benchmarkï¼Œè¿œè¶…ç°æœ‰ multi-task BayesOpt æ•°æ®é›†ã€‚

#### ï¼ˆ2ï¼‰**LCBenchï¼ˆå…¬å¼€åŸºå‡†ï¼‰**
- **ä»»åŠ¡**ï¼šç¥ç»ç½‘ç»œè¶…å‚æ•°è°ƒä¼˜ï¼ˆ35ä¸ªåˆ†ç±»ä»»åŠ¡ï¼‰
- **$ x $**: è¶…å‚æ•°é…ç½®ï¼ˆå­¦ä¹ ç‡ã€dropoutã€å±‚æ•°ç­‰ï¼‰
- **$ f(x) $**: æœ€ä½³éªŒè¯å‡†ç¡®ç‡
- **$ h(x) $**: å®Œæ•´è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¯ epoch å­¦ä¹ æ›²çº¿ï¼ˆè®­ç»ƒ/éªŒè¯å‡†ç¡®ç‡ã€å¹³è¡¡å‡†ç¡®ç‡ã€å­¦ä¹ ç‡ç­‰ï¼Œå…±5æ¡æ›²çº¿ Ã— 50 epochï¼‰
- **ç”¨é€”**ï¼šéªŒè¯æ–¹æ³•åœ¨æ ‡å‡† HPO åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### **è®­ç»ƒé˜¶æ®µï¼ˆMeta-Trainingï¼‰**
- åœ¨æ‰€æœ‰è®­ç»ƒä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹ $ \mathcal{P}_\theta $
- æ¯è½®éšæœºé‡‡æ ·ä¸€ä¸ªä»»åŠ¡ï¼Œä»ä¸­æŠ½å– context set $ C $ å’Œ target set $ T $
- Context size: [5,30]ï¼ˆgripperï¼‰æˆ– [3,30]ï¼ˆHPOï¼‰
- Target size: å›ºå®šä¸º 100
- æŸå¤±å‡½æ•°ï¼šè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰æœ€å°åŒ–ï¼Œé¢„æµ‹åˆ†å¸ƒä¸º $ \mathcal{N}(\mu_k, \sigma_k^2) $

#### **ä¼˜åŒ–é˜¶æ®µï¼ˆTest-Time Optimizationï¼‰**
- ä½¿ç”¨è®­ç»ƒå¥½çš„ $ \mathcal{P}_\theta $ ä½œä¸º surrogate model
- åº”ç”¨äºæ–°ä»»åŠ¡çš„ **discrete BayesOpt loop**
- Acquisition Function: **Probability of Improvement (PI)**
- åˆå§‹ context: 5ä¸ªï¼ˆgripperï¼‰æˆ– 3ä¸ªï¼ˆHPOï¼‰ä½è´¨é‡æ ·æœ¬
- ä¼˜åŒ–æ­¥æ•°ï¼š30 steps
- ä¸æ›´æ–°æ¨¡å‹æƒé‡ï¼ˆzero-shot transferï¼‰

#### **è¯„ä¼°æŒ‡æ ‡**
| ç±»å‹ | æŒ‡æ ‡ |
|------|------|
| **Prediction Accuracy** | Target MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰ |
| **Optimization Performance** | - å¹³å‡å½’ä¸€åŒ–æœ€ä½³ $ f(x) $<br>- å¹³å‡ regret<br>- â€œè§£å†³â€çš„ä»»åŠ¡æ¯”ä¾‹ï¼ˆregret â‰¤ thresholdï¼‰ |

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| åŸºçº¿åç§° | æè¿° |
|--------|------|
| **Ours w/o h** | æ¶ˆèç‰ˆæœ¬ï¼šè¾“å…¥ä¸­ç§»é™¤ $ h(x) $ï¼Œä»…ä½¿ç”¨ $ f(x) $ å’Œ $ x $ |
| **DGP (Deep Kernel GP)** | SoTA å¤šä»»åŠ¡ BayesOpt æ–¹æ³•ï¼šç”¨ç¥ç»ç½‘ç»œç¼–ç å™¨å­¦ä¹ å…±äº« kernel å’Œ mean function |
| **GP-H** | æ–°å¢åŸºçº¿ï¼šä½¿ç”¨ multi-output GP è”åˆå»ºæ¨¡ $ (E_\phi(h(x)), f(x)) $ï¼Œå…¶ä¸­ $ E_\phi $ æ˜¯ä¸æœ¬æ–‡ç›¸åŒçš„ $ h(x) $ ç¼–ç å™¨ |
| **STGP (Single-Task GP)** | ç»å…¸å•ä»»åŠ¡ GPï¼Œåœ¨æµ‹è¯•æ—¶éš context åŠ¨æ€æ‹Ÿåˆ |

> âœ… æ‰€æœ‰åŸºçº¿è¦ä¹ˆå¿½ç•¥ $ h(x) $ï¼Œè¦ä¹ˆä»¥æ¬¡ä¼˜æ–¹å¼ä½¿ç”¨ã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### ï¼ˆ1ï¼‰**Gripper Design Task**

| æŒ‡æ ‡ | æœ¬æ–‡æ–¹æ³• | æœ€æ¥è¿‘åŸºçº¿ï¼ˆOurs w/o hï¼‰ | æå‡å¹…åº¦ |
|------|---------|--------------------------|--------|
| **Target MSE @ context=5** | 170.9 | 201.6 | â†“ 15.2% |
| **Best $ f(x) $ @ 30 trials (avg.)** | ~90% of max | æ˜æ˜¾æ›´ä½ | æ˜¾è‘—é¢†å…ˆ |
| **Fraction Solved (regret â‰¤ 0.5)** | **67.2%** | 58.0% | â†‘ 9.2 pts |
| **Reached Max Reward** | 34.4% | 26.7% | â†‘ 7.7 pts |

> ğŸ“ˆ å›¾6æ˜¾ç¤ºï¼šæœ¬æ–‡æ–¹æ³•åœ¨æ—©æœŸå³æ˜¾è‘—é¢†å…ˆï¼Œä¼˜åŠ¿è´¯ç©¿æ•´ä¸ªä¼˜åŒ–è¿‡ç¨‹ã€‚

#### ï¼ˆ2ï¼‰**Hyperparameter Tuning (LCBench)**

| æŒ‡æ ‡ | æœ¬æ–‡æ–¹æ³• | æœ€æ¥è¿‘åŸºçº¿ï¼ˆOurs w/o hï¼‰ | æå‡å¹…åº¦ |
|------|---------|--------------------------|--------|
| **Target MSE @ small context** | æ˜æ˜¾æ›´ä½ | è¾ƒé«˜ | å°¤å…¶åœ¨å° context ä¸‹ä¼˜åŠ¿æ˜æ˜¾ |
| **Best Accuracy @ 30 trials** | æ¥è¿‘ç†è®ºæœ€ä¼˜ | æ”¶æ•›æ…¢æˆ–åœæ» | æ›´å¿«é€¼è¿‘â€œæœ€åä¸€è‹±é‡Œâ€ |
| **Fraction Solved (regret â‰¤ 0.01)** | **89.7%** | 82.1% | â†‘ 7.6 pts |

> ğŸ“Š å›¾10æ˜¾ç¤ºï¼šæœ¬æ–‡æ–¹æ³•èƒ½æ›´å¿«æ”¶æ•›è‡³æœ€ä¼˜é…ç½®ï¼Œåœ¨ learning rate å’Œ hidden units ä¸Šè¡¨ç°å°¤ä¸ºç¨³å®šã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**
- **Ours vs. Ours w/o h**ï¼šåœ¨ä¸¤ç§ä»»åŠ¡ä¸Šå‡æ˜¾è‘—èƒœå‡ºï¼Œè¯æ˜ $ h(x) $ çš„æœ‰æ•ˆæ€§ã€‚
  - åœ¨ context=5 æ—¶ï¼ŒMSE ä¸‹é™è¾¾ 15%
  - è¡¨æ˜å³ä½¿åœ¨å¤±è´¥å°è¯•ä¸­ï¼Œ$ h(x) $ ä»è•´å«æ”¹è¿›çº¿ç´¢ï¼ˆå¦‚è§¦è§‰åˆ†å¸ƒæ¨¡å¼ï¼‰
- **Ours vs. GP-H**ï¼šå°½ç®¡ GP-H ä¹Ÿä½¿ç”¨ $ h(x) $ï¼Œä½†æ€§èƒ½ä¸å¦‚æœ¬æ–‡æ–¹æ³•ã€‚
  - è¯´æ˜ç®€å•çš„è”åˆå»ºæ¨¡ä¸è¶³ä»¥æ•æ‰ $ h(x) $ ä¸­æ·±å±‚è¯­ä¹‰
  - ç¥ç»ç½‘ç»œ + Transformer æ¶æ„æ›´èƒ½çµæ´»å­¦ä¹ æœ‰ç”¨è¡¨ç¤º

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. âœ… **è¾…åŠ©ä¿¡æ¯ $ h(x) $ å…·æœ‰å·¨å¤§æ½œåŠ›**ï¼šå³ä½¿ $ f(x) $ ç›¸åŒï¼Œä¸åŒçš„ $ h(x) $ å¯æ­ç¤ºè®¾è®¡æˆè´¥çš„æ ¹æœ¬åŸå› ï¼Œä»è€ŒæŒ‡å¯¼æ›´æ™ºèƒ½çš„æœç´¢ã€‚
2. âœ… **Few-shot prediction + auxiliary info = æ›´å¿«ä¼˜åŒ–**ï¼šæ¨¡å‹é€šè¿‡å†å²ä»»åŠ¡å­¦ä¼šâ€œè¯»æ‡‚â€ $ h(x) $ï¼Œåœ¨æ–°ä»»åŠ¡ä¸­ä»…éœ€å°‘é‡è¯•éªŒå³å¯åšå‡ºç²¾å‡†é¢„æµ‹ã€‚
3. âœ… **Transformer æ¶æ„é€‚åˆè¯¥ä»»åŠ¡**ï¼šèƒ½å¤Ÿè‡ªç„¶å¤„ç†å˜é•¿ä¸Šä¸‹æ–‡ã€å®ç°ä¸Šä¸‹æ–‡-ç›®æ ‡é—´æ³¨æ„åŠ›ï¼Œä¸”æ˜“äºæ‰©å±•åˆ°é«˜ç»´ $ h(x) $ã€‚
4. âœ… **æ–¹æ³•å…·æœ‰è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›**ï¼šåœ¨åŒä¸€æ¡†æ¶ä¸‹æˆåŠŸåº”ç”¨äºæœºå™¨äººç¡¬ä»¶è®¾è®¡å’Œç¥ç»ç½‘ç»œ HPOï¼Œè¡¨æ˜å…¶é€šç”¨æ€§ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- â— **ä¾èµ–ä»»åŠ¡åŒåˆ†å¸ƒå‡è®¾**ï¼šæµ‹è¯•ä»»åŠ¡å¿…é¡»æ¥è‡ªä¸è®­ç»ƒä»»åŠ¡ç›¸ä¼¼çš„ä»»åŠ¡æ—ï¼Œå¦åˆ™è¿ç§»æ•ˆæœå¯èƒ½ä¸‹é™ã€‚
- â— **æœªå»ºæ¨¡é•¿æœŸè§„åˆ’**ï¼šå½“å‰ä½¿ç”¨ one-step lookaheadï¼ˆPIï¼‰ï¼Œæœªè€ƒè™‘é€‰æ‹©æŸäº›è®¾è®¡ä»¥è·å–æ›´å¤šä¿¡æ¯ï¼ˆinformation gatheringï¼‰ã€‚
- â— **$ h(x) $ ç¼–ç ä»å…·æŒ‘æˆ˜æ€§**ï¼šè™½ç„¶ç«¯åˆ°ç«¯è®­ç»ƒæœ‰æ•ˆï¼Œä½†å¯¹äºæé«˜ç»´æˆ–ç¨€ç–ä¿¡å·ï¼ˆå¦‚è§¦è§‰å›¾ï¼‰ï¼Œè¡¨ç¤ºå­¦ä¹ ä»æ˜¯éš¾ç‚¹ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **Out-of-Distribution Task Handling**ï¼šå½“é‡åˆ°æç«¯ OOD ä»»åŠ¡æ—¶ï¼Œå¯ç»“åˆ cold-start modelï¼ˆå¦‚ STGPï¼‰è¿›è¡Œæ··åˆå†³ç­–ã€‚
2. **Long-Horizon Acquisition Functions**ï¼šæ¢ç´¢åŸºäº $ h(x) $ çš„ä¿¡æ¯å¢ç›Šé©±åŠ¨ç­–ç•¥ï¼Œä¸»åŠ¨é€‰æ‹©èƒ½æä¾›æœ€å¤šæ´å¯Ÿçš„è®¾è®¡ã€‚
3. **Two-Stage Representation Learning**ï¼šå…ˆå¯¹ $ h(x) $ è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼ˆå¦‚é¢„æµ‹åŠ¨åŠ›å­¦ï¼‰ï¼Œå†å°†å…¶åµŒå…¥é¢„æµ‹æ¨¡å‹ã€‚
4. **æ‰©å±•è‡³æ›´å¤šé¢†åŸŸ**ï¼šå¦‚è¯ç‰©åˆ†å­è®¾è®¡ï¼ˆ$ h(x) $ ä¸ºåˆ†å­åŠ¨æ€æ¨¡æ‹Ÿè½¨è¿¹ï¼‰ã€èŠ¯ç‰‡è®¾è®¡ï¼ˆ$ h(x) $ ä¸ºç”µç£åœºä»¿çœŸè¾“å‡ºï¼‰ç­‰ã€‚

---

## **æ€»ç»“**

ğŸ“Œ æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé¢å‘**ä¿¡æ¯ä¸°å¯Œç¯å¢ƒ**çš„æ–°å‹è®¾è®¡ä¼˜åŒ–èŒƒå¼ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨**è¾…åŠ©ä¿¡æ¯ $ h(x) $** å’Œ**å¤šä»»åŠ¡å†å²**æ¥åŠ é€Ÿæ–°ä»»åŠ¡çš„ few-shot ä¼˜åŒ–ã€‚

ğŸ”§ æ–¹æ³•ä¸Šï¼Œæå‡ºåŸºäº Transformer çš„ç¥ç»é¢„æµ‹æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸Šä¸‹æ–‡ä¸­èåˆ $ h(x) $ å¹¶å®ç°é«˜ç²¾åº¦æ€§èƒ½é¢„æµ‹ã€‚

ğŸ“Š å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæå…·æŒ‘æˆ˜æ€§çš„é¢†åŸŸï¼ˆæœºå™¨äººæŠ“æ‰‹è®¾è®¡ã€HPOï¼‰ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ SoTA æ–¹æ³•ï¼Œå°¤å…¶åœ¨å°æ ·æœ¬åœºæ™¯ä¸‹ä¼˜åŠ¿æ˜æ˜¾ã€‚

ğŸŒ è¯¥å·¥ä½œæ¨åŠ¨äº† AI-driven design å‘æ›´è´´è¿‘çœŸå®ç§‘ç ”å·¥ç¨‹åœºæ™¯çš„æ–¹å‘å‘å±•ï¼Œå…·å¤‡å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

</details>

---

### 10. [Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge](https://arxiv.org/abs/2602.11340)

**Authors**: Bo Pan, Xuan Kan, Kaitai Zhang, Yan Yan, Shunwen Tan, Zihao He, Zixin Ding, Junjie Wu, Liang Zhao  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.11340v1  

#### Abstract
Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šBi-Level Prompt Optimization for Multimodal LLM-as-a-Judge

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
è¯¥è®ºæ–‡èšç„¦äº **Multimodal LLM-as-a-Judge**ï¼ˆMLLM-as-a-Judgeï¼‰åœºæ™¯ä¸‹çš„è‡ªåŠ¨è¯„ä¼°ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯å¯¹ AI ç”Ÿæˆå›¾åƒçš„è´¨é‡è¿›è¡Œè¯„åˆ†ã€‚å°½ç®¡ Large Language Models (LLMs) å·²è¢«å¹¿æ³›ç”¨ä½œè‡ªåŠ¨åŒ–è¯„åˆ¤è€…ï¼ˆLLM-as-a-Judgeï¼‰ï¼Œä½†åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­ï¼Œå…¶è¯„ä»·ç»“æœä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚

ç°æœ‰åŸºäº trial-and-error çš„ **Auto Prompt Optimization (APO)** æ–¹æ³•åœ¨æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†å›¾åƒæ—¶é¢ä¸´ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼š  
> **è§†è§‰ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶**ï¼ˆcontext window constraintsï¼‰å¯¼è‡´ MLLM éš¾ä»¥åŒæ—¶å¤„ç†å¤šä¸ªå›¾åƒè¾“å…¥ï¼Œä»è€Œé™åˆ¶äº†é”™è¯¯æ ·æœ¬çš„æ‰¹é‡æ”¶é›†å’Œæœ‰æ•ˆæç¤ºä¼˜åŒ–ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä½œè€…æå‡ºäº† **BLPO**ï¼ˆ**Bi-Level Prompt Optimization**ï¼‰æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- å°†å›¾åƒé€šè¿‡ **Image-to-Text (I2T)** æ¨¡å—è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œä»¥èŠ‚çœè§†è§‰ token å¼€é”€ï¼›
- åŒæ—¶è”åˆä¼˜åŒ–ä¸¤ä¸ªå±‚çº§çš„ promptï¼š
  - **Judge Prompt**ï¼šæŒ‡å¯¼ LLM å¦‚ä½•æ‰“åˆ†ï¼›
  - **I2T Prompt**ï¼šæŒ‡å¯¼ MLLM å¦‚ä½•ç”Ÿæˆä¸è¯„ä¼°ç›¸å…³çš„å›¾åƒæè¿°ï¼ˆè€Œéé€šç”¨ captionï¼‰ã€‚

è¿™ç§ **bi-level optimization** ç»“æ„å®ç°äº†ï¼š
- åœ¨æœ‰é™ context budget ä¸‹é«˜æ•ˆåˆ©ç”¨é”™è¯¯æ ·ä¾‹ï¼›
- åŠ¨æ€å­¦ä¹ â€œå“ªäº›è§†è§‰ç‰¹å¾å¯¹è¯„ä¼°æœ€é‡è¦â€ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯ä¼ é€’çš„æ–‡æœ¬çº¿ç´¢ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ APO-imageï¼‰ | BLPO |
|------|------------------------|-------|
| è¾“å…¥å½¢å¼ | åŸå§‹å›¾åƒï¼ˆæ¶ˆè€—å¤§é‡è§†è§‰ tokenï¼‰ | æ–‡æœ¬åŒ–æè¿°ï¼ˆèŠ‚çœ contextï¼‰ |
| I2T èƒ½åŠ› | å›ºå®šã€é€šç”¨ captionï¼ˆä¿¡æ¯ä¸¢å¤±ä¸¥é‡ï¼‰ | å¯å­¦ä¹ ã€ä»»åŠ¡ç›¸å…³æè¿° |
| ä¼˜åŒ–æœºåˆ¶ | å•å±‚ prompt æ›´æ–° | åŒå±‚ååŒä¼˜åŒ–ï¼ˆJudge + I2T Promptï¼‰ |
| æ‰©å±•æ€§ | å—é™äºæ¨¡å‹è§†è§‰ context é•¿åº¦ | æ›´é€‚ç”¨äºå¤§è§„æ¨¡é”™è¯¯é›†è®­ç»ƒ |

å› æ­¤ï¼ŒBLPO åœ¨ä¿æŒé«˜ human-alignment çš„åŒæ—¶æ˜¾è‘—æå‡äº† prompt ä¼˜åŒ–æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
å®éªŒåœ¨å››ä¸ªä¸»æµå¤šæ¨¡æ€è¯„ä¼°åŸºå‡†ä¸Šè¿›è¡Œï¼š

| æ•°æ®é›† | ä»»åŠ¡ç±»å‹ | æ ‡ç­¾èŒƒå›´ | æ ·æœ¬æ•°ï¼ˆæ¯é›†åˆï¼‰ |
|--------|----------|---------|------------------|
| **AGIN** | å›¾åƒè‡ªç„¶åº¦è¯„åˆ† | 1â€“7 åˆ† | 100 è®­ç»ƒ/100 æµ‹è¯• |
| **SeeTRUE** | å›¾æ–‡ä¸€è‡´æ€§åˆ¤æ–­ | äºŒåˆ†ç±»ï¼ˆ0/1ï¼‰ | 200/200 |
| **ImageReward** | å›¾æ–‡åå¥½è¯„åˆ† | 1â€“7 åˆ† | 140/140 |
| **UnsafeBench** | å®‰å…¨æ€§åˆ¤æ–­ | äºŒåˆ†ç±»ï¼ˆSafe/Unsafeï¼‰ | 110/110 |

è¿™äº›æ•°æ®é›†è¦†ç›–äº†ä»è´¨é‡è¯„åˆ†åˆ°å®‰å…¨å®¡æ ¸ç­‰å¤šç§å®é™…åº”ç”¨åœºæ™¯ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **Backbone Judge Models**ï¼š
  - `Qwen2.5-VL-32B-instruct`
  - `Llama-4-Scout-17B-16E-instruct`
  - `Llama-4-Maverick-17B-128E-instruct`

- **Optimizer LLM**ï¼šOpenAIâ€™s `GPT-o3`ï¼ˆtemperature=0.0ï¼‰

- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - åˆ†ç±»ä»»åŠ¡ï¼š**Accuracy** å’Œ **Macro F1**
  - å›å½’ä»»åŠ¡ï¼ˆå¦‚ AGINï¼‰ï¼šå°†é¢„æµ‹ç¦»æ•£åŒ–åè®¡ç®— F1/Accuracy

- **ä¼˜åŒ–å‚æ•°**ï¼š
  - æœ€å¤§è¿­ä»£è½®æ¬¡ï¼š5 è½®ï¼ˆouter loopï¼‰
  - æ¯è½®é”™è¯¯é›†å¤§å°ï¼šæœ€å¤š 10 ä¸ªé”™è¯¯æ ·æœ¬
  - å†…å±‚ I2T prompt ä¼˜åŒ–æ­¥æ•°ï¼šK=5

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ¯”è¾ƒäº†å¤šç§å…ˆè¿›çš„ LLM-based prompt optimization æ–¹æ³•ï¼š
- **OPRO**ï¼šåŸºäº LLM çš„ prompt è¿›åŒ–æœç´¢
- **TextGrad**ï¼šåŸºäºâ€œæ–‡æœ¬æ¢¯åº¦â€çš„è‡ªåŠ¨å¾®åˆ†å¼ä¼˜åŒ–
- **APO-image**ï¼šç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒä½œä¸ºè¾“å…¥çš„ APO ç‰ˆæœ¬ï¼ˆGPT-o3 ä¸º optimizerï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰
åœ¨æ‰€æœ‰ä¸‰ä¸ª judge æ¨¡å‹å’Œå››ä¸ªæ•°æ®é›†ä¸Šï¼Œ**BLPO å‡è¾¾åˆ°æœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜æ€§èƒ½**ï¼š

| æ–¹æ³• | UnsafeBench (F1) | AGIN (F1) | SeeTRUE (F1) | ImageReward (F1) |
|------|-------------------|-----------|---------------|--------------------|
| No Optim. | ~0.65 | ~0.15 | ~0.73 | ~0.20 |
| TextGrad | ~0.75 | ~0.25 | ~0.77 | ~0.28 |
| OPRO | ~0.72 | ~0.26 | ~0.78 | ~0.32 |
| APO-image | ~0.70 | ~0.15 | ~0.75 | ~0.27 |
| **BLPO (Ours)** | **0.80â€“0.89** | **0.23â€“0.33** | **0.80â€“0.82** | **0.32â€“0.36** |

> âœ… **å¹³å‡æå‡çº¦ 8% F1 äºç¬¬äºŒä½³æ–¹æ³•ï¼ˆå¦‚ OPRO æˆ– TextGradï¼‰ä¹‹ä¸Š**ï¼Œå°¤å…¶åœ¨ UnsafeBench ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **ç¨³å®šæ€§æ›´å¼º**ï¼šå¦‚å›¾ 3 æ‰€ç¤ºï¼ŒBLPO åœ¨å„æ•°æ®é›†ä¸Šçš„ä¼˜åŒ–æ›²çº¿æ›´å¹³ç¨³ä¸”æ”¶æ•›æ›´å¿«ï¼›
- **è·¨æ¨¡å‹é²æ£’æ€§å¼º**ï¼šåœ¨ä¸åŒ backboneï¼ˆQwen / Llama4 ç³»åˆ—ï¼‰ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´ä¼˜è¶Šæ€§ï¼›
- **ä¼˜äºç›´æ¥ä¼ å›¾æ–¹æ³•ï¼ˆAPO-imageï¼‰**ï¼šè¯´æ˜ç®€å•å¢åŠ è§†è§‰è¾“å…¥å¹¶ä¸èƒ½è§£å†³ context æ‹¥å¡é—®é¢˜ï¼Œåè€Œå¯èƒ½é™ä½æ¨ç†è´¨é‡ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 2ï¼‰
åœ¨ `Llama-4-Scout` ä¸Šè¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼š

| å˜ä½“ | æ–¹æ³•è¯´æ˜ | ImageReward (F1) | UnsafeBench (F1) |
|------|--------|-------------------|-------------------|
| Fixed I2T Prompt | ä½¿ç”¨å›ºå®šæŒ‡ä»¤ `"Describe in detail"` | 0.25 | 0.73 |
| Judge Prompt-based I2T | ç”¨å½“å‰ judge prompt å¼•å¯¼ I2T æè¿° | 0.32 | 0.78 |
| **BLPO (Proposed)** | åŒå±‚è”åˆä¼˜åŒ– I2T + Judge Prompt | **0.34** | **0.81** |

> ğŸ” å‘ç°ï¼š**å¯å­¦ä¹ çš„ I2T prompt è‡³å…³é‡è¦**ï¼Œä»…é å…±äº« judge prompt ä¸è¶³ä»¥å®ç°æœ€ä½³æ•ˆæœã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **å¤šæ¨¡æ€ prompt optimization çš„æ ¸å¿ƒç“¶é¢ˆåœ¨äºè§†è§‰ context é™åˆ¶**ï¼Œè€Œéè¯­è¨€å»ºæ¨¡æœ¬èº«ï¼›
2. **å°†å›¾åƒè½¬ä¸ºä»»åŠ¡å¯¼å‘çš„æ–‡æœ¬æè¿°ï¼ˆlearnable I2Tï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„ä¿¡æ¯å‹ç¼©æ–¹å¼**ï¼›
3. **I2T prompt åº”ä¸ judge prompt å…±åŒä¼˜åŒ–**ï¼Œæ‰èƒ½æ•æ‰åˆ°çœŸæ­£å½±å“è¯„ä¼°çš„å…³é”®è§†è§‰ç»†èŠ‚ï¼›
4. **BLPO æ˜¾è‘—æé«˜äº† MLLM-as-a-Judge ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§**ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„è·¨ä»»åŠ¡è¿ç§»æ€§å’Œç¨³å®šæ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰ I2T è½¬æ¢ä¾èµ–å¤–éƒ¨ MLLMï¼Œå¸¦æ¥é¢å¤–å»¶è¿Ÿå’Œæˆæœ¬ï¼›
- å¯¹æç«¯ç»†ç²’åº¦ç¼ºé™·ï¼ˆå¦‚åƒç´ çº§ä¼ªå½±ï¼‰ä»å¯èƒ½å­˜åœ¨ä¿¡æ¯æŸå¤±ï¼›
- æ‰€æœ‰å®éªŒåŸºäº black-box optimizerï¼ˆGPT-o3ï¼‰ï¼Œæœªæ¢ç´¢å¼€æºæ›¿ä»£æ–¹æ¡ˆã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ç«¯åˆ°ç«¯å¯å¾®çš„ I2T æ¨¡å—ï¼Œå‡å°‘å¯¹å¤–éƒ¨ LLM çš„ä¾èµ–ï¼›
- å°† BLPO æ‰©å±•è‡³è§†é¢‘æˆ–å¤šå›¾æ¨ç†ä»»åŠ¡ï¼›
- ç ”ç©¶å¦‚ä½•å°† human feedback æ›´æœ‰æ•ˆåœ°èå…¥ bi-level loop ä¸­ï¼Œå½¢æˆé—­ç¯ä¼˜åŒ–ç³»ç»Ÿï¼›
- æ¢ç´¢è½»é‡åŒ– I2T prompt è®¾è®¡ï¼Œè¿›ä¸€æ­¥å‹ç¼© context å¼€é”€ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **BLPO æ˜¯é¦–ä¸ªé’ˆå¯¹å¤šæ¨¡æ€ LLM-as-a-Judge åœºæ™¯è®¾è®¡çš„åŒå±‚ prompt ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡è”åˆä¼˜åŒ– Judge Prompt ä¸ I2T Promptï¼Œåœ¨å—é™è§†è§‰ context ä¸‹å®ç°äº†æ›´é«˜æ•ˆã€æ›´è´´è¿‘äººç±»åˆ¤æ–­çš„è‡ªåŠ¨è¯„ä¼°èƒ½åŠ›ã€‚**

</details>

---

### 11. [Robust Optimization Approach and Learning Based Hide-and-Seek Game for Resilient Network Design](https://arxiv.org/abs/2602.11854)

**Authors**: Mohammad Khosravi, Setareh Maghsudi  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2602.11854v1  

#### Abstract
We study the design of resilient and reliable communication networks in which a signal can be transferred only up to a limited distance before its quality falls below an acceptable threshold. When excessive signal degradation occurs, regeneration is required through regenerators installed at selecte...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Robust Optimization Approach and Learning Based Hide-and-Seek Game for Resilient Network Design*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡ç ”ç©¶çš„æ˜¯**Resilient Communication Network Design**ä¸­çš„**Regenerator Location Problem (RLP)**ï¼Œå³åœ¨é€šä¿¡ç½‘ç»œä¸­éƒ¨ç½²ä¿¡å·å†ç”Ÿå™¨ï¼ˆregeneratorï¼‰ä»¥åº”å¯¹ä¿¡å·è¡°å‡é—®é¢˜ã€‚è¯¥é—®é¢˜çš„å…³é”®æŒ‘æˆ˜åœ¨äºï¼š
- ä¿¡å·åªèƒ½åœ¨æœ‰é™è·ç¦» $d_{\text{max}}$ å†…ä¿æŒè´¨é‡ï¼Œè¶…è¿‡åˆ™éœ€é€šè¿‡ regenerator æ¢å¤ã€‚
- **èŠ‚ç‚¹å®‰è£…æˆæœ¬**å’Œ**é“¾è·¯é•¿åº¦**å‡å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œå½±å“ç½‘ç»œè¿é€šæ€§å’Œæ€»æˆæœ¬ã€‚

ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å‡è®¾å‚æ•°å›ºå®šæˆ–ä»…è€ƒè™‘é™æ€ä¸ç¡®å®šæ€§ï¼Œéš¾ä»¥åæ˜ ç°å®ä¸–ç•Œä¸­éšæ—¶é—´å˜åŒ–çš„åŠ¨æ€æ³¢åŠ¨ï¼ˆå¦‚æ¸©åº¦ã€æµé‡å˜åŒ–å¯¼è‡´çš„å…‰çº¤è´¨é‡æ³¢åŠ¨ï¼‰ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

#### ï¼ˆ1ï¼‰å¼•å…¥**Dynamic Budgeted Uncertainty Set**
- **Static Budgeted Uncertainty**ï¼šç”¨äºå»ºæ¨¡ regenerator å®‰è£…æˆæœ¬çš„ä¸ç¡®å®šæ€§ï¼Œå…è®¸æœ€å¤š $I_v$ ä¸ªèŠ‚ç‚¹çš„æˆæœ¬ä¸Šå‡è‡³å…¶ä¸Šé™ã€‚
- **Dynamic Budgeted Uncertainty**ï¼š**é¦–æ¬¡æå‡º**ç”¨äºå»ºæ¨¡è¾¹é•¿ï¼ˆedge lengthï¼‰çš„æ—¶é—´ä¾èµ–æ€§åå·®ã€‚æ¯ä¸ªæ—¶é—´æ®µ $t$ï¼Œæœ€å¤š $I_e$ æ¡è¾¹å¯è¢«â€œæ¿€æ´»â€ä»¥å¼•å…¥æœ€å¤§åå·®ï¼Œæ›´è´´è¿‘å®é™…ç½‘ç»œç¯å¢ƒï¼ˆå¦‚æ˜¼å¤œæ¸©å·®ã€ç»´æŠ¤å‘¨æœŸç­‰ï¼‰ã€‚

#### ï¼ˆ2ï¼‰æ„å»ºä¸‰ç§æ•°å­¦æ¨¡å‹
| æ¨¡å‹ | æè¿° |
|------|------|
| **DWC (Deterministic Worst-Case)** | æ‰€æœ‰å‚æ•°å–ä¸Šç•Œå€¼ï¼Œæœ€ä¿å®ˆä½†å¯èƒ½è¿‡åº¦è®¾è®¡ |
| **RSB (Robust Static Budgeted)** | èŠ‚ç‚¹ä¸è¾¹å‡é‡‡ç”¨é™æ€ budgeted ä¸ç¡®å®šæ€§ |
| **RDB (Robust Dynamic Budgeted)** | **æ ¸å¿ƒåˆ›æ–°**ï¼šè¾¹é‡‡ç”¨åŠ¨æ€ budgeted ä¸ç¡®å®šæ€§ï¼ŒèŠ‚ç‚¹ä¸ºé™æ€ |

#### ï¼ˆ3ï¼‰å¼€å‘å¤šç§é«˜æ•ˆæ±‚è§£ç®—æ³•
é’ˆå¯¹å¤§è§„æ¨¡å®ä¾‹ï¼Œæå‡ºä»¥ä¸‹å¯æ‰©å±•æ–¹æ³•ï¼š
- **Column-and-Constraint Generation (CCG)**ï¼šè¿­ä»£ç”Ÿæˆå¯¹æŠ—åœºæ™¯ï¼Œæ”¶æ•›å¿«ã€‚
- **Benders Decomposition (BDC)**ï¼šå°†ä¸»é—®é¢˜ï¼ˆplacementï¼‰ä¸å­é—®é¢˜ï¼ˆconnectivity checkï¼‰åˆ†ç¦»ã€‚
- **Iterative Robust Optimization (IRO)**ï¼šäº¤æ›¿ä¼˜åŒ–å›¾ç»“æ„ $M$ å’Œ regenerator æ”¾ç½®æ–¹æ¡ˆã€‚
- **Learning-based Hide-and-Seek Game (HSL)**ï¼šå°†é²æ£’ä¼˜åŒ–å»ºæ¨¡ä¸ºå­¦ä¹ è€…ï¼ˆseekerï¼‰ä¸å¯¹æ‰‹ï¼ˆadversaryï¼‰ä¹‹é—´çš„åšå¼ˆï¼Œåˆ©ç”¨æ¢¯åº¦æ›´æ–°æ¨¡æ‹Ÿè‡ªé€‚åº”æ”»å‡»ä¸é˜²å¾¡ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **å»ºæ¨¡èƒ½åŠ›** | æ˜¾å¼æ•æ‰è¾¹é•¿çš„**æ—¶é—´åŠ¨æ€æ€§**ï¼Œä¼˜äºé™æ€é²æ£’æˆ–ç¡®å®šæ€§æœ€åæƒ…å†µæ¨¡å‹ |
| **è§£å†³æ–¹æ¡ˆè´¨é‡** | åœ¨å¤šæ•°æƒ…å†µä¸‹å®ç° **10%-30% æˆæœ¬é™ä½**ï¼Œé¿å…è¿‡åº¦ä¿å®ˆè®¾è®¡ |
| **å¯æ‰©å±•æ€§** | CCG å’Œ BDC åœ¨å¤§å°ºåº¦ç½‘ç»œä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç›´æ¥æ±‚è§£æ··åˆæ•´æ•°è§„åˆ’ |
| **åˆ†æå·¥å…·** | HSL æä¾›äº†ä¸€ç§æ–°é¢–çš„è§†è§’æ¥ç†è§£è„†å¼±è·¯å¾„å’Œå…³é”®è¾¹ï¼Œæ”¯æŒå¿«é€Ÿè¿‘ä¼¼æ±‚è§£ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†ç”Ÿæˆæ–¹å¼
æ‰€æœ‰æµ‹è¯•å®ä¾‹å‡ä¸º**éšæœºç”Ÿæˆ**ï¼Œéµå¾ªç»Ÿä¸€è¿‡ç¨‹ï¼š
- å›¾ $G=(V,E)$ ä¸º ErdÅ‘sâ€“RÃ©nyi éšæœºå›¾ï¼Œå¯†åº¦ `dens=0.3`
- èŠ‚ç‚¹æ•° $n \in \{10,12,\dots,60\}$
- è¾¹ $(i,j)$ çš„æ ‡ç§°é•¿åº¦ $c_e \sim \text{Uniform}(350,600)$ï¼Œæœ€å¤§åå·® $d_e \sim \text{Uniform}(1,250)$
- èŠ‚ç‚¹å®‰è£…æˆæœ¬ $c_v \sim \text{Uniform}(250,300)$ï¼Œæœ€å¤§åå·® $d_v \sim \text{Uniform}(1,50)$
- $d_{\text{max}} = 1000$

å…±è¿›è¡Œå››ç»„å®éªŒï¼ˆExp-1 è‡³ Exp-4ï¼‰ï¼Œåˆ†åˆ«æ§åˆ¶ä¸åŒå˜é‡ç»„åˆã€‚

---

### å®éªŒè®¾ç½®
| è®¾ç½®é¡¹ | è¯´æ˜ |
|--------|------|
| å¹³å° | Intel Core i7-10510U @1.8GHz, 15GB RAM, å•çº¿ç¨‹è¿è¡Œ |
| æ±‚è§£å™¨ | IBM ILOG CPLEX 22.11 |
| æ¯ç»„å®ä¾‹æ•°é‡ | 50 æ¬¡ç‹¬ç«‹è¿è¡Œå–å¹³å‡ |
| æ—¶é—´é™åˆ¶ | æ— æ˜¾å¼è¶…æ—¶ï¼ˆå°è§„æ¨¡å¯è§£ï¼‰ï¼Œå¤§å®ä¾‹è®¾æœ€å¤§è¿­ä»£æ¬¡æ•°ä¸º 10ï¼ˆExp-4ï¼‰ |

---

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **Objective Value (Cost)** | æ€»å®‰è£…æˆæœ¬ï¼ˆå«ä¸ç¡®å®šæ€§ä¸‹çš„æœ€åæƒ…å†µï¼‰ |
| **R-DWC (%)** | ç›¸å¯¹äº DWC çš„æˆæœ¬èŠ‚çœç™¾åˆ†æ¯” |
| **R-RSB (%)** | ç›¸å¯¹äº RSB çš„æˆæœ¬èŠ‚çœç™¾åˆ†æ¯” |
| **Solution Time** | æ±‚è§£è€—æ—¶ï¼ˆç§’ï¼‰ |
| **Iterations** | è¿­ä»£ç±»ç®—æ³•ï¼ˆCCG/BDC/IRO/HSLï¼‰æ‰€éœ€è¿­ä»£è½®æ¬¡ |
| **Performance Profile** | å±•ç¤ºå„ç®—æ³•åœ¨å¤šå¤§æ¯”ä¾‹å®ä¾‹ä¸Šè¾¾åˆ°ä¸€å®šæ€§èƒ½å€ç‡ä»¥å†…ï¼ˆè§ Appendix Bï¼‰ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| **DWC** | ç¡®å®šæ€§æœ€åæƒ…å†µ | å‚æ•°å…¨å–ä¸Šç•Œï¼ŒåŸºå‡†ä¸­æœ€ä¿å®ˆ |
| **RSB** | é™æ€é²æ£’ä¼˜åŒ– | ç»å…¸ budgeted robust å¯¹ç…§ç»„ |
| **RDB** | åŠ¨æ€é²æ£’ä¼˜åŒ–ï¼ˆæœ¬æ–‡æå‡ºï¼‰ | ä¸»è¦æ¯”è¾ƒå¯¹è±¡ |
| **CCG / BDC / IRO / HSL** | å¯æ‰©å±•ç®—æ³• | ç”¨äºæ›¿ä»£ç›´æ¥æ±‚è§£çš„å¤§è§„æ¨¡æ±‚è§£ç­–ç•¥ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### Exp-1ï¼šå°è§„æ¨¡ç½‘ç»œï¼ˆ$n=10$ åˆ° $30$ï¼‰
| æ–¹æ³• | æˆæœ¬è¶‹åŠ¿ | R-DWC æœ€é«˜æå‡ | æ—¶é—´è¡¨ç° |
|------|----------|------------------|-----------|
| DWC | ~860 | â€” | æœ€å¿« |
| RSB | ç•¥ä½äº DWC | ~1.4% | å¿« |
| **RDB** | **æŒç»­æœ€ä½** | **11.69%** ($n=30$) | æœ€æ…¢ï¼ˆå› å¤æ‚å»ºæ¨¡ï¼‰ |

> âœ… **ç»“è®º**ï¼šåŠ¨æ€ä¸ç¡®å®šæ€§å»ºæ¨¡å¸¦æ¥æ˜¾è‘—æˆæœ¬èŠ‚çº¦ï¼Œå°¤å…¶åœ¨ç½‘ç»œå¢å¤§æ—¶æ•ˆæœæ›´æ˜æ˜¾ã€‚

---

### Exp-2ï¼šå˜åŒ–æ”»å‡»é¢„ç®—ï¼ˆ$I_e \in \{1,2\}, I_v \in \{1,2,3\}$ï¼‰
| åœºæ™¯ | RDB vs DWC æˆæœ¬èŠ‚çœ |
|------|--------------------|
| $I_e=1$ | é«˜è¾¾ **28.03%** |
| $I_e=2$ | çº¦ **10.95%** |

> ğŸ” å‘ç°ï¼šå½“æ•Œæ–¹å¯¹è¾¹çš„æ”»å‡»é¢„ç®—è¾ƒå°æ—¶ï¼ˆ$I_e=1$ï¼‰ï¼ŒRDB çš„ä¼˜åŠ¿æ›´ä¸ºçªå‡ºâ€”â€”è¯´æ˜åŠ¨æ€å»ºæ¨¡èƒ½æ›´å¥½åˆ©ç”¨â€œä½é¢‘é«˜åç§»â€äº‹ä»¶çš„ä¿¡æ¯ã€‚

---

### Exp-3 & Exp-4ï¼šå¤§è§„æ¨¡ç½‘ç»œï¼ˆ$n=40$ åˆ° $60$, $n=50$ï¼‰
| æ–¹æ³• | æˆæœ¬æ”¹è¿›ï¼ˆvs DWCï¼‰ | è¿­ä»£æ¬¡æ•° | æ±‚è§£æ—¶é—´ |
|------|--------------------|------------|------------|
| **CCG** | æœ€é«˜è¾¾ **2.40%** | **æœ€å°‘ï¼ˆ~2.00ï¼‰** | **æœ€å¿«ï¼Œç”šè‡³ä¼˜äº DWC** |
| BDC | æ¥è¿‘ CCG | ç¨å¤š | æ¬¡ä¼˜ |
| IRO / HSL | å¯è¾¾åŒç­‰æˆæœ¬ | æ›´å¤šï¼ˆ~2.4â€“2.8ï¼‰ | è¾ƒæ…¢ |

> âš¡ **å…³é”®å‘ç°**ï¼š
> - **CCG æ˜¯æœ€ä¼˜é€‰æ‹©**ï¼šå…¼å…·é«˜è´¨é‡è§£ä¸æœ€çŸ­æ—¶é—´ã€‚
> - åˆ†è§£ç±»æ–¹æ³•ï¼ˆCCG/BDCï¼‰æ”¶æ•›é€Ÿåº¦å¿«äºè¿­ä»£æ³•ï¼ˆIRO/HSLï¼‰ã€‚
> - HSL è™½æ…¢ï¼Œä½†æä¾›äº†åšå¼ˆæ¼”åŒ–è·¯å¾„ï¼Œæœ‰åŠ©äºè§£é‡Šæ€§åˆ†æã€‚

---

### æ•°å€¼ç¤ºä¾‹ï¼ˆHSL æ”¶æ•›æ¼”ç¤ºï¼‰
åœ¨ä¸€ä¸ªäº”èŠ‚ç‚¹ç½‘ç»œä¸­ï¼š
- åˆå§‹ placement: {2,4} â†’ æˆæœ¬ 17
- ç»è¿‡ 3 è½®å¯¹æŠ—å­¦ä¹ åç¨³å®šäº {2,4,5}
- æœ€ç»ˆæˆæœ¬çº¦ **20.6**ï¼Œä½†ä»æ¯” DWC ä½çº¦ **10%**

> ğŸ¯ è¡¨æ˜ HSL èƒ½æœ‰æ•ˆè¯†åˆ«æ•æ„Ÿè¾¹å¹¶å¼•å¯¼éƒ¨ç½²è°ƒæ•´ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **åŠ¨æ€ä¸ç¡®å®šæ€§å»ºæ¨¡è‡³å…³é‡è¦**  
   å¼•å…¥ **dynamic budgeted uncertainty** æ˜¾è‘—æå‡äº†æ¨¡å‹çš„çœŸå®æ€§ï¼Œå¹¶å¸¦æ¥äº† **æœ€é«˜è¾¾ 30% çš„æˆæœ¬èŠ‚çº¦**ï¼Œå°¤å…¶æ˜¯åœ¨ $I_e$ è¾ƒå°æ—¶ä¼˜åŠ¿æ˜æ˜¾ã€‚

2. **é™æ€é²æ£’æ€§å¢ç›Šæœ‰é™**  
   RSB ç›¸æ¯” DWC æå‡å¾®å¼±ï¼ˆé€šå¸¸ <1.5%ï¼‰ï¼Œè¡¨æ˜ä»…å¼•å…¥é™æ€ä¸ç¡®å®šæ€§ä¸è¶³ä»¥é‡Šæ”¾é²æ£’ä¼˜åŒ–æ½œåŠ›ã€‚

3. **CCG æ˜¯æœ€ä½³å®ç”¨ç®—æ³•**  
   åœ¨å¤§è§„æ¨¡ç½‘ç»œä¸­ï¼Œ**CCG** ä¸ä»…æ±‚è§£é€Ÿåº¦æœ€å¿«ï¼Œä¸”è¿­ä»£æ¬¡æ•°æœ€å°‘ï¼Œæ˜¯å·¥ç¨‹éƒ¨ç½²é¦–é€‰ã€‚

4. **HSL æä¾›å¯è§£é‡Šçš„å¯¹æŠ—æœºåˆ¶**  
   å­¦ä¹ å‹ hide-and-seek æ¸¸æˆä¸ä»…èƒ½é€¼è¿‘æœ€ä¼˜è§£ï¼Œè¿˜èƒ½æ­ç¤ºç½‘ç»œä¸­æœ€æ˜“å—æ”»å‡»çš„è¾¹å’ŒèŠ‚ç‚¹ï¼Œå…·æœ‰è¯Šæ–­ä»·å€¼ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
| å±€é™ | è¯´æ˜ |
|------|------|
| **è®¡ç®—å¼€é”€ä»è¾ƒé«˜ï¼ˆRDBï¼‰** | å°½ç®¡ CCG åŠ é€Ÿæ˜æ˜¾ï¼Œä½†æ•´ä½“æ¡†æ¶ä»æ¶‰åŠå¤šæ¬¡ MIP æ±‚è§£ï¼Œä¸é€‚åˆå®æ—¶åº”ç”¨ |
| **å‡è®¾ç¦»æ•£æ—¶é—´å‘¨æœŸ** | åŠ¨æ€åå·®æŒ‰ç¦»æ•£æ—¶æ®µå»ºæ¨¡ï¼Œæœªè€ƒè™‘è¿ç»­æ—¶é—´æ¼‚ç§» |
| **æœªè€ƒè™‘ regenerator æ•…éšœå®¹é”™** | å½“å‰æ¨¡å‹èšç„¦æˆæœ¬ä¸è¿é€šæ€§ï¼Œæœªæ•´åˆå†—ä½™å¤‡ä»½æœºåˆ¶ |
| **HSL ç¼ºä¹ç†è®ºæ”¶æ•›ä¿è¯** | åŸºäºæ¢¯åº¦çš„å­¦ä¹ è§„åˆ™åœ¨éå‡¸ç¯å¢ƒä¸­å¯èƒ½é™·å…¥å±€éƒ¨å‡è¡¡ |

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **é›†æˆå¯å‘å¼æˆ–å…ƒå¯å‘å¼ç®—æ³•**  
   å¼€å‘åŸºäº GAã€SA æˆ– RL çš„å¿«é€Ÿè¿‘ä¼¼ç®—æ³•ï¼Œè¿›ä¸€æ­¥æå‡å¯æ‰©å±•æ€§ã€‚

2. **æ‰©å±•ä¸ç¡®å®šæ€§è¡¨ç¤ºå½¢å¼**  
   æ¢ç´¢ **dynamic interval uncertainty** æˆ– **distributionally robust optimization (DRO)** æ¡†æ¶ã€‚

3. **å¼•å…¥ survivability çº¦æŸ**  
   è€ƒè™‘è¾¹/èŠ‚ç‚¹å¤±æ•ˆåœºæ™¯ä¸‹çš„ **k-connectivity** æˆ– **fault-tolerant RLP** æ‰©å±•ã€‚

4. **è”åˆä¼˜åŒ–å›¾å˜æ¢ä¸ regenerator placement**  
   å½“å‰ä¸¤é˜¶æ®µåˆ†ç¦»å¤„ç†ï¼Œæœªæ¥å¯å°è¯•ç«¯åˆ°ç«¯è”åˆå»ºæ¨¡ã€‚

5. **åº”ç”¨äºçœŸå®ç½‘ç»œæ‹“æ‰‘**  
   åœ¨å®é™…å…‰ç½‘ç»œæˆ– LEO satellite network ä¸ŠéªŒè¯æ¨¡å‹æœ‰æ•ˆæ€§ã€‚

---

> âœ… **æ€»ä½“è¯„ä»·**ï¼šæœ¬æ–‡ç³»ç»Ÿåœ°æ¨è¿›äº†é²æ£’ RLP çš„å»ºæ¨¡ä¸æ±‚è§£è¾¹ç•Œï¼Œæå‡ºçš„ **dynamic budgeted uncertainty + CCG/HSL æ¡†æ¶** ä¸º resilient network design æä¾›äº†ä¸€ä¸ªå¼ºå¤§è€Œçµæ´»çš„æ–°èŒƒå¼ã€‚

</details>

---

### 12. [Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning](https://arxiv.org/abs/2602.11455)

**Authors**: Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao, Hu Wei, Linfeng Zhang  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.11455v1  

#### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attent...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**Credit Where Itâ€™s Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ

å½“å‰çš„ **Reinforcement Learning with Verifiable Rewards (RLVR)** åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¨ç†ä»»åŠ¡ä¸­æ™®éå­˜åœ¨â€œ**uniform credit assignment**â€é—®é¢˜â€”â€”å³æ— è®ºç”Ÿæˆçš„ token æ˜¯å¦çœŸæ­£ä¾èµ–è§†è§‰è¾“å…¥ï¼Œæ‰€æœ‰ token éƒ½è¢«å¹³ç­‰åœ°èµ‹äºˆå¼ºåŒ–ä¿¡å·ã€‚è¿™å¯¼è‡´ï¼š

- è§†è§‰è¯æ®æœªè¢«æœ‰æ•ˆåˆ©ç”¨ï¼›
- æ¨ç†è¿‡ç¨‹ä¸­çš„â€œæ„ŸçŸ¥é”šç‚¹â€ï¼ˆperceptual anchorsï¼‰æ— æ³•è¢«ç²¾å‡†è¯†åˆ«å’Œä¼˜åŒ–ï¼›
- å¤§é‡è¯­è¨€ç»“æ„å‹ token è¢«é”™è¯¯åœ°åˆ†é…ä¿¡ç”¨ï¼Œç¨€é‡Šäº†å¯¹å…³é”®è§†è§‰-æ–‡æœ¬è€¦åˆéƒ¨åˆ†çš„å­¦ä¹ ã€‚

è¯¥é—®é¢˜é™åˆ¶äº† MLLM åœ¨æ•°å­¦ã€å›¾è¡¨ç†è§£ç­‰éœ€è¦å¼ºè§†è§‰æ¥åœ°ï¼ˆvisual groundingï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

---

### ğŸš€ æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯ï¼Ÿ

ä½œè€…æå‡º **Anchor-Token Reinforcement Learning (AT-RL)**ï¼Œä¸€ç§è½»é‡çº§ã€å³æ’å³ç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ†æè·¨æ¨¡æ€æ³¨æ„åŠ›æ‹“æ‰‘ç»“æ„ï¼Œå®ç°ç²¾ç»†åŒ–çš„ä¿¡ç”¨åˆ†é…ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- å‘ç° MLLM çš„ Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä»…æœ‰çº¦ **15% çš„ token è¡¨ç°å‡ºé«˜ cross-modal connectivity**ï¼ˆå³ä¸å›¾åƒ patch å¼ºå…³è”ï¼‰ï¼Œè¿™äº› token æ˜¯â€œ**æ„ŸçŸ¥é”šç‚¹**â€ï¼ˆPerceptual Anchorsï¼‰ï¼Œè´Ÿè´£å°†æ¨ç†é”šå®šåœ¨è§†è§‰è¯æ®ä¸Šã€‚
- å¤šæ•° token ä»…æ‰¿æ‹…è¯­è¨€è¿è´¯æ€§å’Œé€»è¾‘è¿‡æ¸¡åŠŸèƒ½ï¼Œå±äºâ€œä½è¿æ¥æ€§ tokenâ€ã€‚

#### AT-RL ä¸‰é˜¶æ®µæµç¨‹ï¼š
1. **å®šä½æ„ŸçŸ¥é”šç‚¹ï¼ˆLocating Anchorsï¼‰**  
   - æå–å¹¶æ ¡æ­£è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡ï¼ˆdebiasingï¼‰ï¼Œè®¡ç®—æ¯ä¸ª token çš„ **Connectivity Density** $ C_i = \sum_{j \in V} A[i,j] $
   - é«˜ $ C_i $ çš„ token è¢«è¯†åˆ«ä¸ºæ½œåœ¨é”šç‚¹ã€‚

2. **å›¾èšç±»åˆ†ç»„ï¼ˆToken Grouping via Graph Partitioningï¼‰**  
   - æ„å»º token çº§åŠŸèƒ½ä¾èµ–å›¾ï¼ˆåŸºäºæ³¨æ„åŠ›è¶³è¿¹çš„ cosine similarityï¼‰
   - ä½¿ç”¨ METIS ç®—æ³•è¿›è¡Œå›¾åˆ†å‰²ï¼Œå½¢æˆè¯­ä¹‰ä¸€è‡´çš„ cluster
   - å¼•å…¥å»å™ªå’Œæ‹“æ‰‘ä¼ æ’­æœºåˆ¶å¢å¼ºé²æ£’æ€§

3. **ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…ï¼ˆReinforced Credit Assignmentï¼‰**  
   - æ¯ä¸ª cluster çš„é‡è¦æ€§ç”±å…¶å†…éƒ¨æ€» connectivity density å æ¯”å†³å®šï¼š  
     $$
     W(C_k) = \frac{\sum_{t_i \in C_k} C_i}{\sum_j C_j}
     $$
   - å°†åŸå§‹åºåˆ—çº§ advantage $ A^{(2)} $ æŒ‰ cluster æƒé‡è½¯åŠ æƒä¸º token çº§ä¼˜åŠ¿ä¿¡å·ï¼š  
     $$
     A^{(i,2)} = W(C_k) \cdot A^{(2)}, \quad \forall t_i \in C_k
     $$

æœ€ç»ˆç›®æ ‡å‡½æ•°ä¿æŒä¸ GRPO/PPO ä¸€è‡´å½¢å¼ï¼Œä»…æ›¿æ¢ advantage é¡¹ï¼Œå› æ­¤å¯æ— ç¼é›†æˆåˆ°ç°æœ‰ RLVR æ¡†æ¶ä¸­ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç‰¹æ€§ | ä¼ ç»Ÿ RLVRï¼ˆå¦‚ GRPO, DAPOï¼‰ | AT-RL |
|------|----------------------------|-------|
| Credit Assignment | Uniform across all tokens | Selective on high-connectivity anchors |
| è§†è§‰æ¥åœ°èƒ½åŠ› | å¼±ï¼Œæ˜“å—è¯­è¨€å…ˆéªŒå¹²æ‰° | å¼ºï¼Œèšç„¦äºè§†è§‰ç›¸å…³ token |
| å¯æ‰©å±•æ€§ | é€šç”¨ä½†æ•ˆç‡ä½ | Plug-and-playï¼Œå¼€é”€ä»… **1.2%** |
| æ³›åŒ–æ€§ | ä¾èµ–ä»»åŠ¡ç‰¹å®šè®¾è®¡ | å•ä¸€ç»„è¶…å‚æ•°é€‚ç”¨äºå¤šç§ä»»åŠ¡ï¼ˆSTEMã€VQAã€Videoï¼‰ |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šAT-RL ä¸æ”¹å˜æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒæµç¨‹ï¼Œä»…é€šè¿‡å¯¹ attention topology çš„åˆ†æå®ç°æ›´ç²¾ç¡®çš„æ¢¯åº¦å¼•å¯¼ï¼Œåœ¨ä¸å¢åŠ å¤æ‚æ€§çš„å‰æä¸‹æ˜¾è‘—æå‡æ¨ç†è´¨é‡ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†

| ç±»åˆ« | æ•°æ®é›† | æè¿° |
|------|--------|------|
| ä¸»è®­ç»ƒé›† | **ViRL-39K** | åŒ…å« 39K æ¡ç”¨äºè§†è§‰æ¨ç†çš„ RL è®­ç»ƒæ ·æœ¬ |
| è·¨åŸŸæ³›åŒ– | **Geometry-3K**, **GeoQA-8K** | å‡ ä½•æ¨ç†ä¸“ç”¨æ•°æ®é›†ï¼Œæµ‹è¯•è¿ç§»èƒ½åŠ› |
| è§†é¢‘ç†è§£ | **Video-R1**ï¼ˆå« VSI-Bench, VideoMMMU, MMVUï¼‰ | å¤šå¸§è§†é¢‘æ¨ç†åŸºå‡† |

---

### âš™ï¸ å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

- **æ¨¡å‹ç³»åˆ—**ï¼šQwen2.5-VL-3B / 7B / 32B / 72B-Instruct
- **è¯„ä¼°åè®®**ï¼š
  - ä½¿ç”¨ `LLMs-Eval` ç»Ÿä¸€è¯„æµ‹å¥—ä»¶
  - æµ‹è¯•é›†ï¼šå„ benchmark çš„ `testmini` split
  - æŒ‡æ ‡ï¼š**Acc@1**ï¼Œæ¸©åº¦è®¾ä¸º 0.1
  - è§†é¢‘è¾“å…¥å¸§æ•°ï¼š16 / 32 / 64 å¸§
- **ç­”æ¡ˆæå–ç­–ç•¥**ï¼š
  - ä¼˜å…ˆä» `\boxed{}` ä¸­æå–
  - å¦åˆ™ä½¿ç”¨å…¨æ–‡æœç´¢åŒ¹é…é€‰é¡¹æˆ–æ•°å€¼
- **éªŒè¯æ¨¡å—**ï¼šä½¿ç”¨ Qwen2.5-72B-Instruct ä½œä¸ºè£åˆ¤æ¨¡å‹åˆ¤æ–­æ­£ç¡®æ€§ï¼ˆbinary è¾“å‡ºï¼‰

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

| åŸºçº¿ç±»åˆ« | æ–¹æ³• | ç®€ä»‹ |
|---------|------|------|
| Uniform Credit | GRPO, DAPO, GSPO, SAPO | å½“å‰ä¸»æµ RLVR æ–¹æ³•ï¼Œå…¨å±€å¹¿æ’­ credit |
| Fine-grained Credit | StepGRPO, FT-RL, VPPO | å°è¯•å¼•å…¥ step/token-level ä¿¡ç”¨åˆ†é… |
| Others | SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ | ä½œä¸º zero-shot æ€§èƒ½å¯¹ç…§ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 3ï¼‰

| Benchmark | Qwen2.5-VL-32B (Ours) | Qwen2.5-VL-72B (Baseline) | æå‡æƒ…å†µ |
|----------|------------------------|----------------------------|---------|
| **MathVista** | **80.2** | 77.8 | **â†‘2.4 pts** |
| **MathVerse** | **56.6** | 57.2 | æ¥è¿‘æŒå¹³ï¼ˆå°é™ï¼‰ |
| **MathVision** | **44.2** | 43.1 | â†‘1.1 pts |
| **MMMU-Pro** | **51.9** | 51.6 | â†‘0.3 pts |
| **MEGA** | **53.0** | 49.6 | â†‘3.4 pts |
| **ChartQAPro** | **48.4** | 47.2 | â†‘1.2 pts |

> ğŸ’¡ **äº®ç‚¹**ï¼š**32B æ¨¡å‹è¶…è¶Š 72B æ¨¡å‹çš„æ•´ä½“è¡¨ç°**ï¼Œå°¤å…¶åœ¨ MathVista ä¸Šå–å¾— SOTA æˆç»©ã€‚

---

### ğŸ” ä¸å…¶ä»– RL æ–¹æ³•å¯¹æ¯”ï¼ˆTable 1 & 2ï¼‰

ä»¥ **Qwen2.5-VL-7B** ä¸ºä¾‹ï¼Œåœ¨å¹³å‡æ€§èƒ½ä¸Šçš„æå‡ï¼š

| æ–¹æ³• | Average Score | ç›¸å¯¹ Zero-shot æå‡ |
|------|---------------|--------------------|
| GRPO | 47.31 | +2.30 |
| DAPO | 48.04 | +3.03 |
| SAPO | 49.94 | +4.93 |
| **SAPO + AT-RL** | **53.25** | **+8.24** |

> âœ… æœ€ä½³ç»„åˆ **SAPO + AT-RL** è¾¾åˆ° **+8.24 ä¸ªç™¾åˆ†ç‚¹** çš„ç»å¯¹å¢ç›Šã€‚

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰è½¯åŠ æƒ vs ç¡¬æˆªæ–­ï¼ˆTable 5ï¼‰

| æ–¹æ³• | MathVerse Acc |
|------|----------------|
| Zero-shot | 39.62 |
| GRPO (uniform) | 41.48 |
| Hard truncation (top 15%) | 42.58 |
| **AT-RL (soft weighting)** | **45.27** |

> â— ç»“è®ºï¼š**è½¯åŠ æƒè¿œä¼˜äºç¡¬æ©ç **ï¼Œè¯´æ˜ä¿ç•™éé”šç‚¹ token çš„å¼±æ¢¯åº¦å¯¹ç»´æŒè¯­è¨€è¿è´¯æ€§è‡³å…³é‡è¦ã€‚

#### ï¼ˆ2ï¼‰åç½®æ ¡æ­£å¿…è¦æ€§ï¼ˆTable 6ï¼‰

| æ–¹æ³• | MathVerse Acc |
|------|----------------|
| AT-RL w/o Bias Correction | 34.20 |
| **AT-RL w/ Bias Correction** | **37.50** |

> âœ… å»é™¤ attention sinks å’Œä½ç½®åå·®åæ€§èƒ½æ˜æ˜¾æå‡ã€‚

#### ï¼ˆ3ï¼‰ç»„ä»¶æœ‰æ•ˆæ€§ï¼ˆTable 7ï¼‰

| é…ç½® | MathVista Î” |
|------|-------------|
| w/o Grouping | -2.30 |
| w/o Cluster Refinement | -0.70 |
| w/o Neighborhood Expansion | -0.65 |
| **Full AT-RL** | **0.00 (best)** |

> âœ… æ‰€æœ‰æ¨¡å—å‡è´¡çŒ®æ­£å‘æ”¶ç›Šï¼Œå°¤å…¶æ˜¯è¯­ä¹‰åˆ†ç»„å’Œæ‹“æ‰‘ä¼ æ’­ã€‚

---

### â±ï¸ è®¡ç®—æ•ˆç‡åˆ†æ

- **AT-RL æ¨¡å—é¢å¤–å¼€é”€ä»…ä¸º 1.2%**ï¼ˆforward/backward å  98.8%ï¼‰
- æ”¯æŒå¤§è§„æ¨¡è®­ç»ƒï¼Œæ— æ˜¾è‘—ç“¶é¢ˆ
- æ”¯æŒ batch-size=512, group-size=8 çš„é«˜æ•ˆå¹¶è¡Œ

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **æ„ŸçŸ¥é”šç‚¹çš„å­˜åœ¨æ˜¯æ™®éç°è±¡**ï¼š
   - åœ¨ CoT æ¨ç†ä¸­ï¼Œä»…çº¦ **15% çš„ token å±•ç°å‡ºé«˜ cross-modal connectivity**
   - è¿™äº› token å¤šä¸ºåŠ¨è¯å¯¼å‘å‹è¯æ±‡ï¼ˆå¦‚ â€œcalculateâ€, â€œgivenâ€, â€œassumeâ€ï¼‰ï¼Œç›´æ¥å¼•ç”¨å›¾åƒå†…å®¹
   - å…¶ä½™ token å¤šä¸ºæŠ½è±¡æœ¯è¯­æˆ–è¿æ¥è¯ï¼ˆå¦‚ â€œthereforeâ€, â€œrelationâ€ï¼‰ï¼ŒæœåŠ¡äºè¯­è¨€æµç•…æ€§

2. **æœ‰æ•ˆçš„ RLVR å¿…é¡»åŒºåˆ† token çš„è§’è‰²**ï¼š
   - ç»Ÿä¸€ credit assignment æ˜¯æ¬¡ä¼˜çš„
   - æ­£ç¡®çš„ç­”æ¡ˆå¥–åŠ±åº”é›†ä¸­åœ¨æ„ŸçŸ¥é”šç‚¹ä¸Šï¼›é”™è¯¯æ—¶ä¹Ÿåº”æƒ©ç½šè¿™äº›é”šç‚¹ï¼ˆå› å…¶æœ€å¯èƒ½è¯¯è§£å›¾åƒï¼‰

3. **AT-RL å®ç°äº†ç²¾å‡†çš„ credit assignment**ï¼š
   - é€šè¿‡ graph-based clustering æå‡ä¿¡å·ç¨³å®šæ€§
   - è½¯åŠ æƒæœºåˆ¶å…¼é¡¾ grounding ä¸ coherence
   - åœ¨å¤šä¸ªå°ºåº¦å’Œä»»åŠ¡ä¸Šå®ç°ä¸€è‡´å¢ç›Š

4. **RL ä¸ SFT/CPT æ˜¯äº’è¡¥èŒƒå¼**ï¼š
   - RLï¼ˆå¦‚ AT-RLï¼‰æ“…é•¿ä¼˜åŒ–â€œå¦‚ä½•æ­£ç¡®æ¥åœ°â€
   - SFT/CPT æ“…é•¿æ³¨å…¥â€œçŸ¥é“ä»€ä¹ˆâ€ï¼ˆdomain knowledgeï¼‰
   - äºŒè€…ç»“åˆæ‰èƒ½åŒæ—¶è§£å†³ **Knowledge Deployment Error (KDE)** å’Œ **Visual Perception Error (VPE)**

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

1. **ä¾èµ–é«˜è´¨é‡ attention è¾“å‡º**ï¼š
   - è‹¥ base model çš„ attention å·²ä¸¥é‡åç§»ï¼ˆå¦‚ sink at start tokenï¼‰ï¼Œéœ€ä¾èµ– debiasing æ¨¡å—
   - å¯¹ attention å¯è§£é‡Šæ€§æœ‰ä¸€å®šå‡è®¾

2. **ä¸èƒ½å¼¥è¡¥çŸ¥è¯†ç¼ºå¤±**ï¼š
   - å¦‚ Discussion 4 æ‰€ç¤ºï¼ŒAT-RL æ˜¾è‘—é™ä½ VPEï¼Œä½†å¯¹ KDE å½±å“æœ‰é™
   - è‹¥æ¨¡å‹ç¼ºä¹é¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚å‡ ä½•å…¬å¼ï¼‰ï¼Œä»ä¼šå¤±è´¥

3. **è¶…å‚æ•°æ•æ„Ÿæ€§è™½ä½ä½†ä»å­˜åœ¨**ï¼š
   - å¦‚ Tcen, Î±, R ç­‰éœ€è°ƒä¼˜ï¼Œå°½ç®¡é»˜è®¤å€¼å·²è¶³å¤Ÿç¨³å®š

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **è”åˆè®­ç»ƒ SFT + AT-RL pipeline**  
   â†’ åŒæ—¶æ³¨å…¥çŸ¥è¯†å¹¶å¼ºåŒ–è§†è§‰æ¥åœ°

2. **åŠ¨æ€è°ƒæ•´ anchor threshold**  
   â†’ æ ¹æ®ä»»åŠ¡éš¾åº¦è‡ªé€‚åº”é€‰æ‹©é”šç‚¹æ¯”ä¾‹

3. **æ‰©å±•è‡³å…¶ä»–æ¨¡æ€**  
   â†’ å¦‚éŸ³é¢‘-æ–‡æœ¬ã€3D åœºæ™¯-æŒ‡ä»¤ç­‰è·¨æ¨¡æ€åœºæ™¯

4. **æ¢ç´¢åäº‹å® anchor åˆ†æ**  
   â†’ è‹¥æŸä¸ª anchor è¢«ä¿®æ”¹ï¼Œæ˜¯å¦ä¼šå¯¼è‡´æ¨ç†è·¯å¾„å´©æºƒï¼Ÿå¯ç”¨äºå¯è§£é‡Šæ€§ç ”ç©¶

---

## âœ… æ€»ç»“ä¸€å¥è¯

> **Reasoning quality is governed not by token quantity, but by the fidelity of cross-modal anchoring.**  
> â€”â€” AT-RL é€šè¿‡è¯†åˆ«å¹¶å¼ºåŒ–é‚£ **15% çš„å…³é”®æ„ŸçŸ¥é”šç‚¹ token**ï¼Œå®ç°äº†è½»é‡ã€é«˜æ•ˆä¸”å¯æ‰©å±•çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–°èŒƒå¼ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè®©å°æ¨¡å‹è¶…è¶Šæ›´å¤§æ¨¡å‹çš„è¡¨ç°ã€‚

</details>

---

### 13. [MAPLE: Modality-Aware Post-training and Learning Ecosystem](https://arxiv.org/abs/2602.11596)

**Authors**: Nikhil Verma, Minjung Kim, JooYoung Yoo, Kyung-Min Jin, Manasa Bharadwaj, Kevin Ferreira, Ko Keun Kim, Youngjoon Kim  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.11596v1  

#### Abstract
Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows con...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMAPLE: Modality-Aware Post-training and Learning Ecosystem

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜

å½“å‰çš„ **Multimodal Language Models (MLMs)** åœ¨è¿›è¡Œ **Reinforcement Learning (RL) post-training** æ—¶ï¼Œæ™®éé‡‡ç”¨â€œæ¨¡æ€ç›²â€ï¼ˆmodality-blindï¼‰ç­–ç•¥ï¼Œå³ä¸åŒºåˆ†è¾“å…¥ä¿¡å·çš„æ¨¡æ€ç»„åˆï¼ˆå¦‚ä»…è§†é¢‘ã€éŸ³é¢‘+å­—å¹•ç­‰ï¼‰ï¼Œå°†æ‰€æœ‰æ ·æœ¬æ··åˆè®­ç»ƒã€‚è¿™ç§åšæ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

- **æ¢¯åº¦æ–¹å·®å¤§**ï¼šä¸åŒæ¨¡æ€ç»„åˆçš„ä»»åŠ¡éš¾åº¦ã€å¥–åŠ±å°ºåº¦å·®å¼‚æ˜¾è‘—ï¼Œæ··åˆæ‰¹æ¬¡å¯¼è‡´ policy gradient æ–¹å·®å¢å¤§ã€‚
- **æ”¶æ•›æ…¢ä¸”ä¸ç¨³å®š**ï¼šç”±äºå¼‚æ„ä»»åŠ¡å…±ç”¨ä¼˜åŒ–è·¯å¾„ï¼Œç®€å•ä»»åŠ¡ä¸»å¯¼æ›´æ–°ï¼Œéš¾ä»»åŠ¡è¢«å¿½è§†ã€‚
- **ç°å®éƒ¨ç½²é²æ£’æ€§å·®**ï¼šçœŸå®åœºæ™¯å¸¸å‡ºç°æ¨¡æ€ç¼ºå¤±ï¼ˆmissing-at-testï¼‰ã€å†—ä½™æˆ–æƒé‡å˜åŒ–ï¼Œè€Œä¼ ç»Ÿè®­ç»ƒå‡è®¾å…¨æ¨¡æ€å¯ç”¨ï¼Œå¯¼è‡´åˆ†å¸ƒåç§»ä¸‹æ€§èƒ½éª¤é™ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯

ä½œè€…æå‡º **MAPLE** â€”â€”ä¸€ä¸ªå®Œæ•´çš„ **modality-aware** åè®­ç»ƒä¸å­¦ä¹ ç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…å«ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ï¼š

#### ï¼ˆ1ï¼‰MAPLE-benchï¼šé¦–ä¸ªæ˜¾å¼æ ‡æ³¨æœ€å°å¿…è¦æ¨¡æ€çš„åŸºå‡†
- å¯¹æ¯ä¸ªä»»åŠ¡å®ä¾‹æ ‡æ³¨ **Required Modality Tags (RMTs)**ï¼Œæ˜ç¡®è§£å†³è¯¥ä»»åŠ¡æ‰€éœ€çš„æœ€å°æ¨¡æ€å­é›†ï¼ˆå¦‚ `V`, `VA`, `VAS` ç­‰ï¼‰ã€‚
- æ”¯æŒåœ¨ **7 ç§æ¨¡æ€ç»„åˆ**ï¼ˆå•æ¨¡ã€åŒæ¨¡ã€ä¸‰æ¨¡ï¼‰ä¸Šè¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚
- åŒ…å«ä¸¤ä¸ªä»»åŠ¡ï¼š**MAPLE-QA**ï¼ˆé€‰æ‹©é¢˜é—®ç­”ï¼‰å’Œ **MAPLE-Caption**ï¼ˆå¼€æ”¾ç”Ÿæˆå¼æè¿°ï¼‰ã€‚

#### ï¼ˆ2ï¼‰MAPOï¼šModality-Aware Policy Optimization
- åœ¨ **GRPO** æ¡†æ¶åŸºç¡€ä¸Šå¼•å…¥ **æŒ‰ RMT åˆ†å±‚è®­ç»ƒï¼ˆstratified trainingï¼‰**ï¼š
  - æ¯ä¸ª batch åªåŒ…å«ç›¸åŒ RMT çš„æ ·æœ¬ã€‚
  - ä¼˜åŠ¿å‡½æ•°ï¼ˆadvantageï¼‰åœ¨åŒè´¨ç»„å†…å½’ä¸€åŒ–ï¼Œæ¶ˆé™¤è·¨æ¨¡æ€å¥–åŠ±å°ºåº¦å·®å¼‚ã€‚
- ç†è®ºè¯æ˜ï¼š`Var(g_MA) â‰¤ Var(g_MU)`ï¼Œå³æ¨¡æ€æ„ŸçŸ¥è®­ç»ƒèƒ½é™ä½æ¢¯åº¦æ–¹å·®ã€‚

#### ï¼ˆ3ï¼‰è‡ªé€‚åº”è®­ç»ƒç­–ç•¥ï¼ˆAdaptive Trainingï¼‰
- **åŠ¨æ€åŠ æƒï¼ˆadpwï¼‰**ï¼šåŸºäºå†å² KL æ•£åº¦è¡¡é‡ä»»åŠ¡éš¾åº¦ï¼Œä¸ºå›°éš¾ RMT åˆ†é…æ›´é«˜æƒé‡ã€‚
- **è¯¾ç¨‹å­¦ä¹ ï¼ˆcurriculum learningï¼‰**ï¼šæŒ‰å¤æ‚åº¦æ’åºè®­ç»ƒé¡ºåºï¼ˆuni â†’ bi â†’ tri-modalï¼‰ï¼Œå¹¶å¯åŠ¨æ€è°ƒæ•´ï¼ˆadpcurï¼‰ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | MAPLE ä¼˜åŠ¿ |
|------|-----------|
| **æ•ˆç‡** | æ”¶æ•›é€Ÿåº¦æå‡ **3.18Ã—** |
| **é²æ£’æ€§** | åœ¨æ¨¡æ€ç¼ºå¤±/å†—ä½™ä¸‹ä¿æŒç¨³å®šï¼Œå‡†ç¡®ç‡å·®è·ç¼©å° |
| **æ€§èƒ½** | å¹³å‡å‡†ç¡®ç‡æ›´é«˜ï¼Œå°¤å…¶æå‡å›°éš¾æ¨¡æ€è¡¨ç° |
| **æ³›åŒ–èƒ½åŠ›** | èƒ½æ­£ç¡®è¯†åˆ«â€œä¿¡æ¯ä¸è¶³â€æƒ…å†µå¹¶ä¸»åŠ¨ abstain |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†

#### ï¼ˆ1ï¼‰MAPLE-bench
- **MAPLE-QA-train**: 47,893 ä¸ª QA å¯¹ï¼Œæ¥è‡ª 546 ä¸ªè§†é¢‘ã€‚
- **MAPLE-QA-eval**: 5,001 æµ‹è¯•æ ·æœ¬ï¼Œäººå·¥å®¡æ ¸ï¼Œè¦†ç›–å…¨éƒ¨ 7 ç±» RMTã€‚
- **MAPLE-Caption-train**: 5,120 ä¸ªç”Ÿæˆæ ·æœ¬ã€‚
- **MAPLE-Caption-eval**: 5,348 ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ¯ç±» RMT å‡è¡¡ï¼ˆ764 ä¸ªï¼‰ã€‚

#### ï¼ˆ2ï¼‰MAPLE-QA+
- æ‰©å±•ç‰ˆ QA æ•°æ®é›†ï¼Œç”¨äºéªŒè¯æ¨¡å‹æ˜¯å¦çœŸæ­£å…·å¤‡æ¨¡æ€æ„ŸçŸ¥æ¨ç†èƒ½åŠ›ã€‚
- åŒ…å«ä¸‰ç§è®¾å®šï¼š
  - **Modality-Exact**ï¼šæä¾›æ°å¥½æ‰€éœ€æ¨¡æ€ã€‚
  - **Modality-Superset**ï¼šé¢å¤–æä¾›æ— å…³æ¨¡æ€ã€‚
  - **Modality-Deficit**ï¼šç¼ºå°‘å¿…è¦æ¨¡æ€ï¼Œéœ€å›ç­” â€œNoneâ€ã€‚
- æ€»è®¡ 137,313 ä¸ªæ ·æœ¬ï¼ˆè®­ç»ƒ 103,265ï¼Œæµ‹è¯• 34,048ï¼‰ã€‚

### å®éªŒè®¾ç½®

- **æ¨¡å‹**ï¼šQwen2.5-Omni-3Bï¼ˆæ”¯æŒæ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘è¾“å…¥ï¼‰ã€‚
- **è®­ç»ƒæ¡†æ¶**ï¼šveRL åº“ï¼Œä½¿ç”¨ GRPO è¿›è¡Œæ—  critic çš„ RL ä¼˜åŒ–ã€‚
- **è¶…å‚æ•°**ï¼š
  - å­¦ä¹ ç‡ï¼š2Ã—10â»â¶
  - Batch sizeï¼šå…¨å±€ 256ï¼ˆmini-batch 32ï¼‰
  - Rollout æ•°é‡ï¼šG=8
  - åºåˆ—é•¿åº¦ï¼š10,240 tokens
- **ç¡¬ä»¶**ï¼š4Ã— NVIDIA H100-80GB

### è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **Pass@1** | å¤šé€‰é¢˜ä¸­æœ€é«˜å¾—åˆ†æ ·æœ¬æ­£ç¡®çš„æ¯”ä¾‹ï¼ˆMAPLE-QAï¼‰ |
| **LLM-as-Judge Score** | ä½¿ç”¨ GPT-4o å¯¹ç”Ÿæˆæè¿°æ‰“åˆ†ï¼Œç»¼åˆ missing infoã€hallucinationã€fusion quality å¾—åˆ° [0,1] åˆ†æ•°ï¼ˆMAPLE-Captionï¼‰ |
| **Modality Gap (%)** | å•æ¨¡ï¼ˆUniï¼‰ã€åŒæ¨¡ï¼ˆBiï¼‰ã€ä¸‰æ¨¡ï¼ˆTriï¼‰ä¹‹é—´çš„å¹³å‡æ€§èƒ½å·®è·ï¼Œè¶Šå°è¶Šå¥½ |
| **Training Efficiency** | æ¯æ­¥è®­ç»ƒè€—æ—¶ï¼ˆç§’/stepï¼‰ï¼Œå½’ä¸€åŒ–åæ¯”è¾ƒ |
| **Fusion Gain** | å¤šæ¨¡æ€è¾“å‡ºä¼˜äºæœ€ä½³å•æ¨¡æ€è¾“å‡ºçš„æ¯”ä¾‹ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”

- **Zero-shot**ï¼šæœªç»è¿‡ post-training çš„åŸå§‹æ¨¡å‹ã€‚
- **MUPO (Modality-Unaware Post-training)**ï¼šæ ‡å‡†å…¨æ¨¡æ€è®­ç»ƒï¼Œå¿½ç•¥ RMTï¼Œæ··åˆæ‰€æœ‰æ‰¹æ¬¡ã€‚
- **MAPO-base**ï¼šä»…ä½¿ç”¨ stratified batching çš„åŸºç¡€ç‰ˆæœ¬ã€‚
- å¤šç§å˜ä½“å¯¹æ¯”ï¼šloss aggregationã€clippingã€samplingã€curriculum ç­‰è®¾è®¡è½´ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

| æ–¹æ³• | MAPLE-QA Avg Pass@1 (%) | Modality Gap (Avg %) | Time (s/step) |
|------|--------------------------|------------------------|---------------|
| MUPO (baseline) | 58.58 | 2.27 | 523.28 |
| MAPO (base) | 58.68 | 2.11 | 389.11 |
| **Full-recipe (MAPLE)** | **58.72** | **1.74** | **164.72** |

> âœ… **MAPLE æ”¶æ•›é€Ÿåº¦å¿« 3.18Ã—ï¼Œæ¨¡æ€å·®è·ç¼©å° 30.24%**

#### MAPLE-QA æœ€ç»ˆç»“æœï¼ˆTable 2ï¼‰

| Method | V | A | S | VA | VS | AS | VAS | **Avg** |
|--------|----|----|----|-----|-----|-----|-----|--------|
| MUPO | 55.08 | 65.34 | 63.82 | 57.47 | 60.15 | 58.77 | 58.14 | 58.58 |
| MAPO | 55.79 | 65.78 | 61.40 | 57.85 | 60.26 | 59.06 | 57.76 | 58.68 |
| **adpw + adpcur** | **57.13** | **66.40** | **61.35** | **59.16** | **61.64** | **58.33** | **58.67** | **59.82** |

> ğŸ”º æå‡æ˜¾è‘—é›†ä¸­åœ¨æœ€éš¾çš„è§†è§‰ç›¸å…³ä»»åŠ¡ï¼ˆV, VAï¼‰ä¸Šã€‚

#### MAPLE-Caption æœ€ç»ˆç»“æœï¼ˆTable 3ï¼‰

| Method | V | A | S | VA | VS | AS | VAS | **Avg** |
|--------|----|----|----|-----|-----|-----|-----|--------|
| MUPO | 63.78 | 73.69 | 68.20 | 64.46 | 65.69 | 71.71 | 65.18 | 67.67 |
| MAPO | 66.78 | 83.46 | 87.50 | 66.44 | 67.99 | 78.59 | 66.40 | 73.88 |
| **adpw + adpcur** | **66.20** | **81.42** | **84.97** | **66.98** | **69.10** | **81.71** | **67.58** | **74.00** |

> âœ… å¤šæ¨¡æ€èåˆèƒ½åŠ›å¤§å¹…æå‡ï¼š**Fusion Gain è¾¾ 30.24%**ï¼ˆvs. MAPO çš„ 18.19%ï¼‰

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰ç®—æ³•è®¾è®¡è½´æ¶ˆèï¼ˆTable 1ï¼‰

| è®¾è®¡è½´ | æœ€ä¼˜é…ç½® | æå‡æ•ˆæœ |
|-------|---------|--------|
| **Loss Aggregation** | Sample-level > Token-level | æ›´å¥½ä¿ç•™ query-level ä¿¡ç”¨åˆ†é… |
| **Clipping** | Asymmetric (eâº=0.3, eâ»=0.2) | Clip fraction â†“71.65%ï¼Œæ¢ç´¢æ›´å……åˆ† |
| **Sampling** | Early filteringï¼ˆå»é™¤é›¶æ–¹å·®æ ·æœ¬ï¼‰ | è®­ç»ƒé€Ÿåº¦ â†‘1.98Ã— |
| **Curriculum** | Uni â†’ Bi â†’ Tri æŒ‰å¤æ‚åº¦æ’åº | å‡†ç¡®ç‡ â†‘è‡³ 59.05% |

æœ€ç»ˆ **Full-recipe** ç»„åˆå››é¡¹æœ€ä¼˜ç­–ç•¥ï¼Œåœ¨æ‰€æœ‰ç»´åº¦å‡èƒœå‡ºã€‚

#### ï¼ˆ2ï¼‰è‡ªé€‚åº”ç­–ç•¥æ¶ˆèï¼ˆTables 2 & 3ï¼‰

- **+adpw**ï¼ˆåŠ æƒï¼‰ï¼šæå‡å›°éš¾æ ‡ç­¾è¡¨ç°ï¼ˆå¦‚ V, VAï¼‰ã€‚
- **+cur/adpcur**ï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰ï¼šæ§åˆ¶æ›´æ–°æ—¶æœºï¼Œé˜²æ­¢æ˜“ä»»åŠ¡ä¸»å¯¼ã€‚
- **è”åˆä½¿ç”¨ï¼ˆadpw + adpcurï¼‰**ï¼šè¾¾åˆ°æœ€é«˜æ€§èƒ½ï¼Œè¯´æ˜â€œ**ä½•æ—¶æ›´æ–°**â€ä¸â€œ**å¦‚ä½•åŠ æƒ**â€äº’è¡¥ã€‚

#### ï¼ˆ3ï¼‰æ³›åŒ–èƒ½åŠ›éªŒè¯ï¼ˆTable 4ï¼‰

| è®­ç»ƒæ•°æ® | MAPLE-QA+ å¹³å‡å‡†ç¡®ç‡ |
|----------|--------------------|
| MAPLE-QA (exact) | 51.90% |
| **MAPLE-QA+ (augmented)** | **76.99%** |

> ğŸ¯ æ¨¡å‹ä¸ä»…èƒ½å¤„ç† exact æƒ…å†µï¼Œè¿˜èƒ½åœ¨ superset/deficit ä¸‹æ­£ç¡®åˆ¤æ–­æ˜¯å¦å¯ç­”ï¼Œè¯æ˜å…¶å…·å¤‡çœŸæ­£çš„ **modality-aware reasoning** èƒ½åŠ›ã€‚

#### ï¼ˆ4ï¼‰å¯¹æ¯”å­¦ä¹ å¢å¼ºï¼ˆCRWï¼‰

åœ¨ captioning ä¸­å¼•å…¥ **Contrastive Reward Weighting (CRW)**ï¼š
- é¼“åŠ±å®Œæ•´æ¨¡æ€ä¸‹çš„è¾“å‡ºä¸ç¼ºå¤±æ¨¡æ€ä¸‹çš„è¾“å‡ºåœ¨è¯­ä¹‰ç©ºé—´åˆ†ç¦»ã€‚
- ç»“æœï¼š
  - Intra-group dispersion â†‘ï¼ˆå¤šæ ·æ€§æå‡ï¼‰
  - Inter-group separation â†‘ï¼ˆæ¨¡æ€æ¡ä»¶æ›´å¼ºï¼‰
  - t-SNE æ˜¾ç¤ºèšç±»æ›´æ¸…æ™°ï¼ˆå›¾16ï¼‰

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°

1. **æ¨¡æ€å¼‚æ„æ€§æ˜¯ RL post-training çš„å…³é”®æŒ‘æˆ˜**  
   ä¸åŒ RMT çš„ä»»åŠ¡å…·æœ‰ä¸åŒçš„å¥–åŠ±åˆ†å¸ƒã€å™ªå£°æ°´å¹³å’Œå­¦ä¹ åŠ¨æ€ï¼Œç»Ÿä¸€å¤„ç†ä¼šå¼•å…¥ä¸å¿…è¦çš„æ–¹å·®ã€‚

2. **åˆ†å±‚è®­ç»ƒï¼ˆstratified trainingï¼‰æœ‰æ•ˆé™ä½æ¢¯åº¦æ–¹å·®**  
   MAPO é€šè¿‡æŒ‰ RMT åˆ†ç»„å½’ä¸€åŒ–ä¼˜åŠ¿å‡½æ•°ï¼Œæ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ä¸æ•ˆç‡ã€‚

3. **è‡ªé€‚åº”æœºåˆ¶è¿›ä¸€æ­¥ç¼“è§£â€œæ˜“éš¾å¤±è¡¡â€é—®é¢˜**  
   å›°éš¾ä»»åŠ¡æ¢¯åº¦å¼±ï¼Œå®¹æ˜“è¢«æ©ç›–ï¼›é€šè¿‡ **KL-based weighting** å’Œ **dynamic curriculum** å¯ä¸»åŠ¨è¡¥å¿ã€‚

4. **MAPLE å®ç°äº†é«˜æ•ˆã€é²æ£’ã€çœŸå®çš„å¤šæ¨¡æ€èåˆ**  
   - æ”¶æ•›æ›´å¿«ï¼ˆ3.18Ã—ï¼‰
   - æ¨¡æ€å·®è·ç¼©å° 30.24%
   - åœ¨éƒ¨åˆ†è§‚æµ‹ä¸‹ä»ä¿æŒé«˜æ€§èƒ½
   - èƒ½ä¸»åŠ¨ abstain å½“ä¿¡æ¯ä¸è¶³

5. **benchmark è®¾è®¡è‡³å…³é‡è¦**  
   ç°æœ‰ benchmark å¿½ç•¥æ¨¡æ€ä¾èµ–å…³ç³»ï¼ŒMAPLE-bench å¼•å…¥ RMT æ ‡æ³¨ï¼Œä½¿è¯„ä¼°æ›´å…·ç°å®æ„ä¹‰ã€‚

### æ–¹æ³•çš„å±€é™æ€§

- **ä¾èµ–é«˜è´¨é‡ RMT æ ‡æ³¨**ï¼šè‹¥ RMT é”™è¯¯æ ‡æ³¨ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒåå·®ã€‚
- **å½“å‰ä»…é€‚ç”¨äºç¦»æ•£æ¨¡æ€ç»„åˆ**ï¼šéš¾ä»¥å¤„ç†è¿ç»­æ¨¡æ€æƒé‡å˜åŒ–ã€‚
- **è®¡ç®—å¼€é”€ç•¥é«˜**ï¼šè™½ç„¶æ€»æ—¶é—´å‡å°‘ï¼Œä½†éœ€ç»´æŠ¤å¤šä¸ª KL å†å²ä¸è°ƒåº¦é€»è¾‘ã€‚
- **å¯¹ reward model æ•æ„Ÿ**ï¼šå°¤å…¶åœ¨ captioning ä¸­ä¾èµ– LLM-as-Judge çš„ä¸€è‡´æ€§ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘

1. **è‡ªåŠ¨åŒ– RMT æ¨æ–­**ï¼šå¼€å‘æ— éœ€äººå·¥æ ‡æ³¨å³å¯æ¨æ–­æœ€å°å¿…è¦æ¨¡æ€çš„æ–¹æ³•ã€‚
2. **åœ¨çº¿ curriculum learning**ï¼šç»“åˆ uncertainty estimation åŠ¨æ€è°ƒæ•´è®­ç»ƒé¡ºåºã€‚
3. **æ‰©å±•è‡³æ›´å¤šæ¨¡æ€**ï¼šå¦‚è§¦è§‰ã€ä½ç½®ã€ä¼ æ„Ÿå™¨æ•°æ®ç­‰ã€‚
4. **åº”ç”¨äº real-world agents**ï¼šå¦‚æœºå™¨äººã€æ™ºèƒ½åŠ©æ‰‹ï¼Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­è‡ªé€‚åº”æ¨¡æ€ä½¿ç”¨ã€‚
5. **ç»“åˆ test-time adaptation**ï¼šåœ¨æ¨ç†æ—¶æ ¹æ®å¯ç”¨æ¨¡æ€è‡ªåŠ¨åˆ‡æ¢ç­–ç•¥ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> MAPLE æ­ç¤ºäº† **â€œæ¨¡æ€åˆ†å±‚ä¼˜åŒ–â€** æ˜¯å®ç°é«˜æ•ˆã€é²æ£’å¤šæ¨¡æ€ RL çš„å…³é”®ï¼Œæå‡ºäº†ä» benchmark åˆ°ç®—æ³•å†åˆ°è®­ç»ƒç­–ç•¥çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œä¸ºä¸‹ä¸€ä»£ **deployment-ready multimodal systems** å¥ å®šäº†åŸºç¡€ã€‚

</details>

---

### 14. [LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training](https://arxiv.org/abs/2602.11686)

**Authors**: Xinyi Liu, Yujie Wang, Fangcheng Fu, Xuefeng Xiao, Huixia Li, Jiashi Li, Bin Cui  
**Category**: cs.DC  
**Published**: 2026-02-13  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.11686v1  

#### Abstract
Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šLAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
åœ¨ **Mixture-of-Experts (MoE)** æ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œ**expert parallelism (EP)** è™½ç„¶èƒ½æœ‰æ•ˆç¼“è§£å•è®¾å¤‡å†…å­˜å‹åŠ›ï¼Œä½†ç”±äºåŠ¨æ€è·¯ç”±ï¼ˆdynamic routingï¼‰å¯¼è‡´ä¸“å®¶è´Ÿè½½ä¸¥é‡ä¸å‡è¡¡ï¼ˆload imbalanceï¼‰ï¼Œå½¢æˆâ€œé•¿å°¾å»¶è¿Ÿâ€ï¼ˆtail latencyï¼‰ï¼Œä¸¥é‡å½±å“è®­ç»ƒæ•ˆç‡ã€‚

ç°æœ‰ç³»ç»Ÿçº§è§£å†³æ–¹æ¡ˆå¦‚ **expert replication**ï¼ˆå¤åˆ¶é«˜è´Ÿè½½ä¸“å®¶ï¼‰å’Œ **expert relocation**ï¼ˆè¿ç§»ä¸“å®¶ä½ç½®ï¼‰è™½ç„¶èƒ½æ”¹å–„è´Ÿè½½ï¼Œä½†ä¼šå¼•å…¥æ˜¾è‘—çš„ **re-layout å¼€é”€**ï¼ˆé€šä¿¡å’Œå†…å­˜å¼€é”€ï¼‰ï¼Œé™åˆ¶äº†å…¶è°ƒæ•´é¢‘ç‡å’Œçµæ´»æ€§ï¼Œæ— æ³•åŠæ—¶å“åº”è·¯ç”±åˆ†å¸ƒçš„å¿«é€Ÿå˜åŒ–ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯

è®ºæ–‡æå‡º **LAER-MoE**ï¼Œä¸€ä¸ªé«˜æ•ˆçš„ MoE è®­ç»ƒæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯ä¸¤ä¸ªåˆ›æ–°ï¼š

#### ï¼ˆ1ï¼‰**Fully Sharded Expert Parallelism (FSEP)**  
ä¸€ç§å…¨æ–°çš„å¹¶è¡ŒèŒƒå¼ï¼Œç»“åˆäº† **FSDP (Fully Sharded Data Parallelism)** å’Œ **EP** çš„æ€æƒ³ï¼š
- å°†æ¯ä¸ª **expert çš„å‚æ•°å®Œå…¨åˆ†ç‰‡ï¼ˆfully shardï¼‰**ï¼Œæ¯ä¸ªè®¾å¤‡ä»…å­˜å‚¨æ‰€æœ‰ä¸“å®¶çš„ä¸€ä¸ªåˆ†ç‰‡ï¼ˆchunkï¼‰ã€‚
- åœ¨å‰å‘/åå‘ä¼ æ’­æ—¶ï¼Œé€šè¿‡ **All-to-All é€šä¿¡** åŠ¨æ€æ¢å¤æ‰€éœ€ä¸“å®¶çš„å®Œæ•´å‚æ•°ã€‚
- **å…³é”®è®¾è®¡**ï¼šå°† expert re-layout è¿‡ç¨‹ä¸ FSDP çš„ **parameter prefetching** å’Œ **gradient reduce-scattering** èåˆï¼Œä½¿å¾— re-layout çš„é€šä¿¡å¼€é”€å¯ä»¥è¢«è®¡ç®—è¿‡ç¨‹å®Œç¾æ©ç›–ï¼ˆoverlapï¼‰ï¼Œä»è€Œå®ç° **æ¯è½®è¿­ä»£éƒ½å¯çµæ´»é‡å¸ƒå±€**ï¼Œä¸”æ— é¢å¤–æ—¶é—´æˆæœ¬ã€‚

#### ï¼ˆ2ï¼‰**æ™ºèƒ½è´Ÿè½½å‡è¡¡è§„åˆ’å™¨ï¼ˆIntelligent Load Balancing Plannerï¼‰**
- **å¼‚æ­¥ä¸“å®¶å¸ƒå±€è°ƒä¼˜å™¨ï¼ˆasynchronous expert layout tunerï¼‰**ï¼šåŸºäºå†å²è·¯ç”±ä¿¡æ¯ï¼Œåœ¨ CPU ä¸Šä¸ºä¸‹ä¸€è½®è¿­ä»£ç”Ÿæˆæœ€ä¼˜çš„ä¸“å®¶å‰¯æœ¬æ•°é‡å’Œæ”¾ç½®ç­–ç•¥ã€‚
- **åŒæ­¥ token åˆ†å‘å™¨ï¼ˆsynchronous token dispatcherï¼‰**ï¼šåœ¨ GPU ä¸Šå¿«é€Ÿå†³å®šæ¯ä¸ª token åº”è·¯ç”±åˆ°å“ªä¸ªä¸“å®¶å‰¯æœ¬ï¼Œé‡‡ç”¨è½»é‡çº§æ‹“æ‰‘æ„ŸçŸ¥çš„ **lite routing ç®—æ³•**ï¼Œä¼˜å…ˆå‡å°‘è·¨èŠ‚ç‚¹é€šä¿¡ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹é¢ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ FlexMoE, SmartMoEï¼‰ | LAER-MoE |
|------|-------------------------------|----------|
| **re-layout é¢‘ç‡** | ä½é¢‘ï¼ˆæ•°ç™¾è½®ä¸€æ¬¡ï¼‰ | **æ¯è½®è¿­ä»£å‡å¯è°ƒæ•´** |
| **re-layout å¼€é”€** | æ˜¾è‘—ï¼ˆéœ€è¿ç§»å‚æ•°/æ¢¯åº¦ï¼Œå³°å€¼å†…å­˜é«˜ï¼‰ | **é€šä¿¡å¼€é”€è¢«è®¡ç®—æ©ç›–ï¼Œæ— é¢å¤–å»¶è¿Ÿ** |
| **è´Ÿè½½å‡è¡¡èƒ½åŠ›** | å—é™äºè°ƒæ•´ç­–ç•¥å’Œå¼€é”€æƒ©ç½š | **æ›´çµæ´»ã€æ›´åŠæ—¶ã€æ›´ä¼˜çš„å‡è¡¡æ•ˆæœ** |
| **ç³»ç»Ÿå…¼å®¹æ€§** | é€šå¸¸ç‹¬ç«‹äºå¹¶è¡ŒèŒƒå¼ | **ä¸ FSDP æ·±åº¦é›†æˆï¼Œæ”¯æŒ fully sharded model state** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **WikiText-103**ï¼šç»´åŸºç™¾ç§‘æ–‡æœ¬ï¼Œç”¨äºè¯­è¨€å»ºæ¨¡ã€‚
- **C4**ï¼šCommon Crawl æ¸…æ´—åçš„å¤§å‹è¯­æ–™åº“ã€‚

### æ¨¡å‹æ¶æ„
åœ¨ä»¥ä¸‹ä¸‰ç§ MoE æ¨¡å‹ä¸Šè¿›è¡Œå®éªŒï¼š
- **Mixtral-8x7B**
- **Mixtral-8x22B**
- **Qwen-8x7B**

é…ç½®åŒ…æ‹¬ï¼š
- **e8k2**ï¼š8 ä¸ªä¸“å®¶ï¼Œtop-2 è·¯ç”±
- **e16k4**ï¼š16 ä¸ªä¸“å®¶ï¼Œtop-4 è·¯ç”±ï¼ˆä¿æŒæ€»è®¡ç®—é‡ä¸å˜ï¼‰

### ç¡¬ä»¶ç¯å¢ƒ
- **4 èŠ‚ç‚¹é›†ç¾¤**ï¼Œæ¯èŠ‚ç‚¹ 8 å— **NVIDIA A100 80GB GPU**
- èŠ‚ç‚¹å†…é€šè¿‡ **NVLink** äº’è”ï¼ˆ300 GB/sï¼‰
- èŠ‚ç‚¹é—´é€šè¿‡ **Infiniband** äº’è”ï¼ˆ800 Gbpsï¼‰

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Megatron-LM**ï¼šæ”¯æŒå¼‚æ„å¹¶è¡Œçš„ SOTA MoE è®­ç»ƒæ¡†æ¶ã€‚
- **FSDP+EP**ï¼šåŸºäº PyTorch FSDP æ‰©å±•çš„ baselineï¼ŒMoE å±‚åŒæ—¶ä½¿ç”¨ FSDP å’Œ EPã€‚
- **FlexMoE**ï¼šå½“å‰æœ€å…ˆè¿›çš„åŠ¨æ€ä¸“å®¶æ”¾ç½®æ–¹æ³•ï¼ˆä½œè€…å¤ç°å…¶è°ƒåº¦å™¨å¹¶ä¸ FSEP ç»“åˆå¯¹æ¯”ï¼‰ã€‚

### è¯„ä¼°æŒ‡æ ‡
- **ç«¯åˆ°ç«¯è®­ç»ƒé€Ÿåº¦ï¼ˆThroughput, tokens/sï¼‰**
- **åŠ é€Ÿæ¯”ï¼ˆSpeedupï¼‰** vs. Megatron å’Œ FSDP+EP
- **æ”¶æ•›æ›²çº¿ï¼ˆLoss vs. Time/Stepsï¼‰**
- **All-to-All é€šä¿¡å æ¯”**
- **æœ€å¤§è®¾å¤‡ token æ•°ï¼ˆè¡¡é‡è´Ÿè½½å‡è¡¡ç¨‹åº¦ï¼‰**
- **æ¶ˆèå®éªŒ**ï¼šéªŒè¯ FSEP é€šä¿¡ä¼˜åŒ–å’Œè§„åˆ’å™¨çš„æœ‰æ•ˆæ€§

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
- LAER-MoE åœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸‹å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚
- **æœ€é«˜è¾¾åˆ° 1.69Ã— åŠ é€Ÿæ¯”**ï¼ˆvs. Megatronï¼‰ï¼Œå¹³å‡åŠ é€Ÿ **1.50Ã—**ï¼ˆvs. FSDP+EPï¼‰ã€‚
- ä¸ FlexMoE ç›¸æ¯”ï¼Œ**æœ€é«˜æå‡ 1.39Ã—ï¼Œå¹³å‡ 1.20Ã—**ã€‚

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”ç»“æœ
| åœºæ™¯ | LAER-MoE vs. Megatron | LAER-MoE vs. FSDP+EP |
|------|------------------------|------------------------|
| e8k2 æ¨¡å‹ | å¹³å‡ ~1.4â€“1.6Ã— | ç•¥ä¼˜æˆ–æŒå¹³ |
| e16k4 æ¨¡å‹ | æ˜¾è‘—ä¼˜åŠ¿ï¼ˆ~1.6Ã—ï¼‰ | ç•¥é€Šäº Megatronï¼Œä½† LAER-MoE ä»æœ€ä¼˜ |
| å«è¾…åŠ©æŸå¤±ï¼ˆauxiliary lossï¼‰ | ä¿æŒé«˜æ€§èƒ½ï¼Œæ”¶æ•›é€Ÿåº¦æœ€å¿« |

> **åŸå› åˆ†æ**ï¼š
> - FSDP+EP åœ¨ e8k2 ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œå› å…¶ fully sharded ç‰¹æ€§èŠ‚çœå†…å­˜ï¼Œå…è®¸æ›´å¤§ micro-batchã€‚
> - Megatron åœ¨ e16k4 ä¸Šå› å‚æ•°è¾ƒå°‘ï¼Œå¯ä½¿ç”¨æ›´å°çš„ TPï¼Œæ•ˆç‡æ›´é«˜ã€‚
> - ä½†ä¸¤è€…å‡å— **è´Ÿè½½ä¸å‡è¡¡** å½±å“ï¼Œè€Œ LAER-MoE é€šè¿‡åŠ¨æ€ re-layout æ˜¾è‘—ç¼“è§£æ­¤é—®é¢˜ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰
- **ç§»é™¤é€šä¿¡ä¼˜åŒ–**ï¼ˆno comm optï¼‰ï¼šæ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼Œè¯æ˜ **prefetching ä¸ gradient sync çš„ç²¾ç»†è°ƒåº¦è‡³å…³é‡è¦**ã€‚
- **å•ä¸€å‰¯æœ¬åˆ†é…ç­–ç•¥**ï¼ˆonly 'pq' æˆ– 'even'ï¼‰ï¼šæ€§èƒ½ä¸ç¨³å®šï¼Œè¯´æ˜ **å¤šç­–ç•¥ç»„åˆæœç´¢ï¼ˆmultiple replica schemesï¼‰èƒ½æ›´å¥½é€‚åº”ä¸åŒè·¯ç”±æ¨¡å¼**ã€‚
- **è§„åˆ’å™¨æ•ˆç‡**ï¼šlite routing å ç”¨æ—¶é—´ < 0.1%ï¼Œä¸“å®¶å¸ƒå±€æ±‚è§£æ—¶é—´è¿œä½äºå•å±‚è®¡ç®—æ—¶é—´ï¼Œ**ä¸ä¼šæˆä¸ºç“¶é¢ˆ**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **FSEP æˆåŠŸè§£è€¦äº† re-layout å¼€é”€ä¸è®­ç»ƒæ•ˆç‡**ï¼Œå®ç°äº† **zero-overhead çš„åŠ¨æ€ä¸“å®¶é‡å¸ƒå±€**ã€‚
2. **æ¯è½®è¿­ä»£çš„è´Ÿè½½è‡ªé€‚åº”è°ƒæ•´** æ˜¾è‘—ä¼˜äºä½é¢‘è°ƒæ•´æ–¹æ³•ï¼Œåœ¨åŠ¨æ€è·¯ç”±åœºæ™¯ä¸‹æ›´å…·é²æ£’æ€§ã€‚
3. LAER-MoE åœ¨ **ä¿æŒæ¨¡å‹æ”¶æ•›è´¨é‡** çš„å‰æä¸‹ï¼Œå¤§å¹…æå‡è®­ç»ƒååï¼Œ**æ— éœ€ä¾èµ–é«˜æƒé‡çš„è¾…åŠ©æŸå¤±å‡½æ•°** æ¥å¼ºåˆ¶å‡è¡¡ã€‚
4. å…¶ä¼˜åŠ¿åœ¨ **ä¸“å®¶æ•°æ›´å¤šï¼ˆe16k4ï¼‰** çš„æ¨¡å‹ä¸Šæ›´ä¸ºæ˜¾è‘—ï¼Œå› æä¾›äº†æ›´å¤§çš„ä¼˜åŒ–ç©ºé—´ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰å®ç°ä¾èµ–äºå¯¹ FSDP çš„æ·±åº¦å®šåˆ¶ï¼Œé›†æˆå¤æ‚åº¦è¾ƒé«˜ã€‚
- è§„åˆ’å™¨è™½é«˜æ•ˆï¼Œä½†åœ¨è¶…å¤§è§„æ¨¡é›†ç¾¤ï¼ˆå¦‚ >128 GPUsï¼‰ä¸Šçš„æ‰©å±•æ€§ä»éœ€è¿›ä¸€æ­¥éªŒè¯ï¼ˆå°½ç®¡æ¨¡æ‹Ÿæ˜¾ç¤ºç¨³å®šï¼‰ã€‚
- å¯¹æç«¯ç¨€ç–æˆ–é«˜åº¦åæ–œçš„è·¯ç”±æ¨¡å¼æœªåšä¸“é—¨ä¼˜åŒ–ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- è®¾è®¡æ›´é«˜æ•ˆçš„ **planner ç®—æ³•**ï¼Œæ¢ç´¢å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•è¿›è¡Œå…¨å±€ä¼˜åŒ–ã€‚
- æ”¯æŒ **å¼‚æ„ç¡¬ä»¶é›†ç¾¤**ï¼ˆå¦‚æ··åˆ GPU ç±»å‹ï¼‰ä¸‹çš„ä¸“å®¶å¸ƒå±€ã€‚
- å°† FSEP æ€æƒ³æ¨å¹¿è‡³å…¶ä»–ç¨€ç–æ¨¡å‹ç»“æ„ï¼ˆå¦‚ activation sparsityï¼‰ã€‚
- æ¢ç´¢ **è®­ç»ƒ-æ¨ç†ä¸€è‡´æ€§**ï¼Œä½¿æ¨ç†æ—¶ä¹Ÿèƒ½å—ç›Šäºè®­ç»ƒæœŸçš„è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚

---

> **ä»£ç å¼€æº**ï¼š`https://github.com/PKUDAIR/Hetu-Galvatron/tree/laer-moe`

</details>

---

### 15. [CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer](https://arxiv.org/abs/2602.11410)

**Authors**: David Pardoe, Neil Daftary, Miro Furtado, Aditya Aiyer, Yu Wang, Liuqing Li, Tao Song, Lars Hertel, Young Jin Yun, Senthil Radhakrishnan, Zhiwei Wang, Tommy Li, Khai Tran, Ananth Nagarajan, Ali Naqvi, Yue Zhang, Renpeng Fang, Avi Romascanu, Arjun Kulothungun, Deepak Kumar, Praneeth Boda, Fedor Borisyuk, Ruoyan Wang  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.11410v1  

#### Abstract
Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. H...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡é’ˆå¯¹åœ¨å·¥ä¸šçº§å¹¿å‘Šç³»ç»Ÿä¸­åº”ç”¨ **decoder-only Transformer** è¿›è¡Œ **CTR é¢„æµ‹** æ‰€é¢ä¸´çš„å››å¤§æŒ‘æˆ˜ï¼š

1. **Post-scoring context ä¾èµ–é—®é¢˜**ï¼šå¦‚å¹¿å‘Šä½ç½®ï¼ˆpositionï¼‰ç­‰ä¸Šä¸‹æ–‡ä¿¡å·åœ¨æ¨ç†æ—¶ä¸å¯è§ï¼Œä½†åœ¨è®­ç»ƒæ—¶å¯ç”¨ï¼Œå¯¼è‡´è®­ç»ƒ-æœåŠ¡ä¸ä¸€è‡´ï¼ˆtrain-serve skewï¼‰ã€‚
2. **Offline-Online ä¸ä¸€è‡´æ€§**ï¼šç”±äºç³»ç»Ÿå»¶è¿Ÿã€ä¼šè¯å†…äº‹ä»¶ä¸å¯è§‚æµ‹ç­‰é—®é¢˜ï¼Œæ¨¡å‹å¯èƒ½åœ¨è®­ç»ƒä¸­â€œå·çœ‹â€æ¨ç†æ—¶æ— æ³•è·å–çš„ä¿¡æ¯ã€‚
3. **è®­ç»ƒä¸ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§å·®**ï¼šå¤§è§„æ¨¡åºåˆ—å»ºæ¨¡ä¸‹ï¼Œæ ‡å‡† Transformer å®¹æ˜“å‡ºç°æ³¨æ„åŠ›ç—…æ€è¡Œä¸ºï¼ˆattention sinkï¼‰ã€æ¢¯åº¦çˆ†ç‚¸ç­‰é—®é¢˜ã€‚
4. **é«˜è®¡ç®—å¼€é”€**ï¼šé•¿åºåˆ—å¤„ç†å¸¦æ¥ $O(n^2)$ çš„ self-attention å¼€é”€ï¼Œéš¾ä»¥æ»¡è¶³ä½å»¶è¿Ÿåœ¨çº¿æœåŠ¡éœ€æ±‚ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°ç‚¹

CADETï¼ˆ**Context-Conditioned Ads Decoder-Only Transformer**ï¼‰æå‡ºäº†ä¸€ç³»åˆ—æ¶æ„ä¸å·¥ç¨‹å±‚é¢çš„åˆ›æ–°ï¼š

#### ï¼ˆ1ï¼‰**Context-Conditioned Decoding Architecture**
- å¼•å…¥å¤šå¡”é¢„æµ‹å¤´ï¼ˆmulti-tower prediction headsï¼‰ï¼Œæ¯ä¸ª head å¯¹åº”ä¸€ä¸ª post-scoring context bucketï¼ˆå¦‚ä¸åŒ ad position åŒºé—´ï¼‰ã€‚
- åœ¨è®­ç»ƒæ—¶ä»…ä½¿ç”¨çœŸå® context bucket çš„ head å‚ä¸ loss è®¡ç®—ï¼›åœ¨æ¨ç†æ—¶å¹¶è¡Œè¾“å‡ºæ‰€æœ‰ context ä¸‹çš„ CTR é¢„æµ‹ã€‚
- **ä¼˜åŠ¿**ï¼šä¸€æ¬¡æ€§è§£å†³â€œCTR é¢„æµ‹ â†’ æ’åº â†’ ä½ç½®ç¡®å®šâ€çš„é¸¡ç”Ÿè›‹é—®é¢˜ï¼Œæ— éœ€è¿­ä»£é‡æ’åºæˆ–å¤šè½®æ‰“åˆ†ã€‚

#### ï¼ˆ2ï¼‰**Self-Gated Attention Mechanism**
- åœ¨ **representation-level** å’Œ **interaction-level** å¼•å…¥è‡ªé—¨æ§æœºåˆ¶ï¼ˆgating on Q/K è¾“å…¥å‰ï¼‰ã€‚
- æŠ‘åˆ¶å™ªå£°ç»´åº¦ã€è°ƒèŠ‚ query-key ç›¸äº’ä½œç”¨å¼ºåº¦ï¼Œç¼“è§£ attention sink ç°è±¡ã€‚
- **ä¼˜åŠ¿**ï¼šæ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§ï¼Œé¿å… AUC æ³¢åŠ¨ï¼Œå¢å¼ºæ”¶æ•›å¯é æ€§ã€‚

#### ï¼ˆ3ï¼‰**Timestamp-based RoPEï¼ˆRotary Position Embeddingï¼‰**
- å°†åŸå§‹åŸºäºåºåˆ—ä½ç½®çš„ RoPE æ”¹ä¸ºåŸºäº **Unix æ—¶é—´æˆ³** çš„æ—‹è½¬ç¼–ç ã€‚
- æ”¯æŒè·¨æ—¶é—´å°ºåº¦å»ºæ¨¡ï¼ˆä»ç§’åˆ°æœˆï¼‰ï¼Œæ•æ‰ç”¨æˆ·è¡Œä¸ºçš„æ—¶é—´åŠ¨æ€æ€§ã€‚
- **ä¼˜åŠ¿**ï¼šæ›´ç¬¦åˆå¹¿å‘Šåœºæ™¯ä¸­æ—¶é—´é—´éš”è¿œæ¯”é¡ºåºé‡è¦çš„ç‰¹æ€§ã€‚

#### ï¼ˆ4ï¼‰**Session-Aware Masking**
- è®¾è®¡è®­ç»ƒä¸“ç”¨æ©ç ç­–ç•¥ï¼šåªå…è®¸ token æ³¨æ„ $\Delta t > \Delta_{\text{delay}}$ çš„å†å²äº‹ä»¶ï¼ˆä¾‹å¦‚ä¿è¯è‡³å°‘ 1 å°æ—¶å»¶è¿Ÿï¼‰ã€‚
- **ä¼˜åŠ¿**ï¼šé˜²æ­¢æ¨¡å‹ä¾èµ–å®æ—¶æœªæ›´æ–°çš„æ•°æ®ï¼Œæå‡ offline-online ä¸€è‡´æ€§ã€‚

#### ï¼ˆ5ï¼‰**Production Engineering Optimizations**
- **Tensor Packing**ï¼šæ¶ˆé™¤ paddingï¼Œå°†å¤šä¸ªçŸ­åºåˆ—å‹ç¼©æˆè¿ç»­ token æµï¼Œæå‡ batch åˆ©ç”¨ç‡ã€‚
- **Sequence Chunking**ï¼šå°†é•¿ç”¨æˆ·å†å²åˆ‡åˆ†ä¸ºå›ºå®šé•¿åº¦ chunkï¼Œé™ä½ attention æˆæœ¬ã€‚
- **Custom FlashAttention Kernel**ï¼šä¸“ä¸º multi-item scoring åœºæ™¯ä¼˜åŒ–ï¼Œè·³è¿‡æ— æ•ˆè®¡ç®— tileï¼Œå®ç° 3Ã— æ¨ç†åŠ é€Ÿã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ç°æœ‰ä¸»æµæ–¹æ³•ï¼ˆå¦‚ LiRankï¼‰ | CADET |
|------|--------------------------|--------|
| æ¶æ„ | DLRM + Sequential Encoder æ··åˆæ¨¡å‹ | ç«¯åˆ°ç«¯ decoder-only Transformer |
| ç‰¹å¾äº¤äº’ | æ˜¾å¼è®¾è®¡ï¼ˆDCNv2, DINï¼‰ | éšå¼å­¦ä¹ æ—¶åºæ¨¡å¼ |
| ä¸Šä¸‹æ–‡å»ºæ¨¡ | debiasing / counterfactual learning | context-conditioned å¤šå¡” head |
| æ—¶é—´å»ºæ¨¡ | åºåˆ—ä½ç½®æˆ– RNN ç»“æ„ | Timestamp-RoPE |
| å·¥ç¨‹æ•ˆç‡ | å•ç‹¬æ‰“åˆ†å€™é€‰ | å¤šå€™é€‰ä¸€æ¬¡ forward pass |
| ç³»ç»Ÿå¤æ‚åº¦ | å¤šæ¨¡å—é›†æˆ | ç»Ÿä¸€æ¨¡å‹ï¼Œç®€åŒ– pipeline |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿**ï¼š**ç»Ÿä¸€æ¶æ„ + æ›´å¼ºè¡¨è¾¾èƒ½åŠ› + æ›´å¥½å·¥ç¨‹æ•ˆç‡ + æ›´é«˜ä¸šåŠ¡æŒ‡æ ‡**

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†
- æ¥æºï¼š**LinkedIn èµåŠ©å†…å®¹å¹¿å‘Šç³»ç»Ÿï¼ˆHomefeed Sponsored Updatesï¼‰**
- æ•°æ®å½¢å¼ï¼šç”¨æˆ·çº§è¡Œä¸ºåºåˆ—ï¼ˆinterleaved impression-action sequenceï¼‰
- æ—¶é—´èŒƒå›´ï¼š**ä¸€å¹´å†å²æ•°æ®**ï¼Œæœ€åä¸€å¤©ç”¨äºéªŒè¯
- ç‰¹å¾æ•°é‡ï¼šä»ä¼ ç»Ÿ >100 ä¸ªç‰¹å¾ç²¾ç®€è‡³ **12 ä¸ª high-utility signals**

---

### âš™ï¸ å®éªŒè®¾ç½®

| å‚æ•° | è®¾ç½® |
|------|------|
| æ¨¡å‹ç»“æ„ | `nlayers=8`, `nheads=4`, `d_model=352`, `seq_len=4096` |
| è®­ç»ƒæ¡†æ¶ | PyTorch + HSDPï¼ˆHybrid Sharded Data Parallelï¼‰ |
| GPU | NVIDIA H200 |
| ä¼˜åŒ–å™¨ | AdamW |
| Batch Size | ä½¿ç”¨ packing åæœ‰æ•ˆ batch size æå‡çº¦ 4Ã— |
| $\Delta_{\text{delay}}$ | 1 å°æ—¶ï¼ˆç”¨äº session maskingï¼‰ |

---

### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

| ç±»å‹ | æŒ‡æ ‡ |
|------|------|
| Offline | **ROC AUC**, **AAUCï¼ˆç›¸å¯¹ AUC å˜åŒ–ï¼‰**, æ¶ˆèå®éªŒ |
| Online | **A/B Test ä¸­çš„ CTR æå‡**, **Revenue å˜åŒ–** |
| æ•ˆç‡ | è®­ç»ƒæˆæœ¬ï¼ˆTFLOPsï¼‰ã€æ¨ç†å»¶è¿Ÿï¼ˆp99 < 50msï¼‰ã€ååé‡ï¼ˆreq/sec/GPUï¼‰ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”

- **Control Baseline**: **LiRank [3]**  
  - å½“å‰ç”Ÿäº§ç³»ç»Ÿï¼Œhybrid ensemble æ¨¡å‹
  - åŒ…å« **DCNv2**ï¼ˆç”¨äºç‰¹å¾äº¤å‰ï¼‰ + **TransAct**ï¼ˆç”¨äºåºåˆ—å»ºæ¨¡ï¼‰
- **Treatment Group**: æ›¿æ¢ä¸ºå®Œæ•´çš„ **CADET æ¨¡å‹**

> å®éªŒç›®æ ‡ï¼šéªŒè¯å•ä¸€ generative recommender æ˜¯å¦èƒ½è¶…è¶Šé«˜åº¦ä¼˜åŒ–çš„å¤šç»„ä»¶æ··åˆç³»ç»Ÿã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®

#### âœ… **Online A/B Test ç»“æœï¼ˆè¡¨ 3ï¼‰**

| Metric       | Relative Change |
|------------|------------------|
| **CTR**     | **+11.04%** â†‘â†‘â†‘ |
| **Revenue** | +0.14% â†‘         |

> ğŸ’¡ **è¯´æ˜**ï¼šå°½ç®¡æ”¶å…¥æå‡è¾ƒå°ï¼Œä½† **CTR æ˜¾è‘—å¢é•¿ 11.04%** æ˜¯å·¨å¤§çªç ´ï¼Œå°¤å…¶é¢å¯¹å·²é«˜åº¦ä¼˜åŒ–çš„ baselineã€‚

---

#### ğŸ” **æ¶ˆèå®éªŒç»“æœï¼ˆè¡¨ 1ï¼‰**

| Model Variant             | Î” AUC (vs Full CADET) |
|---------------------------|------------------------|
| Full CADET                | â€”                     |
| - Context-Conditioned     | **-0.61%**            |
| - Temporal RoPE           | -0.13%                |
| - Session Masking         | -0.31%                |
| - Pairwise Loss           | -0.03%                |

> ğŸ” **åˆ†æ**ï¼š
> - **Context-conditioned decoding** å½±å“æœ€å¤§ï¼Œè¯´æ˜ post-scoring context å»ºæ¨¡è‡³å…³é‡è¦ã€‚
> - **Session masking** è´¡çŒ®ç¬¬äºŒï¼Œè¯æ˜ train-serve skew æ˜¯å®é™…ç“¶é¢ˆã€‚
> - Pairwise loss è™½å°ä½†ç¨³å®šï¼Œæœ‰åŠ©äºå¯¹æŠ—æ›å…‰åå·®ï¼ˆexposure biasï¼‰ã€‚

---

#### âš–ï¸ **å‡†ç¡®ç‡-æ•ˆç‡æƒè¡¡ï¼ˆè¡¨ 2ï¼‰**

| Sequence Length | Training Cost | Î” AUC     |
|------------------|---------------|-----------|
| 4096 (baseline)  | 1.0Ã—          | â€”         |
| 2048             | **0.46Ã—**     | -0.04%    |
| 1024             | **0.29Ã—**     | -0.29%    |

> âœ… **ç»“è®º**ï¼šé€šè¿‡ **chunking** å¯å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬ï¼ˆâ†“54% @ 2048ï¼‰ï¼Œä¸”ç²¾åº¦æŸå¤±æå°ã€‚

---

#### âš¡ **æ¨ç†ä¼˜åŒ–æ•ˆæœ**

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| Baseline fmha kernel | 34 TFLOPS/s |
| **Custom FlashAttention Kernel** | **>100 TFLOPS/s** |
| Attention latency | 792Î¼s â†’ **262Î¼s** (**3Ã— speedup**) |
| Throughput | **330 req/sec/GPU** |
| Latency (p99) | **< 50ms** âœ… |

> âœ… å®ç°é«˜æ•ˆæ‰¹é‡å€™é€‰æ‰“åˆ†ï¼Œæ”¯æŒçº¿ä¸Šå®æ—¶æ‹å–ç³»ç»Ÿã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **Generative Recommenders å¯èƒœè¿‡ä¼ ç»Ÿ DLRM æ··åˆç³»ç»Ÿ**  
   - å•ä¸€ decoder-only Transformer åœ¨å·¥ä¸šå¹¿å‘Š CTR ä»»åŠ¡ä¸Š **å‡»è´¥ LiRank +11.04% CTR**ï¼Œæ‰“ç ´â€œå¿…é¡» ensembleâ€çš„è®¤çŸ¥ã€‚

2. **Context-conditioned decoding æ˜¯å…³é”®åˆ›æ–°**  
   - æˆåŠŸè§£è€¦ â€œCTR é¢„æµ‹â€ ä¸ â€œæœ€ç»ˆå±•ç¤ºä½ç½®â€ çš„å¾ªç¯ä¾èµ–ï¼Œæä¾› policy-ready è¾“å‡ºã€‚

3. **è®­ç»ƒç¨³å®šæ€§å¯é€šè¿‡ self-gated attention æ˜¾è‘—æ”¹å–„**  
   - representation-level + interaction-level gating æœ‰æ•ˆæŠ‘åˆ¶ pathological attention è¡Œä¸ºã€‚

4. **Timestamp-RoPE æ¯”ä¼ ç»Ÿä½ç½®ç¼–ç æ›´é€‚åˆå¹¿å‘Šåœºæ™¯**  
   - æ—¶é—´å·®æ¯”é¡ºåºæ›´é‡è¦ï¼Œtimestamp-aware ç¼–ç èƒ½æ•æ‰è·¨å°ºåº¦è¡Œä¸ºæ¨¡å¼ã€‚

5. **Production Engineering å†³å®šè½åœ°æˆè´¥**  
   - Packingã€chunkingã€custom kernel ç­‰æŠ€æœ¯ä½¿å¤§æ¨¡å‹å¯åœ¨å·¥ä¸šç¯å¢ƒé«˜æ•ˆè¿è¡Œã€‚

---

### âš ï¸ å±€é™æ€§

1. **å¯¹å†·å¯åŠ¨ç”¨æˆ·æ•æ„Ÿ**ï¼Ÿ  
   - æçŸ­åºåˆ—ç”¨æˆ·å¯èƒ½å—ç›Šæœ‰é™ï¼Œå› æ¨¡å‹ä¾èµ–å†å²è¡Œä¸ºå»ºæ¨¡ã€‚

2. **Context bucket è®¾è®¡éœ€äººå·¥åˆ’åˆ†**  
   - å¦‚ä½•è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜ context åˆ†æ¡¶ä»å¾…ç ”ç©¶ã€‚

3. **Semantic ID Tokenization æ•ˆæœä¸ä½³**  
   - å°è¯•å¼•å…¥è¯­ä¹‰ embeddingï¼ˆRO-VAEï¼‰ä»…å¸¦æ¥ 0.02% AUC æå‡ï¼Œä½†å¢åŠ å·¥ç¨‹å¤æ‚åº¦ã€‚

4. **Revenue æå‡å¾®å¼±**  
   - CTR å¤§å¹…ä¸Šå‡ä½† revenue å‡ ä¹ä¸å˜ï¼Œå¯èƒ½å­˜åœ¨ç«ä»·æœºåˆ¶æˆ–å…¶ä»–å› ç´ å‹åˆ¶è½¬åŒ–ä»·å€¼ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **Dynamic Context Bucketing**  
   - å­¦ä¹ è‡ªé€‚åº” context åˆ†ç»„ï¼Œè€Œéå›ºå®šåŒºé—´ã€‚

2. **End-to-End Bidding Integration**  
   - å°† CTR é¢„æµ‹ä¸ bidding ç­–ç•¥è”åˆå»ºæ¨¡ï¼Œè¿›ä¸€æ­¥é‡Šæ”¾ revenue æ½œåŠ›ã€‚

3. **Long-Term Engagement Modeling**  
   - å¼•å…¥ dwell timeã€conversion ç­‰è¾…åŠ©ä»»åŠ¡ï¼Œè¶…è¶Šç‚¹å‡»è¡Œä¸ºã€‚

4. **æ›´é«˜æ•ˆçš„ç¨€ç– attention è®¾è®¡**  
   - æ¢ç´¢ LLM-inspired sparse attentionï¼ˆå¦‚ StreamingLLMï¼‰ä»¥æ”¯æŒæ— é™ä¸Šä¸‹æ–‡ã€‚

5. **Cross-domain Generalization**  
   - éªŒè¯ CADET æ˜¯å¦é€‚ç”¨äºå…¶ä»–å¹³å°ï¼ˆå¦‚ç”µå•†ã€ç¤¾äº¤ feedï¼‰ã€‚

---

## âœ… æ€»ç»“

CADET æ˜¯é¦–ä¸ªæˆåŠŸå°† **decoder-only Transformer** åº”ç”¨äºå·¥ä¸šçº§å¹¿å‘Š CTR é¢„æµ‹çš„ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚å®ƒä¸ä»…åœ¨ **å‡†ç¡®æ€§ä¸Šå–å¾—æ˜¾è‘—çªç ´ï¼ˆ+11.04% CTRï¼‰**ï¼Œè¿˜é€šè¿‡ä¸€ç³»åˆ— **æ¶æ„åˆ›æ–°ä¸å·¥ç¨‹ä¼˜åŒ–** è§£å†³äº†ç”Ÿæˆå¼æ¨èåœ¨å¹¿å‘Šåœºæ™¯ä¸‹çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚å…¶æˆåŠŸè¡¨æ˜ï¼š**ç»Ÿä¸€çš„ generative paradigm æ­£åœ¨æˆä¸ºä¸‹ä¸€ä»£æ¨èç³»ç»Ÿçš„æœ‰åŠ›å€™é€‰æ–¹æ¡ˆ**ã€‚

</details>

---

### 16. [Improved state mixing in higher-order and block diagonal linear recurrent networks](https://arxiv.org/abs/2602.12021)

**Authors**: Igor Dubinin, Antonio Orvieto, Felix Effenberger  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2602.12021v1  

#### Abstract
Linear recurrent networks (LRNNs) and linear state space models (SSMs) promise computational and memory efficiency on long-sequence modeling tasks, yet their diagonal state transitions limit expressivity. Dense and nonlinear architectures (e.g., LSTMs) on the other hand are provably more expressive,...

---

### 17. [Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty](https://arxiv.org/abs/2602.12113)

**Authors**: Zewei Yu, Lirong Gao, Yuke Zhu, Bo Zheng, Sheng Guo, Haobo Wang, Junbo Zhao  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.12113v1  

#### Abstract
Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high ...

---

### 18. [Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation](https://arxiv.org/abs/2602.12172)

**Authors**: Bowei He, Yankai Chen, Xiaokun Zhang, Linghe Kong, Philip S. Yu, Xue Liu, Chen Ma  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.12172v1  

#### Abstract
Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and trai...

---

### 19. [CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use](https://arxiv.org/abs/2602.12268)

**Authors**: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.12268v1  

#### Abstract
AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behav...

---

### 20. [GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices](https://arxiv.org/abs/2602.11186)

**Authors**: Zhihan Zeng, Kaihe Wang, Zhongpei Zhang, Yue Xiu  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.11186v1  

#### Abstract
The integration of Generative AI (GenAI) into Consumer Electronics (CE)--from AI-powered assistants in wearables to generative planning in autonomous Uncrewed Aerial Vehicles (UAVs)--has revolutionized user experiences. However, these GenAI applications impose immense computational burdens on edge h...

---

### 21. [TUBO: A Tailored ML Framework for Reliable Network Traffic Forecasting](https://arxiv.org/abs/2602.11759)

**Authors**: Zhihang Yuan, Leyang Xue, Waleed Ahsan, Mahesh K. Marina  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.11759v1  

#### Abstract
Traffic forecasting based network operation optimization and management offers enormous promise but also presents significant challenges from traffic forecasting perspective. While deep learning models have proven to be relatively more effective than traditional statistical methods for time series f...

---

### 22. [In-Context Function Learning in Large Language Models](https://arxiv.org/abs/2602.11863)

**Authors**: Elif Akata, Konstantinos Voudouris, Vincent Fortuin, Eric Schulz  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.11863v1  

#### Abstract
Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn ...

---

### 23. [FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client](https://arxiv.org/abs/2602.12014)

**Authors**: Gongxi Zhu, Hanlin Gu, Lixin Fan, Qiang Yang, Yuxing Han  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2602.12014v1  

#### Abstract
One important direction of Federated Foundation Models (FedFMs) is leveraging data from small client models to enhance the performance of a large server-side foundation model. Existing methods based on model level or representation level knowledge transfer either require expensive local training or ...

---

### 24. [AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition](https://arxiv.org/abs/2602.11348)

**Authors**: Ruipeng Wang, Yuxin Chen, Yukai Wang, Chang Wu, Junfeng Fang, Xiaodong Cai, Qi Gu, Hui Su, An Zhang, Xiang Wang, Xunliang Cai, Tat-Seng Chua  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.11348v1  

#### Abstract
Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely ...

---

### 25. [AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.11510)

**Authors**: Faouzi El Yagoubi, Ranwa Al Mallah, Godwin Badu-Marfo  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.11510v1  

#### Abstract
Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, ...

---

### 26. [Charting Empirical Laws for LLM Fine-Tuning in Scientific Multi-Discipline Learning](https://arxiv.org/abs/2602.11215)

**Authors**: Lintao Wang, Zhuqiang Lu, Yilin Zhu, Kun Hu, Zhenfei Yin, Shixiang Tang, Zhiyong Wang, Wanli Ouyang, Xinzhu Ma  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.11215v1  

#### Abstract
While large language models (LLMs) have achieved strong performance through fine-tuning within individual scientific domains, their learning dynamics in multi-disciplinary contexts remains poorly understood, despite the promise of improved generalization and broader applicability through cross-domai...

---

### 27. [WSBD: Freezing-Based Optimizer for Quantum Neural Networks](https://arxiv.org/abs/2602.11383)

**Authors**: Christopher Kverne, Mayur Akewar, Yuqian Huo, Tirthak Patel, Janki Bhimani  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.11383v1  

#### Abstract
The training of Quantum Neural Networks (QNNs) is hindered by the high computational cost of gradient estimation and the barren plateau problem, where optimization landscapes become intractably flat. To address these challenges, we introduce Weighted Stochastic Block Descent (WSBD), a novel optimize...

---

### 28. [Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning](https://arxiv.org/abs/2602.11779)

**Authors**: Haoran Dang, Cuiling Lan, Hai Wan, Xibin Zhao, Yan Lu  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.11779v1  

#### Abstract
Temperature is a crucial hyperparameter in large language models (LLMs), controlling the trade-off between exploration and exploitation during text generation. High temperatures encourage diverse but noisy outputs, while low temperatures produce focused outputs but may cause premature convergence. Y...

---

### 29. [Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards](https://arxiv.org/abs/2602.12049)

**Authors**: Ryo Mikasa, Shun-ichiro Hayashi, Daichi Mukunoki, Tetsuya Hoshino, Takahiro Katagiri  
**Category**: cs.LG  
**Published**: 2026-02-13  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2602.12049v1  

#### Abstract
Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning app...

---

### 30. [Latent Generative Solvers for Generalizable Long-Term Physics Simulation](https://arxiv.org/abs/2602.11229)

**Authors**: Zituo Chen, Haixu Wu, Sili Deng  
**Category**: cs.AI  
**Published**: 2026-02-13  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2602.11229v1  

#### Abstract
We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
