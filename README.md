# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-02-02 06:47:39 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [Autonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection](https://arxiv.org/abs/2601.22949)

**Authors**: Yuan Li, Jun Hu, Bryan Hooi, Bingsheng He, Cheng Chen  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.22949v1  

#### Abstract
Graph-based fraud detection on text-attributed graphs (TAGs) requires jointly modeling rich textual semantics and relational dependencies. However, existing LLM-enhanced GNN approaches are constrained by predefined prompting and decoupled training pipelines, limiting reasoning autonomy and weakening...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Autonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

ç°æœ‰çš„ **LLM-enhanced GNN** æ–¹æ³•åœ¨åŸºäºæ–‡æœ¬å±æ€§å›¾ï¼ˆText-Attributed Graphs, TAGsï¼‰çš„æ¬ºè¯ˆæ£€æµ‹ä¸­é¢ä¸´ä¸¤å¤§ç“¶é¢ˆï¼š

1. **ç¼ºä¹è‡ªä¸»æ¨ç†èƒ½åŠ›ï¼ˆLack of Autonomous Reasoningï¼‰**  
   å½“å‰æ–¹æ³•ä¾èµ–é¢„å®šä¹‰çš„æç¤ºæ¨¡æ¿ï¼ˆpredefined promptingï¼‰æ¥å¼•å¯¼ LLM ç”Ÿæˆè¾…åŠ©ä¿¡æ¯ï¼ˆå¦‚å…³é”®è¯ã€æƒ…æ„Ÿç‰¹å¾ï¼‰ï¼Œè¿™ç§æ¨¡å¼é™åˆ¶äº† LLM è¿›è¡Œå¤šè·³ã€è·¨é‚»åŸŸçš„æ·±å±‚æ¨ç†ï¼Œä»…åœç•™åœ¨æµ…å±‚æ¨¡å¼åŒ¹é…ã€‚

2. **è¯­ä¹‰-ç»“æ„å¯¹é½å¼±åŒ–ï¼ˆWeakened Semantic-Structural Alignmentï¼‰**  
   å¤šæ•°æ–¹æ³•é‡‡ç”¨è§£è€¦è®­ç»ƒèŒƒå¼ï¼ˆdecoupled trainingï¼‰ï¼Œå³å…ˆç”¨ LLM ç¼–ç æ–‡æœ¬ï¼Œå†ç”¨ GNN å­¦ä¹ ç»“æ„ï¼Œå¯¼è‡´è¯­ä¹‰è¡¨ç¤ºæ— æ³•ä¸å›¾ç»“æ„è”åˆä¼˜åŒ–ï¼Œå‰Šå¼±äº†äºŒè€…ä¹‹é—´çš„ååŒæ•ˆåº”ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **FraudCoT** â€”â€”ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œé€šè¿‡**å›¾æ„ŸçŸ¥çš„é“¾å¼æ€ç»´è’¸é¦**ï¼ˆgraph-aware Chain-of-Thought Distillationï¼‰ä¸**é«˜æ•ˆçš„ç«¯åˆ°ç«¯å…±è®­ç»ƒæœºåˆ¶**ï¼Œæå‡æ¬ºè¯ˆæ£€æµ‹æ€§èƒ½ä¸æ•ˆç‡ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š

- âœ… **æ¬ºè¯ˆæ„ŸçŸ¥çš„é€‰æ‹©æ€§ CoT è’¸é¦æœºåˆ¶ï¼ˆFraud-Aware Selective CoT Distillationï¼‰**  
  åˆ©ç”¨æ•™å¸ˆ LLM åœ¨èŠ‚ç‚¹åŠå…¶é‚»å±…ä¸Šä¸‹æ–‡ä¸­è‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼ˆåŒ…æ‹¬æ­£ç¡®ä¸é”™è¯¯è·¯å¾„ï¼‰ï¼Œå¹¶é€šè¿‡æ­£è´Ÿæ ·æœ¬è’¸é¦è®­ç»ƒå­¦ç”Ÿ LLMï¼Œä½¿å…¶å­¦ä¼šæ¨¡ä»¿åˆç†æ¨ç†ã€æŠ‘åˆ¶è¯¯å¯¼æ€§é€»è¾‘ï¼Œä»è€Œå®ç°**è‡ªç”±å½¢å¼çš„å›¾æ„ŸçŸ¥æ¨ç†**ã€‚

- âœ… **é«˜æ•ˆéå¯¹ç§°å…±è®­ç»ƒç­–ç•¥ï¼ˆEfficient Asymmetric Co-trainingï¼‰**  
  åœ¨è®­ç»ƒæ—¶ä»…å¯¹ç›®æ ‡èŠ‚ç‚¹ä½¿ç”¨ LLM ç¼–ç ï¼Œè€Œå¯¹é‚»å±…èŠ‚ç‚¹ä½¿ç”¨ç¼“å­˜çš„åˆå§‹åµŒå…¥ï¼Œå¤§å¹…é™ä½è®¡ç®—å¼€é”€ã€‚è¯¥è®¾è®¡å°† LLM æ¨ç†å¤æ‚åº¦ä»æŒ‡æ•°çº§é™è‡³å¸¸æ•°çº§ï¼Œæ”¯æŒç«¯åˆ°ç«¯ä¼˜åŒ–çš„åŒæ—¶ä¿æŒé«˜ååé‡ã€‚

- âœ… **CoT å¢å¼ºçš„èŠ‚ç‚¹è¡¨ç¤ºé›†æˆ**  
  å°†è’¸é¦å¾—åˆ°çš„ CoT æ¨ç†è·¯å¾„æ‹¼æ¥åˆ°åŸå§‹æ–‡æœ¬ä¸­ï¼Œä½œä¸º GNN çš„è¾“å…¥ï¼Œä¸ºä¸‹æ¸¸æ¨¡å‹æä¾›å¯Œå«å¤šè·³è¯­ä¹‰ä¸ç»“æ„çº¿ç´¢çš„ä¿¡æ¯ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ TAPEã€FLAGï¼‰ | FraudCoT |
|------|--------------------------|--------|
| æ¨ç†æ–¹å¼ | é¢„å®šä¹‰æ¨¡æ¿é©±åŠ¨ | è‡ªä¸»ã€è‡ªç”±å½¢å¼çš„å¤šæ­¥æ¨ç† |
| è®­ç»ƒèŒƒå¼ | è§£è€¦è®­ç»ƒï¼ˆDecoupledï¼‰ | ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼ˆEnd-to-endï¼‰ |
| è¯­ä¹‰-ç»“æ„å¯¹é½ | å¼± | å¼ºï¼ˆé€šè¿‡å…±è®­ç»ƒå®ç°ï¼‰ |
| è®¡ç®—æ•ˆç‡ | é«˜ï¼ˆä½†ç‰ºç‰²æ€§èƒ½ï¼‰ | æé«˜ï¼ˆè¾¾ 1,066Ã— åŠ é€Ÿï¼‰ |
| å¯è§£é‡Šæ€§ | æœ‰é™ | æ¯ä¸ªé¢„æµ‹é™„å¸¦äººç±»å¯è¯»çš„æ¨ç†è·¯å¾„ |

> FraudCoT æˆåŠŸå¼¥åˆäº†â€œè‡ªä¸»æ¨ç†â€ä¸â€œé«˜æ•ˆè®­ç»ƒâ€çš„é¸¿æ²Ÿï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡å®ç°çªç ´ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

| æ•°æ®é›† | ç±»å‹ | èŠ‚ç‚¹æ•° | è¾¹æ•° | åœºæ™¯è¯´æ˜ |
|-------|------|--------|------|---------|
| **InstantVideo** | å…¬å…±åŸºå‡† | 37,126 | 988,340 | Amazon è§†é¢‘ç±»å•†å“è¯„è®ºï¼Œåˆ¤æ–­æ˜¯å¦â€œæœ‰å¸®åŠ©â€ï¼ˆä»£ç†æ¬ºè¯ˆä¿¡å·ï¼‰ |
| **DigitalMusic** | å…¬å…±åŸºå‡† | 64,706 | 7,732,420 | Amazon æ•°ç éŸ³ä¹è¯„è®ºï¼ŒåŒä¸Šä»»åŠ¡ |
| **PromotionAbuse** | å·¥ä¸šç•Œä¸“æœ‰ | 371,464 | 1,388,598 | å­—èŠ‚è·³åŠ¨çœŸå®ä¿ƒé”€æ»¥ç”¨å›¾è°±ï¼Œæ£€æµ‹è™šå‡è¡Œä¸º |

> å›¾å‡ä¸ºå¼‚æ„å›¾ï¼ˆheterogeneousï¼‰ï¼Œå«ä¸‰ç§è¾¹ç±»å‹ï¼šåŒç”¨æˆ·ï¼ˆR-U-Rï¼‰ã€åŒäº§å“ï¼ˆR-P-Rï¼‰ã€ç›¸ä¼¼è¯„åˆ†+æ—¶é—´ï¼ˆR-S-Rï¼‰ã€‚

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

#### å®éªŒç¯å¢ƒ
- ç¡¬ä»¶ï¼šNVIDIA A100 GPUï¼ˆ80GBï¼‰ï¼Œ64æ ¸CPUï¼Œ1TBå†…å­˜
- LLM éª¨å¹²ï¼šQwen3-8B
- å¾®è°ƒæŠ€æœ¯ï¼šLoRAï¼ˆåº”ç”¨äºæ³¨æ„åŠ›å±‚ï¼‰
- ä¼˜åŒ–å™¨ï¼šAdamW
- æ‰¹å¤§å°ï¼š128ï¼ˆå¯æ‰©å±•è‡³256ï¼‰

#### è¯„ä¼°æŒ‡æ ‡
- **Macro-F1**ï¼šç±»åˆ«ä¸å¹³è¡¡ä¸‹çš„ç»¼åˆæ€§èƒ½
- **AUROC**ï¼šROC æ›²çº¿ä¸‹é¢ç§¯
- **AUPRC**ï¼šç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ä¸‹é¢ç§¯ï¼ˆå°¤å…¶å…³æ³¨æ­£ç±»è¡¨ç°ï¼‰

æ‰€æœ‰ç»“æœæŠ¥å‘Š 5 æ¬¡éšæœºç§å­çš„å‡å€¼ Â± æ ‡å‡†å·®ã€‚

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

æ¶µç›–å››ç±»ä¸»æµæ–¹æ³•ï¼š

| ç±»åˆ« | ä»£è¡¨æ–¹æ³• |
|------|--------|
| **çº¯ GNN** | GraphSAGE, HGT, ConsisGAD, PMP, GAAP |
| **å›¾æ— å…³æ¨¡å‹** | MLP, LLM, LLM-SFTï¼ˆå¾®è°ƒç‰ˆï¼‰ |
| **å›¾å¢å¼º LLM** | LLaGA, GraphGPT, HiGPT, InstructGLM |
| **LLM-enhanced GNN** | TAPE, FLAG |

> æ‰€æœ‰åŸºçº¿ä½¿ç”¨å®˜æ–¹ä»£ç å¤ç°ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ‘˜å½•è‡ª Table 3ï¼‰**

| æ–¹æ³• | InstantVideo (AUPRCâ†‘) | DigitalMusic (AUPRCâ†‘) | PromotionAbuse (AUPRCâ†‘) |
|------|------------------------|------------------------|--------------------------|
| **TAPE** | 79.28 Â± 0.58 | 81.99 Â± 0.19 | 67.21 Â± 0.49 |
| **FLAG** | 41.75 Â± 0.05 | 38.59 Â± 0.23 | 42.17 Â± 0.52 |
| **FraudCoT (Ours)** | **84.10 Â± 0.26** | **84.49 Â± 0.28** | **73.65 Â± 0.41** |

> ğŸ’¡ **æœ€é«˜æå‡è¾¾ +8.8% AUPRC**ï¼ˆç›¸å¯¹äºæœ€å¼ºåŸºçº¿ï¼‰ï¼Œä¸”åœ¨å·¥ä¸šæ•°æ®é›†ä¸Šä¼˜åŠ¿æœ€æ˜¾è‘—ã€‚

æ­¤å¤–ï¼š
- **AUROC æå‡æ˜æ˜¾**ï¼šåœ¨ InstantVideo ä¸Šè¾¾åˆ° 90.73%ï¼Œä¼˜äºç¬¬äºŒåè¿‘ 2 ä¸ªç™¾åˆ†ç‚¹ã€‚
- **ç¨³å®šæ€§æ›´å¼º**ï¼šæ ‡å‡†å·®æ›´å°ï¼Œè¡¨æ˜æ¨¡å‹é²æ£’æ€§æ›´å¥½ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

- ğŸ“Œ **è¶…è¶Šæ‰€æœ‰ GNN å’Œ LLM åŸºçº¿**ï¼šå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ ConsisGAD æˆ– GAAPï¼Œä¹Ÿæ— æ³•å……åˆ†åˆ©ç”¨æ–‡æœ¬æ·±å±‚è¯­ä¹‰ã€‚
- ğŸ“Œ **ä¼˜äºå›¾å¢å¼º LLM**ï¼šå¦‚ GraphGPTã€HiGPT ç”Ÿæˆçš„æ¨ç†è¾ƒæ³›åŒ–ï¼Œéš¾ä»¥èšç„¦å…·ä½“æ¬ºè¯ˆæ¨¡å¼ã€‚
- ğŸ“Œ **æ˜¾è‘—ä¼˜äº LLM-enhanced GNN**ï¼šTAPE å’Œ FLAG å—é™äºå›ºå®šæ¨¡æ¿ï¼Œæ— æ³•è¿›è¡Œè·¨é‚»åŸŸæ¨ç†ã€‚
- âœ… **å”¯ä¸€åŒæ—¶å®ç°é«˜æ€§èƒ½ä¸é«˜æ•ˆç‡çš„æ–¹æ³•**ï¼šè§ä¸‹è¡¨ã€‚

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰**

ç§»é™¤ä¸åŒç»„ä»¶åçš„æ€§èƒ½ä¸‹é™ï¼ˆä»¥ AUPRC ä¸ºä¾‹ï¼‰ï¼š

| å˜ä½“ | InstantVideo â†“ | DigitalMusic â†“ | PromotionAbuse â†“ |
|------|----------------|----------------|------------------|
| w/o FASCDï¼ˆæ—  CoT è’¸é¦ï¼‰ | -4.82 | -2.50 | -4.83 |
| w/o EACï¼ˆæ— éå¯¹ç§°å…±è®­ç»ƒï¼‰ | -3.10 | -2.20 | -3.44 |
| w/o NegDisï¼ˆæ— è´Ÿå‘è’¸é¦ï¼‰ | -2.10 | -1.80 | -2.30 |

> ğŸ” **å…³é”®å‘ç°**ï¼š
- ç§»é™¤ **FASCD** å½±å“æœ€å¤§ â†’ è¡¨æ˜ CoT è’¸é¦æ˜¯æ€§èƒ½å¢ç›Šçš„æ ¸å¿ƒé©±åŠ¨åŠ›ã€‚
- ç§»é™¤ **EAC** å¯¼è‡´è¯­ä¹‰-ç»“æ„å¤±é… â†’ éªŒè¯äº†ç«¯åˆ°ç«¯è®­ç»ƒçš„é‡è¦æ€§ã€‚
- ç§»é™¤ **NegDis** é™ä½é²æ£’æ€§ â†’ è¯´æ˜æŠ‘åˆ¶é”™è¯¯æ¨ç†è·¯å¾„å¯¹é˜²æ­¢è¿‡æ‹Ÿåˆè‡³å…³é‡è¦ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. âœ… **è‡ªä¸»æ¨ç†ä¼˜äºé¢„å®šä¹‰æç¤º**  
   è‡ªç”±å½¢å¼çš„ CoT æ¨ç†èƒ½æ•æ‰å¤æ‚çš„è·¨é‚»åŸŸæ¬ºè¯ˆæ¨¡å¼ï¼ˆå¦‚â€œåŒä¸€ç”¨æˆ·å‘å¸ƒæç«¯å¥½è¯„ä¸å·®è¯„â€ï¼‰ï¼Œè¿œè¶…æ¨¡æ¿æå–çš„èƒ½åŠ›ã€‚

2. âœ… **æ­£è´Ÿè’¸é¦æœºåˆ¶æœ‰æ•ˆæå‡æ¨ç†è´¨é‡**  
   ä¸ä»…å­¦ä¹ â€œæ€ä¹ˆæƒ³å¯¹â€ï¼Œè¿˜å­¦ä¹ â€œæ€ä¹ˆæƒ³é”™â€ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹æ›´å…·åˆ¤åˆ«åŠ›å’ŒæŠ—å¹²æ‰°èƒ½åŠ›ã€‚

3. âœ… **éå¯¹ç§°å…±è®­ç»ƒå®ç°æ•ˆç‡é£è·ƒ**  
   åœ¨ DigitalMusic ä¸Šå®ç° **1,066Ã— è®­ç»ƒåååŠ é€Ÿ**ï¼Œæœ€å¤§æ‰¹å¤§å°æå‡ **128 å€**ï¼Œä½¿ç«¯åˆ°ç«¯è®­ç»ƒå˜å¾—å¯è¡Œã€‚

4. âœ… **è¯­ä¹‰ä¸ç»“æ„æ·±åº¦èåˆå¸¦æ¥ç¨³å®šå¢ç›Š**  
   CoT æä¾›é«˜å±‚è¯­ä¹‰çº¿ç´¢ï¼ŒGNN å®ç°ç»“æ„ä¼ æ’­ï¼ŒäºŒè€…é€šè¿‡è”åˆè®­ç»ƒåŠ¨æ€å¯¹é½ï¼Œå½¢æˆé—­ç¯å¢å¼ºã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

- âš ï¸ **ä¾èµ–é«˜è´¨é‡æ•™å¸ˆ LLM**ï¼šè‹¥æ•™å¸ˆæœ¬èº«æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œè’¸é¦æ•ˆæœå—é™ã€‚
- âš ï¸ **è’¸é¦æ ·æœ¬éœ€äººå·¥æ ‡æ³¨æˆ–ç­›é€‰**ï¼šç›®å‰ä»…åœ¨éƒ¨åˆ†èŠ‚ç‚¹ä¸Šç”Ÿæˆ CoTï¼Œæ‰©å±•è‡³å…¨å›¾ä»æœ‰æˆæœ¬ã€‚
- âš ï¸ **å¯¹æç¨€ç–æ ‡ç­¾åœºæ™¯æ•æ„Ÿ**ï¼šå½“æ¬ºè¯ˆæ ·æœ¬æå°‘æ—¶ï¼Œè´Ÿä¾‹è’¸é¦å¯èƒ½å¼•å…¥å™ªå£°ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

- ğŸ”® æ¢ç´¢ **æ— éœ€æ•™å¸ˆçš„è‡ªç›‘ç£ CoT ç”Ÿæˆ**ï¼Œè¿›ä¸€æ­¥é™ä½æˆæœ¬ã€‚
- ğŸ”® å°† FraudCoT æ‰©å±•è‡³ **åŠ¨æ€å›¾** å’Œ **å¤šæ¨¡æ€å›¾**ï¼ˆå¦‚å›¾æ–‡æ··åˆï¼‰ã€‚
- ğŸ”® ç»“åˆ **å¼ºåŒ–å­¦ä¹ ** ä¼˜åŒ–æ¨ç†è·¯å¾„é€‰æ‹©ç­–ç•¥ã€‚
- ğŸ”® å¼€å‘ **è½»é‡åŒ–å­¦ç”Ÿæ¨¡å‹**ï¼Œä¾¿äºéƒ¨ç½²è‡³ç”Ÿäº§ç³»ç»Ÿã€‚

---

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **FraudCoT é€šè¿‡â€œé€‰æ‹©æ€§ CoT è’¸é¦ + éå¯¹ç§°å…±è®­ç»ƒâ€ï¼Œé¦–æ¬¡å®ç°äº†é«˜æ•ˆã€è‡ªä¸»ã€å¯è§£é‡Šçš„å›¾æ„ŸçŸ¥æ¬ºè¯ˆæ¨ç†ï¼Œåœ¨æ€§èƒ½ä¸æ•ˆç‡ä¸Šå…¨é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä¸º LLM-GNN ååŒå»ºæ¨¡æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚**

</details>

---

### 2. [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)

**Authors**: Zhanglu Yan, Kaiwen Tang, Zixuan Zhu, Zhenyu Bai, Qianhui Liu, Weng-Fai Wong  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.22876v1  

#### Abstract
Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 8...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMatterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
å½“å‰åŸºäº Spiking Neural Networks (SNNs) çš„ LLM æ¨ç†æ¶æ„è™½ç„¶åœ¨ç†è®ºä¸Šå…·æœ‰é«˜èƒ½æ•ˆæ½œåŠ›ï¼Œä½†å…¶èƒ½é‡è¯„ä¼°å¤šä¾èµ–äºç®€åŒ–çš„æ“ä½œè®¡æ•°ï¼ˆå¦‚ ACC vs MACï¼‰ï¼Œ**å¿½ç•¥äº†çœŸå®ç¡¬ä»¶ä¸­å ä¸»å¯¼åœ°ä½çš„æ•°æ®ç§»åŠ¨å¼€é”€**ï¼ˆdata movementï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å…ˆè¿›å·¥è‰ºä¸‹ï¼Œ**spike è½¬ç§»ï¼ˆ42â€“55%ï¼‰å’Œæƒé‡è®¿é—®ï¼ˆ27â€“32%ï¼‰æ˜¯ä¸»è¦èƒ½è€—æ¥æº**ï¼Œè€Œè®¡ç®—ä»…å  12â€“20%ã€‚

æ­¤å¤–ï¼Œä¼ ç»Ÿ TTFS ç¼–ç å°†â€œé™é»˜çŠ¶æ€â€ï¼ˆall-zero spike trainï¼‰åˆ†é…ç»™æœ€å°è†œç”µä½å€¼ï¼Œä½†ç”±äºè¯¥å€¼åœ¨å®é™…åˆ†å¸ƒä¸­æä¸ºç½•è§ï¼Œå¯¼è‡´èŠ‚èƒ½æ½œåŠ›æœªè¢«æœ‰æ•ˆåˆ©ç”¨ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡º **Matterhorn**ï¼Œä¸€ç§é«˜æ•ˆçš„æ¨¡æ‹Ÿç¨€ç–è„‰å†² Transformer æ¶æ„ï¼ŒåŒ…å«ä¸¤å¤§æ ¸å¿ƒæŠ€æœ¯ï¼š

#### ï¼ˆ1ï¼‰Masked Time-To-First-Spike (M-TTFS) ç¼–ç 
- **æ ¸å¿ƒæ€æƒ³**ï¼šé‡æ–°å®šä¹‰â€œé›¶èƒ½è€—é™é»˜çŠ¶æ€â€ï¼Œå°†å…¶ä»æœ€ä½è†œç”µä½è½¬ç§»åˆ°**æœ€é¢‘ç¹å‡ºç°çš„è†œç”µä½å¯¹åº”çš„æ—¶é—´æ­¥ `Imax`**ã€‚
- å¼•å…¥ **â€˜Dead Zoneâ€™ ç­–ç•¥**ï¼šå°†å›´ç»• `Imax` çš„ä¸€ä¸ªæ—¶é—´çª—å£ `[Imaxâˆ’k, Imax+k]` å†…çš„æ‰€æœ‰ spike æŠ‘åˆ¶ä¸ºé™é»˜çŠ¶æ€ï¼Œä»è€Œæœ€å¤§åŒ– sparsityã€‚
- æ‰€æœ‰éé›¶è¾“å‡ºä»åªäº§ç”Ÿä¸€ä¸ª spikeï¼Œä¿è¯ sparsity ä¸”ä¸ç ´åæ•°æ®åˆ†å¸ƒç»“æ„ã€‚

#### ï¼ˆ2ï¼‰Memristive Synapse Unit (MSU)
- åŸºäº **Compute-in-Memory (CIM)** æŠ€æœ¯æ„å»ºçš„æ··åˆä¿¡å·å•å…ƒã€‚
- åˆ©ç”¨ **nT1R crossbar + RRAM** å®ç°æƒé‡é©»ç•™å†…å­˜ä¸­çš„æ¨¡æ‹Ÿç§¯åˆ†ã€‚
- æ¶ˆé™¤æƒé‡è¯»å–å¼€é”€ï¼Œå¹¶é€šè¿‡ bit-serial VMM å’Œæ•°å­—ç´¯åŠ å™¨é‡å»ºå®Œæ•´ç»“æœã€‚
- é›¶è¾“å…¥ç”µæµå¯ç‰©ç†å…³é—­æ¨¡æ‹Ÿé€šè·¯ï¼Œå®ç°**åŠ¨æ€åŠŸè€—é—¨æ§**ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | Matterhorn ä¼˜åŠ¿ |
|------|----------------|
| **ç®—æ³•å±‚é¢** | M-TTFS æ˜¾è‘—é™ä½ spike rateï¼ˆè‡³ 1.65%ï¼‰ï¼Œå‡å°‘ spike movement èƒ½è€—è¾¾ 2.46Ã— |
| **ç¡¬ä»¶å±‚é¢** | MSU æ¶ˆé™¤ weight access æˆæœ¬ï¼Œæ¨¡æ‹Ÿ MAC èƒ½è€—ä½è‡³ fJ/spike çº§åˆ« |
| **ç³»ç»ŸååŒä¼˜åŒ–** | M-TTFS çš„ç¨€ç–æ€§ç›´æ¥è½¬åŒ–ä¸º MSU çš„ç‰©ç†èŠ‚èƒ½ï¼Œå½¢æˆè½¯ç¡¬ååŒå¢ç›Š |
| **æ•´ä½“æ€§èƒ½** | åœ¨ GLUE ä¸Šè¾¾åˆ° SOTA å‡†ç¡®ç‡ï¼ˆ84.64%ï¼‰ï¼ŒåŒæ—¶å®ç° **2.31Ã— èƒ½æ•ˆæå‡** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
- ä¸»è¦åŸºå‡†ï¼š**GLUE benchmark** åŒ…å«ä»¥ä¸‹ 7 ä¸ªä»»åŠ¡ï¼š
  - QQP, MNLI-m, SST-2, QNLI, RTE, MRPC, STS-B
- å…·ä½“åˆ†æä»¥ **SST-2** æ•°æ®é›†ä¸ºä¸»è¿›è¡Œèƒ½è€—åˆ†è§£ä¸æ¶ˆèç ”ç©¶ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
| è®¾ç½®é¡¹ | æè¿° |
|-------|------|
| **æ¨¡å‹æ¶æ„** | BERTbase ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œé‡‡ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒï¼›ä½¿ç”¨ 4-bit é‡åŒ–ï¼Œå¯¹åº” T=16 æ—¶é—´æ­¥ |
| **éƒ¨ç½²å¹³å°** | å‡è®¾ç©ºé—´æ•°æ®æµæ¶æ„ï¼ˆspatial dataflowï¼‰ï¼ŒåŸºäº NoC è¿›è¡Œæ ¸é—´é€šä¿¡ |
| **å·¥è‰ºèŠ‚ç‚¹** | æ•°å­—éƒ¨åˆ†å»ºæ¨¡äºå•†ç”¨ **22nm å·¥è‰º**ï¼›æ¨¡æ‹Ÿ CIM å‚æ•°æ¥è‡ª Ye et al. (2023) çš„ 28nm RRAM å® |
| **è¯„ä¼°æŒ‡æ ‡** | 
  - **å‡†ç¡®ç‡ Accuracy / Pearson ç›¸å…³ç³»æ•°ï¼ˆSTS-Bï¼‰**
  - **æ€»èƒ½è€—ï¼ˆmJ/blockï¼‰**ï¼šç»†åˆ†ä¸º spike movementã€weight accessã€digital computingã€analog computingã€leakage |
  - **spike rate (%)**
  - **èƒ½æ•ˆæ¯”ï¼ˆEnergy Efficiency Improvementï¼‰**

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿æ¨¡å‹ | ç±»å‹ | ç‰¹ç‚¹ |
|--------|------|------|
| **Spiking Otters (Yan et al., 2025a)** | SNN (TTFS) | å½“å‰æœ€ä¼˜è„‰å†² Transformerï¼Œ13.4M å‚æ•° |
| **SpikingLM (Xing et al., 2024b)** | SNN (Rate) | å¤§è§„æ¨¡è„‰å†²è¯­è¨€æ¨¡å‹ |
| **Sorbet (Tang et al., 2025)** | SNN (Rate) | é«˜æ•ˆè„‰å†² Transformer |
| **SpikingBERT (Bal & Sengupta, 2024)** | SNN (Rate) | è„‰å†²ç‰ˆ BERTï¼Œ50M å‚æ•° |
| **Q2BERT / BiT / DistilBERT** | Quantized ANN | ä½ä½å®½ ANN å¯¹ç…§ç»„ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®
| æ¨¡å‹ | å¹³å‡å‡†ç¡®ç‡ | æœ€ä½³å‡†ç¡®ç‡ï¼ˆRTEï¼‰ | æ€»èƒ½è€—ï¼ˆmJ/blockï¼‰ | Spike Rate |
|------|------------|-------------------|--------------------|-----------|
| BERTbase | 87.31% | 72.6% | â€” | â€” |
| Spiking Otters | 83.22% | 68.95% | 14.21 mJ | ~5% |
| **Matterhorn (k=0)** | **85.87%** | **72.56%** | 8.31 mJ | 2.77% â†’ 34% silence |
| **Matterhorn (k=1)** | **84.64%** | 71.84% | **6.14 mJ** | **1.65%** â†’ 61.2% silence |

> âœ… **Matterhorn (k=1)** è¾¾åˆ° **84.64%** å¹³å‡å‡†ç¡®ç‡ï¼Œ**è¶…è¶Šæ‰€æœ‰ç°æœ‰ SNN æ¨¡å‹ 1.42%**  
> âœ… èƒ½è€—é™è‡³ **6.14 mJ/block**ï¼Œç›¸æ¯” Spiking Otters å®ç° **2.31Ã— èƒ½æ•ˆæå‡**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
| å¯¹æ¯”é¡¹ | èƒ½æ•ˆæå‡å€æ•° |
|-------|--------------|
| vs. Spiking Otters (k=1, no MSU) | 1.72Ã— |
| vs. Spiking Otters (with MSU) | **2.31Ã—** |
| vs. SpikingLM | ~4.24Ã— |
| vs. Sorbet | ~6.98Ã— |
| vs. SpikingBERT | >13.13Ã— |

> ğŸ’¡ å³ä½¿ä¸ç­‰æ•ˆ QNN æ¨¡å‹æ¯”è¾ƒï¼ŒMatterhorn ä¹Ÿèƒ½å®ç° **2.35Ã— èƒ½æ•ˆæå‡**ï¼ŒéªŒè¯äº† SNN + CIM çš„æœ¬è´¨ä¼˜åŠ¿ã€‚

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰M-TTFS + Dead Zone å¯¹ spike åˆ†å¸ƒçš„å½±å“ï¼ˆSST-2ï¼‰
| é…ç½® | é™é»˜ç¥ç»å…ƒæ¯”ä¾‹ | spike movement energy |
|------|----------------|------------------------|
| Standard TTFS | 0.26% | 6.98 mJ |
| M-TTFS (k=0) | 34.0% | 4.75 mJ |
| M-TTFS (k=1) | 61.2% | 2.84 mJ |
| M-TTFS (k=2) | 76.4% | 1.80 mJ |

> ğŸ” M-TTFS å°†æœ€å¸¸è§æ¿€æ´»æ˜ å°„ä¸ºé™é»˜çŠ¶æ€ï¼Œæ˜¾è‘—æå‡ sparsityã€‚

#### ï¼ˆ2ï¼‰èƒ½é‡é€çº§ä¼˜åŒ–è·¯å¾„ï¼ˆSST-2ï¼‰
| æ–¹æ³• | æ€»èƒ½è€— | ç›¸å¯¹æ”¹è¿› |
|------|--------|----------|
| Traditional TTFS | 16.80 mJ | Baseline |
| + M-TTFS | 12.24 mJ | â†“27% |
| + Dead Zone (k=1) | 8.31 mJ | â†“50% |
| + MSU | **6.14 mJ** | **â†“2.7Ã—** |

> ğŸ“ˆ è½¯ä»¶ç¨€ç–ç¼–ç ä¸ç¡¬ä»¶ CIM ååŒå¸¦æ¥å åŠ èŠ‚èƒ½æ•ˆæœã€‚

#### ï¼ˆ3ï¼‰ç²¾åº¦-èƒ½æ•ˆæƒè¡¡åˆ†æ
- **k=1 æ˜¯æœ€ä½³å¹³è¡¡ç‚¹**ï¼šèƒ½è€—ä¸‹é™ 59%ï¼Œå‡†ç¡®ç‡ä»…è½»å¾®ä¸‹é™ 0.9%
- **kâ‰¥2 æ—¶è¿›å…¥æ¿€è¿›ç¨€ç–åŒº**ï¼šå‡†ç¡®ç‡æ˜æ˜¾ä¸‹é™ï¼ˆå¦‚ k=4 æ—¶é™è‡³ 83.83%ï¼‰

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **æ•°æ®ç§»åŠ¨æ˜¯ SNN èƒ½è€—ç“¶é¢ˆ**ï¼šåœ¨ç°ä»£ç¡¬ä»¶ä¸Šï¼Œspike transfer å’Œ weight access å æ€»èƒ½è€—è¶… 70%ï¼Œè¿œé«˜äºè®¡ç®—æˆæœ¬ã€‚
2. **ç¼–ç ç­–ç•¥åº”åŒ¹é…æ•°æ®åˆ†å¸ƒ**ï¼šå°†é™é»˜çŠ¶æ€åˆ†é…ç»™é«˜é¢‘å€¼ï¼ˆè€Œéæå°å€¼ï¼‰å¯å¤§å¹…æå‡ sparsity è€Œæ— ä¿¡æ¯æŸå¤±ã€‚
3. **æ­»åŒºç­–ç•¥å¯æ˜¾å¼æ§åˆ¶ç¨€ç–-ç²¾åº¦æƒè¡¡**ï¼šé€šè¿‡è°ƒèŠ‚ `k` å¯çµæ´»é€‚åº”ä¸åŒèµ„æºçº¦æŸåœºæ™¯ã€‚
4. **CIM æ˜¯çªç ´å†¯Â·è¯ºä¾æ›¼ç“¶é¢ˆçš„å…³é”®**ï¼šMSU æˆåŠŸæ¶ˆé™¤ weight access å¼€é”€ï¼Œå¹¶æ”¯æŒç‰©ç†çº§åŠŸè€—é—¨æ§ã€‚
5. **è½¯ç¡¬ååŒè®¾è®¡å®ç° SOTA è¡¨ç°**ï¼šMatterhorn åŒæ—¶å®ç°äº†æœ€é«˜å‡†ç¡®ç‡ä¸æœ€å¼ºèƒ½æ•ˆï¼Œè¯æ˜äº†å…¶å·¥ç¨‹å¯è¡Œæ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–é¢„è®­ç»ƒ QNN è¿›è¡Œè½¬æ¢**ï¼šç›®å‰é‡‡ç”¨ QNN-to-SNN conversion æ¡†æ¶ï¼Œå°šæœªæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒã€‚
- **æ¨¡æ‹Ÿå™ªå£°å®¹å¿åº¦è™½å¼ºä½†ä»æœ‰é™**ï¼šå°½ç®¡å•æ¯”ç‰¹æƒé‡å¢å¼ºäº†é²æ£’æ€§ï¼Œä½†åœ¨æç«¯å·¥è‰ºåå·®ä¸‹å¯èƒ½å½±å“ç¨³å®šæ€§ã€‚
- **ç¡¬ä»¶é¢ç§¯éšæ¨¡å‹è§„æ¨¡å¢é•¿è¾ƒå¿«**ï¼šæ¯ä¸ª transformer block éœ€çº¦ 108 ä¸ª MSU å®ï¼Œå¤§è§„æ¨¡éƒ¨ç½²éœ€ tile æ¶æ„ä¼˜åŒ–ã€‚
- **å½“å‰è¯„ä¼°åŸºäºæ¨¡å‹ä¼°ç®—**ï¼šç¼ºä¹çœŸå®èŠ¯ç‰‡æµç‰‡åçš„å®æµ‹æ•°æ®æ”¯æ’‘ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢ **M-TTFS æ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒ** çš„å¯èƒ½æ€§ï¼Œæ‘†è„±å¯¹ QNN çš„ä¾èµ–ã€‚
- æ‰©å±•è‡³ **æ›´å¤§è§„æ¨¡ LLMs**ï¼ˆå¦‚ Llamaã€Phi ç³»åˆ—ï¼‰å¹¶ç ”ç©¶ long-context ä¸‹çš„ spike åŠ¨æ€ç‰¹æ€§ã€‚
- è®¾è®¡ **å¯é‡æ„ MSU æ¶æ„**ï¼Œæ”¯æŒåŠ¨æ€è°ƒæ•´ bit-serial æ·±åº¦ä¸ç²¾åº¦é…ç½®ã€‚
- æ¨è¿› **åŸå‹èŠ¯ç‰‡æµç‰‡ä¸å®æµ‹éªŒè¯**ï¼Œå»ºç«‹æ›´ç²¾ç¡®çš„èƒ½è€—-å»¶è¿Ÿæ¨¡å‹ã€‚
- æ¢ç´¢ **å…‰å­ CIM æˆ–å…¶ä»–æ–°å‹å™¨ä»¶** æ›¿ä»£ RRAMï¼Œè¿›ä¸€æ­¥æå‡å¸¦å®½ä¸èƒ½æ•ˆã€‚

--- 

> âœ… **æ€»ç»“ä¸€å¥è¯**ï¼š  
> Matterhorn é€šè¿‡ **M-TTFS ç¼–ç åŒ¹é…æ•°æ®åˆ†å¸ƒä»¥æœ€å°åŒ– spike movement**ï¼Œç»“åˆ **MSU åŸºäº CIM æ¶ˆé™¤ weight access å¼€é”€**ï¼Œé¦–æ¬¡åœ¨è„‰å†² Transformer ä¸­å®ç°äº† **é«˜ç²¾åº¦ä¸è¶…é«˜èƒ½æ•ˆçš„ç»Ÿä¸€**ï¼Œæ ‘ç«‹äº†æ–°çš„ SOTA æ ‡æ†ã€‚

</details>

---

### 3. [TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification](https://arxiv.org/abs/2601.23180)

**Authors**: Haoyun Jiang, Junqi He, Feng Hong, Xinlong Yang, Jianwei Zhang, Zheng Li, Zhengyang Zhuge, Zhiyong Chen, Bo Han, Junyang Lin, Jiangchao Yao  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2601.23180v1  

#### Abstract
Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡ã€ŠTriSpec: Ternary Speculative Decoding via Lightweight Proxy Verificationã€‹æ ¸å¿ƒæ€»ç»“

---

## 1. ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ•ˆç‡å—é™äºå…¶**ä¸²è¡Œè‡ªå›å½’ç”Ÿæˆæœºåˆ¶**ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å“åº”åºåˆ—æ›´é•¿æ—¶ï¼Œå»¶è¿Ÿé—®é¢˜å°¤ä¸ºä¸¥é‡ã€‚è™½ç„¶ç°æœ‰çš„ **Speculative Decoding (SD)** å·²é€šè¿‡è½»é‡çº§ drafter å’Œå¹¶è¡ŒéªŒè¯æ˜¾è‘—åŠ é€Ÿæ¨ç†ï¼Œä½†å½“å‰ç ”ç©¶å¤§å¤šèšç„¦äºä¼˜åŒ– **drafting æ•ˆç‡** å’Œ **acceptance rate**ï¼Œè€Œå¿½ç•¥äº†å¦ä¸€ä¸ªå…³é”®ç“¶é¢ˆâ€”â€”**verification æˆæœ¬**ã€‚

æœ¬æ–‡æŒ‡å‡ºï¼š**æ¯è½®éªŒè¯ä¸­è°ƒç”¨å¤§è§„æ¨¡ target model çš„è®¡ç®—å¼€é”€å·²æˆä¸ºæ–°çš„æ€§èƒ½ç“¶é¢ˆ**ï¼ŒäºŸéœ€ä¼˜åŒ–ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šTriSpec
æå‡º **TriSpec** â€”â€”ä¸€ç§å…¨æ–°çš„**ä¸‰å…ƒæ¨æµ‹è§£ç æ¡†æ¶ï¼ˆternary speculative decodingï¼‰**ï¼Œå¼•å…¥ä¸€ä¸ª**è½»é‡çº§ä»£ç†éªŒè¯å™¨ï¼ˆlightweight proxy verifierï¼‰** æ¥åˆ†æ‹… target model çš„éªŒè¯è´Ÿæ‹…ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- åˆ©ç”¨**åŒå®¶æ—çš„å°è§„æ¨¡æ¨¡å‹**ï¼ˆå¦‚ Qwen3-1.7B éªŒè¯ Qwen3-32Bï¼‰ä½œä¸º proxy verifierã€‚
- è¯¥ proxy èƒ½å¤Ÿå¿«é€Ÿé¢„éªŒè¯å¤§éƒ¨åˆ† draft tokensï¼Œå¹¶åœ¨ç½®ä¿¡åº¦é«˜æ—¶ç›´æ¥æ¥å—æˆ–å±€éƒ¨çº é”™ã€‚
- åªæœ‰å½“ proxy çš„åˆ¤æ–­â€œä¸å¯ä¿¡â€æ—¶ï¼Œæ‰å°†æ§åˆ¶æƒäº¤è¿˜ç»™æ˜‚è´µçš„ target model è¿›è¡Œæƒå¨éªŒè¯ã€‚

#### å…³é”®æœºåˆ¶ï¼š
- **Margin-based routing criterion**ï¼šä½¿ç”¨ proxy è¾“å‡ºçš„ top-1 ä¸ top-2 æ¦‚ç‡å·®å€¼ï¼ˆmarginï¼‰ä½œä¸ºå¯ä¿¡åº¦åˆ¤æ®ã€‚è‹¥ margin â‰¥ Î»ï¼Œåˆ™è®¤ä¸º proxy åˆ¤æ–­å¯ä¿¡ï¼›å¦åˆ™å‡çº§è‡³ target modelã€‚
- **Case Iï¼ˆæœ¬åœ°çº æ­£ï¼‰**ï¼šproxy åœ¨é¦–æ¬¡æ‹’ç»å‰ä»å¯ä¿¡ â†’ æ¥å—éƒ¨åˆ† token å¹¶ç”¨ proxy çš„é¢„æµ‹è¿›è¡Œå±€éƒ¨ä¿®æ­£ã€‚
- **Case IIï¼ˆç›®æ ‡å‡çº§ï¼‰**ï¼šproxy åœ¨é¦–æ¬¡æ‹’ç»å‰å·²ä¸å¯ä¿¡ â†’ å°†å‰©ä½™ token æäº¤ç»™ target model éªŒè¯ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿ SD æ–¹æ³•ï¼ˆå¦‚ EAGLE-3ï¼‰ | TriSpec |
|------|----------------------------|--------|
| **Focus** | ä¼˜åŒ– drafting æ•ˆç‡å’Œ acceptance length (T) | æ–°å¢å¯¹ verification time (t_v) çš„ä¼˜åŒ– |
| **Verifier** | å§‹ç»ˆä¾èµ– full target model | å¼•å…¥ lightweight proxy åˆ†æµç®€å•éªŒè¯ |
| **æˆæœ¬æ§åˆ¶** | æ¯è½®éƒ½éœ€å®Œæ•´æ‰§è¡Œ target model forward pass | æ˜¾è‘—å‡å°‘ target model è°ƒç”¨æ¬¡æ•° |
| **çµæ´»æ€§** | å¯¹ draft åˆ†å¸ƒè´¨é‡æ•æ„Ÿ | æ›´é²æ£’ï¼Œé€‚ç”¨äºå¤šç§ drafting æ¶æ„ |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šTriSpec ä¸æ˜¯æ›¿ä»£è€Œæ˜¯å¢å¼ºç°æœ‰ SD æ–¹æ³•ï¼ˆå¯ä¸ EAGLE-3 ç­‰ç»“åˆï¼‰ï¼Œä»**éªŒè¯ä¾§**åˆ‡å…¥å®ç°è¿›ä¸€æ­¥åŠ é€Ÿï¼Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„å‰æä¸‹å¤§å¹…æå‡ååé‡ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
ä¸»è¦è¯„ä¼°åŸºäºä»¥ä¸‹äº”ç±»æ¨ç†åŸºå‡†ï¼š
- **æ•°å­¦æ¨ç†**ï¼š`GSM8K`, `MATH500`, `Gaokao-2023-EN (GK23-en)`
- **ä»£ç ç”Ÿæˆ**ï¼š`HumanEval`, `MBPP`
- **ç»¼åˆä¸æŒ‘æˆ˜æ€§ä»»åŠ¡æ‰©å±•æµ‹è¯•**ï¼š
  - `SpecBench`ï¼ˆå¤šè½®å¯¹è¯ã€ç¿»è¯‘ã€æ‘˜è¦ã€QAã€æ•°å­¦ã€RAGï¼‰
  - `HotpotQA`ï¼ˆå¤šè·³é—®ç­”ï¼‰
  - `Polymath`ï¼ˆå¤šè¯­è¨€æ•°å­¦ï¼‰
  - `AIME24/AIME25`ï¼ˆç«èµ›çº§æ•°å­¦ï¼‰
  - `GPQA-Diamond`ï¼ˆä¸“å®¶çº§ STEM å¤šé€‰é¢˜ï¼‰

---

### âš™ï¸ å®éªŒè®¾ç½®
| é¡¹ç›® | è®¾ç½®è¯´æ˜ |
|------|----------|
| **Model Families** | `Qwen3`, `DeepSeek-R1-Distill-Qwen (DSQ)`, `DeepSeek-R1-Distill-LLaMA (DSL)` |
| **Target Models** | Qwen3-32B, DSQ-32B, DSL-70B |
| **Proxy Verifiers** | åŒå®¶æ—å°æ¨¡å‹ï¼šQwen3-1.7B, DSQ-1.5B, DSL-8B |
| **Drafters** | ä½¿ç”¨ HASS æˆ– EAGLE-3 æ¶æ„çš„ single-layer drafter |
| **Draft Tree Config** | Depth=6, Top-k=10, Budget Tokens=60 |
| **Margin Threshold Î»** | é»˜è®¤è®¾ä¸º 0.5 |
| **Hardware** | å•å¼  NVIDIA A100 80GB GPUï¼ˆæ¨ç†ï¼‰ï¼›8Ã—A100 è®­ç»ƒ |

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **Accuracy** | Pass@1 å‡†ç¡®ç‡ï¼ˆreasoningï¼‰ã€LLM-judge å¾—åˆ†ï¼ˆSpecBenchï¼‰ |
| **Speedup** | ç›¸å¯¹äºçº¯ target model è‡ªå›å½’ç”Ÿæˆçš„é€Ÿåº¦æå‡å€æ•° |
| **Throughput (TPS)** | Tokens per secondï¼Œè¡¡é‡å®é™…ååèƒ½åŠ› |
| **Target-invocation ratio (r_t)** | æ¯ç”Ÿæˆä¸€ä¸ª token è°ƒç”¨ target model çš„æ¯”ä¾‹ï¼Œåæ˜ éªŒè¯å¼€é”€ |
| **End-to-end Latency (L)** | æ€»ä½“ç”Ÿæˆå»¶è¿Ÿ |

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Single Model (Target/Proxy)**ï¼šä»…ç”¨ target æˆ– proxy è‡ªå›å½’ç”Ÿæˆ
- **HASS / EAGLE-3**ï¼šä»£è¡¨å…ˆè¿› SD æ–¹æ³•
- **SpecCascade variants**ï¼š
  - SC[Chow]ï¼šåŸºäº drafter ç½®ä¿¡åº¦è¿‡æ»¤
  - SC[OPT]ï¼šåŸºäºåˆ†å¸ƒè·ç¦»çš„æœ€ä¼˜æ‹’ç»å¯¹ç­–
  - SC[Token]ï¼šå®Œå…¨ä¾èµ– target åˆ†å¸ƒå†³ç­–ï¼ˆä¸ä¾èµ– drafter è¾“å‡ºï¼‰

> TriSpec å¯ä¸ HASS/EAGLE-3 ç»“åˆå½¢æˆ â€œHASS+TriSpecâ€ã€â€œEAGLE3+TriSpecâ€

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1 & Table 3ï¼‰

#### åœ¨ `Qwen3-32B` ä¸Šçš„è¡¨ç°ï¼ˆTemperature=0ï¼‰ï¼š
| æ–¹æ³• | Avg Accuracy | Speedup | Target Invocation Ratio (r_t) |
|------|-------------|---------|-------------------------------|
| EAGLE-3 | 84.4% | 2.85Ã— | ~22% |
| **EAGLE3+TriSpec** | **84.2%** (-0.2%) | **3.55Ã—** (**â†‘24.6%**) | **~10%** (**â†“55%**) |

> âœ… **æœ€é«˜è¾¾ 35% çš„é¢å¤–åŠ é€Ÿ**ï¼ŒåŒæ—¶ **target model è°ƒç”¨å‡å°‘è¶…è¿‡ä¸€åŠ**

#### åœ¨ `DSQ-32B` ä¸Šçš„è¡¨ç°ï¼š
| æ–¹æ³• | Speedup | r_t |
|------|--------|-----|
| EAGLE-3 | 3.22Ã— | 21.45% |
| **EAGLE3+TriSpec** | **3.67Ã—** (**â†‘14%**) | **13.25%** (**â†“38%**) |

#### åœ¨ `DSL-70B` ä¸Šçš„è¡¨ç°ï¼š
| æ–¹æ³• | Speedup | r_t |
|------|--------|-----|
| EAGLE-3 | 2.48Ã— | 26.88% |
| **EAGLE3+TriSpec** | **3.36Ã—** (**â†‘35.5%**) | **15.18%** (**â†“43%**) |

> ğŸ’¡ æ‰€æœ‰æ¨¡å‹æ—å‡å®ç° **15%-35% çš„ç›¸å¯¹é€Ÿåº¦æå‡**ï¼Œä¸” **accuracy ä¸‹é™ <1%**

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰

#### ï¼ˆ1ï¼‰Draft Training Strategiesï¼ˆTable 5ï¼‰
| è®­ç»ƒæ–¹å¼ | Speedup (Qwen3) | è¯´æ˜ |
|--------|----------------|------|
| Joint from scratch | 3.55Ã— | æ€§èƒ½ç•¥ä¼˜ï¼Œä½†è®­ç»ƒæˆæœ¬æ›´é«˜ |
| Adapter-only finetune | 3.46Ã— | æˆæœ¬ä½ï¼Œé€‚åˆå·²æœ‰ drafter æƒé‡åœºæ™¯ |

> âœ… ä¸¤ç§ç­–ç•¥å‡å¯æœ‰æ•ˆæ”¯æŒ TriSpecï¼Œæä¾›çµæ´»éƒ¨ç½²é€‰æ‹©ã€‚

#### ï¼ˆ2ï¼‰Token Pruningï¼ˆæ˜¯å¦å‰ªæå·²éªŒè¯ tokenï¼‰
| ç­–ç•¥ | Acceptance Length T | Speedup |
|------|--------------------|--------|
| w/o pruning | 4.58 | 3.46Ã— |
| **w/ pruning** | **4.71** | **3.55Ã—** |

> âœ… Token pruning å¯è¿›ä¸€æ­¥æå‡ T å’Œæ•´ä½“é€Ÿåº¦ï¼Œè¯æ˜ proxy åˆ¤æ–­å¯é ã€‚

#### ï¼ˆ3ï¼‰Routing Criteria å¯¹æ¯”ï¼ˆFigure 5bï¼‰
æ¯”è¾ƒä¸åŒè·¯ç”±ç­–ç•¥ï¼š
- Top-1 Probability
- Entropy
- **Top-1 - Top-2 Marginï¼ˆæœ¬æ–‡é‡‡ç”¨ï¼‰**
- Learned Router (R2R)

> âœ… **Margin-based è·¯ç”±åœ¨ accuracy-speedup trade-off ä¸Šè¡¨ç°æœ€ä½³**ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚

#### ï¼ˆ4ï¼‰Draft Length å½±å“ï¼ˆFigure 5cï¼‰
- TriSpec æœ€ä½³ draft length ä¸æ ‡å‡† SD æ¥è¿‘ï¼ˆ4~5ï¼‰ï¼Œè¡¨æ˜å…¼å®¹æ€§å¼ºã€‚
- åœ¨å¤šæ•°é•¿åº¦ä¸‹ TriSpec å‡ä¼˜äº baselineã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Verification cost æ˜¯ SD ä¸­è¢«å¿½è§†çš„å…³é”®ç“¶é¢ˆ**ï¼Œå°¤å…¶åœ¨ high-efficiency drafterï¼ˆå¦‚ EAGLE-3ï¼‰æ™®åŠåæ›´ä¸ºçªå‡ºã€‚
2. **åŒå®¶æ—å°æ¨¡å‹å…·å¤‡å¼º token-level alignment**ï¼ˆQwen3-1.7B vs 32B è¾¾ 82% exact matchï¼‰ï¼Œæ˜¯ç†æƒ³çš„ lightweight proxy verifierã€‚
3. **Margin-based criterion å¯æœ‰æ•ˆåŒºåˆ† proxy æ˜¯å¦å¯ä¿¡**ï¼Œå®ç°æ™ºèƒ½è·¯ç”±ï¼Œé¿å…ç›²ç›®å‡çº§ã€‚
4. **TriSpec å¯æ— ç¼é›†æˆåˆ°ç°æœ‰ SD æ¡†æ¶ä¸­**ï¼Œå¸¦æ¥æ˜¾è‘—åŠ é€Ÿï¼ˆup to 35%â†‘ speedupï¼‰ï¼ŒåŒæ—¶å°† target model è°ƒç”¨é™ä½ 50% ä»¥ä¸Šã€‚
5. **å³ä½¿åœ¨éæ¨ç†æ¨¡å‹ï¼ˆLLaMAï¼‰ä¸Šä¹Ÿæœ‰æ•ˆ**ï¼ˆAppendix B.5ï¼‰ï¼Œæ˜¾ç¤ºè‰¯å¥½æ³›åŒ–æ€§ã€‚

---

### âš ï¸ å±€é™æ€§
1. **ä¾èµ–åŒå®¶æ—æ¨¡å‹çš„å­˜åœ¨**ï¼šè‹¥æ— åˆé€‚è§„æ¨¡çš„ proxy modelï¼Œéš¾ä»¥æ„å»ºé«˜æ•ˆ pipelineã€‚
2. **Margin threshold Î» éœ€è°ƒå‚**ï¼šè™½é»˜è®¤å€¼ 0.5 è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æç«¯ä»»åŠ¡ä¸­å¯èƒ½éœ€è¦è°ƒæ•´ä»¥å¹³è¡¡ accuracy-speedã€‚
3. **è®­ç»ƒæˆæœ¬ç•¥å¢**ï¼šéœ€é¢å¤–è®­ç»ƒ adapter æ¨¡å—ï¼ˆjoint training å¤šè€—çº¦ 50% æ—¶é—´ï¼‰ã€‚
4. **å¯¹æä½è´¨é‡ draft æ›´æ•æ„Ÿ**ï¼šè‹¥ drafter æœ¬èº«è¾“å‡ºæå·®ï¼Œproxy ä¹Ÿå¯èƒ½é¢‘ç¹è¯¯åˆ¤å¯¼è‡´å‡çº§è¿‡å¤šã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **è‡ªåŠ¨åŒ– margin threshold tuning**ï¼šæ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´ Î»ï¼Œå®ç°è‡ªé€‚åº”ç²¾åº¦-å»¶è¿Ÿæƒè¡¡ã€‚
2. **æ¢ç´¢è·¨å®¶æ— proxy**ï¼šç ”ç©¶ä¸åŒæ¶æ„é—´æ˜¯å¦ä¹Ÿå¯å»ºç«‹å¯é çš„ proxy-target å¯¹é½å…³ç³»ã€‚
3. **ç¡¬ä»¶æ„ŸçŸ¥è°ƒåº¦**ï¼šç»“åˆå†…å­˜å¸¦å®½ã€KV cache çŠ¶æ€ç­‰ç³»ç»Ÿå› ç´ ä¼˜åŒ– routing å†³ç­–ã€‚
4. **å¤šçº§ proxy cascade**ï¼šæ„å»º hierarchical proxy hierarchyï¼Œå®ç°æ›´ç»†ç²’åº¦çš„æˆæœ¬åˆ†å±‚å¸è½½ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯
> **TriSpec é€šè¿‡å¼•å…¥ lightweight proxy verifier å’Œ margin-based routingï¼Œåœ¨å‡ ä¹ä¸æŸå¤± accuracy çš„å‰æä¸‹ï¼Œå°† speculative decoding çš„éªŒè¯æˆæœ¬å¤§å¹…é™ä½ï¼Œå®ç°äº†é«˜è¾¾ 35% çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œä¸º LLM é«˜æ•ˆæ¨ç†æä¾›äº†æ–°èŒƒå¼ã€‚**

</details>

---

### 4. [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)

**Authors**: Julien Delavande, Regis Pierrard, Sasha Luccioni  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.22362v1  

#### Abstract
Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šUnderstanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
éšç€ Large Language Modelsï¼ˆLLMsï¼‰ä»ç ”ç©¶é˜¶æ®µè½¬å‘å¤§è§„æ¨¡ç”Ÿäº§éƒ¨ç½²ï¼Œ**æ¨ç†é˜¶æ®µçš„èƒ½æºæ¶ˆè€—**å·²æˆä¸ºAIå¯æŒç»­å‘å±•çš„å…³é”®ç“¶é¢ˆã€‚å°½ç®¡å·²æœ‰ç ”ç©¶å…³æ³¨è®­ç»ƒé˜¶æ®µçš„ç¢³è¶³è¿¹ï¼Œä½†å¯¹**ç³»ç»Ÿçº§è®¾è®¡é€‰æ‹©å¦‚ä½•å½±å“æ¨ç†èƒ½æ•ˆ**çš„ç†è§£ä»ä¸è¶³ã€‚

æœ¬æ–‡èšç„¦äºä»¥ä¸‹æ ¸å¿ƒé—®é¢˜ï¼š
- æ•°å€¼ç²¾åº¦ï¼ˆå¦‚ float32 vs int4ï¼‰ã€æ‰¹å¤„ç†ç­–ç•¥ï¼ˆbatchingï¼‰å’ŒæœåŠ¡æ¶æ„ï¼ˆå¦‚è¯·æ±‚è°ƒåº¦ï¼‰å¦‚ä½•å½±å“ LLM æ¨ç†è¿‡ç¨‹ä¸­çš„ **energy consumption å’Œ latency**ï¼Ÿ
- æ˜¯å¦å­˜åœ¨æŸäº›é…ç½®åœ¨ç†è®ºä¸ŠèŠ‚èƒ½ä½†åœ¨å®é™…ä¸­åè€Œå¢åŠ èƒ½è€—ï¼Ÿ

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
æœ¬ç ”ç©¶å¹¶æœªæå‡ºæ–°çš„æ¨¡å‹ç»“æ„ï¼Œè€Œæ˜¯é€šè¿‡**ç³»ç»Ÿçº§è§†è§’**æ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å› ç´ å¯¹ LLM èƒ½æºæ•ˆç‡çš„å½±å“æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§â€œ**ç›¸ä½æ„ŸçŸ¥çš„èƒ½æ•ˆåˆ†ææ¡†æ¶**â€ï¼ˆphase-aware energy profilingï¼‰ï¼Œå°†æ¨ç†åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè¿›è¡Œç‹¬ç«‹å»ºæ¨¡ä¸ä¼˜åŒ–ï¼š

1. **Prefill Phase**ï¼šå¤„ç†è¾“å…¥ prompt çš„å‰å‘ä¼ æ’­ã€‚
2. **Decode Phase**ï¼šè‡ªå›å½’ç”Ÿæˆè¾“å‡º tokenã€‚

åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…ç³»ç»Ÿåœ°è¯„ä¼°äº†ï¼š
- ä¸åŒ **numerical precision** åœ¨ä¸åŒè®¡ç®—èŒƒå¼ä¸‹çš„è¡¨ç°ï¼›
- **static batching** å¯¹ energy/token çš„å½±å“ï¼›
- ä½¿ç”¨ **Text Generation Inference (TGI)** æœåŠ¡æ—¶ï¼Œ**request arrival shaping** å¦‚ä½•æå‡ batching æ•ˆç‡å¹¶é™ä½èƒ½è€—ã€‚

æ­¤å¤–ï¼Œè®ºæ–‡å¼ºè°ƒäº†â€œ**the how of inference**â€â€”â€”å³æ¨ç†çš„æœåŠ¡æ–¹å¼æœ¬èº«ï¼ˆè€Œä¸ä»…ä»…æ˜¯æ¨¡å‹å¤§å°æˆ–å‚æ•°é‡ï¼‰æ˜¯å†³å®šèƒ½æ•ˆçš„å…³é”®å˜é‡ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ä¼ ç»Ÿåšæ³• | æœ¬æ–‡ä¼˜åŠ¿ |
|------|--------|---------|
| åˆ†æç²’åº¦ | æŠ¥å‘Šæ•´ä½“ energy/query | æŒ‰ **prefill / decode é˜¶æ®µæ‹†è§£**ï¼Œå®ç°ç»†ç²’åº¦è¯Šæ–­ |
| é‡åŒ–æ•ˆæœè¯„ä¼° | é»˜è®¤è®¤ä¸ºä½ç²¾åº¦ä¸€å®šæ›´é«˜æ•ˆ | æ­ç¤ºå…¶ä»…åœ¨ **compute-bound åœºæ™¯æœ‰æ•ˆ**ï¼Œmemory-bound ä¸‹å¯èƒ½é€‚å¾—å…¶å |
| æ‰¹å¤„ç†åˆ†æ | å¿½ç•¥ padding å¼€é”€ | åŒºåˆ† **effective vs computed tokens**ï¼Œæ­ç¤º padding æµªè´¹ |
| è¯·æ±‚æ¨¡å¼ | å‡è®¾éšæœºåˆ°è¾¾ | å¼•å…¥ **arrival shaping**ï¼Œå±•ç¤ºå›ºå®šé—´éš”å¯æå‡æ•ˆç‡ç™¾å€ |

> âœ… **æ ¸å¿ƒåˆ›æ–°**ï¼šé¦–æ¬¡å®è¯è¡¨æ˜ï¼Œ**ä»…é è°ƒæ•´è¯·æ±‚åˆ°è¾¾æ—¶é—´ï¼ˆarrival shapingï¼‰å³å¯å®ç°é«˜è¾¾ 100Ã— çš„ per-request energy reduction**ï¼Œä¸”æ— éœ€ä¿®æ”¹æ¨¡å‹æˆ–ç¡¬ä»¶ã€‚

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- æ•°æ®æ¥è‡ªä¸€ä¸ªåŒ¿åè¯„å®¡ä¸­çš„ç ”ç©¶ï¼ˆAnonymous & Anonymous, 2025ï¼‰ï¼Œåä¸º `ultrachat-10k` å­é›†ã€‚
- åŒ…å« **10,000 æ¡ç¤¼è²Œå‹ prompt**ï¼ˆä»¥ "thank you" ç»“å°¾ï¼‰ï¼Œæ¨¡æ‹ŸçœŸå®äººæœºå¯¹è¯åœºæ™¯ã€‚
- è¾“å…¥é•¿åº¦èŒƒå›´ï¼š**200â€“4000 tokens**ï¼ˆå‡å€¼çº¦ 1200ï¼‰
- è¾“å‡ºè¾ƒçŸ­ï¼š**10â€“300 tokens**

> æ‰€æœ‰ prompts å·²é€‚é…å„æ¨¡å‹çš„è¾“å…¥æ ¼å¼ï¼ˆå¦‚ chat templateï¼‰

### **å®éªŒè®¾ç½®**
#### **ç¡¬ä»¶å¹³å°**
- GPU: **NVIDIA H100 SXM 80GB**
- CPU: **8 Ã— AMD EPYC 7R13 cores**
- æ— å…¶ä»–å…±é©»ä»»åŠ¡ï¼ˆdedicated nodeï¼‰

#### **è½¯ä»¶æ ˆ**
- æ¨¡å‹åŠ è½½ï¼š`Transformers` + `bitsandbytes`ï¼ˆç”¨äº int8/int4 é‡åŒ–ï¼‰
- æœåŠ¡ç«¯æµ‹è¯•ï¼š`Hugging Face Text Generation Inference (TGI)` v3.3.4
- èƒ½è€—æµ‹é‡ï¼š`CodeCarbon`ï¼ˆé›†æˆ NVML å’Œ pyRAPLï¼‰å®æ—¶ç›‘æ§ GPU/CPU/RAM èƒ½è€—
- å†…å­˜ä¼°ç®—ï¼šRAM èƒ½è€—åŸºäº heuristic å…¬å¼æ¨ç®—

#### **è¢«æµ‹æ¨¡å‹**
- **Qwen 2.5**: 0.5B, 1.5B, 3B, 7B, 14B
- **Mistral-7B-Instruct-v0.3**
- **LLaMA 3.1-8B-Instruct**
- **LLaMA 3.1-70B-Instruct**ï¼ˆå¤šå¡æµ‹è¯•ï¼‰

#### **æ•°å€¼ç²¾åº¦è®¾ç½®ï¼ˆ5 ç§ dtypeï¼‰**
| ç±»å‹ | å®ç°æ–¹å¼ |
|------|--------|
| float32, bfloat16, float16 | PyTorch åŸç”Ÿæ”¯æŒ |
| int8 | LLM.int8()ï¼ˆoutlier-aware mixed precisionï¼‰ |
| int4 | NF4 æ ¼å¼ + on-the-fly dequantization |

#### **è¯„ä¼°æŒ‡æ ‡**
| æŒ‡æ ‡ | å®šä¹‰ |
|------|-----|
| **Energy per request** | å•ä¸ªè¯·æ±‚çš„æ€»èƒ½è€—ï¼ˆWhï¼‰ |
| **Energy per token** | åˆ†ä¸º input/output tokenï¼Œè¿›ä¸€æ­¥åŒºåˆ† effective vs padded |
| **Latency** | CUDA kernel çº§åˆ«è®°å½•ï¼Œå¹³å‡ 10 æ¬¡è¿è¡Œç»“æœ |
| **Batching Quality** | å¹³å‡ batch sizeã€idle timeã€kernel fusion ç¨‹åº¦ |

#### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| åŸºçº¿ | æè¿° |
|------|------|
| Baseline 1 | `transformers` åº“é»˜è®¤é¡ºåºæ‰§è¡Œï¼ˆno batchingï¼‰ |
| Baseline 2 | ä½¿ç”¨ float32 ç²¾åº¦ |
| TGI Baseline | å¯ç”¨ continuous batching å’Œ kernel fusion |
| Arrival Pattern | å¯¹æ¯”éšæœºå»¶è¿Ÿ vs å›ºå®šé—´éš”ï¼ˆ50ms, 300ms, 500msï¼‰ |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **(1) æ•°å€¼ç²¾åº¦çš„å½±å“**
| åœºæ™¯ | è§‚å¯Ÿç»“æœ |
|------|--------|
| **Prefill Phase**ï¼ˆå¤§æ¨¡å‹ï¼‰ | float16/bfloat16 ç›¸æ¯” float32 å¯å‡å°‘ **~4Ã— GPU energy**ï¼ˆå› å¯ç”¨ Tensor Coresï¼‰ |
| **Prefill Phase**ï¼ˆå°æ¨¡å‹ï¼‰ | å°æ¨¡å‹ï¼ˆå¦‚ Qwen-0.5Bï¼‰ä¸º memory-boundï¼Œä½ç²¾åº¦æ— ç›Šç”šè‡³è½»å¾®å¢è€— |
| **Decode Phase** | float32 / float16 / bfloat16 èƒ½è€—å‡ ä¹ä¸å˜ï¼›**int8 åè€Œé«˜å‡º 2â€“3Ã—**ï¼ˆdue to dequantization overheadï¼‰ |
| **int4 è¡¨ç°** | ä¸ float32 ç›¸å½“ï¼Œæœªè§æ˜¾è‘—èŠ‚èƒ½ï¼ˆå°¤å…¶ decode é˜¶æ®µï¼‰ |

> ğŸ“‰ å›¾1 æ˜¾ç¤ºï¼šdecode é˜¶æ®µ energy/token å‡ ä¹ä¸éš dtype æ”¹å˜ï¼Œè¯´æ˜å¸¦å®½é™åˆ¶ä¸‹å‹ç¼©æ— æ•ˆã€‚

#### **(2) æ‰¹å¤„ç†ï¼ˆBatchingï¼‰çš„å½±å“**
| æŒ‡æ ‡ | å‘ç° |
|------|------|
| **Energy per effective input token (prefill)** | éš batch size ä¸Šå‡è€Œä¸Šå‡ â†’ å›  **padding å¯¼è‡´å†—ä½™è®¡ç®—** |
| **Energy per computed input token (prefill)** | åŸºæœ¬æ’å®š â†’ ç¬¦åˆ compute-bound ç‰¹æ€§ |
| **Energy per output token** | éš batch size å¢åŠ æŒç»­ä¸‹é™ï¼Œ**logarithmic trend**ï¼Œæœ€å¤§èŠ‚çœè¾¾ **~65%**ï¼ˆvs b=1ï¼‰ |
| **Optimal batch size** | decode é˜¶æ®µåœ¨ **b=4 è¾¾åˆ°æœ€ä¼˜**ï¼Œæ›´å¤§ batch æ”¶ç›Šé€’å‡ |

> âš–ï¸ Trade-offï¼šprefill å— padding æ‹–ç´¯ï¼Œdecode å—å¹¶è¡Œæ”¶ç›Šæ¨åŠ¨ã€‚

#### **(3) TGI + Arrival Shaping çš„æ•ˆæœ**
| è®¾ç½® | LLaMA 8B per-request energy | æå‡å€æ•° |
|------|-------------------------------|----------|
| transformers (float32, no batching) | 1.2 Ã— 10â»Â¹ Wh | 1Ã—ï¼ˆbaselineï¼‰ |
| TGIï¼ˆrandom arrivalï¼‰ | 9.6 Ã— 10â»Â³ Wh | **~12.5Ã— æ›´ä¼˜** |
| TGI + fixed 500ms interval | **1.1 Ã— 10â»Â³ Wh** | **~100Ã— æ›´ä¼˜** |

> ğŸ’¡ è¿™æ„å‘³ç€ï¼š**ä»…é€šè¿‡æ§åˆ¶å®¢æˆ·ç«¯è¯·æ±‚èŠ‚å¥ï¼ˆarrival shapingï¼‰ï¼Œå°±èƒ½å®ç°ç™¾å€èŠ‚èƒ½ï¼**

#### **(4) å¤§è§„æ¨¡æ‰©å±•éªŒè¯ï¼ˆLLaMA 70Bï¼‰**
- ä½¿ç”¨ 4Ã— H100 GPUs
- TGI ä»å¯å°† energy per request æ§åˆ¶åœ¨ **2.4 Ã— 10â»Â² Wh**
- **ä¼˜äº 8B æ¨¡å‹åœ¨åŸå§‹è®¾ç½®ä¸‹çš„è¡¨ç°**ï¼Œè¯æ˜åŠ¨æ€ batching å’Œ traffic shaping å¯è‰¯å¥½æ‰©å±•

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. ğŸ”¹ **Numerical precision reduction only helps in compute-bound regimes**
   - Prefill é˜¶æ®µï¼ˆå°¤å…¶æ˜¯å¤§æ¨¡å‹ï¼‰å—ç›Šæ˜æ˜¾ï¼ˆTensor Core åŠ é€Ÿï¼‰
   - Decode é˜¶æ®µå§‹ç»ˆ memory-boundï¼Œ**aggressive quantizationï¼ˆå¦‚ int8/int4ï¼‰ä¼šå¼•å…¥é¢å¤– kernel launch å’Œ idle timeï¼Œåè€Œå¢åŠ èƒ½è€—**

2. ğŸ”¹ **Batching improves energy efficiency â€” but mechanism differs by phase**
   - Prefillï¼šå—é™äº paddingï¼Œenergy/effective-token å¯èƒ½æ¶åŒ–
   - Decodeï¼šå¼ºçƒˆå—ç›Šäº batchingï¼Œå› å…±äº« KV cache å’Œ memory access
   - Output-token normalization æ˜¯æ›´åˆç†çš„è¯„ä¼°æ–¹å¼

3. ğŸ”¹ **Serving infrastructure and request scheduling matter enormously**
   - ä» `transformers` åˆ° `TGI` å¯å¸¦æ¥ **12.5Ã— èŠ‚èƒ½**
   - å†åŠ ä¸Š **arrival shapingï¼ˆå›ºå®šé—´éš”ï¼‰å¯è¾¾ 100Ã— èŠ‚èƒ½**
   - â€œHow you serveâ€ æ¯” â€œWhat you serveâ€ æ›´é‡è¦

4. ğŸ”¹ **Phase-aware profiling is essential**
   - Prefill å’Œ decode å…·æœ‰æ ¹æœ¬ä¸åŒçš„è®¡ç®—ç‰¹å¾ï¼ˆcompute-bound vs memory-boundï¼‰
   - ç»Ÿä¸€æŠ¥å‘Š aggregate energy ä¼šæ©ç›–å…³é”®ç“¶é¢ˆ

5. ğŸ”¹ **Idle power is a major contributor**
   - GPU idle power â‰ˆ 120Wï¼Œå³ä½¿çŸ­æš‚ç©ºé—²ä¹Ÿæ¶ˆè€—å¯è§‚èƒ½é‡
   - Kernel fragmentationï¼ˆå¦‚ dequantization kernelsï¼‰åŠ å‰§æ­¤é—®é¢˜

---

### **æ–¹æ³•çš„å±€é™æ€§**
| å±€é™ | è¯´æ˜ |
|------|------|
| **Promptå¤šæ ·æ€§æœ‰é™** | ä½¿ç”¨çš„æ˜¯çŸ­ prompt + çŸ­ response åœºæ™¯ï¼Œæœªæ¶µç›–é•¿ä¸Šä¸‹æ–‡æˆ–å¤šè½®å¤æ‚äº¤äº’ |
| **ä»…åŸºäº H100 æµ‹è¯•** | ç»“æœå¯èƒ½æ— æ³•ç›´æ¥è¿ç§»åˆ° AMD Instinctã€AWS Inferentia æˆ– TPU ç­‰å¹³å° |
| **å¿½ç•¥ç³»ç»Ÿçº§å¼€é”€** | å½“å‰ energy æµ‹é‡é›†ä¸­äº GPUï¼Œæœªå……åˆ†è®¡å…¥ CPUã€å†…å­˜ä¼ è¾“ã€ç½‘ç»œ I/O ç­‰æˆæœ¬ |
| **é™æ€ batching vs åŠ¨æ€ batching** | å®éªŒä¸­ prefill ä½¿ç”¨ static batchingï¼Œæœªèƒ½å®Œå…¨åæ˜  vLLM æˆ– TGI çš„ full potential |

---

### **æœªæ¥å·¥ä½œæ–¹å‘**
1. **è·¨ç¡¬ä»¶å¹³å°è¿ç§»æ€§ç ”ç©¶**  
   æ‰©å±•è‡³ AMDã€Apple Siliconã€Google TPU ç­‰è®¾å¤‡ï¼Œå»ºç«‹é€šç”¨èƒ½æ•ˆé¢„æµ‹æ¨¡å‹ã€‚

2. **å…¨æ ˆèƒ½æ•ˆå»ºæ¨¡ï¼ˆFull-stack Energy Modelingï¼‰**  
   æ•´åˆ GPU + CPU + RAM + Network çš„è”åˆèƒ½è€—æ¨¡å‹ï¼Œæä¾› end-to-end footprint è¯„ä¼°ã€‚

3. **å®¢æˆ·ç«¯ååŒä¼˜åŒ–ï¼ˆClient-Aware Schedulingï¼‰**  
   è®¾è®¡è½»é‡çº§åè®®å¼•å¯¼ç”¨æˆ·/å‰ç«¯æŒ‰æœ€ä¼˜èŠ‚å¥å‘é€è¯·æ±‚ï¼ˆe.g., rate limiting hintsï¼‰ã€‚

4. **åŠ¨æ€ç›¸ä½åˆ‡æ¢ä¼˜åŒ–ï¼ˆPhase-adaptive Servingï¼‰**  
   æ ¹æ®å½“å‰ workload è‡ªåŠ¨åˆ‡æ¢ precision / batching strategyï¼ˆå¦‚ prefill ç”¨ int4ï¼Œdecode ç”¨ bfloat16ï¼‰ã€‚

5. **æ ‡å‡†åŒ–èƒ½æ•ˆæŠ¥å‘Šè§„èŒƒ**  
   æ¨åŠ¨ç¤¾åŒºé‡‡ç”¨ phase-separatedã€token-normalizedã€arrival-pattern-aware çš„ energy reporting protocolã€‚

---

> âœ… **æœ€ç»ˆæ´è§**ï¼š  
> â€œå¯æŒç»­çš„ LLM éƒ¨ç½²ä¸ä»…å–å†³äºæ¨¡å‹æœ¬èº«ï¼Œæ›´å–å†³äºæ•´ä¸ª serving stack çš„ååŒè®¾è®¡ã€‚â€  
> â€”â€” **Efficiency is not just about the model, but about the orchestration.**

</details>

---

### 5. [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)

**Authors**: Andrei Panferov, Erik Schultheis, Soroush Tabesh, Dan Alistarh  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.22813v1  

#### Abstract
The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰åŸºäº **NVFP4** çš„å…¨é‡åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒè™½ç„¶èƒ½æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å¹¶æå‡ååé‡ï¼Œä½†åœ¨ç²¾åº¦ä¸Šä»æ˜æ˜¾è½åäº FP16/FP8 è®­ç»ƒã€‚å…¶ä¸»è¦åŸå› åœ¨äºï¼š
- **å‰å‘ä¼ æ’­**ä¸­ä¸ºèŠ‚çœé‡åŒ–å¼€é”€è€Œé‡‡ç”¨çš„â€œsquare-block quantizationâ€ç‰ºç‰²äº†è¡¨ç¤ºèƒ½åŠ›ï¼ˆrepresentation capacityï¼‰ï¼Œå¯¼è‡´æ¿€æ´»å€¼å’Œæƒé‡çš„é‡åŒ–è¯¯å·®å¢å¤§ï¼›
- **åå‘ä¼ æ’­**ä¸­å¹¿æ³›ä½¿ç”¨çš„ **Stochastic Rounding (SR)** è™½ç„¶ä¿è¯æ— åæ€§ï¼ˆunbiasednessï¼‰ï¼Œä½†å¼•å…¥äº†è¿‡é«˜çš„æ–¹å·®ï¼Œæ˜¾è‘—å¢åŠ äº†æ¢¯åº¦ä¼°è®¡è¯¯å·®ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯

#### ï¼ˆ1ï¼‰**MicroScaling EDEN (MS-EDEN)**  
ä¸€ç§å…¨æ–°çš„ã€é€‚ç”¨äºå¾®ç¼©æ”¾æ ¼å¼ï¼ˆmicroscaling formatsï¼‰å¦‚ NVFP4 çš„**æ— åé‡åŒ–åŸè¯­ï¼ˆunbiased quantization primitiveï¼‰**ï¼š
- å°†éšæœºæ€§ä»å•ä¸ª FP4 å…ƒç´ è½¬ç§»åˆ°æ›´é«˜ç²¾åº¦çš„ **microscale å› å­ï¼ˆFP8 scaleï¼‰** ä¸Šï¼›
- åˆ©ç”¨ **Randomized Hadamard Transform (RHT)** è¿›è¡Œåˆ†å¸ƒå¹³æ»‘ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå¯è¯æ˜æ— åçš„æ ¡æ­£å› å­ $ S $ æ¥è¡¥å¿é‡åŒ–åå·®ï¼›
- åœ¨ FP8 scale å±‚é¢ä½¿ç”¨ **Stochastic Rounding** å®ç°å¯¹æ ¡æ­£å› å­çš„æ— æŸç¼–ç ï¼Œä»è€Œåœ¨ç¡¬ä»¶å…¼å®¹çš„å‰æä¸‹ä¿æŒæ•´ä½“æœŸæœ›æ— åã€‚

> ğŸ’¡ **æ ¸å¿ƒæ€æƒ³**ï¼šé¿å…åœ¨ä½ç²¾åº¦ FP4 å±‚é¢å¼•å…¥é«˜æ–¹å·®çš„éšæœºèˆå…¥ï¼Œè½¬è€Œåœ¨ç›¸å¯¹é«˜ç²¾åº¦çš„ scale ä¸Šè¿›è¡Œå¯æ§çš„éšæœºåŒ–ï¼Œå¤§å¹…é™ä½ MSEã€‚

#### ï¼ˆ2ï¼‰**Quartet IIï¼šå…¨ NVFP4 çº¿æ€§å±‚è®­ç»ƒå›¾**
æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ã€ç«¯åˆ°ç«¯çš„ NVFP4 è®­ç»ƒæ–¹æ¡ˆï¼š
- **å‰å‘ä¼ æ’­**ï¼šä½¿ç”¨ **Round-to-Nearest (RTN)** + **FourOverSix (4/6)** scale selection å¯å‘å¼ï¼Œæœ€å¤§åŒ–è¡¨ç¤ºèƒ½åŠ›ï¼›
- **åå‘ä¼ æ’­**ï¼šé‡‡ç”¨ **MS-EDEN** è¿›è¡Œæ¢¯åº¦é‡åŒ–ï¼Œç»“åˆå†…ç»´åº¦çš„ RHT æ—‹è½¬ï¼Œå®ç°é«˜è´¨é‡æ— åæ¢¯åº¦ä¼°è®¡ï¼›
- æ”¾å¼ƒâ€œsquare-block quantizationâ€ï¼Œå…è®¸æƒé‡å’Œæ¿€æ´»åœ¨åå‘ä¼ æ’­æ—¶é‡æ–°é‡åŒ–ï¼Œä»¥æ¢å–æ›´å¥½çš„å‰å‘è¡¨ç¤ºå’Œæ›´ä¼˜çš„æ•´ä½“æ€§èƒ½ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| ç»´åº¦ | ä¼˜åŠ¿è¯´æ˜ |
|------|--------|
| **é‡åŒ–è¯¯å·®** | MS-EDEN çš„ MSE æ¯” SR é™ä½ **è¶…è¿‡ 2 å€**ï¼ˆè§ Table 1ï¼‰ï¼› |
| **å‰å‘è¡¨ç¤ºèƒ½åŠ›** | ä½¿ç”¨ native 1x16 åˆ†ç»„è€Œé 16x16 square-blockï¼Œä¿ç•™æ›´å¤šåŠ¨æ€èŒƒå›´ç»†èŠ‚ï¼› |
| **æ¢¯åº¦è´¨é‡** | åœ¨ä¿è¯æ— åæ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—ä¼˜äº SR å’Œ TetraJet-v2ï¼› |
| **ç³»ç»Ÿæ•ˆç‡** | æä¾›é«˜æ•ˆ CUDA kernelï¼Œæ”¯æŒ Blackwell GPUï¼Œå®æµ‹è®­ç»ƒé€Ÿåº¦è¾¾ BF16 çš„ **4.2x**ï¼› |
| **ååŒä¼˜åŒ–** | ä¸ â€œ4/6â€ ç­‰æŠ€æœ¯æ­£äº¤ä¸”äº’è¡¥ï¼Œå½¢æˆæ›´å¼ºç»„åˆã€‚ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **ä¸»å®éªŒ**ï¼š`C4` æ•°æ®é›†ï¼Œç”¨äº Llama-2 æ¶æ„é£æ ¼æ¨¡å‹çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼›
- **å¤§è§„æ¨¡éªŒè¯**ï¼š`FineWeb-Edu` æ•°æ®é›†ï¼Œé…åˆ `Nanochat` è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«åç»­ fine-tuning é˜¶æ®µï¼›
- æµ‹è¯•åŸºå‡†åŒ…æ‹¬ï¼š`ARC`, `GSM8K`, `HumanEval`, `MMLU` ç­‰é›¶æ ·æœ¬è¯„æµ‹ä»»åŠ¡ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹æ¶æ„**ï¼šTransformer-basedï¼Œéµå¾ª Llama-2 è®¾è®¡ï¼›
- **å‚æ•°è§„æ¨¡**ï¼šä»å°å‹ï¼ˆ30Mâ€“200Mï¼‰åˆ°å¤§å‹ï¼ˆ560M, 1.9Bï¼‰ä¸ç­‰ï¼›
- **è®­ç»ƒé•¿åº¦**ï¼šæœ€é•¿è¾¾ **38B tokens**ï¼Œè¦†ç›– compute-optimal è‡³ over-trained åŒºåŸŸï¼›
- **ä¼˜åŒ–å™¨**ï¼šAdamW / Muonï¼Œcosine å­¦ä¹ ç‡è°ƒåº¦ï¼›
- **è¶…å‚ä¸€è‡´æ€§**ï¼šæ‰€æœ‰ QAT å®éªŒå¤ç”¨ BF16 åŸºçº¿çš„è¶…å‚æ•°ï¼ˆLRã€weight decay ç­‰ï¼‰ã€‚

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **Validation Loss / Perplexity** | ä¸»è¦è¡¡é‡é¢„è®­ç»ƒé˜¶æ®µçš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼› |
| **Bits-per-byte (BPB)** | Nanochat ä¸­çš„å…³é”®å‹ç¼©æŒ‡æ ‡ï¼Œè¶Šä½è¶Šå¥½ï¼› |
| **Zero-shot Accuracy (%)** | åœ¨ ARCã€GSM8K ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼› |
| **Relative Loss Gap vs BF16** | è¡¡é‡é‡åŒ–æ–¹æ³•ç›¸å¯¹äºå…¨ç²¾åº¦è®­ç»ƒçš„é€€åŒ–ç¨‹åº¦ï¼› |
| **Speedup over BF16** | å®é™… kernel å±‚é¢å’Œç«¯åˆ°ç«¯è®­ç»ƒçš„åŠ é€Ÿæ¯”ã€‚ |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **NVIDIA et al. (2025)**ï¼šé¦–ä¸ªç«¯åˆ°ç«¯ NVFP4 é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿ç”¨ square-block + SRï¼›
- **TetraJet-v2 (Chen et al., 2025b)**ï¼šæ”¹è¿›ç‰ˆï¼ŒåŠ å…¥ outlier æ§åˆ¶ä¸æŒ¯è¡æŠ‘åˆ¶ï¼›
- **FourOverSix (Cook et al., 2025)**ï¼šæå‡ºè‡ªé€‚åº”é€‰æ‹© scale ç½‘æ ¼ä»¥å‡å°‘ MSEï¼›
- **Stochastic Rounding (SR)**ï¼šå½“å‰ä¸»æµçš„æ— åé‡åŒ–æ‰‹æ®µï¼Œä½œä¸ºåå‘ä¼ æ’­åŸºçº¿ã€‚

> æ³¨ï¼šä½œè€…æŒ‡å‡º TetraJet-v2 çš„éƒ¨åˆ†è®¾è®¡ï¼ˆå¦‚ä¸­é—´ FP32 scaleï¼‰éš¾ä»¥åœ¨ GPU ä¸Šé«˜æ•ˆå®ç°ï¼Œå› æ­¤ä»…æ¯”è¾ƒå…¶â€œGPU å¯è¡Œç‰ˆæœ¬â€ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **MS-EDEN vs SR çš„ MSE** | MS-EDEN: **9.8Ã—10â»Â³**ï¼ŒSR: **23.5Ã—10â»Â³** â†’ **é™ä½ >2x**ï¼ˆTable 1ï¼‰ |
| **æœ€å¤§è®­ç»ƒåŠ é€Ÿæ¯”** | è¾¾åˆ° **4.2x over BF16**ï¼ˆFigure 6ï¼‰ |
| **çœŸå®è®­ç»ƒååæå‡** | 1.1B æ¨¡å‹ä¸Šè¾¾åˆ° **2.45x** å®é™… token/s æå‡ï¼ˆAppendix Dï¼‰ |
| **é¢„è®­ç»ƒæŸå¤±ç¼©å‡** | ç›¸æ¯” TetraJet-v2 å¹³å‡ç¼©å° **15â€“25%** çš„ BF16 å·®è·ï¼ˆFigure 5ï¼‰ |

### ğŸ” ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

#### ï¼ˆ1ï¼‰**å®Œæ•´è®­ç»ƒæ€§èƒ½ï¼ˆFigure 4ï¼‰**
- Quartet II åœ¨ä¸åŒå‚æ•°é‡ï¼ˆ30Mâ€“200Mï¼‰å’Œæ•°æ®æ¯”ä¾‹ä¸‹ï¼Œ**å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿**ï¼›
- ç›¸æ¯” TetraJet-v2ï¼Œå¹³å‡ **loss gap ç¼©å°è‡³å°‘ 20%**ï¼›
- ç‰¹åˆ«æ˜¯åœ¨é«˜ D/Nï¼ˆover-trainedï¼‰åŒºåŸŸï¼Œä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚

#### ï¼ˆ2ï¼‰**Nanochat å¤§è§„æ¨¡è®­ç»ƒï¼ˆFigure 5 & Table 5ï¼‰**
- åœ¨ 1.9B å‚æ•°ã€38B tokens è®¾ç½®ä¸‹ï¼š
  - **Val BPB æå‡æ˜¾è‘—**ï¼šQuartet II è¾¾åˆ° 0.7025ï¼Œä¼˜äº TetraJet-v2 çš„ 0.7044ï¼›
  - **ç›¸å¯¹ BF16 çš„ BPB å¢åŠ æœ€å°**ï¼šä»… **+1.44%**ï¼Œè€Œ TetraJet-v2 ä¸º +1.72%ï¼ŒNVIDIA ä¸º +1.92%ï¼›
- Zero-shot æ€§èƒ½å·®å¼‚ä¸æ˜¾è‘—ï¼Œå¯èƒ½å—é™äºå¾®è°ƒæ•°æ®é‡å’Œæµ‹è¯•é›†å¤§å°ã€‚

### ğŸ” æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰**åå‘ä¼ æ’­é‡åŒ–ç­–ç•¥åˆ†æï¼ˆFigure 1ï¼‰**
- å½“åŒæ—¶é‡åŒ–ä¸¤ä¸ª GEMM è¾“å…¥å¼ é‡æ—¶ï¼ˆå³ fully quantized backwardï¼‰ï¼š
  - MS-EDEN with weight re-quantization (**e**) æ˜æ˜¾ä¼˜äº SR without re-quantization (**d**)ï¼›
  - è¯æ˜å³ä½¿å¢åŠ  re-quantization å¼€é”€ï¼ŒMS-EDEN ä»èƒ½æä¾›æ›´ä¼˜æ¢¯åº¦ä¼°è®¡ã€‚

#### ï¼ˆ2ï¼‰**å‰å‘ä¼ æ’­ç­–ç•¥åˆ†æï¼ˆFigure 2ï¼‰**
- â€œ4/6â€ æŠ€æœ¯åœ¨ native 1x16 åˆ†ç»„ä¸‹æ•ˆæœæœ€ä½³ï¼Œç›¸æ¯” square-block æå‡çº¦ **ä¸¤å€**ï¼›
- native åˆ†ç»„æœ¬èº«ä¹Ÿä¼˜äº square-blockï¼Œè¯´æ˜æ›´å¥½çš„è¡¨ç¤ºèƒ½åŠ›è‡³å…³é‡è¦ã€‚

#### ï¼ˆ3ï¼‰**æ— åæ€§éªŒè¯ï¼ˆFigure 9ï¼‰**
- å¯¹å¹³å‡æ¢¯åº¦çš„æ”¶æ•›è¡Œä¸ºè¿›è¡Œæµ‹è¯•ï¼š
  - Quartet IIã€NVIDIAã€TetraJet-v2 çš„è¯¯å·®éš accumulation steps ä¸‹é™ç¬¦åˆ $ \sim 1/B $ï¼Œè¡¨æ˜**çœŸæ­£æ— å**ï¼›
  - â€œ4/6 + SRâ€ å‡ºç° plateauï¼Œè¯´æ˜å…¶åœ¨åå‘ä¼ æ’­ä¸­**å¼•å…¥ç³»ç»Ÿæ€§åå·®**ï¼Œä¸é€‚åˆç”¨äº backward passã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Stochastic Rounding ä¸æ˜¯æœ€ä¼˜è§£**ï¼šå°½ç®¡è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†åœ¨ NVFP4 åœºæ™¯ä¸‹æ–¹å·®è¿‡é«˜ï¼Œé™åˆ¶äº†ç²¾åº¦ä¸Šé™ï¼›
2. **MS-EDEN æ›´ä¼˜**ï¼šé€šè¿‡å°†éšæœºæ€§ç§»è‡³ scale å±‚é¢ï¼Œå¯åœ¨ä¿æŒæ— åçš„åŒæ—¶å¤§å¹…é™ä½ MSEï¼›
3. **æ”¾å¼ƒ square-block æ˜¯å€¼å¾—çš„**ï¼šè™½ç„¶éœ€ re-quantize æƒé‡ï¼Œä½†æ¢æ¥å‰å‘è¡¨ç¤ºèƒ½åŠ›å’Œæ•´ä½“æ€§èƒ½çš„å‡€å¢ç›Šï¼›
4. **â€œ4/6â€ åº”ç”¨äºå‰å‘ä¼ é€’æœ€æœ‰æ•ˆ**ï¼šä¸åº”ä¸ SR è”åˆç”¨äº backwardï¼Œå¦åˆ™ç ´åæ— åæ€§ï¼›
5. **è½¯ç¡¬ååŒè®¾è®¡å¯è¡Œ**ï¼šæå‡ºçš„ kernel æ”¯æŒå·²åœ¨ Blackwell GPU ä¸ŠéªŒè¯ï¼Œå…·å¤‡å®ç”¨ä»·å€¼ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ– RHT æ—‹è½¬**ï¼šè¦æ±‚é‡åŒ–å—æ˜¯æ—‹è½¬å—çš„å­é›†ï¼Œé™åˆ¶äº†æŸäº›å¹¶è¡Œç­–ç•¥ï¼›
- **é¢å¤–è®¡ç®—å¼€é”€**ï¼šRHT å’Œä¸¤æ¬¡ quantization/dequantization å¼•å…¥ä¸€å®šå»¶è¿Ÿï¼ˆä½†å¯é€šè¿‡ fused kernel ç¼“è§£ï¼‰ï¼›
- **ç›®å‰ä»…é™çº¿æ€§å±‚**ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å…¶ä»–ç»„ä»¶æœªå®Œå…¨é‡åŒ–ï¼›
- **zero-shot è¡¨ç°å·®å¼‚ä¸å¤§**ï¼šå¯èƒ½å› å¾®è°ƒé˜¶æ®µå…³é—­äº† backward quantization æ‰€è‡´ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- å°† MS-EDEN æ¨å¹¿è‡³ **MXFP4** æˆ–å…¶ä»–å¾®ç¼©æ”¾æ ¼å¼ï¼›
- æ¢ç´¢ **éå‡åŒ€åˆ†ç»„ç­–ç•¥** ä»¥è¿›ä¸€æ­¥ä¼˜åŒ– outlier å¤„ç†ï¼›
- å®ç° **å®Œå…¨èåˆçš„è®­ç»ƒ kernel**ï¼Œæ¶ˆé™¤ memory boundï¼›
- å°†è¯¥æ¡†æ¶æ‰©å±•è‡³ **fine-tuning å’Œæ¨ç†é˜¶æ®µ**ï¼Œæ‰“é€ ç»Ÿä¸€çš„ 4-bit è®­ç»ƒ-éƒ¨ç½²æµæ°´çº¿ï¼›
- æ¢ç´¢ **æ›´ä½æ¯”ç‰¹ï¼ˆå¦‚ 2-bitï¼‰ä¸‹çš„ç±»ä¼¼æ„é€ **ã€‚

---

> âœ… **ä¸€å¥è¯æ€»ç»“**ï¼š  
> *Quartet II é€šè¿‡æå‡ºæ–°å‹æ— åé‡åŒ–æ–¹æ³• MS-EDENï¼Œè§£å†³äº† NVFP4 è®­ç»ƒä¸­æ¢¯åº¦ä¼°è®¡è¯¯å·®å¤§çš„é—®é¢˜ï¼Œåœ¨ä¸ç‰ºç‰²ç¨³å®šæ€§çš„å‰æä¸‹æ˜¾è‘—æå‡äº†æ¨¡å‹ç²¾åº¦ï¼Œå¹¶å®ç°äº†é«˜è¾¾ 4.2x çš„è®­ç»ƒåŠ é€Ÿï¼Œæ¨åŠ¨äº†å…¨ 4-bit LLM é¢„è®­ç»ƒè¿ˆå‘å®ç”¨åŒ–ã€‚*

</details>

---

### 6. [SPLA: Block Sparse Plus Linear Attention for Long Context Modeling](https://arxiv.org/abs/2601.22379)

**Authors**: Bailin Wang, Dan Friedman, Tao Lei, Chong Wang  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.22379v1  

#### Abstract
Block-wise sparse attention offers significant efficiency gains for long-context modeling, yet existing methods often suffer from low selection fidelity and cumulative contextual loss by completely discarding unselected blocks. To address these limitations, we introduce Sparse Plus Linear Attention ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šSPLA: Block Sparse Plus Linear Attention for Long Context Modeling**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
å½“å‰åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­ï¼Œ**Block-wise Sparse Attention**ï¼ˆå¦‚ NSAã€InfLLMï¼‰è™½ç„¶æå‡äº†æ¨ç†æ•ˆç‡ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼š
1. **ä½é€‰æ‹©ä¿çœŸåº¦ï¼ˆLow Selection Fidelityï¼‰**ï¼šåŸºäºå‹ç¼©è¡¨ç¤ºï¼ˆå¦‚å‡å€¼æ± åŒ–ï¼‰çš„å—é€‰æ‹©æœºåˆ¶éš¾ä»¥å‡†ç¡®è¯†åˆ«çœŸæ­£ç›¸å…³çš„ KV Cache å—ï¼Œå¯¼è‡´å…³é”®è¯­ä¹‰ä¿¡æ¯è¢«é—æ¼ã€‚
2. **ç´¯ç§¯ä¸Šä¸‹æ–‡ä¸¢å¤±ï¼ˆCumulative Contextual Lossï¼‰**ï¼šé€šè¿‡ç¡¬æˆªæ–­ï¼ˆhard truncationï¼‰ä¸¢å¼ƒæœªé€‰ä¸­çš„â€œé•¿å°¾â€å—ï¼Œéšç€åºåˆ—å¢é•¿ï¼Œè¿™äº›è¢«å¿½ç•¥çš„æ¦‚ç‡è´¨é‡é€æ¸ç´¯ç§¯ï¼Œå¯¼è‡´è¾“å‡ºä¸¥é‡åç¦»å…¨æ³¨æ„åŠ›ï¼ˆdense attentionï¼‰æ¨¡å‹ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
æœ¬æ–‡æå‡º **SPLA**ï¼ˆSparse Plus Linear Attentionï¼‰ï¼Œä¸€ç§ç»“åˆ**ç²¾ç¡®ç¨€ç–æ³¨æ„åŠ›**ä¸**æ®‹å·®çº¿æ€§æ³¨æ„åŠ›**ï¼ˆResidual Linear Attention, RLAï¼‰çš„æ–°æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸Šä¸‹æ–‡åˆ’åˆ†ä¸ºä¸¤ä¸ªäº’è¡¥éƒ¨åˆ†ï¼š
- **Exact Component**ï¼šå¯¹é«˜ç›¸å…³æ€§çš„â€œå³°å€¼â€å—æ‰§è¡Œæ ‡å‡†çš„ç¨€ç–æ³¨æ„åŠ›ã€‚
- **Approximate Component**ï¼šå¯¹å‰©ä½™çš„â€œé•¿å°¾â€å—é€šè¿‡ RLA è¿›è¡Œé«˜æ•ˆå‹ç¼©å¹¶ä¿ç•™å…¶å…¨å±€å½±å“ã€‚

å…·ä½“åˆ›æ–°åŒ…æ‹¬ï¼š
- **åŸºäºäºŒé˜¶æ³°å‹’å±•å¼€çš„å—é€‰æ‹©æœºåˆ¶**ï¼šåˆ©ç”¨æ¯ä¸ªå—çš„å‡å€¼å’Œåæ–¹å·®ç»Ÿè®¡é‡æ„å»ºæ›´ç²¾ç¡®çš„é€‰æ‹©åˆ†æ•°ï¼Œå»ºç«‹ä» token-level æ³¨æ„åŠ›åˆ° block-level æ£€ç´¢çš„æ•°å­¦æ¡¥æ¢ã€‚
- **æ— éœ€æ˜¾å¼è®¿é—®çš„æ®‹å·®çº¿æ€§æ³¨æ„åŠ›ï¼ˆRLAï¼‰**ï¼šé€šè¿‡å‡æ³•å½¢å¼å®ç° RLAï¼Œå³ `o_residual = o_global_linear - o_selected_linear`ï¼Œé¿å…åœ¨æ¨ç†æ—¶æ˜¾å¼è¯»å–æœªé€‰å—ï¼Œä¿æŒ I/O æ•ˆç‡ã€‚
- **é›¶æ–°å¢æŠ•å½±å‚æ•°çš„è®¾è®¡**ï¼šå…±äº«åŸå§‹ Q/K/V æŠ•å½±æƒé‡ï¼Œä»…å¼•å…¥ä¸€ä¸ªæ— å‚æ•°çš„æŒ‡æ•°ç‰¹å¾æ˜ å°„ï¼ˆexponential feature mapï¼‰ï¼Œç¡®ä¿å¯ç›´æ¥é€‚é…é¢„è®­ç»ƒæ¨¡å‹ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| å¯¹æ¯”ç»´åº¦ | SPLA | NSA / InfLLM |
|--------|------|-------------|
| **é€‰æ‹©ç²¾åº¦** | é«˜ï¼ˆåˆ©ç”¨äºŒé˜¶ç»Ÿè®¡ï¼‰ | ä½ï¼ˆä¾èµ–ä¸€é˜¶å‡å€¼æˆ–å¯å‘å¼ï¼‰ |
| **ä¸Šä¸‹æ–‡å®Œæ•´æ€§** | ä¿ç•™å…¨éƒ¨ä¸Šä¸‹æ–‡ï¼ˆé€šè¿‡ RLA å‹ç¼©é•¿å°¾ï¼‰ | æˆªæ–­é•¿å°¾ï¼Œé€ æˆä¿¡æ¯æŸå¤± |
| **I/O å¼€é”€** | ä¸å¢åŠ ï¼ˆæœªé€‰å—ä¸è®¿é—®ï¼‰ | åŒç­‰ç¨€ç–æ¨¡å¼ä¸‹ç›¸å½“ |
| **é€‚é…æˆæœ¬** | æä½ï¼ˆä»…å¢åŠ å°‘é‡ RMSNorm å‚æ•°ï¼‰ | å¯èƒ½éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
å®éªŒå›´ç»• **Continual Pretraining (CPT)** åœºæ™¯å±•å¼€ï¼Œæ¶µç›–ä¸‰ç±»ä»»åŠ¡ï¼š
1. **General Knowledge**ï¼šæ··åˆæ•°å­¦ã€ä»£ç å’Œé€šç”¨çŸ¥è¯†é¢†åŸŸï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¸º 32kã€‚
2. **Long-Context**ï¼šé•¿æ–‡æ¡£ä¸åˆæˆé—®ç­”å¯¹ï¼Œç”¨äºè¯„ä¼° RULER åŸºå‡†ä¸Šçš„é•¿ç¨‹å»ºæ¨¡èƒ½åŠ›ï¼ˆæœ€é•¿è‡³ 256kï¼‰ã€‚
3. **Reasoning**ï¼šå¤æ‚æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬ AIMEã€HMMTã€LiveCodeBenchã€HumanEvalã€GPQAã€MMLU Pro ç­‰ã€‚

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**
- **æ¨¡å‹è§„æ¨¡**ï¼š14B å‚æ•°æ¨¡å‹ï¼ˆè¯¦è§ Table 1ï¼‰
- **è®­ç»ƒé…ç½®**ï¼š
  - å…¨å±€ batch sizeï¼š32M tokens
  - ä¸Šä¸‹æ–‡é•¿åº¦ï¼š8,192ï¼ˆé¢„è®­ç»ƒé˜¶æ®µï¼‰
  - ç¡¬ä»¶å¹³å°ï¼šCloud TPU v6eï¼ˆ1024 èŠ¯ç‰‡ï¼Œ8Ã—128 åˆ‡ç‰‡ï¼‰
  - å¹¶è¡Œç­–ç•¥ï¼šFSDP + Sequence Parallelism
  - ä¼˜åŒ–å™¨ï¼šAdamW
- **ç¨€ç–è®¾ç½®**ï¼š
  - Block size $ B = 128 $
  - Stride $ s = 16 $ï¼ŒCompression window $ C = 32 $
  - Top-$ k = 32 $ å—æ£€ç´¢
  - å¼ºåˆ¶åŒ…å«åˆå§‹å—ï¼ˆattention sinkï¼‰å’Œæœ€è¿‘ 4 ä¸ªå±€éƒ¨å—
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼šå„åŸºå‡†æµ‹è¯•é›†å¾—åˆ†
  - æ¨ç†å»¶è¿Ÿï¼ˆDecoding Latencyï¼‰ï¼šstep time speedup %
  - è®­ç»ƒååé‡ï¼ˆThroughputï¼‰

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
- **DENSE**ï¼šå…¨æ³¨æ„åŠ›æ¨¡å‹ï¼ˆä¸Šé™ï¼‰
- **NSA**ï¼ˆNative Sparse Attentionï¼‰ï¼šå¤šåˆ†æ”¯ç»“æ„ï¼Œå­˜åœ¨ä¿¡æ¯é‡å¤è®¡ç®—
- **InfLLM-v2**ï¼šåŸºäºå‡å€¼é€‰æ‹© + ç¡¬æˆªæ–­ï¼Œä»£è¡¨â€œtruncate-onlyâ€èŒƒå¼
- **SPA**ï¼ˆæ¶ˆèå˜ä½“ï¼‰ï¼šSPLA ä½†ç§»é™¤ RLA æ¨¡å—ï¼Œç”¨äºéªŒè¯ RLA æœ‰æ•ˆæ€§

æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€åœ¨ JAX + TPU ä¸Šå®ç°ï¼Œä¿è¯å…¬å¹³æ¯”è¾ƒã€‚

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**

#### **è¡¨ 2ï¼šé€šç”¨çŸ¥è¯†åŸºå‡†ï¼ˆå¹³å‡å¾—åˆ†ï¼‰**
| Method | Average (0/1-shot) | MMLU (5-shot) | GSM8K (8-shot) |
|-------|--------------------|---------------|----------------|
| DENSE | 67.5               | 77.3          | 90.2           |
| NSA   | 66.5               | 77.4          | 90.5           |
| INF-V2| 67.3               | 77.8          | 90.0           |
| SPA   | 67.3               | 78.1          | 90.2           |
| **SPLA** | **67.7**       | **78.6**      | **90.5**       |

> âœ… SPLA åœ¨å¤šæ•°æŒ‡æ ‡ä¸Šè¶…è¶Š dense æ¨¡å‹ï¼Œå°¤å…¶åœ¨ MMLU ä¸Šè¡¨ç°çªå‡ºã€‚

#### **è¡¨ 3ï¼šRULER é•¿ä¸Šä¸‹æ–‡åŸºå‡†ï¼ˆæœ€é«˜ä¸ºä¼˜ï¼‰**
| Model \ Length | 4k  | 8k  | 16k | 32k | 64k | 128k | 256k |
|----------------|-----|-----|-----|-----|-----|------|------|
| DENSE          | 95.8| 94.9| 93.6| 91.4| 87.1| 83.2 | 69.3 |
| NSA            | 92.3| 91.4| 90.7| 84.6| 83.2| 51.3 | 32.5 |
| INF-V2         | 95.9| 94.1| 92.1| 89.3| 86.7| 61.6 | 42.6 |
| SPA            | 95.9| 94.2| 93.3| 90.4| 87.1| 62.4 | 44.5 |
| **SPLA**       | **95.9**| **94.7**| **94.2**| **91.7**| **88.3**| **85.2** | **72.3** |

> ğŸ”¥ **SPLA åœ¨ 256k ä¸Šè¾¾åˆ° 72.3ï¼Œè¿œè¶… dense çš„ 69.3 å’Œå…¶ä»–ç¨€ç–æ–¹æ³•ï¼ˆ<45ï¼‰**ï¼Œé¦–æ¬¡å®ç°â€œè¶Šç¨€ç–è¶Šå¼ºâ€ã€‚

#### **è¡¨ 4ï¼šæ¨ç†ä¸çŸ¥è¯†ç»¼åˆä»»åŠ¡**
| Metric        | DENSE | NSA  | INF-V2 | SPA  | **SPLA** |
|--------------|-------|------|--------|------|----------|
| AIME 2024     | 77.1  | 73.1 | 76.8   | 77.2 | **78.3** |
| HumanEval    | 85.4  | 78.7 | 86.6   | 86.0 | **86.6** |
| GPQA         | 68.5  | 59.6 | 68.7   | 69.2 | **69.5** |
| MMLU Pro     | 78.9  | 68.8 | 79.3   | 79.1 | **79.3** |

> âœ… SPLA åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äº NSAï¼Œå¹¶å°å¹…é¢†å…ˆ INF-V2 å’Œ SPAã€‚

---

### **æ¶ˆèå®éªŒç»“æœ**
- **SPA vs. InfLLM-v2**ï¼šSPA ä½¿ç”¨äºŒé˜¶æ³°å‹’é€‰æ‹©ï¼Œåœ¨ç›¸åŒæ¶æ„ä¸‹å…¨é¢ä¼˜äº InfLLM-v2 â†’ è¡¨æ˜**é€‰æ‹©æœºåˆ¶æ”¹è¿›æœ‰æ•ˆ**ã€‚
- **SPLA vs. SPA**ï¼šSPLA åœ¨é•¿ä¸Šä¸‹æ–‡ï¼ˆå°¤å…¶æ˜¯ >64kï¼‰æŒç»­æå‡æ€§èƒ½ â†’ æ˜ç¡®è¯æ˜**RLA æ¨¡å—èƒ½ç¼“è§£é•¿å°¾å‘æ•£é—®é¢˜**ã€‚
- **NSA æ€§èƒ½ä¸‹é™æ˜æ˜¾**ï¼šåœ¨ 128k å’Œ 256k ä¸Šæ€¥å‰§é€€åŒ–ï¼ˆ51.3 â†’ 32.5ï¼‰ï¼Œå› å…¶åŒåˆ†æ”¯è®¾è®¡å¯¼è‡´åˆ†å¸ƒåç§»ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **SPLA æˆåŠŸå¼¥åˆäº†ç¨€ç–ä¸å…¨æ³¨æ„åŠ›ä¹‹é—´çš„æ€§èƒ½é¸¿æ²Ÿ**ï¼Œåœ¨é•¿è¾¾ 256k çš„åºåˆ—ä¸Šä»èƒ½é€¼è¿‘ç”šè‡³è¶…è¶Š dense æ¨¡å‹ã€‚
2. **æ®‹å·®çº¿æ€§æ³¨æ„åŠ›ï¼ˆRLAï¼‰æ˜¯å…³é”®**ï¼šå®ƒä»¥æä½æˆæœ¬ä¿ç•™äº†è¢«ä¼ ç»Ÿç¨€ç–æ–¹æ³•ä¸¢å¼ƒçš„â€œé•¿å°¾â€ä¿¡æ¯ï¼Œé˜²æ­¢éšé•¿åº¦å¢é•¿è€Œå‘æ•£ã€‚
3. **åŸºäºäºŒé˜¶æ³°å‹’å±•å¼€çš„é€‰æ‹©æœºåˆ¶æ›´å¯é **ï¼šç›¸æ¯”ä»…ç”¨å‡å€¼çš„æ–¹æ³•ï¼Œèƒ½æ›´å‡†ç¡®è¯†åˆ«é‡è¦å—ï¼Œæå‡ recallã€‚
4. **æä½å‚æ•°å¼€é”€å³å¯å®Œæˆé€‚é…**ï¼šSPLA å‡ ä¹æ— éœ€æ–°å¢å‚æ•°ï¼ˆä»… RMSNorm ç¼©æ”¾å› å­ï¼‰ï¼Œéå¸¸é€‚åˆå°†å·²æœ‰ dense æ¨¡å‹æ— ç¼å‡çº§ä¸ºé«˜æ•ˆç¨€ç–ç‰ˆæœ¬ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
- å½“å‰å®ç°ä¾èµ–äº TPU çš„å¤§ block sizeï¼ˆB=128ï¼‰ï¼Œåœ¨ GPU ä¸Šå¯èƒ½éœ€è¦è°ƒæ•´ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚
- RLA çš„ç¨³å®šæ€§ä¾èµ–äºéšè—çŠ¶æ€ä½¿ç”¨ float32 ç²¾åº¦ç§¯ç´¯ï¼Œå¯¹ä½ç²¾åº¦éƒ¨ç½²æœ‰ä¸€å®šè¦æ±‚ã€‚
- è™½ç„¶è®­ç»ƒæ•ˆç‡æ¥è¿‘ dense æ¨¡å‹ï¼Œä½†åœ¨å° group sizeï¼ˆG=5ï¼‰æ—¶ä»æœ‰è½»å¾®å¼€é”€ï¼Œå»ºè®®ä½¿ç”¨æ›´å¤§çš„ GQA ç»„ä»¥æå‡å¹¶è¡Œæ•ˆç‡ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- æ¢ç´¢ä»å¤´è®­ç»ƒï¼ˆtraining from scratchï¼‰åœºæ™¯ä¸‹çš„è§£è€¦è®¾è®¡ï¼Œä¾‹å¦‚ä¸º RLA åˆ†æ”¯å­¦ä¹ ä¸“ç”¨æŠ•å½±çŸ©é˜µã€‚
- å°è¯•æ›´çµæ´»çš„ grouping ç­–ç•¥ï¼ˆå¦‚ multi-value head ç»“æ„ï¼‰ä»¥è¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚
- æ‰©å±•è‡³ prefill é˜¶æ®µçš„ç¨€ç–ä¼˜åŒ–ï¼Œå®ç°ç«¯åˆ°ç«¯çš„é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡å¤„ç†ã€‚

---

> ğŸ’¡ **æ€»ä½“è¯„ä»·**ï¼š  
> SPLA æ˜¯ä¸€ç§**åŸåˆ™æ€§å¼ºã€å·¥ç¨‹å‹å¥½ã€æ€§èƒ½å“è¶Š**çš„ç¨€ç–æ³¨æ„åŠ›æ–°èŒƒå¼ã€‚å®ƒä¸ä»…è§£å†³äº†ç°æœ‰æ–¹æ³•çš„æ ¹æœ¬ç¼ºé™·ï¼Œè¿˜æä¾›äº†ä¸€æ¡**å¹³æ»‘æ¼”è¿›è·¯å¾„**ï¼š**å…ˆç”¨ dense attention å®Œæˆæ˜‚è´µçš„é¢„è®­ç»ƒï¼Œå†é€šè¿‡ SPLA å®ç°é«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡é€‚é…**ã€‚è¿™ä¸€æ€è·¯æœ‰æœ›æˆä¸ºä¸‹ä¸€ä»£ Foundation Model æœåŠ¡çš„æ ‡å‡†ç»„ä»¶ã€‚

</details>

---

### 7. [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)

**Authors**: Abhishek Tyagi, Yunuo Cen, Shrey Dhorajiya, Bharadwaj Veeravalli, Xuanyao Fong  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22632v1  

#### Abstract
Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šDART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨æ˜¾è‘—çš„å‚æ•°å†—ä½™ï¼Œå°¤å…¶æ˜¯åœ¨ **Feed-Forward Networks (FFNs)** ä¸­ã€‚ç°æœ‰çš„å‰ªææ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š
1. **ä¾èµ–ç‰¹å®šæ•°æ®é›†è¿›è¡Œæ ¡å‡†**ï¼ˆdataset-specific calibrationï¼‰ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ä¸”æ³›åŒ–èƒ½åŠ›å·®ï¼›
2. **é™æ€å‰ªæç­–ç•¥**æ— æ³•é€‚åº”è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸Šä¸‹æ–‡åŠ¨æ€æ¼”åŒ–çš„ç‰¹æ€§ï¼Œå¯¼è‡´â€œ**çŸ¥è¯†æ¼‚ç§»**â€ï¼ˆknowledge driftï¼‰â€”â€”å³åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼ŒåŸæœ¬è¢«å‰ªæ‰çš„é‡è¦ç¥ç»å…ƒå¯èƒ½å˜å¾—å…³é”®ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šDARTï¼ˆDynamic Attention-Guided Runtime Tracingï¼‰
DART æ˜¯ä¸€ç§è½»é‡çº§ã€æ— éœ€è®­ç»ƒã€è¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´çš„æ¨ç†é˜¶æ®µå‰ªææ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- åˆ©ç”¨ **attention è¾“å‡ºåˆ†å¸ƒçš„å˜åŒ–** æ¥æ£€æµ‹è¯­ä¹‰ä¸Šä¸‹æ–‡çš„è½¬ç§»ï¼›
- åŠ¨æ€æ›´æ–° **neuron-level masks**ï¼Œä¿ç•™å½“å‰ä¸Šä¸‹æ–‡ä¸­é‡è¦çš„çŸ¥è¯†ç¥ç»å…ƒã€‚

#### ä¸»è¦æ¨¡å—ï¼š
1. **Context-aware Neuron Selector**  
   - åŸºäºæ¯å±‚ FFN å¯¹æ®‹å·®æµçš„å‡ ä½•è´¡çŒ®ï¼Œè‡ªåŠ¨åˆ†é…ç¨€ç–é¢„ç®—ï¼›
   - ä¸ä¾èµ–å¤–éƒ¨æ•°æ®æˆ–è¾…åŠ©æ¨¡å‹ï¼Œå®ç°ç¡®å®šæ€§çš„é€å±‚å‰ªæã€‚

2. **Context Switch Detectorï¼ˆçŸ¥è¯†æ¼‚ç§»æ¢æµ‹å™¨ï¼‰**  
   - ç›‘æ§ attention å±‚è¾“å‡ºå‘é‡çš„åˆ†å¸ƒåç§»ï¼ˆcosine similarity ä¸‹é™ï¼‰ï¼›
   - å½“åç§»è¶…è¿‡é˜ˆå€¼æ—¶è§¦å‘ mask æ›´æ–°ï¼Œæ¢å¤å› ä¸Šä¸‹æ–‡å˜åŒ–è€Œéœ€è¦çš„çŸ¥è¯†ç¥ç»å…ƒã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | é™æ€å‰ªæï¼ˆå¦‚ WANDA, SPARSEGPTï¼‰ | åŠ¨æ€å‰ªæï¼ˆå¦‚ DEJAVU, GRIFFINï¼‰ | **DARTï¼ˆæœ¬æ–‡ï¼‰** |
|------|-------------------------------|------------------------------|------------------|
| æ˜¯å¦éœ€è®­ç»ƒ | å¦ï¼ˆéƒ¨åˆ†ï¼‰ | æ˜¯ï¼ˆDEJAVUï¼‰ | âŒ å¦ |
| æ˜¯å¦ä¾èµ–æ ¡å‡†æ•°æ® | æ˜¯ | æ˜¯ | âŒ å¦ |
| æ˜¯å¦æ„ŸçŸ¥ä¸Šä¸‹æ–‡æ¼”åŒ– | âŒ å¦ | æœ‰é™æ”¯æŒ | âœ… å¼ºæ„ŸçŸ¥ |
| æ˜¯å¦å¤„ç†è·¨å±‚äº¤äº’ | âŒ å¦ | æœ‰é™ | âœ… æ˜¾å¼å»ºæ¨¡ |
| å†…å­˜å¼€é”€ | ä½ | ä¸­é«˜ | **æä½ï¼ˆ<10MB for Llama-3.1-8Bï¼‰** |
| FLOPs å¼€é”€ | ä½ | ä¸­ | **ä»… 0.1%** |

> âœ… DART åœ¨ä¿æŒæä½èµ„æºæ¶ˆè€—çš„åŒæ—¶å®ç°äº†æ›´å¼ºçš„ä¸Šä¸‹æ–‡é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
å®éªŒè¦†ç›–å¤šä¸ªä»»åŠ¡ç±»å‹ä»¥éªŒè¯é€šç”¨æ€§ï¼š

#### é›¶æ ·æœ¬ä»»åŠ¡ï¼ˆZero-Shot Tasksï¼‰ï¼š
- **BoolQ**, **RTE**, **HellaSwag**, **Winogrande**, **ARC-E/C**, **OBQA**

#### é¢†åŸŸç‰¹å®šå¤šè·³ä»»åŠ¡ï¼ˆMulti-Shot Domain-Specificï¼‰ï¼š
- **MMLU**ï¼ˆæ¶µç›–52ä¸ªå­¦ç§‘ï¼‰
- **GPQA**ï¼ˆç ”ç©¶ç”Ÿçº§åˆ«é—®ç­”ï¼‰
- **MEDMCQA**ï¼ˆåŒ»å­¦é¢†åŸŸé—®ç­”ï¼‰

#### é•¿æ–‡æœ¬ç”Ÿæˆä¸æ‘˜è¦ä»»åŠ¡ï¼ˆLong-Horizon Generation & Summarizationï¼‰ï¼š
- **CNN/DailyMail**
- **Multi-News**
- **GovReport**

æ­¤å¤–è¿˜æ„å»ºäº†ä¸€ä¸ª**è‡ªå®šä¹‰å¤šä¸»é¢˜æç¤ºæ•°æ®é›†**ï¼ˆCustom Prompt Dataset, 500 promptsï¼‰ï¼Œç”¨äºæµ‹è¯•è·¨é¢†åŸŸç”Ÿæˆä¸­çš„çŸ¥è¯†è¿½è¸ªèƒ½åŠ›ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹**ï¼šLLAMA-3.2-3B, LLAMA-3.1-8Bï¼Œæ‰©å±•è‡³ QWEN3, DeepSeek-R1, Mistral-7B ç­‰
- **å‰ªææ¯”ä¾‹**ï¼šFFN å­å±‚è¾¾åˆ° **70% ç¨€ç–åº¦**
- **ç²¾åº¦æ ¼å¼**ï¼šFP8 è®¡ç®— / FP16 é€šä¿¡
- **ç¡¬ä»¶å¹³å°**ï¼šåŒ AMD EPYC 9654 CPU + 8Ã—NVIDIA L40S GPU

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| ç±»å‹ | æŒ‡æ ‡ |
|------|------|
| åˆ†ç±»/ç†è§£ä»»åŠ¡ | **Accuracy (%)** |
| æ‘˜è¦ä»»åŠ¡ | **ROUGE-L**, **BLEU**, **BERTScore**, **Embedding Similarity**, **Coverage** |
| åŠ¨æ€è¡Œä¸ºåˆ†æ | **Cosine Similarity è½¨è¿¹**, **Retrigger æ¬¡æ•°ç»Ÿè®¡** |

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| æ–¹æ³• | ç±»å‹ | æ˜¯å¦è®­ç»ƒ | æ˜¯å¦åŠ¨æ€ |
|------|------|--------|---------|
| **WANDA** | é™æ€ã€æƒé‡çº§å‰ªæ | å¦ | âŒ |
| **SPARSEGPT** | é™æ€ã€éç»“æ„åŒ–å‰ªæ | å¦ | âŒ |
| **DLP / OWL** | é™æ€ã€åŸºäºé‡è¦æ€§è¯„åˆ† | å¦ | âŒ |
| **DEJAVU** | åŠ¨æ€ã€éœ€è®­ç»ƒé¢„æµ‹å™¨ | âœ… | âœ… |
| **GRIFFIN** | åŠ¨æ€ã€å¤ç”¨é¢„å¡«å……æ¿€æ´» | å¦ | âœ…ï¼ˆæœ‰é™ï¼‰ |
| **DART (ours)** | **åŠ¨æ€ã€æ— è®­ç»ƒã€æ³¨æ„åŠ›å¼•å¯¼** | âŒ | âœ…âœ… |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®ï¼ˆ70% FFN Sparsityï¼‰

#### è¡¨æ ¼ï¼šåœ¨ LLAMA-3.1-8B ä¸Šçš„é›¶æ ·æœ¬ä¸é¢†åŸŸä»»åŠ¡å‡†ç¡®ç‡å¯¹æ¯”ï¼ˆå•ä½ï¼š%ï¼‰

| BENCHMARK | DENSE | WANDA | DEJAVU | **DART (Ours)** |
|----------|-------|--------|--------|----------------|
| **BoolQ** | 83.09 | 67.79 | 42.14 | **66.20** |
| **RTE** | 71.12 | 52.70 | 47.65 | **55.23** |
| **HellaSwag** | 79.32 | 44.99 | 26.68 | **64.58** |
| **Winogrande** | 74.51 | 56.43 | 50.19 | **65.98** |
| **ARC-E** | 82.53 | 50.34 | 25.08 | **59.43** |
| **ARC-C** | 54.95 | 28.16 | 26.27 | **38.99** |
| **OBQA** | 45.60 | 27.80 | 29.00 | **29.40** |
| **MMLU (5-shot)** | 66.61 | 29.70 | 25.20 | **34.14** |
| **GPQA (5-shot)** | 31.38 | 25.24 | 24.40 | **27.21** |
| **MEDMCQA (5-shot)** | 56.66 | 25.65 | 28.16 | **29.35** |

> ğŸ’¡ **DART å¹³å‡æå‡é«˜è¾¾ +14.5%~19.6%**ï¼Œå°¤å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ HellaSwag, ARC-Eï¼‰ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚

---

### ğŸ” åŠ¨æ€å‰ªæ vs é™æ€å‰ªæï¼ˆæ‘˜è¦ä»»åŠ¡ï¼‰

| æ¨¡å‹ | æ–¹æ³• | ROUGE-L | BLEU | BERTScore | Coverage |
|------|------|---------|-------|-----------|----------|
| Llama-3.1-8B | Dense | 0.42 | 0.28 | 0.85 | 0.91 |
| | Static Pruning | 0.14 | 0.09 | 0.76 | 0.63 |
| | **DART (w/ tracing)** | **0.38** | **0.26** | **0.83** | **0.89** |

> âœ… DART å®ç°äº† **3Ã— æ›´é«˜çš„ ROUGE-L åˆ†æ•°**ï¼Œæ¥è¿‘åŸå§‹å¯†é›†æ¨¡å‹è¡¨ç°ã€‚

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

| è®¾ç½® | ROUGE-L | EMBED SIM | COVERAGE |
|------|---------|-----------|----------|
| 50% Sparsityï¼ˆæ— è¿½è¸ªï¼‰ | 0.27 | 0.85 | 0.77 |
| **+ Knowledge Tracing** | **0.38** | **0.92** | **0.89** |

> âœ… åŠ å…¥çŸ¥è¯†è¿½è¸ªæœºåˆ¶åï¼Œæ‰€æœ‰æŒ‡æ ‡æ˜¾è‘—æå‡ï¼Œè¯æ˜å…¶å¯¹ç¼“è§£çŸ¥è¯†æ¼‚ç§»è‡³å…³é‡è¦ã€‚

#### è§¦å‘é‡å‰ªæ¬¡æ•°ç»Ÿè®¡ï¼š
- åœ¨ **45k~98k tokens** çš„é•¿æ–‡æ¡£ä¸­ï¼Œå¹³å‡ä»…è§¦å‘ **~100 æ¬¡ mask æ›´æ–°**
- è¡¨æ˜ä¸Šä¸‹æ–‡æ¼”å˜å¹³æ»‘ï¼ŒDART åªåœ¨å¿…è¦æ—¶åˆ»å¹²é¢„ï¼Œæ•ˆç‡æé«˜ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **çŸ¥è¯†æ¼‚ç§»æ˜¯åŠ¨æ€å‰ªæçš„å…³é”®å¤±è´¥æ¨¡å¼**  
   - å›ºå®š mask ä¼šé˜»ç¢æ¨¡å‹è¿›å…¥æ–°çš„çŸ¥è¯†é¢†åŸŸï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹é€€åŒ–ä¸ºé‡å¤ç‰‡æ®µæˆ–æ— æ„ä¹‰æ ‡é¢˜ï¼ˆgibberishï¼‰ã€‚
   
2. **attention è¾“å‡ºåˆ†å¸ƒå¯ä½œä¸ºä¸Šä¸‹æ–‡åˆ‡æ¢çš„æœ‰æ•ˆä¿¡å·**  
   - attention å‘é‡çš„ cosine similarity ä¸‹é™èƒ½æå‰é¢„è­¦è¯­ä¹‰è¿ç§»ï¼Œä¸º mask æ›´æ–°æä¾›å¯é ä¾æ®ã€‚

3. **DART å®ç°é«˜æ•ˆä¸”ç²¾å‡†çš„è¿è¡Œæ—¶å‰ªæ**  
   - ä»…å¢åŠ  **0.1% FLOPs å¼€é”€** å’Œ **<10MB å†…å­˜å ç”¨**ï¼Œå³å¯å¤§å¹…æå‡å‰ªææ¨¡å‹çš„è¡¨ç°ï¼›
   - æ€§èƒ½é€¼è¿‘åŸå§‹å¯†é›†æ¨¡å‹ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰é™æ€ä¸åŠ¨æ€æ–¹æ³•ã€‚

4. **æ›´å¤§çš„æ¨¡å‹æ›´å—ç›Šäº DART**  
   - å¤§æ¨¡å‹ä¸­çŸ¥è¯†è¡¨ç¤ºæ›´è§£è€¦ï¼ˆdisentangledï¼‰ï¼ŒåŠŸèƒ½åˆ†ç¦»æ›´æ¸…æ™°ï¼Œå› æ­¤æ›´å®¹æ˜“é€šè¿‡é€‰æ‹©æ€§æ¿€æ´»æ¥ç»´æŒæ€§èƒ½ã€‚

---

### âš ï¸ å±€é™æ€§
1. **ç›®å‰ä»…å‰ªæ FFN å±‚**ï¼Œæœªæ¶‰åŠ attention å­å±‚ï¼›
2. **mask æ›´æ–°å»¶è¿Ÿé—®é¢˜**ï¼šè‹¥æ£€æµ‹åˆ°æ¼‚ç§»å¤ªæ™šï¼ˆå¦‚ >500 tokensï¼‰ï¼Œå¯èƒ½å·²é™·å…¥ä¸å¯é€†çš„è¯­ä¹‰é™·é˜±ï¼›
3. **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼šæ¼‚ç§»æ£€æµ‹é˜ˆå€¼ `Ïƒ` å’Œçª—å£å¤§å° `T` éœ€åˆç†è®¾ç½®ï¼›
4. **ä¸é€‚ç”¨äºæç«¯ç¨€ç–åœºæ™¯**ï¼ˆ>90%ï¼‰ï¼Œæ­¤æ—¶å³ä½¿åŠ¨æ€æ›´æ–°ä¹Ÿéš¾ä»¥æ¢å¤å®Œæ•´è¡¨è¾¾èƒ½åŠ›ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. å°† DART æ‰©å±•è‡³ **attention å±‚å‰ªæ**ï¼Œæ¢ç´¢å…¨æ¨¡å—è”åˆç¨€ç–åŒ–ï¼›
2. ç»“åˆ **KV Cache å‹ç¼©** æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥é™ä½é•¿åºåˆ—æ¨ç†å†…å­˜ï¼›
3. æ¢ç´¢ **learnable drift detector**ï¼Œæ›¿ä»£æ‰‹å·¥è®¾å®šçš„ç»Ÿè®¡é˜ˆå€¼ï¼›
4. åº”ç”¨äº **è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²**ï¼Œæ¨åŠ¨ LLM åœ¨ç§»åŠ¨ç«¯çš„å®æ—¶é«˜æ•ˆè¿è¡Œï¼›
5. ç ”ç©¶ DART åœ¨ **å¤šæ¨¡æ€æ¨¡å‹** ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

---

## âœ… æ€»ç»“
DART æå‡ºäº†ä¸€ç§æ–°é¢–çš„ **åŠ¨æ€æ³¨æ„åŠ›å¼•å¯¼è¿è¡Œæ—¶è¿½è¸ªæœºåˆ¶**ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿå‰ªææ–¹æ³•åœ¨è‡ªå›å½’ç”Ÿæˆä¸­å› â€œçŸ¥è¯†æ¼‚ç§»â€è€Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚å®ƒæ— éœ€è®­ç»ƒã€ä¸ä¾èµ–æ ¡å‡†æ•°æ®ã€èµ„æºå¼€é”€æå°ï¼Œå´èƒ½åœ¨å¤šç§ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰é™æ€ä¸åŠ¨æ€å‰ªææ–¹æ³•ï¼Œæ˜¯è¿ˆå‘é«˜æ•ˆã€è‡ªé€‚åº” LLM æ¨ç†çš„é‡è¦ä¸€æ­¥ã€‚

> ğŸ”— ä»£ç å¼€æºåœ°å€ï¼š[https://github.com/seeder/research/DART](https://github.com/seeder/research/DART)

</details>

---

### 8. [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)

**Authors**: Yuxuan Li, Qijun He, Mingqi Yuan, Wen-Tse Chen, Jeff Schneider, Jiayu Chen  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22475v1  

#### Abstract
Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize t...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šContinual Policy Distillation from Distributed Reinforcement Learning Teachers**

---

## **1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### **è§£å†³çš„é—®é¢˜**
è¯¥è®ºæ–‡èšç„¦äº**Continual Reinforcement Learning (CRL)** ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼š
- **ç¾éš¾æ€§é—å¿˜ï¼ˆCatastrophic Forgettingï¼‰**ï¼šæ¨¡å‹åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶é—å¿˜æ—§ä»»åŠ¡çš„çŸ¥è¯†ã€‚
- **ç¨³å®šæ€§-å¯å¡‘æ€§å›°å¢ƒï¼ˆStability-Plasticity Dilemmaï¼‰**ï¼šå¦‚ä½•åœ¨ä¿æŒå·²æœ‰æŠ€èƒ½ç¨³å®šçš„åŒæ—¶é«˜æ•ˆå­¦ä¹ æ–°æŠ€èƒ½ã€‚
- **æ ·æœ¬å¤æ‚åº¦é«˜**ï¼šä¼ ç»Ÿå¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼ˆMulti-task RLï¼‰éœ€è¦å¤§é‡ç¯å¢ƒäº¤äº’ï¼ˆå¦‚ Meta-World ä¸Šéœ€åƒä¸‡çº§æ­¥æ•°ï¼‰ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚

### **æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯**
ä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„ **teacher-student æ¡†æ¶**ï¼Œå°† CRL åˆ†è§£ä¸ºä¸¤ä¸ªç‹¬ç«‹è¿‡ç¨‹ï¼š
1. **åˆ†å¸ƒå¼ RL æ•™å¸ˆè®­ç»ƒ**ï¼šå¤šä¸ªå•ä»»åŠ¡ RL agent å¹¶è¡Œè®­ç»ƒï¼Œä½œä¸ºâ€œä¸“å®¶æ•™å¸ˆâ€ã€‚
2. **æŒç»­ç­–ç•¥è’¸é¦ï¼ˆContinual Policy Distillationï¼‰**ï¼šå°†æ•™å¸ˆç­–ç•¥é€šè¿‡æ¨¡ä»¿å­¦ä¹ ï¼ˆImitation Learningï¼‰é€æ­¥è’¸é¦åˆ°ä¸€ä¸ªä¸­å¿ƒåŒ–çš„é€šç”¨å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚

#### **å…³é”®æŠ€æœ¯ç»„ä»¶**ï¼š
- **Transformer-based Mixture-of-Experts (MoE) æ¶æ„**ï¼š
  - å­¦ç”Ÿæ¨¡å‹é‡‡ç”¨åŸºäº Transformer çš„ MoE ç»“æ„ï¼Œæ”¯æŒç¨€ç–æ¿€æ´»å’ŒåŠ¨æ€æ‰©å±•ã€‚
  - æ¯ä¸ªæ–°ä»»åŠ¡åˆ°æ¥æ—¶ï¼Œ**å¢é‡æ·»åŠ æ–°çš„ expert**ï¼Œæå‡æ¨¡å‹å®¹é‡ï¼ˆplasticityï¼‰ã€‚
- **Contrastive Task Embedding**ï¼š
  - ä¸ä¾èµ– one-hot ä»»åŠ¡ IDï¼Œè€Œæ˜¯ä»çŠ¶æ€åºåˆ—ä¸­å­¦ä¹ è¯­ä¹‰ä¸°å¯Œçš„ task embeddingï¼Œæ›´é€‚ç”¨äºå¼€æ”¾ä¸–ç•Œè®¾å®šã€‚
- **æ··åˆé˜²é—å¿˜æœºåˆ¶**ï¼š
  - **Experience Replay + Parameter Regularization** è”åˆä½¿ç”¨ï¼š
    - **Diversity-Aware Trajectory Replay**ï¼šåˆ©ç”¨ Determinantal Point Process (DPP) é€‰æ‹©æœ€å…·ä»£è¡¨æ€§å’Œå¤šæ ·æ€§çš„è½¨è¿¹å­˜å…¥å›æ”¾ç¼“å†²åŒºï¼ˆä»…å æ€»æ•°æ® <10%ï¼‰ã€‚
    - **Hierarchical Parameter Masking**ï¼š
      - ç¬¬ä¸€é˜¶æ®µåå†»ç»“å…±äº«ä¸»å¹²ï¼ˆå¦‚ Attentionã€Embedding å±‚ï¼‰ã€‚
      - æ–°ä¸“å®¶åˆå§‹åŒ–é‡‡ç”¨ Net2WiderNet ç­–ç•¥ï¼Œå¹¶è®¾ç½®å†·å¯åŠ¨åç½®ï¼ˆCold-Start Bias Initializationï¼‰é˜²æ­¢è·¯ç”±ä¸ç¨³å®šã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **å¯æ‰©å±•æ€§** | å°†å¤æ‚çš„åœ¨çº¿ RL å­¦ä¹ è½¬åŒ–ä¸ºç¨³å®šçš„ç›‘ç£å¼ç­–ç•¥è’¸é¦ï¼Œæ›´é€‚åˆå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚ |
| **æ•ˆç‡** | åˆ©ç”¨å¹¶è¡Œæ•™å¸ˆè®­ç»ƒé¿å…é¡ºåº RL çš„é•¿å‘¨æœŸï¼›è’¸é¦ä»…éœ€çº¦ 70k æ­¥æ•™å¸ˆç»éªŒå³å¯æ¢å¤ >95% æ•™å¸ˆæ€§èƒ½ã€‚ |
| **ç¨³å®šæ€§ä¸å¯å¡‘æ€§å¹³è¡¡** | MoE æ‰©å±• + å‚æ•°æ©ç æœ‰æ•ˆç¼“è§£å¹²æ‰°ï¼Œå®ç°ä½é—å¿˜ä¸‹çš„æŒç»­å­¦ä¹ ã€‚ |
| **æ¶æ„å…¼å®¹æ€§** | æ”¯æŒç°ä»£å¤§æ¨¡å‹æ¶æ„ï¼ˆå¦‚ Transformerï¼‰ï¼Œè€Œä¼ ç»Ÿ actor-critic æ–¹æ³•éš¾ä»¥ä¼˜åŒ–æ­¤ç±»ç»“æ„ã€‚ |

---

## **2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### **ä½¿ç”¨çš„æ•°æ®é›†**
- **Meta-World MT25**ï¼šåŒ…å« 25 ä¸ªä¸åŒæœºå™¨äººæ“ä½œä»»åŠ¡ï¼ˆå¦‚ `peg-insert`, `door-open`, `button-press` ç­‰ï¼‰ï¼Œæ˜¯å½“å‰ä¸»æµçš„å¤šä»»åŠ¡/æŒç»­å­¦ä¹  RL åŸºå‡†ã€‚
- ä½¿ç”¨ V2 å¯†é›†å¥–åŠ±å‡½æ•°ï¼Œæ¯ä»»åŠ¡æœ€å¤§æ­¥æ•°ä¸º 500ã€‚

### **å®éªŒè®¾ç½®**
- **æ•™å¸ˆè®­ç»ƒ**ï¼š
  - ä½¿ç”¨ **PPO** ç®—æ³•ç‹¬ç«‹è®­ç»ƒæ¯ä¸ªä»»åŠ¡çš„ expert policyã€‚
  - æ€»å…±æ”¶é›†çº¦ 70,000 æ­¥é«˜è´¨é‡ç¤ºèŒƒè½¨è¿¹ç”¨äºè’¸é¦ã€‚
- **å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ**ï¼š
  - è¾“å…¥å†å²çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œé¢„æµ‹è¿ç»­åŠ¨ä½œã€‚
  - é‡‡ç”¨ä¸¤é˜¶æ®µå’Œäº”é˜¶æ®µè’¸é¦åè®®ï¼š
    - **Two-stage**ï¼šå…ˆå­¦ MT10 â†’ å†å­¦å‰©ä½™ 15 ä¸ªä»»åŠ¡ã€‚
    - **Five-stage**ï¼šæ¯æ¬¡å­¦ä¹  5 ä¸ªä»»åŠ¡ï¼Œå…± 5 é˜¶æ®µã€‚
- **æ¨¡å‹ç»“æ„**ï¼š
  - Transformer-MoEï¼Œéšè—ç»´åº¦ 256ï¼Œ5 å±‚ï¼Œåˆå§‹ 8 ä¸ª expertsï¼Œæ¯é˜¶æ®µæ–°å¢ 1 ä¸ª expert/å±‚ã€‚

### **è¯„ä¼°æŒ‡æ ‡**
| æŒ‡æ ‡ | å®šä¹‰ | å«ä¹‰ |
|------|------|------|
| **Average Accuracy (Acc)** | $ \frac{1}{K}\sum_{k=1}^{K} a_{K,k} $ | æ‰€æœ‰ä»»åŠ¡ä¸Šçš„å¹³å‡æœ€ç»ˆæˆåŠŸç‡ |
| **Backward Transfer (BWT)** | $ \frac{1}{K-1}\sum_{j=1}^{K-1}(a_{K,j} - a_{j,j}) $ | è¡¡é‡é—å¿˜ç¨‹åº¦ï¼Œæ­£å€¼è¡¨ç¤ºæ­£å‘è¿ç§»ï¼Œè´Ÿå€¼è¡¨ç¤ºé—å¿˜ |

> æ³¨ï¼š$ a_{k,j} $ è¡¨ç¤ºç¬¬ $ k $ é˜¶æ®µåä»»åŠ¡ $ j $ çš„æµ‹è¯•æ€§èƒ½ã€‚

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| ç±»å‹ | æ–¹æ³• | æè¿° |
|------|------|------|
| ä¸‹ç•Œ | **Finetune**, **Trac** | æ— ä»»ä½•é˜²é—å¿˜æœºåˆ¶çš„é¡ºåºå¾®è°ƒ |
| ä¸Šç•Œ | **Independent** | æ¯é˜¶æ®µè®­ç»ƒç‹¬ç«‹æ¨¡å‹ï¼Œæ— é—å¿˜ä½†æ— æ³•å…±äº«çŸ¥è¯† |
| æ­£åˆ™åŒ– | **EWC**, **KL** | åŸºäº Fisher ä¿¡æ¯æˆ– KL æ•£åº¦çº¦æŸå‚æ•°å˜åŒ– |
| å›æ”¾ | **Replay** | ä»…ä½¿ç”¨è½¨è¿¹å›æ”¾ï¼Œæ— ç»“æ„æ‰©å±• |
| æ¶ˆè | **Expert Only**, **Replay Only** | ç¼ºå°‘æŸä¸€å…³é”®æ¨¡å—çš„å˜ä½“ |

---

## **3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### **å…³é”®æ€§èƒ½æ•°æ®**
#### âœ… **ä¸¤é˜¶æ®µè’¸é¦ç»“æœï¼ˆMT10 â†’ MT25ï¼‰**
- å­¦ç”Ÿæ¨¡å‹æ¢å¤äº†è¶…è¿‡ **85% çš„æ•™å¸ˆæ€§èƒ½**ã€‚
- ä»»åŠ¡çº§åˆ«é—å¿˜æ§åˆ¶åœ¨ **<10%**ï¼ˆBWT æ¥è¿‘é›¶ç”šè‡³ä¸ºæ­£ï¼‰ã€‚
- åœ¨ MT10 å¤šä»»åŠ¡è’¸é¦ä¸­è¾¾åˆ° **88.9% æˆåŠŸç‡**ï¼Œæ¥è¿‘è”åˆè®­ç»ƒä¸Šé™ï¼ˆ90.8%ï¼‰ï¼Œä¼˜äºæ‰€æœ‰ multi-task RL æ–¹æ³•ï¼ˆè§ä¸‹è¡¨ï¼‰ã€‚

##### **Table 1: MT10 å¤šä»»åŠ¡æ€§èƒ½å¯¹æ¯”ï¼ˆOur vs. Multi-task RL Baselinesï¼‰**
| æ–¹æ³• | å¹³å‡å‡†ç¡®ç‡ |
|------|-----------|
| SAC | 61.9Â±3.3 |
| MTSAC | 62.9Â±8.0 |
| Soft-Module | 63.0Â±4.2 |
| CARE | 76.0Â±6.9 |
| PaCo | 85.4Â±4.5 |
| MOORE | 88.7Â±5.6 |
| **Ours (Central Model)** | **88.9Â±1.9** |
| **Ours (Avg Teachers)** | **90.8Â±1.4** |

> â¤ æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…è¶…è¶Šæ‰€æœ‰ RL æ–¹æ³•ï¼Œä¸”ä»…ç”¨ **3M è’¸é¦æ­¥æ•°**ï¼Œè¿œä½äº RL æ–¹æ³•æ‰€éœ€çš„ **20M ç¯å¢ƒäº¤äº’æ­¥æ•°**ã€‚

#### âœ… **äº”é˜¶æ®µæŒç»­å­¦ä¹ ç»“æœï¼ˆTable 2ï¼‰**
| æ–¹æ³• | Stage 5 Acc | Stage 5 BWT |
|------|-------------|-------------|
| Finetune | 26.1% | -55.3% |
| EWC | 35.9% | -37.9% |
| KL | 21.6% | -24.7% |
| Replay Only | 57.2% | -11.0% |
| Expert Only | 28.0% | -51.3% |
| **Ours** | **69.4%** | **+2.3%** |

> â¤ **Ours åœ¨ Acc å’Œ BWT ä¸Šå…¨é¢é¢†å…ˆ**ï¼Œå®ç°äº†é«˜å‡†ç¡®æ€§ä¸æ­£å‘è¿ç§»èƒ½åŠ›ã€‚

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰**
| å˜ä½“ | Acc â†“ | BWT â†“ | å‘ç° |
|------|--------|--------|------|
| **No Freeze-Gating** | 64.5% â†’ 69.4% | 0.8% â†’ 2.3% | å†»ç»“é—¨æ§æœ‰åŠ©äºä¿ç•™è·¯ç”±è·¯å¾„ |
| **No Shared-Frozen** | 60.0% â†’ 69.4% | -8.0% â†’ 2.3% | å†»ç»“ä¸»å¹²è‡³å…³é‡è¦ |
| **No TE (Task Embedding)** | 57.9% | -0.7% | Task embedding å¯¹ MoE è·¯ç”±è‡³å…³é‡è¦ |
| **Replay Only** | 57.2% | -11.0% | å•é å›æ”¾ä¸è¶³ä»¥ç»´æŒç¨³å®šæ€§ |
| **Expert Only** | 28.0% | -51.3% | å•é æ‰©å±•ä¸“å®¶ä¼šå¯¼è‡´ä¸¥é‡é—å¿˜ |
| **Ours (Full)** | **69.4%** | **+2.3%** | **å¿…é¡»ç»“åˆç»“æ„æ‰©å±• + å›æ”¾ + å‚æ•°æ©ç æ‰èƒ½æˆåŠŸ** |

> â¤ ç»“æœè¡¨æ˜ï¼š**ä¸‰ç§æœºåˆ¶ç¼ºä¸€ä¸å¯**ï¼ŒååŒä½œç”¨æ‰å®ç°æœ€ä½³æ€§èƒ½ã€‚

---

## **4. å…³é”®ç»“è®ºå’Œå‘ç°**

### **ä¸»è¦å‘ç°**
1. **Teacher-Student æ¡†æ¶æ˜¾è‘—æå‡ CRL å¯è¡Œæ€§**ï¼š
   - å°†å›°éš¾çš„ online RL å­¦ä¹ è½¬ä¸ºç¨³å®šçš„ offline è’¸é¦ä»»åŠ¡ï¼Œå¤§å¹…é™ä½ä¼˜åŒ–éš¾åº¦ã€‚
2. **MoE + å¢é‡æ‰©å±•æ˜¯ç»´æŒé•¿æœŸå¯å¡‘æ€§çš„å…³é”®**ï¼š
   - åŠ¨æ€å¢åŠ ä¸“å®¶ä½¿æ¨¡å‹èƒ½æŒç»­å®¹çº³æ–°æŠ€èƒ½ï¼Œé¿å…å®¹é‡é¥±å’Œã€‚
3. **æ··åˆé˜²é—å¿˜ç­–ç•¥æœ€æœ‰æ•ˆ**ï¼š
   - **Experience Replay (DPP)** + **Parameter Masking (Freeze Shared Layers + Two-Phase Training)** æ˜¾è‘—æŠ‘åˆ¶ç¾éš¾æ€§é—å¿˜ã€‚
4. **Learned Task Embedding æ•è·è¯­ä¹‰ç›¸ä¼¼æ€§**ï¼š
   - PCA å¯è§†åŒ–æ˜¾ç¤ºç›¸ä¼¼ä»»åŠ¡ï¼ˆå¦‚æŒ‰é’®æŒ‰å‹ã€æŠ½å±‰å¼€åˆï¼‰è‡ªåŠ¨èšç±»ï¼Œä¿ƒè¿›æ­£å‘è¿ç§»ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**
1. **ä¾èµ–é«˜è´¨é‡æ•™å¸ˆæ¨¡å‹**ï¼š
   - è‹¥æŸä¸ªæ•™å¸ˆæœªèƒ½å­¦ä¼šä»»åŠ¡ï¼ˆå¦‚ `disassemble-v3` æˆåŠŸç‡ä¸º 0ï¼‰ï¼Œåˆ™å­¦ç”Ÿä¹Ÿæ— æ³•æŒæ¡ã€‚
2. **ä»…åœ¨ Meta-World ä¸ŠéªŒè¯**ï¼š
   - å°šæœªåœ¨ Atari æˆ–å…¶ä»–è§†è§‰è¾“å…¥ç¯å¢ƒä¸­æµ‹è¯•ï¼Œæ³›åŒ–æ€§æœ‰å¾…éªŒè¯ã€‚
3. **ç¦»çº¿è’¸é¦ä¸é€‚ç”¨äºéå¹³ç¨³ç¯å¢ƒ**ï¼š
   - å½“å‰æ¡†æ¶å‡è®¾æ•™å¸ˆç­–ç•¥å›ºå®šï¼Œè‹¥ç¯å¢ƒåŠ¨æ€å˜åŒ–åˆ™éœ€é‡æ–°è®¾è®¡ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**
- æ‰©å±•è‡³ **vision-based tasks**ï¼ˆå¦‚ DeepMind Control Suiteï¼‰ã€‚
- å¼•å…¥ **online distillation** æœºåˆ¶ï¼Œå…è®¸æ•™å¸ˆæŒç»­æ”¹è¿›ã€‚
- æ¢ç´¢ **automatic expert pruning** ä»¥æ§åˆ¶æ¨¡å‹å¢é•¿ã€‚
- åº”ç”¨äº **embodied AI** å’Œ **robotics pre-training** åœºæ™¯ï¼Œæ„å»ºé€šç”¨æ™ºèƒ½ä½“ã€‚

---

> ğŸ’¡ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡æå‡ºçš„ **Continual Policy Distillation** æ¡†æ¶é€šè¿‡â€œ**åˆ†å¸ƒå¼æ•™å¸ˆè®­ç»ƒ + MoE å­¦ç”Ÿè’¸é¦ + æ··åˆé˜²é—å¿˜æœºåˆ¶**â€ï¼Œåœ¨ Meta-World ä¸Šå®ç°äº†é«˜æ•ˆã€ç¨³å®šã€å¯æ‰©å±•çš„æŒç»­å¼ºåŒ–å­¦ä¹ ï¼Œä¸ºæ„å»ºé€šç”¨æ™ºèƒ½ä½“æä¾›äº†å¯è¡Œè·¯å¾„ã€‚

</details>

---

### 9. [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)

**Authors**: Chuxue Cao, Jinluan Yang, Haoran Li, Kunhao Pan, Zijian Zhao, Zhengyu Chen, Yuchen Tian, Lijun Wu, Conghui He, Sirui Han, Yike Guo  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.22642v1  

#### Abstract
Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves form...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šPushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åŸºäºæ¦‚ç‡çš„é€è¯ç”Ÿæˆæœºåˆ¶å®¹æ˜“å¯¼è‡´**é€»è¾‘ä¸ä¸€è‡´**ï¼ˆlogical inconsistencyï¼‰å’Œ**å¥–åŠ±æ¬ºéª—**ï¼ˆreward hackingï¼‰ã€‚å…·ä½“è¡¨ç°ä¸ºï¼š
- æ¨ç†é“¾ä¸­å­˜åœ¨ä¸­é—´æ­¥éª¤é”™è¯¯ï¼Œå³ä½¿æœ€ç»ˆç­”æ¡ˆæ­£ç¡®ï¼›
- æ¨¡å‹é€šè¿‡è¡¨é¢æ¨¡å¼â€œçŒœå¯¹â€ç­”æ¡ˆï¼Œè€Œéå»ºç«‹ä¸¥è°¨çš„é€»è¾‘åŸºç¡€ï¼›
- ç°æœ‰ç¥ç»ç¬¦å·ç³»ç»Ÿï¼ˆneuro-symbolicï¼‰æ–¹æ³•å¤šä¸ºè¢«åŠ¨çš„äº‹åéªŒè¯ï¼ˆpost-hoc validationï¼‰ï¼Œæ— æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸»åŠ¨çº æ­£é”™è¯¯ã€‚

### æå‡ºçš„æ–°æ–¹æ³•
ä½œè€…æå‡ºäº†ä¸€ç§**å½¢å¼åŒ–é€»è¾‘éªŒè¯å¼•å¯¼çš„æ¡†æ¶**ï¼ˆFormal Logic Verification-guided Frameworkï¼‰ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äº**åŠ¨æ€äº¤é”™**ï¼ˆdynamically interleavesï¼‰è‡ªç„¶è¯­è¨€æ¨ç†ä¸å½¢å¼åŒ–ç¬¦å·éªŒè¯è¿‡ç¨‹ã€‚

#### ä¸»è¦åˆ›æ–°ç‚¹ï¼š
- âœ… **åŠ¨æ€äº¤é”™éªŒè¯æœºåˆ¶**ï¼šä¸åŒäºä¼ ç»Ÿçš„é™æ€è¿‡æ»¤æˆ–äº‹åéªŒè¯ï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†ç”Ÿæˆçš„æ¯ä¸€æ­¥éƒ½å¼•å…¥å½¢å¼åŒ–éªŒè¯å™¨ï¼ˆå¦‚ z3ã€sympyï¼‰è¿›è¡Œå®æ—¶åé¦ˆï¼Œå®ç°**å®æ—¶é”™è¯¯æ£€æµ‹ä¸ä¿®æ­£**ã€‚
- âœ… **ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼**ï¼š
  1. **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼šé€šè¿‡ä¸€ä¸ªåˆ†å±‚çš„æ•°æ®åˆæˆç®¡é“ï¼Œç”ŸæˆåŒ…å«è‡ªç„¶è¯­è¨€æ¨ç†ã€å½¢å¼åŒ–è¯æ˜å’Œæ‰§è¡Œè¾“å‡ºçš„ä¸‰å…ƒç»„æ•°æ®ï¼Œå¹¶é€šè¿‡æ‰§è¡ŒéªŒè¯ç¡®ä¿ä¸‰è€…ä¸€è‡´æ€§ã€‚
  2. **ç­–ç•¥ä¼˜åŒ–ï¼ˆRLï¼‰**ï¼šé‡‡ç”¨ **Group Relative Policy Optimization (GRPO)**ï¼Œç»“åˆå¤åˆå¥–åŠ±å‡½æ•°ï¼Œå¯¹ç»“æ„å®Œæ•´æ€§ã€é€»è¾‘æ­£ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡è¿›è¡Œè”åˆä¼˜åŒ–ã€‚
- âœ… **å¯æ‰©å±•çš„å½¢å¼åŒ–éªŒè¯**ï¼šå°†å½¢å¼åŒ–éªŒè¯ä»ç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ•°å­¦å®šç†è¯æ˜ï¼‰æ¨å¹¿åˆ°é€šç”¨é€»è¾‘æ¨ç†é¢†åŸŸï¼Œé€‚ç”¨äºæ•°å­¦ã€é€»è¾‘å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | æœ¬æ–‡æ–¹æ³• |
|------|--------|--------|
| éªŒè¯æ—¶æœº | äº‹åéªŒè¯ï¼ˆPost-hocï¼‰ | **å®æ—¶äº¤é”™éªŒè¯**ï¼ˆInterleavedï¼‰ |
| é”™è¯¯å¤„ç† | è¢«åŠ¨è¿‡æ»¤ | **ä¸»åŠ¨çº æ­£** |
| éªŒè¯ç²’åº¦ | æœ€ç»ˆç­”æ¡ˆ | **ä¸­é—´æ­¥éª¤** |
| åº”ç”¨èŒƒå›´ | å¤šé™äºæ•°å­¦é¢†åŸŸ | **è·¨é¢†åŸŸé€šç”¨**ï¼ˆé€»è¾‘ã€æ•°å­¦ã€ä¸€èˆ¬æ¨ç†ï¼‰ |
| æ•°æ®è´¨é‡ | ä¾èµ–äººå·¥æ ‡æ³¨æˆ–å¼±ç›‘ç£ | **æ‰§è¡ŒéªŒè¯ä¿è¯é«˜å¯¹é½æ€§** |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **é€»è¾‘æ¨ç†**ï¼š
  - **KOR-Bench**ï¼šæ¶µç›–å¤šé¢†åŸŸçš„çŸ¥è¯†æ­£äº¤é€»è¾‘æ¨ç†ä»»åŠ¡ã€‚
  - **BIG-Bench Hard (BBH)**ï¼šåŒ…å«éœ€è¦å¤šæ­¥æ¨ç†çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚
- **æ•°å­¦æ¨ç†**ï¼š
  - **MATH-500**ï¼šç«èµ›çº§æ•°å­¦é—®é¢˜ã€‚
  - **AIME 2024**ï¼šå¥¥æ•°çº§åˆ«æ•°å­¦æ¨ç†æŒ‘æˆ˜ã€‚
- **ä¸€èˆ¬æ¨ç†**ï¼š
  - **GPQA-Diamond**ï¼šç ”ç©¶ç”Ÿçº§åˆ«çš„ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ç­‰å­¦ç§‘æ¨ç†ã€‚
  - **TheoremQA**ï¼šæµ‹è¯•æ¨¡å‹åœ¨æ•°å­¦ã€ç‰©ç†ã€EE&CS å’Œé‡‘èç­‰é¢†åŸŸåº”ç”¨å®šç†çš„èƒ½åŠ›ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **ä¸»å¹²æ¨¡å‹**ï¼šQwen2.5-7B å’Œ Qwen2.5-14Bã€‚
- **è®­ç»ƒæµç¨‹**ï¼š
  1. **SFTé˜¶æ®µ**ï¼šä½¿ç”¨çº¦17kæ ·æœ¬è¿›è¡Œç›‘ç£å¾®è°ƒã€‚
  2. **RLé˜¶æ®µ**ï¼šåœ¨éš¾åº¦è¾ƒé«˜çš„æ ·æœ¬ï¼ˆteacher model pass rate < 50%ï¼‰ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚
- **è¯„ä¼°æ–¹å¼**ï¼š
  - æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨ **OpenCompass** è¿›è¡Œè¯„ä¼°ã€‚
  - é™¤ AIME24 ä½¿ç”¨ `avg@16` æŠ½æ ·å¤–ï¼Œå…¶ä½™å‡ä½¿ç”¨è´ªå¿ƒè§£ç ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | åŸºçº¿æ¨¡å‹ |
|------|--------|
| åŸºç¡€æ¨¡å‹ | Qwen2.5-7B/14B |
| å¼ºåŒ–å­¦ä¹ æ¨¡å‹ | SimpleRL-Zoo, General-Reasoner, RLPR |
| é€»è¾‘å¢å¼ºæ¨¡å‹ | SynLogic-7B |
| å·¥å…·é›†æˆæ¨ç† | ZeroTIR, SimpleTIR |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰
| æ¨¡å‹ | é€»è¾‘å¹³å‡ | æ•°å­¦å¹³å‡ | ä¸€èˆ¬å¹³å‡ | æ€»ä½“å¹³å‡ |
|------|---------|---------|---------|---------|
| **Qwen2.5-7B-Base** | 27.6 | 33.4 | 29.7 | 30.2 |
| **Qwen2.5-7B-Instruct** | 53.6 | 42.2 | 39.9 | 45.2 |
| **FLV-SFT (Ours, 7B)** | 58.3 | 48.6 | 42.7 | **49.8** |
| **FLV-RL (Ours, 7B)** | 60.5 | 49.7 | 45.5 | **51.9** |
| **FLV-SFT (Ours, 14B)** | 65.8 | 50.9 | 48.5 | **55.7** |
| **FLV-RL (Ours, 14B)** | 67.5 | 55.8 | 52.5 | **58.6** |

> ğŸ“Œ **æ€»ä½“æå‡**ï¼šç›¸æ¯”å½“å‰æœ€ä¼˜ï¼ˆSOTAï¼‰åŸºçº¿ï¼Œ**7B æ¨¡å‹å¹³å‡æå‡ 10.4%**ï¼Œ**14B æ¨¡å‹å¹³å‡æå‡ 14.2%**ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- åœ¨ **AIME 2024** ä¸Šï¼ŒFLV-RL-14B è¾¾åˆ° **30.2%**ï¼Œè¿œè¶… General-Reasoner çš„ 17.5% å’Œ Base æ¨¡å‹çš„ 3.6%ã€‚
- åœ¨ **TheoremQA** ä¸Šï¼Œ14B æ¨¡å‹è¾¾åˆ° **63.5%**ï¼Œé¢†å…ˆç¬¬äºŒåè¶…è¿‡ 8 ä¸ªç™¾åˆ†ç‚¹ã€‚
- åœ¨ **KOR-Bench** ä¸Šï¼Œ14B æ¨¡å‹è¾¾åˆ° 57.0ï¼Œç›¸æ¯” General-Reasoner çš„ 41.3 æå‡ **15.7%**ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 2ï¼‰
| æ¨¡å‹ | KOR-Bench | BBH | MATH-500 | AIME24 | æ€»ä½“å¹³å‡ |
|------|----------|-----|---------|-------|---------|
| **Natural-SFT** | 30.4 | 55.9 | 56.6 | 8.5 | 36.5 |
| **FLV-SFT (Ours)** | 48.0 | 68.5 | 77.2 | 20.0 | **49.8** |
| **Natural-RL** | 35.7 | 55.4 | 54.4 | 4.8 | 37.0 |
| **FLV-RL (Ours)** | 51.0 | 70.0 | 78.6 | 20.8 | **51.9** |

> ğŸ” **å…³é”®å‘ç°**ï¼š
> - å½¢å¼åŒ–éªŒè¯ï¼ˆFLVï¼‰å¸¦æ¥ **+13.3%** çš„å¹³å‡æå‡ï¼ˆ49.8 vs 36.5ï¼‰ã€‚
> - åœ¨é€»è¾‘å¯†é›†ä»»åŠ¡ï¼ˆå¦‚ KOR-Benchï¼‰ä¸Šæå‡å°¤ä¸ºæ˜¾è‘—ï¼ˆ+17.6ï¼‰ã€‚
> - è‡ªç„¶è¯­è¨€ RL å‡ ä¹æ— æå‡ï¼ˆ37.0 vs 36.5ï¼‰ï¼Œè€Œ FLV-RL æ˜¾è‘—ä¼˜äº FLV-SFTï¼ˆ+2.1ï¼‰ï¼Œè¯´æ˜å½¢å¼åŒ–éªŒè¯æä¾›äº†æ›´ç¨³å®šå¯é çš„å¥–åŠ±ä¿¡å·ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **å½¢å¼åŒ–éªŒè¯èƒ½æ˜¾è‘—æå‡ LLM æ¨ç†èƒ½åŠ›**ï¼šé€šè¿‡å®æ—¶åé¦ˆï¼Œæ¨¡å‹èƒ½å¤Ÿé¿å… reward hackingï¼Œå»ºç«‹çœŸæ­£ç¨³å¥çš„é€»è¾‘é“¾æ¡ã€‚
2. âœ… **ä»â€œè®¡ç®—â€åˆ°â€œæ¨ç†â€çš„èŒƒå¼è½¬å˜**ï¼šåˆ†æè¡¨æ˜ï¼ŒFLV-RL æ›´å¤šåœ°è°ƒç”¨ **Symbolic & Logic** ç±»å·¥å…·ï¼ˆå¦‚ z3ã€sympyï¼‰ï¼Œè€Œå‡å°‘å¯¹ **Algorithmic & Search**ï¼ˆå¦‚ itertoolsï¼‰çš„ä¾èµ–ï¼Œè¡¨æ˜æ¨¡å‹ä»â€œæš´åŠ›æœç´¢â€è½¬å‘â€œæŠ½è±¡æ¨ç†â€ã€‚
3. âœ… **å°æ•°æ®å¤§æ•ˆæœ**ï¼šå°½ç®¡ä»…ä½¿ç”¨çº¦ 17k è®­ç»ƒæ ·æœ¬ï¼Œæ€§èƒ½ä»å¤§å¹…è¶…è¶Šä¾èµ–å¤§è§„æ¨¡æ•°æ®çš„åŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†æ–¹æ³•çš„**æ•°æ®é«˜æ•ˆæ€§**ï¼ˆdata efficiencyï¼‰ã€‚
4. âœ… **çµæ´»çš„éªŒè¯ç­–ç•¥æ›´ä¼˜**ï¼šå¼ºåˆ¶æ¯æ­¥éªŒè¯ä¼šæŠ‘åˆ¶ç›´æ¥è®¡ç®—èƒ½åŠ›ï¼Œé™ä½æ•°å­¦æ€§èƒ½ï¼›è€Œ**çµæ´»éªŒè¯ç­–ç•¥**ï¼ˆdecoupling calculation from validationï¼‰åœ¨ä¿æŒé€»è¾‘ä¸¥è°¨çš„åŒæ—¶æå‡äº†æ•°å­¦è¡¨ç°ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. âš ï¸ **è®¡ç®—å¼€é”€å¢åŠ **ï¼šé›†æˆå®æ—¶å½¢å¼åŒ–éªŒè¯ä½¿ RL è®­ç»ƒæ—¶é—´å¢åŠ çº¦ **2å€**ã€‚
2. âš ï¸ **å½¢å¼åŒ–è½¬æ¢æŒ‘æˆ˜**ï¼šå°†æ¨¡ç³Šæˆ–å¸¸è¯†æ€§å¼ºçš„è‡ªç„¶è¯­è¨€æè¿°è‡ªåŠ¨è½¬åŒ–ä¸ºå½¢å¼åŒ–è¡¨ç¤ºä»å­˜åœ¨å›°éš¾ï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯çš„éªŒè¯åé¦ˆã€‚
3. âš ï¸ **GPQA æ•°æ®é›†å¯é æ€§é—®é¢˜**ï¼šä½œè€…å¼•ç”¨ç ”ç©¶æŒ‡å‡º GPQA å­˜åœ¨äº‹å®æ€§é”™è¯¯å’Œå‚æ•°ç¼ºå¤±ï¼Œå¯èƒ½å½±å“è¯„ä¼°å…¬æ­£æ€§ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼€å‘æ›´é²æ£’çš„ **auto-formalization** æŠ€æœ¯ï¼Œæå‡è‡ªç„¶è¯­è¨€åˆ°å½¢å¼åŒ–é€»è¾‘çš„è½¬æ¢æˆåŠŸç‡ã€‚
- æ¢ç´¢æ›´é«˜æ•ˆçš„éªŒè¯è°ƒåº¦æœºåˆ¶ï¼Œå¹³è¡¡æ¨ç†æˆæœ¬ä¸æ€§èƒ½å¢ç›Šã€‚
- å°†è¯¥æ¡†æ¶æ‰©å±•è‡³å¼€æ”¾åŸŸã€éç»“æ„åŒ–æ¨ç†ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥éªŒè¯å…¶é€šç”¨æ€§ã€‚

> ğŸ’¡ **æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºçš„ **FLV æ¡†æ¶**æˆåŠŸåœ°å°†å½¢å¼åŒ–é€»è¾‘çš„ä¸¥è°¨æ€§ä¸ LLM çš„è¯­è¨€æµç•…æ€§ç›¸ç»“åˆï¼Œé€šè¿‡**åŠ¨æ€äº¤é”™éªŒè¯**å’Œ**ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä¸ºæ„å»ºå¯ä¿¡ AI æ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦è·¯å¾„ã€‚

</details>

---

### 10. [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)

**Authors**: Ji Shi, Peiming Guo, Meishan Zhang, Miao Zhang, Xuebo Liu, Min Zhang, Weili Guan  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.22803v1  

#### Abstract
Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models thro...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆç³»ç»Ÿä¾èµ–**ä»£ç éªŒè¯å™¨ï¼ˆcode verifierï¼‰**åœ¨æ¨ç†æ—¶å¯¹å¤šä¸ªå€™é€‰ä»£ç è¿›è¡Œç­›é€‰ï¼Œä»¥é€‰å‡ºæœ€ä¼˜è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„éªŒè¯å™¨è®­ç»ƒæ–¹æ³•ä¸»è¦ä¾èµ–**ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼Œé¢ä¸´ä»¥ä¸‹ä¸‰å¤§æŒ‘æˆ˜ï¼š
- **æ•°æ®ç¨€ç¼º**ï¼šé«˜è´¨é‡çš„å•å…ƒæµ‹è¯•æ ‡æ³¨æ•°æ®éš¾ä»¥è·å–ï¼›
- **é«˜é”™è¯¯ç‡**ï¼šSFTæ¨¡å‹ç”Ÿæˆçš„å•å…ƒæµ‹è¯•å¸¸å­˜åœ¨è¯­æ³•é”™è¯¯æˆ–é€»è¾‘å¤±è´¥ï¼›
- **æ¨ç†æ•ˆç‡ä½**ï¼šéœ€é‡‡æ ·å¤§é‡æµ‹è¯•ç”¨ä¾‹æ‰èƒ½ä¿è¯å¯é æ€§ï¼Œå¯¼è‡´ä¸¥é‡æ•ˆç‡ç“¶é¢ˆã€‚

å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºæ— ç›‘ç£è®­ç»ƒæä¾›äº†å¯èƒ½ï¼Œä½†**ä»…ä½¿ç”¨åŠŸèƒ½æ­£ç¡®æ€§å¥–åŠ±ï¼ˆfunctionality rewardsï¼‰çš„æœ´ç´ RLæ–¹æ³•**æ— æ³•æœ‰æ•ˆè¦†ç›–å¤æ‚åˆ†æ”¯å’Œå›°éš¾æ ·æœ¬ï¼Œå¯¼è‡´éªŒè¯æ•ˆæœä¸ä½³ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡º **CVeDRL**ï¼ˆCode Verifier via Difficulty-aware Reinforcement Learningï¼‰ï¼Œä¸€ç§åŸºäº**éš¾åº¦æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ **çš„é«˜æ•ˆä»£ç éªŒè¯å™¨è®­ç»ƒæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### ï¼ˆ1ï¼‰ç†è®ºåˆ†ææŒ‡å¯¼å¤šç»´å¥–åŠ±è®¾è®¡
é¦–æ¬¡ä»ç†è®ºä¸Šåˆ†æäº†**å•å…ƒæµ‹è¯•å¤šæ•°æŠ•ç¥¨æœºåˆ¶**ä¸­ï¼Œ**æµ‹è¯•é€šè¿‡ç‡ï¼ˆpass rateï¼‰ã€åˆ†æ”¯è¦†ç›–ç‡ï¼ˆbranch coverageï¼‰ä¸éªŒè¯å™¨å¯é æ€§ä¹‹é—´çš„å…³ç³»**ï¼Œæ¨å¯¼å‡ºæå‡éªŒè¯ä¿¡å¿ƒçš„å…³é”®å› ç´ ï¼Œä»è€Œä¸ºRLä¸­çš„å¤šç»´åº¦å¥–åŠ±è®¾è®¡æä¾›ç†è®ºä¾æ®ã€‚

#### ï¼ˆ2ï¼‰æ„å»ºè¯­æ³•-åŠŸèƒ½å¤åˆå¥–åŠ±ï¼ˆSyntax-Functionality Composite Rewardï¼‰
å¼•å…¥ä¸¤ç§äº’è¡¥å¥–åŠ±ä¿¡å·ï¼š
- **Syntax Reward**ï¼šåŸºäºASTè§„åˆ™åˆ¤æ–­è¾“å‡ºæ˜¯å¦ç¬¦åˆ `unittest` æ ¼å¼è¦æ±‚ï¼›
- **Functionality Reward**ï¼šæ ¹æ®æ‰§è¡Œç»“æœåˆ†ç±»ä¸º errorã€failure æˆ– passï¼Œå¹¶ç»“åˆ**åˆ†æ”¯è¦†ç›–ç‡**ä½œä¸ºæ­£å‘æ¿€åŠ±ã€‚

#### ï¼ˆ3ï¼‰æå‡ºâ€œåˆ†æ”¯-æ ·æœ¬â€åŒéš¾åº¦æ„ŸçŸ¥æœºåˆ¶
- **Branch-Difficulty-aware RL**ï¼šé‡‡ç”¨**æŒ‡æ•°å‹å¥–åŠ±å¡‘å½¢ï¼ˆexponential reward shapingï¼‰**ï¼Œæ”¾å¤§å¯¹ç¨€æœ‰åˆ†æ”¯ï¼ˆå¦‚è¾¹ç•Œæ¡ä»¶ï¼‰çš„å¥–åŠ±ï¼Œé¼“åŠ±æ¢ç´¢éš¾è¦†ç›–è·¯å¾„ã€‚
- **Sample-Difficulty-aware RL**ï¼šå¼•å…¥é™æ€ä»£ç åˆ†ææŒ‡æ ‡ä½œä¸ºå…ˆéªŒéš¾åº¦ä¼°è®¡ï¼š
  - **Halstead Complexity**ï¼ˆè®¤çŸ¥è´Ÿè·ï¼‰
  - **Maintainability Index (MI)**ï¼ˆå¯ç»´æŠ¤æ€§å€’ç½®ï¼‰
  ç»“åˆäºŒè€…å‡ ä½•å¹³å‡å¾—åˆ°ç»¼åˆéš¾åº¦ $D$ï¼Œç”¨äºè°ƒèŠ‚å¥–åŠ±å¼ºåº¦ã€‚

#### ï¼ˆ4ï¼‰ä½¿ç”¨ GRPO è¿›è¡Œé«˜æ•ˆè®­ç»ƒ
é‡‡ç”¨ **Group Relative Policy Optimization (GRPO)**ï¼Œæ— éœ€ä»·å€¼ç½‘ç»œï¼Œç›´æ¥ç»„å†…æ¯”è¾ƒä¼˜åŠ¿ï¼Œé™ä½è®­ç»ƒå¼€é”€ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | CVeDRL ä¼˜åŠ¿ |
|------|-------------|
| **æ€§èƒ½** | åœ¨ä»… 0.6B å‚æ•°è§„æ¨¡ä¸‹è¶…è¶Š GPT-3.5 å’Œ CodeRM-8B |
| **æ•ˆç‡** | æ¨ç†é€Ÿåº¦æ¯” SFT æ¨¡å‹å¿« **20å€ä»¥ä¸Š**ï¼ˆtoken throughputï¼‰ |
| **è´¨é‡** | ç”Ÿæˆæ›´å°‘å†—ä½™æ–­è¨€ï¼Œæ›´é«˜é€šè¿‡ç‡ä¸åˆ†æ”¯è¦†ç›–ç‡ |
| **æ³›åŒ–æ€§** | æ˜¾è‘—æå‡å¼€æºä¸é—­æºç­–ç•¥æ¨¡å‹çš„éªŒè¯æ•ˆæœ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
å…±ä½¿ç”¨å››ä¸ªåŸºå‡†æ•°æ®é›†è¿›è¡Œç»¼åˆè¯„ä¼°ï¼š
| æ•°æ®é›† | æè¿° |
|--------|------|
| **HuManEval+** | HumanEval çš„å¢å¼ºç‰ˆæœ¬ï¼Œå«å‡½æ•°çº§ç¼–ç¨‹ä»»åŠ¡ |
| **MBPP+** | MBPP æ‰©å±•ç‰ˆï¼Œä¾§é‡å®é™…åœºæ™¯ä¸‹çš„å°è§„æ¨¡ç¼–ç¨‹é—®é¢˜ |
| **LiveCodeBench** | åŒ…å«å¼€å‘è€…çœŸå®æäº¤ä»£ç çš„çœŸå®ä¸–ç•Œè¯„æµ‹é›†ï¼ˆé€‰å–2024å¹´1â€“9æœˆå‘å¸ƒçš„168ä¸ªå‡½æ•°å¼é—®é¢˜ï¼‰ |
| **LeetCode** | ç®—æ³•é¢˜å­é›†ï¼Œè¿‡æ»¤æ‰ç³»ç»Ÿè®¾è®¡ç±»é¢˜ç›®åä¿ç•™542é“çº¯å‡½æ•°é¢˜ |

è®­ç»ƒæ•°æ®æ¥è‡ª **CoDERM** æ•°æ®é›†ï¼ˆ>50K åˆæˆé—®é¢˜ï¼‰ï¼Œç¡®ä¿è§£å†³æ–¹æ¡ˆæ­£ç¡®ä¸”é€‚é…ã€‚

---

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### éªŒè¯-ç¼–ç å™¨æ€§èƒ½ï¼ˆValidation-Coder Performanceï¼‰
- **ç›®æ ‡**ï¼šè¡¡é‡ä¸åŒ reward model å¯¹ policy model è¾“å‡ºçš„ç­›é€‰èƒ½åŠ›ã€‚
- **æµç¨‹**ï¼šæ¯ä¸ªé—®é¢˜ç”Ÿæˆ N ä¸ªå€™é€‰ä»£ç  â†’ ç”±éªŒè¯å™¨ç”Ÿæˆ M ä¸ªæµ‹è¯•ç”¨ä¾‹ â†’ æ‰§è¡Œå¹¶ç»Ÿè®¡é€šè¿‡æ•° â†’ å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€ä¼˜ã€‚
- **ä¸»æŒ‡æ ‡**ï¼šPass@Nï¼ˆæœ€ç»ˆé€‰ä¸­æœ€ä¼˜è§£çš„æ¯”ä¾‹ï¼‰

#### å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°ï¼ˆTest Quality Evaluationï¼‰
ç›´æ¥è¿è¡Œç”Ÿæˆçš„æµ‹è¯•å¥—ä»¶ï¼Œè¯„ä¼°å…¶å†…åœ¨è´¨é‡ï¼š
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **Error Rate (ER%)** â†“ | å› è¯­æ³•é”™è¯¯ç­‰å¯¼è‡´æ— æ³•æ‰§è¡Œçš„æ¦‚ç‡ |
| **Failure Rate (FR%)** â†“ | æµ‹è¯•æˆåŠŸè¿è¡Œä½†é¢„æœŸè¾“å‡ºé”™è¯¯çš„æ¦‚ç‡ |
| **Pass Rate (PR%)** â†‘ | æ­£ç¡®é€šè¿‡æ‰€æœ‰æ–­è¨€çš„æ¯”ä¾‹ |
| **Branch Coverage (BC%)** â†‘ | è¦†ç›–çš„ä»£ç åˆ†æ”¯/è¡Œæ¯”ä¾‹ |
| **Assertion Number (AN)** â†“ | å¹³å‡æ¯ä¸ªæµ‹è¯•ç±»ä¸­çš„æ–­è¨€æ•°é‡ï¼ˆè¶Šå°‘è¶Šå¥½ï¼Œè¡¨ç¤ºç®€æ´ï¼‰ |

---

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
#### ç­–ç•¥æ¨¡å‹ï¼ˆPolicy Modelsï¼‰
- GPT-3.5, GPT-4o-mini, LLaMA3-8B, LLaMA3-70B

#### éªŒè¯å™¨åŸºçº¿ï¼ˆReward Modelsï¼‰
- **Vanilla**ï¼šä¸é‡æ’åºï¼Œå– top-1
- **MBR-E**ï¼šåŸºäºæ‰§è¡Œé£é™©æœ€å°åŒ–çš„è´å¶æ–¯å†³ç­–
- **CodeT**ï¼šåŒæ‰§è¡Œä¸€è‡´æ€§è¯„åˆ†
- **CodeRM-8B**ï¼šSFTè®­ç»ƒçš„8Bå‚æ•°æµ‹è¯•ç”Ÿæˆå™¨
- **LLaMA3-70B**ï¼šç›´æ¥ç”¨å¤§æ¨¡å‹ä½œéªŒè¯å™¨

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 1 & 2ï¼‰

#### âœ… åœ¨ MBPP+ ä¸Šçš„è¡¨ç°ï¼ˆPass Rateï¼‰
| æ–¹æ³• | GPT-3.5 | GPT-4o-mini |
|------|---------|-------------|
| Vanilla | 49.17% | 71.32% |
| CodeRM-8B | 66.63% | 75.18% |
| **CVeDRL-0.6B** | **66.79%** | **76.93%** |

ğŸ‘‰ **ç›¸æ¯” GPT-4o-mini + CodeRM æå‡ 1.75ppï¼Œæ˜¯å½“å‰ SOTA**

#### âœ… åœ¨ HuManEval+ ä¸Šçš„è¡¨ç°
| æ–¹æ³• | GPT-3.5 | GPT-4o-mini |
|------|---------|-------------|
| Vanilla | 53.43% | 82.57% |
| CodeRM-8B | 78.13% | 86.49% |
| **CVeDRL-0.6B** | **78.96%** | **87.05%** |

ğŸ‘‰ è¾¾åˆ°å½“å‰æœ€ä¼˜æ°´å¹³ï¼Œå°å¹…é¢†å…ˆ CodeRM-8B

#### âœ… å•å…ƒæµ‹è¯•è´¨é‡ï¼ˆTable 2ï¼‰
åœ¨ MBPP+ ä¸Šï¼š
| æ¨¡å‹ | ER% â†“ | FR% â†“ | PR% â†‘ | BC% â†‘ | AN â†“ |
|------|-------|-------|--------|--------|-----|
| GPT-4o | 3.98 | 29.89 | 66.13 | 96.91 | 6.12 |
| CodeRM-8B | 2.44 | 52.86 | 44.70 | 97.11 | 7.88 |
| **CVeDRL-0.6B** | **0.53** | **15.79** | **83.68** | **97.37** | **3.13** |

âœ… **é€šè¿‡ç‡æå‡é«˜è¾¾ 17.55% vs GPT-4o-miniï¼ŒåŒæ—¶å‡å°‘è¶…è¿‡ä¸€åŠçš„å†—ä½™æ–­è¨€**

---

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Study, Table 3ï¼‰

| æ¶ˆèé…ç½® | MBPP+ PR% | BC% |
|----------|------------|------|
| Full CVeDRL (0.6B) | **83.68** | **97.37** |
| w/o Sample-Difficulty (SDA) | 71.42 | 96.75 |
| w/o Branch-Difficulty (BDA) | 79.96 | 92.14 |
| w/o Syntax Reward | 51.53 | 85.11 |

ğŸ‘‰ **ä¸‰ä¸ªç»„ä»¶å‡æœ‰æ˜¾è‘—è´¡çŒ®ï¼Œç»„åˆä½¿ç”¨æ•ˆæœæœ€ä½³**ï¼š
- **æŒ‡æ•°å¥–åŠ±å¡‘å½¢ï¼ˆBDAï¼‰** æ˜¾è‘—æå‡è¦†ç›–ç‡ï¼ˆ+5.23ppï¼‰
- **é™æ€éš¾åº¦æ„ŸçŸ¥ï¼ˆSDAï¼‰** æå¤§é™ä½å¤±è´¥ç‡ï¼Œæé«˜é€šè¿‡ç‡ï¼ˆ+13.72ppï¼‰
- **è¯­æ³•å¥–åŠ±** æ˜¯åŸºç¡€ä¿éšœï¼Œç¼ºå¤±å°†å¯¼è‡´å´©æºƒå¼ä¸‹é™

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å¤šç»´RLå¥–åŠ±ä¼˜äºå•ä¸€åŠŸèƒ½æ€§å¥–åŠ±**ï¼šå•çº¯ä¾èµ–åŠŸèƒ½åé¦ˆä¸è¶³ä»¥å¼•å¯¼é«˜è´¨é‡æµ‹è¯•ç”Ÿæˆï¼›åŠ å…¥è¯­æ³•çº¦æŸã€è¦†ç›–ç‡ã€éš¾åº¦æ„ŸçŸ¥å¯æ˜¾è‘—æå‡éªŒè¯å¯é æ€§ã€‚
2. **å°æ¨¡å‹ä¹Ÿèƒ½èƒœä»»é«˜æ•ˆéªŒè¯**ï¼šCVeDRL ä»…ç”¨ **0.6B å‚æ•°**å³è¾¾åˆ°ç”šè‡³è¶…è¶Š GPT-3.5 å’Œ CodeRM-8B æ€§èƒ½ï¼Œè¯æ˜**ç´§å‡‘æ¨¡å‹ + å¼ºåŒ–å­¦ä¹  > å·¨å‹æ¨¡å‹ + SFT**ã€‚
3. **æ¨ç†æ•ˆç‡æå¤§æå‡**ï¼šç›¸æ¯”ä¼ ç»Ÿ SFT æ–¹æ³•ï¼ˆå¦‚ CodeRMï¼‰ï¼ŒCVeDRL å®ç° **>20x çš„ token throughput åŠ é€Ÿ**ï¼ˆè§ Table 4ï¼‰ã€‚
4. **é‡‡æ ·æ•ˆç‡æ›´é«˜**ï¼šå›¾2æ˜¾ç¤ºï¼ŒCVeDRL åœ¨ä»…éœ€ **10ä¸ªæµ‹è¯•ç”¨ä¾‹**å³å¯è¾¾åˆ°æ€§èƒ½é¥±å’Œï¼Œè¿œä½äºå…¶ä»–æ–¹æ³•æ‰€éœ€çš„100æ¬¡é‡‡æ ·ï¼Œå¤§å¹…ç¼©çŸ­éªŒè¯æ—¶é—´ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
1. **éƒ¨åˆ†ä»£ç æ”¯æŒä¸è¶³**ï¼šç›®å‰ä»…é€‚ç”¨äºå®Œæ•´å‡½æ•°çº§åˆ«çš„éªŒè¯ï¼Œæ— æ³•å¤„ç†ä»£ç è¡¥å…¨ç­‰å±€éƒ¨ç”Ÿæˆä»»åŠ¡ã€‚
2. **ç¼ºä¹æ­£ç¡®æ€§åŒºåˆ†èƒ½åŠ›**ï¼šè™½ç„¶èƒ½æœ‰æ•ˆè¿‡æ»¤é”™è¯¯ä»£ç ï¼Œä½†å°šä¸èƒ½ç²¾ç¡®åˆ¤æ–­ä¸¤ä¸ªâ€œéƒ½é€šè¿‡â€çš„å®ç°å“ªä¸ªæ›´ä¼˜ã€‚
3. **è¯­è¨€ä¸ç”Ÿæ€æ‰©å±•æœ‰é™**ï¼šå½“å‰å·¥ä½œèšç„¦äº Python åŠå…¶æ ‡å‡†åº“ï¼Œå°šæœªæ‰©å±•è‡³å¤šè¯­è¨€æˆ–å¤šä¾èµ–ç¯å¢ƒã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. å°†éªŒè¯ç®¡é“æ‰©å±•è‡³**éƒ¨åˆ†ä»£ç ï¼ˆpartial codeï¼‰å’Œåº“çº§åˆ«æµ‹è¯•ç”Ÿæˆ**ï¼›
2. å¼•å…¥**ä»£ç å˜å¼‚ï¼ˆmutation testingï¼‰æˆ–å½¢å¼åŒ–åŸåˆ™**æ¥å¢å¼ºå¯¹â€œä¼ªæ­£ç¡®â€ä»£ç çš„è¯†åˆ«èƒ½åŠ›ï¼›
3. æ”¯æŒæ›´å¤šç¼–ç¨‹è¯­è¨€åŠå¤æ‚ä¾èµ–ç®¡ç†ç¯å¢ƒï¼›
4. æ¢ç´¢å°†è¯¥æ¡†æ¶åº”ç”¨äºæŒç»­é›†æˆï¼ˆCIï¼‰è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆåœºæ™¯ã€‚

---

> ğŸ”— **ä»£ç å·²å¼€æº**ï¼š[https://github.com/LIGHTCHASER1/CVeDRL.git](https://github.com/LIGHTCHASER1/CVeDRL.git)

</details>

---

### 11. [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)

**Authors**: Heehoon Kim, Jaehwan Lee, Taejeoung Kim, Jongwon Park, Jinpyo Kim, Pyongwon Suh, Ryan H. Choi, Sangwoo Lee, Jaejin Lee  
**Category**: cs.DC  
**Published**: 2026-02-02  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.22585v1  

#### Abstract
The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*HetCCL: Accelerating LLM Training with Heterogeneous GPUs*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å½“å‰ä¸»æµçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ PyTorchï¼‰ä¾èµ–äºå‚å•†ä¸“ç”¨çš„é›†ä½“é€šä¿¡åº“ï¼ˆCCLï¼‰ï¼Œä¾‹å¦‚ NVIDIA çš„ **NCCL** å’Œ AMD çš„ **RCCL**ã€‚è¿™äº›åº“äº’ä¸å…¼å®¹ï¼Œå¯¼è‡´åœ¨æ··åˆä½¿ç”¨ä¸åŒå‚å•† GPUï¼ˆå¦‚ NVIDIA ä¸ AMDï¼‰çš„å¼‚æ„é›†ç¾¤ä¸­æ— æ³•è¿›è¡Œé«˜æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒã€‚

å°½ç®¡ç»„ç»‡å¸¸é€šè¿‡é€æ­¥é‡‡è´­æ„å»ºå¼‚æ„ GPU é›†ç¾¤ä»¥é™ä½æˆæœ¬ï¼Œä½†ç°æœ‰æŠ€æœ¯æ— æ³•å®ç°è·¨å‚å•† GPU çš„é€æ˜ååŒï¼Œé€ æˆç¡¬ä»¶èµ„æºæµªè´¹ã€è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡º **HetCCL** â€”â€” é¦–ä¸ªæ”¯æŒè·¨å‚å•† GPU çš„é›†ä½“é€šä¿¡åº“ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹ä»»ä½•åº”ç”¨ä»£ç çš„å‰æä¸‹ï¼Œç»Ÿä¸€è°ƒåº¦ NVIDIA å’Œ AMD GPU è¿›è¡Œé«˜æ•ˆçš„å¤§æ¨¡å‹è®­ç»ƒã€‚

å…¶æ ¸å¿ƒåˆ›æ–°æœºåˆ¶åŒ…æ‹¬ï¼š

- **Runtime API æŠ½è±¡å±‚ï¼ˆTACCï¼‰**  
  è®¾è®¡äº†ä¸€ä¸ªç»Ÿä¸€çš„è¿è¡Œæ—¶æ¥å£æŠ½è±¡å±‚ **TACC**ï¼ˆTransparent Accelerator Communication Channelï¼‰ï¼Œå°† CUDA å’Œ HIP çš„è¿è¡Œæ—¶ API ç»Ÿä¸€ä¸ºå¹³å°æ— å…³çš„è°ƒç”¨ï¼Œå±è”½åº•å±‚å·®å¼‚ã€‚

- **å¹³å°ç‰¹å®šè®¾å¤‡ä»£ç ç¼–è¯‘ç­–ç•¥**  
  å°† GPU å†…æ ¸ä»£ç è§£è€¦ä¸ºç‹¬ç«‹çš„å…±äº«åº“ï¼ˆ`.so`ï¼‰ï¼Œåˆ†åˆ«ä½¿ç”¨ `nvcc` å’Œ `hipcc` ç¼–è¯‘ï¼Œå¹¶åœ¨è¿è¡Œæ—¶åŠ¨æ€åŠ è½½å¯¹åº”å¹³å°çš„å†…æ ¸ï¼Œå®ç°å¤šå‚å•†äºŒè¿›åˆ¶å…±å­˜ã€‚

- **åŸºäº RDMA çš„è·¨å‚å•†ç‚¹å¯¹ç‚¹é€šä¿¡**  
  åˆ©ç”¨æ ‡å‡† **RDMA** åè®®ï¼ˆå¦‚ InfiniBandï¼‰ç›´æ¥è®¿é—®å¯¹æ–¹ GPU æ˜¾å­˜ï¼Œæ— éœ€ä¿®æ”¹é©±åŠ¨æˆ–å›ºä»¶ï¼Œå®ç°äº†é›¶æ‹·è´ã€é«˜æ€§èƒ½çš„æ•°æ®ä¼ è¾“ã€‚

- **GPU æ„ŸçŸ¥çš„å¾®æ‰¹æ¬¡è´Ÿè½½å‡è¡¡ï¼ˆGPU-aware micro-batch sizingï¼‰**  
  æ ¹æ®å„ GPU çš„å®æµ‹ååé‡ï¼ˆtokens/sï¼‰åŠ¨æ€åˆ†é…å¾®æ‰¹æ¬¡å¤§å°ï¼Œç¼“è§£å› æ€§èƒ½å·®å¼‚å¯¼è‡´çš„ **straggler effect**ã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| æ–¹é¢ | ç°æœ‰æ–¹æ¡ˆï¼ˆNCCL/RCCL/MPIï¼‰ | HetCCL |
|------|----------------------------|--------|
| è·¨å‚å•†æ”¯æŒ | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ NVIDIA + AMD |
| æ€§èƒ½ | åœ¨åŒæ„ç¯å¢ƒä¸­æœ€ä¼˜ | æ¥è¿‘åŸç”Ÿæ€§èƒ½ï¼ˆ~97% æ•ˆç‡ï¼‰ |
| å…¼å®¹æ€§ | éœ€è¦é‡æ–°ç¼–è¯‘æˆ–ä¿®æ”¹ä»£ç  | âœ… æ— éœ€ä¿®æ”¹åº”ç”¨ï¼Œæ”¯æŒ `LD_PRELOAD` æ³¨å…¥ |
| æ‰©å±•æ€§ | å—é™äºå•ä¸€å‚å•†ç”Ÿæ€ | å¯æ‰©å±•è‡³å…¶ä»–åŠ é€Ÿå™¨ï¼ˆNEW acceleratorï¼‰ |
| é€šä¿¡æœºåˆ¶ | å¤šæ•°éœ€ä¸»æœºå†…å­˜ä¸­è½¬ | âœ… åŸç”Ÿæ”¯æŒ GPUDirect RDMA |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ¨¡å‹ä¸ä»»åŠ¡**

- **æ¨¡å‹**ï¼šGPT-125Mã€GPT-355Mã€LLaMA-1Bã€LLaMA-3B
- **ä»»åŠ¡**ï¼šå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç«¯åˆ°ç«¯è®­ç»ƒ
- **ç²¾åº¦**ï¼šFP16ï¼ˆæ€§èƒ½æµ‹è¯•ï¼‰ã€BF16ï¼ˆæ”¶æ•›æ€§åˆ†æï¼‰
- **å¹¶è¡Œç­–ç•¥**ï¼šDeepSpeed çš„ **ZeRO-1** å’Œ **ZeRO-3**ï¼ˆå³ FSDPï¼‰

### **å®éªŒè®¾ç½®**

- **ç¡¬ä»¶é…ç½®**ï¼š
  - 4 èŠ‚ç‚¹é›†ç¾¤ï¼Œæ¯èŠ‚ç‚¹ 4 å— GPU
    - 2 èŠ‚ç‚¹ Ã— 4 Ã— NVIDIA Tesla V100 (PCIe Gen3)
    - 2 èŠ‚ç‚¹ Ã— 4 Ã— AMD Radeon Pro W7800 (PCIe Gen4)
  - ç½‘ç»œäº’è”ï¼šMellanox ConnectX-6 InfiniBand HDR
  - è½¯ä»¶æ ˆï¼šPyTorch + DeepSpeed + ROCm/CUDA

- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - é€šä¿¡å¸¦å®½ï¼ˆGB/sï¼‰
  - è®­ç»ƒååé‡ï¼ˆtokens/sï¼‰
  - åŠ é€Ÿæ¯”ï¼ˆspeedupï¼‰
  - æ¨¡å‹æ”¶æ•›æ›²çº¿ä¸æœ€ç»ˆ loss
  - æ•ˆç‡ï¼ˆEfficiencyï¼‰ = $ \frac{\text{HetCCL (NVIDIA+AMD)}}{\text{NCCL (NVIDIA-only)} + \text{RCCL (AMD-only)}} $

- **åŸºçº¿æ–¹æ³•å¯¹æ¯”**ï¼š
  - **NCCL**ï¼šNVIDIA åŒæ„ç¯å¢ƒåŸºå‡†
  - **RCCL**ï¼šAMD åŒæ„ç¯å¢ƒåŸºå‡†
  - **GPU-aware MPI**ï¼šä½œä¸ºè·¨å‚å•†é€šä¿¡çš„æ›¿ä»£æ–¹æ¡ˆè¿›è¡Œæ¯”è¾ƒ

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

#### âœ… **é€šä¿¡æ€§èƒ½**

- **ç‚¹å¯¹ç‚¹é€šä¿¡ï¼ˆRDMAï¼‰**ï¼š
  - HetCCL åœ¨è·¨å‚å•†é€šä¿¡ä¸­è¾¾åˆ°æ¥è¿‘ NCCL çš„å¸¦å®½æ°´å¹³ï¼ˆè§ Figure 8ï¼‰
  - å¯¹äº 1GB æ•°æ®ï¼ŒHetCCL(HET) è¾¾åˆ°çº¦ **90%** çš„ NCCL å³°å€¼å¸¦å®½

- **é›†ä½“é€šä¿¡æ“ä½œï¼ˆAll-Reduce / All-Gather / Reduce-Scatterï¼‰**ï¼š
  - åœ¨ 16 GPU å¼‚æ„ç¯å¢ƒä¸‹ï¼ˆ8N + 8Aï¼‰ï¼ŒHetCCL å®ç°ç¨³å®šå¯æ‰©å±•çš„é€šä¿¡æ€§èƒ½
  - æ€§èƒ½å—é™äºè¾ƒæ…¢çš„ä¸€æ–¹ï¼ˆé€šå¸¸ä¸º RCCLï¼‰ï¼Œä½†ä»ä¿æŒçº¿æ€§æ‰©å±•è¶‹åŠ¿ï¼ˆè§ Figure 7ï¼‰

#### âœ… **ç«¯åˆ°ç«¯è®­ç»ƒæ€§èƒ½**

- **æœ€å¤§åŠ é€Ÿæ¯”**ï¼š
  - ç›¸æ¯”ä»…ç”¨ AMD GPUï¼ˆRCCLï¼‰ï¼š**æœ€é«˜è¾¾ 2.97Ã— speedup**
  - ç›¸æ¯”ä»…ç”¨ NVIDIA GPUï¼ˆNCCLï¼‰ï¼š**æœ€é«˜è¾¾ 1.48Ã— speedup**

- **ç³»ç»Ÿæ•ˆç‡**ï¼š
  - å¹³å‡æ•ˆç‡çº¦ä¸º **90%**
  - æœ€é«˜å¯è¾¾ **97%**ï¼Œè¡¨æ˜ HetCCL å‡ ä¹æ— é¢å¤–å¼€é”€åœ°èšåˆäº†å¼‚æ„ç®—åŠ›

- **å¯æ‰©å±•æ€§**ï¼š
  - ä» 8 GPUï¼ˆ4A+4Nï¼‰æ‰©å±•åˆ° 16 GPUï¼ˆ8A+8Nï¼‰æ—¶ï¼Œååé‡å‡ ä¹ç¿»å€ï¼Œæ˜¾ç¤ºè‰¯å¥½çš„è¿‘çº¿æ€§æ‰©å±•èƒ½åŠ›

#### âœ… **æ¶ˆèå®éªŒç»“æœ**

- **RDMA çš„å½±å“ï¼ˆAppendix F.1ï¼‰**ï¼š
  - å…³é—­ RDMA åï¼Œé€šä¿¡è·¯å¾„é€€åŒ–ä¸º GPU â†’ CPU â†’ NIC â†’ CPU â†’ GPU
  - å°æ¶ˆæ¯å»¶è¿Ÿæ˜¾è‘—ä¸Šå‡ï¼Œå¤§æ¶ˆæ¯å¸¦å®½ä¸‹é™è¶…è¿‡ 50%
  - è¯æ˜ **RDMA æ˜¯å®ç°é«˜æ€§èƒ½çš„å…³é”®**

- **è´Ÿè½½å‡è¡¡çš„å½±å“ï¼ˆAppendix F.2ï¼‰**ï¼š
  - ä½¿ç”¨ GPU æ„ŸçŸ¥çš„å¾®æ‰¹æ¬¡åˆ†é…åï¼Œç«¯åˆ°ç«¯è®­ç»ƒé€Ÿåº¦æå‡ **1.08Ã— ~ 1.22Ã—**
  - ç‰¹åˆ«æ˜¯åœ¨å¤§æ¨¡å‹ä¸Šæ•ˆæœæ›´æ˜æ˜¾ï¼ˆå¦‚ LLaMA-3B æå‡ 1.08Ã—ï¼‰
  - åˆ†ææŒ‡å‡ºï¼šé€šä¿¡å¼€é”€é™åˆ¶äº†è¿›ä¸€æ­¥æé€Ÿçš„ç©ºé—´

- **ä¸ GPU-aware MPI å¯¹æ¯”ï¼ˆAppendix E.3ï¼‰**ï¼š
  - åœ¨å°æ¶ˆæ¯åœºæ™¯ä¸‹ï¼ŒMPI å»¶è¿Ÿæ›´ä½
  - ä½†åœ¨å¤§æ¶ˆæ¯å’Œé›†ä½“æ“ä½œä¸­ï¼ŒHetCCL æ˜¾è‘—ä¼˜äº MPIï¼ˆå°¤å…¶ All-Reduceï¼‰
  - åŸå› ï¼šMPI é€šå¸¸åœ¨ CPU ä¸Šæ‰§è¡Œå½’çº¦æ“ä½œï¼Œè€Œ HetCCL å®Œå…¨åœ¨ GPU ä¸Šå®Œæˆï¼Œé¿å…äº†ä¸»æœºå†…å­˜æ‹·è´

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. âœ… **è·¨å‚å•† GPU é€šä¿¡æ˜¯å¯è¡Œä¸”é«˜æ•ˆçš„**  
   æ— éœ€ä¿®æ”¹é©±åŠ¨æˆ–ç¡¬ä»¶ï¼Œä»…é€šè¿‡æ ‡å‡† RDMA åè®®å³å¯å®ç°å¼‚æ„ GPU é—´çš„é«˜æ€§èƒ½é€šä¿¡ã€‚

2. âœ… **HetCCL å¯æ— ç¼é›†æˆç°æœ‰è®­ç»ƒæ ˆ**  
   é€šè¿‡ `LD_PRELOAD` æ›¿æ¢ NCCL/RCCL ç¬¦å·ï¼Œæ— éœ€æ›´æ”¹ä»»ä½• PyTorch æˆ– DeepSpeed ä»£ç ã€‚

3. âœ… **æ¥è¿‘åŸç”Ÿæ€§èƒ½ + é«˜æ•ˆèµ„æºåˆ©ç”¨**  
   åœ¨å¼‚æ„ç¯å¢ƒä¸­ï¼ŒHetCCL å®ç°é«˜è¾¾ **97% çš„é€šä¿¡æ•ˆç‡**ï¼Œæœ‰æ•ˆèšåˆäº†åŸæœ¬å­¤ç«‹çš„è®¡ç®—èµ„æºã€‚

4. âœ… **æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬**  
   é€šè¿‡æ•´åˆæ—§æœ‰å’Œæ–°å‹ GPUï¼Œå‡å°‘ GPU-hour æ¶ˆè€—ï¼Œç›´æ¥é™ä½è®­ç»ƒæ€»æˆæœ¬ã€‚

5. âœ… **ä¸å½±å“æ¨¡å‹å‡†ç¡®æ€§**  
   LLaMA-1B çš„è®­ç»ƒæŸå¤±æ›²çº¿ä¸ NCCL/RCCL å®Œå…¨ä¸€è‡´ï¼Œæœ€ç»ˆ loss å·®å¼‚å°äº $7\times10^{-3}$ï¼Œå¤„äº BF16 æ•°å€¼è¯¯å·®èŒƒå›´å†…ã€‚

---

### **å±€é™æ€§ï¼ˆLimitationsï¼‰**

1. **èŠ‚ç‚¹çº§å¼‚æ„ï¼Œéå¡çº§æ··åˆ**  
   å½“å‰è®¾è®¡å‡è®¾æ¯ä¸ªèŠ‚ç‚¹åªæœ‰ä¸€ç§å‚å•† GPUï¼ˆinter-node heterogeneityï¼‰ï¼Œå°šæœªæ”¯æŒå•èŠ‚ç‚¹å†…æ··æ’ï¼ˆintra-node mixed-vendorï¼‰ã€‚

2. **é™æ€è´Ÿè½½åˆ†é…**  
   å¾®æ‰¹æ¬¡æ¯”ä¾‹åŸºäºåˆå§‹ profiling ç¡®å®šï¼Œæœªè€ƒè™‘è¿è¡Œæ—¶æ€§èƒ½æ³¢åŠ¨ï¼ˆå¦‚çƒ­èŠ‚æµã€èµ„æºç«äº‰ï¼‰ã€‚æœªæ¥å¯å¼•å…¥åœ¨çº¿é‡æ ¡å‡†æœºåˆ¶ã€‚

3. **å®éªŒè§„æ¨¡æœ‰é™**  
   å®éªŒæœ€å¤šä½¿ç”¨ 16 å— GPUï¼Œæ¨¡å‹å‚æ•°ä¸Šé™ä¸º 3Bã€‚æ›´å¤§è§„æ¨¡é›†ç¾¤çš„è¡¨ç°æœ‰å¾…éªŒè¯ã€‚

4. **ä¾èµ– RDMA ç¯å¢ƒ**  
   è‹¥æ—  InfiniBand/RoCE æ”¯æŒï¼Œåˆ™é€€åŒ–ä¸ºä¸»æœºä¸­ç»§æ¨¡å¼ï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **æ”¯æŒ intra-node æ··åˆéƒ¨ç½²**  
   æ‰©å±•è‡³å•èŠ‚ç‚¹å†…åŒæ—¶è¿è¡Œ NVIDIA ä¸ AMD GPU çš„åœºæ™¯ã€‚

2. **åŠ¨æ€è´Ÿè½½å†å¹³è¡¡ï¼ˆDynamic Load Balancingï¼‰**  
   å¼•å…¥ per-step profilingï¼Œå®æ—¶è°ƒæ•´ micro-batch size ä»¥åº”å¯¹è¿è¡Œæ—¶å˜åŒ–ã€‚

3. **æ‰©å±•è‡³æ›´å¤šå‚å•†**  
   æ”¯æŒ Intel GPUã€åä¸º Ascendã€Apple Silicon ç­‰å…¶ä»–æ¶æ„ï¼Œæ‰“é€ é€šç”¨å¼‚æ„é€šä¿¡å¹³å°ã€‚

4. **ä¸ MoE æ¶æ„ç»“åˆä¼˜åŒ–**  
   æ¢ç´¢åœ¨ Mixture-of-Experts ä¸­æŒ‰ä¸“å®¶èƒ½åŠ›åˆ†é…ä¸åŒå‚å•† GPU çš„è°ƒåº¦ç­–ç•¥ã€‚

5. **é›†æˆè‡³ä¸»æµæ¡†æ¶é»˜è®¤åˆ†å‘ç‰ˆ**  
   æ¨åŠ¨ PyTorch/TensorFlow åŸç”Ÿæ”¯æŒ HetCCL ç±»æœºåˆ¶ï¼Œæˆä¸ºå¼‚æ„è®­ç»ƒçš„æ ‡å‡†ç»„ä»¶ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **HetCCL æˆåŠŸæ‰“ç ´äº† GPU å‚å•†ä¹‹é—´çš„é€šä¿¡å£å’ï¼Œåœ¨ä¸æ”¹åŠ¨ä»»ä½•è®­ç»ƒä»£ç çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†è·¨ NVIDIA ä¸ AMD GPU çš„é«˜æ€§èƒ½ã€é«˜æ•ˆç‡ LLM è®­ç»ƒï¼Œä¸ºæ„å»ºä½æˆæœ¬ã€å¯æŒç»­æ¼”è¿›çš„ AI åŸºç¡€è®¾æ–½æä¾›äº†å…³é”®æŠ€æœ¯è·¯å¾„ã€‚**

</details>

---

### 12. [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)

**Authors**: Bo Yuan, Yun Zhou, Zhichao Xu, Kiran Ramnath, Aosong Feng, Balasubramaniam Srinivasan  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.22305v1  

#### Abstract
Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow ge...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šBayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿçš„ **agentic workflow** è®¾è®¡ä¾èµ–äººå·¥è®¾è®¡å’Œåå¤è¯•é”™ï¼Œè¿‡ç¨‹è€—æ—¶ä¸”éœ€è¦å¤§é‡é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œé™åˆ¶äº†å…¶åœ¨æ–°ä»»åŠ¡ä¸Šçš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚ç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•å¤šå°† workflow ç”Ÿæˆè§†ä¸º**ä¼˜åŒ–é—®é¢˜**ï¼ˆå¦‚åŸºäºæœç´¢çš„ç­–ç•¥ï¼‰ï¼Œç¼ºä¹ç†è®ºåŸºç¡€ï¼Œé€šå¸¸åªäº§å‡ºå•ä¸€é«˜åˆ†æ–¹æ¡ˆï¼Œå¤šæ ·æ€§ä¸è¶³ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
æœ¬æ–‡æå‡º **Bayesian Workflow Generation (BWG)**ï¼Œå°†è‡ªåŠ¨ workflow ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä¸€ä¸ª**è´å¶æ–¯åéªŒé‡‡æ ·é—®é¢˜**ï¼Œè€Œéä¼ ç»Ÿä¼˜åŒ–é—®é¢˜ã€‚å…·ä½“å®ç°ä¸º **BayesFlow**ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹æ ¸å¿ƒæœºåˆ¶ï¼š

- **å¹¶è¡Œå‰ç» rollout (Parallel look-ahead rollouts)**ï¼š  
  åœ¨æ¯ä¸€æ­¥ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„åç»­è·¯å¾„ï¼ˆrolloutï¼‰ï¼Œé€šè¿‡å…¶æ½œåœ¨å¥–åŠ±ï¼ˆå¦‚éªŒè¯å‡†ç¡®ç‡ï¼‰å¯¹å½“å‰å‰ç¼€è¿›è¡Œé‡è¦æ€§åŠ æƒï¼Œä»è€Œåœ¨æ— ä¸­é—´å¥–åŠ±çš„æƒ…å†µä¸‹å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æœºåˆ¶é¿å…äº†å¯¹æ˜‚è´µçš„ reward model æˆ–é—­æºå¼ºæ¨¡å‹çš„ä¾èµ–ã€‚

- **é¡ºåºå¾ªç¯ç²¾ç‚¼ (Sequential in-loop refinement)**ï¼š  
  åœ¨æ¯è½®è¿­ä»£ä¸­å¼•å…¥ä¸€ä¸ªå…¨å±€ç²¾ç‚¼æ¨¡å—ï¼ˆrefinerï¼‰ï¼Œåˆ©ç”¨å·²æœ‰å®Œæ•´ workflow æ± è¿›è¡Œæ”¹è¿›ï¼ˆå¦‚é€šè¿‡ MCTSï¼‰ï¼Œä¿®æ­£æ—©æœŸé”™è¯¯ï¼Œæå‡æ•´ä½“è´¨é‡ã€‚

- **ç†è®ºä¿è¯**ï¼š  
  è¯æ˜åœ¨æ—  refiner çš„æƒ…å†µä¸‹ï¼ŒåŠ æƒç»éªŒåˆ†å¸ƒ**æ¸è¿‘æ”¶æ•›äºç›®æ ‡åéªŒåˆ†å¸ƒ**ï¼ˆTheorem 1ï¼‰ã€‚å³ä½¿å¼•å…¥ refinerï¼Œä¹Ÿæä¾›äº† TV è·ç¦»ç•Œé™ï¼Œè¯´æ˜å…¶åœ¨å¯æ§èŒƒå›´å†…æ‰©å±•æ¢ç´¢ï¼ˆTheorem 2ï¼‰ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **ç†è®ºæ›´æ‰å®**ï¼šæä¾›æ”¶æ•›æ€§è¯æ˜ï¼ŒåŒºåˆ«äºé»‘ç®±æœç´¢æ–¹æ³•ã€‚
- **å¤šæ ·æ€§æ›´å¼º**ï¼šé€šè¿‡é‡‡æ ·è‡ªç„¶äº§ç”Ÿå¤šæ ·åŒ–é«˜è´¨é‡ workflowã€‚
- **æ•ˆç‡æ›´é«˜**ï¼šå‡å°‘å†—ä½™è¯„ä¼°ï¼Œtoken ä½¿ç”¨é‡æ˜¾è‘—ä½äº AFlowã€‚
- **é€šç”¨æ€§å¼º**ï¼šä¸ä¾èµ–é¢„å®šä¹‰ agent æ¨¡å—ï¼ˆå¦‚ ENSEMBLE, REVISEï¼‰ï¼Œä»…æš´éœ²è½»é‡çº§æ¥å£ï¼ˆ`chat_completion`, `exec_code`ï¼‰ï¼Œç”± meta optimizer LLM è‡ªä¸»è®¾è®¡é€»è¾‘ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
åœ¨å…­ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–ä¸‰å¤§ç±»ä»»åŠ¡ï¼š
- **æ•°å­¦æ¨ç† (Math Reasoning)**ï¼šMATH, GSM8K
- **é—®ç­” (Question Answering)**ï¼šHotpotQA, DROP
- **ä¸“å®¶çŸ¥è¯† (Expert Knowledge)**ï¼šGPQA, MMLU-Pro

æ­¤å¤–ï¼Œåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ **MBPP** ä¸Šè¿›è¡Œäº†é¢å¤–æµ‹è¯•ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **Meta Optimizer LLM**ï¼šClaude 3.5-Sonnet
- **Executor LLM**ï¼šClaude 3.7-Sonnet æˆ– Qwen2.5-7B-Instruct
- **è¶…å‚æ•°**ï¼š`K=1`ï¼ˆå‰ç» rollout æ•°ï¼‰ã€`N=10`ï¼ˆæ¯è½®éƒ¨åˆ† workflow æ•°ï¼‰ã€`M=10`ï¼ˆç²¾ç‚¼ workflow æ•°ï¼‰ã€`T=5`ï¼ˆæœ€å¤§æ­¥æ•°ï¼‰
- **è¯„ä¼°æ–¹å¼**ï¼šæ¯ä¸ªé€‰å‡ºçš„ workflow åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° 5 æ¬¡ï¼ŒæŠ¥å‘Šå‡å€¼ä¸æ ‡å‡†å·®ã€‚

#### è¯„ä¼°æŒ‡æ ‡
| æ•°æ®é›†        | æŒ‡æ ‡         |
|---------------|--------------|
| GSM8K, MATH   | Solve Rate   |
| HotpotQA, DROP| F1 Score     |
| GPQA, MMLU-Pro| Accuracy     |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Prompting åŸºçº¿**ï¼š
  - Zero-shot (IO)
  - Zero-shot Chain-of-Thought (CoT)
  - CoT with Self-Consistency (CoT-SC)
- **è‡ªåŠ¨ workflow ç”ŸæˆåŸºçº¿**ï¼š
  - ADAS (çº¿æ€§å¯å‘å¼æœç´¢)
  - AFlow (Monte Carlo Tree Search)
  - MaAS (å¤šæ™ºèƒ½ä½“æ¶æ„æœç´¢)

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1ï¼‰

#### ä½¿ç”¨ **Claude 3.7-Sonnet** ä½œä¸ºæ‰§è¡Œå™¨ï¼š
| æ–¹æ³•          | å¹³å‡å‡†ç¡®ç‡ | æœ€ä½³å•é¡¹è¡¨ç° |
|---------------|------------|-------------|
| AFlow (SOTA)  | 76.2%      | â€”           |
| **BayesFlow (Ours)** | **80.8%**  | **MATH: 69.4%**, **GPQA: 69.2%** |

- åœ¨ **MATH** ä¸Šè¶…è¶Š AFlow **9.3ä¸ªç™¾åˆ†ç‚¹**
- åœ¨ **GPQA** ä¸Šè¶…è¶Š AFlow **3.9ä¸ªç™¾åˆ†ç‚¹**
- æ‰€æœ‰æ•°æ®é›†ä¸Šå‡å–å¾—**æœ€é«˜å¹³å‡å‡†ç¡®ç‡**

#### ä½¿ç”¨ **Qwen 2.5-7B-Instruct** ä½œä¸ºæ‰§è¡Œå™¨ï¼š
| æ–¹æ³•          | å¹³å‡å‡†ç¡®ç‡ |
|---------------|------------|
| AFlow         | 59.9%      |
| **BayesFlow (Ours)** | **63.4%**  |

- å¹³å‡æå‡ **3.5ä¸ªç™¾åˆ†ç‚¹**ï¼Œè¡¨æ˜æ–¹æ³•å¯¹å°æ¨¡å‹åŒæ ·æœ‰æ•ˆã€‚

#### åœ¨ MBPP ä¸Šçš„è¡¨ç°ï¼š
- **BayesFlow**: **Pass@1 = 85.0%**
- **AFlow**: 75.5%
- æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- ç›¸æ¯” **SOTA workflow æ–¹æ³• (AFlow)**ï¼ŒBayesFlow å¹³å‡æå‡ **4.6â€“6.5ä¸ªç™¾åˆ†ç‚¹**ã€‚
- ç›¸æ¯” **zero-shot prompting**ï¼Œæå‡é«˜è¾¾ **65ä¸ªç™¾åˆ†ç‚¹**ã€‚
- åœ¨ **HotpotQA** ä¸Šï¼ŒAFlow æ–¹å·®å¤§ï¼ˆå¶å°”å¤±è´¥ï¼‰ï¼Œè€Œ BayesFlow æ›´ç¨³å®šã€‚

---

### æ¶ˆèå®éªŒç»“æœ
#### ï¼ˆ1ï¼‰Token æ•ˆç‡å¯¹æ¯”ï¼ˆTable 2ï¼‰
BayesFlow åœ¨å¤šæ•°åœºæ™¯ä¸‹ä»¥**æ›´ä½çš„ token æ¶ˆè€—**è¾¾åˆ°**æ›´é«˜çš„å‡†ç¡®ç‡**ï¼Œå°¤å…¶åœ¨è¾“å…¥ token ä¸Šä¼˜åŠ¿æ˜æ˜¾ï¼ˆå¦‚ DROP ä¸ŠèŠ‚çœçº¦ 87M tokensï¼‰ã€‚

#### ï¼ˆ2ï¼‰ä¸åŒ N å’Œ M çš„é²æ£’æ€§ï¼ˆTable 3ï¼‰
å›ºå®šæ€»é¢„ç®— `N + M = 20`ï¼Œè°ƒæ•´ `N âˆˆ {5,10,15}`ï¼š
- æ€§èƒ½åœ¨ä¸åŒé…ç½®ä¸‹ä¿æŒç¨³å®šï¼Œè¡¨æ˜æ–¹æ³•å¯¹æ¢ç´¢-åˆ©ç”¨æƒè¡¡ä¸æ•æ„Ÿã€‚

#### ï¼ˆ3ï¼‰æ¨ç†æ—¶ç¼©æ”¾æ•ˆæœï¼ˆTable 4ï¼‰
æ¯”è¾ƒ Top-L workflows çš„èšåˆæ€§èƒ½ï¼ˆBest@L, Mean@L, Majority@Lï¼‰ï¼š
- BayesFlow åœ¨ **Best@L** ä¸Šä¼˜åŠ¿æ˜¾è‘—ï¼ˆå¦‚ L=8 æ—¶ 86 vs 81ï¼‰ï¼Œè¯´æ˜å…¶ç”Ÿæˆçš„ workflow **æ›´å¤šæ ·ä¸”é«˜è´¨é‡**ï¼Œè‡³å°‘æœ‰ä¸€ä¸ªèƒ½è§£å‡ºé—®é¢˜çš„æ¦‚ç‡æ›´é«˜ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
- å°† workflow ç”Ÿæˆå»ºæ¨¡ä¸º **Bayesian posterior sampling** æ˜¯ä¸€ä¸ª**åŸåˆ™æ€§æ›´å¼º**çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¼˜äºçº¯ä¼˜åŒ–è§†è§’ã€‚
- **å¹¶è¡Œå‰ç» rollout** èƒ½æœ‰æ•ˆä¼°è®¡å‰ç¼€ä»·å€¼ï¼ŒæŒ‡å¯¼ç”Ÿæˆï¼Œä¸”å…·å¤‡ç†è®ºæ”¶æ•›ä¿è¯ã€‚
- **å¾ªç¯ç²¾ç‚¼æœºåˆ¶** è™½ç‰ºç‰²ä¸¥æ ¼æ”¶æ•›æ€§ï¼Œä½†å¸¦æ¥æ˜¾è‘—å®è¯æ”¶ç›Šï¼Œæå‡å¤šæ ·æ€§å’Œæœ€ç»ˆæ€§èƒ½ã€‚
- BayesFlow åœ¨å¤šç§æ¨¡å‹å’Œä»»åŠ¡ä¸Šå‡**ä¸€è‡´ä¼˜äºç°æœ‰ SOTA æ–¹æ³•**ï¼Œä¸”æ›´é«˜æ•ˆã€æ›´ç¨³å®šã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **æ¨ç†æˆæœ¬è¾ƒé«˜**ï¼šå¹¶è¡Œ rollout å¢åŠ äº†æ¨ç†æ—¶è®¡ç®—å¼€é”€ã€‚
- **çŸ­æœŸè®°å¿†**ï¼šå¤§éƒ¨åˆ† rollout è¢«ä¸¢å¼ƒï¼Œæœªé•¿æœŸå¤ç”¨é«˜ä»·å€¼è½¨è¿¹ã€‚
- **è¯„ä¼°èŒƒå›´æœ‰é™**ï¼šç›®å‰ä»…åœ¨ 7 ä¸ªåŸºå‡†å’Œä¸¤ç±»æ‰§è¡Œå™¨ä¸ŠéªŒè¯ï¼Œå°šæœªè¦†ç›–å¤šæ¨¡æ€æˆ–æ›´å¤æ‚ agent åœºæ™¯ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å¼•å…¥**åŠ¨æ€æ§åˆ¶å™¨**ï¼Œæ ¹æ®åœ¨çº¿ä¿¡å·è‡ªé€‚åº”è°ƒæ•´è¶…å‚æ•°ä»¥å¹³è¡¡æˆæœ¬ä¸ç²¾åº¦ã€‚
- è®¾è®¡æ›´å¼ºçš„**è®°å¿†æœºåˆ¶**ï¼Œä¿ç•™å¹¶ä¼˜å…ˆé‡ç”¨é«˜ä»·å€¼ workflow è½¨è¿¹ã€‚
- å°† BWG æ¡†æ¶æ‰©å±•è‡³**å¤šæ¨¡æ€ä»»åŠ¡**å’Œ**æ›´å¤æ‚çš„ agent åä½œç³»ç»Ÿ**ï¼Œè¿›ä¸€æ­¥éªŒè¯å…¶é€šç”¨æ€§ä¸é¢„ç®—-ç²¾åº¦æƒè¡¡èƒ½åŠ›ã€‚

</details>

---

### 13. [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)

**Authors**: Babak Shahbaba, Zahra Moslemi  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.22539v1  

#### Abstract
Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compu...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šNeural-Inspired Posterior Approximation (NIPA)

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è´å¶æ–¯æ¨æ–­ï¼ˆBayesian inferenceï¼‰åœ¨å¤æ‚é«˜ç»´æ¨¡å‹ï¼ˆå¦‚ **Bayesian Deep Learning**ï¼‰ä¸­é¢ä¸´ä¸¥é‡çš„è®¡ç®—ç“¶é¢ˆã€‚ä¼ ç»Ÿçš„ **MCMC** æ–¹æ³•ï¼ˆå¦‚ HMCï¼‰è™½ç„¶æ— åï¼Œä½†è®¡ç®—ä»£ä»·é«˜æ˜‚ï¼›è€Œ **Variational Bayes (VB)** è™½ç„¶é«˜æ•ˆï¼Œå´å¸¸å¼•å…¥åå·®å¹¶ä½ä¼°ä¸ç¡®å®šæ€§ã€‚å¦‚ä½•åœ¨ä¿è¯åéªŒè¿‘ä¼¼è´¨é‡çš„åŒæ—¶æå‡é‡‡æ ·æ•ˆç‡ï¼Œæ˜¯å½“å‰å¤§è§„æ¨¡è´å¶æ–¯å­¦ä¹ çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§å—ç¥ç»ç§‘å­¦å¯å‘çš„é€šç”¨æ¡†æ¶â€”â€”**Neural-Inspired Posterior Approximation (NIPA)**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯æ¨¡æ‹Ÿäººç±»å¤§è„‘ä¸­ä¸‰ç§äº’è¡¥çš„å­¦ä¹ ä¸å†³ç­–æœºåˆ¶ï¼š

- **Model-Based (MB) æ¨¡å—**ï¼šåŸºäºç›®æ ‡åˆ†å¸ƒè¿›è¡Œç²¾ç¡®ä½†ç¼“æ…¢çš„æ¢ç´¢ï¼ˆç±»æ¯” HMCï¼‰ï¼Œç¡®ä¿é‡‡æ ·çš„å‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚
- **Model-Free (MF) æ¨¡å—**ï¼šåˆ©ç”¨å†å²æ ·æœ¬è®­ç»ƒä¸€ä¸ª **surrogate function**ï¼ˆå¦‚ DNNï¼‰ï¼Œå®ç°å¿«é€Ÿã€åå°„å¼çš„é‡‡æ ·ï¼Œé¿å…é‡å¤è®¡ç®—æ˜‚è´µçš„ç›®æ ‡å‡½æ•°ã€‚
- **Episodic Control (EC) æ¨¡å—**ï¼šé€šè¿‡æ£€ç´¢æœ€è¿‘é‚»çš„å†å²æ ·æœ¬ï¼ˆâ€œè®°å¿†â€ï¼‰ï¼Œç›´æ¥å¤ç”¨å…¶ log-posterior å€¼ï¼Œå®ç°â€œä¸€é”®å¼â€å¿«é€Ÿæ¥å—/æ‹’ç»åˆ¤æ–­ã€‚

NIPA åŠ¨æ€åœ°åœ¨ä¸‰ä¸ªæ¨¡å—ä¹‹é—´åˆ‡æ¢ï¼Œä¾æ®å½“å‰æè®®çŠ¶æ€ä¸å·²æœ‰æ ·æœ¬æ± çš„è·ç¦»ï¼ˆæ ‡å‡†åŒ–è·ç¦» $d^*$ï¼‰å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å—ï¼š
- è‹¥è·ç¦»è¿œï¼ˆ$d^* > t_2$ï¼‰â†’ å¯ç”¨ MB è¿›è¡Œç¨³å¥æ¢ç´¢ï¼›
- ä¸­ç­‰è·ç¦»ï¼ˆ$t_1 < d^* \leq t_2$ï¼‰â†’ å¯ç”¨ MF åˆ©ç”¨å­¦ä¹ åˆ°çš„æ¨¡å¼ï¼›
- æ¥è¿‘å·²æœ‰æ ·æœ¬ï¼ˆ$d^* \leq t_1$ï¼‰â†’ å¯ç”¨ EC å¿«é€Ÿå†³ç­–ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **ç»Ÿä¸€è§†è§’**ï¼šå°† MCMCã€VBã€CES ç­‰æ–¹æ³•è§†ä¸º NIPA çš„ç‰¹ä¾‹ï¼ˆå¦‚ä»…ç”¨ MB å³ä¸ºæ ‡å‡† MCMCï¼Œä»…ç”¨ MF ç±»ä¼¼ CESï¼‰ï¼Œæä¾›äº†ä¸€ä¸ªæ›´å¹¿æ³›çš„ç†è®ºæ¡†æ¶ã€‚
- **é«˜æ•ˆæ€§**ï¼šæ˜¾è‘—å‡å°‘å¯¹æ˜‚è´µç›®æ ‡å‡½æ•°ï¼ˆå¦‚å®Œæ•´æ•°æ®é›†ä¸Šçš„ log-posteriorï¼‰çš„è°ƒç”¨æ¬¡æ•°ï¼Œå°¤å…¶åœ¨å·²æ¢ç´¢åŒºåŸŸé™„è¿‘é€šè¿‡ EC å’Œ MF é¿å…é‡å¤è®¡ç®—ã€‚
- **å‡†ç¡®æ€§**ï¼šä¿ç•™äº† MB æ¨¡å—çš„ç²¾ç¡®æ€§ï¼Œé˜²æ­¢å› è¿‡åº¦ä¾èµ–è¿‘ä¼¼å¯¼è‡´çš„åå·®ç´¯ç§¯ã€‚
- **å¯æ‰©å±•æ€§**ï¼šé€‚ç”¨äºé«˜ç»´å‚æ•°ç©ºé—´ï¼ˆå¦‚ BNNsï¼‰ï¼Œå…‹æœä¼ ç»Ÿ MCMC åœ¨å¤§æ•°æ®ä¸‹çš„å¯æ‰©å±•æ€§é—®é¢˜ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **å›å½’ä»»åŠ¡**ï¼š
  - **åˆæˆæ•°æ®é›†**ï¼šåŸºäºä¸¤å±‚ ReLU ç½‘ç»œç”Ÿæˆï¼Œå« 5,000 æ ·æœ¬ã€100 ç‰¹å¾ã€çº¦ 3.5k å‚æ•°ã€‚
  - **Year Prediction MSD**ï¼šçœŸå®éŸ³ä¹æ•°æ®é›†ï¼Œé¢„æµ‹æ­Œæ›²å‘è¡Œå¹´ä»½ï¼Œå« 515,345 æ ·æœ¬ã€90 ç‰¹å¾ã€çº¦ 310k å‚æ•°ã€‚
- **åˆ†ç±»ä»»åŠ¡**ï¼š
  - **åˆæˆäºŒåˆ†ç±»æ•°æ®é›†**ï¼šä¸¤å±‚ ReLU ç½‘ç»œç”Ÿæˆï¼Œ20,000 æ ·æœ¬ã€512 ç‰¹å¾ã€çº¦ 148k å‚æ•°ã€‚
  - **MNIST Odd vs. Even**ï¼šåŒºåˆ†å¥‡å¶æ•°å­—ï¼Œ70,000 æ ·æœ¬ã€784 ç‰¹å¾ã€çº¦ 218k å‚æ•°ã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **é‡‡æ ·å™¨é…ç½®**ï¼šæ‰€æœ‰æ–¹æ³•ç”Ÿæˆ 2,000 ä¸ª posterior samplesã€‚
- **è¯„ä¼°ç»´åº¦**ï¼š
  - **é¢„æµ‹æ€§èƒ½**ï¼š
    - å›å½’ï¼š**RMSE**ï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰
    - åˆ†ç±»ï¼š**Accuracy**
  - **ä¸ç¡®å®šæ€§é‡åŒ–è´¨é‡**ï¼š
    - å›å½’ï¼š**CP95**ï¼ˆ95% é¢„æµ‹åŒºé—´çš„è¦†ç›–ç‡ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
    - åˆ†ç±»ï¼š**ECE**ï¼ˆExpected Calibration Errorï¼Œè¶Šä½è¶Šå¥½ï¼‰
  - **è®¡ç®—æ•ˆç‡ä¸é‡‡æ ·è´¨é‡**ï¼š
    - **Wall-clock time (s)**ï¼šæ€»è¿è¡Œæ—¶é—´
    - **Effective Sample Size (ESS)**ï¼šæœ€å°ã€ä¸­ä½æ•°ã€æœ€å¤§ ESSï¼Œè¡¡é‡é‡‡æ ·é“¾æ··åˆé€Ÿåº¦
    - **minESS/s**ï¼šå•ä½æ—¶é—´å†…çš„æœ€å° ESSï¼Œç»¼åˆæ•ˆç‡æŒ‡æ ‡
    - **Speedup**ï¼šç›¸å¯¹äº BNN-HMC åŸºçº¿çš„åŠ é€Ÿæ¯”

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **MCMC ç±»**ï¼šBNN-HMCï¼ˆåŸºå‡†ï¼‰ã€BNN-SGHMCã€BNN-pCN
- **å˜åˆ†ç±»**ï¼šBNN-VIã€BNN-LASSO
- **è¿‘ä¼¼/é›†æˆç±»**ï¼šBNN-MCDï¼ˆMC Dropoutï¼‰ã€SWAGã€BNN-RNSï¼ˆRandom Network Surrogateï¼‰
- **éè´å¶æ–¯å‚è€ƒ**ï¼šDNNã€DNN-Ensemble

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Tables 1 & 2ï¼‰

| æ–¹æ³• | ä»»åŠ¡ | Speedup (vs HMC) | RMSE / Accuracy | CP95 / ECE | minESS/s |
|------|------|------------------|------------------|-------------|----------|
| **BNN-HMC (Baseline)** | Year Prediction | 1.00 | 8.96 | 85.89 | 0.0053 |
| **NIPA (MB/MF/EC)** | Year Prediction | **6.99** | **8.89** | **83.15** | **0.0370** |
| **BNN-HMC (Baseline)** | MNIST (Class.) | 1.00 | 98.07 | 0.51 | 0.0051 |
| **NIPA (MB/MF/EC)** | MNIST (Class.) | **8.65** | **98.01** | **0.39** | **0.028** |

> æ³¨ï¼šNIPA åœ¨ä¿æŒé¢„æµ‹æ€§èƒ½ï¼ˆRMSE/Accuracyï¼‰ä¸ HMC ç›¸å½“ç”šè‡³ç•¥ä¼˜çš„åŒæ—¶ï¼Œå®ç°äº† **7â€“9 å€çš„é€Ÿåº¦æå‡**ï¼Œä¸”ä¸ç¡®å®šæ€§æ ¡å‡†æ›´å¥½ï¼ˆæ›´ä½ ECE æˆ–åˆç† CP95ï¼‰ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- NIPA æ˜¾è‘—ä¼˜äºæ‰€æœ‰å…¶ä»–æ–¹æ³•åœ¨ **minESS/s** ä¸Šçš„è¡¨ç°ï¼Œè¯´æ˜å…¶åœ¨å•ä½æ—¶é—´å†…èƒ½äº§ç”Ÿæ›´å¤šæœ‰æ•ˆæ ·æœ¬ã€‚
- ç›¸æ¯”äºçº¯è¿‘ä¼¼æ–¹æ³•ï¼ˆå¦‚ VIã€MCDã€SWAGï¼‰ï¼ŒNIPA æä¾›äº†æ›´å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆæ›´å¥½çš„ CP95/ECEï¼‰ã€‚
- ç›¸æ¯”äºå…¶ä»– MCMC å˜ä½“ï¼ˆå¦‚ SGHMCã€pCNï¼‰ï¼ŒNIPA åœ¨ç›¸åŒä»»åŠ¡ä¸Šå®ç°äº†æ›´é«˜çš„åŠ é€Ÿæ¯”ã€‚

### æ¶ˆèå®éªŒç»“æœ
ä½œè€…å¯¹ NIPA çš„ä¸‰ä¸ªç»„ä»¶è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼ˆè§ Table 1 & 2 ä¸­çš„å­ç‰ˆæœ¬ï¼‰ï¼š

| å˜ä½“ | ç‰¹ç‚¹ | æ€§èƒ½è¡¨ç° |
|------|------|---------|
| **NIPA (MB only)** | ä»…ä½¿ç”¨ HMCï¼Œæ— åŠ é€Ÿæœºåˆ¶ | é€Ÿåº¦æ¥è¿‘æˆ–æ…¢äº HMCï¼ˆSpeedup â‰ˆ1ï¼‰ï¼Œæ•ˆç‡æœ€ä½ |
| **NIPA (MF only)** | ä»…ä½¿ç”¨ surrogate å‡½æ•° | é€Ÿåº¦å¿«ï¼ˆSpeedup è¾¾ 7.99â€“10.60ï¼‰ï¼Œä½† ECE æœ€å·®ï¼ˆ0.27ï¼‰ï¼Œè¡¨æ˜è¿‡åº¦ä¾èµ–è¿‘ä¼¼å¯èƒ½æŸå®³æ ¡å‡† |
| **NIPA (EC only)** | ä»…ä½¿ç”¨è®°å¿†æ£€ç´¢ | åŠ é€Ÿæ˜æ˜¾ï¼ˆSpeedupâ‰ˆ2.5â€“15.7ï¼‰ï¼Œä½†åœ¨é«˜ç»´ä¸‹æ³›åŒ–èƒ½åŠ›æœ‰é™ |
| **NIPA (MB/MF/EC)** | å®Œæ•´ä¸‰æ¨¡å—ååŒ | **æœ€ä½³å¹³è¡¡**ï¼šé«˜é€Ÿåº¦ + é«˜è´¨é‡é‡‡æ · + è‰¯å¥½ä¸ç¡®å®šæ€§é‡åŒ– |

> ç»“è®ºï¼š**ä¸‰è€…ååŒè‡³å…³é‡è¦**ã€‚å•ç‹¬ä½¿ç”¨ä»»ä¸€æ¨¡å—æ— æ³•å…¼é¡¾æ•ˆç‡ä¸å¯é æ€§ï¼Œè€Œç»„åˆç­–ç•¥å®ç°äº†æœ€ä¼˜æƒè¡¡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ç”Ÿç‰©å¯å‘æœºåˆ¶å¯æœ‰æ•ˆæŒ‡å¯¼ç®—æ³•è®¾è®¡**ï¼šå°†äººè„‘ä¸­çš„ model-basedã€model-free å’Œ episodic æ§åˆ¶æœºåˆ¶æ˜ å°„åˆ°è´å¶æ–¯é‡‡æ ·ä¸­ï¼Œèƒ½å¤Ÿæ„å»ºå‡ºé«˜æ•ˆä¸”é²æ£’çš„ posterior approximation æ¡†æ¶ã€‚
2. **åŠ¨æ€æ¨¡å—åˆ‡æ¢æ˜¾è‘—æå‡æ•ˆç‡**ï¼šé€šè¿‡ç®€å•çš„è·ç¦»é—¨æ§æœºåˆ¶ï¼Œåœ¨æœªå……åˆ†æ¢ç´¢åŒºåŸŸä½¿ç”¨ç²¾ç¡® MBï¼Œåœ¨å·²çŸ¥åŒºåŸŸä½¿ç”¨å¿«é€Ÿ MF/ECï¼Œå¤§å¹…å‡å°‘äº†æ˜‚è´µå‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚
3. **NIPA å®ç°äº†æ•ˆç‡ä¸ç²¾åº¦çš„è‰¯å¥½å¹³è¡¡**ï¼šåœ¨å¤šä¸ªé«˜ç»´ BNN ä»»åŠ¡ä¸Šï¼Œç›¸æ¯”æ ‡å‡† HMC å®ç°äº† **7â€“9 å€åŠ é€Ÿ**ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡äº†ä¸ç¡®å®šæ€§é‡åŒ–è´¨é‡ï¼ˆæ›´ä½ ECEã€æ›´é«˜ CP95ï¼‰ã€‚
4. **ç°æœ‰æ–¹æ³•æ˜¯ NIPA çš„ç‰¹ä¾‹**ï¼šè¯¥æ¡†æ¶å…·æœ‰é«˜åº¦é€šç”¨æ€§ï¼Œæ¶µç›–äº†ä»çº¯ MCMC åˆ° surrogate-based æ–¹æ³•ï¼ˆå¦‚ CESï¼‰ç­‰å¤šç§èŒƒå¼ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼šé˜ˆå€¼ $t_1, t_2$ å½“å‰ä½œä¸ºæ‰‹åŠ¨è°ƒå‚é¡¹ï¼Œç¼ºä¹è‡ªé€‚åº”æœºåˆ¶ï¼Œå½±å“æ³›åŒ–èƒ½åŠ›ã€‚
- **surrogate æ¨¡å‹é€‰æ‹©**ï¼šå½“å‰ä½¿ç”¨ autoencoder + DNN æ„å»º surrogateï¼Œè™½æœ‰æ•ˆä½†éå”¯ä¸€é€‰æ‹©ï¼Œæ¨¡å‹å®¹é‡ä¸è®­ç»ƒç¨³å®šæ€§éœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚
- **EC æ£€ç´¢æœºåˆ¶ç®€å•**ï¼šä»…ä½¿ç”¨æœ€è¿‘é‚»æ£€ç´¢ï¼Œæœªè€ƒè™‘å±€éƒ¨æ›²ç‡æˆ–å¤šç‚¹æ’å€¼ï¼Œå¯èƒ½åœ¨å¤æ‚æµå½¢ä¸Šå¤±æ•ˆã€‚
- **åˆå§‹åŒ–ä¾èµ–**ï¼šåˆå§‹æ ·æœ¬æ± ç”± SGHMC ç”Ÿæˆï¼Œè‹¥åˆå§‹æ¢ç´¢ä¸ä½³ï¼Œå¯èƒ½å½±å“åç»­ surrogate å­¦ä¹ æ•ˆæœã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- **è‡ªé€‚åº”é—¨æ§æœºåˆ¶**ï¼šå€Ÿé‰´è®¤çŸ¥ç§‘å­¦ä¸­åŸºäºâ€œä¸ç¡®å®šæ€§ç«äº‰â€çš„ç†è®ºï¼Œè®©ç³»ç»Ÿæ ¹æ®å„æ¨¡å—çš„ç½®ä¿¡åº¦è‡ªåŠ¨åˆ†é…æ§åˆ¶æƒã€‚
- **æ›´çµæ´»çš„ surrogate æ¶æ„**ï¼šå°è¯•å…¶ä»–è½»é‡çº§æ¨¡å‹ï¼ˆå¦‚ ELMï¼‰æˆ–ç»“æ„åŒ–å…ˆéªŒï¼ˆå¦‚ GP ä¸ DNN ç»“åˆï¼‰ã€‚
- **é«˜çº§ EC æ£€ç´¢ç­–ç•¥**ï¼šå¼•å…¥ k-NN æ’å€¼ã€å±€éƒ¨æ‹Ÿåˆæˆ–æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºè®°å¿†æ£€ç´¢èƒ½åŠ›ã€‚
- **æ‰©å±•è‡³å…¶ä»–é¢†åŸŸ**ï¼šåº”ç”¨äº **Gaussian Processes**ã€**Bayesian Inverse Problems** ç­‰åŒæ ·å—é™äºè®¡ç®—æˆæœ¬çš„è´å¶æ–¯å»ºæ¨¡åœºæ™¯ã€‚
- **ç†è®ºåˆ†æ**ï¼šå»ºç«‹ NIPA çš„æ”¶æ•›æ€§ã€åå·®-æ–¹å·®æƒè¡¡ç­‰ç†è®ºä¿éšœã€‚

---

> **æ€»ç»“**ï¼šNIPA æ˜¯ä¸€ç§æ–°é¢–ä¸”å®ç”¨çš„è´å¶æ–¯æ¨ç†æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ¨¡ä»¿å¤§è„‘å¤šç³»ç»Ÿåä½œæœºåˆ¶ï¼ŒæˆåŠŸè§£å†³äº†é«˜ç»´æ¨¡å‹ä¸­é‡‡æ ·æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´çš„çŸ›ç›¾ï¼Œä¸ºå¤§è§„æ¨¡ **Bayesian Deep Learning** çš„å®é™…åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„æ–°å·¥å…·ã€‚

</details>

---

### 14. [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)

**Authors**: Mingyu Lu, Soham Gadgil, Chris Lin, Chanwoo Kim, Su-In Lee  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.22276v1  

#### Abstract
As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically ground...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šSurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
åœ¨ Text-to-Image (T2I) æ‰©æ•£æ¨¡å‹æ—¥ç›Šå¹¿æ³›åº”ç”¨äºåˆ›æ„å·¥ä½œæµçš„èƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•å…¬å¹³åœ°å¯¹**æ•°æ®è´¡çŒ®è€…**ï¼ˆå¦‚è‰ºæœ¯å®¶ã€å“ç‰Œã€åŒ»é™¢ç­‰ï¼‰è¿›è¡Œä»·å€¼è¯„ä¼°æˆä¸ºä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ Shapley value è™½ç„¶ç†è®ºä¸Šæ˜¯å…¬å¹³çš„å½’å› æ¡†æ¶ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´ä¸¤å¤§ç“¶é¢ˆï¼š
- **è®¡ç®—æˆæœ¬é«˜**ï¼šéœ€è¦å¯¹æ¯ä¸ªæ•°æ®å­é›†è¿›è¡Œé‡æ–°è®­ç»ƒï¼ˆretrainingï¼‰ï¼Œå¯¹äºå¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹è€Œè¨€ä¸å¯è¡Œã€‚
- **ä¼°è®¡æ–¹å·®å¤§**ï¼šç”±äºè´¡çŒ®è€…ä¹‹é—´å­˜åœ¨å¤æ‚çš„éçº¿æ€§äº¤äº’æ•ˆåº”ï¼ŒåŸºäºé‡‡æ ·çš„ Shapley ä¼°è®¡éœ€è¦æå¤šæ ·æœ¬æ‰èƒ½æ”¶æ•›ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº† **SurrogateSHAP**ï¼Œä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒçš„é«˜æ•ˆè´¡çŒ®è€…å½’å› æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³åŒ…å«ä¸¤ä¸ªäº’è¡¥éƒ¨åˆ†ï¼š

1. **Training-Free Proxy Gameï¼ˆæ— è®­ç»ƒä»£ç†æ¸¸æˆï¼‰**  
   åˆ›æ–°æ€§åœ°å°†åŸå§‹çš„â€œé‡è®­ç»ƒæ¸¸æˆâ€ï¼ˆretraining gameï¼‰è½¬åŒ–ä¸ºä¸€ä¸ª**æ— éœ€é‡æ–°è®­ç»ƒçš„ä»£ç†ç‰ˆæœ¬**ã€‚è¯¥æ–¹æ³•å‡è®¾é¢„è®­ç»ƒæ¨¡å‹çš„æ¡ä»¶ç”Ÿæˆèƒ½åŠ›å¯ä»¥ä½œä¸ºé‡è®­ç»ƒåæ¨¡å‹çš„ä»£ç†ï¼Œé€šè¿‡å°†å­é›† $S$ å¯¹åº”çš„ç”Ÿæˆåˆ†å¸ƒå»ºæ¨¡ä¸ºå¤šä¸ªæ¡ä»¶åˆ†å¸ƒçš„æ··åˆï¼ˆmixtureï¼‰ï¼Œä»è€Œä»…éœ€æ¨ç†å³å¯è¯„ä¼°ä»»æ„å­é›†çš„æ•ˆç”¨ $v_g(S)$ï¼Œé¿å…äº†æ˜‚è´µçš„ retrainingã€‚

2. **TreeSHAP-based Surrogate Modelï¼ˆåŸºäºæ ‘çš„ä»£ç†æ¨¡å‹ï¼‰**  
   ä¸ºäº†æå‡ Shapley å€¼ä¼°è®¡çš„æ ·æœ¬æ•ˆç‡ï¼Œå¼•å…¥æ¢¯åº¦æå‡æ ‘ï¼ˆGradient-Boosted Tree, GBTï¼‰æ¥å­¦ä¹ ä»£ç†æ•ˆç”¨å‡½æ•° $v_g(S)$ çš„è¿‘ä¼¼ã€‚éšååˆ©ç”¨ **TreeSHAP** ç®—æ³•ä»è¯¥æ ‘æ¨¡å‹ä¸­**è§£æåœ°è®¡ç®—å‡ºç²¾ç¡®çš„ Shapley å€¼**ï¼Œæ˜¾è‘—å‡å°‘äº†è¾¾åˆ°ç¨³å®šä¼°è®¡æ‰€éœ€çš„å­é›†æŸ¥è¯¢æ•°é‡ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **å…é‡è®­ç»ƒ**ï¼šå®Œå…¨é¿å…äº† retraining æˆ– fine-tuningï¼Œå¤§å¹…é™ä½è®¡ç®—å¼€é”€ã€‚
- **é«˜æ ·æœ¬æ•ˆç‡**ï¼šç›¸æ¯” KernelSHAP ç­‰é‡‡æ ·æ–¹æ³•ï¼Œåœ¨ç›¸åŒé¢„ç®—ä¸‹è¯¯å·®æ›´ä½ï¼Œæ”¶æ•›æ›´å¿«ã€‚
- **é«˜ä¿çœŸåº¦**ï¼šç†è®ºè¯æ˜å¹¶å®éªŒè¯å®ï¼Œä»£ç†æ•ˆç”¨ $v_g(S)$ ä¸çœŸå® retraining æ•ˆç”¨ $v(S)$ ä¹‹é—´çš„è¯¯å·®å—æ§äºè¡¨ç¤ºç©ºé—´æ¼‚ç§»ï¼ˆrepresentation driftï¼‰ã€‚
- **å¯æ‰©å±•æ€§å¼º**ï¼šé€‚ç”¨äºå¤§è§„æ¨¡ T2I æ¨¡å‹ï¼ˆå¦‚ Stable Diffusion, FLUX.1ï¼‰ï¼Œæ”¯æŒå¤šç§ä¸‹æ¸¸æ•ˆç”¨æŒ‡æ ‡ï¼ˆå¦‚ç¾å­¦è¯„åˆ†ã€å¤šæ ·æ€§ã€FID ç­‰ï¼‰ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
å®éªŒåœ¨ä¸‰ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œæ¶µç›–å›¾åƒè´¨é‡ã€ç¾å­¦å’Œäº§å“å¤šæ ·æ€§ä»»åŠ¡ï¼š

| æ•°æ®é›† | æè¿° | è´¡çŒ®è€…å•ä½ |
|--------|------|-----------|
| **CIFAR-20** | CIFAR-100 çš„ 20 ç±»å­é›†ï¼Œ32Ã—32 å›¾åƒ | æ¯ä¸ªç±»åˆ«ä½œä¸ºä¸€ä¸ªè´¡çŒ®è€… |
| **ArtBench (Post-Impressionism)** | åå°è±¡æ´¾è‰ºæœ¯ä½œå“ï¼Œ258 ä½è‰ºæœ¯å®¶ï¼Œ256Ã—256 å›¾åƒ | æ¯ä½è‰ºæœ¯å®¶ä½œä¸ºä¸€ä¸ªè´¡çŒ®è€… |
| **Fashion-Product** | æ—¶å°šå•†å“å›¾åƒï¼Œ100 ä¸ªå“ç‰Œï¼Œ256Ã—256 å›¾åƒ | æ¯ä¸ªå“ç‰Œä½œä¸ºä¸€ä¸ªè´¡çŒ®è€… |

æ­¤å¤–ï¼Œåœ¨ä¸´åºŠæ¡ˆä¾‹ç ”ç©¶ä¸­ä½¿ç”¨äº† **ISIC** çš®è‚¤ç—…å­¦æ•°æ®é›†ï¼Œä»¥åŒ»é™¢ç«™ç‚¹ä½œä¸ºè´¡çŒ®è€…ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### ä¸»è¦è¯„ä¼°æ–¹å¼ï¼š
- **Proxy Game Fidelity**ï¼šæ¯”è¾ƒä»£ç†æ•ˆç”¨ $v_g(S)$ ä¸ retraining æ•ˆç”¨ $v(S)$ çš„ä¸€è‡´æ€§ï¼Œä½¿ç”¨ NMAEã€NRMSE å’Œåƒç´ /è¡¨ç¤ºçº§ç›¸ä¼¼æ€§ï¼ˆMAE, SSIM, RMS-LPIPS, RMS-CLIPï¼‰ã€‚
- **Linear Datamodel Score (LDS)**ï¼šè¡¡é‡å½’å› æ–¹æ³•é¢„æµ‹æœªè§å­é›†æ•ˆç”¨çš„èƒ½åŠ›ï¼Œä½¿ç”¨ Spearman ç›¸å…³ç³»æ•°ï¼Œè¶Šé«˜è¶Šå¥½ã€‚
- **Counterfactual Evaluation**ï¼šé€šè¿‡ç§»é™¤ Top-K% æœ€é‡è¦è´¡çŒ®è€…ï¼Œè§‚å¯Ÿæ¨¡å‹æ€§èƒ½ä¸‹é™ç¨‹åº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰ï¼ŒéªŒè¯å½’å› å‡†ç¡®æ€§ã€‚
- **Case Study**ï¼šåœ¨ ISIC æ•°æ®ä¸Šå®¡è®¡ç”±åŒ»é™¢å¼•å…¥çš„è™šå‡ç›¸å…³æ€§ï¼ˆæ€§åˆ«ä¸é»‘è‰²ç´ ç˜¤è¯Šæ–­é—´çš„ spurious correlationï¼‰ã€‚

#### è®­ç»ƒç»†èŠ‚ï¼š
- åœ¨ CIFAR-20 ä¸Šä½¿ç”¨ DDPM-CFG è¿›è¡Œå…¨å‚æ•°å¾®è°ƒã€‚
- åœ¨ ArtBench å’Œ Fashion-Product ä¸Šä½¿ç”¨ LoRA å¾®è°ƒ Stable Diffusion å’Œ FLUX.1ã€‚
- SurrogateSHAP ä½¿ç”¨ XGBoost æ„å»ºä»£ç†æ¨¡å‹ï¼Œå¹¶é€šè¿‡ 5 æŠ˜äº¤å‰éªŒè¯ä¼˜åŒ–è¶…å‚æ•°ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ¶µç›–äº†å››ç±»ä¸»æµæ–¹æ³•ï¼š
1. **Similarity-based**: Pixel/CLIP/GMValuatorï¼ˆåŸºäºç”Ÿæˆæ ·æœ¬ä¸è®­ç»ƒæ ·æœ¬çš„ç›¸ä¼¼æ€§ï¼‰
2. **LOO & Influence Functions**: LOO, Relative IF, Renormalized IF
3. **TRAK-based**: TRAK, Journey-TRAK, D-TRAK, DAS
4. **Shapley-based Retraining Proxies**: Sparsified Fine-Tuning (sFT) + KernelSHAPï¼ˆæ¥è‡ª Lu et al., 2025ï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

#### âœ… LDS æ€§èƒ½ï¼ˆ$\alpha=0.5$ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
| Method | CIFAR-20 (FID) | ArtBench (Aesthetic) | Fashion-Product (Diversity) |
|--------|----------------|-----------------------|------------------------------|
| **SurrogateSHAP (Ours)** | **51.21Â±6.78** | **44.14Â±5.27** | **32.41Â±5.31** |
| Sparsified-FT Shapley | -14.1Â±3.51 | 18.85Â±1.89 | 20.34Â±12.43 |
| LOO | 34.82Â±0.92 | -0.18Â±3.19 | -3.88Â±3.00 |
| DAS | 13.59Â±2.71 | -3.53Â±4.09 | 6.35Â±6.10 |

> SurrogateSHAP åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡æ˜¾è‘—é¢†å…ˆï¼Œå°¤å…¶åœ¨å¤æ‚æ•ˆç”¨ï¼ˆå¦‚ç¾å­¦ã€å¤šæ ·æ€§ï¼‰ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚

#### âœ… è®¡ç®—æ•ˆç‡ï¼ˆæ¯å­é›†æŸ¥è¯¢æ—¶é—´ï¼Œè¶Šä½è¶Šå¥½ï¼‰
| Method | CIFAR-20 (min) | ArtBench (min) | Fashion-Product (min) |
|--------|----------------|----------------|------------------------|
| Retraining | 234.36 | 128.83 | 105.24 |
| sFT | 27.42 | 14.15 | 54.33 |
| **SurrogateSHAP (Ours)** | **10.72** | **10.92** | **35.23** |

> SurrogateSHAP æ¯” retraining å¿« **12â€“23å€**ï¼Œæ¯” sFT å¿« **1.4â€“2.5å€**ã€‚

#### âœ… Counterfactual ç§»é™¤æ•ˆæœï¼ˆTop 10% ç§»é™¤åæ€§èƒ½ä¸‹é™ï¼‰
| Method | CIFAR-20 (FID â†‘) | ArtBench (Aesthetic â†“) | Fashion-Product (Diversity â†“) |
|--------|------------------|-------------------------|-------------------------------|
| **SurrogateSHAP (Ours)** | **31.70Â±2.47%** | **-5.76Â±0.49%** | **-17.49Â±8.34%** |
| LOO | 17.48Â±2.68% | -4.99Â±1.13% | -7.18Â±6.23% |
| CLIP Score | 17.53Â±1.58% | -4.30Â±1.10% | -6.09Â±9.24% |

> SurrogateSHAP è¯†åˆ«å‡ºçš„è´¡çŒ®è€…å¯¹æ¨¡å‹å½±å“æœ€å¤§ï¼Œç§»é™¤åå¯¼è‡´æœ€ä¸¥é‡çš„æ€§èƒ½é€€åŒ–ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **åˆæˆæ¸¸æˆæµ‹è¯•**ï¼ˆSynthetic Gamesï¼‰æ˜¾ç¤ºï¼Œåœ¨å…·æœ‰é«˜é˜¶äº¤äº’çš„éçº¿æ€§æ•ˆç”¨å‡½æ•°ä¸­ï¼ŒSurrogateSHAP çš„ $l_2$ ä¼°è®¡è¯¯å·®ä»…ä¸º KernelSHAP çš„ **1/8**ï¼ˆ0.010 vs 0.085 @ M=512ï¼‰ã€‚
- **Axiomatic Checks** éªŒè¯äº† SurrogateSHAP æ»¡è¶³ Shapley å€¼çš„åŸºæœ¬å…¬ç†ï¼ˆå¦‚æ•ˆç‡æ€§ã€é›¶ç©å®¶æ€§ï¼‰ï¼Œä¸”éšæ ·æœ¬å¢åŠ è¶‹äºæ”¶æ•›ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **ä»£ç†æ¸¸æˆé«˜åº¦ä¿çœŸ**ï¼šæå‡ºçš„ training-free proxy game èƒ½å¤Ÿä»¥æå°è¯¯å·®é€¼è¿‘ retraining æ¸¸æˆçš„æ•ˆç”¨ï¼Œå°¤å…¶æ˜¯åœ¨è¡¨ç¤ºç©ºé—´æ¼‚ç§»è¾ƒå°æ—¶ã€‚
2. **SurrogateSHAP æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•**ï¼šæ— è®ºæ˜¯åœ¨ LDS é¢„æµ‹èƒ½åŠ›ã€counterfactual å½±å“åŠ›è¿˜æ˜¯è®¡ç®—æ•ˆç‡æ–¹é¢ï¼ŒSurrogateSHAP å‡å…¨é¢è¶…è¶ŠåŒ…æ‹¬ sFT-Shapley åœ¨å†…çš„æ‰€æœ‰åŸºçº¿ã€‚
3. **é€‚ç”¨äºå¤šåœºæ™¯å½’å› **ï¼šä¸ä»…å¯ç”¨äºè¯„ä¼°ç”Ÿæˆè´¨é‡ï¼Œè¿˜èƒ½æœ‰æ•ˆå®šä½å¯¼è‡´è™šå‡ç›¸å…³æ€§çš„æ•°æ®æºï¼ˆå¦‚ ISIC æ¡ˆä¾‹ä¸­ Hospital 7 å¼•å…¥çš„æ¯›å‘ä¼ªå½±ä¸æ€§åˆ«å…³è”ï¼‰ã€‚
4. **TreeSHAP æå‡ä¼°è®¡æ•ˆç‡**ï¼šä½¿ç”¨ GBT + TreeSHAP å¯è§£æè®¡ç®— Shapley å€¼ï¼Œæå¤§æå‡äº†ä¼°è®¡ç¨³å®šæ€§ä¸é€Ÿåº¦ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä¾èµ–æ¡ä»¶æ ‡ç­¾ç»“æ„**ï¼šè¦æ±‚æ•°æ®æœ‰æ˜ç¡®çš„ prompt metadataï¼ˆå¦‚è‰ºæœ¯å®¶åã€å“ç‰Œåï¼‰ä»¥å®šä¹‰è´¡çŒ®è€…é›†åˆã€‚
- **ä»£ç†è¯¯å·®ä»å­˜åœ¨**ï¼šè™½ç„¶è¯¯å·®å¯æ§ï¼Œä½†åœ¨æç«¯æ•°æ®åˆ†å¸ƒåç§»ä¸‹ï¼Œä»£ç†æ•ˆç”¨å¯èƒ½åç¦»çœŸå® retraining ç»“æœã€‚
- **æ ‘æ¨¡å‹å®¹é‡é™åˆ¶**ï¼šå°½ç®¡ TreeSHAP æ˜¯ç²¾ç¡®çš„ï¼Œä½† GBT å¯¹æé«˜é˜¶äº¤äº’çš„æ•æ‰èƒ½åŠ›æœ‰é™ï¼ˆä¸è¿‡å®éªŒè¡¨æ˜å·²è¶³å¤Ÿï¼‰ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å°† SurrogateSHAP æ‰©å±•åˆ°æ— æ˜¾å¼æ ‡ç­¾çš„èšç±»å¼è´¡çŒ®è€…åˆ’åˆ†ã€‚
- æ¢ç´¢æ›´å¼ºå¤§çš„ä»£ç†æ¨¡å‹ï¼ˆå¦‚ MLP æˆ– Transformerï¼‰ä»¥è¿›ä¸€æ­¥æå‡å¤æ‚æ•ˆç”¨çš„æ‹Ÿåˆèƒ½åŠ›ã€‚
- ç»“åˆå¤šç›®æ ‡å½’å› æ¡†æ¶ï¼Œå¹³è¡¡ç”Ÿæˆè´¨é‡ä¸æ•°æ®å¤šæ ·æ€§/å…¬å¹³æ€§ï¼Œé˜²æ­¢è¿‡åº¦ä¿®å‰ªå°‘æ•°ç¾¤ä½“æ•°æ®ã€‚

---

> **æ€»ç»“**ï¼šSurrogateSHAP ä¸º T2I æ¨¡å‹ä¸­çš„æ•°æ®è´¡çŒ®è€…æä¾›äº†ä¸€ä¸ª**é«˜æ•ˆã€å‡†ç¡®ã€å…é‡è®­ç»ƒ**çš„å½’å› æ¡†æ¶ï¼Œæ¨åŠ¨äº†ç”Ÿæˆæ¨¡å‹çš„é€æ˜åŒ–ã€å…¬å¹³åŒ–ä¸å®‰å…¨å®¡è®¡ï¼Œå…·æœ‰é‡è¦çš„å®è·µæ„ä¹‰ã€‚

</details>

---

### 15. [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)

**Authors**: Xin Jennifer Chen, Yunjin Tong  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.22315v1  

#### Abstract
Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡é’ˆå¯¹**æ˜‚è´µåé¦ˆä¸‹çš„è¿ç»­ç©ºé—´ä¼˜åŒ–é—®é¢˜**ï¼Œå°¤å…¶æ˜¯åœ¨ç§‘å­¦å‡è®¾ç”Ÿæˆã€å·¥ç¨‹è®¾è®¡ç­‰åœºæ™¯ä¸­ï¼Œå­˜åœ¨ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- **Ground-truth oracleï¼ˆçœŸå®åé¦ˆï¼‰** éå¸¸æ˜‚è´µï¼ˆå¦‚äººç±»è¯„ä¼°ã€ç‰©ç†å®éªŒï¼‰ï¼Œéš¾ä»¥é¢‘ç¹è°ƒç”¨ï¼›
- å­˜åœ¨ä¸€ä¸ª**å»‰ä»·ä½†æœ‰åçš„é¢„æµ‹æ¨¡å‹**ï¼ˆå¦‚ LLMã€ä»¿çœŸå™¨ï¼‰å¯æä¾›å¿«é€Ÿä¼°è®¡ï¼›
- åŒæ—¶æ‹¥æœ‰**ä¸°å¯Œçš„ç¦»çº¿æ•°æ®**å¯ç”¨äºé¢„è®­ç»ƒæ¨¡å‹æˆ–æ„å»ºå…ˆéªŒã€‚

ä¼ ç»Ÿæ–¹æ³•å¦‚ Vanilla GP-UCB ä»…ä¾èµ–æ˜‚è´µçš„çœŸå®åé¦ˆï¼Œæ ·æœ¬æ•ˆç‡ä½ï¼›è€Œå¤šä¿çœŸåº¦ï¼ˆmulti-fidelityï¼‰æ–¹æ³•é€šå¸¸éœ€è¦å¯¹ä¸åŒä¿çœŸåº¦è¿›è¡Œæˆæœ¬æƒè¡¡ï¼Œå¹¶ä¸é€‚ç”¨äºâ€œé¢„æµ‹è¿‘ä¹å…è´¹â€çš„æƒ…å†µã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šPA-GP-UCB
ä½œè€…æå‡º **Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB)**ï¼Œä¸€ç§ç»“åˆé«˜æˆæœ¬çœŸå®åé¦ˆã€ä½æˆæœ¬æœºå™¨å­¦ä¹ é¢„æµ‹ä¸ç¦»çº¿æ•°æ®çš„è´å¶æ–¯ä¼˜åŒ–ç®—æ³•ã€‚

#### æ ¸å¿ƒæ€æƒ³ä¸åˆ›æ–°ç‚¹ï¼š
- **è”åˆå»ºæ¨¡**ï¼šå°†çœŸå®å‡½æ•° $f$ å’Œ ML é¢„æµ‹å‡½æ•° $f_{\text{ML}}$ è§†ä¸ºå…±äº«åæ–¹å·®ç»“æ„çš„åŒä»»åŠ¡ Gaussian Processï¼ˆmulti-task GPï¼‰ã€‚
- **ä¸¤é˜¶æ®µè®¾è®¡**ï¼š
  - **Offline é˜¶æ®µ**ï¼šåˆ©ç”¨é¢„æµ‹æ¨¡å‹åœ¨ $\epsilon$-net ä¸Šæ‰¹é‡æŸ¥è¯¢ï¼Œè·å¾—å¤§é‡ä½è´¨é‡é¢„æµ‹æ•°æ®ä»¥æ„å»ºå¼ºå…ˆéªŒï¼›
  - **Online é˜¶æ®µ**ï¼šæ¯æ¬¡æŸ¥è¯¢çœŸå®å€¼çš„åŒæ—¶ä¹Ÿè·å–å¯¹åº”ç‚¹çš„é¢„æµ‹å€¼ï¼Œç”¨äºåŠ¨æ€æ ¡æ­£åå·®ã€‚
- **Control-variates ä¼°è®¡å™¨**ï¼šå¼•å…¥å½¢å¦‚  
  $$
  \hat{\mu}(x) = \mu^{\text{true}}(x) - \frac{\rho(x)\sigma^{\text{true}}(x)}{\sigma^{\text{ML}}(x)} (\mu^{\text{ML}}(x) - \mu^{\text{ML,all}}(x))
  $$
  çš„åå·®æ ¡æ­£æœºåˆ¶ï¼Œå®ç°**æ–¹å·®ç¼©å‡ä¸ç³»ç»Ÿæ€§åå·®çº æ­£**ã€‚
- **ç†è®ºä¿è¯**ï¼šè¯æ˜å…¶ç´¯ç§¯åæ‚”ï¼ˆcumulative regretï¼‰æ»¡è¶³ï¼š
  $$
  R_T = O\left(\sqrt{C \beta_T \gamma_T [1 - (1-R)\rho^2]}\right)
  $$
  å…¶ä¸­ $R$ æ˜¯ç¦»çº¿æ•°æ®å¸¦æ¥çš„ä¸ç¡®å®šæ€§å‹ç¼©å› å­ï¼Œ$\rho$ æ˜¯é¢„æµ‹ç›¸å…³æ€§ã€‚ç›¸æ¯” Vanilla GP-UCB çš„ $O(\sqrt{\beta_T \gamma_T})$ï¼ŒPA-GP-UCB åœ¨å¸¸æ•°é¡¹ä¸Šæœ‰ä¸¥æ ¼æ”¹è¿›ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | ç¼ºé™· | PA-GP-UCB æ”¹è¿› |
|------|------|----------------|
| **Vanilla GP-UCB** | å¿½è§†é¢„æµ‹ä¿¡å·ï¼Œæ”¶æ•›æ…¢ | åˆ©ç”¨é¢„æµ‹åŠ é€Ÿæ¢ç´¢ï¼Œæ˜¾è‘—é™ä½é‡‡æ ·æ¬¡æ•° |
| **Naive prediction-augmented GP-UCB** | ä¸çº æ­£åå·®ï¼Œæ˜“é™·å…¥é”™è¯¯åŒºåŸŸ | æ˜¾å¼åå·®æ ¡æ­£ï¼Œé¿å…è¢«è¯¯å¯¼ |
| **Multi-fidelity BO** | å¼ºè°ƒæˆæœ¬åˆ†é…ï¼Œä¸é€‚åˆâ€œé¢„æµ‹å…è´¹â€åœºæ™¯ | å°†é¢„æµ‹è§†ä¸ºé›¶æˆæœ¬è¾…åŠ©ä¿¡æ¯ï¼Œæ›´é«˜æ•ˆ |
| **PPI / MLA-UCB** | é™äºé™æ€æ¨æ–­æˆ–å¤šè‡‚è€è™æœºï¼ˆdiscreteï¼‰ | æ‰©å±•è‡³è¿ç»­åŸŸï¼Œæ”¯æŒç»“æ„åŒ–æ¢ç´¢ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
1. **åˆæˆæ•°æ®ï¼ˆSynthetic Benchmarksï¼‰**
   - ä» RBF æ ¸ GP ä¸­é‡‡æ ·çœŸå®å‡½æ•° $f$ï¼›
   - æ„é€ ç›¸å…³ä½†å¸¦åçš„é¢„æµ‹å‡½æ•°ï¼š$f_{\text{ML}} = \rho f + \sqrt{1-\rho^2}g$ï¼Œå…¶ä¸­ $g$ ä¸ºç‹¬ç«‹å™ªå£° GPï¼›
   - å±€éƒ¨åç›¸å…³è®¾å®šï¼šåœ¨åŒºé—´ $[0.4, 0.6]$ å†…ç¿»è½¬ $f_{\text{ML}}$ ç¬¦å·ï¼Œæµ‹è¯•é²æ£’æ€§ã€‚

2. **çœŸå®ä¸–ç•Œæ•°æ®ï¼ˆHuman Behavioral Datasetï¼‰**
   - æ¥æºäº Milkman et al. (2021) çš„å¤§è§„æ¨¡è¡Œä¸ºå¹²é¢„å®éªŒæ•°æ®ï¼ˆå…± 54 ç§å¹²é¢„æªæ–½ï¼‰ï¼›
   - **Ground-truth $f(x)$**ï¼šæ¯ç§å¹²é¢„ä¸‹ç”¨æˆ·çš„å¹³å‡è®¿é—®é‡ï¼ˆé€šè¿‡å®åœ°å®éªŒæµ‹é‡ï¼‰ï¼›
   - **Predictor $f_{\text{ML}}(x)$**ï¼š
     - **æœ‰é™åŠ¨ä½œç©ºé—´**ï¼šè½»é‡å›å½’æ¨¡å‹åŸºäºæ–‡æœ¬åµŒå…¥é¢„æµ‹ï¼›
     - **è¿ç»­ç©ºé—´**ï¼šä½¿ç”¨ OpenAI o4-mini LLM è¿›è¡Œé¢„æµ‹ï¼Œé‡‡ç”¨ä¸‰ç§æç¤ºæ–¹å¼ï¼š
       - K-shotï¼ˆK=4 æˆ– 12ï¼‰ï¼šæä¾›è‹¥å¹²çœŸå®ç¤ºä¾‹ï¼›
       - Scale-onlyï¼šåªç»™å…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼ã€æœ€å¤§æœ€å°å€¼ï¼‰ï¼›
   - **Embedding æ„é€ **ï¼šä½¿ç”¨ TF-IDF + UMAP å°†æ–‡æœ¬æè¿°æ˜ å°„åˆ°äºŒç»´è¯­ä¹‰æµå½¢ä¸Šï¼Œæ„é€ è¿ç»­å‡è®¾ç©ºé—´ã€‚

### å®éªŒè®¾ç½®
- **Horizon**ï¼š$T = 200$ è½®ï¼›
- **é‡å¤æ¬¡æ•°**ï¼š50 æ¬¡ç‹¬ç«‹è¿è¡Œå–å¹³å‡ï¼›
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **Cumulative Regret**: $R_T = \sum_{t=1}^T [f(x^*) - f(x_t)]$ï¼›
  - **Optimum Localization Accuracy**ï¼šæœ€ç»ˆæ¨èç‚¹æ˜¯å¦æ¥è¿‘çœŸå®æœ€ä¼˜ï¼›
  - **Posterior Variance Reduction**ï¼šæ¯”è¾ƒä¸ç¡®å®šæ€§ä¸‹é™é€Ÿåº¦ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Vanilla GP-UCB**ï¼šæ ‡å‡†è´å¶æ–¯ä¼˜åŒ–ï¼›
- **GP-UCB with offline prediction only**ï¼šä»…ç”¨ç¦»çº¿é¢„æµ‹åˆå§‹åŒ–å…ˆéªŒï¼›
- **GP-UCB with offline + online prediction**ï¼šåŠ å…¥æ‰€æœ‰é¢„æµ‹è§‚æµ‹ä½†æ— åå·®æ ¡æ­£ï¼›
- **PA-GP-UCB variants**ï¼šæ¶ˆèä¸åŒå‚æ•°ï¼ˆ$M$: grid size, $N$: é‡å¤æ¬¡æ•°, $\rho$: ç›¸å…³æ€§ï¼‰çš„å½±å“ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ
#### ï¼ˆ1ï¼‰åˆæˆç¯å¢ƒç»“æœï¼ˆå›¾2ï¼‰
- **PA-GP-UCB æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿**ï¼š
  - åœ¨ $T=200$ æ—¶ï¼Œ**ç´¯è®¡åæ‚”é™ä½çº¦ 30â€“60%**ï¼›
  - å³ä½¿é¢„æµ‹ç›¸å…³æ€§è¾ƒä½ï¼ˆ$\rho=0.5$ï¼‰ï¼Œä»èƒ½å–å¾—æ˜æ˜¾å¢ç›Šï¼›
  - å½“ $\rho=0.9$ æ—¶ï¼Œæ”¶æ•›é€Ÿåº¦æå‡è¿‘ä¸€å€ã€‚
- **å±€éƒ¨åç›¸å…³æµ‹è¯•**ï¼ˆå›¾1ï¼‰ï¼š
  - Naive æ–¹æ³•å› æœªæ ¡æ­£åå·®ï¼ŒæŒç»­åå‘é¢„æµ‹åå¥½åŒºï¼Œæ— æ³•æ‰¾åˆ°çœŸå®æœ€ä¼˜ï¼›
  - PA-GP-UCB æˆåŠŸè¯†åˆ«å¹¶ä¿®æ­£åå·®ï¼Œåœ¨åæœŸé›†ä¸­æ¢ç´¢çœŸå®å³°å€¼é™„è¿‘ã€‚

#### ï¼ˆ2ï¼‰çœŸå®æ•°æ®å®éªŒç»“æœï¼ˆå›¾3ï¼‰
##### æœ‰é™åŠ¨ä½œç©ºé—´ï¼ˆFinite-Arm Settingï¼‰
- é¢„æµ‹æ¨¡å‹ä¸çœŸå®åé¦ˆçš„ç›¸å…³æ€§ $\rho \approx 0.66$ï¼›
- PA-GP-UCB åœ¨æ‰€æœ‰è®¾ç½®ä¸‹å‡ä¼˜äº Vanilla GP-UCBï¼›
- éšç€ç¦»çº¿é‡å¤æŸ¥è¯¢æ•° $N$ å¢åŠ ï¼ˆ1 â†’ 100ï¼‰ï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡ï¼Œè¡¨æ˜**æ›´å¤šç¦»çº¿é¢„æµ‹æœ‰åŠ©äºç¨³å®šæ¢ç´¢**ã€‚

##### è¿ç»­ç©ºé—´å‡è®¾ç”Ÿæˆï¼ˆContinuous Hypothesis Evaluationï¼‰
- ä½¿ç”¨ LLM ä½œä¸ºé¢„æµ‹å™¨ï¼š
  - **Scale-only æç¤ºè¡¨ç°æœ€ä½³**ï¼šè™½æ— å…·ä½“æ ‡ç­¾ï¼Œä½†æä¾›äº†å¯é çš„å…¨å±€å°ºåº¦ä¿¡æ¯ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆå°æ ·æœ¬ï¼›
  - **K=12 > K=4**ï¼šæ›´å¤šä¿¡æ¯å¸¦æ¥æ›´å¥½å¼•å¯¼ï¼›
- Vanilla GP-UCB å‡ºç°è¿‘ä¼¼çº¿æ€§åæ‚”ï¼Œè¯´æ˜å…¶åéªŒä¸¥é‡è¯¯è®¾ï¼›
- PA-GP-UCB å®ç°ç¨³å®šäºšçº¿æ€§æ”¶æ•›ï¼Œ**éªŒè¯äº†å…¶å¯¹ kernel misspecification çš„é²æ£’æ€§**ã€‚

### æ¶ˆèå®éªŒç»“æœ
| å˜é‡ | å‘ç° |
|------|------|
| **Offline æ•°æ®é‡ $M, N$**ï¼ˆå›¾2cï¼‰ | å³ä½¿ $M=N=1$ï¼ˆæå°‘é‡ç¦»çº¿æ•°æ®ï¼‰ï¼Œä¹Ÿèƒ½å¸¦æ¥æ€§èƒ½æå‡ï¼›å¢å¤§ $M,N$ ç»§ç»­æ”¹å–„æ•ˆæœ |
| **é¢„æµ‹ç›¸å…³æ€§ $\rho$**ï¼ˆå›¾2bï¼‰ | æ€§èƒ½éš $\rho$ æé«˜å•è°ƒä¸Šå‡ï¼Œä½†å³ä½¿ $\rho=0.5$ ä¹Ÿæœ‰æ˜¾è‘—æ”¶ç›Š |
| **æ§åˆ¶å˜é‡æœ‰æ•ˆæ€§** | è‹¥ç§»é™¤ control-variates ç»“æ„ï¼Œæ€§èƒ½é€€åŒ–è‡³ naive baseline æ°´å¹³ |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **PA-GP-UCB èƒ½æœ‰æ•ˆèåˆå»‰ä»·é¢„æµ‹ä¸æ˜‚è´µåé¦ˆ**ï¼Œåœ¨ä¿æŒç†è®ºä¿è¯çš„å‰æä¸‹æ˜¾è‘—æé«˜æ ·æœ¬æ•ˆç‡ï¼›
2. âœ… **Control-variates ä¼°è®¡å™¨æ˜¯å…³é”®**ï¼Œå®ƒä¸ä»…èƒ½å‡å°‘æ–¹å·®ï¼Œè¿˜èƒ½ä¸»åŠ¨çº æ­£ç³»ç»Ÿæ€§åå·®ï¼›
3. âœ… **å³ä½¿é¢„æµ‹è´¨é‡ä¸€èˆ¬ï¼ˆ$\rho \sim 0.5â€“0.8$ï¼‰æˆ–ä»…æœ‰å°‘é‡ç¦»çº¿æ•°æ®ï¼Œä¹Ÿèƒ½å¸¦æ¥å®è´¨æ€§æ”¹è¿›**ï¼›
4. âœ… åœ¨çœŸå®å‡è®¾ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒPA-GP-UCB æˆåŠŸå‘ç°äº†é«˜è´¨é‡ç­–ç•¥ï¼š
   > â€œç»“åˆé«˜é¢‘å°é¢é‡‘é’±æ¿€åŠ± + è§„åˆ’æé†’æœºåˆ¶â€çš„å¹²é¢„æœ€èƒ½ä¿ƒè¿›å¥èº«æˆ¿è®¿é—®è¡Œä¸ºï¼›
5. âœ… è¯¥æ¡†æ¶ç‰¹åˆ«é€‚åˆ**ç¨€ç–æ•°æ®ã€æ¨¡å‹è¯¯è®¾ã€ç»“æ„å¤æ‚**çš„ç§‘å­¦å‘ç°åœºæ™¯ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ç†è®ºæ¡ä»¶è¾ƒä¿å®ˆ**ï¼šå®šç†è¦æ±‚çš„ $(\epsilon, N)$ å‚æ•°è¿œé«˜äºå®é™…æ‰€éœ€ï¼ˆå®éªŒæ˜¾ç¤ºå³ä½¿ç²—ç²’åº¦ç½‘æ ¼ä¹Ÿæœ‰æ•ˆï¼‰ï¼›
- **ä¾èµ–é¢„æµ‹ç›¸å…³æ€§ $\rho > 0$**ï¼šè‹¥é¢„æµ‹å®Œå…¨æ— å…³ï¼ˆ$\rho \approx 0$ï¼‰ï¼Œåˆ™æ— å¢ç›Šï¼›
- **éœ€æ‰‹åŠ¨è®¾å®š $\rho$**ï¼šå½“å‰ä½¿ç”¨å…¨å±€ä¼°è®¡å€¼ï¼Œæœªæ¥å¯è€ƒè™‘è‡ªé€‚åº”å­¦ä¹ ï¼›
- **å¹¶è¡ŒåŒ–éœ€æ±‚é«˜**ï¼šOffline é˜¶æ®µéœ€å¤§è§„æ¨¡å¹¶è¡ŒæŸ¥è¯¢é¢„æµ‹æ¨¡å‹ï¼Œå¯èƒ½å—é™äº API æˆæœ¬æˆ–å»¶è¿Ÿã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- â— **å¼±åŒ–ç¦»çº¿æ•°æ®å‡è®¾**ï¼šå‘å±•è‡ªé€‚åº”æˆ–éå‡åŒ€ç¦»çº¿é‡‡æ ·ç­–ç•¥ï¼Œæ›¿ä»£å›ºå®š $\epsilon$-netï¼›
- â— **åŠ¨æ€ä¼°è®¡ $\rho(x)$**ï¼šå…è®¸ç›¸å…³æ€§éšè¾“å…¥å˜åŒ–ï¼Œæå‡å±€éƒ¨é€‚åº”èƒ½åŠ›ï¼›
- â— **Kernel robustness analysis**ï¼šç ”ç©¶å¦‚ä½•é€‰æ‹©æˆ–å­¦ä¹ æ›´é€‚åˆè¯­ä¹‰ç©ºé—´çš„ kernelï¼›
- â— **Cost-aware offline design**ï¼šä¼˜åŒ–ç¦»çº¿é¢„ç®—åˆ†é…ï¼Œåœ¨æœ‰é™èµ„æºä¸‹æœ€å¤§åŒ–æ”¶ç›Šï¼›
- â— **æ‰©å±•è‡³å¤§è§„æ¨¡ç§‘å­¦å‡è®¾æœç´¢**ï¼šåº”ç”¨äºè¯ç‰©è®¾è®¡ã€ææ–™å‘ç°ç­‰é¢†åŸŸï¼Œé›†æˆ LLM è‡ªåŠ¨ç”Ÿæˆæ–°å‡è®¾ã€‚

---

> **Impact Statement**ï¼š  
> æœ¬æ–‡æå‡ºçš„ PA-GP-UCB ä¸º **human-AI collaborative discovery** æä¾›äº†ä¸€ä¸ªé€šç”¨ä¸”é«˜æ•ˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘æ˜‚è´µäººç±»è¯„ä¼°çš„åŒæ—¶ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢åˆ›æ–°å‡è®¾ç©ºé—´ï¼Œæ¨åŠ¨ç§‘å­¦ç ”ç©¶å‘æ›´é«˜æ•ˆã€å¯æŒç»­çš„æ–¹å‘å‘å±•ã€‚

</details>

---

### 16. [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)

**Authors**: Tian-Tian Lin, Yi Liu, Xiao-Wei Tang, Yunmei Shi, Yi Huang, Zhongxiang Wei, Qingqing Wu, Yuhan Dong  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.22512v1  

#### Abstract
Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, w...

---

### 17. [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)

**Authors**: Zhiyuan Cao, Zeyu Ma, Chenhao Yang, Han Zheng, Mingang Chen  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.22752v1  

#### Abstract
We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Seman...

---

### 18. [From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents](https://arxiv.org/abs/2601.22607)

**Authors**: Jiaxuan Gao, Jiaao Chen, Chuyi He, Wei-Chen Wang, Shusheng Xu, Hanrui Wang, Di Jin, Yi Wu  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.22607v1  

#### Abstract
Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-qua...

---

### 19. [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)

**Authors**: Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuefeng Xiao, Hongyan Xie, Li Huaqiu, Songshi Liang, Zhongxiang Dai, Fuzhen Zhuang, Jianxin Li, Yikun Ban, Deqing Wang  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.22664v1  

#### Abstract
Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capt...

---

### 20. [$\rho$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs](https://arxiv.org/abs/2601.22527)

**Authors**: Jingyi Yang, Yuxian Jiang, Jing Shao  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.22527v1  

#### Abstract
Beyond parallel generation and global context modeling, current masked diffusion large language models (dLLMs) suffer from a fundamental limitation: they require a predefined, fixed generation length, which lacks flexibility and forces an inevitable trade-off between output quality and computational...

---

### 21. [Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs](https://arxiv.org/abs/2601.22795)

**Authors**: Corentin Kervadec, Iuliia Lysova, Marco Baroni, Gemma Boleda  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.22795v1  

#### Abstract
Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performanc...

---

### 22. [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)

**Authors**: Rosen Ting-Ying Yu, Nicholas Sung, Faez Ahmed  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.22371v1  

#### Abstract
Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We intro...

---

### 23. [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)

**Authors**: Gudrun Thorkelsdottir, Arindam Banerjee  
**Category**: cs.LG  
**Published**: 2026-02-02  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.22495v1  

#### Abstract
Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empi...

---

### 24. [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)

**Authors**: Jinwoo Jang, Minjong Yoo, Sihyung Yoon, Honguk Woo  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.22647v1  

#### Abstract
Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we ext...

---

### 25. [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)

**Authors**: Wei Zhu, Lixing Yu, Hao-Ren Yao, Zhiwen Tang, Kun Yue  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.22662v1  

#### Abstract
Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability...

---

### 26. [A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization](https://arxiv.org/abs/2601.22718)

**Authors**: Shiye Lei, Zhihao Cheng, Dacheng Tao  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.22718v1  

#### Abstract
Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current t...

---

### 27. [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)

**Authors**: Shichao Ma, Zhiyuan Ma, Ming Yang, Xiaofan Li, Xing Wu, Jintao Du, Yu Cheng, Weiqiang Wang, Qiliang Liu, Zhengyang Zhou, Yang Wang  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.22776v1  

#### Abstract
Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Ho...

---

### 28. [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)

**Authors**: Xuancheng Li, Haitao Li, Yujia Zhou,  YiqunLiu, Qingyao Ai  
**Category**: cs.AI  
**Published**: 2026-02-02  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.22900v1  

#### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In ...

---

### 29. [SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization](https://arxiv.org/abs/2601.22491)

**Authors**: Jinyang Wu, Changpeng Yang, Yuhao Shen, Fangzhi Xu, Bolin Ni, Chonghua Liao, Yuchen Liu, Hongzhen Wang, Shuai Nie, Shuai Zhang, Haoran Luo, Jiaming Xu  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.22491v1  

#### Abstract
Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversi...

---

### 30. [RASST: Fast Cross-modal Retrieval-Augmented Simultaneous Speech Translation](https://arxiv.org/abs/2601.22777)

**Authors**: Jiaxuan Luo, Siqi Ouyang, Lei Li  
**Category**: cs.CL  
**Published**: 2026-02-02  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.22777v1  

#### Abstract
Simultaneous speech translation (SST) produces target text incrementally from partial speech input. Recent speech large language models (Speech LLMs) have substantially improved SST quality, yet they still struggle to correctly translate rare and domain-specific terminology. While retrieval augmenta...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
