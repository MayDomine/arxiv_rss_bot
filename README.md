# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2026-01-15 05:52:39 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Contrastive Bi-Encoder Models for Multi-Label Skill Extraction: Enhancing ESCO Ontology Matching with BERT and Attention Mechanisms](https://arxiv.org/abs/2601.09119)

**Authors**: Yongming Sun  
**Category**: cs.CL  
**Published**: 2026-01-15  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2601.09119v1  

#### Abstract
Fine-grained labor market analysis increasingly relies on mapping unstructured job advertisements to standardized skill taxonomies such as ESCO. This mapping is naturally formulated as an Extreme Multi-Label Classification (XMLC) problem, but supervised solutions are constrained by the scarcity and ...

---

### 2. [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)

**Authors**: Abhijnan Nath, Alireza Bagheri Garakani, Tianchen Zhou, Fan Yang, Nikhil Krishnaswamy  
**Category**: cs.AI  
**Published**: 2026-01-15  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.08403v1  

#### Abstract
Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when mo...

---

### 3. [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)

**Authors**: Zenghua Liao, Jinzhi Liao, Xiang Zhao  
**Category**: cs.AI  
**Published**: 2026-01-15  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.08653v1  

#### Abstract
Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approach...

---

### 4. [Attention Consistency Regularization for Interpretable Early-Exit Neural Networks](https://arxiv.org/abs/2601.08891)

**Authors**: Yanhua Zhao  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.08891v1  

#### Abstract
Early-exit neural networks enable adaptive inference by allowing predictions at intermediate layers, reducing computational cost. However, early exits often lack interpretability and may focus on different features than deeper layers, limiting trust and explainability. This paper presents Explanatio...

---

### 5. [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)

**Authors**: Bo Wang, Junzhuo Li, Hong Chen, Yuanlin Chu, Yuxuan Fan, Xuming Hu  
**Category**: cs.AI  
**Published**: 2026-01-15  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.08383v1  

#### Abstract
Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures,...

---

### 6. [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)

**Authors**: Giulio Corallo, Paolo Papotti  
**Category**: cs.AI  
**Published**: 2026-01-15  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.08670v1  

#### Abstract
Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decodin...

---

### 7. [A.X K1 Technical Report](https://arxiv.org/abs/2601.09200)

**Authors**: Sung Jun Cheon, Jaekyung Cho, Seongho Choi, Hyunjun Eun, Seokhwan Jo, Jaehyun Jun, Minsoo Kang, Jin Kim, Jiwon Kim, Minsang Kim, Sungwan Kim, Seungsik Kim, Tae Yoon Kim, Youngrang Kim, Hyeongmun Lee, Sangyeol Lee, Sungeun Lee, Youngsoon Lee, Yujin Lee, Seongmin Ok, Chanyong Park, Hyewoong Park, Junyoung Park, Hyunho Yang, Subin Yi, Soohyun Bae, Dhammiko Arya, Yongseok Choi, Sangho Choi, Dongyeon Cho, Seungmo Cho, Gyoungeun Han, Yong-jin Han, Seokyoung Hong, Hyeon Hwang, Wonbeom Jang, Minjeong Ju, Wonjin Jung, Keummin Ka, Sungil Kang, Dongnam Kim, Joonghoon Kim, Jonghwi Kim, SaeRom Kim, Sangjin Kim, Seongwon Kim, Youngjin Kim, Seojin Lee, Sunwoo Lee, Taehoon Lee, Chanwoo Park, Sohee Park, Sooyeon Park, Yohan Ra, Sereimony Sek, Seungyeon Seo, Gun Song, Sanghoon Woo, Janghan Yoon, Sungbin Yoon  
**Category**: cs.CL  
**Published**: 2026-01-15  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.09200v1  

#### Abstract
We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, curated by a...

---

### 8. [Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats](https://arxiv.org/abs/2601.09555)

**Authors**: Manyi Zhang, Ji-Fu Li, Zhongao Sun, Haoli Bai, Hui-Ling Zhen, Zhenhua Dong, Xianzhi Yu  
**Category**: cs.CL  
**Published**: 2026-01-15  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.09555v1  

#### Abstract
Microscaling Floating-Point (MXFP) has emerged as a promising low-precision format for large language models (LLMs). Despite various post-training quantization (PTQ) algorithms being proposed, they mostly focus on integer quantization, while their applicability and behavior under MXFP formats remain...

---

### 9. [LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed LLM Inference](https://arxiv.org/abs/2601.09258)

**Authors**: Du Yin, Jiayi Ren, Xiayu Sun, Tianyao Zhou, Haizhu Zhou, Ruiyan Ma, Danyang Zhang  
**Category**: cs.DC  
**Published**: 2026-01-15  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.09258v1  

#### Abstract
LLM inference latency critically determines user experience and operational costs, directly impacting throughput under SLO constraints. Even brief latency spikes degrade service quality despite acceptable average performance. However, distributed inference environments featuring diverse software fra...

---

### 10. [Efficient Clustering in Stochastic Bandits](https://arxiv.org/abs/2601.09162)

**Authors**: G Dhinesh Chandran, Kota Srinivas Reddy, Srikrishna Bhashyam  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.09162v1  

#### Abstract
We study the Bandit Clustering (BC) problem under the fixed confidence setting, where the objective is to group a collection of data sequences (arms) into clusters through sequential sampling from adaptively selected arms at each time step while ensuring a fixed error probability at the stopping tim...

---

### 11. [BalDRO: A Distributionally Robust Optimization based Framework for Large Language Model Unlearning](https://arxiv.org/abs/2601.09172)

**Authors**: Pengyang Shao, Naixin Zhai, Lei Chen, Yonghui Yang, Fengbin Zhu, Xun Yang, Meng Wang  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.09172v1  

#### Abstract
As Large Language Models (LLMs) increasingly shape online content, removing targeted information from well-trained LLMs (also known as LLM unlearning) has become critical for web governance. A key challenge lies in sample-wise imbalance within the forget set: different samples exhibit widely varying...

---

### 12. [Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs](https://arxiv.org/abs/2601.09527)

**Authors**: Jonathan Knoop, Hendrik Holtmann  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.09527v1  

#### Abstract
SMEs increasingly seek alternatives to cloud LLM APIs, which raise data privacy concerns. Dedicated cloud GPU instances offer improved privacy but with limited guarantees and ongoing costs, while professional on-premise hardware (A100, H100) remains prohibitively expensive. We present a systematic e...

---

### 13. [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)

**Authors**: Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed Saifullah, Ali Jannesari  
**Category**: cs.AI  
**Published**: 2026-01-15  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.08166v1  

#### Abstract
Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for...

---

### 14. [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)

**Authors**: Abdelaziz Bounhar, Rania Hossam Elmohamady Elbadry, Hadi Abdine, Preslav Nakov, Michalis Vazirgiannis, Guokan Shang  
**Category**: cs.AI  
**Published**: 2026-01-15  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.08441v1  

#### Abstract
Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference dat...

---

### 15. [Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2601.08963)

**Authors**: Adrita Das, Peiran Jiang, Dantong Zhu, Barnabas Poczos, Jose Lugo-Martinez  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.08963v1  

#### Abstract
Diffusion models have emerged as a powerful class of generative models for molecular design, capable of capturing complex structural distributions and achieving high fidelity in 3D molecule generation. However, their widespread use remains constrained by long sampling trajectories, stochastic varian...

---

### 16. [SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache](https://arxiv.org/abs/2601.09083)

**Authors**: Chi-Chih Chang, Siqi Zhu, Zhichen Zeng, Haibin Lin, Jiaxuan You, Mohamed S. Abdelfattah, Ziheng Jiang, Xuehai Qian  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.09083v1  

#### Abstract
We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training...

---

### 17. [Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction](https://arxiv.org/abs/2601.09285)

**Authors**: Mianzhi Pan, JianFei Li, Peishuo Liu, Botian Wang, Yawen Ouyang, Yiming Rong, Hao Zhou, Jianbing Zhang  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.09285v1  

#### Abstract
Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their applic...

---

### 18. [From Prompt to Protocol: Fast Charging Batteries with Large Language Models](https://arxiv.org/abs/2601.09626)

**Authors**: Ge Lei, Ferran Brosa Planella, Sterling G. Baird, Samuel J. Cooper  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.09626v1  

#### Abstract
Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventi...

---

### 19. [Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.09093)

**Authors**: Zhixiang Liang, Beichen Huang, Zheng Wang, Minjia Zhang  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.09093v1  

#### Abstract
Large Language Models (LLMs) can enhance reasoning capabilities through test-time scaling by generating multiple traces. However, the combination of lengthy reasoning traces with multiple sampling introduces substantial computation and high end-to-end latency. Prior work on accelerating this process...

---

### 20. [RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning](https://arxiv.org/abs/2601.09253)

**Authors**: Zehua Liu, Shuqi Liu, Tao Zhong, Mingxuan Yuan  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.09253v1  

#### Abstract
While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effectiv...

---

### 21. [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)

**Authors**: Nawazish Alia, Rachael Shawb, Karl Mason  
**Category**: cs.AI  
**Published**: 2026-01-15  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.08052v1  

#### Abstract
Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean ...

---

### 22. [Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks](https://arxiv.org/abs/2601.08254)

**Authors**: Abdikarim Mohamed Ibrahim, Rosdiadee Nordin  
**Category**: cs.AI  
**Published**: 2026-01-15  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.08254v1  

#### Abstract
Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LL...

---

### 23. [PediaMind-R1: A Temperament-Aware Language Model for Personalized Early Childhood Care Reasoning via Cognitive Modeling and Preference Alignment](https://arxiv.org/abs/2601.08848)

**Authors**: Zihe Zhang, Can Zhang, Yanheng Xu, Xin Hu, Jichao Leng  
**Category**: cs.CL  
**Published**: 2026-01-15  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.08848v1  

#### Abstract
This paper presents PediaMind-R1, a domain-specialized large language model designed to achieve active personalization in intelligent parenting scenarios. Unlike conventional systems that provide generic suggestions, PediaMind-R1 draws on insights from developmental psychology. It introduces tempera...

---

### 24. [When to Invoke: Refining LLM Fairness with Toxicity Assessment](https://arxiv.org/abs/2601.09250)

**Authors**: Jing Ren, Bowen Li, Ziqi Xu, Renqiang Luo, Shuo Yu, Xin Ye, Haytham Fayek, Xiaodong Li, Feng Xia  
**Category**: cs.CL  
**Published**: 2026-01-15  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.09250v1  

#### Abstract
Large Language Models (LLMs) are increasingly used for toxicity assessment in online moderation systems, where fairness across demographic groups is essential for equitable treatment. However, LLMs often produce inconsistent toxicity judgements for subtle expressions, particularly those involving im...

---

### 25. [Optimizing View Change for Byzantine Fault Tolerance in Parallel Consensus](https://arxiv.org/abs/2601.09184)

**Authors**: Yifei Xie, Btissam Er-Rahmadi, Xiao Chen, Tiejun Ma, Jane Hillston  
**Category**: cs.DC  
**Published**: 2026-01-15  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.09184v1  

#### Abstract
The parallel Byzantine Fault Tolerant (BFT) protocol is viewed as a promising solution to address the consensus scalability issue of the permissioned blockchain. One of the main challenges in parallel BFT is the view change process that happens when the leader node fails, which can lead to performan...

---

### 26. [Single-Round Clustered Federated Learning via Data Collaboration Analysis for Non-IID Data](https://arxiv.org/abs/2601.09304)

**Authors**: Sota Sugawara, Yuji Kawamata, Akihiro Toyoda, Tomoru Nakayama, Yukihiko Okada  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.09304v1  

#### Abstract
Federated Learning (FL) enables distributed learning across multiple clients without sharing raw data. When statistical heterogeneity across clients is severe, Clustered Federated Learning (CFL) can improve performance by grouping similar clients and training cluster-wise models. However, most CFL a...

---

### 27. [GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR](https://arxiv.org/abs/2601.09361)

**Authors**: Jiaying Zhang, Lei Shi, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.09361v1  

#### Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric ...

---

### 28. [Deep Operator Networks for Surrogate Modeling of Cyclic Adsorption Processes with Varying Initial Conditions](https://arxiv.org/abs/2601.09491)

**Authors**: Beatrice Ceccanti, Mattia Galanti, Ivo Roghair, Martin van Sint Annaland  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.09491v1  

#### Abstract
Deep Operator Networks are emerging as fundamental tools among various neural network types to learn mappings between function spaces, and have recently gained attention due to their ability to approximate nonlinear operators. In particular, DeepONets offer a natural formulation for PDE solving, sin...

---

### 29. [Parallelizable memory recurrent units](https://arxiv.org/abs/2601.09495)

**Authors**: Florent De Geeter, Gaspard Lambrechts, Damien Ernst, Guillaume Drion  
**Category**: cs.LG  
**Published**: 2026-01-15  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.09495v1  

#### Abstract
With the emergence of massively parallel processing units, parallelization has become a desirable property for new sequence models. The ability to parallelize the processing of sequences with respect to the sequence length during training is one of the main factors behind the uprising of the Transfo...

---

### 30. [Consistency-Aware Editing for Entity-level Unlearning in Language Models](https://arxiv.org/abs/2601.08840)

**Authors**: Xiaoqi Han, V\'ictor Guti\'errez-Basulto, Ru Li, Xiaoli Li, Jiye Liang, Jeff Z. Pan  
**Category**: cs.CL  
**Published**: 2026-01-15  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.08840v1  

#### Abstract
Large language models (LLMs) risk retaining sensitive, copyrighted, or harmful information from their training data. Entity-level unlearning addresses this issue by removing all knowledge of a specific entity while preserving the model's overall capabilities. Existing approaches typically rely on fu...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
