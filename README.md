# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2026-01-28 06:00:40 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [DART: Diffusion-Inspired Speculative Decoding for Fast LLM Inference](https://arxiv.org/abs/2601.19278)

**Authors**: Fuliang Liu, Xue Li, Ketai Zhao, Yinxi Gao, Ziyan Zhou, Zhonghui Zhang, Zhibin Wang, Wanchun Dou, Sheng Zhong, Chen Tian  
**Category**: cs.CL  
**Published**: 2026-01-28  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2601.19278v1  

#### Abstract
Speculative decoding is an effective and lossless approach for accelerating LLM inference. However, existing widely adopted model-based draft designs, such as EAGLE3, improve accuracy at the cost of multi-step autoregressive inference, resulting in high drafting latency and ultimately rendering the ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# DART: Diffusion-Inspired Speculative Decoding for Fast LLM Inference è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³äº†ä»€ä¹ˆé—®é¢˜
ç°æœ‰çš„ **speculative decoding** æ–¹æ³•åœ¨åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ—¶é¢ä¸´ä¸€ä¸ªæ ¸å¿ƒç“¶é¢ˆï¼š**drafting latency**ï¼ˆèµ·è‰é˜¶æ®µå»¶è¿Ÿï¼‰ã€‚å°½ç®¡å¦‚ EAGLE3 ç­‰æ¨¡å‹é€šè¿‡è½»é‡çº§è®¾è®¡æå‡äº†é¢„æµ‹å‡†ç¡®æ€§ï¼ˆä»¥ **Average Acceptance Length (T)** è¡¡é‡ï¼‰ï¼Œä½†å…¶ä»é‡‡ç”¨è‡ªå›å½’ï¼ˆautoregressiveï¼‰ç”Ÿæˆæ–¹å¼ï¼Œå¯¼è‡´èµ·è‰è¿‡ç¨‹æœ¬èº«æˆä¸ºæ€§èƒ½ç“¶é¢ˆï¼Œå æ€»æ¨ç†æ—¶é—´çš„ 20%-40%ã€‚

å¦ä¸€æ–¹é¢ï¼ŒåŸºäº N-gram æˆ–æ£€ç´¢å¯å‘å¼çš„æ–¹æ³•ï¼ˆå¦‚ Lookaheadï¼‰è™½ç„¶å»¶è¿Ÿæä½ï¼Œä½†é¢„æµ‹è´¨é‡å·®ï¼ŒT å€¼å¾ˆä½ï¼Œæ— æ³•å¸¦æ¥æ˜¾è‘—ç«¯åˆ°ç«¯åŠ é€Ÿã€‚

æ­¤å¤–ï¼Œç›´æ¥å°† **diffusion-based LLMs (dLLMs)** ç”¨ä½œ draft model å­˜åœ¨æ ¹æœ¬æ€§ä¸åŒ¹é…ï¼š
- dLLMs æ˜¯åŒå‘å»ºæ¨¡ã€å…¨åºåˆ—å»å™ªï¼Œè€Œ speculative decoding è¦æ±‚ä¸¥æ ¼åŸºäºå‰ç¼€çš„å› æœé¢„æµ‹ï¼›
- dLLMs å‚æ•°é‡å¤§ï¼Œç‹¬ç«‹éƒ¨ç½²å¯¼è‡´æ¨ç†å¼€é”€é«˜ï¼›
- å­˜åœ¨ tokenizer ä¸å…¼å®¹ç­‰é—®é¢˜ã€‚

### æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯
æœ¬æ–‡æå‡º **DART**ï¼ˆDiffusion-Inspired Speculative Decodingï¼‰ï¼Œä¸€ç§å—æ‰©æ•£æ¨¡å‹å¯å‘ä½†ä¸“ä¸º speculative decoding å®šåˆ¶çš„æ–°æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- **å¹¶è¡Œèµ·è‰æœºåˆ¶**ï¼šå€Ÿé‰´ dLLMs å¹¶è¡Œé¢„æµ‹å¤šä¸ªæ©ç ä½ç½®çš„èƒ½åŠ›ï¼Œä½†åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­å¹¶è¡Œé¢„æµ‹å¤šä¸ªæœªæ¥ token çš„ logitsï¼Œå½»åº•æ¶ˆé™¤ draft model ä¸­çš„è‡ªå›å½’ rolloutã€‚
- **è½»é‡åŒ–è€¦åˆæ¶æ„**ï¼šdraft model å¤ç”¨ target model çš„éšè—çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œä»…æ·»åŠ ä¸€ä¸ªè½»é‡çº§çš„å®šåˆ¶åŒ– Transformer è§£ç å±‚è¿›è¡Œé¢„æµ‹ï¼Œé¿å…å¼•å…¥é‡å‹ç‹¬ç«‹æ¨¡å‹ã€‚
- **Shifted Logits Prediction**ï¼šåˆ›æ–°åœ°å°†æ¯ä¸ªä½ç½®è¾“å‡ºçš„ logit æ˜ å°„ä¸ºå¯¹â€œä¸‹ä¸€ä¸ª tokenâ€çš„é¢„æµ‹ï¼Œæå‡æ—©æœŸä½ç½®çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚
- **N-gram å¼•å¯¼çš„æ ‘å‰ªæç®—æ³•**ï¼šåˆ©ç”¨ N-gram æ¨¡å‹ä½œä¸ºè¿ç»­æ€§æ„ŸçŸ¥çš„å‰ªææœºåˆ¶ï¼Œåœ¨ç”±å¹¶è¡Œ logits æ„æˆçš„å·¨å¤§ç»„åˆç©ºé—´ä¸­é«˜æ•ˆæ„å»ºè¯­ä¹‰è¿è´¯çš„é«˜è´¨é‡ draft token treeã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **Latency** | èµ·è‰å‰å‘å»¶è¿Ÿç›¸æ¯” EAGLE3 æœ€å¤šé™ä½ **6.8Ã—**ï¼Œç›¸æ¯” SPS é™ä½ **53.3Ã—** |
| **Throughput** | ç«¯åˆ°ç«¯è§£ç é€Ÿåº¦æå‡ **2.03Ã—â€“3.44Ã—**ï¼Œå¹³å‡æ¯” EAGLE3 å¿« **30%** |
| **Accuracy** | ä¿æŒä¸ EAGLE3 ç›¸å½“ç”šè‡³æ›´é«˜çš„ **T å€¼**ï¼ˆå¹³å‡æ¥å—é•¿åº¦ï¼‰ |
| **Efficiency** | è®¾è®¡è½»é‡ã€æ— éœ€å¤æ‚ KV cache ç®¡ç†ï¼Œé€‚åˆå®é™…éƒ¨ç½² |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨äº†å“ªäº›æ•°æ®é›†
å®éªŒè¦†ç›–å¤šç§ä»»åŠ¡ç±»å‹ï¼Œè¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼š
- **æŒ‡ä»¤è·Ÿéšä¸å¯¹è¯**ï¼šMT-Bench, Alpaca
- **æ•°å­¦æ¨ç†**ï¼šMath500
- **ä»£ç ç”Ÿæˆ**ï¼šHumanEval, CodeAlpaca, LiveCodeBench, MBPP

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡

#### ç¡¬ä»¶ç¯å¢ƒ
- æœåŠ¡å™¨é…ç½®ï¼š8Ã—NVIDIA H20-3e GPUs (141GB), 90 CPU cores, 900GB RAM
- éƒ¨åˆ†è·¨å¹³å°æµ‹è¯•ä¹Ÿåœ¨å•å¼  A100-40G ä¸Šå®Œæˆ

#### ç›®æ ‡æ¨¡å‹ï¼ˆTarget Modelsï¼‰
- ä¸»è¦åŸºäº **Qwen3** ç³»åˆ—ï¼šQwen3-1.7B, 4B, 8B, 14B, 32B
- å¯¹æ¯”éƒ¨åˆ†æ–¹æ³•æ—¶ä½¿ç”¨ **LLaMA2-Chat-7B**

#### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **Throughput / Speedup** | ç›¸å¯¹äºæ ‡å‡†è‡ªå›å½’è§£ç çš„ç«¯åˆ°ç«¯ååæå‡å€æ•° |
| **Average Acceptance Length (T)** | æ¯è½®éªŒè¯ä¸­è¢«æ¥å—çš„ draft token æ•°é‡å‡å€¼ï¼Œåæ˜  draft å‡†ç¡®æ€§ |
| **Drafting Latency** | draft model å‰å‘ + tree search æ‰€è€—æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ |

#### Draft é•¿åº¦
- DART å’Œ EAGLE3 å‡è®¾ä¸º **8**
- SPS ä½¿ç”¨ Qwen3-1.7B ä½œä¸º draft modelï¼Œé•¿åº¦ä¸º 5

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®

| æ¨¡å‹ | æ–¹æ³• | å¹³å‡ Speedup | å¹³å‡ T å€¼ |
|------|------|-------------|----------|
| Qwen3-14B | EAGLE3 | 2.02Ã— | 3.48 |
| Qwen3-14B | **DART** | **2.77Ã—** | **3.67** |
| Qwen3-32B | EAGLE3 | 2.11Ã— | 3.85 |
| Qwen3-32B | **DART** | **2.42Ã—** | **3.76** |
| LLaMA2-Chat-7B | EAGLE3 | 2.66Ã— | 3.55 |
| LLaMA2-Chat-7B | **DART** | **2.85Ã—** | **4.08** |

> âœ… **æœ€å¤§ç«¯åˆ°ç«¯åŠ é€Ÿè¾¾ 3.44Ã—ï¼Œå¹³å‡æ¯” EAGLE3 å¿« 30%**

> ğŸ”¥ åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºï¼Œä¾‹å¦‚åœ¨ CodeAlpaca ä¸Šå¯¹ Qwen3-14B çš„åŠ é€Ÿæ¯” EAGLE3 æå‡ **65%**

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

| æ–¹æ³• | ç‰¹ç‚¹ | DART ç›¸å¯¹ä¼˜åŠ¿ |
|------|------|----------------|
| **SPS** (Standard Speculative Sampling) | ä½¿ç”¨å°æ¨¡å‹è‡ªå›å½’ draft | DART drafting forward å¿« **53.3Ã—**ï¼Œæ•´ä½“åŠ é€Ÿè¿œè¶… |
| **EAGLE3** | å•å±‚è‡ªå›å½’ draftï¼Œç²¾åº¦é«˜ä½†å»¶è¿Ÿä»é«˜ | DART drafting forward å¿« **6.8Ã—**ï¼Œç«¯åˆ°ç«¯å¿« **30%** |
| **Medusa / Hydra** | å¤šå¤´å¹¶è¡Œé¢„æµ‹ï¼Œéè‡ªå›å½’ | DART åœ¨å¤šæ•°ä»»åŠ¡ä¸Š throughput æ›´é«˜ |
| **Lookahead / PLD** | æ— æ¨¡å‹å¯å‘å¼ draft | DART çš„ T å€¼æ›´é«˜ï¼Œå¸¦æ¥æ›´ç¨³å®šæ˜¾è‘—çš„åŠ é€Ÿ |

### æ¶ˆèå®éªŒç»“æœ

#### âœ… N-gram å‰ªææœ‰æ•ˆæ€§ï¼ˆTable 2ï¼‰
| è®¾ç½® | HumanEval (T) | Alpaca (T) | CodeAlpaca (T) |
|------|---------------|-----------|----------------|
| æ—  N-gram | 3.13 | 2.76 | 3.85 |
| æœ‰ N-gram | **3.63 (+0.5)** | **3.26 (+0.5)** | **4.59 (+0.74)** |

> N-gram æ˜¾è‘—æå‡ T å€¼ï¼Œè¯æ˜å…¶åœ¨è¿‡æ»¤è¯­ä¹‰ä¸åˆç†è·¯å¾„ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

#### âœ… Shifted Logits Predictionï¼ˆTable 3ï¼‰
| ä½ç½® | Unshifted (Hit@1) | Shifted (Hit@1) | æå‡ |
|------|--------------------|------------------|------|
| Q-1 | 57.7% | **71.1%** | +13.4% |
| Q-2 | 38.1% | **41.0%** | +2.9% |
| Q-3 | 25.1% | **27.6%** | +2.5% |

> â€œShiftedâ€ è®¾è®¡å¤§å¹…æå‡é¦–ä¸ªä½ç½®é¢„æµ‹å‡†ç¡®ç‡ï¼Œè¿™å¯¹ speculative decoding è‡³å…³é‡è¦ã€‚

#### âœ… Annealing Coefficient Î³ é€‰æ‹©ï¼ˆTable 4ï¼‰
æœ€ä¼˜è®¾ç½®ä¸º **Î³ = 0.6**ï¼Œæ­¤æ—¶ï¼š
- æ—©æœŸä½ç½®ï¼ˆQ-1 ~ Q-3ï¼‰å‡†ç¡®ç‡è¾ƒé«˜
- æ•´ä½“ T å€¼è¾¾åˆ°æœ€é«˜ï¼ˆ3.63 vs baseline 3.48ï¼‰

> è¯æ˜è¡°å‡å¼ KL ç›®æ ‡å‡½æ•°èƒ½æœ‰æ•ˆå¹³è¡¡çŸ­ç¨‹ä¸é•¿ç¨‹é¢„æµ‹ç¨³å®šæ€§ã€‚

#### âœ… æ›´å¤§ batch size ä¸‹çš„è¡¨ç°ï¼ˆTable 5ï¼‰
å³ä½¿ batch size å¢å¤§è‡³ 64ï¼ŒDART ä¾ç„¶ä¼˜äº EAGLE3ï¼š
| Batch Size | EAGLE3 Speedup | DART Speedup |
|------------|----------------|--------------|
| 2 | 1.84Ã— | **2.16Ã—** |
| 32 | 1.28Ã— | **1.48Ã—** |
| 64 | 1.22Ã— | **1.45Ã—** |

> è¡¨æ˜ DART çš„ä¼˜åŠ¿åœ¨ä¸åŒè´Ÿè½½ä¸‹å…·æœ‰é²æ£’æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### è®ºæ–‡çš„ä¸»è¦å‘ç°
1. **å¹¶è¡Œ drafting æ˜¯å¯è¡Œä¸”é«˜æ•ˆçš„**ï¼šé€šè¿‡å®šåˆ¶åŒ–çš„ diffusion-style å¹¶è¡Œé¢„æµ‹æœºåˆ¶ï¼Œå¯åœ¨ä¸ç‰ºç‰²å‡†ç¡®æ€§çš„å‰æä¸‹å¤§å¹…é™ä½ drafting latencyã€‚
2. **æœ€å°åŒ– drafting overhead æ¯”å•çº¯è¿½æ±‚é«˜ T æ›´é‡è¦**ï¼šDART çš„ T å€¼ç•¥ä½äºæˆ–æ¥è¿‘ EAGLE3ï¼Œä½†ç”±äº drafting latency æä½ï¼Œæœ€ç»ˆç«¯åˆ°ç«¯æ€§èƒ½åè€Œæ›´å¼ºã€‚
3. **N-gram åº”ç”¨äº continuity-aware pruning è€Œéç‹¬ç«‹ predictor æ›´æœ‰æ•ˆ**ï¼šç»“åˆæ¨¡å‹ logits ä¸ N-gram è¿ç»­æ€§è¯„åˆ†å¯æ„å»ºé«˜è´¨é‡ draft treeã€‚
4. **DART è®¾è®¡å…·æœ‰ç¡¬ä»¶é€šç”¨æ€§**ï¼šåœ¨ H20-3e å’Œ A100 ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„ä¼˜è¶Šæ€§èƒ½ï¼ˆè§ Appendix Fï¼‰ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **å†…å­˜å ç”¨å¢åŠ **ï¼šN-gram æ¨¡å‹éœ€åŠ è½½çº¦ **100GB CPU RAM**ï¼Œè™½å¯å…±äº«ä½†ä»æœ‰ä¸€å®šèµ„æºé—¨æ§›ã€‚
- **ä¾èµ– target model çš„ä¸­é—´éšè—çŠ¶æ€**ï¼šéœ€è¦è®¿é—® target model çš„ç‰¹å®šå±‚è¾“å‡ºï¼Œå¯èƒ½é™åˆ¶æŸäº›é—­æº API åœºæ™¯çš„åº”ç”¨ã€‚
- **è®­ç»ƒæˆæœ¬å­˜åœ¨**ï¼šéœ€é’ˆå¯¹æ¯ä¸ª target model å•ç‹¬è®­ç»ƒ draft modelï¼Œå°½ç®¡è®­ç»ƒæµç¨‹å·²ä¼˜åŒ–ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢æ›´ç´§å‡‘çš„ continuity model æ›¿ä»£å¤§å‹ N-gram trieï¼Œé™ä½å†…å­˜éœ€æ±‚ã€‚
- å°† DART æ€è·¯æ‰©å±•è‡³ MoE æ¨¡å‹æˆ–å…¶ä»–æ–°å‹æ¶æ„ã€‚
- ç ”ç©¶ zero-shot æˆ– prompt-driven draft modelï¼Œå‡å°‘è®­ç»ƒä¾èµ–ã€‚
- ç»“åˆåŠ¨æ€ draft length æ§åˆ¶ç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> **DART é€šè¿‡â€œå¹¶è¡Œ logits é¢„æµ‹ + N-gram å¼•å¯¼å‰ªæâ€çš„æ–°èŒƒå¼ï¼Œåœ¨å‡ ä¹ä¸å¢åŠ  drafting å¼€é”€çš„å‰æä¸‹å®ç°äº†ä¸æœ€å¼ºè‡ªå›å½’ draft model ç›¸å½“çš„å‡†ç¡®æ€§ï¼Œä»è€Œè¾¾æˆå½“å‰æœ€ä¼˜çš„ç«¯åˆ°ç«¯ speculative decoding åŠ é€Ÿæ•ˆæœã€‚**

</details>

---

### 2. [Learning Adaptive Parallel Execution for Efficient Code Localization](https://arxiv.org/abs/2601.19568)

**Authors**: Ke Xu, Siyang Xiao, Ming Liang, Yichen Yu, Zhixiang Wang, Jingxuan Xu, Dajun Chen, Wei Jiang, Yong Li  
**Category**: cs.AI  
**Published**: 2026-01-28  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.19568v1  

#### Abstract
Code localization constitutes a key bottleneck in automated software development pipelines. While concurrent tool execution can enhance discovery speed, current agents demonstrate a 34.9\% redundant invocation rate, which negates parallelism benefits. We propose \textbf{FuseSearch}, reformulating pa...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šLearning Adaptive Parallel Execution for Efficient Code Localization

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä»£ç å®šä½ï¼ˆcode localizationï¼‰æ˜¯è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘æµç¨‹ä¸­çš„å…³é”®ç“¶é¢ˆï¼Œå½“å‰åŸºäºLLMçš„ä»£ç†ï¼ˆagentï¼‰åœ¨è¯¥ä»»åŠ¡ä¸Šæ¶ˆè€—è¶…è¿‡50%çš„è®¡ç®—èµ„æºã€‚ä¼ ç»Ÿé¡ºåºå·¥å…·è°ƒç”¨æ–¹å¼å¯¼è‡´â€œä¿¡æ¯é¥¥é¥¿â€ï¼ˆinformation starvationï¼‰â€”â€”åœ¨æœ‰é™çš„äº¤äº’è½®æ¬¡ï¼ˆturn budgetï¼‰ä¸‹æ— æ³•è·å–è¶³å¤Ÿä¸Šä¸‹æ–‡ã€‚å°½ç®¡å¹¶è¡Œæ‰§è¡Œå¯æå‡ä¿¡æ¯å¯†åº¦ï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸å¼•å…¥é«˜è¾¾34.9%çš„å†—ä½™å·¥å…·è°ƒç”¨ï¼Œé€ æˆèµ„æºæµªè´¹å’Œå™ªå£°å¹²æ‰°ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°
æœ¬æ–‡æå‡º **FuseSearch**ï¼Œä¸€ç§é€šè¿‡å­¦ä¹ è‡ªé€‚åº”å¹¶è¡Œæ‰§è¡Œå®ç°é«˜æ•ˆä»£ç å®šä½çš„æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **å·¥å…·æ•ˆç‡ï¼ˆtool efficiencyï¼‰åº¦é‡**ï¼šå®šä¹‰ä¸ºæœ‰æ•ˆä¿¡æ¯å¢ç›Šï¼ˆnovel information gainï¼‰ä¸æ€»è°ƒç”¨æ¬¡æ•°çš„æ¯”ç‡ï¼Œé‡åŒ–æ¯æ¬¡å·¥å…·è°ƒç”¨çš„ä¿¡æ¯æ–°é¢–æ€§ï¼Œç›´æ¥æƒ©ç½šå†—ä½™æ¢ç´¢ã€‚
- **åŒç›®æ ‡ä¼˜åŒ–æ¡†æ¶**ï¼šè”åˆä¼˜åŒ–å®šä½è´¨é‡ï¼ˆF1-scoreï¼‰ä¸æœç´¢æ•ˆç‡ï¼ˆtool efficiencyï¼‰ï¼Œé€šè¿‡SFT + RLä¸¤é˜¶æ®µè®­ç»ƒï¼Œä½¿æ¨¡å‹å­¦ä¼šåœ¨ä¿è¯ç²¾åº¦çš„åŒæ—¶æœ€å°åŒ–å†—ä½™ã€‚
- **æç®€ä¸»ä¹‰å·¥å…·é›†**ï¼šä»…ä¾èµ–ä¸‰ä¸ªè¯­è¨€æ— å…³ã€åªè¯»çš„å·¥å…·ï¼ˆ`grep`, `glob`, `read_file`ï¼‰ï¼Œæ— éœ€ASTè§£æå™¨æˆ–ä»£ç å›¾ç­‰å¤æ‚åŸºç¡€è®¾æ–½ï¼Œæå‡éƒ¨ç½²é€šç”¨æ€§ã€‚
- **åŠ¨æ€å¹¶è¡Œç­–ç•¥**ï¼šä¸åŒäºå›ºå®šå®½åº¦çš„å¹¶è¡Œæ–¹æ¡ˆï¼ŒFuseSearchæ ¹æ®ä»»åŠ¡ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ¯è½®å¹¶è¡Œå·¥å…·æ•°é‡ï¼Œä»å¹¿åº¦æ¢ç´¢é€æ­¥è¿‡æ¸¡åˆ°ç²¾ç»†å®šä½ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´é«˜æ•ˆ**ï¼šæ˜¾è‘—å‡å°‘äº¤äº’è½®æ•°ã€æ—¶é—´ä¸tokenæ¶ˆè€—ã€‚
- **æ›´ç²¾å‡†**ï¼šé€šè¿‡æŠ‘åˆ¶å†—ä½™è°ƒç”¨ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡è½½ï¼Œæå‡å®šä½å‡†ç¡®ç‡ã€‚
- **æ›´è½»é‡**ï¼šä¸ä¾èµ–é¢„æ„å»ºçš„ä»£ç å›¾æˆ–è¯­è¨€ç‰¹å®šè§£æå™¨ï¼Œéƒ¨ç½²æˆæœ¬ä½ã€‚
- **å¯æ‰©å±•**ï¼šé€‚ç”¨äºä¸åŒè§„æ¨¡æ¨¡å‹ï¼ˆå¦‚4B/30Bå‚æ•°ï¼‰ï¼Œä¸”èƒ½åŠ é€Ÿä¸‹æ¸¸ä»»åŠ¡ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **ä¸»è¯„ä¼°æ•°æ®é›†**ï¼š**SWE-bench Verified**ï¼ˆJimenez et al., 2024ï¼‰ï¼Œä¸€ä¸ªé¢å‘çœŸå®GitHub issueä¿®å¤çš„ä»“åº“çº§åŸºå‡†æµ‹è¯•ã€‚
- **è¿‡æ»¤æ¡ä»¶**ï¼šæ’é™¤è¡¥ä¸ä¸­å¼•å…¥å…¨æ–°æ–‡ä»¶æˆ–å‡½æ•°çš„æ ·æœ¬ï¼Œä¿ç•™386ä¸ªå®ä¾‹ç”¨äºè¯„ä¼°ã€‚
- **è®­ç»ƒæ•°æ®é›†**ï¼šä»233ä¸ªé«˜è´¨é‡GitHubä»“åº“ä¸­æ„å»ºçº¦21Kä¸ªissue-patchå¯¹ï¼Œç»ç­›é€‰åç”¨äºSFTå’ŒRLè®­ç»ƒã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
#### è¯„ä¼°ç»´åº¦
- **å®šä½è´¨é‡**ï¼š
  - æ–‡ä»¶çº§ä¸å‡½æ•°çº§çš„ Precisionã€Recall å’Œ F1-scoreã€‚
- **æœç´¢æ•ˆç‡**ï¼š
  - äº¤äº’è½®æ•°ï¼ˆ#Turnï¼‰
  - æ€»è€—æ—¶ï¼ˆT(s)ï¼Œwall-clock timeï¼‰
  - æ€»tokenæ¶ˆè€—ï¼ˆTok.(k)ï¼‰
  - å·¥å…·æ•ˆç‡ï¼ˆeï¼‰

#### æ¨¡å‹é…ç½®
- ä¸»å¹²æ¨¡å‹ï¼šQwen3-4B-Instruct å’Œ Qwen3-30B-A3B-Instructã€‚
- è®­ç»ƒæµç¨‹ï¼š
  1. **SFTé˜¶æ®µ**ï¼šä½¿ç”¨æ•™å¸ˆæ¨¡å‹ï¼ˆKimi-K2-Instructï¼‰ç”Ÿæˆè½¨è¿¹ï¼Œå¹¶é‡‡ç”¨**åŒæŒ‡æ ‡è¿‡æ»¤**ï¼ˆF1 â‰¥ 0.6 ä¸” e â‰¥ 0.6ï¼‰ç¡®ä¿é«˜è´¨é‡ç¤ºèŒƒæ•°æ®ã€‚
  2. **RLé˜¶æ®µ**ï¼šåŸºäºGRPOç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¥–åŠ±å‡½æ•°è®¾è®¡ä¸ºï¼š
     $$
     R(T) = \alpha \cdot F1(T) + \gamma \cdot (F1(T) \cdot e(T))
     $$
     å…¶ä¸­ $\alpha=0.8$, $\gamma=0.2$ï¼Œå¼ºè°ƒé«˜F1ä¸‹çš„é«˜æ•ˆç‡è¡Œä¸ºã€‚

#### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | æ–¹æ³• | ç‰¹ç‚¹ |
|------|------|------|
| Workflow-based | Agentless (Xia et al., 2024) | å›ºå®šåˆ†å±‚ç­–ç•¥ï¼Œæ— çŠ¶æ€ç»´æŠ¤ |
| Agent-based | LocAgent (Chen et al., 2025) | åŸºäºé™æ€ä»£ç å›¾å¯¼èˆª |
| | CoSIL (Jiang et al., 2025) | åŠ¨æ€æ„å»ºæ¨¡å—è°ƒç”¨å›¾ |
| | RepoSearcher (Ma et al., 2025) | è½»é‡å·¥å…·å¥—ä»¶ï¼Œæ— å›¾ç»“æ„ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆSWE-bench Verifiedï¼‰

| æ¨¡å‹ | æ–‡ä»¶F1 | å‡½æ•°F1 | #Turn | æ—¶é—´(s) | Token(k) | æ¨¡å¼ |
|------|--------|--------|-------|----------|-----------|------|
| **FuseSearch-4B (train)** | **84.7%** | **56.4%** | **4.78** | **5.43** | **30.9** | Par |
| RepoSearcher (Qwen3-4B) | 38.1% | 21.7% | 14.8 | 85.3 | 99.2 | Seq |
| Agentless (Haiku 4.5) | 54.5% | 31.8% | 2.00 | 7.32 | 10.6 | Seq |
| LocAgent (Qwen3-4B) | 72.3% | 50.6% | 17.3 | 185 | 67 | Seq |

> âœ… **æ€§èƒ½æå‡æ€»ç»“**ï¼š
- ç›¸æ¯”RepoSearcherï¼Œ**æ–‡ä»¶F1æå‡123%**ï¼Œ**å‡½æ•°F1æå‡160%**ã€‚
- **å‡å°‘67.7%äº¤äº’è½®æ•°**ï¼Œ**é™ä½93.6%æ—¶é—´å¼€é”€**ï¼Œ**èŠ‚çœ68.9% tokenæ¶ˆè€—**ã€‚
- åœ¨åŒç­‰backboneä¸‹è¶…è¶Šä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚Haiku 4.5ï¼‰ã€‚

### ä¸å…¶ä»–åŸºçº¿å¯¹æ¯”
- **ä¼˜äºLocAgent**ï¼šå°½ç®¡LocAgentä½¿ç”¨å¤æ‚å›¾ç»“æ„ï¼ŒFuseSearchä»ä»¥æ›´å°‘è½®æ¬¡å’Œæ›´ä½å»¶è¿Ÿå–å¾—æ›´é«˜F1ã€‚
- **è¶…è¶ŠAgentless**ï¼šè™½åè€…ä¸ºworkflowæ¨¡å¼ï¼Œä½†FuseSearchåœ¨recallå’Œprecisionä¸Šå…¨é¢é¢†å…ˆã€‚

### æ¶ˆèå®éªŒç»“æœ

#### å¹¶è¡Œ vs é¡ºåºæ‰§è¡Œï¼ˆTable 3ï¼‰
| æ¨¡å¼ | é˜¶æ®µ | æ–‡ä»¶F1 | å‡½æ•°F1 | #Turn | Tok.(k) |
|------|------|--------|--------|-------|---------|
| Sequential | SFT+RL | 78.82 | 50.21 | 7.52 | 59.4 |
| **Parallel** | **SFT+RL** | **84.65** | **56.45** | **5.60** | **30.9** |

> å¹¶è¡Œæ‰§è¡Œä¸ä»…æé€Ÿï¼Œè¿˜å› æ¯æ­¥è·å–æ›´å¤šä¿¡æ¯è€Œæå‡æœ€ç»ˆF1ã€‚

#### SFTæ•°æ®è¿‡æ»¤ç­–ç•¥ï¼ˆTable 4ï¼‰
| è¿‡æ»¤æ–¹å¼ | æ–‡ä»¶F1 | å‡½æ•°F1 | æ•ˆç‡e | Tok.(k) |
|--------|--------|--------|--------|---------|
| æ— è¿‡æ»¤ | 75.44 | 43.52 | 55.77 | 60.7 |
| F1-only | 78.55 | 45.43 | 56.72 | 73.2 |
| e-only | 76.74 | 42.63 | 60.14 | 61.8 |
| **Joint (F1+e)** | **78.86** | **47.94** | **62.03** | **54.8** |

> åŒæŒ‡æ ‡è¿‡æ»¤åŒæ—¶ä¼˜åŒ–è´¨é‡å’Œæ•ˆç‡ï¼Œä¸ºåç»­RLæä¾›æœ€ä¼˜åˆå§‹åŒ–ã€‚

#### å¥–åŠ±å‡½æ•°è®¾è®¡ï¼ˆTable 5ï¼‰
| å¥–åŠ±ç±»å‹ | æ–‡ä»¶F1 | å‡½æ•°F1 | e | T(s) | Tok.(k) |
|--------|--------|--------|-----|------|---------|
| F1 only | 81.84 | 54.90 | 59.66 | 7.28 | 39.4 |
| F + e (additive) | 79.22 | 51.98 | 66.62 | 9.40 | 45.7 |
| **F + FÂ·e (multiplicative)** | **84.65** | **56.45** | **69.00** | **5.43** | **30.9** |

> ä¹˜æ³•å½¢å¼å¥–åŠ±ï¼ˆF + FÂ·eï¼‰å®ç°äº†è´¨é‡ä¸æ•ˆç‡çš„æœ€ä½³å¹³è¡¡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **æ•ˆç‡æ„ŸçŸ¥è®­ç»ƒè‡ªç„¶æå‡è´¨é‡**ï¼šé€šè¿‡æ˜¾å¼å»ºæ¨¡å·¥å…·æ•ˆç‡å¹¶è”åˆä¼˜åŒ–ï¼Œæ¨¡å‹è‡ªåŠ¨è§„é¿å†—ä½™è°ƒç”¨ï¼Œä»è€Œå‡å°‘å™ªå£°å¹²æ‰°ï¼Œåè€Œæå‡äº†æœ€ç»ˆå®šä½ç²¾åº¦ã€‚
2. **å¹¶è¡Œæ‰§è¡Œçš„ä»·å€¼ä¸æ­¢äºé€Ÿåº¦**ï¼šå¹¶è¡Œä¸ä»…æ˜¯é™ä½å»¶è¿Ÿçš„æŠ€æœ¯æ‰‹æ®µï¼Œæ›´æ˜¯æå‡å†³ç­–è´¨é‡çš„å…³é”®â€”â€”æ¯è½®è·å–æ›´å¤šä¸Šä¸‹æ–‡æœ‰åŠ©äºåšå‡ºæ›´ä¼˜åˆ¤æ–­ã€‚
3. **æç®€å·¥å…·é›†è¶³ä»¥èƒœä»»å¤æ‚ä»»åŠ¡**ï¼šæ— éœ€ä»£ç å›¾æˆ–ASTè§£æå™¨ï¼Œä»…é `grep`/`glob`/`read_file`å³å¯å®ç°SOTAæ€§èƒ½ï¼ŒéªŒè¯äº†â€œæˆ˜ç•¥è°ƒåº¦â€ä¼˜äºâ€œå¤æ‚å·¥å…·â€çš„ç†å¿µã€‚
4. **è‡ªé€‚åº”å¹¶è¡Œä¼˜äºå›ºå®šå®½åº¦**ï¼šæ¨¡å‹å­¦ä¼šåœ¨æ—©æœŸå¹¿æ³›æ¢ç´¢ï¼ŒåæœŸèšç„¦ç²¾ä¿®ï¼Œä½“ç°å‡ºæ™ºèƒ½çš„æœç´¢ç­–ç•¥æ¼”åŒ–ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **è¯„ä¼°åå·®**ï¼šå½“å‰benchmarkï¼ˆå¦‚SWE-benchï¼‰ä¸»è¦è¦†ç›–Pythoné¡¹ç›®ï¼Œå¯¹Java/C++ç­‰é™æ€è¯­è¨€æ³›åŒ–èƒ½åŠ›æœ‰å¾…éªŒè¯ã€‚
2. **è§£ç©ºé—´å•ä¸€**ï¼šground truthåŸºäºå®é™…æäº¤çš„patchï¼Œå¯èƒ½å¿½ç•¥å…¶ä»–åˆç†ä½†æœªè¢«é‡‡ç”¨çš„ä¿®æ”¹è·¯å¾„ã€‚
3. **ä»»åŠ¡èŒƒå›´å—é™**ï¼šç›®å‰ä»…é’ˆå¯¹issue-drivenå®šä½ï¼Œå°šæœªæ‹“å±•è‡³ä»£ç é—®ç­”ã€æ–‡æ¡£ç”Ÿæˆç­‰æ›´å¹¿æ³›çš„code searchåœºæ™¯ã€‚
4. **ä¾èµ–é«˜è´¨é‡è®­ç»ƒæ•°æ®**ï¼šSFTé˜¶æ®µéœ€å¤§é‡é«˜è´¨é‡è½¨è¿¹ï¼Œä¾èµ–å¼ºæ•™å¸ˆæ¨¡å‹ç”Ÿæˆã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ„å»ºè·¨è¯­è¨€ã€å¤šä»»åŠ¡çš„ç»¼åˆæ€§ä»£ç å®šä½åŸºå‡†ï¼ˆcomprehensive localization benchmarksï¼‰ã€‚
- æ¢ç´¢åœ¨éissueé©±åŠ¨åœºæ™¯ï¼ˆå¦‚APIç†è§£ã€æ–‡æ¡£ç”Ÿæˆï¼‰ä¸­çš„åº”ç”¨ã€‚
- å°†FuseSearchä½œä¸ºé€šç”¨å­æ¨¡å—é›†æˆè‡³ç«¯åˆ°ç«¯è½¯ä»¶å·¥ç¨‹ä»£ç†ç³»ç»Ÿä¸­ã€‚
- ç ”ç©¶å¦‚ä½•è¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹è§„æ¨¡ï¼ˆå¦‚<1Bï¼‰ä»¥æ”¯æŒè¾¹ç¼˜éƒ¨ç½²ã€‚

> ğŸ”š **æ€»ç»“**ï¼š  
> FuseSearchè¯æ˜äº†**æ•ˆç‡é©±åŠ¨çš„å­¦ä¹ èŒƒå¼**èƒ½å¤Ÿçªç ´ä¼ ç»Ÿç²¾åº¦-æ•ˆç‡æƒè¡¡ï¼Œé€šè¿‡å¼•å…¥**tool efficiency**è¿™ä¸€æ ¸å¿ƒæ¦‚å¿µï¼Œç»“åˆ**SFT+RLåŒé˜¶æ®µè®­ç»ƒ**ï¼Œå®ç°äº†é«˜æ€§èƒ½ã€ä½æˆæœ¬ã€æ˜“éƒ¨ç½²çš„ä»£ç å®šä½è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ„å»ºç”Ÿäº§çº§è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ç³»ç»Ÿæä¾›äº†é‡è¦å®è·µè·¯å¾„ã€‚

</details>

---

### 3. [Flatter Tokens are More Valuable for Speculative Draft Model Training](https://arxiv.org/abs/2601.18902)

**Authors**: Jiaming Fan, Daming Cao, Xiangzhong Luo, Jiale Fu, Chonghan Liu, Xu Yang  
**Category**: cs.CL  
**Published**: 2026-01-28  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.18902v1  

#### Abstract
Speculative Decoding (SD) is a key technique for accelerating Large Language Model (LLM) inference, but it typically requires training a draft model on a large dataset. We approach this problem from a data-centric perspective, finding that not all training samples contribute equally to the SD accept...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Flatter Tokens are More Valuable for Speculative Draft Model Training*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

è¯¥è®ºæ–‡é’ˆå¯¹ **Speculative Decoding (SD)** ä¸­ **draft model** è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚å°½ç®¡ SD èƒ½é€šè¿‡ draft model é¢„æµ‹å¤šä¸ª token å¹¶ç”± target model å¹¶è¡ŒéªŒè¯æ¥åŠ é€Ÿ LLM æ¨ç†ï¼Œä½†è®­ç»ƒä¸€ä¸ªé«˜è´¨é‡çš„ draft model é€šå¸¸éœ€è¦åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚

å½“å‰ä¸»æµæ–¹æ³•ï¼ˆå¦‚ EAGLEï¼‰é‡‡ç”¨æ ‡å‡†çš„ **Knowledge Distillation (KD)**ï¼Œå¯¹æ‰€æœ‰ token å’Œæ ·æœ¬ä¸€è§†åŒä»åœ°è®­ç»ƒï¼Œå¿½ç•¥äº†ä¸åŒè®­ç»ƒæ ·æœ¬å¯¹æå‡ **acceptance rate** çš„è´¡çŒ®å·®å¼‚ï¼Œå¯¼è‡´è®­ç»ƒèµ„æºæµªè´¹ã€‚

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…ä» **data-centric** è§’åº¦å‡ºå‘ï¼Œæå‡ºäº†ä¸€é¡¹å…³é”®ç†è®ºæ´å¯Ÿå’Œä¸€ç§é«˜æ•ˆçš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼š

- **æ ¸å¿ƒç†è®ºæ´å¯Ÿ**ï¼š  
  åœ¨ SD åœºæ™¯ä¸‹ï¼Œ**ç›®æ ‡æ¨¡å‹ (target model) è¾“å‡ºåˆ†å¸ƒè¶Šå¹³å¦ (flatter) çš„ tokenï¼Œå¯¹æå‡ acceptance rate çš„è®­ç»ƒä»·å€¼è¶Šé«˜**ã€‚è¿™æ˜¯å› ä¸ºå¹³å¦åˆ†å¸ƒæ„å‘³ç€ draft model æœ‰æ›´å¤§çš„â€œæ”¹è¿›ç©ºé—´â€ï¼ˆheadroomï¼‰ï¼Œå•æ­¥æ›´æ–°èƒ½å¸¦æ¥æ›´å¤§çš„ L1-norm ä¸‹é™ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æå‡ acceptance rateã€‚

- **æå‡ºçš„æŒ‡æ ‡**ï¼š  
  å®šä¹‰äº†ä¸€ä¸ªæ–°çš„ **flatness** æŒ‡æ ‡ï¼Œé€šè¿‡è®¡ç®—ç›®æ ‡æ¨¡å‹è¾“å‡ºåˆ†å¸ƒ $ p $ ä¸å‡åŒ€åˆ†å¸ƒ $ U $ ä¹‹é—´çš„ **cosine similarity** æ¥é‡åŒ–å…¶å¹³å¦ç¨‹åº¦ï¼š
  $$
  \text{flatness}(t) = \cos(p_t, U)
  $$
  å€¼è¶Šå¤§è¡¨ç¤ºåˆ†å¸ƒè¶Šæ¥è¿‘å‡åŒ€ï¼Œtoken çš„è®­ç»ƒä»·å€¼è¶Šé«˜ã€‚

- **æå‡ºçš„æ–¹æ³•**ï¼š  
  åŸºäºä¸Šè¿°æŒ‡æ ‡ï¼Œæå‡ºäº† **Sample-level-flatness-based Dataset Distillation (SFDD)** æ–¹æ³•ï¼š
  1. å¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè®¡ç®—å…¶æ‰€æœ‰ token çš„ flatness å¹³å‡å€¼ï¼Œå¾—åˆ° **sample-level flatness**ã€‚
  2. æ ¹æ®è¯¥åˆ†æ•°å¯¹æ‰€æœ‰æ ·æœ¬æ’åºï¼Œä¿ç•™åˆ†æ•°æœ€é«˜çš„å‰ $ k\% $ æ ·æœ¬ã€‚
  3. ä»…ç”¨è¿™äº›é«˜ä»·å€¼æ ·æœ¬æ¥è®­ç»ƒ draft modelã€‚

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

- **é«˜æ•ˆæ€§**ï¼š  
  ä»…éœ€ 50% çš„æ•°æ®å³å¯å®ç°æ¥è¿‘å…¨é‡æ•°æ®è®­ç»ƒçš„æ€§èƒ½ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘è¶…è¿‡ **2Ã—**ã€‚
- **æœ‰æ•ˆæ€§**ï¼š  
  åœ¨ç›¸åŒæ•°æ®ä¿ç•™ç‡ä¸‹ï¼ŒSFDD æ˜¾è‘—ä¼˜äºåŸºäº **entropy**, **top-1 probability**, **margin**, **Energy Score**, **PPL** ç­‰å¸¸è§æŒ‡æ ‡çš„é€‰æ‹©æ–¹æ³•ã€‚
- **æ­£äº¤æ€§**ï¼š  
  SFDD æ˜¯ä¸€ç§æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå¯ä¸ä»»ä½•åŸºäºè®­ç»ƒçš„ SD æ¡†æ¶ï¼ˆå¦‚ EAGLEï¼‰ç»“åˆï¼Œä¸å¢åŠ éƒ¨ç½²å¤æ‚æ€§ã€‚
- **ç¦»çº¿è®¡ç®—**ï¼š  
  flatness åˆ†æ•°ä»…ä¾èµ–äºå›ºå®šçš„ target modelï¼Œå¯åœ¨è®­ç»ƒå‰ä¸€æ¬¡æ€§ç¦»çº¿è®¡ç®—ï¼Œæ— é¢å¤–åŠ¨æ€å¼€é”€ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- **è®­ç»ƒæ•°æ®é›†**ï¼š`ShareGPT` æ•°æ®é›†ï¼ˆç»è¿‡è¿‡æ»¤ï¼‰ã€‚
- **è¯„ä¼°ä»»åŠ¡**ï¼šåœ¨äº”ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œæµ‹è¯•ï¼š
  - `GSM8K` (æ•°å­¦æ¨ç†)
  - `Alpaca` (æŒ‡ä»¤éµå¾ª)
  - `MT-Bench` (å¤šè½®å¯¹è¯)
  - `CNN/DM` (æ‘˜è¦ç”Ÿæˆ)
  - `Natural Questions (NQ)` (é—®ç­”)

### **å®éªŒè®¾ç½®**

- **æ¡†æ¶**ï¼šåŸºäº **EAGLE-2** çš„è®­ç»ƒæµç¨‹ã€‚
- **Target Model**ï¼š`LLaMA3-8B-Instruct`ã€‚
- **Draft Model**ï¼šè½»é‡çº§å•å±‚ Transformerã€‚
- **ç¡¬ä»¶**ï¼šNVIDIA H800 GPUsã€‚
- **è¶…å‚æ•°**ï¼š
  - è®­ç»ƒ 30 ä¸ª epoch
  - å­¦ä¹ ç‡ $5 \times 10^{-5}$
  - Batch size ä¸º 2
  - è§£ç æ¸©åº¦ (decoding temperature) é»˜è®¤ä¸º 1.0ï¼Œéƒ¨åˆ†å®éªŒåœ¨ 0 ä¸‹æµ‹è¯•ã€‚

### **è¯„ä¼°æŒ‡æ ‡**

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| **Speedup** | è‡ªå›å½’è§£ç è€—æ—¶ / SD è§£ç è€—æ—¶ã€‚è¶Šé«˜è¶Šå¥½ã€‚ |
| **Average acceptance length ($\bar{l}$)** | æ¯æ¬¡éªŒè¯å‘¨æœŸå¹³å‡æ¥å—çš„ draft token æ•°é‡ã€‚è¶Šé«˜è¶Šå¥½ã€‚ |
| **Training time** | æ€»è®­ç»ƒè€—æ—¶ï¼ˆç§’ï¼‰ï¼Œç”¨äºè¡¡é‡è®­ç»ƒæ•ˆç‡ã€‚ |

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

- **No Filter**ï¼šä½¿ç”¨å…¨é‡æ•°æ®è®­ç»ƒï¼ˆåŸºçº¿ï¼‰ã€‚
- **Random Filtering**ï¼šéšæœºä¿ç•™ $k\%$ æ•°æ®ã€‚
- **å…¶ä»–é‡è¦æ€§æŒ‡æ ‡**ï¼š
  - **Entropy** (ç†µ)
  - **Top-1 Probability** (æœ€é«˜æ¦‚ç‡)
  - **Margin** (å‰ä¸¤åæ¦‚ç‡å·®)
  - **Energy Score**
  - **Perplexity (PPL)**

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

åœ¨ **50% æ•°æ®ä¿ç•™ç‡** ä¸‹ï¼ŒSFDD å–å¾—äº†æ¥è¿‘å…¨é‡æ•°æ®è®­ç»ƒçš„æ€§èƒ½ï¼š

- **å¹³å‡ Speedup**ï¼š**2.41Ã—**ï¼ˆå…¨é‡æ•°æ®ä¸º 2.49Ã—ï¼‰ï¼Œæ€§èƒ½æŸå¤±ä»… **<4%**ã€‚
- **è®­ç»ƒé€Ÿåº¦æå‡**ï¼šè®­ç»ƒæ—¶é—´ä» 58,227 ç§’é™è‡³ 28,787 ç§’ï¼Œå®ç° **2.02Ã—** çš„è®­ç»ƒåŠ é€Ÿã€‚
- **æ•°æ®é€‰æ‹©å¼€é”€**ï¼šSFDD çš„ç¦»çº¿æ•°æ®ç­›é€‰è€—æ—¶ 2,242 ç§’ï¼Œå å…¨é‡è®­ç»ƒæ—¶é—´çº¦ 3.85%ï¼Œå¯å¿½ç•¥ä¸è®¡ã€‚

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

| æ–¹æ³• | å¹³å‡ Speedup | å¹³å‡ $\bar{l}$ |
|------|--------------|----------------|
| **No Filter** | 2.49Ã— | 2.78 |
| **Random Filtering** | 2.20Ã— | 2.46 |
| **Entropy** | 2.20Ã— | 2.49 |
| **Top-1 Probability** | 2.23Ã— | 2.49 |
| **SFDD (Ours)** | **2.41Ã—** | **2.56** |

- SFDD åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„ **Speedup** å’Œ **$\bar{l}$** å‡æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿ã€‚
- åœ¨ **70% æ•°æ®ä¿ç•™ç‡** ä¸‹ï¼ŒSFDD çš„æ€§èƒ½å·²åŸºæœ¬ä¸å…¨é‡æ•°æ®æŒå¹³ï¼Œç”šè‡³åœ¨ `Alpaca` ä¸Šç•¥æœ‰è¶…è¶Šã€‚

### **æ¶ˆèå®éªŒç»“æœ**

- **ä¸åŒä¿ç•™ç‡ä¸‹çš„è¡¨ç°**ï¼ˆTable 2ï¼‰ï¼š
  - SFDD åœ¨ **30% ~ 70%** çš„ä¿ç•™ç‡ä¸‹å‡å¤§å¹…é¢†å…ˆ Random å’Œ Top-1 Probabilityã€‚
  - å³ä½¿åœ¨ **20%** æ•°æ®ä¸‹ï¼ŒSFDD ä»èƒ½ä¿æŒ 2.23Ã— çš„å¹³å‡ Speedupã€‚

- **æç«¯ä½æ•°æ®åœºæ™¯**ï¼ˆTable 3ï¼‰ï¼š
  - åœ¨ **5%** æ•°æ®ä¿ç•™ç‡ä¸‹ï¼ŒSFDD ä»èƒ½å®ç° **1.82Ã—** çš„å¹³å‡ Speedupï¼Œä¼˜äº Random çš„ 1.68Ã—ï¼Œè¯æ˜ flatness æŒ‡æ ‡çš„é²æ£’æ€§ã€‚

- **èšåˆæ–¹å¼æ¶ˆè**ï¼ˆTable 11ï¼‰ï¼š
  - ä½¿ç”¨ **median** èšåˆ token-level flatness å¾—åˆ° sample-level flatnessï¼Œæ€§èƒ½ä¸ **mean** èšåˆåŒç­‰ï¼Œè¡¨æ˜æ–¹æ³•å¯¹èšåˆç­–ç•¥ä¸æ•æ„Ÿã€‚

- **è·¨æ¨¡å‹ä¸è·¨æ•°æ®é›†æ³›åŒ–**ï¼ˆTable 9 & 10ï¼‰ï¼š
  - åœ¨ `Vicuna-7B-v1.3` å’Œ `GSM8K` è®­ç»ƒé›†ä¸ŠéªŒè¯ï¼ŒSFDD ä¾ç„¶æœ‰æ•ˆï¼Œè¯æ˜å…¶é€šç”¨æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **å¹¶éæ‰€æœ‰ token éƒ½åŒç­‰é‡è¦**ï¼š  
   ç›®æ ‡æ¨¡å‹è¾“å‡ºåˆ†å¸ƒ **å¹³å¦çš„ token**ï¼ˆflatness é«˜ï¼‰æ˜¯æå‡ SD acceptance rate çš„å…³é”®é©±åŠ¨åŠ›ï¼Œè€Œå°–é”åˆ†å¸ƒçš„ token è´¡çŒ®æå°ä¸”è¿…é€Ÿé¥±å’Œã€‚

2. **flatness æ˜¯æœ‰æ•ˆçš„ä»·å€¼æŒ‡æ ‡**ï¼š  
   åŸºäº cosine similarity çš„ flatness æŒ‡æ ‡èƒ½å¯é é¢„æµ‹ token çš„è®­ç»ƒæ½œåŠ›ï¼Œä¼˜äº entropy ç­‰ä¼ ç»Ÿä¸ç¡®å®šæ€§åº¦é‡ã€‚

3. **æ•°æ®è’¸é¦å¯å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡**ï¼š  
   é€šè¿‡ SFDD è¿‡æ»¤æ‰ä½ä»·å€¼æ ·æœ¬ï¼Œå¯åœ¨å‡ ä¹ä¸æŸå¤±æ¨ç†åŠ é€Ÿæ•ˆæœçš„å‰æä¸‹ï¼Œå°†è®­ç»ƒæˆæœ¬å‡åŠä»¥ä¸Šã€‚

4. **è®­ç»ƒå·²æˆä¸º SD ä¸å¯å¿½è§†çš„æˆæœ¬**ï¼š  
   éšç€ SD æ–¹æ³•å‘å¤šé˜¶æ®µã€RL-style è®­ç»ƒå‘å±•ï¼ˆå¦‚ EAGLE-3, GTOï¼‰ï¼Œè®­ç»ƒæˆæœ¬æ—¥ç›Šå¢åŠ ï¼Œä¼˜åŒ–è®­ç»ƒæ•ˆç‡å˜å¾—è‡³å…³é‡è¦ã€‚

### **æ–¹æ³•çš„å±€é™æ€§**

- **ä¾èµ–äºç›®æ ‡æ¨¡å‹**ï¼šflatness è®¡ç®—ä¾èµ–äºä¸€ä¸ªå›ºå®šçš„ target modelï¼Œè‹¥ target model å‘ç”Ÿå˜åŒ–ï¼Œéœ€é‡æ–°è®¡ç®—ã€‚
- **æœªè€ƒè™‘ä¸Šä¸‹æ–‡åŠ¨æ€æ€§**ï¼šflatness æ˜¯é™æ€è¯„åˆ†ï¼Œæœªå»ºæ¨¡ draft model åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ¨æ€å­¦ä¹ è¿‡ç¨‹ã€‚
- **æ ·æœ¬çº§è¿‡æ»¤**ï¼šç›®å‰æ˜¯æ ·æœ¬çº§è¿‡æ»¤ï¼Œæ— æ³•åœ¨åºåˆ—å†…éƒ¨è¿›è¡Œç»†ç²’åº¦çš„ token çº§è·³è¿‡ï¼ˆå› å½“å‰æ¡†æ¶é™åˆ¶ï¼‰ã€‚

### **æœªæ¥å·¥ä½œæ–¹å‘**

- å°† flatness æŒ‡æ ‡æ¨å¹¿åˆ°å…¶ä»– **LLM åŠ é€Ÿæ¶æ„**ï¼ˆå¦‚ Lookahead Decoding, Chunked Decodingï¼‰ã€‚
- æ¢ç´¢ **åŠ¨æ€é€‰æ‹©ç­–ç•¥**ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ® draft model çš„çŠ¶æ€è°ƒæ•´æ•°æ®æƒé‡ã€‚
- è‹¥æœªæ¥è®­ç»ƒæ¡†æ¶æ”¯æŒï¼Œç ”ç©¶ **token-level flatness filtering** ä»¥è¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚
- å°†è¯¥æ€æƒ³åº”ç”¨äº **å…¶ä»–æ¨¡å‹å‹ç¼©æŠ€æœ¯**ï¼Œå¦‚æ›´é«˜æ•ˆçš„ KD æˆ– pruningã€‚

--- 

> **ä»£ç å¼€æº**ï¼šhttps://anonymous.4open.science/r/Flatness

</details>

---

### 4. [OWLEYE: Zero-Shot Learner for Cross-Domain Graph Data Anomaly Detection](https://arxiv.org/abs/2601.19102)

**Authors**: Lecheng Zheng, Dongqi Fu, Zihao Li, Jingrui He  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2601.19102v1  

#### Abstract
Graph data is informative to represent complex relationships such as transactions between accounts, communications between devices, and dependencies among machines or processes. Correspondingly, graph anomaly detection (GAD) plays a critical role in identifying anomalies across various domains, incl...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# OWLEYE: Zero-Shot Learner for Cross-Domain Graph Data Anomaly Detection è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ç°æœ‰çš„ **Graph Anomaly Detection (GAD)** æ–¹æ³•å¤§å¤šéµå¾ªâ€œone model for one datasetâ€èŒƒå¼ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„å›¾æ•°æ®ã€‚æ–°å…´çš„â€œone-for-allâ€é€šç”¨æ¨¡å‹ï¼ˆå¦‚ ARC å’Œ UNPromptï¼‰è™½ç„¶å°è¯•è·¨åŸŸæ£€æµ‹ï¼Œä½†ä»é¢ä¸´ä»¥ä¸‹ä¸‰å¤§æŒ‘æˆ˜ï¼š
1. **è·¨åŸŸç‰¹å¾å¯¹é½å›°éš¾**ï¼šä¸åŒé¢†åŸŸå›¾æ•°æ®çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦å’Œè¯­ä¹‰å·®å¼‚å¤§ï¼Œç°æœ‰æ–¹æ³•ï¼ˆå¦‚ PCA/SVDï¼‰æ— æ³•æœ‰æ•ˆå¯¹é½ä¸”ä¼šç ´ååŸå§‹ç»“æ„æ¨¡å¼ã€‚
2. **ç¼ºä¹æŒç»­å­¦ä¹ èƒ½åŠ›**ï¼šç°æœ‰æ¨¡å‹ä¸æ”¯æŒåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¢é‡æ›´æ–°æ­£å¸¸/å¼‚å¸¸æ¨¡å¼ã€‚
3. **ä¾èµ–å°‘é‡æ ‡ç­¾è¿›è¡Œå°‘æ ·æœ¬å­¦ä¹ **ï¼šå®é™…åœºæ™¯ä¸­è·å–æ ‡æ³¨æˆæœ¬é«˜ï¼Œé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰æ£€æµ‹æ›´å…·ç°å®æ„ä¹‰ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šOWLEYE
ä½œè€…æå‡º **OWLEYE**ï¼Œä¸€ä¸ªå…¨æ–°çš„é›¶æ ·æœ¬è·¨åŸŸå›¾å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œå…·å¤‡ä»¥ä¸‹ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ï¼š

#### ï¼ˆ1ï¼‰Cross-Domain Feature Alignment Module
- åˆ©ç”¨æˆå¯¹èŠ‚ç‚¹è·ç¦»ç»Ÿè®¡é‡è¿›è¡Œç‰¹å¾å½’ä¸€åŒ–ï¼Œå°†ä¸åŒå›¾åµŒå…¥åˆ°å…±äº«è¾“å…¥ç©ºé—´ã€‚
- åœ¨å¯¹é½è¿‡ç¨‹ä¸­ä¿ç•™é¢†åŸŸç‰¹å®šè¯­ä¹‰ï¼Œé¿å… UNPrompt ç­‰æ–¹æ³•ç ´å Normal-Normal å¯†åº¦å…³ç³»çš„é—®é¢˜ã€‚

#### ï¼ˆ2ï¼‰Multi-Domain Multi-Pattern Dictionary Learning
- æ„å»ºåŠ¨æ€å­—å…¸ï¼Œå­˜å‚¨æ¥è‡ªå¤šä¸ªæºå›¾çš„å±æ€§çº§ï¼ˆattribute-levelï¼‰å’Œç»“æ„çº§ï¼ˆstructure-levelï¼‰æ­£å¸¸è¡Œä¸ºæ¨¡å¼ã€‚
- æ”¯æŒæ— ç¼é›†æˆæ–°å›¾çš„çŸ¥è¯†ï¼Œå®ç°**æŒç»­å­¦ä¹ **ï¼ˆcontinual learningï¼‰ï¼Œæ— éœ€é‡è®­å³å¯æå‡æ€§èƒ½ã€‚

#### ï¼ˆ3ï¼‰Truncated Attention-Based Reconstruction
- è®¾è®¡åŸºäºæˆªæ–­æ³¨æ„åŠ›æœºåˆ¶çš„é‡æ„æ¨¡å—ï¼Œåœ¨æ— ç›‘ç£æƒ…å†µä¸‹é²æ£’åœ°è¯†åˆ«å¼‚å¸¸èŠ‚ç‚¹ã€‚
- è¿‡æ»¤æ½œåœ¨å¼‚å¸¸èŠ‚ç‚¹ä½œä¸ºä¼ªæ”¯æŒé›†ï¼Œä»…åˆ©ç”¨æœ€ä»£è¡¨æ€§çš„æ­£å¸¸èŠ‚ç‚¹è¿›è¡Œé‡æ„ï¼Œå¢å¼ºæ¨ç†ç¨³å®šæ€§ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | OWLEYE | ARC / UNPrompt | ä¼ ç»Ÿæ–¹æ³• |
|------|--------|----------------|----------|
| è·¨åŸŸå¯¹é½ | âœ… ä¿ç•™ç»“æ„æ¨¡å¼ | âŒ ç ´åå¯†åº¦å…³ç³»æˆ–åˆ†ç¦»å›¾ | ä¸é€‚ç”¨ |
| æŒç»­å­¦ä¹  | âœ… å¯åŠ¨æ€æ‰©å±•å­—å…¸ | âŒ å›ºå®šæ¨¡å‹å‚æ•° | âŒ |
| é›¶æ ·æœ¬èƒ½åŠ› | âœ… å®Œå…¨æ— éœ€ç›®æ ‡åŸŸæ ‡ç­¾ | âš ï¸ é€šå¸¸éœ€ few-shot æ ‡ç­¾ | âŒ |
| æ³›åŒ–æ€§ | å¼º | ä¸­ç­‰ | å¼± |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **è®­ç»ƒé›† $ \mathcal{T}_{\text{train}} $**: PubMed, CiteSeer, Questions, YelpChi  
  ï¼ˆæ¶µç›–ç¤¾äº¤ç½‘ç»œã€å¼•ç”¨ç½‘ç»œã€ç”µå•†è¯„è®ºç­‰å¤šé¢†åŸŸï¼‰
- **æµ‹è¯•é›† $ \mathcal{T}_{\text{test}} $**: Cora, Flickr, ACM, BlogCatalog, Facebook, Weibo, Reddit, Amazon  
  ï¼ˆå®Œå…¨æœªè§çš„å›¾ç»“æ„ä¸ç‰¹å¾åˆ†å¸ƒï¼‰

> æ‰€æœ‰å›¾å‡å«æ³¨å…¥æˆ–çœŸå®å¼‚å¸¸ï¼ŒéªŒè¯è·¨åŸŸè¿ç§»èƒ½åŠ›ã€‚

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡
- **ä»»åŠ¡è®¾å®š**ï¼šZero-shot å’Œ 10-shot setting
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - **AUPRC**ï¼ˆArea Under Precision-Recall Curveï¼‰â€”â€”é‡ç‚¹å…³æ³¨ï¼Œå› å¼‚å¸¸ç¨€ç–
  - **AUROC**ï¼ˆArea Under ROC Curveï¼‰
- **æŠ¥å‘Šæ–¹å¼**ï¼š5æ¬¡è¿è¡Œå¹³å‡å€¼ Â± æ ‡å‡†å·®

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **Supervised (10-shot)** | BWGNN, GHRN |
| **Unsupervised (zero-shot)** | DOMINANT, SLGAD, TAM, CARE |
| **One-for-all (zero-shot)** | ARC, UNPrompt |
| **Finetuned versions** | DOMINANT, TAM, CARE, ARC, UNPrompt, OWLEYE åœ¨ 10-shot ä¸‹å¾®è°ƒ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆAUPRC å¹³å‡å€¼ï¼‰

| æ–¹æ³• | Average AUPRC (Zero-Shot) | Average AUPRC (10-Shot) |
|------|----------------------------|----------------------------|
| **OWLEYE (Ours)** | **36.17%** | **36.73%** |
| ARC | 30.74% | 31.68% |
| CARE | 28.72% | 30.74% |
| DOMINANT | 24.46% | 26.02% |
| UNPrompt | 12.82% | 13.51% |

> âœ… OWLEYE åœ¨é›¶æ ·æœ¬ä¸‹æ¯”æœ€ä½³åŸºçº¿ **ARC æå‡è¶…è¿‡ 5% AUPRC**

### ä¸åŸºçº¿æ–¹æ³•çš„å…³é”®å¯¹æ¯”å‘ç°
1. **å³ä½¿æä¾›æ ‡ç­¾ï¼ŒOWLEYE ä»ä¼˜äºç›‘ç£æ–¹æ³•**ï¼š
   - åœ¨ 6/8 æµ‹è¯•å›¾ä¸Šè¶…è¶Š BWGNN å’Œ GHRNï¼ˆå‡ä¸º 10-shotï¼‰ã€‚
2. **åœ¨ 10-shot è®¾ç½®ä¸‹è¾¾åˆ° SOTA**ï¼š
   - å¹³å‡ AUPRC è¾¾ 36.73%ï¼Œé¢†å…ˆç¬¬äºŒåè¿‘ 5%ã€‚
3. **æ˜¾è‘—ä¼˜äºå…¶ä»– zero-shot æ–¹æ³•**ï¼š
   - æ¯” UNPrompt æå‡è¶… **23% AUPRC**ï¼Œæ˜¾ç¤ºæ›´å¼ºçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰
åœ¨ 8 ä¸ªæµ‹è¯•å›¾ä¸Šçš„å¹³å‡ AUPRC å¯¹æ¯”å¦‚ä¸‹ï¼š

| æ–¹æ³•å˜ä½“ | AUPRC | ç›¸å¯¹ä¸‹é™ |
|---------|-------|----------|
| **OWLEYE (Full)** | **36.17%** | â€” |
| OWLEYE-Nï¼ˆæ— ç‰¹å¾å½’ä¸€åŒ–ï¼‰ | 35.13% | â†“1.04% |
| OWLEYE-Sï¼ˆæ— ç»“æ„æ¨¡å¼ï¼‰ | 35.06% | â†“1.11% |
| OWLEYE-Tï¼ˆæ ‡å‡†æ³¨æ„åŠ›ï¼‰ | 34.08% | â†“2.09% |

> ğŸ” ç»“æœè¡¨æ˜ä¸‰ä¸ªç»„ä»¶å‡æœ‰æ•ˆï¼Œå°¤å…¶æ˜¯**æˆªæ–­æ³¨æ„åŠ›æœºåˆ¶**è´¡çŒ®æœ€å¤§ã€‚

æ­¤å¤–ï¼š
- **å­—å…¸å¤§å°å½±å“**ï¼šå½“ `nsup` â‰¥ 200 åæ€§èƒ½è¶‹äºé¥±å’Œï¼Œè¯´æ˜å°è€Œç²¾çš„æ¨¡å¼åº“å·²è¶³å¤Ÿã€‚
- **å¤šå›¾æ¨¡å¼å¢ç›Š**ï¼šç›¸æ¯”ä»…ç”¨ç›®æ ‡å›¾è‡ªèº«æ¨¡å¼ï¼Œå¼•å…¥å¤šæºå›¾æ¨¡å¼ä½¿ AUPRC æå‡ **0.32%**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **æœ‰æ•ˆçš„è·¨åŸŸç‰¹å¾å¯¹é½æ˜¯å…³é”®**ï¼šç®€å•çš„ PCA + åŸºäºè·ç¦»çš„å½’ä¸€åŒ–èƒ½æ›´å¥½ä¿æŒå›¾å†…ç»“æ„æ¨¡å¼ï¼ˆå¦‚ Normal-Normal å¯†åº¦ï¼‰ï¼Œä¼˜äº ARC å’Œ UNPromptã€‚
2. âœ… **ç»“æ„æ¨¡å¼ç‹¬ç«‹å»ºæ¨¡è‡³å…³é‡è¦**ï¼šä»…ä¾èµ–å±æ€§å¯èƒ½å¯¼è‡´ä¼ªè£…å¼‚å¸¸ï¼ˆcamouflaged anomaliesï¼‰è¢«è¯¯åˆ¤ï¼›ç»“æ„è¡¨ç¤ºå¯æœ‰æ•ˆåŒºåˆ†è§’è‰²å·®å¼‚ã€‚
3. âœ… **æˆªæ–­æ³¨æ„åŠ›æå‡é²æ£’æ€§**ï¼šé€šè¿‡è¿‡æ»¤ä½ç›¸ä¼¼åº¦èŠ‚ç‚¹ï¼Œé˜²æ­¢å¼‚å¸¸æ±¡æŸ“é‡æ„è¿‡ç¨‹ã€‚
4. âœ… **å­—å…¸æœºåˆ¶æ”¯æŒæŒç»­æ¼”åŒ–**ï¼šæ–°å¢å›¾çš„æ¨¡å¼å¯ç›´æ¥åŠ å…¥å­—å…¸ï¼Œæ— éœ€é‡è®­å³æå‡æ€§èƒ½ï¼ˆè§ Case Study 1ï¼‰ã€‚
5. âœ… **é›¶æ ·æœ¬ä¼˜äºç›‘ç£æ–¹æ³•**ï¼šåœ¨å¤šæ•°å›¾ä¸Šï¼ŒOWLEYEï¼ˆzero-shotï¼‰è¡¨ç°ä¼˜äº BWGNN/GHRNï¼ˆ10-shotï¼‰ï¼Œè¯æ˜å…¶å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚

### æ–¹æ³•çš„å±€é™æ€§
1. **é«˜åº¦é¢†åŸŸåç§»æ—¶æ€§èƒ½ä¸‹é™**ï¼š
   - å½“è®­ç»ƒé›†ä¸­ç¼ºå¤±æ•´ä¸ªé¢†åŸŸï¼ˆå¦‚ç§»é™¤ YelpChi å½±å“ Amazonï¼‰ï¼ŒAUPRC ä¸‹é™çº¦ **4.6%**ï¼Œè™½ä»å¯ç”¨ä½†æç¤ºä¾èµ–è·¨åŸŸå…±æ€§ã€‚
2. **æç«¯å¼‚å¸¸ç‡å¯èƒ½å½±å“æ•ˆæœ**ï¼š
   - å°½ç®¡åœ¨åˆæˆé«˜å¼‚å¸¸ç‡ä¸‹ä»ä¼˜äºåŸºçº¿ï¼Œä½†æç«¯ä¸å¹³è¡¡åœºæ™¯ä»å…·æŒ‘æˆ˜ã€‚
3. **å­—å…¸è§„æ¨¡å—é™äºå†…å­˜ä¸æ•ˆç‡æƒè¡¡**ï¼š
   - è™½ç„¶æ€§èƒ½éšå­—å…¸å¢å¤§è¶‹ç¨³ï¼Œä½†è¿‡å¤§å­—å…¸å¯èƒ½å½±å“æ¨ç†é€Ÿåº¦ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **Multi-modal Graph Foundation Models**ï¼š
   - æ¢ç´¢èåˆæ–‡æœ¬ã€æ—¶é—´åºåˆ—ã€å›¾åƒç­‰å¤šæ¨¡æ€ä¿¡å·çš„ç»Ÿä¸€å›¾åŸºç¡€æ¨¡å‹ã€‚
2. **Test-Time Adaptation for Heavy Domain Shift**ï¼š
   - å¼€å‘æ— éœ€é¢å¤–æ ‡æ³¨çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç­–ç•¥ï¼Œåº”å¯¹è®­ç»ƒæœªè§é¢†åŸŸã€‚
3. **Dynamic Pattern Evolution Modeling**ï¼š
   - å»ºæ¨¡æ­£å¸¸æ¨¡å¼çš„æ—¶é—´æ¼”åŒ–ï¼Œç”¨äºåŠ¨æ€å›¾ä¸­çš„å¼‚å¸¸æ£€æµ‹ã€‚
4. **Explainable Anomaly Reasoning via Attention Maps**ï¼š
   - åˆ©ç”¨æ³¨æ„åŠ›å¯è§†åŒ–è§£é‡Šä¸ºä½•æŸèŠ‚ç‚¹è¢«åˆ¤ä¸ºå¼‚å¸¸ï¼Œå¢å¼ºå¯ä¿¡åº¦ã€‚

---

> ğŸ“Œ **æ€»ç»“**ï¼šOWLEYE æ˜¯é¦–ä¸ªçœŸæ­£å®ç° **zero-shot + continual learning + robust cross-domain alignment** çš„å›¾å¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚å…¶å®éªŒå……åˆ†éªŒè¯äº†ç»“æ„åŒ–çŸ¥è¯†å­˜å‚¨ï¼ˆpattern dictionaryï¼‰ä¸æ³¨æ„åŠ›é©±åŠ¨é‡æ„çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºå¯æ‰©å±•ã€å…æ ‡æ³¨çš„é€šç”¨å›¾å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿå¥ å®šäº†åšå®åŸºç¡€ã€‚

</details>

---

### 5. [Length-Adaptive Interest Network for Balancing Long and Short Sequence Modeling in CTR Prediction](https://arxiv.org/abs/2601.19142)

**Authors**: Zhicheng Zhang, Zhaocheng Du, Jieming Zhu, Jiwei Tang, Fengyuan Lu, Wang Jiaheng, Song-Li Wu, Qianhui Zhu, Jingyu Li, Hai-Tao Zheng, Zhenhua Dong  
**Category**: cs.AI  
**Published**: 2026-01-28  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.19142v1  

#### Abstract
User behavior sequences in modern recommendation systems exhibit significant length heterogeneity, ranging from sparse short-term interactions to rich long-term histories. While longer sequences provide more context, we observe that increasing the maximum input sequence length in existing CTR models...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**Length-Adaptive Interest Network for Balancing Long and Short Sequence Modeling in CTR Prediction**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ç°ä»£æ¨èç³»ç»Ÿä¸­çš„ç”¨æˆ·è¡Œä¸ºåºåˆ—å­˜åœ¨æ˜¾è‘—çš„**é•¿åº¦å¼‚è´¨æ€§**ï¼ˆlength heterogeneityï¼‰ï¼š  
- é•¿åºåˆ—ç”¨æˆ·æ‹¥æœ‰ä¸°å¯Œçš„å†å²äº¤äº’ï¼ˆå¦‚æ•°ç™¾æ¬¡ç‚¹å‡»ï¼‰ï¼Œå¯æä¾›ä¸°å¯Œä¸Šä¸‹æ–‡ï¼›
- çŸ­åºåˆ—ç”¨æˆ·ï¼ˆå¦‚æ–°ç”¨æˆ·æˆ–ä½æ´»è·ƒç”¨æˆ·ï¼‰ä»…æœ‰å°‘é‡è¡Œä¸ºã€‚

ç„¶è€Œï¼Œä½œè€…é€šè¿‡å®è¯åˆ†æå‘ç°ä¸€ä¸ªåç›´è§‰ç°è±¡ï¼š**å½“å¢åŠ æœ€å¤§è¾“å…¥åºåˆ—é•¿åº¦æ—¶ï¼Œé•¿åºåˆ—ç”¨æˆ·çš„æ€§èƒ½æå‡ï¼Œä½†çŸ­åºåˆ—ç”¨æˆ·çš„æ€§èƒ½åè€Œä¸‹é™**ã€‚è¿™ç§â€œ**é•¿åº¦ä¸å¹³è¡¡**â€ï¼ˆlength imbalanceï¼‰å¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒä¸­åå‘äºé«˜é¢‘ã€é•¿åºåˆ—ç”¨æˆ·ï¼Œè€Œç‰ºç‰²äº†å å¤šæ•°çš„çŸ­åºåˆ—ç”¨æˆ·ä½“éªŒã€‚

è¯¥é—®é¢˜æºäºä¸¤ä¸ªå…³é”®å› ç´ ï¼š
- **Attention Polarizationï¼ˆæ³¨æ„åŠ›æåŒ–ï¼‰**ï¼šåœ¨é•¿åºåˆ—ä¸­ï¼Œsoftmax æ³¨æ„åŠ›å€¾å‘äºè¿‡åº¦é›†ä¸­åœ¨å°‘æ•°æ˜¾è‘—è¡Œä¸ºä¸Šï¼›è€Œåœ¨çŸ­åºåˆ—ä¸­ï¼Œè¿™ç§é›†ä¸­ä¼šæ”¾å¤§å™ªå£°ï¼Œå¯¼è‡´å…´è¶£è¡¨ç¤ºä¸ç¨³å®šã€‚
- **Length Signal Deficiencyï¼ˆé•¿åº¦ä¿¡å·ç¼ºå¤±ï¼‰**ï¼šç°æœ‰ CTR æ¨¡å‹å°†è¡Œä¸ºåºåˆ—è§†ä¸ºåŒè´¨äº‹ä»¶é›†åˆï¼Œæœªæ˜¾å¼åˆ©ç”¨â€œåºåˆ—é•¿åº¦â€è¿™ä¸€åæ˜ ç”¨æˆ·çŠ¶æ€çš„é‡è¦å…ˆéªŒä¿¡æ¯ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šLAINï¼ˆLength-Adaptive Interest Networkï¼‰
ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä½œè€…æå‡º **LAIN** â€”â€”ä¸€ç§è½»é‡çº§ã€å³æ’å³ç”¨çš„æ¡†æ¶ï¼Œ**æ˜¾å¼åœ°å°†åºåˆ—é•¿åº¦ä½œä¸ºæ¡ä»¶ä¿¡å·**ï¼Œè‡ªé€‚åº”è°ƒæ•´å»ºæ¨¡ç­–ç•¥ã€‚

#### æ ¸å¿ƒç»„ä»¶ï¼š
1. **Spectral Length Encoder (SLE)**  
   å°†åŸå§‹é•¿åº¦ $ L $ æ˜ å°„ä¸ºè¿ç»­å‘é‡è¡¨ç¤ºï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„å‚…é‡Œå¶åŸºè¿›è¡Œç¼–ç ï¼ˆinspired by NeRFï¼‰ï¼Œé¿å…ç¦»æ•£åŒ–æŸå¤±ï¼Œæ•æ‰å¹³æ»‘çš„é•¿åº¦è¯­ä¹‰ã€‚

2. **Length-Conditioned Prompting (LCP)**  
   åŸºäºé•¿åº¦åµŒå…¥ç”Ÿæˆè½¯æç¤º tokenï¼Œå¹¶å‰ç½®åˆ°é•¿çŸ­åºåˆ—çš„è¡Œä¸ºè¾“å…¥ä¸­ï¼Œå¼•å¯¼æ¨¡å‹æ³¨å…¥å…¨å±€ç”¨æˆ·çŠ¶æ€ä¿¡æ¯ã€‚

3. **Length-Modulated Attention (LMA)**  
   åŠ¨æ€è°ƒèŠ‚æ³¨æ„åŠ›æœºåˆ¶ï¼š
   - **Query-Key Conditioning**ï¼šå°†é•¿åº¦åµŒå…¥æ‹¼æ¥åˆ° query å’Œ key ä¸­ï¼›
   - **Temperature Scaling**ï¼šæ ¹æ®é•¿åº¦åŠ¨æ€è°ƒæ•´ softmax æ¸©åº¦ï¼š
     $$
     T = 1 + \text{sigmoid}(-\beta(L - L_0)) \cdot \gamma
     $$
     å½“ $ L $ è¾ƒå°æ—¶å¢å¤§æ¸©åº¦ä»¥å¹³æ»‘æ³¨æ„åŠ›åˆ†å¸ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼›$ L $ å¤§æ—¶é™ä½æ¸©åº¦å¢å¼ºèšç„¦èƒ½åŠ›ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | LAIN çš„ä¼˜åŠ¿ |
|------|-------------|
| **é€šç”¨æ€§** | å¯æ— ç¼é›†æˆåˆ°ä¸»æµ CTR æ¨¡å‹ï¼ˆDINã€DIENã€SIMã€SDIMã€TWINç­‰ï¼‰ä¸­ï¼Œæ— éœ€æ¶æ„é‡æ„ |
| **å…¬å¹³æ€§** | æ˜¾è‘—æå‡çŸ­åºåˆ—ç”¨æˆ·é¢„æµ‹ç²¾åº¦ï¼Œç¼“è§£å› æ•°æ®åˆ†å¸ƒåæ–œé€ æˆçš„æ¨èåå·® |
| **æ•ˆç‡** | å¼•å…¥å‚æ•° <1.5%ï¼Œæ¨ç†å¼€é”€ä»…å¢åŠ çº¦ 2.3%ï¼Œé€‚åˆå·¥ä¸šéƒ¨ç½² |
| **ç†è®ºåŠ¨æœºå¼º** | ä» attention polarization å’Œ gradient conflict è§’åº¦ç³»ç»Ÿè¯Šæ–­é—®é¢˜ï¼Œè®¾è®¡æœ‰é’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†
åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œçš„å…¬å¼€ CTR æ•°æ®é›†ä¸ŠéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼š

| æ•°æ®é›† | é¢†åŸŸ | ç”¨æˆ·æ•° | ç‰©å“æ•° | å¹³å‡åºåˆ—é•¿åº¦ |
|--------|------|--------|--------|--------------|
| **EBNeRD-small** | æ–°é—»æ¨è | 18,828 | 20,739 | 147 |
| **KuaiVideo** | çŸ­è§†é¢‘æ¨è | 10,000 | 3,239,534 | 278 |
| **MicroVideo1.7M** | å¾®è§†é¢‘æ¨è | 10,951 | 1,704,880 | 148 |

æ‰€æœ‰æ•°æ®é›†å‡å‘ˆç°æ˜æ˜¾çš„**é•¿å°¾åˆ†å¸ƒ**ï¼šçŸ­åºåˆ—ç”¨æˆ·å æ¯”é«˜ä½†è®­ç»ƒæ ·æœ¬å°‘ï¼Œé•¿åºåˆ—ç”¨æˆ·æ ·æœ¬å¯†é›†ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **æœ€å¤§åºåˆ—é•¿åº¦**ï¼šç»Ÿä¸€è®¾ä¸º 1000ï¼Œä»¥æ”¯æŒé•¿åºåˆ—å»ºæ¨¡
- **ç‰¹å¾ç»´åº¦**ï¼šembedding dim = 64
- **ä¼˜åŒ–å™¨**ï¼šAdamï¼Œlr = 0.001ï¼Œæ—©åœåŸºäºéªŒè¯é›† AUC
- **Head ç»“æ„**ï¼šMLP + ReLU + Dropout(0.2)
- **LAIN å‚æ•°é…ç½®**ï¼š
  - Fourier dimension $ d_f = 32 $
  - Prompt token æ•°é‡ $ k = 4 $
  - MLP hidden dim = 512
  - Temperature åˆå§‹å€¼ $ \gamma=0.5, \beta=0.01 $

### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ |
|------|------|
| **AUC** | è¡¡é‡æ•´ä½“æ’åºè´¨é‡ï¼Œè¶Šé«˜è¶Šå¥½ |
| **GAUC** | åˆ†ç”¨æˆ·è®¡ç®— AUC å†å¹³å‡ï¼Œå‡å°‘é«˜æµé‡ç”¨æˆ·ä¸»å¯¼å½±å“ |
| **LogLoss** | è¡¡é‡æ¦‚ç‡æ ¡å‡†ç¨‹åº¦ï¼Œè¶Šä½è¶Šå¥½ |

åŒæ—¶æŒ‰åºåˆ—é•¿åº¦åˆ†ç»„ï¼ˆ<100, 100â€“200, â‰¥200ï¼‰è¿›è¡Œç»†ç²’åº¦åˆ†æã€‚

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
é€‰æ‹©äº”ç±»ä»£è¡¨æ€§ CTR æ¨¡å‹ä½œä¸º backbone è¿›è¡Œå¢å¼ºæ¯”è¾ƒï¼š
1. **DIN**ï¼štarget-aware attention
2. **DIEN**ï¼šGRU + attention å»ºæ¨¡å…´è¶£æ¼”åŒ–
3. **SIM**ï¼šä¸¤é˜¶æ®µæ¶æ„ï¼ˆGSU + ESUï¼‰
4. **SDIM**ï¼šmulti-head attention æ‰©å±• SIM
5. **TWIN**ï¼šåŒå¡”ç»“æ„å»ºæ¨¡è¶…é•¿åºåˆ—

æ‰€æœ‰åŸºçº¿å‡ä½¿ç”¨ FuxiCTR æ¡†æ¶å®ç°ï¼Œä¿è¯ç‰¹å¾å·¥ç¨‹ä¸è¶…å‚ä¸€è‡´ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ æ•´ä½“æ€§èƒ½æå‡ï¼ˆè§ Table 3ï¼‰
LAIN åœ¨æ‰€æœ‰ backbone å’Œæ•°æ®é›†ä¸Šå‡å¸¦æ¥ç¨³å®šå¢ç›Šï¼š

| æŒ‡æ ‡ | æœ€å¤§ç›¸å¯¹æå‡ |
|------|------------|
| **AUC** | **+1.15%**ï¼ˆTWIN on MicroVideo1.7Mï¼‰ |
| **LogLoss** | **-2.25%**ï¼ˆDIEN on EBNeRD-smallï¼‰ |
| **GAUC** | **+1.19%**ï¼ˆTWIN on EBNeRD-smallï¼‰ |

> âœ… ç‰¹åˆ«æ˜¯åœ¨ **TWIN** ä¸Šæ•ˆæœæœ€æ˜¾è‘—ï¼Œè¯´æ˜ LAIN å¯¹ä¸¤é˜¶æ®µ attention æ¶æ„ä¸­çš„é•¿åº¦å¤±è¡¡é—®é¢˜å°¤ä¸ºæœ‰æ•ˆã€‚

### ğŸ” é•¿çŸ­åºåˆ—ç”¨æˆ·æ€§èƒ½å¯¹æ¯”ï¼ˆè§ Table 4ï¼‰
å¯¹ TWIN åœ¨ MicroVideo1.7M ä¸Šè¿›ä¸€æ­¥åˆ†ç»„æµ‹è¯•æ˜¾ç¤ºï¼š

| åºåˆ—é•¿åº¦ | AUC æå‡ | LogLoss ä¸‹é™ |
|---------|----------|-------------|
| **<100ï¼ˆçŸ­åºåˆ—ï¼‰** | **+1.08%** | **-2.17%** |
| **100â€“200ï¼ˆä¸­åºåˆ—ï¼‰** | +0.50% | -1.26% |
| **â‰¥200ï¼ˆé•¿åºåˆ—ï¼‰** | +1.58% | -1.74% |

> ğŸ’¡ **å…³é”®å‘ç°**ï¼šLAIN ä¸ä»…æ²¡æœ‰ç‰ºç‰²é•¿åºåˆ—æ€§èƒ½ï¼Œåè€ŒåŒæ­¥æå‡äº†ä¸¤ç±»ç”¨æˆ·çš„é¢„æµ‹å‡†ç¡®æ€§ï¼ŒçœŸæ­£å®ç°äº†â€œå¹³è¡¡å»ºæ¨¡â€ã€‚

### ğŸ§ª æ¶ˆèå®éªŒï¼ˆAblation Studyï¼Œè§ Table 6ï¼‰
åœ¨ TWIN + MicroVideo1.7M ä¸Šç§»é™¤å„æ¨¡å—çš„ç»“æœè¡¨æ˜ï¼š

| å˜ä½“ | AUC | Î”AUC |
|------|-----|-------|
| **LAIN (Full)** | 0.7233 | â€” |
| w/o LCPï¼ˆæ— æç¤ºï¼‰ | 0.7228 | -0.05% |
| w/o Query-Key Conditioning | 0.7195 | -0.38% |
| w/o Temperature Scaling | 0.7212 | -0.21% |
| w/o LMAï¼ˆå®Œæ•´ç§»é™¤ï¼‰ | 0.7189 | **-0.44%** |
| w/o Short-term Branch | 0.7226 | -0.07% |

> ğŸ” **ç»“è®º**ï¼š
- **LMA è´¡çŒ®æœ€å¤§**ï¼Œå°¤å…¶æ˜¯ query-key conditioning ä¸ temperature scaling ååŒä½œç”¨æ˜æ˜¾ï¼›
- **LCP æœ‰åŠ©äºå…¨å±€çŠ¶æ€æ³¨å…¥**ï¼›
- å³ä½¿åœ¨çŸ­åºåˆ—åˆ†æ”¯ä¹Ÿéœ€åº”ç”¨ LAINï¼Œä½“ç°å…¶æ™®é€‚ä»·å€¼ã€‚

### ğŸ“‰ æ³¨æ„åŠ›æåŒ–ç¼“è§£ï¼ˆè§ Table 5ï¼‰
é€šè¿‡ Gini ç³»æ•°è¡¡é‡ attention åˆ†å¸ƒé›†ä¸­åº¦ï¼š

| æ¨¡å‹é…ç½® | Giniï¼ˆçŸ­åºåˆ—ï¼‰ | Giniï¼ˆé•¿åºåˆ—ï¼‰ |
|----------|----------------|----------------|
| Baseline (L=1000) | 0.346 | 0.352 |
| **LAIN** | **0.318** | **0.321** |

> âœ… LAIN æ˜¾è‘—é™ä½äº†æ‰€æœ‰é•¿åº¦ç»„çš„æ³¨æ„åŠ›æåŒ–ç¨‹åº¦ï¼ˆGini â†“ï¼‰ï¼Œå°¤å…¶æ”¹å–„äº†çŸ­åºåˆ—ä¸‹çš„ over-concentration é—®é¢˜ã€‚

### ğŸ”„ è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æï¼ˆè§ Figure 4ï¼‰
åœ¨ä¸åŒ Fourier dimensionã€hidden sizeã€prompt æ•°é‡ç»„åˆä¸‹ï¼ŒAUC å‡ç¨³å®šä¼˜äº baselineï¼ˆ0.7158 â†’ 0.7216~0.7248ï¼‰ï¼Œè¯æ˜æ–¹æ³•é²æ£’æ€§å¼ºã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **åºåˆ—é•¿åº¦ä¸ä»…æ˜¯ç»Ÿè®¡é‡ï¼Œæ›´æ˜¯é‡è¦ç”¨æˆ·çŠ¶æ€ä¿¡å·**ï¼šåº”è¢«æ˜¾å¼å»ºæ¨¡ç”¨äºæŒ‡å¯¼ interest extractionã€‚
2. **å½“å‰ CTR æ¨¡å‹å­˜åœ¨â€œé•¿åº¦è¯±å¯¼åå·®â€**ï¼ˆlength-induced biasï¼‰ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­éšå¼åå¥½é•¿åºåˆ—æ¨¡å¼ï¼ŒæŸå®³çŸ­åºåˆ—æ³›åŒ–ã€‚
3. **LAIN æˆåŠŸè§£è€¦äº†é•¿çŸ­åºåˆ—çš„ä¼˜åŒ–å†²çª**ï¼šé€šè¿‡æ¡ä»¶åŒ– prompt å’Œ attention è°ƒåˆ¶ï¼Œå®ç°åŠ¨æ€é€‚åº”ã€‚
4. **æ–¹æ³•å…·æœ‰é«˜åº¦é€šç”¨æ€§å’Œå®ç”¨æ€§**ï¼šé€‚ç”¨äºå¤šç§ backboneï¼Œä¸”å‡ ä¹æ— é¢å¤–éƒ¨ç½²æˆæœ¬ã€‚

### âš ï¸ å±€é™æ€§
- å½“å‰ä»…è€ƒè™‘**å•ä¸€é•¿åº¦ä¿¡å·**ï¼Œæœªèåˆå…¶ä»–ç”¨æˆ·å…ƒä¿¡æ¯ï¼ˆå¦‚æ³¨å†Œæ—¶é—´ã€æ´»è·ƒå‘¨æœŸç­‰ï¼‰ï¼›
- Prompt è®¾è®¡ä¸ºé™æ€æ•°é‡ï¼ˆå¦‚ k=4ï¼‰ï¼Œæœªæ¢ç´¢é•¿åº¦è‡ªé€‚åº” prompt é•¿åº¦ï¼›
- åœ¨æç«¯ç¨€ç–åœºæ™¯ï¼ˆå¦‚é•¿åº¦ â‰¤ 5ï¼‰çš„æ•ˆæœä»å¾…æ·±å…¥ç ”ç©¶ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ¢ç´¢å¤šç»´åº¦ç”¨æˆ·çŠ¶æ€å»ºæ¨¡ï¼ˆç»“åˆ length + activity level + noveltyï¼‰ï¼›
- å°† LAIN æ€è·¯æ‰©å±•è‡³ item-side åºåˆ—å»ºæ¨¡ï¼›
- åœ¨ online learning åœºæ™¯ä¸­åŠ¨æ€æ›´æ–° length-aware å‚æ•°ï¼›
- æ¢ç´¢æ›´é«˜æ•ˆçš„ prompt compression æŠ€æœ¯ä»¥åº”å¯¹å¤§è§„æ¨¡æœåŠ¡å»¶è¿Ÿè¦æ±‚ã€‚

---

## âœ… æ€»ç»“
æœ¬æ–‡æå‡ºçš„ **LAIN** æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§è§£å†³ CTR é¢„æµ‹ä¸­â€œé•¿çŸ­åºåˆ—å»ºæ¨¡å¤±è¡¡â€é—®é¢˜çš„å·¥ä½œã€‚å®ƒä¸ä»…æ­ç¤ºäº† attention polarization å’Œ length signal deficiency ä¸¤å¤§æœºåˆ¶ï¼Œè¿˜æä¾›äº†è½»é‡ã€é«˜æ•ˆã€å¯æ’æ‹”çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶å¯¹æ•´ä½“æ€§èƒ½åŠçŸ­åºåˆ—ç”¨æˆ·çš„æ˜¾è‘—æå‡ï¼Œä¸ºæ„å»ºæ›´å…¬å¹³ã€æ›´å…·åŒ…å®¹æ€§çš„æ¨èç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 6. [EPAS: Efficient Training with Progressive Activation Sharing](https://arxiv.org/abs/2601.19089)

**Authors**: Rezaul Karim, Maryam Dialameh, Yang Liu, Boxing Chen, Walid Ahmed  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.19089v1  

#### Abstract
We present a novel method for Efficient training with Progressive Activation Sharing (EPAS). This method bridges progressive training paradigm with the phenomenon of redundant QK (or KV ) activations across deeper layers of transformers. EPAS gradually grows a sharing region during training by switc...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šEPAS: Efficient Training with Progressive Activation Sharing

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰å¤§æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå‡é¢ä¸´é«˜è®¡ç®—æˆæœ¬ä¸å†…å­˜å¼€é”€é—®é¢˜ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶åˆ©ç”¨ **cross-layer activation sharing**ï¼ˆå¦‚å…±äº« QK æˆ– KVï¼‰æ¥æå‡æ•ˆç‡ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸å­˜åœ¨ä»¥ä¸‹å±€é™ï¼š
- å¤šæ•°ä»…åœ¨æ¨ç†é˜¶æ®µåº”ç”¨æ¿€æ´»å…±äº«ï¼Œæ— æ³•ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ï¼›
- ç›´æ¥ä»å¤´è®­ç»ƒæ¿€æ´»å…±äº«æ¨¡å‹æ˜“å¯¼è‡´æ˜¾è‘—çš„ç²¾åº¦ä¸‹é™ï¼›
- ä¾èµ–å¤æ‚çš„çŸ¥è¯†è’¸é¦ï¼ˆknowledge distillationï¼‰æˆ–å¤šè½®è®­ç»ƒæµç¨‹ï¼Œå¢åŠ å®ç°å¤æ‚åº¦ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šEPASï¼ˆEfficient training with Progressive Activation Sharingï¼‰
EPAS æ˜¯ä¸€ç§å°† **progressive training** ä¸ **activation sharing** ç»“åˆçš„ç»Ÿä¸€é«˜æ•ˆè®­ç»ƒæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å°†æ·±å±‚ decoder å±‚åˆ‡æ¢è‡³ **activation sharing mode**ï¼Œå½¢æˆä¸€ä¸ªä»æ·±åˆ°æµ…é€æ¸æ‰©å±•çš„â€œå…±äº«åŒºåŸŸâ€ï¼›
- ä½¿ç”¨ **switchable decoder block** å®ç°çƒ­åˆ‡æ¢ï¼ˆhot-switchingï¼‰ï¼Œå…è®¸æ¨¡å‹åŠ¨æ€é€‰æ‹©æ˜¯å¦å¤ç”¨å‰ä¸€å±‚çš„ QK æˆ– KV æ¿€æ´»ï¼›
- æ”¯æŒåœ¨å•æ¬¡ç«¯åˆ°ç«¯è®­ç»ƒä¸­ç”Ÿæˆä¸€ç³»åˆ—å¯å˜é•¿åº¦å…±äº«é…ç½®çš„å­æ¨¡å‹ï¼ˆmany-in-one modelï¼‰ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | EPAS | ä¼ ç»Ÿæ–¹æ³• |
|------|------|----------|
| **è®­ç»ƒæ•ˆç‡** | æ˜¾è‘—æå‡è®­ç»ƒååé‡ï¼ˆup to 11.1%â†‘ï¼‰ | å¤šæ•°ä¸ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ |
| **æ¨ç†æ•ˆç‡** | æ¨ç†ååæœ€é«˜æå‡è¾¾ 29% | ä»…åœ¨æ¨ç†æ—¶ç”Ÿæ•ˆ |
| **ç²¾åº¦ä¿æŒ** | å‡ ä¹æ— æŸï¼Œloss æ›²çº¿ä¸ baseline å¯¹é½ | è’¸é¦å¤±è´¥åˆ™ç²¾åº¦å¤§å¹…ä¸‹é™ |
| **éƒ¨ç½²çµæ´»æ€§** | å•ä¸€æ¨¡å‹æ”¯æŒå¤šç§å…±äº«å±‚æ•°é…ç½® | å›ºå®šæ¶æ„ |
| **è¿ç§»èƒ½åŠ›** | å¯ç”¨äº continual pretraining å°†é¢„è®­ç»ƒæ¨¡å‹è½¬ä¸ºé«˜æ•ˆç»“æ„ | éœ€é‡æ–°è®­ç»ƒæˆ–å¤æ‚è’¸é¦ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- ä¸»è¦ä½¿ç”¨å¼€æºæ•°æ®é›† **SlimPajama-627B** çš„å­é›†è¿›è¡Œ pretraining å’Œ continual pretrainingï¼›
- å®éªŒèšç„¦äºæ•ˆç‡åˆ†æè€Œéå®Œæ•´æ”¶æ•›ï¼Œå› æ­¤é‡‡ç”¨å°è§„æ¨¡ token æ•°é‡ï¼ˆä¾‹å¦‚ 1B~4B tokensï¼‰ä»¥åŠ é€ŸéªŒè¯ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
| é¡¹ç›® | è®¾ç½®è¯´æ˜ |
|------|---------|
| **æ¨¡å‹èŒƒå›´** | LLaMA æ¶æ„ç³»åˆ—ï¼š125M, 1.1B, 3B, 7B å‚æ•°æ¨¡å‹ï¼›ä¸»å®éªŒåŸºäº TinyLLaMA-1.1B |
| **ç¡¬ä»¶å¹³å°** | å¤šè®¾å¤‡å¯¹æ¯”ï¼šNvidia V100 GPUã€Ascend 910A NPUã€Ascend 910B NPU |
| **è®­ç»ƒæ–¹å¼** | åˆ†å¸ƒå¼è®­ç»ƒï¼ˆ8-deviceï¼‰ï¼Œbatch size per device = 8ï¼Œgradient accumulation steps = 16 |
| **å…±äº«ç­–ç•¥** | QK sharingï¼ˆå…¼å®¹ Flash-Attentionï¼‰ï¼Œå…±äº«åŒºåŸŸä»æœ€æ·±å±‚å¼€å§‹é€å±‚å‘å‰å¢é•¿ |
| **è°ƒåº¦æœºåˆ¶** | æ¯éš” I æ­¥åˆ‡æ¢ B å±‚è¿›å…¥ sharing modeï¼Œæœ€ç»ˆç›®æ ‡ä¸ºæœ€å¤š S å±‚å‚ä¸å…±äº«ï¼ˆå¦‚ 25% æˆ– 50%ï¼‰ |

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
| ç±»å‹ | æŒ‡æ ‡ |
|------|------|
| **è®­ç»ƒæ•ˆç‡** | Training Throughput (tokens/sec)ã€Total Training Timeã€Loss Curve æ”¶æ•›é€Ÿåº¦ |
| **æ¨ç†æ•ˆç‡** | Inference Throughput (tokens/sec) |
| **æ¨¡å‹æ€§èƒ½** | Validation Lossã€LM Benchmark Accuracyï¼ˆPIQA, BoolQ, ARC, OBQA ç­‰ï¼‰ |
| **ç†è®ºèŠ‚çœ** | FLOPs Reductionï¼ˆç†è®ºè®¡ç®—é‡é™ä½ç™¾åˆ†æ¯”ï¼‰ |

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Baseline**: åŸå§‹ LLaMA/TinyLLaMA æ¨¡å‹ï¼ˆæ—  activation sharingï¼‰ï¼›
- **ShareAttn / Beyond-KV-Cache [10]**: æ¨ç†é˜¶æ®µåº”ç”¨ activation sharing çš„ä»£è¡¨æ–¹æ³•ï¼›
- **EchoAtt [19]**: åŸºäº distillation çš„ QK sharing æ–¹æ³•ï¼›
- **YOLO [20]**: KV sharing æ–¹æ³•ï¼›
- **æ¶ˆèå®éªŒè®¾è®¡**ï¼šæ¯”è¾ƒä¸åŒå…±äº«ç­–ç•¥ï¼ˆå•ä¸€å¤§å— vs å¤šä¸ªå°å—ï¼‰ã€æ˜¯å¦åŒ…å«æœ€åä¸€å±‚ç­‰ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### ï¼ˆ1ï¼‰è®­ç»ƒæ•ˆç‡æå‡
| æ¨¡å‹ | FLOPs é™ä½ | è®­ç»ƒååæå‡ï¼ˆV100ï¼‰ | æ€»è®­ç»ƒæ—¶é—´å‡å°‘ |
|------|------------|------------------------|----------------|
| 125M | 4.9% | +10.8% | 54min â†’ 51min |
| 1.1B | 4.3% | **+11.1%** | 3h7min â†’ 2h56min |
| 3B  | 7.3% | +10.9% | 6h3min â†’ 5h39min |
| 7B  | 8.1% | +8.6%  | 11h9min â†’ 10h25min |

> âœ… æ‰€æœ‰æ¨¡å‹æœ€ç»ˆ validation loss å·®å¼‚ < 0.05ï¼Œè¡¨æ˜ç²¾åº¦å‡ ä¹æ— æŸã€‚

#### ï¼ˆ2ï¼‰è·¨ç¡¬ä»¶ä¸€è‡´æ€§
| è®¾å¤‡ | ååæå‡å¹…åº¦ |
|------|---------------|
| V100 GPU | +11.1% |
| 910A NPU | +9.57% |
| 910B NPU | +7.3% |

> ğŸ’¡ è¡¨æ˜ EPAS çš„æ•ˆç‡å¢ç›Šå…·æœ‰è‰¯å¥½çš„ç¡¬ä»¶æ³›åŒ–æ€§ã€‚

#### ï¼ˆ3ï¼‰æ¨ç†æ•ˆç‡æå‡ï¼ˆQK sharingï¼‰
| æ¨¡å‹ | Baseline (tok/s) | 25% Sharing | 50% Sharing |
|------|------------------|-----------|-----------|
| 125M | 63.8 | 78.3 (**+22.7%**) | 82.4 (**+29.2%**) |
| 1.1B | 39.1 | 42.9 (+9.7%) | 44.8 (+14.6%) |
| 3B   | 38.1 | 40.9 (+7.3%) | 43.2 (+13.4%) |
| 7B   | 24.46 | 25.27 (+3.3%) | 26.03 (+6.4%) |

> âœ… æœ€é«˜æ¨ç†ååæå‡è¾¾ **29%**ï¼Œå°¤å…¶åœ¨å°æ¨¡å‹ä¸Šæ›´æ˜¾è‘—ã€‚

#### ï¼ˆ4ï¼‰è¯­è¨€æ¨¡å‹ä»»åŠ¡è¡¨ç°ï¼ˆTinyLLaMA continual pretrainingï¼‰
| é…ç½® | Average Accuracy |
|------|------------------|
| w/o EPASï¼ˆno sharingï¼‰ | 56.70 |
| w/ EPASï¼ˆno sharingï¼‰ | **57.07** |
| w/o EPASï¼ˆ5/22 sharingï¼‰ | 44.99 |
| w/ EPASï¼ˆ5/22 sharingï¼‰ | **54.47**ï¼ˆâ†‘ ~10%ï¼‰ |

> âœ… EPAS è®­ç»ƒçš„æ¨¡å‹åœ¨ activation sharing æ¨¡å¼ä¸‹ä»èƒ½ä¿æŒé«˜å‡†ç¡®ç‡ï¼Œè€Œç›´æ¥åº”ç”¨ sharing åˆ°æ™®é€šæ¨¡å‹ä¼šå¯¼è‡´ä¸¥é‡æ€§èƒ½é€€åŒ–ã€‚

### ğŸ” æ¶ˆèå®éªŒç»“æœ
| å®éªŒé¡¹ | å‘ç° |
|-------|------|
| **å•ä¸€å¤§å— vs å¤šä¸ªå°å—å…±äº«** | å•ä¸€å¤§å—ï¼ˆsingle large blockï¼‰ä¼˜äºå¤šä¸ªå°å—ï¼ˆmultiple small blocksï¼‰ï¼Œç®€åŒ–ç»“æ„ä¸”æ€§èƒ½æ›´é«˜ï¼ˆè§ Table 8ï¼‰ |
| **æ˜¯å¦åŒ…å«æœ€åä¸€å±‚** | åŒ…å«æœ€åä¸€å±‚ç•¥ä¼˜ï¼ˆ+~0.5% avgï¼‰ï¼Œæ•…æ¨èçº³å…¥å…±äº«åŒº |
| **å…±äº«ç±»å‹** | QK sharing æ›´åˆ©äºè®¡ç®—å‡è´Ÿï¼›KV sharing æ›´çœå†…å­˜ï¼Œä½†æœ¬æ–‡ä»¥ QK ä¸ºä¸»ï¼ˆå…¼å®¹ Flash-Attentionï¼‰ |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **å†—ä½™å­˜åœ¨äºæ·±å±‚ attention æ¿€æ´»ä¸­**ï¼Œå¯é€šè¿‡ progressive activation sharing å®‰å…¨åˆ©ç”¨è€Œä¸ç‰ºç‰²ç²¾åº¦ï¼›
2. **æ¸è¿›å¼è®­ç»ƒç­–ç•¥è‡³å…³é‡è¦**ï¼šç›¸æ¯”ä¸€æ¬¡æ€§å¯ç”¨æ‰€æœ‰å…±äº«å±‚ï¼Œé€æ­¥æ‰©å±•å…±äº«åŒºåŸŸå¯æ˜¾è‘—æé«˜ç¨³å®šæ€§ä¸æœ€ç»ˆæ€§èƒ½ï¼›
3. **EPAS å®ç°è®­ç»ƒä¸æ¨ç†åŒä¼˜**ï¼šæ—¢åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆthroughput â†‘11.1%ï¼‰ï¼Œåˆæå‡æ¨ç†æ•ˆç‡ï¼ˆâ†‘29%ï¼‰ï¼ŒåŒæ—¶ä¿æŒ loss å’Œ accuracy åŸºæœ¬ä¸å˜ï¼›
4. **æ”¯æŒçµæ´»éƒ¨ç½²**ï¼šå•ä¸€ EPAS è®­ç»ƒæ¨¡å‹å¯åœ¨æ¨ç†æ—¶é€‰æ‹©ä¸åŒæ•°é‡çš„å…±äº«å±‚ï¼Œé€‚åº”å¤šæ ·åŒ–çš„ compute budgetï¼›
5. **å¯ç”¨äº continual pretraining**ï¼šæ— éœ€ distillationï¼Œå³å¯å°†å·²æœ‰é¢„è®­ç»ƒæ¨¡å‹é«˜æ•ˆè½¬åŒ–ä¸º activation-sharing æ¶æ„ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- å½“å‰ä¸»è¦éªŒè¯åŸºäº **LLaMA/TinyLLaMA** æ¶æ„ï¼Œå°šæœªå¹¿æ³›æµ‹è¯•å…¶ä»–ç»“æ„ï¼ˆå¦‚ encoder-decoderï¼‰ï¼›
- å…±äº«æœºåˆ¶ç›®å‰é›†ä¸­åœ¨ decoder å±‚ï¼Œæœªæ¢ç´¢ encoder å†…éƒ¨æˆ–è·¨æ¨¡æ€åœºæ™¯ï¼›
- è™½ç„¶æ”¯æŒå¤šç§ activation sharingï¼ˆQK/KVï¼‰ï¼Œä½†å®éªŒé›†ä¸­äº QKï¼Œå¯¹ KV çš„ä¼˜åŒ–æ½œåŠ›æœ‰å¾…æ·±å…¥æŒ–æ˜ï¼›
- å¯¹ extremely long context çš„å½±å“å°šæœªç³»ç»Ÿè¯„ä¼°ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³ vision transformer å’Œ speech models ä¸­çš„ activation sharingï¼›
- æ¢ç´¢ post-training adaptation åœºæ™¯ä¸‹çš„ EPAS åº”ç”¨ï¼›
- åŠ¨æ€è°ƒæ•´å…±äº«åŒºåŸŸå¤§å°ï¼ˆadaptive schedulingï¼‰ä»¥å“åº”è¾“å…¥å¤æ‚åº¦å˜åŒ–ï¼›
- ç»“åˆé‡åŒ–ã€ç¨€ç–åŒ–ç­‰æŠ€æœ¯æ„å»ºæ›´å…¨é¢çš„é«˜æ•ˆè®­ç»ƒèŒƒå¼ï¼›
- ç ”ç©¶ activation sharing å¯¹æ¨¡å‹é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„å½±å“ã€‚

---

## æ€»ç»“
EPAS æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å®ç”¨çš„ **progressive activation sharing** æ¡†æ¶ï¼ŒæˆåŠŸæ‰“é€šäº†é«˜æ•ˆè®­ç»ƒä¸é«˜æ•ˆæ¨ç†ä¹‹é—´çš„é¸¿æ²Ÿã€‚é€šè¿‡åœ¨è®­ç»ƒåˆæœŸä¿ç•™å®Œæ•´è®¡ç®—è·¯å¾„ï¼Œå¹¶é€æ­¥å¼•å…¥ activation sharingï¼ŒEPAS ä¸ä»…æå‡äº†è®­ç»ƒååå’Œæ¨ç†é€Ÿåº¦ï¼Œè¿˜é¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ç²¾åº¦å´©æºƒé—®é¢˜ã€‚å…¶å®éªŒå……åˆ†éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹å°ºåº¦å’Œç¡¬ä»¶å¹³å°ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºâ€œmany-in-oneâ€çš„é«˜æ•ˆå¤§æ¨¡å‹æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯è·¯å¾„ã€‚

</details>

---

### 7. [GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery](https://arxiv.org/abs/2601.19149)

**Authors**: Jingjie Ning, Xiangzhen Shen, Li Hou, Shiyi Shen, Jiahao Yang, Junrui Li, Hong Shan, Sanan Wu, Sihan Gao, Huaqiang Eric Xu, Xinheng He  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2601.19149v1  

#### Abstract
G protein-coupled receptors (GPCRs) govern diverse physiological processes and are central to modern pharmacology. Yet discovering GPCR modulators remains challenging because receptor activation often arises from complex allosteric effects rather than direct binding affinity, and conventional assays...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
Gè›‹ç™½å¶è”å—ä½“ï¼ˆ**GPCRs**ï¼‰æ˜¯ç°ä»£è¯ç†å­¦ä¸­æœ€é‡è¦çš„è¯ç‰©é¶ç‚¹ä¹‹ä¸€ï¼Œçº¦36%çš„FDAæ‰¹å‡†è¯ç‰©ä½œç”¨äºGPCRsã€‚ç„¶è€Œï¼Œå‘ç°æœ‰æ•ˆçš„GPCRè°ƒèŠ‚å‰‚ï¼ˆmodulatorï¼‰ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› åŒ…æ‹¬ï¼š
- GPCRså…·æœ‰é«˜åº¦åŠ¨æ€çš„æ„è±¡å˜åŒ–å’Œå˜æ„æ•ˆåº”ï¼ˆallosteric effectsï¼‰ï¼Œå¯¼è‡´**é…ä½“ç»“åˆä¸åŠŸèƒ½æ¿€æ´»è§£è€¦**ï¼›
- ä¼ ç»Ÿè®¡ç®—ç­›é€‰æ–¹æ³•ï¼ˆå¦‚åŸºäºç»“æ„çš„å¯¹æ¥ï¼‰éš¾ä»¥æ•æ‰è¿™äº›å¤æ‚çš„åŠ¨æ€å…³ç³»ï¼›
- å®éªŒéªŒè¯æˆæœ¬é«˜ã€é€šé‡ä½ï¼Œä¸”ç°æœ‰AIæ¨¡å‹å¤šèšç„¦äºé€šç”¨çš„**Drug-Target Interaction (DTI)** é¢„æµ‹ï¼Œç¼ºä¹å¯¹GPCRç‰¹å¼‚æ€§æœºåˆ¶çš„æ•æ„Ÿæ€§ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡ºäº† **GPCR-Filter** â€”â€”ä¸€ä¸ªä¸“ä¸ºGPCRè°ƒèŠ‚å‰‚å‘ç°è®¾è®¡çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

1. **æ„å»ºé«˜è´¨é‡ã€å¤§è§„æ¨¡çš„GPCR-ligandæ•°æ®é›†**  
   æ•´åˆæ¥è‡ª **GPCRdb** å’Œ **GtoPdb** çš„å®éªŒéªŒè¯æ•°æ®ï¼Œå…±æ”¶é›† **91,396ä¸ª**äººç±»GPCR-é…ä½“ç›¸äº’ä½œç”¨å¯¹ï¼Œæ¶µç›–527ä¸ªç‹¬ç‰¹GPCRså’Œ72,177ä¸ªä¸åŒé…ä½“ï¼Œæ ‡å‡†åŒ–ä¸ºæ°¨åŸºé…¸åºåˆ—å’ŒSMILESæ ¼å¼ã€‚

2. **èåˆè›‹ç™½è´¨è¯­è¨€æ¨¡å‹ä¸å›¾ç¥ç»ç½‘ç»œçš„åŒæ¨¡æ€æ¶æ„**  
   - ä½¿ç”¨é¢„è®­ç»ƒçš„ **ESM-3 è›‹ç™½è´¨è¯­è¨€æ¨¡å‹** ç¼–ç GPCRåºåˆ—ï¼Œè·å¾—æ®‹åŸºçº§ï¼ˆper-residueï¼‰è¡¨ç¤ºï¼›
   - ä½¿ç”¨ **Graph Neural Network (GNN)** ç¼–ç é…ä½“çš„åˆ†å­å›¾ç»“æ„ï¼›
   - å¼•å…¥ **æ³¨æ„åŠ›æœºåˆ¶ï¼ˆcross-attentionï¼‰** å®ç°å—ä½“-é…ä½“ç‰¹å¾èåˆï¼Œå­¦ä¹ åŠŸèƒ½æ€§äº¤äº’æ¨¡å¼ï¼Œè€Œä¸ä»…ä»…æ˜¯ç»“åˆäº²å’ŒåŠ›ã€‚

3. **ä¸“æ³¨äºâ€œåŠŸèƒ½è°ƒèŠ‚â€è€Œéâ€œç®€å•ç»“åˆâ€çš„é¢„æµ‹èƒ½åŠ›**  
   æ¨¡å‹ç›®æ ‡æ˜¯è¯†åˆ«èƒ½å¼•å‘ä¸‹æ¸¸ä¿¡å·çš„åŠŸèƒ½æ€§è°ƒèŠ‚å‰‚ï¼ˆå¦‚æ¿€åŠ¨å‰‚ï¼‰ï¼Œè€Œéä»…é¢„æµ‹ç‰©ç†ç»“åˆï¼Œæ›´è´´è¿‘çœŸå®è¯ç‰©å‘ç°éœ€æ±‚ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | GPCR-Filter | ç°æœ‰DTIæ¨¡å‹ï¼ˆå¦‚TransformerCPI2.0, ConPLexï¼‰ |
|------|-------------|----------------------------------------------|
| **ä»»åŠ¡é’ˆå¯¹æ€§** | ä¸“ä¸ºGPCRè®¾è®¡ï¼Œè€ƒè™‘å…¶åŠŸèƒ½å¤æ‚æ€§ | é€šç”¨DTIæ¨¡å‹ï¼Œæœªé’ˆå¯¹GPCRä¼˜åŒ– |
| **æ³›åŒ–èƒ½åŠ›** | åœ¨è·¨é¶ç‚¹ï¼ˆinter-targetï¼‰åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ | æ³›åŒ–èƒ½åŠ›å¼±ï¼Œå°¤å…¶åœ¨æ–°é¶ç‚¹ä¸Šæ¥è¿‘éšæœº |
| **å¯è§£é‡Šæ€§** | æ³¨æ„åŠ›æƒé‡é›†ä¸­åœ¨å·²çŸ¥ç»“åˆå£è¢‹æ®‹åŸº | ç¼ºä¹æ˜ç¡®ç”Ÿç‰©å­¦è§£é‡Š |
| **è¾“å…¥ä¾èµ–** | ä»…éœ€åºåˆ— + SMILESï¼Œæ— éœ€ä¸‰ç»´ç»“æ„ | éƒ¨åˆ†ä¾èµ–ç»“æ„ä¿¡æ¯æˆ–æ— æ³•å¤„ç†æ–°é¶ç‚¹ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š æ•°æ®é›†
- **æ¥æº**ï¼šGPCRdb å’Œ GtoPdb
- **è§„æ¨¡**ï¼š91,396ä¸ªå®éªŒéªŒè¯çš„GPCR-ligandå¯¹
- **GPCRs**ï¼š527ä¸ªï¼ˆUniProtæ ‡å‡†åºåˆ—ï¼‰
- **Ligands**ï¼š72,177ä¸ªï¼ˆæ ‡å‡†åŒ–SMILESï¼‰
- **è´Ÿæ ·æœ¬æ„é€ **ï¼šé€šè¿‡æšä¸¾æ‰€æœ‰å¯èƒ½ç»„åˆå¹¶å»é™¤å·²çŸ¥é˜³æ€§ï¼Œè¿›è¡Œ1:1è´Ÿé‡‡æ ·ä»¥å¹³è¡¡ç±»åˆ«

### âš™ï¸ å®éªŒè®¾ç½®
é‡‡ç”¨ä¸‰ç§è¯„ä¼°åè®®æ¥å…¨é¢æµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œéš¾åº¦é€’å¢ï¼š

| è®¾ç½® | æè¿° | ç›®æ ‡ |
|------|------|------|
| **Random Split** | æ‰€æœ‰æ ·æœ¬æŒ‰80/10/10éšæœºåˆ’åˆ† | æµ‹è¯•åˆ†å¸ƒå†…æ€§èƒ½ï¼ˆin-distributionï¼‰ |
| **Intra-target Split** | æ¯ä¸ªGPCRå‡ºç°åœ¨è®­ç»ƒå’Œæµ‹è¯•é›†ä¸­ï¼Œä½†é…ä½“äº’æ–¥ | æµ‹è¯•å¯¹åŒä¸€é¶ç‚¹çš„æ–°é…ä½“æ³›åŒ–èƒ½åŠ› |
| **Inter-target Split** | è®­ç»ƒä¸æµ‹è¯•çš„GPCRå®Œå…¨ä¸é‡å ï¼ˆ9:1åˆ’åˆ†ï¼‰ | æµ‹è¯•å¯¹å…¨æ–°é¶ç‚¹çš„è·¨é¶ç‚¹æ³›åŒ–èƒ½åŠ› |

### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡
- **AUC**ï¼ˆROCæ›²çº¿ä¸‹é¢ç§¯ï¼‰
- **AP**ï¼ˆAverage Precisionï¼‰
- **ACC**ï¼ˆAccuracyï¼‰
- **Precision**
> æ‰€æœ‰æŒ‡æ ‡è¶Šé«˜è¶Šå¥½ï¼›AUCå’ŒAPä¸ºé˜ˆå€¼æ— å…³æŒ‡æ ‡ï¼Œæ›´å…·é²æ£’æ€§ã€‚

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **ConPLex**ï¼šåŸºäºå¯¹æ¯”å­¦ä¹ çš„DTIæ¨¡å‹ï¼Œä½¿ç”¨PLexè›‹ç™½ç¼–ç å™¨
- **TransformerCPI2.0**ï¼šç»“åˆTAPE-BERTå’ŒGNNçš„åºåˆ—çº§DTIæ¨¡å‹
- æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ç›¸åŒçš„è¾“å…¥ï¼ˆGPCRåºåˆ— + é…ä½“SMILESï¼‰å’Œè¶…å‚æ•°è®¾ç½®ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§Table 1ï¼‰

| Method | Random AUC | Intra-target AUC | Inter-target AUC |
|--------|------------|------------------|------------------|
| **ConPLex** | 53.03% | 52.89% | **44.91%** |
| **TransformerCPI2.0** | 65.94% | 62.02% | 66.31% |
| **GPCR-Filter (Ours)** | **99.02%** | **97.98%** | **80.35%** |

> æ³¨ï¼šAUC < 50% è¡¨ç¤ºæ¨¡å‹è¡¨ç°ä¸å¦‚éšæœºçŒœæµ‹ã€‚

#### âœ… ä¸»è¦å‘ç°ï¼š
- åœ¨**Random Split**ä¸‹ï¼ŒGPCR-Filterè¾¾åˆ°è¿‘å¤©èŠ±æ¿æ€§èƒ½ï¼ˆAUC ~99%ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼›
- åœ¨**Intra-target**è®¾ç½®ä¸­ï¼Œä»ä¿æŒæé«˜åˆ¤åˆ«åŠ›ï¼ˆAUC >97%ï¼‰ï¼Œè¡¨æ˜å¯¹æ–°é…ä½“çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼›
- åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„**Inter-target**ä»»åŠ¡ä¸­ï¼ŒGPCR-Filterä»¥**80.35% AUC**è¿œè¶…TransformerCPI2.0ï¼ˆ66.31%ï¼‰å’ŒConPLexï¼ˆ44.91%ï¼Œå·²è¿‡æ‹Ÿåˆï¼‰ï¼›
- åŒæ—¶åœ¨APã€Precisionç­‰æŒ‡æ ‡ä¸Šä¹Ÿå…¨é¢é¢†å…ˆã€‚

### ğŸ” å¯è§£é‡Šæ€§åˆ†æï¼ˆInterpretabilityï¼‰
- å¯¹ä¸¤ä¸ªæ–°è§£æçš„å¤åˆç‰©ï¼ˆPDB: 9bsb, 9jclï¼‰è¿›è¡Œæ³¨æ„åŠ›å¯è§†åŒ–ï¼š
  - æ¨¡å‹å…³æ³¨çš„Top-20æ®‹åŸºä¸­æœ‰**6â€“8ä¸ªä½äºæ™¶ä½“ç»“æ„ç»“åˆå£è¢‹å†…ï¼ˆ<5Ã…ï¼‰**ï¼›
  - æ³¨æ„åŠ›æ¨¡å¼åœ¨ä¸åŒè®­ç»ƒåˆ†å‰²ä¸‹ä¿æŒä¸€è‡´ï¼Œè¯´æ˜å­¦åˆ°çš„æ˜¯çœŸå®çš„ç»“åˆä½ç‚¹ç‰¹å¾ï¼Œè€Œéè®°å¿†ï¼›
  - æ”¯æŒæ¨¡å‹å…·å¤‡**æœºæ¢°å¯è§£é‡Šæ€§ï¼ˆmechanistic interpretabilityï¼‰**ã€‚

### ğŸ’¡ æ¹¿å®éªŒéªŒè¯ï¼ˆWet-lab Validationï¼‰
- åœ¨**5-HT1A receptor**ä¸Šè¿›è¡Œè™šæ‹Ÿç­›é€‰ä¸å®éªŒéªŒè¯ï¼š
  1. ä»ChemDivæ•°æ®åº“ä¸­å¯¹æ¥160ä¸‡åŒ–åˆç‰©ï¼Œå–å‰8,705ä¸ªï¼›
  2. ä½¿ç”¨GPCR-Filterè¿›ä¸€æ­¥ç­›é€‰ï¼Œé€‰æ‹©97ä¸ªé¢„æµ‹æ¦‚ç‡ >0.5 çš„åŒ–åˆç‰©ï¼›
  3. æˆåŠŸé‡‡è´­å¹¶æµ‹è¯•å…¶ä¸­52ä¸ªï¼›
  4. **å•æµ“åº¦ï¼ˆ30 Î¼Mï¼‰ç­›é€‰ä¸­ï¼Œ4ä¸ªåŒ–åˆç‰©ï¼ˆD24, D29, D34, D47ï¼‰è¡¨ç°å‡ºå¼ºæ¿€æ´»æ•ˆåº”**ï¼›
  5. å¤šæµ“åº¦æ›²çº¿æ˜¾ç¤ºå‡ä¸º**å¾®æ‘©å°”çº§ï¼ˆmicromolar-levelï¼‰æ¿€åŠ¨å‰‚**ï¼Œå…·æœ‰è¾ƒé«˜æœ€å¤§æ•ˆåº”ï¼ˆEmaxï¼‰ï¼Œè™½EC50åå³ç§»ä½†ä»å…·å¼€å‘æ½œåŠ›ã€‚

> âœ… è¿™æ˜¯é¦–æ¬¡é€šè¿‡çº¯åºåˆ—+SMILESé©±åŠ¨çš„AIæ¨¡å‹æˆåŠŸå‘ç°æ–°å‹éª¨æ¶çš„æ´»æ€§GPCRæ¿€åŠ¨å‰‚ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦ç»“è®º
1. **GPCR-Filteræ˜¾è‘—ä¼˜äºç°æœ‰DTIæ¨¡å‹**ï¼Œå°¤å…¶åœ¨è·¨é¶ç‚¹æ³›åŒ–ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼›
2. æ¨¡å‹ä¸ä»…èƒ½é¢„æµ‹ç»“åˆï¼Œè¿˜èƒ½æœ‰æ•ˆè¯†åˆ«**åŠŸèƒ½æ€§è°ƒèŠ‚å‰‚**ï¼Œå·²åœ¨æ¹¿å®éªŒä¸­éªŒè¯å‘ç°å¤šä¸ª**æ–°å‹éª¨æ¶çš„5-HT1Aæ¿€åŠ¨å‰‚**ï¼›
3. ç»“åˆ**è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆESM-3ï¼‰ä¸GNN + cross-attention**çš„è®¾è®¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰GPCRç‰¹æœ‰çš„åºåˆ—-åŠŸèƒ½å…³ç³»ï¼›
4. æ•°æ®ç»„ç»‡æœ¬èº«å­˜åœ¨åŒ–å­¦ç©ºé—´ä¸Šçš„èšç±»ç»“æ„ï¼ˆligand-profile similarity clusteringï¼‰ï¼Œä¸ºè·¨é¶ç‚¹è¿ç§»æä¾›äº†æ½œåœ¨åŸºç¡€ï¼›
5. æ³¨æ„åŠ›æœºåˆ¶å¯è§£é‡Šæ€§å¼ºï¼Œèšç„¦äºçœŸå®ç»“åˆå£è¢‹æ®‹åŸºï¼Œæ”¯æŒå…¶ç”Ÿç‰©å­¦åˆç†æ€§ã€‚

### âš ï¸ å±€é™æ€§
1. **è´Ÿæ ·æœ¬ä¸å¯é **ï¼šè´Ÿä¾‹é€šè¿‡æœªè§‚æµ‹åˆ°çš„ç»„åˆç”Ÿæˆï¼Œå¯èƒ½åŒ…å«æœªçŸ¥çš„çœŸå®é˜³æ€§ï¼ˆfalse negativesï¼‰ï¼›
2. **Inter-target splitä¸­å…è®¸é…ä½“é‡ç”¨**ï¼šå¯èƒ½è®©æŸäº›åŒ–å­¦æ¯æ ¸æä¾›â€œæ·å¾„ä¿¡å·â€ï¼Œå‰Šå¼±å¯¹çœŸæ­£æ–°é¢–æ€§çš„è€ƒéªŒï¼›
3. å°šæœªæ•´åˆ**ä¸‰ç»´ç»“æ„ä¿¡æ¯æˆ–å£è¢‹çº¦æŸ**ï¼Œæœªæ¥å¯ç»“åˆAlphaFold3æˆ–Dockingæå‡ç²¾åº¦ï¼›
4. å½“å‰æ¨¡å‹æœªåŒºåˆ†æ¿€åŠ¨å‰‚/æ‹®æŠ—å‰‚ç±»å‹ï¼Œä»…é¢„æµ‹â€œæ˜¯å¦è°ƒèŠ‚â€ã€‚

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. æ‰©å±•è‡³å…¶ä»–GPCRå®¶æ—åŠå…¶ä»–è†œè›‹ç™½é¶ç‚¹ï¼›
2. æ”¹è¿›è´Ÿæ ·æœ¬ç­–ç•¥ï¼Œä¾‹å¦‚ä½¿ç”¨**decoyç”Ÿæˆ**æˆ–**å¯¹æŠ—é‡‡æ ·**ï¼›
3. å¼•å…¥**ç»“æ„æ„ŸçŸ¥æ¨¡å—**ï¼ˆstructure-aware modelingï¼‰ï¼ŒèåˆAF3é¢„æµ‹ç»“æ„æˆ–æ˜¾å¼å»ºæ¨¡ç»“åˆå£è¢‹ï¼›
4. å¼€å‘**å¤šä»»åŠ¡ç‰ˆæœ¬**ï¼ŒåŒæ—¶é¢„æµ‹åŠŸèƒ½ç±»å‹ï¼ˆagonist/antagonistï¼‰ã€ä¿¡å·é€šè·¯åå¥½ï¼ˆG protein vs. arrestin biasï¼‰ï¼›
5. æ¢ç´¢**ä¸»åŠ¨å­¦ä¹ é—­ç¯ç³»ç»Ÿ**ï¼Œå°†GPCR-FilteråµŒå…¥è‡ªåŠ¨åŒ–è¯ç‰©å‘ç°æµç¨‹ã€‚

---

## æ€»ç»“
> **GPCR-Filter æ˜¯é¦–ä¸ªä¸“ä¸ºGPCRè°ƒèŠ‚å‰‚å‘ç°å®šåˆ¶çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œåœ¨æ•°æ®è´¨é‡ã€æ¨¡å‹è®¾è®¡ã€æ³›åŒ–èƒ½åŠ›å’Œå®éªŒéªŒè¯å››ä¸ªç»´åº¦å‡å®ç°çªç ´ã€‚å®ƒä¸ä»…åœ¨å¤šé¡¹åŸºå‡†ä¸Šå¤§å¹…è¶…è¶Šç°æœ‰DTIæ¨¡å‹ï¼Œè¿˜æˆåŠŸæŒ‡å¯¼å‘ç°äº†å…·æœ‰æ–°åŒ–å­¦éª¨æ¶çš„å¾®æ‘©å°”çº§æ¿€åŠ¨å‰‚ï¼Œå±•ç¤ºäº†AIåœ¨å¤æ‚ä¿¡å·ç³»ç»Ÿè¯ç‰©å‘ç°ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚**

è¯¥å·¥ä½œæ ‡å¿—ç€ä»â€œé€šç”¨DTIé¢„æµ‹â€å‘â€œé¶ç‚¹å®¶æ—ç‰¹å¼‚æ€§æ™ºèƒ½ç­›é€‰â€çš„é‡è¦è½¬å˜ï¼Œä¸ºä¸‹ä¸€ä»£GPCRè¯ç‰©ç ”å‘æä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•çš„è®¡ç®—å·¥å…·ã€‚

</details>

---

### 8. [Variational Quantum Circuit-Based Reinforcement Learning for Dynamic Portfolio Optimization](https://arxiv.org/abs/2601.18811)

**Authors**: Vincent Gurgul, Ying Chen, Stefan Lessmann  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.18811v1  

#### Abstract
This paper presents a Quantum Reinforcement Learning (QRL) solution to the dynamic portfolio optimization problem based on Variational Quantum Circuits. The implemented QRL approaches are quantum analogues of the classical neural-network-based Deep Deterministic Policy Gradient and Deep Q-Network al...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# **è®ºæ–‡æ€»ç»“ï¼šVariational Quantum Circuit-Based Reinforcement Learning for Dynamic Portfolio Optimization**

---

## 1. **è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹**

### âœ… **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**
è¯¥è®ºæ–‡é’ˆå¯¹**åŠ¨æ€æŠ•èµ„ç»„åˆä¼˜åŒ–**ï¼ˆDynamic Portfolio Optimizationï¼‰è¿™ä¸€å¤æ‚ã€é«˜ç»´ä¸”éå¹³ç¨³ç¯å¢ƒä¸‹çš„åºè´¯å†³ç­–é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿé™æ€æ–¹æ³•ï¼ˆå¦‚ MVOï¼‰æ— æ³•é€‚åº”å¸‚åœºåŠ¨æ€å˜åŒ–ï¼Œè€Œç»å…¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡çŠ¶æ€-åŠ¨ä½œç©ºé—´ä¸­é¢ä¸´å¯æ‰©å±•æ€§å’Œè®­ç»ƒæ•ˆç‡çš„æŒ‘æˆ˜ã€‚

åŒæ—¶ï¼Œå°½ç®¡å·²æœ‰ç ”ç©¶å°†é‡å­è®¡ç®—ç”¨äºæŠ•èµ„ç»„åˆä¼˜åŒ–ï¼Œä½†å¤§å¤šå±€é™äº**é™æ€ QUBO é—®é¢˜**ï¼ˆå¦‚èµ„äº§é€‰æ‹©ï¼‰ï¼Œç¼ºä¹å¯¹**åŠ¨æ€ã€è¿ç»­å†³ç­–è¿‡ç¨‹**çš„å­¦ä¹ èƒ½åŠ›ã€‚

### âœ… **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº **Variational Quantum Circuits (VQCs)** çš„å…¨é‡å­å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°äº†ä»¥ä¸‹åˆ›æ–°ï¼š

- **é¦–æ¬¡å°† Quantum Reinforcement Learning (QRL)** åº”ç”¨äº**çœŸå®é‡‘èæ•°æ®ä¸Šçš„åŠ¨æ€æŠ•èµ„ç»„åˆç®¡ç†**ã€‚
- è®¾è®¡äº†é‡å­ç‰ˆæœ¬çš„ **Deep Deterministic Policy Gradient (DDPG)** å’Œ **Deep Q-Network (DQN)** ç®—æ³•ï¼Œå…¶ä¸­**Actor å’Œ Critic å‡ç”± VQC æ„å»º**ï¼Œå½¢æˆç«¯åˆ°ç«¯çš„é‡å­ç­–ç•¥ç½‘ç»œã€‚
- å¼•å…¥**æŒ¯å¹…ç¼–ç **ï¼ˆAmplitude Encodingï¼‰ç»“åˆéçº¿æ€§ç‰¹å¾æ˜ å°„ï¼ˆ`Z(x) = [x, xâŠ—Â², sin(x), cos(x)]`ï¼‰ï¼Œæå‡è¾“å…¥è¡¨è¾¾èƒ½åŠ›å¹¶ç¼“è§£â€œè´«ç˜ é«˜åŸâ€ï¼ˆbarren plateausï¼‰é—®é¢˜ã€‚
- é‡‡ç”¨**æœŸæœ›å€¼æµ‹é‡ + â„“Â¹ å½’ä¸€åŒ–**çš„æ–¹å¼è¾“å‡ºåˆæ³•çš„æŠ•èµ„ç»„åˆæƒé‡å‘é‡ï¼Œæ”¯æŒå¤šèµ„äº§ã€é•¿çŸ­ä»“é…ç½®ã€‚

### âœ… **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**
| ç»´åº¦ | ä¼˜åŠ¿ |
|------|------|
| **å‚æ•°æ•ˆç‡** | ä»…éœ€ **30â€“60 ä¸ªå¯è®­ç»ƒå‚æ•°**å³å¯è¾¾åˆ°ä¸æ•°ä¸‡ç”šè‡³åå‡ ä¸‡ä¸ªå‚æ•°çš„ç»å…¸ Deep RL æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„é£é™©è°ƒæ•´æ”¶ç›Šè¡¨ç°ã€‚ |
| **æ¨¡å‹è¡¨è¾¾åŠ›** | åˆ©ç”¨é‡å­çº ç¼ å’Œå åŠ æ€ï¼Œåœ¨ä½å‚æ•°ä¸‹å®ç°é«˜è¡¨è¾¾èƒ½åŠ›ï¼Œå°¤å…¶é€‚åˆå¤„ç†é«˜ç»´é‡‘èæ—¶é—´åºåˆ—ã€‚ |
| **é²æ£’æ€§** | åœ¨ä¸åŒå¸‚åœºå‘¨æœŸä¸‹è¡¨ç°å‡ºæ›´ä½çš„ Sharpe Ratio æ³¢åŠ¨ï¼Œè¯´æ˜å…¶ç­–ç•¥æ›´å…·ç¨³å®šæ€§ã€‚ |
| **ç†è®ºå‰ç»æ€§** | å±•ç¤ºäº† QRL åœ¨å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥åœ¨é‡‘èåŠå…¶ä»–åŠ¨æ€ç³»ç»Ÿä¸­çš„åº”ç”¨é“ºå¹³é“è·¯ã€‚ |

---

## 2. **æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®**

### ğŸ“Š **ä½¿ç”¨çš„æ•°æ®é›†**
- åŒ…å« **15 ç§å¤šæ ·åŒ–èµ„äº§**çš„æ—¥é¢‘æ”¶ç›˜ä»·ï¼Œæ¶µç›–è‚¡ç¥¨ï¼ˆAAPL, MSFTï¼‰ã€ETFï¼ˆSPY, QQQï¼‰ã€å€ºåˆ¸ï¼ˆTLT, LQDï¼‰ã€é»„é‡‘ï¼ˆGLDï¼‰ã€èƒ½æºï¼ˆUSOï¼‰ç­‰ã€‚
- æ—¶é—´è·¨åº¦ï¼š**2011å¹´8æœˆ è‡³ 2025å¹´9æœˆ**ï¼Œå…± **5049ä¸ªäº¤æ˜“æ—¥**ã€‚
- è¾“å…¥çŠ¶æ€åŒ…å«ï¼š
  - è¿‡å» **30 å¤©çš„ä»·æ ¼æ»šåŠ¨çª—å£**ï¼ˆlookback windowï¼‰
  - é€šè¿‡ **Auto ARIMA æ¨¡å‹é¢„æµ‹çš„æœªæ¥ 7 å¤©ä»·æ ¼**ï¼ˆforecast windowï¼‰

### âš™ï¸ **å®éªŒè®¾ç½®**
- **åŠ¨ä½œç©ºé—´**ï¼šè¿ç»­åŠ¨ä½œï¼Œè¡¨ç¤ºæ¯ä¸ªèµ„äº§çš„æŒä»“æƒé‡ `w âˆˆ â„^15`ï¼Œå…è®¸åšç©ºï¼ˆshort selling â‰¤ 100%ï¼‰ã€‚
- **å¥–åŠ±å‡½æ•°**ï¼šé£é™©è°ƒæ•´å›æŠ¥  
  $$
  r_t = \mu_{t-L:t+F+1} - \lambda \cdot \sigma_{t-L:t+F+1}
  $$
  å…¶ä¸­ Î» æ§åˆ¶é£é™©åå¥½ã€‚
- **äº¤æ˜“æˆæœ¬**ï¼šæ¯æ¬¡è°ƒä»“æ”¶å– **0.15%** çš„æ‰‹ç»­è´¹ã€‚
- **è°ƒä»“é¢‘ç‡**ï¼šæ¯ 30 å¤©é‡æ–°å¹³è¡¡ä¸€æ¬¡ã€‚

### ğŸ¯ **è¯„ä¼°æŒ‡æ ‡**
- ä¸»è¦æŒ‡æ ‡ï¼š**æµ‹è¯•é›†ä¸Šçš„å¹³å‡ Sharpe Ratio**ï¼ˆå·²æ‰£é™¤äº¤æ˜“æˆæœ¬ï¼‰
- è¾…åŠ©æŒ‡æ ‡ï¼š
  - Sharpe Ratio çš„æ ‡å‡†å·®ï¼ˆè¡¡é‡è·¨å¸‚åœºå‘¨æœŸçš„ç¨³å¥æ€§ï¼‰
  - æ‰§è¡Œæ—¶é—´ï¼ˆinference latencyï¼‰
  - å‚æ•°æ•°é‡ï¼ˆparameter countï¼‰

### ğŸ” **äº¤å‰éªŒè¯è®¾è®¡**
é‡‡ç”¨ **7 æŠ˜é€’å¢æ»šåŠ¨çª—å£æ—¶é—´åºåˆ—äº¤å‰éªŒè¯**ï¼ˆincreasing rolling-window time-series CVï¼‰ï¼š
- æ¯ä¸€æŠ˜è®­ç»ƒé›†é€æ­¥æ‰©å¤§ï¼Œç¡®ä¿å› æœé¡ºåºã€‚
- éªŒè¯é›†ç”¨äºè¶…å‚è°ƒä¼˜å’Œæ—©åœã€‚
- æµ‹è¯•é›†å®Œå…¨æœªè§ï¼Œç”¨äºæœ€ç»ˆè¯„ä¼°ã€‚

### ğŸ†š **åŸºçº¿æ–¹æ³•å¯¹æ¯”**
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **é™æ€åŸºå‡†** | - Equal Weightsï¼ˆç­‰æƒç»„åˆï¼‰<br> - Mean-Variance Optimization (MVO) |
| **ç»å…¸ Deep RL** | - DDPGï¼ˆ13.5k / 27k / 160k å‚æ•°ï¼‰<br> - DQNï¼ˆ13.5k / 27k / 160k å‚æ•°ï¼‰ |
| **æœ¬ç ”ç©¶æå‡ºçš„ QRL** | - Quantum DDPGï¼ˆ30 / 60 å‚æ•°ï¼‰<br> - Quantum DQNï¼ˆ30 / 60 å‚æ•°ï¼‰ |

æ‰€æœ‰æ¨¡å‹å…±äº«ç›¸åŒçš„çŠ¶æ€å®šä¹‰ã€å¥–åŠ±å‡½æ•°å’Œç¯å¢ƒé€»è¾‘ï¼Œä¿è¯å…¬å¹³æ¯”è¾ƒã€‚

---

## 3. **ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡**

### ğŸ“ˆ **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 5.1 å’Œ Figure 5.1ï¼‰**

| æ¨¡å‹ | Sharpe Ratio (mean) | Std. Dev. | å‚æ•°é‡ | æ¨ç†æ—¶é—´ï¼ˆGPU/QPUï¼‰ |
|------|---------------------|-----------|--------|-----------------------|
| Equal Weights | 0.4375 | 0.6558 | â€“ | <1s |
| MVO | 0.5919 | 0.9170 | â€“ | <1s |
| Classical DDPG (13.5k) | 0.4287 | 0.6151 | 13.5k | 7s (GPU) |
| Classical DQN (160k) | **0.8237** | 0.5367 | 160k | 27s (GPU) |
| **Quantum DDPG (60 params)** | **0.7281** | **0.7395** | **60** | **<1s circuit exec**, ~23min (cloud QPU) |
| **Quantum DQN (60 params)** | 0.6537 | 0.6155 | 60 | <1s circuit exec, ~43min (cloud QPU) |

> æ³¨ï¼šQPU ä¸ŠæŠ¥å‘Šçš„æ—¶é—´ä¸ºâ€œæ€»ä½œä¸šè€—æ—¶â€ï¼ŒåŒ…æ‹¬æ’é˜Ÿã€åˆå§‹åŒ–ã€å¤šæ¬¡ shot æµ‹é‡ç­‰äº‘å¹³å°å¼€é”€ã€‚

### ğŸ” **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**
- **å‚æ•°æ•ˆç‡ç¢¾å‹çº§ä¼˜åŠ¿**ï¼š
  - ä»…ç”¨ **60 ä¸ªå‚æ•°**çš„ QRL æ¨¡å‹ï¼Œæ€§èƒ½æ¥è¿‘ç”šè‡³è¶…è¿‡ **160k å‚æ•°çš„ç»å…¸ RL æ¨¡å‹**ã€‚
  - Quantum DDPG (60) çš„ Sharpe Ratio è¶…è¿‡ MVOï¼ˆ0.728 vs 0.592ï¼‰ï¼Œä¸”æ³¢åŠ¨æ›´å°ã€‚
- **è¶…è¶Šå°è§„æ¨¡ç»å…¸æ¨¡å‹**ï¼š
  - å³ä½¿æ˜¯ 30 å‚æ•°çš„ QRL æ¨¡å‹ä¹Ÿä¼˜äºå¤§å¤šæ•°ä½å®¹é‡ç»å…¸æ¨¡å‹ï¼ˆå¦‚ 13.5k å‚æ•°çš„ DDPGï¼‰ã€‚
- **ç¨³å®šæ€§æ›´å¼º**ï¼š
  - QRL æ¨¡å‹åœ¨å„ fold ä¸­çš„ Sharpe Ratio å˜å¼‚è¾ƒå°ï¼Œè¡¨æ˜å…¶å¯¹å¸‚åœº regime å˜åŒ–çš„é€‚åº”èƒ½åŠ›æ›´å¼ºã€‚

### âŒ **æ¶ˆèå®éªŒï¼ˆéšå«åˆ†æï¼‰**
è™½ç„¶æœªæ˜ç¡®åˆ—å‡ºæ¶ˆèè¡¨ï¼Œä½†æ–‡ä¸­é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œäº†æœ‰æ•ˆæ€§éªŒè¯ï¼š
- å¯¹æ¯”ä¸åŒå‚æ•°é‡ï¼ˆ30 vs 60ï¼‰çš„ QRL æ¨¡å‹ â†’ æ˜¾ç¤ºå¢åŠ å‚æ•°èƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚
- ä½¿ç”¨ä¸åŒ RL æ¶æ„ï¼ˆDDPG vs DQNï¼‰â†’ éªŒè¯ VQC å¯çµæ´»é€‚é…å¤šç§ RL èŒƒå¼ã€‚
- åˆ†æç‰¹å¾æ˜ å°„ `Z(x)` çš„ä½œç”¨ â†’ è¡¨æ˜å…¶æœ‰åŠ©äºæ‰“ç ´å¾„å‘å¯¹ç§°æ€§ï¼Œç¼“è§£è®­ç»ƒå›°éš¾ã€‚

---

## 4. **å…³é”®ç»“è®ºå’Œå‘ç°**

### âœ… **ä¸»è¦å‘ç°**
1. **QRL åœ¨åŠ¨æ€æŠ•èµ„ç»„åˆä¼˜åŒ–ä¸­å…·å¤‡ç«äº‰åŠ›**ï¼š
   - å°½ç®¡å½“å‰ç¡¬ä»¶å—é™ï¼Œä½†åŸºäº VQC çš„ QRL å·²èƒ½åœ¨çœŸå®é‡‘èæ•°æ®ä¸Šå­¦ä¹ å‡ºæœ‰æ•ˆçš„è‡ªé€‚åº”äº¤æ˜“ç­–ç•¥ã€‚
2. **æé«˜çš„å‚æ•°æ•ˆç‡**ï¼š
   - å¾—ç›Šäºé‡å­å åŠ ä¸çº ç¼ ï¼ŒVQC èƒ½ä»¥æå°‘å‚æ•°æ•æ‰å¤æ‚çš„å¸‚åœºåŠ¨æ€å…³ç³»ï¼Œè¿œè¶…ç»å…¸ç¥ç»ç½‘ç»œçš„â€œå‚æ•°çˆ†ç‚¸â€æ¨¡å¼ã€‚
3. **æ›´å¼ºçš„è·¨å¸‚åœºé²æ£’æ€§**ï¼š
   - QRL ç­–ç•¥åœ¨å¤šä¸ªç»æµå‘¨æœŸä¸­è¡¨ç°ç¨³å®šï¼ŒSharpe Ratio æ–¹å·®ä½äºå¤šæ•°ç»å…¸æ¨¡å‹ã€‚
4. **æ¨ç†é€Ÿåº¦æ‚–è®º**ï¼š
   - **ç”µè·¯æ‰§è¡Œæœ¬èº«æå¿«**ï¼ˆæ¯«ç§’çº§ï¼‰ï¼Œä½†**äº‘ç«¯éƒ¨ç½²å»¶è¿Ÿæé«˜**ï¼ˆåˆ†é’Ÿçº§ï¼‰ï¼Œä¸»å› æ˜¯å¤šç§Ÿæˆ·æ’é˜Ÿã€é‡å¤æ ¡å‡†å’Œåˆå§‹åŒ–å¼€é”€ã€‚

### âš ï¸ **æ–¹æ³•çš„å±€é™æ€§**
| å±€é™ | è¯´æ˜ |
|------|------|
| **å½“å‰ç¡¬ä»¶é™åˆ¶** | å®éªŒåªèƒ½åœ¨æ¨¡æ‹Ÿå™¨ä¸Šè®­ç»ƒï¼ˆstatevector simulatorï¼‰ï¼Œå®é™… QPU ä»…ç”¨äºæ¨ç†ï¼›æœ€å¤§ qubit æ•°å— GPU å†…å­˜é™åˆ¶ï¼ˆçº¦ 20 qubitsï¼‰ã€‚ |
| **äº‘å¹³å°å»¶è¿Ÿä¸¥é‡** | å½“å‰ IBM Quantum Cloud çš„è°ƒåº¦æœºåˆ¶å¯¼è‡´ end-to-end æ¨ç†å»¶è¿Ÿé«˜è¾¾ **23â€“43 åˆ†é’Ÿ**ï¼Œä¸é€‚ç”¨äºé«˜é¢‘äº¤æ˜“åœºæ™¯ã€‚ |
| **å°šæ— å®¹é”™èƒ½åŠ›** | ä½¿ç”¨çš„æ˜¯ NISQ è®¾å¤‡ï¼Œå™ªå£°å½±å“å°šæœªæ·±å…¥ç ”ç©¶ï¼ˆæœ¬å·¥ä½œä¾èµ–æ¨¡æ‹Ÿå™¨è·å¾—ç²¾ç¡®æ¢¯åº¦ï¼‰ã€‚ |
| **æ¨¡å‹è§„æ¨¡æœ‰é™** | æœ€å¤§ä»…æµ‹è¯•åˆ° 60 å‚æ•°ï¼Œæ›´å¤§è§„æ¨¡æ½œåŠ›æœªçŸ¥ã€‚ |

### ğŸ”® **æœªæ¥å·¥ä½œæ–¹å‘**
1. **ä¸“ç”¨é‡å­è®¾å¤‡éƒ¨ç½²**ï¼š
   - è‹¥ QPU å¯ä¸“ç”¨äºæŸç±»å›ºå®šç”µè·¯ï¼ˆå¦‚ RL æ¨ç†ï¼‰ï¼Œåˆ™å¯å¤§å¹…å‡å°‘å‡†å¤‡å¼€é”€ï¼Œé‡Šæ”¾åŸç”Ÿé«˜é€Ÿæ½œåŠ›ã€‚
2. **é™ä½äº‘æœåŠ¡å»¶è¿Ÿ**ï¼š
   - å¼€å‘é¢å‘ QML çš„ä½å»¶è¿Ÿé‡å­äº‘è®¡ç®—æ¶æ„ï¼Œä¾‹å¦‚æ‰¹å¤„ç†ã€ç¼“å­˜ç¼–è¯‘ã€æŒä¹…åŒ–é‡å­æ€ç­‰ã€‚
3. **æ¢ç´¢æ›´å¤§è§„æ¨¡ VQC**ï¼š
   - ç»“åˆ error mitigation å’Œ shallow ansatzï¼Œå°è¯•æ›´å¤šèµ„äº§æˆ–æ›´é•¿æ—¶åºè¾“å…¥ã€‚
4. **èåˆç»å…¸-é‡å­æ··åˆæ¶æ„**ï¼š
   - å¦‚ç”¨ç»å…¸æ¨¡å‹æå–ç‰¹å¾ï¼ŒVQC åšæœ€ç»ˆå†³ç­–ï¼Œå…¼é¡¾æ•ˆç‡ä¸è¡¨è¾¾åŠ›ã€‚
5. **æ‰©å±•è‡³å…¶ä»–é‡‘èä»»åŠ¡**ï¼š
   - å¦‚æœŸæƒå®šä»·ã€é£é™©ç®¡ç†ã€ç®—æ³•äº¤æ˜“ç­‰ï¼Œè¿›ä¸€æ­¥éªŒè¯ QRL çš„é€šç”¨æ€§ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯

> æœ¬æ–‡è¯æ˜äº† **VQC-based QRL æ˜¯ä¸€ç§æå…·å‚æ•°æ•ˆç‡å’Œé²æ£’æ€§çš„åŠ¨æ€æŠ•èµ„ç»„åˆä¼˜åŒ–èŒƒå¼**ï¼Œè™½å—é™äºå½“å‰é‡å­äº‘åŸºç¡€è®¾æ–½çš„å»¶è¿Ÿç“¶é¢ˆï¼Œä½†åœ¨ç®—æ³•å±‚é¢å·²å±•ç°å‡ºåª²ç¾ç”šè‡³è¶…è¶Šå¤§å‹ç»å…¸ Deep RL æ¨¡å‹çš„èƒ½åŠ›ï¼Œé¢„ç¤ºç€æœªæ¥åœ¨ä¸“ç”¨ç¡¬ä»¶æ”¯æŒä¸‹å¯èƒ½æˆä¸ºé‡‘èæ™ºèƒ½å†³ç­–çš„æ–°å‰æ²¿ã€‚

ğŸ”— **å¼€æºä»£ç **ï¼š[https://github.com/VincentGurgul/qrl-dpo-public](https://github.com/VincentGurgul/qrl-dpo-public)

</details>

---

### 9. [LoPRo: Enhancing Low-Rank Quantization via Permuted Block-Wise Rotation](https://arxiv.org/abs/2601.19675)

**Authors**: Hongyaoxing Gu, Lijuan Hu, Liye Yu, Haowei Li, Fangfang Liu  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2601.19675v1  

#### Abstract
Post-training quantization (PTQ) enables effective model compression while preserving relatively high accuracy. Current weight-only PTQ methods primarily focus on the challenging sub-3-bit regime, where approaches often suffer significant accuracy degradation, typically requiring fine-tuning to achi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# LoPRo: Enhancing Low-Rank Quantization via Permuted Block-Wise Rotation è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å½“å‰çš„ **Post-Training Quantization (PTQ)** æ–¹æ³•åœ¨ä½æ¯”ç‰¹ï¼ˆsub-3-bitï¼‰é‡åŒ–æ—¶é¢ä¸´æ˜¾è‘—çš„ç²¾åº¦ä¸‹é™é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨**æ— éœ€å¾®è°ƒï¼ˆfine-tuning-freeï¼‰** çš„åœºæ™¯ä¸‹ã€‚å°½ç®¡å·²æœ‰ä½ç§©è¡¥å¿ï¼ˆLow-Rank Compensation, LoRCï¼‰æ–¹æ³•å°è¯•é€šè¿‡å¼•å…¥ä½ç§©çŸ©é˜µæ¥æ¢å¤é‡åŒ–è¯¯å·®ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹æŒ‘æˆ˜ï¼š

- **æ®‹å·®çŸ©é˜µé‡åŒ–å›°éš¾**ï¼šä½ç§©è¿‘ä¼¼åçš„æ®‹å·®çŸ©é˜µä»åŒ…å«éš¾ä»¥é‡åŒ–çš„å¼‚å¸¸å€¼ï¼ˆoutliersï¼‰ï¼Œç›´æ¥åº”ç”¨æ—‹è½¬ç­‰æŠ€æœ¯ä¼šç ´åä½ç§©åˆ†è§£çš„æœ‰æ•ˆæ€§ã€‚
- **é«˜è®¡ç®—ä¸å­˜å‚¨å¼€é”€**ï¼šä¼ ç»Ÿ SVD åˆ†è§£è®¡ç®—å¤æ‚åº¦é«˜ï¼ˆO(NÂ³)ï¼‰ï¼Œä¸”éœ€è¦è¾ƒé«˜ç§©ï¼ˆå¦‚ r=256ï¼‰æ‰èƒ½è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œå¯¼è‡´æ¨ç†å»¶è¿Ÿå¢åŠ ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡º **LoPRo (Low-rank Partial Rotation Quantization)**ï¼Œä¸€ç§æ— éœ€å¾®è°ƒçš„é«˜æ•ˆ PTQ ç®—æ³•ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### ï¼ˆ1ï¼‰**Permuted Block-Wise Rotationï¼ˆç½®æ¢å—çŠ¶éƒ¨åˆ†æ—‹è½¬ï¼‰**
- **åˆ—é‡æ’ï¼ˆColumn Permutationï¼‰**ï¼šåŸºäºä»£ç† Hessian çŸ©é˜µå’Œæ®‹å·®ç»å¯¹å‡å€¼ï¼Œå¯¹æ®‹å·®çŸ©é˜µçš„åˆ—è¿›è¡Œé‡è¦æ€§æ’åºï¼Œå°†æ›´é‡è¦çš„é€šé“ç½®äºå‰åˆ—ã€‚
- **éƒ¨åˆ†å—çŠ¶æ—‹è½¬ï¼ˆPartial Block Rotationï¼‰**ï¼šä»…å¯¹éé‡è¦åˆ—åº”ç”¨ **Walsh-Hadamard Transformation (WHT)** è¿›è¡Œå—çŠ¶æ—‹è½¬ï¼Œè€Œä¿ç•™æœ€é‡è¦åˆ—ä¸æ—‹è½¬ï¼ˆä½¿ç”¨å•ä½çŸ©é˜µï¼‰ï¼Œä»è€Œåœ¨æå‡é‡åŒ–é²æ£’æ€§çš„åŒæ—¶ä¿æŠ¤å…³é”®ä¿¡æ¯ã€‚

> å…¬å¼å½¢å¼ï¼š
> $$
> \begin{bmatrix}
> I_{b_f} & 0 \\
> 0 & H_{\text{wal}}
> \end{bmatrix}
> $$

#### ï¼ˆ2ï¼‰**R1SVDï¼šè½»é‡çº§æ··åˆç²¾åº¦ä½ç§©åˆ†è§£**
- æå‡º **Rank-1 Sketch SVD (R1SVD)**ï¼ŒåŸºäºéšæœºè‰å›¾ï¼ˆrandomized sketchingï¼‰ç®€åŒ– SVD æµç¨‹ï¼Œå°†æ—¶é—´å¤æ‚åº¦ä» O(NÂ³) é™è‡³ O(NÂ²)ï¼Œå¹¶æ”¯æŒè¿­ä»£æ›´æ–°ã€‚
- ä½¿ç”¨ **fp8 å­˜å‚¨ U å’Œ Vï¼Œfp16 å­˜å‚¨å¥‡å¼‚å€¼ Î£**ï¼Œå®ç°å­˜å‚¨å¼€é”€å‡åŠï¼ŒåŒæ—¶ä¿æŒé«˜ç²¾åº¦ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | LoPRo ä¼˜åŠ¿ |
|------|-----------|
| **ç²¾åº¦** | åœ¨ 2-bit å’Œ 3-bit ä¸‹å‡ä¼˜äºæ‰€æœ‰ fine-tuning-free åŸºçº¿ï¼Œç”šè‡³åª²ç¾éœ€å¾®è°ƒçš„æ–¹æ³• |
| **æ•ˆç‡** | é‡åŒ–é€Ÿåº¦å¿«ï¼ˆ7B æ¨¡å‹ < 0.5 å°æ—¶ï¼ŒMixtral-8x7B < 2.5 å°æ—¶ï¼‰ |
| **æ¨ç†å»¶è¿Ÿ** | é¢å¤–å»¶è¿Ÿ < 10%ï¼Œéš batch size å¢å¤§è€Œé™ä½ |
| **å†…å­˜å¼€é”€** | ä»…å¢åŠ çº¦ 2% é¢å¤–å­˜å‚¨ï¼ˆr=16ï¼‰ |
| **é€šç”¨æ€§** | æ”¯æŒ Dense å’Œ MoE æ¶æ„ï¼ˆå¦‚ LLaMAã€Mixtralï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **æ ¡å‡†æ•°æ®é›†ï¼ˆCalibration Setï¼‰**ï¼š
  - `c4` æ•°æ®é›†ï¼Œé‡‡æ · 128 ä¸ªåºåˆ—ï¼Œæ¯ä¸ª 2048 tokens
- **è¯„ä¼°æ•°æ®é›†**ï¼š
  - **Perplexity**: `WikiText2`
  - **Zero-Shot Accuracy**: 
    - ARC-Challenge (AC), ARC-Easy (AE)
    - PIQA (QA), Winogrande (WI)
    - BoolQ (BQ), Hellaswag (HS)
    - OpenBookQA (OB), MathQA (MQ)

### å®éªŒè®¾ç½®
- **æ¨¡å‹å®¶æ—**ï¼š
  - **Dense Models**: LLaMA-2 (7B, 13B), LLaMA-3 (8B)
  - **MoE Model**: Mixtral-8x7B
- **é‡åŒ–é…ç½®**ï¼š
  - æƒé‡é‡åŒ–ä¸º 2-bit / 3-bitï¼Œæ¿€æ´»ä¿æŒ 16-bitï¼ˆW2A16 / W3A16ï¼‰
  - ä½ç§©ç§©æ•°ï¼šr = 16
  - å—å¤§å°ï¼š$ b_r = b_H = 256 $
- **ç¡¬ä»¶å¹³å°**ï¼š
  - å•å¡ A100 40GB æˆ– A800 80GB GPU

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | æè¿° |
|------|------|
| **PPL (Perplexity)** | WikiText2 ä¸Šçš„è¯­è¨€å»ºæ¨¡å›°æƒ‘åº¦ |
| **Zero-Shot Accuracy (%)** | å¤šé¡¹é€‰æ‹©ä»»åŠ¡å‡†ç¡®ç‡ |
| **Quantization Time** | å®Œæ•´é‡åŒ–è€—æ—¶ï¼ˆå°æ—¶/åˆ†é’Ÿï¼‰ |
| **Throughput / Latency** | æ¨ç†ååé‡ï¼ˆtoken/sï¼‰ä¸å»¶è¿Ÿ |
| **Compression Ratio** | å¹³å‡æ¯”ç‰¹å®½åº¦ï¼ˆbits/weightï¼‰ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | åŸºçº¿æ–¹æ³• |
|------|---------|
| **Scalar Quantization** | GPTQ, OmniQuant |
| **Vector Quantization** | GPTVQ, AQLM |
| **Rotation-Based** | QuIP#, QuaRot |
| **Low-Rank Compensation** | LQER, SVD-Quant |
| **Fine-Tuning Based** | RILQ, CALDERA |
| **MoE-Specific** | MoEQuant++ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 & Table 2ï¼‰

#### LLaMA-2 7B ç»“æœï¼ˆ3-bitï¼‰
| æ–¹æ³• | PPL â†“ | Avg Acc â†‘ |
|------|--------|------------|
| FP16 | 5.12 | 66.8 |
| GPTQ | 8.06 | 59.2 |
| GPTVQ | 5.61 | 74.1 |
| LQER | 5.72 | 74.2 |
| **LoPRo** | **5.43** | **76.3** |
| **LoPRo-v (VQ)** | **5.45** | **76.7** |

> âœ… **LoPRo åœ¨æ— éœ€å¾®è°ƒä¸‹è¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼Œæ¥è¿‘ç”šè‡³è¶…è¿‡éœ€å¾®è°ƒæ–¹æ³•**

#### Mixtral-8x7B ç»“æœï¼ˆ2.17-bitï¼‰
| æ–¹æ³• | Bit | PPL â†“ | Zero-Shot Acc â†‘ |
|------|-----|--------|------------------|
| MoEQuant++ | 3.00 | 4.90 | ~60.1 (AC) |
| **LoPRo** | **2.17** | **4.80** | **55.8 â†’ 80.5** |
| **LoPRo-v** | **2.17** | **4.80** | **55.8 â†’ 80.5** |

> âœ… **ä»¥æ›´ä½æ¯”ç‰¹ï¼ˆ2.17 vs 3.0ï¼‰å®ç°æ›´é«˜ç²¾åº¦ï¼Œå‹ç¼©ç‡æå‡æ˜¾è‘—**

#### ä¸å¾®è°ƒæ–¹æ³•å¯¹æ¯”ï¼ˆTable 2ï¼‰
åœ¨ LLaMA-2 7B ä¸Šç»“åˆ RILQ å¾®è°ƒåï¼š
- LoPRo-FT @ 2.2-bit è¾¾åˆ° **PPL=6.06**, è¶…è¿‡ CALDERA (6.79) å’Œ RILQ (6.28)
- è¡¨æ˜ LoPRo å¯ä½œä¸ºæ›´ä¼˜çš„åˆå§‹åŒ–åŸºç¡€ï¼Œæå‡å¾®è°ƒä¸Šé™

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
| å¯¹æ¯”ç»´åº¦ | LoPRo ä¼˜åŠ¿ |
|----------|-----------|
| **vs GPTQ/GPTVQ** | PPL é™ä½ 0.3â€“0.5ï¼Œé›¶æ ·æœ¬å‡†ç¡®ç‡æå‡ 4â€“8% |
| **vs LQER** | ä½¿ç”¨ r=16 vs r=256ï¼Œç²¾åº¦æ›´é«˜ä¸”å†…å­˜å‡å°‘ 75% |
| **vs QuIP#** | åœ¨ 2-bit ä¸‹ PPL é™ä½ >20%ï¼Œå†…å­˜å¢é•¿ <10% |
| **vs MoEQuant** | åœ¨ MoE æ¨¡å‹ä¸Šå®Œæˆé‡åŒ–ä»…éœ€ ~2.5 å°æ—¶ï¼ŒPPL é™ 0.4ï¼Œå‡†ç¡®ç‡å‡ 8% |

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 5 & Appendix Dï¼‰

#### ä¸åŒç»„ä»¶æ¶ˆèï¼ˆ2-bit LLaMA-2 7Bï¼‰
| é…ç½® | PPL â†“ | Avg Acc â†‘ |
|------|--------|------------|
| RTNï¼ˆæ— ä¼˜åŒ–ï¼‰ | 402 | 44.1 |
| + GPTQï¼ˆæ— æ—‹è½¬ï¼‰ | 8.40 | 53.0 |
| + Full HWT æ—‹è½¬ | 9.49 | 51.5 |
| **+ Partial HWT + Permutation** | **7.39** | **57.8** |
| **+ Vector Quantization** | **6.53** | **61.2** |

> ğŸ” **å…³é”®å‘ç°**ï¼š
> - å…¨å±€æ—‹è½¬åè€ŒæŸå®³æ€§èƒ½ï¼ˆç ´åé‡è¦åˆ—ç»“æ„ï¼‰
> - **éƒ¨åˆ†æ—‹è½¬ + åˆ—é‡æ’** æ˜¯æ€§èƒ½è·ƒå‡çš„å…³é”®
> - å‘é‡é‡åŒ–å¯è¿›ä¸€æ­¥æå‡æé™

#### è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ
- **ç§© r=16 å·²è¶³å¤Ÿ**ï¼šr>16 æ”¹è¿›æœ‰é™ï¼Œä½†å­˜å‚¨æˆæœ¬ä¸Šå‡
- **è¿­ä»£æ¬¡æ•° it=8 æœ€ä¼˜**ï¼šå†å¢åŠ æ”¶ç›Šé¥±å’Œ
- **å—å¤§å° br=bH=256 æœ€ä½³**ï¼šå¤ªå°åˆ™æ—‹è½¬ä¸è¶³ï¼Œå¤ªå¤§åˆ™å½±å“é‡è¦åˆ—

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **ä½ç§© + å±€éƒ¨æ—‹è½¬æ˜¯é«˜æ•ˆ PTQ çš„å…³é”®è·¯å¾„**  
   LoPRo æˆåŠŸè§£å†³äº†â€œä½ç§©åˆ†è§£â€ä¸â€œæ—‹è½¬å¢å¼ºé‡åŒ–â€çš„å…¼å®¹æ€§é—®é¢˜ï¼Œé€šè¿‡**ä¿æŠ¤é‡è¦åˆ— + æ—‹è½¬æ¬¡è¦åˆ—**çš„è®¾è®¡å®ç°äº†åŒèµ¢ã€‚

2. **æä½ç§©å³å¯è·å¾—é«˜æ€§èƒ½**  
   ä»…éœ€ **r=16** çš„ä½ç§©è¡¥å¿ï¼Œåœ¨å‡ ä¹ä¸å¢åŠ å­˜å‚¨çš„å‰æä¸‹æ˜¾è‘—æå‡é‡åŒ–ç¨³å®šæ€§ï¼Œè¿œä¼˜äºéœ€ r=256 çš„ LQERã€‚

3. **æ— éœ€å¾®è°ƒå³å¯é€¼è¿‘å¾®è°ƒæ€§èƒ½**  
   åœ¨ 3-bit ä¸‹ï¼ŒLoPRo çš„è¡¨ç°å·²éå¸¸æ¥è¿‘éœ€å¾®è°ƒçš„ SOTA æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶é‡å»ºèƒ½åŠ›çš„å¼ºå¤§ã€‚

4. **é«˜åº¦å¯æ‰©å±•è‡³ MoE æ¶æ„**  
   åœ¨ Mixtral-8x7B ä¸ŠæˆåŠŸéƒ¨ç½²ï¼Œé‡åŒ–æ—¶é—´çŸ­ï¼ˆ<2.5hï¼‰ï¼Œæ€§èƒ½åè¶…æ›´é«˜æ¯”ç‰¹çš„ MoEQuant++ï¼Œå±•ç°å‡ºå·¥ä¸šçº§å®ç”¨æ€§ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ç›®å‰ä»…æ”¯æŒæƒé‡é‡åŒ–**ï¼šæœªæ¶‰åŠ activation quantization å’Œ KV-cache quantizationã€‚
- **ä¾èµ–æ ¡å‡†é›†ç»Ÿè®¡ç‰¹æ€§**ï¼šè™½ç„¶å¯¹ä¸åŒæ ¡å‡†é›†ï¼ˆc4/Pile/WikiText2ï¼‰é²æ£’ï¼Œä½†ä»éœ€ä¸€å®šä»£è¡¨æ€§æ•°æ®ã€‚
- **ç†è®ºè¾¹ç•Œå‡è®¾è¾ƒå¼º**ï¼šå¦‚è¾“å…¥ä¸­å¿ƒåŒ–ã€è¯¯å·®æ­£æ¯”äºæƒé‡ç­‰ï¼Œåœ¨æç«¯åˆ†å¸ƒä¸‹å¯èƒ½å¤±æ•ˆã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- æ‰©å±•è‡³ **activation å’Œ KV-cache é‡åŒ–**ï¼Œæ„å»ºå…¨é‡åŒ– pipeline
- æ¢ç´¢ **rotation ä¸ LoRA åˆå§‹åŒ–çš„è”åˆä¼˜åŒ–**ï¼ŒåŠ é€Ÿä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ
- å°† **R1SVD åº”ç”¨äºå¤§è§„æ¨¡è®­ç»ƒä¸­çš„æ¢¯åº¦å‹ç¼©**
- è®¾è®¡ **è‡ªé€‚åº”å—å¤§å°ä¸ç§©é€‰æ‹©æœºåˆ¶**ï¼Œå®ç°åŠ¨æ€èµ„æºåˆ†é…

---

> ğŸ“Œ **ä¸€å¥è¯æ€»ç»“**ï¼š  
> **LoPRo é€šè¿‡â€œç½®æ¢+å±€éƒ¨æ—‹è½¬+è½»é‡ä½ç§©åˆ†è§£â€ï¼Œåœ¨æ— éœ€å¾®è°ƒçš„å‰æä¸‹å®ç°äº† SOTA çš„ä½æ¯”ç‰¹é‡åŒ–æ€§èƒ½ï¼Œå…¼å…·é«˜ç²¾åº¦ã€é«˜é€Ÿåº¦ä¸ä½å¼€é”€ï¼Œä¸ºå¤§æ¨¡å‹è¾¹ç¼˜éƒ¨ç½²æä¾›äº†å¼ºæœ‰åŠ›çš„æ–°å·¥å…·ã€‚**

</details>

---

### 10. [LLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties](https://arxiv.org/abs/2601.18846)

**Authors**: Urban Skvorc, Niki van Stein, Moritz Seiler, Britta Grimme, Thomas B\"ack, Heike Trautmann  
**Category**: cs.AI  
**Published**: 2026-01-28  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.18846v1  

#### Abstract
Benchmarking in continuous black-box optimisation is hindered by the limited structural diversity of existing test suites such as BBOB. We explore whether large language models embedded in an evolutionary loop can be used to design optimisation problems with clearly defined high-level landscape char...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šLLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰è¿ç»­é»‘ç›’ä¼˜åŒ–ï¼ˆcontinuous black-box optimizationï¼‰é¢†åŸŸçš„åŸºå‡†æµ‹è¯•é¢ä¸´**ç»“æ„æ€§å¤šæ ·æ€§ä¸è¶³**çš„é—®é¢˜ã€‚ä¸»æµæµ‹è¯•å¥—ä»¶å¦‚ **BBOB** è¦†ç›–çš„æ™¯è§‚ç‰¹å¾ç©ºé—´æœ‰é™ï¼Œå¯¼è‡´ï¼š
- è‡ªåŠ¨åŒ–ç®—æ³•é€‰æ‹©ï¼ˆAASï¼‰æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ï¼›
- æ–°ç®—æ³•è¯„ä¼°ä¾èµ–äºè®­ç»ƒ/æµ‹è¯•åˆ’åˆ†æ–¹å¼ï¼Œç¼ºä¹é²æ£’æ€§ã€‚

è¯¥è®ºæ–‡æ—¨åœ¨é€šè¿‡ç”Ÿæˆå…·æœ‰**å¯æ§é«˜é˜¶æ™¯è§‚ç‰¹æ€§**ï¼ˆhigh-level landscape propertiesï¼‰çš„æ–°ä¼˜åŒ–é—®é¢˜ï¼Œæ¥æ‰©å±•ç°æœ‰åŸºå‡†é›†åˆçš„è¦†ç›–èŒƒå›´ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

ä½œè€…æå‡ºå°† **Large Language Model-driven Evolutionary Algorithm (LLaMEA)** æ¡†æ¶ç”¨äº**è‡ªåŠ¨è®¾è®¡ä¼˜åŒ–é—®é¢˜æœ¬èº«**ï¼Œè€Œéä¼ ç»Ÿåœ°ç”¨äºè®¾è®¡æ±‚è§£ç®—æ³•ã€‚

#### æ ¸å¿ƒæ–¹æ³•æµç¨‹ï¼ˆä¸‰æ­¥æ³•ï¼‰ï¼š
1. **Feature Model Definition**  
   åŸºäº BBOB æ•°æ®é›†è®­ç»ƒ XGBoost åˆ†ç±»å™¨ï¼Œé¢„æµ‹äº”ä¸ªé«˜é˜¶å±æ€§ï¼š
   - Multimodalityï¼ˆå¤šæ¨¡æ€ï¼‰
   - Separabilityï¼ˆå¯åˆ†æ€§ï¼‰
   - Basin-size Homogeneityï¼ˆå¸å¼•åŸŸå¤§å°å‡åŒ€æ€§ï¼‰
   - Search-space Homogeneityï¼ˆæœç´¢ç©ºé—´å‡åŒ€æ€§ï¼‰
   - Global-to-local Optima Contrastï¼ˆå…¨å±€ä¸å±€éƒ¨æœ€ä¼˜å¯¹æ¯”åº¦ï¼‰

2. **LLM-driven Problem Generation**  
   åˆ©ç”¨ LLaMEA æ¡†æ¶ä¸­çš„ LLMï¼ˆå¦‚ `gpt5-nano`ï¼‰ï¼Œåœ¨è‡ªç„¶è¯­è¨€æç¤ºå¼•å¯¼ä¸‹ç”Ÿæˆ Python å½¢å¼çš„ä¼˜åŒ–å‡½æ•°ä»£ç ã€‚è¿›åŒ–è¿‡ç¨‹ä¸­ä»¥ ELA ç‰¹å¾é¢„æµ‹å¾—åˆ†ä½œä¸º fitnessã€‚

3. **Verification & Filtering**  
   ä½¿ç”¨åŸºäº **basin-of-attraction analysis** å’Œç»Ÿè®¡æ£€éªŒçš„æ–¹æ³•éªŒè¯ç”Ÿæˆé—®é¢˜æ˜¯å¦çœŸæ­£å…·å¤‡ç›®æ ‡å±æ€§ï¼Œå¹¶è¿›è¡Œè¿‡æ»¤ã€‚

#### åˆ›æ–°ç‚¹æ€»ç»“ï¼š
| åˆ›æ–°ç»´åº¦ | å†…å®¹ |
|--------|------|
| **é¦–æ¬¡åº”ç”¨** | é¦–æ¬¡å°† LLM é©±åŠ¨çš„è¿›åŒ–è®¡ç®—ç”¨äº**ç”Ÿæˆå…·æœ‰æŒ‡å®šé«˜é˜¶ç‰¹æ€§çš„è¿ç»­ä¼˜åŒ–é—®é¢˜** |
| **å¯è§£é‡Šæ§åˆ¶** | é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ§åˆ¶ç›®æ ‡å±æ€§ï¼ˆå¦‚â€œç”Ÿæˆä¸€ä¸ªé«˜åº¦å¤šæ¨¡æ€ä¸”å¯åˆ†çš„é—®é¢˜â€ï¼‰ï¼Œæ¯”çº¯æ•°å€¼ç‰¹å¾åŒ¹é…æ›´ç›´è§‚ |
| **å¤šæ ·æ€§å¢å¼ºæœºåˆ¶** | å¼•å…¥åŸºäº **ELA-space çš„ fitness sharing** æœºåˆ¶ï¼Œé˜²æ­¢ç§ç¾¤é™·å…¥ç›¸ä¼¼æ™¯è§‚ï¼Œæå‡ç”Ÿæˆå¤šæ ·æ€§ |
| **å¼€æ”¾èµ„æºæä¾›** | å‘å¸ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¯å¤ç°ã€å¸¦æ ‡ç­¾çš„æ–°å‹ä¼˜åŒ–é—®é¢˜åº“ï¼Œæ”¯æŒä¸‹æ¸¸ä»»åŠ¡ |

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³•ç±»å‹ | å±€é™æ€§ | æœ¬æ–‡ä¼˜åŠ¿ |
|--------|-------|---------|
| Affine combination of BBOB functions [5,28] | å¤šæ ·æ€§æœ‰é™ï¼Œä»…æ˜¯å·²æœ‰å‡½æ•°çš„çº¿æ€§ç»„åˆ | ç”Ÿæˆå…¨æ–°ç»“æ„ï¼Œéä»¿å°„å˜æ¢äº§ç‰© |
| Tree-based random generators [19,25] | éš¾ä»¥æ§åˆ¶ç‰¹å®šé«˜é˜¶å±æ€§ | å¯å®šå‘ç”ŸæˆæŒ‡å®šå±æ€§ç»„åˆ |
| GP-based ELA matching [15] | æ•°å€¼ç›®æ ‡éœ€ä¸“å®¶è®¾å®šï¼Œä¸å¯è¯» | ä½¿ç”¨è¯­ä¹‰çº§å±æ€§æè¿°ï¼Œæ›´å…·å¯è§£é‡Šæ€§ |
| LLM-EBG [16]ï¼ˆå”¯ä¸€åŒç±»å·¥ä½œï¼‰ | ä»¥ç®—æ³•æ€§èƒ½å·®å¼‚ä¸º fitnessï¼Œåå‘ç‰¹å®š solver | ä»¥æ™¯è§‚å±æ€§ä¸ºç›®æ ‡ï¼Œæ›´é€šç”¨ã€å…¬å¹³ |

> âœ… æ€»ç»“ï¼šæœ¬æ–¹æ³•å®ç°äº†**å¯è§£é‡Šã€å¯æ§ã€å¤šæ ·åŒ–**çš„é—®é¢˜ç”Ÿæˆï¼Œå¡«è¡¥äº† BBOB åœ¨æŸäº›åŒºåŸŸçš„ç©ºç™½ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š ä½¿ç”¨çš„æ•°æ®é›†
- **BBOB benchmark suite**ï¼šå…± 24 ç±»æ ‡å‡†å‡½æ•°ï¼Œæ¯ç±»å–å‰ 50 ä¸ªå®ä¾‹ï¼ˆå…± 1200 ä¸ª/ç»´åº¦ï¼‰
- ç»´åº¦è®¾ç½®ï¼šD âˆˆ {2, 5, 10}
- é‡‡æ ·ç­–ç•¥ï¼šLatin Hypercube Samplingï¼Œæ ·æœ¬é‡ = 250 Ã— D
- å·¥å…·åŒ…ï¼šä½¿ç”¨ `pflacco` æå– ELA ç‰¹å¾ï¼ˆela_meta, ela_distr, nbc, disp, pca, ic ç­‰ç»„ï¼‰

---

### âš™ï¸ å®éªŒè®¾ç½®

| ç»„ä»¶ | è®¾ç½®è¯´æ˜ |
|-----|----------|
| **LLM å€™é€‰æ¨¡å‹** | å¯¹æ¯”äº† 5 ç§ LLMï¼š<br>- `devstral_24b`,<br>- `gemini-2.0-flash`,<br>- `gpt5-nano`ï¼ˆæœ€ç»ˆé€‰ç”¨ï¼‰,<br>- `qwen2.5-coder_14b`,<br>- `math-stral_7b` |
| **Population å‚æ•°** | Î¼ = 8ï¼ˆparentï¼‰ï¼ŒÎ» = 16ï¼ˆoffspringï¼‰ï¼Œé‡‡ç”¨ (Î¼, Î»)-strategyï¼ˆæ— ç²¾è‹±ä¿ç•™ï¼‰ |
| **Prompt è®¾è®¡** | Mutation prompt æ˜ç¡®å¼•ç”¨ç›®æ ‡å±æ€§ï¼ˆå¦‚ â€œmake it more multimodalâ€ï¼‰ |
| **Fitness å®šä¹‰** | å•å±æ€§ï¼šå¯¹åº” XGBoost æ¨¡å‹è¾“å‡ºæ¦‚ç‡<br>åŒå±æ€§ï¼šä¸¤ä¸ªå±æ€§å¾—åˆ†çš„å‡å€¼ |
| **Fitness Sharing** | åœ¨ z-standardized ELA å‘é‡ç©ºé—´ä¸­ä½¿ç”¨ Manhattan è·ç¦»è®¡ç®—å¯†åº¦æƒ©ç½šé¡¹ï¼š<br>$ F_{shared} = F / \sum_j \max(0, 1 - D_{ELA}(f_i, f_j)/\sigma_{share}) $<br>å…¶ä¸­ $\sigma_{share}$ åŠ¨æ€é€‚åº”ç¾¤ä½“å¹³å‡è·ç¦» |

---

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡

| è¯„ä¼°é˜¶æ®µ | æŒ‡æ ‡ |
|--------|------|
| **åœ¨çº¿è¯„ä¼°ï¼ˆgenerationï¼‰** | XGBoost æ¨¡å‹å¯¹äº”é¡¹é«˜é˜¶å±æ€§çš„é¢„æµ‹åˆ†æ•°ï¼ˆ[0,1] åŒºé—´ï¼‰ |
| **ç¦»çº¿éªŒè¯ï¼ˆverificationï¼‰** | åŸºäº basin-of-attraction çš„å®šé‡åˆ†æï¼š<br>- å±€éƒ¨æœ€ä¼˜æ•°é‡ï¼ˆmultimodalityï¼‰<br>- æœ€ä¼˜ä¸å¹³å‡å±€éƒ¨æœ€ä¼˜çš„ fitness å·®å¼‚ï¼ˆglobal-local contrastï¼‰<br>- æœ€å¤§/æœ€å°å¸å¼•åŸŸæ¯”ä¾‹ï¼ˆbasin-size ratioï¼‰ |
| **å¯è§†åŒ–åˆ†æ** | t-SNE æŠ•å½±åˆ°äºŒç»´ç©ºé—´ï¼Œè§‚å¯Ÿç”Ÿæˆé—®é¢˜æ˜¯å¦å¡«è¡¥ BBOB ç©ºç™½åŒº |
| **ç»Ÿè®¡æ£€éªŒ** | Mann-Whitney U test æ£€éªŒä¸¤ç»„åˆ†å¸ƒæ˜¾è‘—æ€§å·®å¼‚ï¼ˆÎ±=0.01ï¼‰ |

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
è™½ç„¶æ²¡æœ‰ç›´æ¥ç«¯åˆ°ç«¯å¯¹æ¯”å…¶ä»–ç”Ÿæˆå™¨ï¼Œä½†æ–‡ä¸­æ˜ç¡®æŒ‡å‡ºä¸ä»¥ä¸‹æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ï¼š
- **MuÃ±oz et al. [15]**ï¼šåŸºäº GP + ELA æ•°å€¼åŒ¹é… â†’ ç¼ºä¹è¯­ä¹‰æ§åˆ¶
- **Vermetten et al. [28]**ï¼šaffine ç»„åˆ â†’ ç»“æ„å—é™
- **LLM-EBG [16]**ï¼šfitness åŸºäºç®—æ³•æ€§èƒ½å·®è· â†’ æœ‰åè§å¯¼å‘

> æœ¬æ–‡å¼ºè°ƒçš„æ˜¯**èŒƒå¼è½¬å˜**ï¼šä»â€œæ•°å€¼ç‰¹å¾æ‹Ÿåˆâ€è½¬å‘â€œè¯­ä¹‰å±æ€§é©±åŠ¨â€ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰LLM é€‰æ‹©å®éªŒï¼ˆå›¾3ï¼‰
| LLM æ¨¡å‹ | Basin-size Homog. + Separability æˆåŠŸç‡ï¼ˆF > 0.5ï¼‰ | Multimodality + Global Structure æˆåŠŸç‡ |
|--------|-----------------------------|-------------------------------|
| `gpt5-nano` | **98.7%** âœ… | >80%ï¼ˆTop 4ï¼‰ |
| `qwen2.5-coder_14b` | ~90% | ~75% |

âœ… æœ€ç»ˆé€‰å®š `gpt5-nano`ï¼šåœ¨æ€§èƒ½ã€æˆæœ¬ã€å¹¶è¡Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚

---

#### ï¼ˆ2ï¼‰Fitness Sharing æ¶ˆèå®éªŒï¼ˆå›¾4ï¼‰
æ¯”è¾ƒå¼€å¯/å…³é—­ ELA-space fitness sharing çš„æ•ˆæœï¼š

| æ¨¡å‹ | æ˜¯å¦å¯ç”¨ sharing | è¿‘é‚» ELA è·ç¦»ä¸­ä½æ•°è¶‹åŠ¿ |
|------|------------------|------------------------|
| `qwen2.5-coder_14b` | å¦ | è¾ƒä½ |
| `qwen2.5-coder_14b` | æ˜¯ | â†‘ æå‡çº¦ 30â€“50% |
| `gpt5-nano` | å¦ | ä¸­ç­‰ |
| `gpt5-nano` | æ˜¯ | â†‘â†‘ æ˜¾è‘—å³ç§»ï¼Œå³°å€¼æ˜æ˜¾å¢å¤§ |

ğŸ‘‰ è¡¨æ˜ fitness sharing æœ‰æ•ˆæå‡äº†ç”Ÿæˆæ™¯è§‚çš„å¤šæ ·æ€§ã€‚

---

#### ï¼ˆ3ï¼‰å±æ€§éªŒè¯ç»“æœï¼ˆå›¾6ï¼‰

##### a. **Multimodality**
- æ‰€æœ‰è¢«ä¼˜åŒ–ä¸ºâ€œå¤šæ¨¡æ€â€çš„ LLaMEA é—®é¢˜å‡æ£€æµ‹å‡º â‰¥2 ä¸ª basinï¼›
- BBOB ä¸­å¤šæ•°ä¸ºå•å³°ï¼Œä»…å°‘æ•°ï¼ˆå¦‚ Step Ellipsoidalï¼‰å›  plateau è¢«è¯¯åˆ¤ä¸ºå¤šå³°ï¼›
- p-value = 7.49e-30 << 0.01 â†’ åˆ†å¸ƒæ˜¾è‘—ä¸åŒã€‚

##### b. **Global-to-local Optima Contrast**
- LLaMEA ç”Ÿæˆçš„é—®é¢˜åœ¨æœ€ä¼˜ä¸å¹³å‡å±€éƒ¨æœ€ä¼˜ä¹‹é—´çš„ fitness å·®æ›´å¤§ï¼›
- å½“ fitness score F â‰¥ 0.99 æ—¶ï¼Œä»…æœ‰ 12% å‡ºç°å° fitness gap â†’ è¡¨æ˜ fitness score ä¸çœŸå®å±æ€§å¼ºç›¸å…³ï¼›
- p-value = 1.59e-05 < 0.01ã€‚

##### c. **Basin-size Homogeneity**
- ä¼˜åŒ–åçš„å‡½æ•° basin size ratio æ›´æ¥è¿‘ 1ï¼ˆå³æ›´å‡åŒ€ï¼‰ï¼›
- å°½ç®¡é«˜å¤šæ¨¡æ€æ€§å¸¦æ¥ä¼°è®¡å›°éš¾ï¼Œä½†ä»èƒ½è§‚å¯Ÿåˆ°è¶‹åŠ¿ï¼›
- p-value = 1.52e-16 < 0.01ã€‚

> â—ä¾‹å¤–ï¼šSeparability å’Œ Search-space Homogeneity æœªåšç»Ÿè®¡æ£€éªŒï¼Œå‰è€…å› æ•°å­¦å®šä¹‰æ˜ç¡®ï¼Œåè€…ç¼ºä¹é‡åŒ–æŒ‡æ ‡ã€‚

---

#### ï¼ˆ4ï¼‰t-SNE å¯è§†åŒ–ï¼ˆå›¾7ï¼‰
- **Trustworthiness (k=10) â‰ˆ 0.997** â†’ å±€éƒ¨ç»“æ„ä¿æŒè‰¯å¥½ï¼›
- LLaMEA ç”Ÿæˆçš„é—®é¢˜ï¼š
  - å¹¶æœªå½¢æˆå­¤ç«‹ç°‡ï¼›
  - è€Œæ˜¯**è‡ªç„¶å»¶ä¼¸ BBOB çš„åˆ†å¸ƒè¾¹ç•Œ**ï¼Œå¡«å……å…¶æœªè¦†ç›–åŒºåŸŸï¼›
- â€œOtherâ€ ç±» BBOB é—®é¢˜é›†ä¸­åœ¨å·¦ä¸‹è§’ï¼ˆä¸»è¦æ˜¯ç®€å•å•å³°å‡½æ•°ï¼‰ï¼Œè€Œ LLaMEA è¦†ç›–æ›´å¤æ‚åŒºåŸŸã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°

1. **LLM å¯ç”¨äºé«˜è´¨é‡ä¼˜åŒ–é—®é¢˜ç”Ÿæˆ**  
   LLaMEA æ¡†æ¶èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆç¬¦åˆé¢„æœŸé«˜é˜¶å±æ€§çš„è¿ç»­ä¼˜åŒ–é—®é¢˜ï¼Œä¸”å¯é€šè¿‡ ELA é¢„æµ‹å™¨å’Œ basin analysis éªŒè¯ã€‚

2. **ç”Ÿæˆé—®é¢˜æ˜¾è‘—æ‰©å±•äº† BBOB å®ä¾‹ç©ºé—´**  
   t-SNE å¯è§†åŒ–æ˜¾ç¤ºæ–°é—®é¢˜å¡«è¡¥äº†åŸæœ‰ç©ºç™½ï¼Œå¢å¼ºäº†åŸºå‡†é›†åˆçš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§ã€‚

3. **fitness sharing æ˜¾è‘—æå‡å¤šæ ·æ€§**  
   ELA-space çš„ niching æœºåˆ¶æœ‰æ•ˆé¿å…æ¨¡å¼åç¼©ï¼Œä¿ƒè¿›æ¢ç´¢ä¸åŒç±»å‹çš„æ™¯è§‚ç»“æ„ã€‚

4. **å±æ€§ç»„åˆç”Ÿæˆç‹¬ç‰¹é—®é¢˜**  
   æ”¯æŒå¤šå±æ€§è”åˆä¼˜åŒ–ï¼ˆå¦‚â€œæ—¢å¤šæ¨¡åˆå¯åˆ†â€ï¼‰ï¼Œäº§ç”Ÿåœ¨ BBOB ä¸­ç½•è§ç”šè‡³ä¸å­˜åœ¨çš„é—®é¢˜ç±»å‹ã€‚

5. **é«˜ fitness score ä¸çœŸå®å±æ€§æ­£ç›¸å…³**  
   ç‰¹åˆ«æ˜¯åœ¨ global-local contrast ä¸Šï¼Œscore è¶Šé«˜ï¼Œå®é™…å·®å¼‚è¶Šå¤§ï¼Œè¯´æ˜åé¦ˆæœºåˆ¶æœ‰æ•ˆã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§

| å±€é™ | è¯´æ˜ |
|------|------|
| **ç»´åº¦é™åˆ¶** | Basin-of-attraction åˆ†æä»…åœ¨ 2D ä¸Šæ‰§è¡Œï¼Œé«˜ç»´éš¾ä»¥å‡†ç¡®è¯†åˆ«å¸å¼•åŸŸ |
| **åˆ†ç¦»æ€§è¯„ä¼°åå·®** | Separability ä½¿ç”¨æ•°å€¼å·®åˆ†æ³•åˆ¤æ–­å˜é‡äº¤äº’ï¼Œå¯èƒ½å—é‡‡æ ·è¯¯å·®å½±å“ |
| **LLM ä¸ç¡®å®šæ€§** | ä¸åŒ LLM è¾“å‡ºè´¨é‡å·®å¼‚å¤§ï¼Œç»“æœå¯¹æ¨¡å‹é€‰æ‹©æ•æ„Ÿ |
| **è®¡ç®—å¼€é”€å¤§** | å°¤å…¶æ˜¯éªŒè¯é˜¶æ®µéœ€è¦å¯†é›†é‡‡æ ·å’Œè·¯å¾„è¿½è¸ªï¼Œä¸é€‚åˆå®æ—¶ä½¿ç”¨ |
| **éƒ¨åˆ†å±æ€§éš¾é‡åŒ–** | å¦‚ search-space homogeneity ç¼ºä¹ç»Ÿä¸€é‡åŒ–æ ‡å‡†ï¼Œä¾èµ–è§†è§‰åˆ¤æ–­ |

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

1. **ç³»ç»Ÿæ€§æ¯”è¾ƒä¸åŒé—®é¢˜ç”Ÿæˆå™¨**  
   å°† LLaMEA ä¸å…¶ä»–æ–¹æ³•ï¼ˆGPã€affineã€RandOptGenï¼‰åœ¨ç›¸åŒè¯„ä¼°æ¡†æ¶ä¸‹æ¨ªå‘å¯¹æ¯”ã€‚

2. **ç ”ç©¶ LLM æ•æ„Ÿæ€§**  
   æ¢ç´¢ prompt engineeringã€temperatureã€top-k ç­‰å‚æ•°å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“ã€‚

3. **åº”ç”¨äºè‡ªåŠ¨åŒ–ç®—æ³•é€‰æ‹©ï¼ˆAASï¼‰éªŒè¯**  
   æµ‹è¯•è¿™äº›æ–°ç”Ÿæˆçš„é—®é¢˜æ˜¯å¦èƒ½æå‡ AAS æ¨¡å‹åœ¨æœªçŸ¥é—®é¢˜ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

4. **å¼•å…¥çº¦æŸæ¡ä»¶æˆ–å¤šç›®æ ‡æ‰©å±•**  
   å°†æ–¹æ³•æ¨å¹¿è‡³ constrained æˆ– multi-objective optimization åœºæ™¯ã€‚

5. **æ„å»ºåŠ¨æ€æ¼”åŒ–çš„é—®é¢˜åº“**  
   æŒç»­è¿è¡Œ LLaMEA å¾ªç¯ï¼Œè‡ªé€‚åº”å¡«å……å½“å‰ç®—æ³•è¡¨ç°æœ€å·®çš„åŒºåŸŸã€‚

---

### ğŸ“¦ å¼€æºèµ„æº
æ‰€æœ‰ç”Ÿæˆçš„é—®é¢˜å’Œä»£ç å·²å…¬å¼€å‘å¸ƒï¼š
ğŸ”— [Zenodo Reproducibility Package](https://doi.org/10.5281/zenodo.18306723)

> åŒ…å«æ˜“ç”¨çš„ Python åº“ï¼Œä¾¿äºç¤¾åŒºç”¨äº benchmarkingã€ELA ç ”ç©¶å’Œ AAS å¼€å‘ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯
> æœ¬æ–‡å¼€åˆ›æ€§åœ°åˆ©ç”¨ LLM + è¿›åŒ–è®¡ç®—æ¡†æ¶ **LLaMEA**ï¼Œå®ç°äº†**å¯è§£é‡Šã€å¯æ§ã€å¤šæ ·åŒ–**çš„è¿ç»­ä¼˜åŒ–é—®é¢˜ç”Ÿæˆï¼ŒæˆåŠŸæ‰©å±•äº† BBOB åŸºå‡†é›†çš„æ™¯è§‚å¤šæ ·æ€§ï¼Œå¹¶æä¾›äº†é¦–ä¸ªå¤§è§„æ¨¡ã€å¸¦å±æ€§æ ‡ç­¾çš„ LLM ç”Ÿæˆé—®é¢˜åº“ã€‚

</details>

---

### 11. [PROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems](https://arxiv.org/abs/2601.19402)

**Authors**: Amit Singh Bhatti, Vishal Vaddina, Dagnachew Birru  
**Category**: cs.AI  
**Published**: 2026-01-28  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.19402v1  

#### Abstract
Production LLM deployments serve diverse workloads where cost and quality requirements vary by customer tier, time of day, and query criticality. Model serving systems accept latency SLOs directly. LLM routers do not. They force operators to tune parameters offline and guess what accuracy might resu...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šPROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰çš„ **LLM è·¯ç”±ç³»ç»Ÿ**ï¼ˆLLM routersï¼‰æ— æ³•ç›´æ¥æ¥å—å¹¶æ»¡è¶³ç”¨æˆ·çš„ **å‡†ç¡®æ€§ç›®æ ‡ï¼ˆaccuracy targetsï¼‰**ï¼Œè€Œæ˜¯ä¾èµ–äºç¦»çº¿å‚æ•°è°ƒä¼˜ï¼ˆå¦‚è®¾å®šâ€œ30% æŸ¥è¯¢èµ° GPT-4â€ï¼‰ï¼Œå¯¼è‡´ï¼š
- å‡†ç¡®æ€§ä¸å‚æ•°ä¹‹é—´çš„å…³ç³»æ˜¯**é—´æ¥ã€éå•è°ƒä¸”æ•°æ®é›†ä¾èµ–**çš„ï¼›
- è¿ç»´äººå‘˜éš¾ä»¥å°†ä¸šåŠ¡éœ€æ±‚ï¼ˆå¦‚â€œè¾¾åˆ° 90% å‡†ç¡®ç‡â€ï¼‰è½¬åŒ–ä¸ºå…·ä½“é…ç½®ï¼›
- éš¾ä»¥æ”¯æŒå¤šçº§æœåŠ¡ï¼ˆtiered SLAï¼‰ï¼Œä¾‹å¦‚ä¸ºä¸åŒå®¢æˆ·ç¾¤ä½“æä¾›å·®å¼‚åŒ–è´¨é‡ä¿éšœã€‚

å°½ç®¡å·²æœ‰æ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼ˆå¦‚ Clipperã€INFaaSï¼‰æ”¯æŒå»¶è¿Ÿ SLO æˆ–å‡†ç¡®ç‡è¾“å…¥ï¼Œä½†å®ƒä»¬ä¸é€‚ç”¨äº**å¼‚æ„å¤§è¯­è¨€æ¨¡å‹æ± ä¸­çš„åŠ¨æ€è·¯ç”±å†³ç­–**ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šPROTEUS
ä½œè€…æå‡º **PROTEUS**ï¼ˆPolymorphic Router for Operational Target Enforcement with Unified SLAï¼‰ï¼Œä¸€ä¸ªèƒ½å¤Ÿ**åœ¨è¿è¡Œæ—¶ç›´æ¥æ¥æ”¶å‡†ç¡®æ€§ç›®æ ‡ T å¹¶ç¡®ä¿å…¶è¢«æ»¡è¶³**çš„ LLM è·¯ç”±å™¨ã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
1. **T-conditioned Policy Network**  
   å°†ç›®æ ‡å‡†ç¡®ç‡ $ T \in [0.85, 0.95] $ ä½œä¸ºç­–ç•¥ç½‘ç»œçš„è¾“å…¥ï¼Œåœ¨æ¨ç†é˜¶æ®µå®ç°**å•æ¨¡å‹è¦†ç›–å…¨ç²¾åº¦è°±ç³»**ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å‚æ•°æ‰«æã€‚

2. **Lagrangian Dual Control with Learned å…¥**  
   åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥å¯å­¦ä¹ çš„æ‹‰æ ¼æœ—æ—¥å¯¹å¶å˜é‡ $ \lambda $ æ¥åŠ¨æ€æƒ©ç½šè¿åçº¦æŸçš„è¡Œä¸ºï¼ˆå³å®é™… accuracy < Tï¼‰ã€‚è¯¥æœºåˆ¶é€šè¿‡åé¦ˆå›è·¯æ•™ä¼šç­–ç•¥ç½‘ç»œå»ºç«‹ $ T \rightarrow u $ï¼ˆquality preferenceï¼‰çš„æ˜ å°„ã€‚

3. **è¿ç»­è´¨é‡åå¥½è¾“å‡ºï¼ˆContinuous Quality Preferenceï¼‰**  
   ç­–ç•¥è¾“å‡ºçš„æ˜¯è¿ç»­å€¼ $ u \in [0,1] $ï¼Œè¡¨ç¤ºå¯¹é«˜è´¨é‡æ¨¡å‹çš„å€¾å‘ç¨‹åº¦ï¼Œè€Œéç¡¬æ€§é€‰æ‹©æŸæ¨¡å‹ï¼Œä»è€Œå®ç°å¹³æ»‘æ’å€¼å’Œæ›´çµæ´»çš„æˆæœ¬-è´¨é‡æƒè¡¡ã€‚

4. **è‡ªé€‚åº”è·¯ç”±è¯„åˆ†å‡½æ•°ï¼ˆAdaptive Routing Scoreï¼‰**  
   è®¾è®¡éçº¿æ€§æ‰“åˆ†å‡½æ•°ï¼š
   $$
   S_i = p_i + \beta \cdot b_i - (1 - p_i)^\gamma \cdot c_i
   $$
   å…¶ä¸­ $ \gamma $ æ˜¯å¯å­¦ä¹ çš„æˆæœ¬æ•æ„Ÿå‚æ•°ï¼Œå…è®¸ç³»ç»Ÿæ ¹æ®ä¸åŒéƒ¨ç½²ç¯å¢ƒè‡ªåŠ¨è°ƒæ•´æˆæœ¬æƒé‡ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ CARROTã€OmniRouterï¼‰ | PROTEUS |
|------|-------------------------------|--------|
| æ˜¯å¦æ”¯æŒè¿è¡Œæ—¶ accuracy è¾“å…¥ | å¦ï¼ˆéœ€é¢„è®¾æ¨¡å¼æˆ–é‡è®­ç»ƒï¼‰ | âœ… æ˜¯ï¼ˆç›´æ¥è¾“å…¥ Tï¼‰ |
| æ˜¯å¦ä¿è¯ floor complianceï¼ˆacc â‰¥ Tï¼‰ | âŒ å¦ï¼ˆä»… 0â€“22% è¾¾æ ‡ï¼‰ | âœ… æ˜¯ï¼ˆ100% è¾¾æ ‡ï¼‰ |
| å‚æ•°è°ƒèŠ‚æ–¹å¼ | æ‰‹åŠ¨è°ƒå‚ / å¤šæ¨¡å‹éƒ¨ç½² | å•ä¸€æ¨¡å‹ + åŠ¨æ€ T è¾“å…¥ |
| æˆæœ¬èŠ‚çº¦èƒ½åŠ› | ä¸­ç­‰ï¼ˆçº¦ 70â€“80%ï¼‰ | â¬†ï¸ é«˜è¾¾ 89.8% |
| é€‚åº”æ€§ | å›ºå®š trade-off | å¯éšè´Ÿè½½ã€æ—¶é—´ã€å®¢æˆ·ç­‰çº§åŠ¨æ€å˜åŒ– |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
ä½¿ç”¨ä¸¤ä¸ªæƒå¨çš„å¤šLLMè·¯ç”±åŸºå‡†æµ‹è¯•é›†ï¼š

| æ•°æ®é›† | æŸ¥è¯¢é‡ | æ¨¡å‹æ•° | åŒ…å«æ¨¡å‹ | Oracle Accuracy | æˆæœ¬èŒƒå›´ |
|-------|--------|--------|----------|------------------|---------|
| **RouterBench** | 405K | 11 | Llama-2, Mistral, GPT-3.5/4, Claude-2 | 91.4% | \$0.0001 â€“ \$0.01/q |
| **SPROUT** | 45K | 14 | GPT-4o, Claude-3.5, o3-mini, Llama up to 405B | 98.6% | \$0.0001 â€“ \$0.05/q |

> ç‰¹ç‚¹äº’è¡¥ï¼šRouterBench å¼ºè°ƒè§„æ¨¡ä¸ä»»åŠ¡å¤šæ ·æ€§ï¼›SPROUT æµ‹è¯•å‰æ²¿æ¨¡å‹ä¸æç«¯æˆæœ¬å·®å¼‚ã€‚

---

### ğŸ§ª å®éªŒè®¾ç½®
- **è®­ç»ƒé…ç½®**ï¼š
  - ç¼–ç å™¨ï¼šDeBERTa-v3-smallï¼ˆ22M å‚æ•°ï¼‰
  - æ‰¹å¤§å°ï¼š32ï¼ˆA100 GPUï¼‰
  - è®­ç»ƒæ­¥æ•°ï¼š10K æ­¥ï¼ˆçº¦ 4 å°æ—¶/æ•°æ®é›†ï¼‰
  - ä½¿ç”¨ PPOï¼ˆProximal Policy Optimizationï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
  - æ‹‰æ ¼æœ—æ—¥ä¹˜å­ $ \lambda $ æ›´æ–°é¢‘ç‡é«˜äºç­–ç•¥ç½‘ç»œï¼ˆâ€œfaster dualâ€ heuristicsï¼‰

- **æ¨ç†å»¶è¿Ÿ**ï¼š
  - å•æŸ¥è¯¢å»¶è¿Ÿ < 8.7msï¼ˆbatch=1ï¼‰ï¼Œæ‰¹é‡å¤„ç†å¯è¾¾ <3ms/query
  - æ˜¾è‘—ä½äº LLM æ¨ç†æœ¬èº«ï¼ˆé€šå¸¸ 2â€“7 ç§’ï¼‰

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡

#### âœ… é€‚åº”æ€§ç›¸å…³æŒ‡æ ‡ï¼ˆAdaptability Metricsï¼‰
| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **T-u Correlation** | è¯·æ±‚çš„ç›®æ ‡ T ä¸ç­–ç•¥è¾“å‡ºçš„è´¨é‡åå¥½ u çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆç†æƒ³ä¸º 1.0ï¼‰ |
| **Floor Compliance (%)** | åœ¨å¤šå°‘æ¯”ä¾‹çš„ T ä¸Šå®ç°äº† $ \text{achieved accuracy} \geq T $ |
| **Tolerance-band Compliance** | åœ¨ Â±2% å’Œ Â±5% å®¹å·®èŒƒå›´å†…åŒ¹é…ç›®æ ‡çš„æ¯”ä¾‹ |

#### ğŸ’° æ•ˆç‡ä¸æ€§èƒ½æŒ‡æ ‡ï¼ˆEfficiency & Performanceï¼‰
| æŒ‡æ ‡ | å®šä¹‰ |
|------|------|
| **Routing Efficiency (RE)** | $ \frac{\text{Acc} - \text{Acc}_{\text{random}}}{\text{Latency}} $ï¼ˆå•ä½ï¼špp/msï¼‰ |
| **Routing Performance Index (RPI)** | ç»¼åˆè€ƒè™‘ Accuracyã€Costã€Latency çš„ç»¼åˆå¾—åˆ†ï¼š<br>$ \text{RPI} = \text{Acc} \times \left(1 - \frac{\text{Cost}}{\text{Cost}_{\max}}\right) \times \left(1 - E_{\text{route}} \times 100\right) $ |

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»åˆ« | æ–¹æ³• |
|------|------|
| **é™æ€ç­–ç•¥** | Always-Best, Always-Cheapest, Random |
| **å­¦ä¹ å‹è·¯ç”±å™¨** | KNN, MLP, CARROTï¼ˆä¸‰ç§æ¨¡å¼ï¼šquality/cost/balancedï¼‰, OmniRouter |
| **æ¶ˆèå˜ä½“** | PROTEUS-ablatedï¼ˆæ— çº¦æŸã€å›ºå®š Î³ã€æ—  criticï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

| æŒ‡æ ‡ | RouterBench | SPROUT |
|------|-------------|--------|
| **T-u Correlation** | 0.973 | 0.981 |
| **Floor Compliance (acc â‰¥ T)** | 100% | 100% |
| **Â±5% Tolerance-band Compliance** | 100% | 67% |
| **PROTEUS Accuracy** | 90.1% | 94.0% |
| **Oracle Accuracy** | 91.4% | 98.6% |
| **ä¸ Oracle å·®è·** | +1.3 pp | +4.6 pp |
| **ç›¸æ¯”æœ€ä½³å›ºå®šæ¨¡å‹æˆæœ¬èŠ‚çœ** | 89.8% | 83% |
| **Routing Efficiency (RE)** | 11.1 pp/ms | 9.5 pp/ms |
| **RPI** | 88.5 | 83.5 |

> æ³¨ï¼šåœ¨ RouterBench ä¸Šï¼ŒPROTEUS çš„ RPI ç”šè‡³ç•¥é«˜äº Oracleï¼ˆ88.5 vs 88.2ï¼‰ï¼Œè¯´æ˜å…¶åœ¨å¾®å° accuracy æŸå¤±ä¸‹å¤§å¹…é™ä½æˆæœ¬ï¼Œæ•´ä½“æ•ˆç‡æ›´é«˜ã€‚

---

### ğŸ” åŠ¨æ€é€‚åº”èƒ½åŠ›éªŒè¯ï¼ˆFigure 2cï¼‰
åœ¨å››ç§åŠ¨æ€ T å˜åŒ–åœºæ™¯ä¸­ï¼ˆStepã€Driftã€Cyclicã€Realisticï¼‰ï¼ŒPROTEUS èƒ½**å³æ—¶å“åº”ç›®æ ‡å˜æ›´**ï¼Œæ— é€‚åº”å»¶è¿Ÿï¼š
- **é›¶ retraining / tuning å¼€é”€**
- è·Ÿè¸ªè¯¯å·®å°ï¼ˆovershoot ä»… 1.6â€“5.2%ï¼‰
- æ”¯æŒ per-queryã€per-user æˆ–åŸºäºè´Ÿè½½çš„å®æ—¶è°ƒæ•´

---

### ğŸ” æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studyï¼‰

| å˜ä½“ | å½±å“ï¼ˆâ–³Accï¼‰ | å…³é”®å‘ç° |
|------|--------------|-----------|
| **-Constraint (Î»)** | â†“0.27% (RB), â†“0.53% (SP) | æœ€å…³é”®ç»„ä»¶ï¼ç§»é™¤å floor compliance å´©æºƒï¼Œç­–ç•¥å˜å¾—è¿‡äº cost-aggressive |
| **-Learnable Î³** | ~0 (RB), â†“0.62% (SP) | å¯¹é«˜æˆæœ¬è·¨åº¦ç¯å¢ƒï¼ˆSPROUT çš„ 9.5Ã—ï¼‰æ›´é‡è¦ï¼Œå¦åˆ™å½±å“å° |
| **-Critic** | â†“<0.05% | å‡ ä¹æ— å½±å“ï¼Œå› è·¯ç”±ä¸º single-step MDPï¼Œcritic éå¿…éœ€ |

> ç»“è®ºï¼š**Lagrangian dual mechanism æ˜¯æ ¸å¿ƒé©±åŠ¨åŠ›**ï¼Œå…¶ä½™æ¨¡å—å¯æ ¹æ®éƒ¨ç½²éœ€æ±‚è£å‰ªã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **é¦–æ¬¡å®ç°â€œæŒ‰éœ€æŒ‡å®š accuracyâ€çš„ LLM è·¯ç”±å™¨**ï¼šOperator å¯ç›´æ¥è®¾ç½® $ T \in [0.85, 0.95] $ï¼ŒPROTEUS èƒ½å¯é åœ°æ»¡è¶³è¯¥ç›®æ ‡ã€‚
2. **å•ä¸€æ¨¡å‹æœåŠ¡æ‰€æœ‰ SLA Tier**ï¼šæ— éœ€ä¸ºæ¯ä¸ªæœåŠ¡è´¨é‡ç­‰çº§è®­ç»ƒç‹¬ç«‹æ¨¡å‹ï¼Œæå¤§é™ä½è¿ç»´å¤æ‚åº¦ã€‚
3. **å¼º floor guarantee èƒ½åŠ›**ï¼šåœ¨æ‰€æœ‰æµ‹è¯• T ä¸‹å‡è¾¾æˆ $ \text{acc} \geq T $ï¼Œåˆè§„ç‡è¾¾ 100%ï¼Œè¿œè¶…åŸºçº¿ï¼ˆOmniRouter ä»… 0â€“22%ï¼‰ã€‚
4. **é«˜æ•ˆç‡ä¸ä½æˆæœ¬å…±å­˜**ï¼šæ¥è¿‘ oracle å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°é«˜è¾¾ 89.8% çš„æˆæœ¬èŠ‚çº¦ã€‚
5. **åŠ¨æ€é€‚åº”æ€§å¼º**ï¼šå¯åœ¨è¿è¡Œæ—¶å®æ—¶å“åº” SLA å˜æ›´ã€è´Ÿè½½æ³¢åŠ¨ã€æ—¶æ®µå®šä»·ç­‰åœºæ™¯ã€‚

---

### âš ï¸ å±€é™æ€§ï¼ˆLimitationsï¼‰
1. **ä¾èµ– ground-truth label è®­ç»ƒ**ï¼šç›®å‰éœ€è¦æ¯æ¡ query çš„æ­£ç¡®ç­”æ¡ˆæ¥ç›‘ç£è®­ç»ƒï¼Œç”Ÿäº§ç¯å¢ƒä¸­å¯èƒ½ä¸å¯å¾—ï¼Œéœ€å€ŸåŠ© LLM-as-a-judge æˆ–äººå·¥æ ‡æ³¨ã€‚
2. **æ•°æ®é›†æ—¶æ•ˆæ€§é™åˆ¶**ï¼šRouterBench ä½¿ç”¨è¾ƒæ—§æ¨¡å‹ï¼ˆå¦‚ GPT-4ã€Claude-2ï¼‰ï¼Œæœªæ¶µç›–æœ€æ–° frontier modelsã€‚
3. **å‹ç¼©çš„ accuracy åˆ†å¸ƒ**ï¼šä¸­æ¡£æ¨¡å‹ä¹‹é—´æ€§èƒ½å·®è·å°ï¼Œä½¿å¾—ç²¾ç»†æ§åˆ¶å¤šä¸ª T ç‚¹è¾ƒéš¾ã€‚
4. **é™æ€æ¨¡å‹æ± å‡è®¾**ï¼šæœªè€ƒè™‘åœ¨çº¿å¢å‡æ¨¡å‹çš„æƒ…å†µï¼Œæœªæ¥éœ€æ”¯æŒåŠ¨æ€æ³¨å†Œã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘ï¼ˆFuture Workï¼‰
1. **Bandit Feedback Learning**ï¼šä»…è§‚å¯Ÿæ‰€é€‰æ¨¡å‹çš„ç»“æœï¼Œå‡å°‘æ ‡æ³¨å¼€é”€ï¼Œæ”¯æŒæŒç»­å­¦ä¹ ã€‚
2. **Latency-aware Routing**ï¼šæ‰©å±•ç›®æ ‡å‡½æ•°ï¼Œè”åˆä¼˜åŒ– accuracyã€cost ä¸ response timeã€‚
3. **Hierarchical Routing**ï¼šå…ˆé€‰ tier å†é€‰ modelï¼Œæå‡å¤§è§„æ¨¡æ¨¡å‹æ± ä¸‹çš„å†³ç­–æ•ˆç‡ã€‚
4. **Multi-objective T Specification**ï¼šåŒæ—¶è®¾ç½® accuracy floor å’Œ cost ceilingï¼Œæ›´å¥½åŒ¹é…çœŸå® SLAã€‚
5. **Multi-turn Conversation Support**ï¼šè€ƒè™‘ä¸Šä¸‹æ–‡ç´¯ç§¯æ•ˆåº”ï¼Œä¼˜åŒ–é•¿å¯¹è¯ä¸­çš„è·¯ç”±ç­–ç•¥ã€‚
6. **æ›´ä¸°å¯Œçš„ benchmark**ï¼šæ„å»ºè·¨è¶Š $[0.5, 0.99]$ accuracy èŒƒå›´çš„å¤šæ ·åŒ–è·¯ç”±æµ‹è¯•é›†ã€‚

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> PROTEUS æˆåŠŸå°† **SLA-aware æ§åˆ¶ç†è®º** ä¸ **æ·±åº¦å¼ºåŒ–å­¦ä¹ ** ç›¸ç»“åˆï¼Œæ‰“é€ äº†ä¸€ä¸ªçœŸæ­£é¢å‘ç”Ÿäº§çš„ã€å¯ç¼–ç¨‹çš„ LLM è·¯ç”±å¼•æ“ï¼Œæ¨åŠ¨ LLM serving ç³»ç»Ÿä»â€œç»éªŒè°ƒå‚â€è¿ˆå‘â€œç›®æ ‡é©±åŠ¨â€çš„æ–°æ—¶ä»£ã€‚

</details>

---

### 12. [Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering](https://arxiv.org/abs/2601.19847)

**Authors**: Fangan Dong, Zuming Yan, Xuri Ge, Zhiwei Xu, Mengqi Zhang, Xuanang Chen, Ben He, Xin Xin, Zhumin Chen, Ying Zhou  
**Category**: cs.CL  
**Published**: 2026-01-28  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.19847v1  

#### Abstract
Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šIdentifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å°½ç®¡å½“å‰çš„ **Large Language Models (LLMs)** åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­ä»å­˜åœ¨**æ¨ç†å¯é æ€§ä¸è¶³**çš„é—®é¢˜ã€‚ä¼ ç»Ÿæå‡å¯é æ€§çš„æ–¹æ³•ä¾èµ–äº **post-training** æˆ– **test-time sampling**ï¼ˆå¦‚ self-consistencyã€prompt engineeringï¼‰ï¼Œè¿™äº›æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ã€æ•ˆç‡ä½ï¼Œä¸”å°†æ¨¡å‹è§†ä¸ºé»‘ç›’ï¼Œç¼ºä¹å¯¹å†…éƒ¨æœºåˆ¶çš„ç†è§£ã€‚

æœ¬æ–‡æå‡ºï¼šèƒ½å¦é€šè¿‡ç›´æ¥å¹²é¢„ LLM å†…éƒ¨çš„ç¥ç»å…ƒæ¿€æ´»æ¥æå‡æ¨ç†å¯é æ€§ï¼Ÿ

### æå‡ºçš„æ–°æ–¹æ³•ï¼šAdaRAS
ä½œè€…æå‡ºäº† **AdaRAS (Adaptive Reasoning Activation Steering)**ï¼Œä¸€ç§**æ— éœ€è®­ç»ƒã€è½»é‡çº§ã€æµ‹è¯•æ—¶å¹²é¢„**çš„æ¡†æ¶ï¼Œç”¨äºæå‡ LLM æ¨ç†çš„å¯é æ€§ã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- å‘ç° LLM ä¸­å­˜åœ¨ä¸€å°éƒ¨åˆ† **Reasoning-Critical Neurons (RCNs)**ï¼Œå…¶æ¿€æ´»çŠ¶æ€ä¸æ¨ç†æ­£ç¡®æ€§é«˜åº¦ç›¸å…³ã€‚
- é€šè¿‡å¯¹æ¯”æ­£ç¡®ä¸é”™è¯¯æ¨ç†è·¯å¾„çš„æ¿€æ´»å·®å¼‚ï¼Œè¯†åˆ«è¿™äº› RCNsã€‚
- åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä»…åœ¨æ£€æµ‹åˆ°å¯èƒ½å¤±è´¥æ—¶ï¼Œé€‰æ‹©æ€§åœ°â€œå¼•å¯¼â€è¿™äº›ç¥ç»å…ƒçš„æ¿€æ´»ï¼Œä»è€Œä¿®æ­£é”™è¯¯è·¯å¾„è€Œä¸ç ´åå·²æ­£ç¡®çš„æ¨ç†ã€‚

#### åˆ›æ–°ç‚¹ï¼š
1. **é¦–æ¬¡ç³»ç»Ÿæ€§è¯æ˜**ï¼šæ¨ç†æ­£ç¡®æ€§å¯é€šè¿‡å°‘é‡ç¥ç»å…ƒçš„æ¿€æ´»å¹²é¢„è¿›è¡Œé¢„æµ‹å’Œæ”¹è¿›ã€‚
2. **æå‡º AdaRAS æ¡†æ¶**ï¼š
   - **RCN è¯†åˆ«**ï¼šåŸºäº **Mean Difference (MD)** å’Œ **ææ€§æ„ŸçŸ¥è¿‡æ»¤ (polarity-aware filtering)**ï¼Œä»å¯¹æ¯”æ¨ç†è½¨è¿¹ä¸­è¯†åˆ«å…³é”®ç¥ç»å…ƒã€‚
   - **è‡ªé€‚åº”å¹²é¢„ (Adaptive Intervention)**ï¼šå¼•å…¥ä¸€ä¸ªè½»é‡çº§é¢„æµ‹å™¨ï¼Œåœ¨æ¨ç†å‰åˆ¤æ–­æ˜¯å¦éœ€è¦å¹²é¢„ï¼Œé¿å…å¯¹æ­£ç¡®æ ·æœ¬é€ æˆè´Ÿé¢å½±å“ã€‚
3. **å¯è¿ç§»æ€§ä¸å¯æ‰©å±•æ€§**ï¼šè¯†åˆ«å‡ºçš„ RCNs å¯è·¨æ•°æ®é›†ã€è·¨ä»»åŠ¡è¿ç§»ï¼Œå¹¶åœ¨æ›´å¼ºæ¨¡å‹ï¼ˆå¦‚ Qwen3-4Bï¼‰ä¸Šä¾ç„¶æœ‰æ•ˆã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³•ç±»å‹ | ç¼ºç‚¹ | AdaRAS ä¼˜åŠ¿ |
|--------|------|------------|
| **Post-training** (SFT, PPO) | éœ€é¢å¤–è®­ç»ƒï¼Œæˆæœ¬é«˜ï¼Œæ³›åŒ–å—é™ | **æ— éœ€è®­ç»ƒ**ï¼Œå³æ’å³ç”¨ |
| **Test-time sampling** (self-consistency) | å¤šæ¬¡é‡‡æ ·ï¼Œè®¡ç®—å¼€é”€å¤§ | **å•æ¬¡æ¨ç†**ï¼Œæ— é¢å¤–é‡‡æ ·æˆæœ¬ |
| **Probing-based steering** | ä¾èµ–æ¢é’ˆæ³›åŒ–èƒ½åŠ›ï¼Œä¸ç¨³å®š | **åŸºäºæ¿€æ´»å·®å€¼**ï¼Œæ›´é²æ£’ |
| **é»‘ç›’æ–¹æ³•** | ç¼ºä¹å¯è§£é‡Šæ€§ | **æä¾›æœºåˆ¶æ€§æ´å¯Ÿ**ï¼Œæ­ç¤ºæ¨ç†å¤±è´¥çš„ç¥ç»åŸºç¡€ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
åœ¨ **10 ä¸ªæ•°å­¦ä¸ç¼–ç¨‹åŸºå‡†**ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–éš¾åº¦è¾ƒé«˜çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼š

| ç±»åˆ« | æ•°æ®é›† |
|------|-------|
| **æ•°å­¦** | AIME-24, AIME-25, AIME-Extend, MATH-500, GSM8K, AMC-12 |
| **ç¼–ç¨‹** | HumanEval, HumanEval+, MBPP, MBPP+ |

å…¶ä¸­ AIME ç³»åˆ—ä¸ºé«˜éš¾åº¦æ•°å­¦ç«èµ›é¢˜ï¼Œæ˜¯ä¸»è¦è¯„ä¼°é‡ç‚¹ã€‚

### å®éªŒè®¾ç½®
- **æ¨¡å‹**ï¼šä»¥ **Qwen3-1.7B** ä¸ºä¸»å¹²æ¨¡å‹ï¼Œéƒ¨åˆ†å®éªŒæ‰©å±•è‡³ **Qwen3-4B**ã€‚
- **æ¨ç†æ–¹å¼**ï¼šä½¿ç”¨ **greedy decoding**ï¼ˆæ¸©åº¦=0ï¼‰ï¼Œç¡®ä¿å¯å¤ç°æ€§ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼š**Accuracy**ï¼ˆç­”æ¡ˆæ­£ç¡®ç‡ï¼‰ã€‚
- **RCN è¯†åˆ«**ï¼š
  - æ„é€ å¯¹æ¯”æ ·æœ¬ï¼šå¯¹æ¯ä¸ªè¾“å…¥ï¼Œé€šè¿‡é«˜æ¸©é‡‡æ ·ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„ï¼ŒæŒ‰ç­”æ¡ˆæ­£ç¡®æ€§åˆ†ä¸ºæ­£è´Ÿæ ·æœ¬å¯¹ã€‚
  - è®¡ç®—æ¯å±‚ç¥ç»å…ƒçš„ **Mean Difference (MD)**ï¼š  
    $$
    S(l,i) = \mathbb{E}[a(l,i,p^+)] - \mathbb{E}[a(l,i,p^-)]
    $$
  - åº”ç”¨ **ææ€§è¿‡æ»¤**ï¼šä»…ä¿ç•™æ¿€æ´»å‡å€¼ç¬¦å·ç›¸åçš„ç¥ç»å…ƒï¼ˆ$ \mathbb{E}[a(p^+)] \cdot \mathbb{E}[a(p^-)] < 0 $ï¼‰ã€‚
  - é€‰å– top-Kï¼ˆé»˜è®¤ K=50ï¼‰æ„å»ºç¨€ç– steering vectorã€‚
- **è‡ªé€‚åº”å¹²é¢„**ï¼š
  - è®­ç»ƒä¸€ä¸ªè½»é‡çº§åˆ†ç±»å™¨ï¼ŒåŸºäºæ—©æœŸæ¿€æ´»é¢„æµ‹æ˜¯å¦éœ€è¦å¹²é¢„ã€‚
  - ä»…å½“é¢„æµ‹ä¸ºâ€œå¤±è´¥â€æ—¶æ‰åº”ç”¨ steeringã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿ç±»å‹ | å…·ä½“æ–¹æ³• |
|--------|--------|
| **Zero-shot åŸºçº¿** | Chain-of-Thought (CoT) |
| **Post-training æ¨¡å‹** | DeepSeek-R1-Distill, Nemotron, OpenThinker |
| **Activation Steering åŸºçº¿** | Probing-based steeringï¼ˆä½¿ç”¨æ¢é’ˆæƒé‡ä½œä¸ºé‡è¦æ€§åˆ†æ•°ï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 2ï¼‰
| æ•°æ®é›† | CoT å‡†ç¡®ç‡ | AdaRAS å‡†ç¡®ç‡ | **æå‡å¹…åº¦** |
|--------|------------|---------------|-------------|
| **AIME-24** | 47.83% | **60.87%** | **+13.04%** |
| **AIME-25** | 40.91% | **54.55%** | **+13.64%** |
| **MATH-500** | 84.80% | **86.40%** | +1.60% |
| **HumanEval** | 77.18% | **79.19%** | +2.01% |
| **MBPP** | 68.78% | **72.22%** | +3.44% |

> âœ… **å¹³å‡æå‡çº¦ 5%**ï¼Œåœ¨æœ€éš¾çš„ AIME ä¸Šæå‡è¶… **13%**ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰å¼€æº post-training æ¨¡å‹ã€‚

### ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **ä¼˜äºæ‰€æœ‰ post-training æ–¹æ³•**ï¼šåœ¨ AIME-24/25 ä¸Šï¼ŒAdaRAS è¶…è¿‡æœ€å¼º post-training æ¨¡å‹ï¼ˆå¦‚ Nemotronã€OpenThinkerï¼‰ã€‚
- **ä¼˜äº probing-based steering**ï¼š
  - åè€…åœ¨ AIME-24 ä¸Šå¯¼è‡´ **-4.35%** æ€§èƒ½ä¸‹é™ï¼Œè¯´æ˜ç›²ç›®ä½¿ç”¨æ¢é’ˆæƒé‡ä¸å¯é ã€‚
  - AdaRAS æ›´ç¨³å®šï¼Œå› å¼•å…¥äº†ææ€§è¿‡æ»¤ä¸è‡ªé€‚åº”é—¨æ§ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 5ï¼‰
| æ–¹æ³•å˜ä½“ | AIME-24 | AIME-25 | åˆ†æ |
|---------|--------|--------|------|
| **Full AdaRAS** | **60.87%** | **54.55%** | å®Œæ•´æ–¹æ³• |
| w/o MD (æ”¹ç”¨ probing) | 43.48% | 50.00% | MD å¯¹è¯†åˆ« RCN è‡³å…³é‡è¦ |
| w/o ææ€§è¿‡æ»¤ (AS) | 52.17% | 45.45% | ææ€§ä¿¡æ¯æ˜¾è‘—æå‡æ•ˆæœ |
| w/o è‡ªé€‚åº”å¹²é¢„ (AI) | 56.52% | 45.45% | å¹²é¢„ä¼šæŸå®³æ­£ç¡®æ ·æœ¬ |
| éšæœºå¹²é¢„ | 34.78% | 27.27% | æ— å·®åˆ«å¹²é¢„æœ‰å®³ |

> ğŸ” æ‰€æœ‰ç»„ä»¶å‡ä¸å¯æˆ–ç¼ºï¼ŒéªŒè¯äº†è®¾è®¡åˆç†æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **RCNs å­˜åœ¨ä¸”å¯è¯†åˆ«**ï¼šLLM æ¨ç†æ­£ç¡®æ€§ä¸å°‘é‡ç¥ç»å…ƒçš„æ¿€æ´»çŠ¶æ€å¼ºç›¸å…³ï¼Œå¯é€šè¿‡å¯¹æ¯”æ¿€æ´»å·®ï¼ˆMDï¼‰æœ‰æ•ˆè¯†åˆ«ã€‚
2. **AdaRAS æ˜¾è‘—æå‡å¯é æ€§**ï¼šåœ¨ä¸å¢åŠ è®­ç»ƒæˆ–é‡‡æ ·æˆæœ¬çš„å‰æä¸‹ï¼Œå®ç°ä¸€è‡´ä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨é«˜éš¾åº¦ä»»åŠ¡ä¸Šã€‚
3. **RCNs å…·æœ‰å¼ºå¯è¿ç§»æ€§**ï¼š
   - åœ¨ AIME ä¸Šè¯†åˆ«çš„ RCNs å¯è¿ç§»åˆ° MATHã€GSM8Kã€ç”šè‡³ç¼–ç¨‹ä»»åŠ¡ï¼ˆHumanEvalï¼‰ï¼Œå¸¦æ¥ç¨³å®šå¢ç›Šï¼ˆè§ Table 3ï¼‰ã€‚
   - è¡¨æ˜ RCNs æ•è·çš„æ˜¯**ä»»åŠ¡æ— å…³çš„é€šç”¨æ¨ç†æœºåˆ¶**ã€‚
4. **å¯æ‰©å±•æ€§å¼º**ï¼šåœ¨æ›´å¼ºæ¨¡å‹ Qwen3-4B ä¸Šä»æœ‰æ•ˆï¼ˆTable 4ï¼‰ï¼Œå¦‚ AIME-25 ä» 59.09% â†’ **72.73%**ã€‚
5. **æœºåˆ¶æ€§è§£é‡Š**ï¼š
   - AdaRAS ä¸»è¦å¹²é¢„**ä¸­åå±‚ç¥ç»å…ƒ**ï¼ˆå›¾5ï¼‰ã€‚
   - èƒ½**ç¨³å®š latent reasoning è½¨è¿¹**ï¼ˆé™ä½ Magnitudeï¼‰ï¼Œä½†**ä¸æ”¹å˜è¯­ä¹‰å»ºæ¨¡**ï¼ˆAngle ä¸å˜ï¼‰ï¼ˆå›¾6ï¼‰ã€‚

### å±€é™æ€§
1. **æ¨¡å‹èŒƒå›´æœ‰é™**ï¼šç›®å‰ä»…åœ¨ Qwen3 ç³»åˆ—æ¨¡å‹ä¸ŠéªŒè¯ï¼Œéœ€æ‰©å±•è‡³å…¶ä»–æ¶æ„ï¼ˆå¦‚ LLaMAã€Phiï¼‰ã€‚
2. **ä¾èµ–å¯¹æ¯”æ•°æ®å¯¹**ï¼šéœ€åŒæ—¶è·å¾—åŒä¸€è¾“å…¥çš„æ­£ç¡®ä¸é”™è¯¯æ¨ç†è·¯å¾„ï¼Œå¯¹æç«¯èƒ½åŠ›å¼º/å¼±çš„æ¨¡å‹å¯èƒ½éš¾ä»¥æ„é€ ã€‚
3. **ä»…å…³æ³¨ MLP æ¿€æ´»**ï¼šæœªè€ƒè™‘ attention head æˆ–å…¶ä»–æ¨¡å—çš„å½±å“ã€‚
4. **STEM å¯¼å‘**ï¼šä¸»è¦åœ¨æ•°å­¦ä¸ç¼–ç¨‹ä»»åŠ¡éªŒè¯ï¼Œå¯¹å¼€æ”¾åŸŸæˆ–å¤šè·³æ¨ç†çš„é€‚ç”¨æ€§å¾…ç ”ç©¶ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•æ¨¡å‹ä¸ä»»åŠ¡èŒƒå›´**ï¼šåº”ç”¨äº vision-languageã€multi-hop QAã€spatial reasoning ç­‰æ›´å¤æ‚åœºæ™¯ã€‚
2. **æ”¹è¿›æ•°æ®æ„é€ ç­–ç•¥**ï¼šæ¢ç´¢æ— éœ€æ˜¾å¼é”™è¯¯æ ·æœ¬çš„æ–¹æ³•ï¼ˆå¦‚åŸºäºä¸ç¡®å®šæ€§é‡‡æ ·ï¼‰ã€‚
3. **ç»“åˆå…¶ä»–å¯è§£é‡ŠæŠ€æœ¯**ï¼šèåˆ **Sparse Autoencoders (SAEs)** æˆ– **dictionary learning** è¿›ä¸€æ­¥è§£æ RCNs çš„è¯­ä¹‰åŠŸèƒ½ã€‚
4. **åŠ¨æ€ steering å¼ºåº¦æ§åˆ¶**ï¼šæ ¹æ®æ¨ç†é˜¶æ®µè‡ªé€‚åº”è°ƒæ•´å¹²é¢„å¼ºåº¦ Î±ã€‚
5. **ä¸ token-level æ–¹æ³•ç»“åˆ**ï¼šä¸ entropy-based guidance ç­‰æ–¹æ³•ååŒï¼Œå®ç°å¤šå±‚çº§å¹²é¢„ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> AdaRAS æ­ç¤ºäº† LLM æ¨ç†å¯é æ€§çš„â€œç¥ç»å¼€å…³â€ï¼Œé€šè¿‡è¯†åˆ«å¹¶å¼•å¯¼ **Reasoning-Critical Neurons**ï¼Œå®ç°äº†**æ— éœ€è®­ç»ƒã€é«˜æ•ˆã€å¯è¿ç§»**çš„æ¨ç†å¢å¼ºï¼Œä¸ºå¯æ§æ¨ç†æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 13. [Towards Self-Optimizing Electron Microscope: Robust Tuning of Aberration Coefficients via Physics-Aware Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2601.18972)

**Authors**: Utkarsh Pratiush, Austin Houston, Richard Liu, Gerd Duscher, Sergei Kalinin  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.18972v1  

#### Abstract
Realizing high-throughput aberration-corrected Scanning Transmission Electron Microscopy (STEM) exploration of atomic structures requires rapid tuning of multipole probe correctors while compensating for the inevitable drift of the optical column. While automated alignment routines exist, convention...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Towards Self-Optimizing Electron Microscope: Robust Tuning of Aberration Coefficients via Physics-Aware Multi-Objective Bayesian Optimization*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ç°ä»£**aberration-corrected Scanning Transmission Electron Microscopy (STEM)** è™½ç„¶èƒ½å¤Ÿå®ç°äºšåŸƒçº§æˆåƒï¼Œä½†å…¶å…‰å­¦å‚æ•°ï¼ˆå¦‚defocusã€astigmatismåŠé«˜é˜¶åƒå·®ï¼‰çš„è°ƒæ ¡è¿‡ç¨‹é«˜åº¦ä¾èµ–äººå·¥æ“ä½œï¼Œå­˜åœ¨ä»¥ä¸‹ç“¶é¢ˆï¼š
- **æ•ˆç‡ä½ä¸‹**ï¼šä¼ ç»Ÿè‡ªåŠ¨å¯¹å‡†é‡‡ç”¨ä¸²è¡Œæœç´¢ï¼ˆå¦‚Nelder-Meadï¼‰ï¼Œæ ·æœ¬åˆ©ç”¨ç‡ä½ï¼›
- **è„†å¼±æ€§å¼º**ï¼šçƒ­æ¼‚ç§»ã€æŸæµå‰‚é‡ç´¯ç§¯å’Œå‚æ•°è€¦åˆå¯¼è‡´éš¾ä»¥åŠ¨æ€ç»´æŒæœ€ä¼˜çŠ¶æ€ï¼›
- **çµæ´»æ€§å·®**ï¼šæ·±åº¦å­¦ä¹ æ–¹æ³•è™½å¿«ï¼Œä½†éœ€å¤§é‡é¢„è®­ç»ƒæ•°æ®ï¼Œéš¾ä»¥é€‚åº”ä¸åŒæ ·å“æˆ–æˆåƒæ¡ä»¶ã€‚

å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§**é«˜æ•ˆã€é²æ£’ä¸”å¯æ³›åŒ–**çš„è‡ªåŠ¨åŒ–åƒå·®æ ¡æ­£æ¡†æ¶ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº**Physics-Aware Multi-Objective Bayesian Optimization (MOBO)** çš„è‡ªä¼˜åŒ–ç”µå­æ˜¾å¾®é•œæ¡†æ¶ï¼Œæ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### ï¼ˆ1ï¼‰å°†åƒå·®æ ¡æ­£å»ºæ¨¡ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼ˆMulti-Objective Optimizationï¼‰
- ä¸å†ä½¿ç”¨å•ä¸€å›¾åƒè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚contrastæœ€å¤§åŒ–ï¼‰ï¼Œè€Œæ˜¯åŒæ—¶ä¼˜åŒ–å¤šä¸ªç‰©ç†é©±åŠ¨çš„rewardå‡½æ•°ï¼Œä¾‹å¦‚ï¼š
  - `R_contrast`ï¼šå½’ä¸€åŒ–å›¾åƒæ–¹å·®ï¼ˆNormalized RMS Contrastï¼‰
  - `R_fft`ï¼šç»é«˜é€šæ»¤æ³¢åçš„FFTè°±åŠŸç‡ï¼ˆåæ˜ æ™¶æ ¼å‘¨æœŸæ€§ï¼‰
- åˆ©ç”¨**Pareto Front**æ­ç¤ºä¸åŒç›®æ ‡ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼Œé¿å…â€œreward hackingâ€ï¼ˆä¾‹å¦‚ä»…å¢å¼ºå™ªå£°ä»¥æé«˜contrastï¼‰ã€‚

#### ï¼ˆ2ï¼‰å¼•å…¥è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimization, BOï¼‰æå‡é‡‡æ ·æ•ˆç‡
- ä½¿ç”¨**Gaussian Process (GP)** æ„å»ºåƒå·®ç©ºé—´çš„æ¦‚ç‡ä»£ç†æ¨¡å‹ï¼ˆsurrogate modelï¼‰ï¼Œä¸»åŠ¨é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„é•œå¤´å‚æ•°è¿›è¡Œä¸‹ä¸€æ¬¡æµ‹é‡ï¼›
- ç»“åˆ**Expected Hypervolume Improvement (EHVI)** ä½œä¸ºacquisition functionï¼Œåœ¨å¤šç›®æ ‡åœºæ™¯ä¸­æœ‰æ•ˆæ‰©å±•Paretoå‰æ²¿ã€‚

#### ï¼ˆ3ï¼‰å¼ºè°ƒâ€œç‰©ç†æ„ŸçŸ¥â€ä¸â€œäººç±»åœ¨ç¯â€è®¾è®¡
- Rewardå‡½æ•°åŸºäºç‰©ç†åŸç†æ„å»ºï¼ˆå¦‚FFTå¯¹åº”æ™¶æ ¼åˆ†è¾¨ç‡ï¼‰ï¼Œç¡®ä¿å¯è§£é‡Šæ€§å’Œè·¨å¹³å°å¯è¿ç§»æ€§ï¼›
- Pareto frontå¯è§†åŒ–ä½¿é¢†åŸŸä¸“å®¶å¯åœ¨å¤šä¸ªéæ”¯é…è§£ä¸­æ‰‹åŠ¨é€‰æ‹©æœ€åˆé€‚çš„é…ç½®ï¼Œæ”¯æŒâ€œhuman-in-the-loopâ€å†³ç­–ã€‚

#### ï¼ˆ4ï¼‰éµå¾ªFAIRæ•°æ®åŸåˆ™
- æ‰€æœ‰ä¼˜åŒ–è½¨è¿¹ï¼ˆaction â†’ observationï¼‰è¢«ç³»ç»Ÿè®°å½•ï¼Œå½¢æˆå¯ç”¨äºåç»­åˆ†æã€æ•°å­—å­ªç”Ÿå»ºæ¨¡æˆ–è¿ç§»å­¦ä¹ çš„é«˜è´¨é‡æ•°æ®é›†ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³•ç±»å‹ | å…¸å‹ä»£è¡¨ | å±€é™æ€§ | æœ¬å·¥ä½œçš„ä¼˜åŠ¿ |
|--------|--------|------|-------------|
| ä¼ ç»Ÿå¯å‘å¼æœç´¢ | Nelder-Mead, Grid Search | æ ·æœ¬æ•ˆç‡ä½ï¼Œæ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ | é«˜æ•ˆæ¢ç´¢ï¼Œåˆ©ç”¨ä¸ç¡®å®šæ€§å¼•å¯¼é‡‡æ · |
| å•ç›®æ ‡BO | å•æŒ‡æ ‡ä¼˜åŒ–ï¼ˆå¦‚maximize contrastï¼‰ | æ˜“å—ä¼ªå½±å¹²æ‰°ï¼Œå¿½ç•¥åˆ†è¾¨ç‡æŸå¤± | å¤šç›®æ ‡å¹³è¡¡ï¼Œé˜²æ­¢reward hacking |
| æ·±åº¦å­¦ä¹  | DeepFocus, Ronchigram NN | éœ€å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›å¼± | æ— éœ€é¢„è®­ç»ƒï¼Œé€‚ç”¨äºæ–°æ ·å“/ä»ªå™¨ |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿æ€»ç»“**ï¼š  
> - æ›´**robust**ï¼ˆå¯¹æŠ—å™ªå£°ä¸ç¡¬ä»¶éç†æƒ³æ€§ï¼‰  
> - æ›´**data-efficient**ï¼ˆ< 50æ¬¡è¿­ä»£å³å¯æ”¶æ•›ï¼‰  
> - æ›´**interpretable & adaptable**ï¼ˆé€šè¿‡Pareto frontç†è§£trade-offï¼‰

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ§ª æ•°æ®é›†ä¸ä»¿çœŸç¯å¢ƒ
- **åˆæˆæ•°æ®**ï¼šåŸºäº`abtem`å’Œ`pyTEMlib`æ¨¡æ‹Ÿå•å±‚WSâ‚‚ï¼ˆTungsten Diselenideï¼‰çš„STEMå›¾åƒï¼›
- **çœŸå®æ•°æ®**ï¼šåœ¨Thermo Fisher Spectra 300 TEMä¸Šé‡‡é›†Auçº³ç±³é¢—ç²’çš„æ ‡å‡†åˆ†è¾¨ç‡æµ‹è¯•æ ·å“å›¾åƒï¼›
- **å™ªå£°æ³¨å…¥**ï¼š
  - ç©ºé—´ç›¸å…³é«˜æ–¯å™ªå£°ï¼ˆæ¨¡æ‹Ÿç¯å¢ƒæ‰°åŠ¨ï¼‰
  - æ³Šæ¾å™ªå£°ï¼ˆæ¨¡æ‹Ÿæœ‰é™ç”µå­è®¡æ•°ä¸‹çš„shot noiseï¼‰

> ç›®çš„ï¼šç¼©å°â€œsim-to-realâ€å·®è·ï¼ŒéªŒè¯ç®—æ³•é²æ£’æ€§ã€‚

---

### âš™ï¸ å®éªŒè®¾ç½®
| ç»„ä»¶ | æè¿° |
|-----|------|
| **ä¼˜åŒ–å˜é‡ç©ºé—´ X** | åŒ…æ‹¬Câ‚â‚€ï¼ˆdefocusï¼‰ã€Câ‚â‚‚â‚/Câ‚â‚‚áµ¦ï¼ˆtwo-fold astigmatismï¼‰ã€Câ‚‚â‚â‚/Câ‚‚â‚áµ¦ã€Câ‚‚â‚ƒâ‚/Câ‚‚â‚ƒáµ¦ç­‰ï¼Œç»´åº¦ä»3åˆ°7ä¸ç­‰ |
| **Reward Functions** | <br>â€¢ $ R_{\text{contrast}} = \frac{\sigma}{\mu + \epsilon} $<br>â€¢ $ R_{\text{fft}} = \langle (\log(1+|\mathcal{F}|) \cdot M) \rangle $ï¼Œå…¶ä¸­Mä¸ºå»é™¤DCåˆ†é‡çš„é«˜é€šmask |
| **ä¼˜åŒ–å¼•æ“** | BoTorch + GPyTorchå®ç°GPå»ºæ¨¡ä¸EHVIé‡‡æ · |
| **ç¡¬ä»¶æ§åˆ¶æ¥å£** | CEOS API æ§åˆ¶åƒå·®é€é•œï¼›AutoScript API æ§åˆ¶æ¢æµ‹å™¨ä¸æ ·å“å° |
| **å¼€æºä»£ç ** | å·²å‘å¸ƒè‡³GitHubï¼š<br>- [Aberrations.ipynb](https://github.com/pycroscopy/asyncroscopy/blob/main/notebooks/Aberrations.ipynb)<br>- [stemOrchestrator](https://github.com/pycroscopy/pyAutoMic/tree/main/TEM/stemOrchestrator)<br>- [asyncroscopy](https://github.com/pycroscopy/asyncroscopy) |

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| **Pareto Front å½¢æ€** | å¯è§†åŒ–contrastä¸resolutioné—´çš„trade-offï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨å”¯ä¸€æœ€ä¼˜è§£ |
| **Hypervolume Improvement** | è¡¡é‡Pareto frontéšè¿­ä»£æ‰©å±•çš„ç¨‹åº¦ï¼Œç”¨äºåˆ¤æ–­æ”¶æ•›æ€§ |
| **Wall-clock Time per Step** | åˆ†è§£è®¡ç®—å¼€é”€ï¼ˆGP training + acquisitionï¼‰ä¸ç¡¬ä»¶å»¶è¿Ÿï¼ˆmicroscope communicationï¼‰ |
| **æœ€ç»ˆå›¾åƒè´¨é‡** | æ˜¯å¦æ¸…æ™°åˆ†è¾¨åŸå­åˆ—ï¼ˆatomic columnsï¼‰ä¸æ™¶æ ¼å‘¨æœŸæ€§ |

---

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
å°½ç®¡æœªç›´æ¥è¿è¡Œå…¶ä»–ç®—æ³•åœ¨åŒä¸€ç¡¬ä»¶ä¸Šï¼Œæ–‡ä¸­æ˜ç¡®æŒ‡å‡ºæ‰€ææ–¹æ³•ä¼˜äºä»¥ä¸‹ç±»åˆ«ï¼š
- **Serial heuristic search**ï¼ˆå¦‚é€è½´æ‰«æ+å³°å€¼æ£€æµ‹ï¼‰
- **Single-objective BO**ï¼ˆä»…ä¼˜åŒ–contrastæˆ–FFTï¼‰
- **Exhaustive grid search**ï¼ˆåœ¨é«˜ç»´ç©ºé—´ä¸å¯è¡Œï¼‰

å¹¶é€šè¿‡æ¶ˆèå®éªŒè¯æ˜MOBOç›¸å¯¹äºå•ç›®æ ‡ç­–ç•¥çš„ä¼˜è¶Šæ€§ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰ä»¿çœŸç¯å¢ƒä¸­å¿«é€Ÿæ”¶æ•›ï¼ˆFig. 4ï¼‰
- åœ¨3ç»´åƒå·®ç©ºé—´ï¼ˆCâ‚â‚€, Câ‚â‚‚â‚, Câ‚â‚‚áµ¦ï¼‰ä¸­ï¼Œ**çº¦25æ¬¡è¿­ä»£å†…æ”¶æ•›è‡³å…¨å±€Pareto front**ï¼›
- Hypervolumeåœ¨å‰20æ­¥è¿…é€Ÿä¸Šå‡ï¼Œè¡¨æ˜EHVIèƒ½æœ‰æ•ˆæ¢ç´¢å…³é”®åŒºåŸŸï¼›
- æœ€ç»ˆè·å¾—åŒæ—¶å…·æœ‰é«˜contrastå’Œé«˜FFT powerçš„åŸå­çº§æ¸…æ™°å›¾åƒï¼ˆFig. 4c(iii)ï¼‰ã€‚

#### ï¼ˆ2ï¼‰çœŸå®ä»ªå™¨ä¸Šçš„æˆåŠŸéƒ¨ç½²ï¼ˆFig. 5ï¼‰
- åœ¨7ç»´åƒå·®ç©ºé—´ï¼ˆå«1st & 2nd order aberrationsï¼‰ä¸­å®Œæˆè°ƒä¼˜ï¼›
- **ä»…ç”¨~40æ¬¡å®éªŒè¿­ä»£**å³æ‰¾åˆ°å¹³è¡¡è§£ï¼›
- å‘ç°ï¼š
  - æœ€å¤§contrastçŠ¶æ€ï¼ˆRed starï¼‰åè€Œ**æ— æ³•è§£ææ™¶æ ¼ç»“æ„**ï¼ˆObjective 2 < 11.90ï¼‰ï¼›
  - Paretoæœ€ä¼˜è§£ï¼ˆGreen starï¼‰è™½ç•¥é™contrastï¼Œä½†æ˜¾è‘—æå‡åˆ†è¾¨ç‡ï¼ŒæˆåŠŸè¯†åˆ«Auçº³ç±³ç²’å­ä¸­çš„åŸå­åˆ—ã€‚

> ğŸ‘‰ è¯æ˜ï¼š**å•ç›®æ ‡ä¼˜åŒ–ä¼šè¯¯å¯¼ç³»ç»Ÿè¿›å…¥â€œè™šå‡æœ€ä¼˜â€é™·é˜±**ã€‚

#### ï¼ˆ3ï¼‰æ—¶é—´æˆæœ¬åˆ†æï¼ˆFig. 7ï¼‰
| å‚æ•°ç»´åº¦ | å¹³å‡æ¯æ­¥è€—æ—¶ | ç¡¬ä»¶é€šä¿¡ï¼ˆHWï¼‰ | è®¡ç®—å¼€é”€ï¼ˆGP + Acquisitionï¼‰ |
|---------|--------------|----------------|-------------------------------|
| 3D       | ~8 s         | ~4 s           | ~4 s                          |
| 4D       | ~15 s        | ~4 s           | ~11 s                         |
| 7D       | >100 s       | ~4 s           | >96 s                         |

> ğŸ’¡ å‘ç°ï¼šéšç€ç»´åº¦å¢åŠ ï¼Œ**è®¡ç®—æˆä¸ºä¸»è¦ç“¶é¢ˆ**ï¼Œè€Œéç¡¬ä»¶å“åº”é€Ÿåº¦ã€‚

---

### ğŸ” æ¶ˆèå®éªŒä¸å…³é”®å‘ç°
- **Grid Search vs. MOBO**ï¼ˆFig. 3ï¼‰ï¼š
  - å¯†é›†ç½‘æ ¼æœç´¢ç”Ÿæˆâ€œç°è‰²äº‘å›¢â€ï¼Œç»å¤§å¤šæ•°ç‚¹ä¸ºæ¬¡ä¼˜ï¼›
  - çœŸæ­£çš„Pareto frontæ˜¯ä¸€æ¡ç‹­çª„æµå½¢ï¼Œä¼ ç»Ÿæ–¹æ³•æéš¾å‘½ä¸­ï¼›
- **Single-objectiveä¼˜åŒ–é£é™©**ï¼š
  - ä»…ä¼˜åŒ–contrast â†’ è¿›å…¥defocusedä½†é«˜ä¿¡å·çŠ¶æ€ï¼ˆbright/bluryï¼‰ï¼›
  - ä»…ä¼˜åŒ–FFT â†’ å¯èƒ½è¿‡åº¦æ”¾å¤§é«˜é¢‘ä¼ªå½±ï¼›
- **MOBOçš„ä¼˜åŠ¿**ï¼š
  - è‡ªåŠ¨é¿å¼€æç«¯è§£ï¼Œå®šä½â€œsweet spotâ€ï¼ˆå¦‚Point C/D in Fig. 3ï¼‰ï¼›
  - æä¾›ä¸€ç»„å€™é€‰æ–¹æ¡ˆä¾›äººå·¥é€‰æ‹©ï¼Œå¢å¼ºå®ç”¨æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **åƒå·®è°ƒä¼˜æœ¬è´¨ä¸Šæ˜¯å¤šç›®æ ‡æƒè¡¡é—®é¢˜**ï¼š
   - â€œæœ€ä½³â€å›¾åƒå–å†³äºå®éªŒç›®çš„ï¼ˆcontrast vs. resolutionï¼‰ï¼Œä¸å­˜åœ¨æ™®é€‚æœ€ä¼˜è§£ï¼›
   - Pareto frontæä¾›äº†ç‰©ç†ä¸Šåˆç†çš„trade-offå¯è§†åŒ–å·¥å…·ã€‚

2. **MOBOæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•**ï¼š
   - æ•°æ®æ•ˆç‡é«˜ï¼ˆ<50æ¬¡æµ‹é‡ï¼‰ï¼›
   - æŠ—å™ªèƒ½åŠ›å¼ºï¼Œèƒ½åœ¨çœŸå®å¤æ‚ç¯å¢ƒä¸‹ç¨³å®šæ”¶æ•›ï¼›
   - æ”¯æŒâ€œè‡ªä¼˜åŒ–æ˜¾å¾®é•œâ€æ„¿æ™¯â€”â€”åœ¨å®éªŒè¿‡ç¨‹ä¸­åŠ¨æ€ç»´æŒæœ€ä½³å…‰è·¯çŠ¶æ€ã€‚

3. **è®¡ç®—å¼€é”€éšç»´åº¦å‰§å¢**ï¼š
   - å½“å‰GPå»ºæ¨¡åœ¨>6ç»´æ—¶æˆä¸ºç“¶é¢ˆï¼›
   - æœªæ¥éœ€å‘å±•æ›´é«˜æ•ˆçš„surrogate modelingç­–ç•¥ï¼ˆå¦‚ç¨€ç–GPã€ç¥ç»è¿‡ç¨‹ï¼‰ã€‚

4. **å¼€å¯é«˜çº§æŸæµå·¥ç¨‹å¯èƒ½æ€§**ï¼š
   - ä¸å†è¿½æ±‚â€œåœ†å½¢å¯¹ç§°æ¢é’ˆâ€ï¼Œè€Œæ˜¯**ä¸»åŠ¨è®¾è®¡éå¯¹ç§°probe wavefunction**ï¼›
   - ä¾‹å¦‚æ²¿ç‰¹å®šæ™¶å‘å¢å¼ºåˆ†è¾¨ç‡ï¼Œç”¨äºåº”å˜åœºmappingç­‰ä»»åŠ¡ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
| å±€é™æ€§ | è¯´æ˜ |
|-------|------|
| **è®¡ç®—å¤æ‚åº¦é«˜** | é«˜ç»´GPè®­ç»ƒæ—¶é—´é•¿ï¼Œé™åˆ¶å®æ—¶æ€§ï¼ˆå°¤å…¶>7å‚æ•°ï¼‰ |
| **ä¾èµ–åˆç†rewardè®¾è®¡** | è‹¥rewardå‡½æ•°ä¸èƒ½åæ˜ çœŸå®ç‰©ç†éœ€æ±‚ï¼Œåˆ™ç»“æœä»å¯èƒ½åç¦»é¢„æœŸ |
| **æœªæ¶µç›–æ‰€æœ‰åƒå·®ç±»å‹** | å½“å‰èšç„¦äºä½é˜¶ä¸éƒ¨åˆ†äºŒé˜¶åƒå·®ï¼Œæ›´é«˜é˜¶ï¼ˆå¦‚Câ‚ƒï¼‰å°šæœªå®Œå…¨é›†æˆ |
| **åˆå§‹ç‚¹æ•æ„Ÿæ€§** | å°½ç®¡æ¯”æ¢¯åº¦æ³•ç¨³å¥ï¼Œä½†ä»å—åˆå§‹æ¢ç´¢ç­–ç•¥å½±å“ |

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **ä¼˜åŒ–-ä»ªå™¨ååŒè®¾è®¡ï¼ˆOptimization-Instrumentation Co-Designï¼‰**
   - åŠ é€Ÿè®¡ç®—ç«¯ï¼šGPUåŠ é€ŸGPæ¨ç†ã€è½»é‡åŒ–kernel approximationï¼›
   - ä¼˜åŒ–ç¡¬ä»¶ç«¯ï¼šé™ä½é€šä¿¡å»¶è¿Ÿã€æå‡æ¢æµ‹å™¨å¸§ç‡ã€‚

2. **å¼•å…¥ä»»åŠ¡å®šåˆ¶åŒ–probe shaping**
   - åˆ©ç”¨MOBOè°ƒæ§éå¯¹ç§°åƒå·®ï¼Œç”Ÿæˆå®šå‘æ‹‰ä¼¸æˆ–æ—‹è½¬æ¢é’ˆï¼ŒæœåŠ¡äºç‰¹å®šææ–™è¡¨å¾ä»»åŠ¡ã€‚

3. **æ„å»ºæ•°å­—å­ªç”Ÿï¼ˆDigital Twinï¼‰ç³»ç»Ÿ**
   - åˆ©ç”¨å†å²ä¼˜åŒ–è½¨è¿¹è®­ç»ƒæ›´å¿«çš„æ›¿ä»£æ¨¡å‹ï¼Œå®ç°è·¨æ ·å“/ä»ªå™¨çš„çŸ¥è¯†è¿ç§»ã€‚

4. **æ‰©å±•è‡³åŸä½åŠ¨æ€å®éªŒ**
   - åœ¨åŠ çƒ­ã€åå‹ã€æ¶²ä½“æ± ç­‰åŠ¨æ€æ¡ä»¶ä¸‹æŒç»­è·Ÿè¸ªå¹¶ä¿®æ­£åƒå·®æ¼‚ç§»ã€‚

---

> ğŸ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬æ–‡æå‡ºçš„**Physics-Aware MOBOæ¡†æ¶**ä¸ºå®ç°çœŸæ­£æ„ä¹‰ä¸Šçš„**Self-Optimizing Electron Microscopy**è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥â€”â€”å®ƒä¸ä»…æå‡äº†åƒå·®æ ¡æ­£çš„é€Ÿåº¦ä¸é²æ£’æ€§ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå°†â€œä»€ä¹ˆæ˜¯å¥½å›¾åƒâ€çš„å®šä¹‰æƒäº¤è¿˜ç»™ç‰©ç†è§„å¾‹ä¸äººç±»ç»éªŒï¼Œè€Œéé»‘ç®±æ¨¡å‹æˆ–å•ä¸€æŒ‡æ ‡ã€‚

</details>

---

### 14. [Learn and Verify: A Framework for Rigorous Verification of Physics-Informed Neural Networks](https://arxiv.org/abs/2601.19818)

**Authors**: Kazuaki Tanaka, Kohei Yatabe  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.19818v1  

#### Abstract
The numerical solution of differential equations using neural networks has become a central topic in scientific computing, with Physics-Informed Neural Networks (PINNs) emerging as a powerful paradigm for both forward and inverse problems. However, unlike classical numerical methods that offer estab...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šLearn and Verify: A Framework for Rigorous Verification of Physics-Informed Neural Networks

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰åŸºäº **Physics-Informed Neural Networks (PINNs)** çš„å¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨è™½ç„¶åœ¨ç§‘å­¦è®¡ç®—ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªæ ¹æœ¬æ€§ç¼ºé™·ï¼š
- ç¼ºä¹**ä¸¥æ ¼çš„è¯¯å·®ç•Œ**ï¼ˆrigorous error boundsï¼‰ï¼Œæ— æ³•æ•°å­¦ä¸Šè®¤è¯å…¶ç²¾åº¦ï¼›
- è®­ç»ƒè¿‡ç¨‹å…·æœ‰**éç¡®å®šæ€§**ï¼ˆstochastic optimizationï¼‰ï¼Œå¯¼è‡´ç»“æœéš¾ä»¥å¤ç°å’ŒéªŒè¯ã€‚

è¿™ä½¿å¾— PINNs éš¾ä»¥åº”ç”¨äºå¯¹å¯é æ€§è¦æ±‚æé«˜çš„ç§‘å­¦æœºå™¨å­¦ä¹ åœºæ™¯ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šLearn and Verify æ¡†æ¶
ä½œè€…æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶â€”â€”â€œ**Learn and Verify**â€ï¼Œå°†æ·±åº¦å­¦ä¹ çš„çµæ´»æ€§ä¸**éªŒè¯æ•°å€¼è®¡ç®—**ï¼ˆverified numerical computationï¼‰çš„ä¸¥è°¨æ€§ç›¸ç»“åˆã€‚

#### æ ¸å¿ƒæ€æƒ³ï¼š
- **Learn é˜¶æ®µ**ï¼šè®­ç»ƒ PINNs æ„é€ ä¸€å¯¹ **sub-solution** å’Œ **super-solution**ï¼Œå³ä¸¥æ ¼åŒ…å›´çœŸå®è§£çš„ä¸Šä¸‹ç•Œå‡½æ•°ã€‚
- **Verify é˜¶æ®µ**ï¼šåˆ©ç”¨ **interval arithmetic** å¯¹è¿™äº›å€™é€‰å‡½æ•°è¿›è¡Œ**æœºå™¨å¯éªŒè¯çš„æ•°å­¦è¯æ˜**ï¼Œç¡®ä¿å®ƒä»¬åœ¨æ•´ä¸ªè¿ç»­åŸŸä¸Šæ»¡è¶³æ‰€éœ€çš„å¾®åˆ†ä¸ç­‰å¼ã€‚

#### åˆ›æ–°ç‚¹ï¼š
1. **é¦–æ¬¡å®ç° PINNs è§£çš„ä¸¥æ ¼éªŒè¯**  
   å°† PINNs è¾“å‡ºè½¬åŒ–ä¸ºå¯è¢« interval arithmetic éªŒè¯çš„å½¢å¼ï¼Œå¡«è¡¥äº†ç¥ç»ç½‘ç»œæ±‚è§£å™¨ä¸ä¼ ç»ŸéªŒè¯æ–¹æ³•ä¹‹é—´çš„é¸¿æ²Ÿã€‚

2. **æå‡º Doubly Smoothed Maximum (DSM) æŸå¤±å‡½æ•°**  
   ç”¨äºå¹³æ»‘åœ°ä¼˜åŒ– sub/super-solution çš„æ®‹å·®çº¦æŸï¼Œå…‹æœäº†ä¼ ç»Ÿ max(Â·,0) å‡½æ•°å¸¦æ¥çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚

3. **å¼•å…¥ Variation Learning ç»“æ„**  
   é€šè¿‡æ„é€  $ u_{\theta}(t) \pm v_g(t), w_g(t) $ çš„å½¢å¼æ˜¾å¼æ§åˆ¶è¯¯å·®è¾¹ç•Œå®½åº¦ï¼Œå¹¶ä¿è¯éè´Ÿæ€§ï¼ˆvia scaled sigmoidï¼‰ï¼Œä½¿è¯¯å·®å®¹å¿åº¦ $ \epsilon $ æˆä¸ºå¯æ§å‚æ•°ã€‚

4. **ç‹¬ç«‹äºç”Ÿæˆæ–¹å¼çš„éªŒè¯æœºåˆ¶**  
   éªŒè¯è¿‡ç¨‹å®Œå…¨ç‹¬ç«‹äºæ¨¡å‹å¦‚ä½•ç”Ÿæˆï¼Œå³ä½¿æ˜¯â€œé»‘ç®±â€PINN è¾“å‡ºä¹Ÿå¯è¢«ä¸¥æ ¼è®¤è¯ã€‚

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼ ç»Ÿ PINNs / æ•°å€¼æ–¹æ³• | æœ¬æ–‡æ–¹æ³• |
|------|------------------------|---------|
| è¯¯å·®ä¿è¯ | æ— æˆ–ä»…æ¸è¿‘ä¼°è®¡ | **å¯è®¡ç®—ã€ä¸¥æ ¼çš„åéªŒè¯¯å·®ç•Œ** |
| éªŒè¯æ–¹å¼ | ä¾èµ–å‚è€ƒè§£æˆ–å¯å‘å¼ | **æ— éœ€çœŸå€¼ï¼Œçº¯æ•°å­¦è¯æ˜** |
| æ•°å€¼ç²¾åº¦ | æµ®ç‚¹è¯¯å·®æœªå—æ§ | **interval arithmetic æ§åˆ¶èˆå…¥è¯¯å·®** |
| åº”ç”¨èŒƒå›´ | ä¸€èˆ¬åªå¤„ç†å…‰æ»‘è§£ | æ”¯æŒ **piecewise CÂ¹ å‡½æ•°**ï¼ˆå¦‚å«çªå˜å¯¼æ•°ï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š å®éªŒå¯¹è±¡ï¼ˆODEsï¼‰
è®ºæ–‡åœ¨ä¸‰ä¸ªéçº¿æ€§å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰ä¸Šè¿›è¡Œäº†éªŒè¯å®éªŒï¼Œæ¶µç›–ä¸åŒæŒ‘æˆ˜ç±»å‹ï¼š

| é—®é¢˜ | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **Classical Logistic Equation** | ç»å…¸äººå£å¢é•¿æ¨¡å‹ | å­˜åœ¨è§£æè§£ï¼Œç”¨äºå®šé‡è¯„ä¼°è¯¯å·® |
| **Generalized Logistic Equation** | æ—¶å˜ç³»æ•°æ‰©å±•æ¨¡å‹ | æ— é—­å¼è§£ï¼Œæµ‹è¯•æ¡†æ¶æ™®é€‚æ€§ |
| **Riccati Equation** | æœ‰é™æ—¶é—´çˆ†ç ´ï¼ˆblow-upï¼‰ | è§£è¶‹äºæ— ç©·ï¼Œæ£€éªŒå¥‡å¼‚æ€§å¤„ç†èƒ½åŠ› |

### âš™ï¸ å®éªŒè®¾ç½®
- **ç½‘ç»œæ¶æ„**ï¼šé‡‡ç”¨ **SIREN**ï¼ˆæ­£å¼¦æ¿€æ´»å‡½æ•°ï¼‰ä½œä¸ºä¸»å¹²ç½‘ç»œï¼Œå…± 4 å±‚ï¼Œæ¯å±‚ 30 ç¥ç»å…ƒï¼›
- **è®­ç»ƒç­–ç•¥**ï¼š
  - Step 1ï¼šæ ‡å‡† PINN è®­ç»ƒè¿‘ä¼¼è§£ $ u_\theta(t) $
  - Step 2ï¼šå›ºå®š $ u_\theta $ï¼Œè®­ç»ƒåå·®ç½‘ç»œ $ v_g(t), w_g(t) $ æ„é€  sub/super-solutions
- **è¾“å‡ºçº¦æŸ**ï¼š$ v_g, w_g $ è¾“å‡ºå±‚ä½¿ç”¨ç¼©æ”¾ sigmoid $ \epsilon \cdot \sigma(\cdot) $ï¼Œä¿è¯éè´Ÿä¸”èŒƒå›´å¯æ§
- **æŸå¤±å‡½æ•°**ï¼š
  - ä¸»è¦ä½¿ç”¨ DSM lossï¼š  
    $$
    \mathcal{L}_{\text{Sub&Sup}} = \text{DSM}_{c_1,c_2}[R(u_\theta - v_g)] + \text{DSM}_{c_1,c_2}[-R(u_\theta + w_g)]
    $$
  - å¼•å…¥ç‰©ç†æ­£åˆ™é¡¹ $ \mathcal{L}_{\text{phys}} $ æŠ‘åˆ¶ä¸ç¨³å®šè¡Œä¸º
- **éªŒè¯å·¥å…·**ï¼šä½¿ç”¨ **INTLAB** è¿›è¡Œ interval arithmetic éªŒè¯ï¼Œç»“åˆè‡ªé€‚åº”ç»†åˆ†ç­–ç•¥ï¼ˆadaptive subdivisionï¼‰
- **ç¡¬ä»¶ç¯å¢ƒ**ï¼šAMD Ryzen 9 + 128GB RAMï¼ŒMATLAB 2025b

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
- **éªŒè¯æˆåŠŸç‡**ï¼ˆVerification Success Rateï¼‰ï¼šå­/è¶…è§£æ˜¯å¦é€šè¿‡ interval arithmetic éªŒè¯
- **æœ€å¤§ç›¸å¯¹è¯¯å·®**ï¼ˆMax Relative Errorï¼‰ vs è§£æè§£ï¼ˆä»… logistic caseï¼‰
- **è¯¯å·®å®¹å¿åº¦ $ \epsilon $** çš„å½±å“åˆ†æï¼ˆæ¶ˆèå®éªŒï¼‰
- **blow-up æ—¶é—´çš„ä¸Šä¸‹ç•Œ**ï¼ˆé’ˆå¯¹ Riccati æ–¹ç¨‹ï¼‰

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
æœ¬æ–‡æœªç›´æ¥ä¸å…¶ä»– PINN å˜ä½“æ¯”è¾ƒæ€§èƒ½ï¼Œè€Œæ˜¯å¼ºè°ƒï¼š
- **ç°æœ‰æ–¹æ³•ç¼ºä¹å¯éªŒè¯æ€§**
- æ‰€ææ¡†æ¶æä¾›çš„æ˜¯ **â€œæœ‰æ•°å­¦è¯æ˜ä¿éšœçš„è¯¯å·®ç•Œâ€**ï¼Œè€Œéå•çº¯æ›´å°çš„æ®‹å·®æˆ– MSE
- å› æ­¤ï¼Œå¯¹æ¯”æœ¬è´¨æ˜¯ **â€œä¸å¯ä¿¡è¿‘ä¼¼â€ vs â€œå¯ä¿¡åŒ…å›´â€**

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### âœ… å…³é”®æ€§èƒ½æ•°æ®

#### ï¼ˆ1ï¼‰Logistic Equation å®éªŒç»“æœ
- åœ¨ $ \epsilon = 2^{-5} $ æ—¶ï¼Œ6 å±‚ç½‘ç»œåœ¨ 300 è½®åéªŒè¯æˆåŠŸç‡è¾¾ **100%**
- å½“ $ \epsilon \leq 2^{-8} $ æ—¶ï¼Œæ‰€æœ‰é…ç½®å‡å¤±è´¥ â†’ è¡¨æ˜é€¼è¿‘è¯¯å·®ä¸‹é™çº¦ä¸º $ 2^{-7} \sim 2^{-8} $
- åŠ å…¥ç‰©ç†æ­£åˆ™é¡¹ $ \mathcal{L}_{\text{phys}} $ åï¼ŒæˆåŠŸç‡ä»ä¸è¶³ 10% æå‡è‡³æ¥è¿‘ 100%

> å›¾è¡¨æ˜¾ç¤ºï¼šéšç€ $ \lambda_{\text{phys}} $ å¢å¤§ï¼Œmean error æ˜¾è‘—ä¸‹é™ï¼Œsuccess rate å‘ˆ S å½¢ä¸Šå‡

#### ï¼ˆ2ï¼‰Generalized Logistic Equation
- ç”±äºæ— è§£æè§£ï¼Œä»¥é«˜ç²¾åº¦ ODE45 æ•°å€¼è§£ä¸ºå‚è€ƒ
- å³ä¾¿å¦‚æ­¤ï¼ŒéªŒè¯æˆåŠŸç‡ä»éš $ \epsilon $ å‡å°è€Œé™ä½
- å¤šå±‚ç½‘ç»œè¡¨ç°ä¼˜äºæµ…å±‚ï¼ˆ>3 å±‚å¿…è¦ï¼‰
- **4 å±‚ç½‘ç»œè¡¨ç°æœ€ä¼˜**ï¼Œè¡¨æ˜å­˜åœ¨è¡¨è¾¾åŠ›ä¸å¯è®­ç»ƒæ€§çš„å¹³è¡¡

#### ï¼ˆ3ï¼‰Riccati Equationï¼ˆBlow-up Caseï¼‰
| $ \epsilon $ | $ c $ | $ u(T) $ | Blow-up ä¸‹ç•Œ | ä¸Šç•Œ |
|--------------|-------|----------|-------------|-----|
| $ 2^{-2} $ | 2.00 | 12.119 | 1.9375 | 2.020 |
| $ 2^{-3} $ | 2.00 | 14.029 | 1.9375 | 2.009 |
| $ 2^{-4} $ | 2.00 | 14.893 | 1.9375 | 2.005 |

- éšç€ $ \epsilon $ å‡å°ï¼Œä¸Šç•Œä¸æ–­æ”¶ç´§ï¼ˆæ›´ç²¾ç¡®ï¼‰
- æˆåŠŸå®ç°äº†å¯¹ **æœ‰é™æ—¶é—´çˆ†ç ´æ—¶é—´ $ T_{\text{blow-up}} $** çš„**ä¸¥æ ¼åŒ…å›´**
- è¯æ˜è¯¥æ¡†æ¶èƒ½å¤„ç†å¥‡å¼‚æ€§é—®é¢˜

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœ
| å› ç´  | å½±å“ |
|------|------|
| **å‡å° $ \epsilon $** | è®­ç»ƒéš¾åº¦æ˜¾è‘—å¢åŠ ï¼Œéœ€æ›´å¤š epoch æ‰èƒ½æ”¶æ•› |
| **å¢åŠ ç½‘ç»œå±‚æ•°** | è¡¨è¾¾èƒ½åŠ›å¢å¼ºï¼Œä½†è¿‡æ·±ï¼ˆå¦‚ 6 å±‚ï¼‰åè€Œä¸å¦‚ 4â€“5 å±‚ç¨³å®š |
| **å¢å¤§ $ \lambda_{\text{phys}} $** | æ˜¾è‘—æé«˜è®­ç»ƒç¨³å®šæ€§ä¸éªŒè¯æˆåŠŸç‡ |
| **ä½¿ç”¨ DSM loss** | å¿…ä¸å¯å°‘ï¼›å°è¯•ç›´æ¥ä¼˜åŒ– max(Â·,0) å¯¼è‡´è®­ç»ƒå¤±è´¥ |

> ç»“è®ºï¼šDSM æä¾›è½¯é—´éš”ï¼ˆsoft marginï¼‰ï¼Œå³ä½¿è½»å¾®è¿åä¹Ÿæ–½åŠ æƒ©ç½šï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€å°

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Learn and Verify æ¡†æ¶å¯è¡Œä¸”æœ‰æ•ˆ**  
   æˆåŠŸä¸ºå¤šä¸ªéçº¿æ€§ ODE æ„å»ºäº†**ä¸¥æ ¼åŒ…å›´çœŸå®è§£çš„ sub/super-solutions**ï¼Œå¹¶å®Œæˆæœºå™¨éªŒè¯ã€‚

2. **è¯¯å·®å¯ä»¥è¢«æ•°å­¦è®¤è¯è€Œéä¼°è®¡**  
   ä¸å†ä¾èµ–æ®‹å·®å¤§å°æˆ–ä¸å‚è€ƒè§£çš„è·ç¦»ï¼Œè€Œæ˜¯é€šè¿‡ interval arithmetic ç»™å‡º**å¯éªŒè¯çš„è¯¯å·®ç•Œ**ã€‚

3. **DSM æŸå¤±å‡½æ•°å¯¹è®­ç»ƒè‡³å…³é‡è¦**  
   å¹³æ»‘è¿‘ä¼¼æå¤§æå‡äº†ä¼˜åŒ–ç¨³å®šæ€§ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å› é‡‡æ ·éšæœºæ€§å¯¼è‡´çš„æœ€å¤§æ®‹å·®æ¼æ£€é—®é¢˜ã€‚

4. **æ”¯æŒå…¨å±€è§£ä¸å¥‡å¼‚è§£çš„éªŒè¯**  
   é€šè¿‡ piecewise CÂ¹ æ¡ä»¶ï¼Œå…è®¸è¿æ¥ç‚¹å¤„å¯¼æ•°ä¸è¿ç»­ï¼Œé€‚ç”¨äºå»¶æ‹“åˆ°æ— é™æ—¶é—´åŸŸæˆ–å¤„ç† blow-up ç°è±¡ã€‚

5. **å‚æ•°é€‰æ‹©å¯è‡ªåŠ¨åŒ–æŒ‡å¯¼**  
   éªŒè¯ç»“æœæœ¬èº«æä¾›äº†åé¦ˆä¿¡å·ï¼Œå¯ç”¨äºè‡ªåŠ¨è°ƒæ•´ $ \epsilon $ã€ç½‘ç»œç»“æ„ç­‰è¶…å‚ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- **è®¡ç®—å¼€é”€é›†ä¸­åœ¨è®­ç»ƒé˜¶æ®µ**ï¼šè™½ç„¶éªŒè¯ä»…éœ€å‡ ç§’ï¼Œä½†è®­ç»ƒ sub/super-solutions è¾ƒæ…¢ï¼ˆ3â€“15 åˆ†é’Ÿï¼‰
- **ç›®å‰ä»…é€‚ç”¨äº ODEs**ï¼šå‘ PDEs æ¨å¹¿å°šå¾…ç ”ç©¶
- **ä¾èµ–è‰¯å¥½åˆå§‹åŒ–ä¸æ­£åˆ™åŒ–**ï¼šè‹¥åˆå§‹è¿‘ä¼¼è§£å¤ªå·®ï¼Œéš¾ä»¥æ„é€ æœ‰æ•ˆåŒ…å›´
- **interval arithmetic çš„ä¿å®ˆæ€§**ï¼šå¯èƒ½å¯¼è‡´ false negativeï¼ˆå®é™…æˆç«‹ä½†æœªèƒ½éªŒè¯ï¼‰

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³ PDEs å’Œé«˜ç»´ç³»ç»Ÿ**
2. **å¼€å‘è‡ªåŠ¨è°ƒå‚æœºåˆ¶**ï¼ˆå¦‚å°† $ \epsilon $ æˆ– $ c $ è®¾ä¸ºå¯å­¦ä¹ å˜é‡ï¼‰
3. **ç»“åˆå…¶ä»– verified learning å·¥å…·**ï¼ˆå¦‚ Taylor modelsï¼‰
4. **GPU åŠ é€Ÿè®­ç»ƒæµç¨‹**
5. **æ¢ç´¢æ›´é«˜æ•ˆçš„ sub/super-solution å‚æ•°åŒ–æ–¹å¼**

---

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> æœ¬è®ºæ–‡æå‡ºäº†é¦–ä¸ªå°† PINNs ä¸ verified computing ç›¸ç»“åˆçš„â€œ**Learn and Verify**â€æ¡†æ¶ï¼Œå®ç°äº†å¯¹ç¥ç»ç½‘ç»œè§£çš„**æ•°å­¦å¯è¯æ˜è¯¯å·®ç•Œè®¤è¯**ï¼Œä¸ºæ„å»º**å¯ä¿¡çš„ç§‘å­¦æœºå™¨å­¦ä¹ **å¥ å®šäº†åŸºç¡€ã€‚

</details>

---

### 15. [Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems](https://arxiv.org/abs/2601.19311)

**Authors**: Anh Khoa Ngo Ho, Martin Chauvin, Simon Gosset, Philippe Cordier, Boris Gamazaychikov  
**Category**: cs.AI  
**Published**: 2026-01-28  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.19311v1  

#### Abstract
As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising respon...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Balancing Sustainability and Performance: The Role of Small-Scale LLMs in Agentic Artificial Intelligence Systems*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬ç ”ç©¶èšç„¦äº**å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ™ºèƒ½ä½“ï¼ˆagenticï¼‰AIç³»ç»Ÿä¸­æ¨ç†é˜¶æ®µçš„é«˜èƒ½è€—é—®é¢˜**ï¼Œæ¢è®¨å¦‚ä½•åœ¨ä¸ç‰ºç‰²å“åº”é€Ÿåº¦å’Œè¾“å‡ºè´¨é‡çš„å‰æä¸‹ï¼Œé€šè¿‡éƒ¨ç½²å°è§„æ¨¡LLMsæ¥æå‡ç³»ç»Ÿçš„å¯æŒç»­æ€§ã€‚å½“å‰ï¼Œéšç€LLMsåœ¨å¤šæ™ºèƒ½ä½“æ¶æ„ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå…¶ç¯å¢ƒè¶³è¿¹ï¼ˆå°¤å…¶æ˜¯èƒ½æºæ¶ˆè€—ï¼‰æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
- **æ„å»ºäº†ä¸€ä¸ªä¸‰ç›®æ ‡ä¼˜åŒ–æ¡†æ¶**ï¼šä» **Environmental Impactï¼ˆç¯å¢ƒå½±å“ï¼‰**ã€**User Experienceï¼ˆç”¨æˆ·ä½“éªŒï¼‰** å’Œ **Output Qualityï¼ˆè¾“å‡ºè´¨é‡ï¼‰** ä¸‰ä¸ªç»´åº¦ç»¼åˆè¯„ä¼°LLMçš„éƒ¨ç½²è¡¨ç°ã€‚
- **æå‡ºå¹¶åº”ç”¨äº†å¯å¤ç°çš„åŸºå‡†æµ‹è¯•æµç¨‹**ï¼šåŸºäº `ML-Energy Benchmark` æ¡†æ¶ï¼Œåœ¨çœŸå®ä¸–ç•Œå¤šæ™ºèƒ½ä½“åœºæ™¯ä¸‹å¯¹å¤šç§å¼€æºæƒé‡ï¼ˆopen-weightsï¼‰LLMsè¿›è¡Œç³»ç»Ÿæ€§æ¯”è¾ƒã€‚
- **å¼•å…¥ Overall Metric (OM)**ï¼šä¸€ä¸ªåŠ æƒç»¼åˆè¯„åˆ†å…¬å¼ï¼Œç”¨äºç»Ÿä¸€è¡¡é‡ä¸åŒæ¨¡å‹åœ¨èƒ½æ•ˆã€å»¶è¿Ÿå’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼š
  $$
  OM = \left(\frac{Quality_E}{Quality_R}\right)^{W_{Quality}} \times \left(\frac{Latency_R}{Latency_E}\right)^{W_{Latency}} \times \left(\frac{Energy_R}{Energy_E}\right)^{W_{Energy}}
  $$
  å…¶ä¸­å€¼å¤§äº1è¡¨ç¤ºä¼˜äºåŸºçº¿æ¨¡å‹ï¼ˆGPT-4oï¼‰ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **æ›´å…¨é¢çš„è¯„ä¼°è§†è§’**ï¼šä¸åŒäºä»¥å¾€ä»…å…³æ³¨å•ä¸€ç»´åº¦ï¼ˆå¦‚å‡†ç¡®ç‡æˆ–å»¶è¿Ÿï¼‰ï¼Œæœ¬æ–‡é¦–æ¬¡åœ¨**çœŸå®éƒ¨ç½²ç¯å¢ƒ**ä¸­è”åˆåˆ†æä¸‰ä¸ªå…³é”®ç›®æ ‡ã€‚
- **å¼ºè°ƒå®é™…å¯è¡Œæ€§**ï¼šä¸ä»…è¯„ä¼°åŸå§‹æ¨¡å‹ï¼Œè¿˜çº³å…¥é‡åŒ–ï¼ˆQuantizationï¼‰ã€çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillation, KDï¼‰ç­‰å‹ç¼©æŠ€æœ¯ï¼Œå¹¶è€ƒè™‘ç¡¬ä»¶é…ç½®ï¼ˆGPUæ•°é‡ï¼‰ã€æ‰¹å¤„ç†å¤§å°ï¼ˆbatch sizeï¼‰çš„å½±å“ã€‚
- **æ”¯æŒå¼€æ”¾æ¨¡å‹æ›¿ä»£é—­æºæ¨¡å‹**ï¼šä¸ºä½¿ç”¨å¼€æºLLMsæ›¿ä»£GPT-4oç±»é—­æºæ¨¡å‹æä¾›äº†å®è¯ä¾æ®å’Œå®ç”¨æŒ‡å—ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- æ•°æ®æ¥æºäºä¸€ä¸ªå·²éƒ¨ç½²çš„çœŸå®å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒåŒ…å« **1,000ä¸ªä»£è¡¨æ€§è¯·æ±‚**ã€‚
- å¹³å‡æ¯ä¸ªè¯·æ±‚çº¦ **8,000 tokens**ï¼Œæœ€é•¿è¾¾ **25,500 tokens**ã€‚
- è¾“å‡ºå¹³å‡é•¿åº¦ä¸º **66 tokens**ï¼Œæœ€å¤§ä¸º **118 tokens**ã€‚
- æ‰€æœ‰å‚è€ƒæ ‡ç­¾ç”± **GPT-4o** åœ¨çœŸå®ç¯å¢ƒä¸­ç”Ÿæˆï¼Œä½œä¸ºè¯„ä¼°å…¶ä»–æ¨¡å‹çš„æ ‡å‡†ã€‚

### å®éªŒè®¾ç½®
- **è¯„ä¼°å¯¹è±¡**ï¼šå…±æµ‹è¯• **28ä¸ªå¼€æºLLMs** åŠå…¶ **7ä¸ªå‹ç¼©ç‰ˆæœ¬**ï¼Œæ¶µç›–å¤šä¸ªæ¨¡å‹å®¶æ—ï¼š
  - Qwen 2.5 / Qwen 3ï¼ˆ0.5Bâ€“72Bï¼‰
  - Mistralï¼ˆ12Bâ€“123Bï¼‰
  - Gemma 3ï¼ˆ4Bâ€“27Bï¼‰
  - Falcon 3ï¼ˆ1Bâ€“10Bï¼‰
  - Phi-4ï¼ˆ14Bï¼‰
  - Llama-4-Scoutï¼ˆMoE, ~109B totalï¼‰
  - è’¸é¦æ¨¡å‹ï¼ˆå¦‚ DeepSeek-R1-Distill-Qwenï¼‰
  - é‡åŒ–æ¨¡å‹ï¼ˆGPTQ-Int4/8, AWQï¼‰
- **éƒ¨ç½²å¹³å°**ï¼šAWS `p4d.24xlarge` å®ä¾‹ï¼ˆ8Ã—NVIDIA A100 40GB GPUï¼‰
- **æ¨ç†å¼•æ“**ï¼š`vLLM v0.5.4`ï¼Œæ”¯æŒé«˜æ•ˆæ‰¹å¤„ç†å’ŒæœåŠ¡
- **èƒ½é‡æµ‹é‡å·¥å…·**ï¼šé›†æˆ `Zeus` åº“ï¼Œé‡‡é›†ç¨³æ€ä¸‹çš„æ¯tokenèƒ½è€—ï¼Œè¿›è€Œä¼°ç®—æ¯è¯·æ±‚èƒ½è€—
- **æ‰¹å¤„ç†å¤§å°ï¼ˆBatch Sizeï¼‰å˜é‡**ï¼šæµ‹è¯•äº†ä» 2 åˆ° 512 çš„å¤šç§æœ€å¤§æ‰¹å¤„ç†é…ç½®

### è¯„ä¼°æŒ‡æ ‡
| ç»´åº¦ | æŒ‡æ ‡ | è¯´æ˜ |
|------|------|------|
| **Environmental Impact** | Energy per Request (Joules) | ä½¿ç”¨ ML-Energy Benchmark æµ‹é‡GPUæ¨ç†èƒ½è€— |
| **User Experience** | Decode Latency per Request (Seconds) | è§£ç é˜¶æ®µè€—æ—¶ï¼Œåæ˜ ç”¨æˆ·æ„ŸçŸ¥å“åº”é€Ÿåº¦ |
| **Output Quality** | F1-score (Macro) | åˆ†ç±»æ ‡ç­¾ï¼ˆgrounded/ungrounded/small talkï¼‰ä¸GPT-4oçš„ä¸€è‡´æ€§ |
| | LLM-as-a-Judge Score (0â€“1) | ä½¿ç”¨æ›´å¼ºLLMï¼ˆGPT-5ï¼‰æŒ‰é¢„å®šä¹‰æ ‡å‡†æ‰“åˆ†ï¼Œè¯„ä¼°å›å¤äº‹å®å‡†ç¡®æ€§ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **ä¸»åŸºçº¿**ï¼š`GPT-4o`ï¼ˆé€šè¿‡APIè®¿é—®ï¼‰
- GPT-4oçš„èƒ½è€—ä¸å»¶è¿ŸåŸºäº **jegham*how*2025** çš„ä¼°ç®—æ–¹æ³•æ¨å¯¼å¾—å‡ºï¼ˆå› æ— æ³•ç›´æ¥ç›‘æ§ç¡¬ä»¶ï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 3ï¼‰

| Model | Energy (J) â†“ | Decode Latency (s) â†“ | F1-score â†‘ | LLM-as-a-Judge â†‘ |
|-------|--------------|------------------------|------------|------------------|
| **GPT-4o (baseline)** | 1499 Â± 287 | 0.58 Â± 0.10 | 1.000 | 0.978 Â± 0.116 |
| **Qwen3-30B-A3B-Instruct** | **456 Â± 97** | **50 Â± 10** | 0.917 | 0.956 Â± 0.156 |
| **Qwen3-32B** | 2382 Â± 725 | 85 Â± 26 | 0.918 | **0.992 Â± 0.064** |
| **Falcon3-7B-Instruct** | **335 Â± 348** | 17 Â± 17 | **0.910** | 0.885 Â± 0.245 |

> æ³¨ï¼šæœ€ä¼˜å€¼åŠ ç²—ï¼›Qwen3-30B-A3B æ˜¯å”¯ä¸€å®ç° **70% èƒ½è€—é™ä½** ä¸”è´¨é‡æ¥è¿‘GPT-4o çš„æ¨¡å‹ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **èƒ½æ•ˆä¼˜åŠ¿æ˜¾è‘—**ï¼š
  - å¤šä¸ªå°æ¨¡å‹ï¼ˆå¦‚ Falcon3-7B, Qwen3-30B-A3Bï¼‰èƒ½è€—ä»…ä¸º GPT-4o çš„ **22%-30%**ã€‚
  - Qwen3-30B-A3B-Instructï¼ˆMoEæ¶æ„ï¼‰æ¿€æ´»å‚æ•°ä»… **3.3B**ï¼Œå´è¾¾åˆ°æ¥è¿‘ GPT-4o çš„è¾“å‡ºè´¨é‡ã€‚
- **å»¶è¿Ÿæ–¹é¢é—­æºä»å ä¼˜**ï¼š
  - GPT-4o çš„ decode latency æœ€çŸ­ï¼ˆ~0.58ç§’ï¼‰ï¼Œå¾—ç›Šäºé«˜åº¦ä¼˜åŒ–çš„æœåŠ¡ç«¯éƒ¨ç½²ã€‚
  - å¼€æºæ¨¡å‹æœ¬åœ°éƒ¨ç½²æ™®éå»¶è¿Ÿæ›´é«˜ï¼Œéƒ¨åˆ†å› è¾“å‡ºæ›´é•¿æˆ–æœªå……åˆ†è°ƒä¼˜ã€‚
- **è¾“å‡ºè´¨é‡å¯åª²ç¾ç”šè‡³è¶…è¶Š**ï¼š
  - Qwen3-32B åœ¨ LLM-as-a-Judge ä¸Šå¾—åˆ† **0.992 > 0.978**ï¼ˆGPT-4oï¼‰ï¼Œè¡¨æ˜å…¶å›å¤æ›´å…·äº‹å®æ€§å’Œæ¸…æ™°åº¦ã€‚
  - å¤šæ•°10Bä»¥ä¸Šæ¨¡å‹F1-scoreä¿æŒåœ¨0.8ä»¥ä¸Šï¼Œæ»¡è¶³ä¼ä¸šçº§éœ€æ±‚ã€‚

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰æ¨¡å‹å°ºå¯¸ç¼©æ”¾åˆ†æï¼ˆQwen 2.5ç³»åˆ—ï¼‰
- **å¹¶éè¶Šå¤§è¶Šå¥½**ï¼š7Bæ¨¡å‹çš„F1-scoreï¼ˆ0.74ï¼‰é«˜äº14Bï¼ˆ0.68ï¼‰å’Œ32Bï¼ˆ0.70ï¼‰ï¼Œè¯´æ˜å­˜åœ¨â€œç”œç‚¹â€è§„æ¨¡ã€‚
- **èƒ½è€—éšè§„æ¨¡æŒ‡æ•°å¢é•¿**ï¼š72Bæ¨¡å‹éœ€8Ã—GPUï¼Œèƒ½è€—æ˜¯7Bçš„æ•°åå€ï¼Œè€Œæ€§èƒ½æå‡æœ‰é™ã€‚
- **decode latency è¿‘ä¼¼çº¿æ€§å¢é•¿**ï¼Œä½†å—å¤šGPUé€šä¿¡å¼€é”€å½±å“ã€‚

#### ï¼ˆ2ï¼‰å‹ç¼©æŠ€æœ¯æ•ˆæœ
| æ–¹æ³• | æ¨¡å‹ | èƒ½è€—å˜åŒ– | å»¶è¿Ÿå˜åŒ– | F1-score |
|------|------|----------|----------|-----------|
| GPTQ-Int4 | Qwen2.5-7B | â†“ ~20% | â†“ ~20% | â†‘ï¼ˆä¼˜äºåŸç‰ˆï¼‰ |
| AWQ | Qwen2.5-7B | å¾®é™ | â†‘ï¼ˆå˜æ…¢ï¼‰ | â€” |
| KDï¼ˆè’¸é¦è‡³1.5Bï¼‰ | DeepSeek-R1 â†’ Qwen | â†“ | â†“ | < 7BåŸæ¨¡å‹ |
| KDï¼ˆåº”ç”¨äº7Bï¼‰ | DeepSeek-R1 â†’ Qwen | â†“ | â†“ | â†“â†“ï¼ˆF1ä¸‹é™29%ï¼‰ |

> ç»“è®ºï¼š**GPTQ-Int4 æ˜¯æœ€æœ‰æ•ˆçš„å‹ç¼©æ–¹æ¡ˆ**ï¼›çŸ¥è¯†è’¸é¦å¯¹å°æ¨¡å‹æœ‰æ•ˆï¼Œä½†åœ¨åŒè§„æ¨¡ä¸Šå¯èƒ½æŸå®³æ€§èƒ½ã€‚

#### ï¼ˆ3ï¼‰æ‰¹å¤„ç†å¤§å°å½±å“ï¼ˆè§ Figure 6ï¼‰
- å­˜åœ¨æ˜æ˜¾çš„ **Energy-Latency æƒè¡¡æ›²çº¿ï¼ˆPareto Frontierï¼‰**
- å°æ¨¡å‹å¯¹ batch size æ›´æ•æ„Ÿï¼š
  - å°† Qwen2.5-1.5B çš„ batch size æå‡è‡³512ï¼Œå¯èŠ‚èƒ½ **81%**ï¼Œä½†å»¶è¿Ÿå¢åŠ  **>400%**
- å¤§æ¨¡å‹å¯¹æ­¤è°ƒèŠ‚ä¸æ•æ„Ÿï¼Œè¾¹é™…æ•ˆç›Šé€’å‡ã€‚

#### ï¼ˆ4ï¼‰é¢å¤–GPUå¯¹ Nano-LLMs çš„å½±å“ï¼ˆè§ Figure 7ï¼‰
- å½“ max batch â‰¤ 64 æ—¶ï¼Œå•GPUå³å¯æ‰¿è½½KV cacheï¼Œå¢åŠ GPUæ— ç›Šã€‚
- å½“ max batch > 64 åï¼Œç¬¬äºŒå—GPUå…è®¸æ›´å¤§å¹¶å‘ï¼Œä½†ä¼šå¸¦æ¥é¢å¤–èƒ½è€—å’Œå»¶è¿Ÿã€‚
- æ¨èç­–ç•¥ï¼š**æ ¹æ®è¯·æ±‚é¢‘ç‡å’ŒKV cacheå®¹é‡åŠ¨æ€åˆ†é…GPUèµ„æº**

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **å°è§„æ¨¡å¼€æºLLMså¯åœ¨ä¿æŒé«˜è´¨é‡çš„åŒæ—¶å¤§å¹…é™ä½èƒ½è€—**ï¼Œæ˜¯æ„å»ºå¯æŒç»­AIç³»ç»Ÿçš„å¯è¡Œè·¯å¾„ã€‚
2. âœ… **æ¨¡å‹å¤§å°ä¸æ€§èƒ½ä¸æˆæ­£æ¯”**ï¼šä¸­ç­‰è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚7Bâ€“14Bï¼‰å¸¸ä¼˜äºæ›´å¤§æ¨¡å‹ï¼Œå°¤å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šã€‚
3. âœ… **æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„æå…·æ½œåŠ›**ï¼šQwen3-30B-A3B ä»¥æä½æ¿€æ´»å‚æ•°å®ç°é«˜æ€§èƒ½ä¸é«˜èƒ½æ•ˆå¹³è¡¡ã€‚
4. âœ… **æ‰¹å¤„ç†å¤§å°æ˜¯å…³é”®è°ƒæ§å‚æ•°**ï¼šåˆç†è®¾ç½®å¯æ˜¾è‘—ä¼˜åŒ–èƒ½æ•ˆï¼Œä½†éœ€è­¦æƒ•å»¶è¿Ÿä»£ä»·ã€‚
5. âœ… **å‹ç¼©æŠ€æœ¯æœ‰æ•ˆä½†éœ€è°¨æ…é€‰æ‹©**ï¼šGPTQ-Int4 è¡¨ç°æœ€ä½³ï¼›çŸ¥è¯†è’¸é¦é€‚ç”¨äºå­¦ç”Ÿæ¨¡å‹è®­ç»ƒï¼Œä¸é€‚åˆç›´æ¥æ›¿æ¢åŒè§„æ¨¡æ¨¡å‹ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **ä»»åŠ¡ç‰¹å®šæ€§**ï¼šå®éªŒé›†ä¸­åœ¨â€œå¹»è§‰æ£€æµ‹â€è¿™ä¸€å…·ä½“ä»»åŠ¡ï¼Œç»“è®ºæœªå¿…æ³›åŒ–åˆ°è§„åˆ’ã€å·¥å…·è°ƒç”¨ç­‰å¤æ‚æ™ºèƒ½ä½“è¡Œä¸ºã€‚
- **æ•°æ®åå·®**ï¼šæç¤ºè¯é’ˆå¯¹ GPT-4o è®¾è®¡ï¼Œå¯èƒ½å¯¼è‡´å°æ¨¡å‹è¾“å‡ºå†—é•¿æˆ–æ ¼å¼é”™è¯¯ã€‚
- **é—­æºæ¨¡å‹ä¾èµ–ä¼°ç®—**ï¼šGPT-4o çš„èƒ½è€—ä¸å»¶è¿Ÿä¸ºé—´æ¥ä¼°è®¡ï¼Œç¼ºä¹ç¡¬ä»¶çº§éªŒè¯ã€‚
- **ç¼ºå°‘äººç±»è¯„ä¼°**ï¼šLLM-as-a-Judge å¯èƒ½å¼•å…¥åè§ï¼Œæœªæ¥åº”ç»“åˆäººå·¥æ ‡æ³¨ã€‚
- **åŸºç¡€è®¾æ–½é™åˆ¶**ï¼šå®éªŒä»…åœ¨ AWS A100 ä¸Šè¿›è¡Œï¼Œæœªæ¢ç´¢H100ã€TPUæˆ–å…¶ä»–äº‘å‚å•†å·®å¼‚ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- ğŸ”„ **æ‰©å±•è‡³æ›´å¤šä»»åŠ¡ç±»å‹**ï¼šå°†åŸºå‡†æ¨å¹¿è‡³å¤šè½®å¯¹è¯ã€ä»»åŠ¡è§„åˆ’ã€å·¥å…·ä½¿ç”¨ç­‰å…¸å‹ agentic åœºæ™¯ã€‚
- ğŸ” **å¼•å…¥å¤šæ ·åŒ–æ•°æ®é›†**ï¼šè¦†ç›–ä¸åŒè¯­è¨€ã€é¢†åŸŸã€prompté•¿åº¦çš„ä»»åŠ¡ï¼Œå¢å¼ºæ™®é€‚æ€§ã€‚
- âš™ï¸ **ä¼˜åŒ–è‡ªæ‰˜ç®¡éƒ¨ç½²æ ˆ**ï¼š
  - å‡çº§è‡³ H100 ç­‰æ–°ä¸€ä»£GPU
  - åº”ç”¨ TensorRT-LLMã€speculative decoding ç­‰é«˜çº§æ¨ç†ä¼˜åŒ–
  - æ¢ç´¢ disaggregated serving æ¶æ„
- âœï¸ **åŠ å¼ºæç¤ºå·¥ç¨‹ç ”ç©¶**ï¼šè®¾è®¡è½»é‡ã€é«˜æ•ˆçš„ prompts ä»¥å‡å°‘ prefill é˜¶æ®µèƒ½è€—ã€‚
- ğŸŒ **å…¨ç”Ÿå‘½å‘¨æœŸç¢³è¶³è¿¹å»ºæ¨¡**ï¼šç»“åˆç”µåŠ›æ¥æºã€PUEã€å†·å´æ°´è€—ç­‰å› ç´ ï¼Œå»ºç«‹æ›´å®Œæ•´çš„å¯æŒç»­æ€§è¯„ä¼°ä½“ç³»ã€‚

---

> **æœ€ç»ˆå»ºè®®**ï¼šå¯¹äºä¼ä¸šçº§ agentic AI ç³»ç»Ÿï¼Œæ¨èä¼˜å…ˆè€ƒè™‘ **Qwen3-30B-A3B-Instruct** æˆ– **Falcon3-7B-Instruct** ç­‰é«˜æ•ˆæ¨¡å‹ï¼Œå¹¶ç»“åˆ **GPTQé‡åŒ–** ä¸ **æœ€ä¼˜batch sizeé…ç½®**ï¼Œåœ¨ä¿éšœæœåŠ¡è´¨é‡çš„åŒæ—¶å®ç°é«˜è¾¾70%çš„èƒ½è€—èŠ‚çº¦ã€‚

</details>

---

### 16. [Modular Foundation Model Inference at the Edge: Network-Aware Microservice Optimization](https://arxiv.org/abs/2601.19563)

**Authors**: Juan Zhu, Zixin Wang, Shenghui Song, Jun Zhang, Khaled Ben Letaief  
**Category**: cs.DC  
**Published**: 2026-01-28  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.19563v1  

#### Abstract
Foundation models (FMs) unlock unprecedented multimodal and multitask intelligence, yet their cloud-centric deployment precludes real-time responsiveness and compromises user privacy. Meanwhile, monolithic execution at the edge remains infeasible under stringent resource limits and uncertain network...

---

### 17. [GraIP: A Benchmarking Framework For Neural Graph Inverse Problems](https://arxiv.org/abs/2601.18917)

**Authors**: Semih Cant\"urk, Andrei Manolache, Arman Mielke, Chendi Qian, Antoine Siraudin, Christopher Morris, Mathias Niepert, Guy Wolf  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.18917v1  

#### Abstract
A wide range of graph learning tasks, such as structure discovery, temporal graph analysis, and combinatorial optimization, focus on inferring graph structures from data, rather than making predictions on given graphs. However, the respective methods to solve such problems are often developed in an ...

---

### 18. [Accelerated training of Gaussian processes using banded square exponential covariances](https://arxiv.org/abs/2601.19007)

**Authors**: Emily C. Ehrhardt, Felipe Tobar  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.19007v1  

#### Abstract
We propose a novel approach to computationally efficient GP training based on the observation that square-exponential (SE) covariance matrices contain several off-diagonal entries extremely close to zero. We construct a principled procedure to eliminate those entries to produce a \emph{banded}-matri...

---

### 19. [E-QRGMM: Efficient Generative Metamodeling for Covariate-Dependent Uncertainty Quantification](https://arxiv.org/abs/2601.19256)

**Authors**: Zhiyang Liang, Qingkai Zhang  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.19256v1  

#### Abstract
Covariate-dependent uncertainty quantification in simulation-based inference is crucial for high-stakes decision-making but remains challenging due to the limitations of existing methods such as conformal prediction and classical bootstrap, which struggle with covariate-specific conditioning. We pro...

---

### 20. [SEAFormer: A Spatial Proximity and Edge-Aware Transformer for Real-World Vehicle Routing Problems](https://arxiv.org/abs/2601.19395)

**Authors**: Saeed Nasehi Basharzad, Farhana Choudhury, Egemen Tanin  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.19395v1  

#### Abstract
Real-world Vehicle Routing Problems (RWVRPs) require solving complex, sequence-dependent challenges at scale with constraints such as delivery time window, replenishment or recharging stops, asymmetric travel cost, etc. While recent neural methods achieve strong results on large-scale classical VRP ...

---

### 21. [OSIRIS: Bridging Analog Circuit Design and Machine Learning with Scalable Dataset Generation](https://arxiv.org/abs/2601.19439)

**Authors**: Giuseppe Chiari, Michele Piccoli, Davide Zoni  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.19439v1  

#### Abstract
The automation of analog integrated circuit (IC) design remains a longstanding challenge, primarily due to the intricate interdependencies among physical layout, parasitic effects, and circuit-level performance. These interactions impose complex constraints that are difficult to accurately capture a...

---

### 22. [SETA: Statistical Fault Attribution for Compound AI Systems](https://arxiv.org/abs/2601.19337)

**Authors**: Sayak Chowdhury, Meenakshi D'Souza  
**Category**: cs.AI  
**Published**: 2026-01-28  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.19337v1  

#### Abstract
Modern AI systems increasingly comprise multiple interconnected neural networks to tackle complex inference tasks. Testing such systems for robustness and safety entails significant challenges. Current state-of-the-art robustness testing techniques, whether black-box or white-box, have been proposed...

---

### 23. [ComAgent: Multi-LLM based Agentic AI Empowered Intelligent Wireless Networks](https://arxiv.org/abs/2601.19607)

**Authors**: Haoyun Li, Ming Xiao, Kezhi Wang, Robert Schober, Dong In Kim, Yong Liang Guan  
**Category**: cs.AI  
**Published**: 2026-01-28  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.19607v1  

#### Abstract
Emerging 6G networks rely on complex cross-layer optimization, yet manually translating high-level intents into mathematical formulations remains a bottleneck. While Large Language Models (LLMs) offer promise, monolithic approaches often lack sufficient domain grounding, constraint awareness, and ve...

---

### 24. [Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search](https://arxiv.org/abs/2601.19622)

**Authors**: Thomas B\"omer, Nico Koltermann, Max Disselnmeyer, Bastian Amberg, Anne Meyer  
**Category**: cs.AI  
**Published**: 2026-01-28  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.19622v1  

#### Abstract
Heuristic functions are essential to the performance of tree search algorithms such as A*, where their accuracy and efficiency directly impact search outcomes. Traditionally, such heuristics are handcrafted, requiring significant expertise. Recent advances in large language models (LLMs) and evoluti...

---

### 25. [Optimizing Conversational Quality in Spoken Dialogue Systems with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2601.19063)

**Authors**: Siddhant Arora, Jinchuan Tian, Jiatong Shi, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe  
**Category**: cs.CL  
**Published**: 2026-01-28  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.19063v1  

#### Abstract
Reinforcement learning from human or AI feedback (RLHF/RLAIF) for speech-in/speech-out dialogue systems (SDS) remains underexplored, with prior work largely limited to single semantic rewards applied at the utterance level. Such setups overlook the multi-dimensional and multi-modal nature of convers...

---

### 26. [Vector-Valued Distributional Reinforcement Learning Policy Evaluation: A Hilbert Space Embedding Approach](https://arxiv.org/abs/2601.18952)

**Authors**: Mehrdad Mohammadi, Qi Zheng, Ruoqing Zhu  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.18952v1  

#### Abstract
We propose an (offline) multi-dimensional distributional reinforcement learning framework (KE-DRL) that leverages Hilbert space mappings to estimate the kernel mean embedding of the multi-dimensional value distribution under a proposed target policy. In our setting, the state-action variables are mu...

---

### 27. [Speed is Confidence](https://arxiv.org/abs/2601.19085)

**Authors**: Joshua V. Dillon  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.19085v1  

#### Abstract
Biological neural systems must be fast but are energy-constrained. Evolution's solution: act on the first signal. Winner-take-all circuits and time-to-first-spike coding implicitly treat when a neuron fires as an expression of confidence. We apply this principle to ensembles of Tiny Recursive Models...

---

### 28. [Smoothing the Score Function for Generalization in Diffusion Models: An Optimization-based Explanation Framework](https://arxiv.org/abs/2601.19285)

**Authors**: Xinyu Zhou, Jiawei Zhang, Stephen J. Wright  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.19285v1  

#### Abstract
Diffusion models achieve remarkable generation quality, yet face a fundamental challenge known as memorization, where generated samples can replicate training samples exactly. We develop a theoretical framework to explain this phenomenon by showing that the empirical score function (the score functi...

---

### 29. [Improving Policy Exploitation in Online Reinforcement Learning with Instant Retrospect Action](https://arxiv.org/abs/2601.19720)

**Authors**: Gong Gao, Weidong Zhao, Xianhui Liu, Ning Jia  
**Category**: cs.LG  
**Published**: 2026-01-28  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.19720v1  

#### Abstract
Existing value-based online reinforcement learning (RL) algorithms suffer from slow policy exploitation due to ineffective exploration and delayed policy updates. To address these challenges, we propose an algorithm called Instant Retrospect Action (IRA). Specifically, we propose Q-Representation Di...

---

### 30. [Neural Theorem Proving for Verification Conditions: A Real-World Benchmark](https://arxiv.org/abs/2601.18944)

**Authors**: Qiyuan Xu, Xiaokun Luan, Renxi Wang, Joshua Ong Jun Leang, Peixin Wang, Haonan Li, Wenda Li, Conrad Watt  
**Category**: cs.AI  
**Published**: 2026-01-28  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.18944v1  

#### Abstract
Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for ...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
