# arXiv Papers Bot 🤖

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## 📊 Statistics

- **Last Updated**: 2025-09-11 12:49:16 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## 📚 Recent Papers

### 1. [FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models](https://arxiv.org/abs/2403.09904)

**Authors**: Kai Yi, Georg Meinhardt, Laurent Condat, Peter Richt\'arik  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 10.5

arXiv:2403.09904v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A criti...

---

### 2. [Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism](https://arxiv.org/abs/2509.08309)

**Authors**: Zizhao Mo, Jianxiong Liao, Huanle Xu, Zhi Zhou, Chengzhong Xu  
**Category**: cs.DC  
**Published**: 2025-09-11  
**Score**: 10.5

arXiv:2509.08309v1 Announce Type: new 
Abstract: The significant resource demands in LLM serving prompts production clusters to fully utilize heterogeneous hardware by partitioning LLM models across a mix of high-end and low-end GPUs. However, existing parallelization approaches often struggle to sc...

---

### 3. [Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing](https://arxiv.org/abs/2509.08721)

**Authors**: Jeffrey Amico, Gabriel Passamani Andrade, John Donaghy, Ben Fielding, Tristin Forbus, Harry Grieve, Semih Kara, Jari Kolehmainen, Yihua Lou, Christopher Nies, Edward Phillip Flores Nu\~no, Diogo Ortega, Shikhar Rastogi, Austin Virts, Matthew J. Wright  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 9.5

arXiv:2509.08721v1 Announce Type: new 
Abstract: Post-training language models (LMs) with reinforcement learning (RL) can enhance their complex reasoning capabilities without supervised fine-tuning, as demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs requires significant p...

---

### 4. [Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization](https://arxiv.org/abs/2509.08233)

**Authors**: Kai Yi  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 9.0

arXiv:2509.08233v1 Announce Type: cross 
Abstract: Distributed and federated learning are essential paradigms for training models across decentralized data sources while preserving privacy, yet communication overhead remains a major bottleneck. This dissertation explores strategies to improve commun...

---

### 5. [KLLM: Fast LLM Inference with K-Means Quantization](https://arxiv.org/abs/2507.23035)

**Authors**: Xueying Wu, Baijun Zhou, Zhihui Gao, Yuzhe Fu, Qilin Zheng, Yintao He, Hai Li  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 9.0

arXiv:2507.23035v3 Announce Type: replace 
Abstract: Large language model (LLM) inference poses significant challenges due to its intensive memory and computation demands. Weight and activation quantization (WAQ) offers a promising solution by reducing both memory footprint and arithmetic complexity...

---

### 6. [Metis: Training Large Language Models with Advanced Low-Bit Quantization](https://arxiv.org/abs/2509.00404)

**Authors**: Hengjie Cao, Mengyi Chen, Yifeng Yang, Ruijun Huang, Fang Dong, Jixian Zhou, Anrui Chen, Mingzhi Dong, Yujiang Wang, Jinlong Hou, Yuan Cheng, Fan Wu, Fan Yang, Tun Lu, Ning Gu, Li Shang  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 9.0

arXiv:2509.00404v2 Announce Type: replace 
Abstract: This work identifies anisotropic parameter distributions as a fundamental barrier to training large language models (LLMs) with low-bit quantization: a few dominant singular values create wide numerical ranges that conflict with the inherent bias ...

---

### 7. [Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators](https://arxiv.org/abs/2507.14652)

**Authors**: Ponkrshnan Thiagarajan, Tamer A. Zaki, Michael D. Shields  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 8.5

arXiv:2507.14652v2 Announce Type: replace-cross 
Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample from the posterior distribution in Bayesian inference. However, HMC techniques are computationally demanding for Bayesian neural networks due to the high dimensionalit...

---

### 8. [SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery](https://arxiv.org/abs/2509.08032)

**Authors**: Fengyu She, Nan Wang, Hongfei Wu, Ziyi Wan, Jingmian Wang, Chang Wang  
**Category**: cs.CL  
**Published**: 2025-09-11  
**Score**: 8.0

arXiv:2509.08032v1 Announce Type: new 
Abstract: Scientific literature is growing exponentially, creating a critical bottleneck for researchers to efficiently synthesize knowledge. While general-purpose Large Language Models (LLMs) show potential in text processing, they often fail to capture scient...

---

### 9. [Towards Communication-Efficient Decentralized Federated Graph Learning over Non-IID Data](https://arxiv.org/abs/2509.08409)

**Authors**: Shilong Wang, Jianchun Liu, Hongli Xu, Chenxia Tang, Qianpiao Ma, Liusheng Huang  
**Category**: cs.DC  
**Published**: 2025-09-11  
**Score**: 8.0

arXiv:2509.08409v1 Announce Type: new 
Abstract: Decentralized Federated Graph Learning (DFGL) overcomes potential bottlenecks of the parameter server in FGL by establishing a peer-to-peer (P2P) communication network among workers. However, while extensive cross-worker communication of graph node em...

---

### 10. [Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs](https://arxiv.org/abs/2509.08016)

**Authors**: Hyungjin Chung, Hyelin Nam, Jiyeon Kim, Hyojun Go, Byeongjun Park, Junho Kim, Joonseok Lee, Seongsu Ha, Byung-Hoon Kim  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 8.0

arXiv:2509.08016v1 Announce Type: cross 
Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck: increasing the number of input frames to capture fine-grained temporal detail leads to prohibitive computational costs and performance degradation from long context lengths. We intr...

---

### 11. [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)

**Authors**: Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 7.5

arXiv:2509.08755v1 Announce Type: cross 
Abstract: Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through explor...

---

### 12. [LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation](https://arxiv.org/abs/2503.13794)

**Authors**: Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Can Jin, Dimitris N. Metaxas  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 7.5

arXiv:2503.13794v5 Announce Type: replace-cross 
Abstract: Large foundation models trained on large-scale vision-language data can boost Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the hand-crafted pipelines often introduce bias and overfit to specific prompts. We sideste...

---

### 13. [MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining](https://arxiv.org/abs/2509.06806)

**Authors**: Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 7.5

arXiv:2509.06806v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonst...

---

### 14. [Associative Knowledge Graphs for Efficient Sequence Storage and Retrieval](https://arxiv.org/abs/2411.14480)

**Authors**: Przemys{\l}aw Stok{\l}osa, Janusz A. Starzyk, Pawe{\l} Raif, Adrian Horzyk, Marcin Kowalik  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2411.14480v2 Announce Type: replace 
Abstract: The paper addresses challenges in storing and retrieving sequences in contexts like anomaly detection, behavior prediction, and genetic information analysis. Associative Knowledge Graphs (AKGs) offer a promising approach by leveraging sparse graph...

---

### 15. [CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning](https://arxiv.org/abs/2502.02390)

**Authors**: Jianfeng Pan, Senyou Deng, Shaomang Huang  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2502.02390v2 Announce Type: replace-cross 
Abstract: Research on LLM technologies is rapidly emerging, with most of them employ a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent...

---

### 16. [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)

**Authors**: Jiaxuan Gao, Shu Yan, Qixin Tan, Lu Yang, Shusheng Xu, Wei Fu, Zhiyu Mei, Kaifeng Lyu, Yi Wu  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2506.07104v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving capabilities through extended Chain-of-Thought (CoT) reasoning but often produce excessively verbose and redundant reasoning traces. This inefficiency incurs high inference...

---

### 17. [Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor](https://arxiv.org/abs/2509.08020)

**Authors**: Matthew Carter, Lee Devlin, Alexander Philips, Edward Pyzer-Knapp, Paul Spirakis, Simon Maskell  
**Category**: cs.DC  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2509.08020v1 Announce Type: cross 
Abstract: Quantitative proteomics plays a central role in uncovering regulatory mechanisms, identifying disease biomarkers, and guiding the development of precision therapies. These insights are often obtained through complex Bayesian models, whose inference ...

---

### 18. [Traversal Learning: A Lossless And Efficient Distributed Learning Framework](https://arxiv.org/abs/2504.07471)

**Authors**: Erdenebileg Batbaatar, Jeonggeol Kim, Yongcheol Kim, Young Yoon  
**Category**: cs.DC  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2504.07471v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL), and S...

---

### 19. [Optimization Methods and Software for Federated Learning](https://arxiv.org/abs/2509.08120)

**Authors**: Konstantin Burlachenko  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2509.08120v1 Announce Type: new 
Abstract: Federated Learning (FL) is a novel, multidisciplinary Machine Learning paradigm where multiple clients, such as mobile devices, collaborate to solve machine learning problems. Initially introduced in Kone{\v{c}}n{\'y} et al. (2016a,b); McMahan et al. ...

---

### 20. [Data Skeleton Learning: Scalable Active Clustering with Sparse Graph Structures](https://arxiv.org/abs/2509.08530)

**Authors**: Wen-Bo Xie, Xun Fu, Bin Chen, Yan-Li Lee, Tao Deng, Tian Zou, Xin Wang, Zhen Liu, Jaideep Srivastavad  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2509.08530v1 Announce Type: new 
Abstract: In this work, we focus on the efficiency and scalability of pairwise constraint-based active clustering, crucial for processing large-scale data in applications such as data mining, knowledge annotation, and AI model pre-training. Our goals are threef...

---

### 21. [ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System](https://arxiv.org/abs/2509.08736)

**Authors**: Dong Han, Zhehong Ai, Pengxiang Cai, Shuzhou Sun, Shanya Lu, Jianpeng Chen, Ben Gao, Lingli Ge, Weida Wang, Xiangxin Zhou, Xihui Liu, Mao Su, Wanli Ouyang, Lei Bai, Dongzhan Zhou, Tao XU, Yuqiang Li, Shufei Zhang  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2509.08736v1 Announce Type: new 
Abstract: The efficiency of Bayesian optimization (BO) in chemistry is often hindered by sparse experimental data and complex reaction mechanisms. To overcome these limitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced Multi-Agent System for a...

---

### 22. [OCTANE -- Optimal Control for Tensor-based Autoencoder Network Emergence: Explicit Case](https://arxiv.org/abs/2509.08169)

**Authors**: Ratna Khatri, Anthony Kolshorn, Colin Olson, Harbir Antil  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2509.08169v1 Announce Type: cross 
Abstract: This paper presents a novel, mathematically rigorous framework for autoencoder-type deep neural networks that combines optimal control theory and low-rank tensor methods to yield memory-efficient training and automated architecture discovery. The le...

---

### 23. [MasconCube: Fast and Accurate Gravity Modeling with an Explicit Representation](https://arxiv.org/abs/2509.08607)

**Authors**: Pietro Fanti, Dario Izzo  
**Category**: cs.LG  
**Published**: 2025-09-11  
**Score**: 6.5

arXiv:2509.08607v1 Announce Type: cross 
Abstract: The geodesy of irregularly shaped small bodies presents fundamental challenges for gravitational field modeling, particularly as deep space exploration missions increasingly target asteroids and comets. Traditional approaches suffer from critical li...

---

### 24. [Efficient Decoding Methods for Language Models on Encrypted Data](https://arxiv.org/abs/2509.08383)

**Authors**: Matan Avitan, Moran Baruch, Nir Drucker, Itamar Zimerman, Yoav Goldberg  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 6.0

arXiv:2509.08383v1 Announce Type: cross 
Abstract: Large language models (LLMs) power modern AI applications, but processing sensitive data on untrusted servers raises privacy concerns. Homomorphic encryption (HE) enables computation on encrypted data for secure inference. However, neural text gener...

---

### 25. [Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension](https://arxiv.org/abs/2502.16523)

**Authors**: Yulong Wu, Viktor Schlegel, Riza Batista-Navarro  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 6.0

arXiv:2502.16523v2 Announce Type: replace-cross 
Abstract: As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evalua...

---

### 26. [MPO: Boosting LLM Agents with Meta Plan Optimization](https://arxiv.org/abs/2503.02682)

**Authors**: Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 6.0

arXiv:2503.02682v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require ...

---

### 27. [UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression](https://arxiv.org/abs/2503.02733)

**Authors**: Jia Wang, Xinfeng Zhang, Gai Zhang, Jun Zhu, Lv Tang, Li Zhang  
**Category**: cs.AI  
**Published**: 2025-09-11  
**Score**: 6.0

arXiv:2503.02733v2 Announce Type: replace-cross 
Abstract: Implicit Neural Representations (INRs) have demonstrated significant potential in video compression by representing videos as neural networks. However, as the number of frames increases, the memory consumption for training and inference incr...

---

### 28. [AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs](https://arxiv.org/abs/2509.08000)

**Authors**: Debdeep Sanyal, Manodeep Ray, Murari Mandal  
**Category**: cs.CL  
**Published**: 2025-09-11  
**Score**: 6.0

arXiv:2509.08000v1 Announce Type: new 
Abstract: The release of open-weight large language models (LLMs) creates a tension between advancing accessible research and preventing misuse, such as malicious fine-tuning to elicit harmful content. Current safety measures struggle to preserve the general ca...

---

### 29. [Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression](https://arxiv.org/abs/2509.08093)

**Authors**: Nathaniel Imel, Noga Zaslavsky  
**Category**: cs.CL  
**Published**: 2025-09-11  
**Score**: 6.0

arXiv:2509.08093v1 Announce Type: new 
Abstract: Converging evidence suggests that systems of semantic categories across human languages achieve near-optimal compression via the Information Bottleneck (IB) complexity-accuracy principle. Large language models (LLMs) are not trained for this objective...

---

### 30. [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)

**Authors**: Bohan Yu, Yekun Chai  
**Category**: cs.CL  
**Published**: 2025-09-11  
**Score**: 6.0

arXiv:2509.08315v1 Announce Type: cross 
Abstract: Existing key-value (KV) cache compression methods typically rely on heuristics, such as uniform cache allocation across layers or static eviction policies, however, they ignore the critical interplays among layer-specific feature patterns and task p...

---

## 🔧 Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## 📅 Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## 🚀 How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## 📝 Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## 🔍 Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
