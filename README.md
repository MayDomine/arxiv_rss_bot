# arXiv Papers Bot ğŸ¤–

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## ğŸ“Š Statistics

- **Last Updated**: 2025-12-08 05:56:03 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## ğŸ“š Recent Papers

### 1. [SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs](https://arxiv.org/abs/2512.05409)

**Authors**: Ruixuan Huang, Hao Zeng, Hantao Huang, Jinyuan Shi, Minghui Yu, Ian En-Hsu Yen, Shuai Wang  
**Category**: cs.CL  
**Published**: 2025-12-08  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.05409v1  

#### Abstract
Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰é¢ä¸´**ç¡¬ä»¶-ç®—æ³•ä¸åŒ¹é…**çš„ç“¶é¢ˆï¼š
- **ä½æ¯”ç‰¹é‡åŒ–ï¼ˆå¦‚ W4A4ï¼‰** è™½ç„¶ç†è®ºä¸Šé«˜æ•ˆï¼Œä½†ä¸¥é‡æŸå®³æ¨¡å‹ç²¾åº¦ã€‚
- **æ··åˆç²¾åº¦ï¼ˆå¦‚ W4A8ï¼‰** åœ¨ç°ä»£GPUä¸Šæ— æ³•é«˜æ•ˆæ‰§è¡Œï¼Œå› ç¼ºä¹åŸç”Ÿæ”¯æŒï¼Œå®é™…é€€åŒ–ä¸ºæ›´æ…¢çš„ W8A8 è·¯å¾„ï¼Œå¯¼è‡´ååä¼˜åŠ¿æ— æ³•ä½“ç°ã€‚
- **ç¨€ç–åŒ–ï¼ˆå¦‚ 2:4 semi-structure sparseï¼‰** è™½è¢«ç¡¬ä»¶æ”¯æŒï¼Œä½†å¯¹éå‡åŒ€åˆ†å¸ƒçš„ä¿¡æ¯å‹ç¼©æ•ˆæœå·®ï¼Œä¸”éš¾ä»¥ä¸é‡åŒ–ååŒä¼˜åŒ–ã€‚

å› æ­¤ï¼Œå¦‚ä½•åœ¨**ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®ç°çœŸæ­£çš„é«˜åå**ï¼Œæ˜¯å½“å‰éƒ¨ç½²LLMçš„å…³é”®æŒ‘æˆ˜ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šSQ-formatï¼ˆSparse-Quantized Formatï¼‰
ä½œè€…æå‡ºä¸€ç§**ç»Ÿä¸€çš„ç¨€ç–-é‡åŒ–ç¡¬ä»¶å‹å¥½æ•°æ®æ ¼å¼**ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
> å°†ä¸€ä¸ªæ“ä½œæ•°ï¼ˆæƒé‡æˆ–æ¿€æ´»ï¼‰åˆ’åˆ†ä¸º**é«˜ç²¾åº¦ç¨€ç–éƒ¨åˆ†**å’Œ**ä½ç²¾åº¦å¯†é›†éƒ¨åˆ†**ï¼Œé€šè¿‡ç¡¬ä»¶å‹å¥½çš„æ–¹å¼å¹¶è¡Œå¤„ç†ï¼Œä»è€Œå®ç°ç²¾åº¦ä¸ååçš„å¸•ç´¯æ‰˜æ”¹è¿›ï¼ˆPareto improvementï¼‰ã€‚

#### åˆ›æ–°ç‚¹ï¼š
1. **ç»Ÿä¸€ç¨€ç–-é‡åŒ–æ¡†æ¶**  
   - å®šä¹‰äº† `SQ-format(X) = ([X_quant], [S_quant], [m], h_high, h_low, b, s)`ï¼Œå…¶ä¸­ï¼š
     - `h_high` å’Œ `h_low` åˆ†åˆ«è¡¨ç¤ºé«˜ä½ç²¾åº¦ä½å®½ï¼ˆå¦‚ INT8 / INT4ï¼‰
     - `b` æ˜¯ bank sizeï¼Œ`s` æ˜¯æ¯ bank å†…çš„ç¨€ç–åº¦
     - `[m]` æ˜¯ç²¾åº¦æ©ç ï¼Œå†³å®šå“ªäº›å…ƒç´ ä½¿ç”¨é«˜ç²¾åº¦
   - æ”¯æŒæ•´å‹ï¼ˆINTï¼‰å’Œæµ®ç‚¹ï¼ˆFPï¼‰æ ¼å¼ï¼ˆå¦‚ FP8/FP4ï¼‰

2. **ç¡¬ä»¶ååŒè®¾è®¡ï¼ˆCo-designï¼‰**
   - è®¾è®¡äº†é€‚ç”¨äºå½“å‰ GPU å’Œä¸‹ä¸€ä»£ AI åŠ é€Ÿå™¨çš„ç¡¬ä»¶å®ç°è·¯å¾„ï¼š
     - å¯¹äºæƒé‡ï¼šé«˜ç²¾åº¦éƒ¨åˆ†ç´§å‡‘å­˜å‚¨ï¼Œç”±ä¸“ç”¨ gather å•å…ƒæå–ï¼Œä¸ä½ç²¾åº¦è·¯å¾„å¹¶è¡Œè®¡ç®—ã€‚
     - å¯¹äºæ¿€æ´»ï¼šæå‡º**é™æ€ç­–ç•¥ï¼ˆstatic strategyï¼‰**ï¼Œåœ¨æ ¡å‡†é›†ä¸Šé¢„ç”Ÿæˆæ©ç ï¼Œé¿å…æ¨ç†æ—¶åŠ¨æ€ Top-K å¼€é”€ã€‚

3. **é™æ€æ¿€æ´»åˆ†å‰²ç­–ç•¥**
   - åŠ¨æ€é€‰æ‹©é«˜ç²¾åº¦æ¿€æ´»ä¼šå¼•å…¥é¢å¤–å¼€é”€ï¼ˆTopKï¼‰ï¼Œä¸ºæ­¤æå‡ºåŸºäº `AÂ·W` è¾“å‡ºé€šé“å¹³å‡å¹…åº¦çš„é‡è¦æ€§è¯„åˆ† `I_j = |A Â· W|` æ¥é¢„å…ˆç¡®å®šé«˜ç²¾åº¦é€šé“ã€‚
   - æ©ç ä»¥**æ¯é€šé“**å½¢å¼å­˜å‚¨ï¼Œå¼€é”€æå°ï¼ˆä¾‹å¦‚ Llama-3-70B ä»…éœ€ 5.94MBï¼‰ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ W4A8, 2:4 sparseï¼‰ | SQ-format |
|------|-------------------------------|-----------|
| **ç²¾åº¦ä¿æŒ** | W4A4 ç²¾åº¦ä¸‹é™æ˜æ˜¾ï¼›2:4 sparse ä¸çµæ´» | æ¥è¿‘ W4A8 ç²¾åº¦ï¼Œä¼˜äº W4A4 |
| **ååæ•ˆç‡** | W4A8 å®é™…é€€åŒ–ä¸º W8A8ï¼Œæ— åŠ é€Ÿ | å¯æ¥è¿‘ W4A4 ååï¼Œå®ç°çœŸå®åŠ é€Ÿ |
| **ç¡¬ä»¶å…¼å®¹æ€§** | ç¼ºä¹æ··åˆç²¾åº¦åŸç”Ÿæ”¯æŒ | å…¼å®¹ç°æœ‰ GPUï¼Œå¹¶æŒ‡å¯¼ä¸‹ä¸€ä»£åŠ é€Ÿå™¨è®¾è®¡ |
| **çµæ´»æ€§** | å›ºå®šç²¾åº¦æˆ–å›ºå®šç¨€ç–æ¨¡å¼ | å¯è°ƒ bank sizeã€sparsityã€bit-width |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **éç”Ÿæˆä»»åŠ¡ï¼ˆNon-generativeï¼‰**ï¼š
  - ARC-easy/challenge, OpenBookQA, Hellaswag, Winogrande, PIQA
- **ç”Ÿæˆä»»åŠ¡ï¼ˆGenerativeï¼‰**ï¼š
  - GSM8kï¼ˆ8-shotï¼‰ã€AGIEval
- **å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰è¯„ä¼°**ï¼š
  - Wikitext, Lambada
- **æ ¡å‡†é›†ï¼ˆCalibration Setï¼‰**ï¼š
  - ä½¿ç”¨ Wikitext ä¸­ 32 ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œæ¯ä¸ª 2048 token

---

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹**ï¼š
  - Llama-3-8B, Llama-3-70B, Qwen3-30B-A3B
- **é‡åŒ–é…ç½®å‘½åè§„åˆ™**ï¼š
  - `W(SQx)Ay`ï¼šæƒé‡ä½¿ç”¨ SQ-formatï¼Œç­‰æ•ˆ x bitï¼Œæ¿€æ´»ä¸º INTy
  - `WxA(SQy)`ï¼šæ¿€æ´»ä½¿ç”¨ SQ-formatï¼Œç­‰æ•ˆ y bit
  - ç¤ºä¾‹ï¼š`B-(8/4)=0.5` è¡¨ç¤º bank size=64, h_high=INT8, h_low=INT4, sparsity=0.5ï¼ˆå³ 2x ç¨€ç–ï¼‰
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - å¹³å‡å‡†ç¡®ç‡ï¼ˆNon-/Generative Accï¼‰
  - Perplexityï¼ˆâ†“è¶Šä½è¶Šå¥½ï¼‰
  - æ¨ç†å»¶è¿Ÿï¼ˆPrefilling Latencyï¼‰
  - æœ‰æ•ˆå¸¦å®½ï¼ˆEffective Bandwidthï¼‰
  - Speedupï¼ˆç›¸å¯¹ W4A8ï¼‰

---

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| ç±»å‹ | æ–¹æ³• |
|------|------|
| **PTQ é‡åŒ–** | GPTQ, SmoothQuant, SpinQuant |
| **ç¨€ç–åŒ–** | SpQR, SparseGPT |
| **åŸºå‡†ç²¾åº¦** | BF16ï¼ˆW16A16ï¼‰ä½œä¸ºä¸Šé™å‚è€ƒ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ª Table 1 & Dï¼‰

#### âœ… åœ¨ Llama-3-70B ä¸Šçš„è¡¨ç°ï¼ˆW4A(SQ6)ï¼Œsparsity=0.5ï¼‰ï¼š
| æŒ‡æ ‡ | SQ-format | W4A8 (GPTQ) | BF16 |
|------|-----------|------------|-------|
| Non-gen Acc (%) | 62.80 â†’ **æ¥è¿‘ GPTQ** | 62.54 | 64.51 |
| Generative Acc (%) | 78.32 | 78.31 | 81.05 |
| Perplexity (Wiki) | 3.31 | 3.48 | 2.92 |
| **Speedup** | **1.32Ã—** | 1.00Ã— | â€” |

#### âœ… æ›´é«˜ç¨€ç–åº¦ä¸‹è¿›ä¸€æ­¥åŠ é€Ÿï¼ˆLlama-3-70Bï¼‰ï¼š
| Sparsity | Speedup vs W4A8 | Perplexity |
|---------|------------------|------------|
| 0.5 (2x) | 1.32Ã— | 3.31 |
| 0.75 (4x) | 1.56Ã— | 3.45 |
| 0.875 (8x) | **1.71Ã—** | 3.44 |
| W4A4ï¼ˆç†è®ºæé™ï¼‰ | 1.92Ã— | 3.73 |

> **ç»“è®º**ï¼šSQ-format åœ¨ä¿æŒç²¾åº¦æŸå¤± <1% çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾ **1.71Ã— çš„é€Ÿåº¦æå‡**ï¼Œè¾¾åˆ° W4A4 ç†è®ºæ€§èƒ½çš„ ~89%ã€‚

---

#### âœ… ä¸ W4A4 å¯¹æ¯”ï¼ˆç²¾åº¦æå‡æ˜¾è‘—ï¼‰ï¼š
- åœ¨ Llama-3-8B ä¸Šï¼š
  - W4A4ï¼šå¹³å‡å‡†ç¡®ç‡ ~47%
  - W(SQ6)A4ï¼šå¹³å‡å‡†ç¡®ç‡ ~49.4%ï¼Œ**æå‡çº¦ 2.4%**
- åœ¨ Llama-3-70B ä¸Šï¼š
  - W4A4ï¼šGSM8k å‡†ç¡®ç‡ 74.82%
  - W(SQ6)A4ï¼š**78.62%**ï¼Œæå‡è¿‘ 4%

> è¡¨æ˜ï¼šå¼•å…¥å°‘é‡é«˜ç²¾åº¦å…ƒç´ å³å¯æ˜¾è‘—æ¢å¤ W4A4 çš„ç²¾åº¦æŸå¤±ã€‚

---

#### âœ… é™æ€ vs åŠ¨æ€æ¿€æ´»ç­–ç•¥ï¼ˆTable 2ï¼‰
| è®¾ç½® | å¹³å‡å‡†ç¡®ç‡å·®å¼‚ | ç»“è®º |
|------|----------------|------|
| Dynamic SQ on A | åŸºå‡† | å¼•å…¥ TopK å¼€é”€ |
| Static SQ on A | å·®å¼‚ < Â±1% | **æ€§èƒ½å‡ ä¹æŒå¹³ï¼Œä½†æ›´é€‚åˆç¡¬ä»¶éƒ¨ç½²** |

> é™æ€ç­–ç•¥æ— éœ€è¿è¡Œæ—¶è®¡ç®—æ©ç ï¼Œæ›´é€‚åˆ GPU å’Œè¾¹ç¼˜è®¾å¤‡ã€‚

---

#### âœ… æ¶ˆèå®éªŒï¼ˆAppendix A.1ï¼‰
- **bank size å½±å“**ï¼š
  - è¾ƒå¤§æ¨¡å‹å»ºè®®ä½¿ç”¨æ›´å¤§ bank sizeï¼ˆå¦‚ 64ï¼‰ï¼Œä»¥æ›´å¥½æ•æ‰ outlier åˆ†å¸ƒã€‚
  - bank size=32 æ¯” 4 æ›´çµæ´»ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°åˆ†ç¦»é‡è¦æ¿€æ´»ï¼ˆè§ Figure 6ï¼‰ã€‚
- **æ ¡å‡†é›†å¤§å°å½±å“**ï¼ˆTable 7ï¼‰ï¼š
  - å³ä½¿ä½¿ç”¨ä»… 8â€“32 ä¸ªæ ·æœ¬ï¼Œé™æ€æ©ç ä»å…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œè¯´æ˜ activation outlier åˆ†å¸ƒç¨³å®šã€‚

---

#### âœ… æµ®ç‚¹éªŒè¯ï¼ˆAppendix Bï¼‰
- åœ¨ DeepSeek-R1ï¼ˆ685Bï¼‰ä¸Šåº”ç”¨ FP8/FP4 SQ-formatï¼ˆsparsity=0.875ï¼‰ï¼š
  - å®ç° **5-bit ç­‰æ•ˆç²¾åº¦**
  - æ€§èƒ½å‡ ä¹æ— æŸï¼ˆGSM8k: 96.21 vs 95.83ï¼‰
  - éªŒè¯äº† SQ-format åœ¨è¶…å¤§è§„æ¨¡æ¨¡å‹ä¸Šçš„å¯æ‰©å±•æ€§ã€‚

---

#### âœ… ç¡¬ä»¶å¯è¡Œæ€§éªŒè¯ï¼ˆAppendix Cï¼‰
- RTL ç»¼åˆæ˜¾ç¤ºï¼Œç›¸æ¯”æ ‡å‡† INT6 MAC é˜µåˆ—ï¼š
  - **æ€»é¢ç§¯å‡å°‘ 35.8%**
  - æ›´é«˜çš„è®¡ç®—å¯†åº¦ï¼Œé€‚åˆä¸‹ä¸€ä»£ AI åŠ é€Ÿå™¨é›†æˆ

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **SQ-format æˆåŠŸå¼¥åˆäº†â€œé«˜ç²¾åº¦â€ä¸â€œé«˜ååâ€çš„é¸¿æ²Ÿ**ï¼š
   - å®ç°äº†ä» W4A8 åˆ° W4A4 çš„å¹³æ»‘è¿‡æ¸¡ï¼Œåœ¨ç²¾åº¦å’Œé€Ÿåº¦ä¹‹é—´å–å¾—å¸•ç´¯æ‰˜æœ€ä¼˜ã€‚
2. **é™æ€æ¿€æ´»åˆ†å‰²ç­–ç•¥å®ç”¨æ€§å¼º**ï¼š
   - ä»…éœ€æå°é¢å¤–å­˜å‚¨ï¼ˆæ¯é€šé“1ä½æ©ç ï¼‰ï¼Œå³å¯æ¶ˆé™¤åŠ¨æ€ TopK å¼€é”€ï¼Œæ€§èƒ½æŸå¤±å¯å¿½ç•¥ã€‚
3. **bank size å’Œ sparsity éœ€ååŒè®¾è®¡**ï¼š
   - å¤§æ¨¡å‹æ¨è bank size â‰¥ 64ï¼Œsparsity â‰¥ 0.75ï¼ˆ4x ç¨€ç–ä»¥ä¸Šï¼‰ä»¥å¹³è¡¡è®¡ç®—è´Ÿè½½ã€‚
4. **ç¡¬ä»¶ååŒè®¾è®¡è‡³å…³é‡è¦**ï¼š
   - å½“å‰ GPU å¯é€šè¿‡ CUDA kernel æ¨¡æ‹Ÿï¼Œä½†ä¸“ç”¨ç¡¬ä»¶ï¼ˆå¦‚ gather unitï¼‰å°†è¿›ä¸€æ­¥é‡Šæ”¾æ½œåŠ›ã€‚

---

### âš ï¸ å±€é™æ€§
1. **ä¾èµ–æ ¡å‡†é›†è¿›è¡Œé™æ€æ©ç ç”Ÿæˆ**ï¼š
   - è™½ç„¶å®éªŒè¯æ˜å°æ ·æœ¬è¶³å¤Ÿï¼Œä½†åœ¨æç«¯åˆ†å¸ƒåç§»åœºæ™¯ä¸‹å¯èƒ½å¤±æ•ˆã€‚
2. **éœ€è¦ç¡¬ä»¶æ”¯æŒæ‰èƒ½å®Œå…¨å‘æŒ¥ä¼˜åŠ¿**ï¼š
   - å½“å‰å®ç°ä¾èµ–æ¨¡æ‹Ÿæˆ–å®šåˆ¶ kernelï¼Œå°šæœªæˆä¸ºä¸»æµç¡¬ä»¶åŸç”ŸåŠŸèƒ½ã€‚
3. **ä»…ä½œç”¨äºå•ä¸ªæ“ä½œæ•°**ï¼š
   - è®ºæ–‡é™åˆ¶ SQ-format åªåº”ç”¨äº weight æˆ– activation ä¹‹ä¸€ï¼ŒåŒç«¯è”åˆå‹ç¼©å°šå¾…æ¢ç´¢ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³ KV Cache å‹ç¼©**ï¼š
   - å°† SQ-format åº”ç”¨äº attention ä¸­çš„ key/valueï¼Œè¿›ä¸€æ­¥é™ä½å†…å­˜å ç”¨ã€‚
2. **æ”¯æŒæ›´å¤šæ•°æ®ç±»å‹**ï¼š
   - å¦‚ MX Formatã€HiFloat8 ç­‰è‡ªé€‚åº”èŒƒå›´æ ¼å¼ç»“åˆ SQã€‚
3. **ç¼–è¯‘å™¨ä¸ç³»ç»Ÿçº§ä¼˜åŒ–**ï¼š
   - æ„å»ºæ”¯æŒ SQ-format çš„æ¨ç†å¼•æ“ï¼ˆç±»ä¼¼ TensorRTï¼‰ï¼Œå®ç°ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–éƒ¨ç½²ã€‚
4. **æ¢ç´¢åŠ¨æ€-é™æ€æ··åˆç­–ç•¥**ï¼š
   - åœ¨å…³é”®å±‚ä½¿ç”¨åŠ¨æ€é€‰æ‹©ï¼Œå…¶ä½™å±‚ä½¿ç”¨é™æ€ç­–ç•¥ï¼Œå…¼é¡¾çµæ´»æ€§ä¸æ•ˆç‡ã€‚

---

## âœ… æ€»ç»“
**SQ-format** æ˜¯ä¸€é¡¹é¢å‘æœªæ¥çš„**ç¡¬ä»¶-ç®—æ³•ååŒè®¾è®¡**æˆæœï¼Œå®ƒä¸ä»…æä¾›äº†ä¸€ç§é«˜æ•ˆçš„ LLM å‹ç¼©æ–¹æ¡ˆï¼Œæ›´é‡è¦çš„æ˜¯æå‡ºäº†ä¸€ä¸ª**å¯æ‰©å±•ã€å¯ç¼–ç¨‹çš„æ··åˆç²¾åº¦è®¡ç®—èŒƒå¼**ã€‚è¯¥æ–¹æ³•åœ¨ç²¾åº¦æŸå¤±æå°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†ååï¼Œä¸ºä¸‹ä¸€ä»£ AI åŠ é€Ÿå™¨çš„è®¾è®¡æä¾›äº†æ¸…æ™°è“å›¾ã€‚

</details>

---

### 2. [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)

**Authors**: Na Li, Hangguan Shan, Wei Ni, Wenjie Zhang, Xinyu Li  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.05291v1  

#### Abstract
Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual st...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šBridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿçš„ **Actor-Critic (AC)** æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è™½ç„¶æœ‰æ•ˆï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§ï¼ˆinterpretabilityï¼‰ã€‚ç°æœ‰å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ ï¼ˆXRLï¼‰æ–¹æ³•å¤§å¤šä¸ºäº‹åè§£é‡Šï¼ˆpost-hocï¼‰ï¼Œå¹¶æœªå°†çŠ¶æ€ç‰¹å¾çš„é‡è¦æ€§ï¼ˆstate attributionsï¼‰èå…¥è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæ ‡å‡†ACæ–¹æ³•é€šå¸¸å¹³ç­‰å¯¹å¾…æ‰€æœ‰çŠ¶æ€ç‰¹å¾ï¼Œå¿½ç•¥äº†ä¸åŒç»´åº¦å¯¹å¥–åŠ±å½±å“çš„å¼‚è´¨æ€§ã€‚

### æå‡ºçš„æ–°æ–¹æ³•
æœ¬æ–‡æå‡ºäº† **RKHS-SHAP-based Advanced Actor-Critic (RSA2C)**ï¼Œä¸€ç§å°†å¯è§£é‡Šæ€§ä¸ä¼˜åŒ–ç›¸ç»“åˆçš„æ–°å‹ACç®—æ³•ã€‚å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹å¦‚ä¸‹ï¼š

- **Attribution-Aware Training**ï¼šé¦–æ¬¡å°†çŠ¶æ€ç‰¹å¾é‡è¦æ€§ï¼ˆé€šè¿‡SHAPè®¡ç®—ï¼‰ç›´æ¥ç”¨äºè°ƒèŠ‚ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æœ¬èº«å…·å¤‡å†…åœ¨å¯è§£é‡Šæ€§ï¼ˆintrinsic interpretabilityï¼‰ã€‚
- **åŸºäºRKHSçš„æ ¸åŒ–æ¡†æ¶**ï¼šå°†Actorã€Value Criticå’ŒAdvantage Criticå‡ç½®äº**Reproducing Kernel Hilbert Space (RKHS)** ä¸­ï¼Œåˆ©ç”¨éå‚æ•°å‡½æ•°é€¼è¿‘ï¼Œæ˜¾å¼æ§åˆ¶å‡ ä½•ç»“æ„å’Œæ¨¡å‹å®¹é‡ã€‚
- **MahalanobisåŠ æƒæ“ä½œç¬¦å€¼æ ¸ï¼ˆOVKï¼‰**ï¼šæå‡ºä¸€ç§è‡ªé€‚åº”çš„æ ¸å‡½æ•°ï¼Œå…¶ä¸­æƒé‡ç”±SHAPå€¼ç”Ÿæˆçš„Mahalanobisè·ç¦»å†³å®šï¼Œä»è€ŒåŠ¨æ€è°ƒæ•´ä¸åŒçŠ¶æ€ç»´åº¦çš„ç›¸ä¼¼æ€§åº¦é‡ã€‚
- **åŒæ—¶é—´å°ºåº¦æœºåˆ¶ä¸ç¨€ç–å­—å…¸**ï¼šé‡‡ç”¨åŒæ—¶é—´å°ºåº¦æ›´æ–°ï¼Œå¹¶é€šè¿‡**Approximate Linear Dependence (ALD)** ç»´æŠ¤ç¨€ç–å­—å…¸ï¼Œæ§åˆ¶è®¡ç®—å¤æ‚åº¦ï¼Œä½¿å…¶é€‚ç”¨äºåœ¨çº¿RLã€‚
- **ç†è®ºæ”¶æ•›ä¿è¯**ï¼šåœ¨å­˜åœ¨çŠ¶æ€æ‰°åŠ¨çš„æƒ…å†µä¸‹ï¼Œæ¨å¯¼äº†å…¨å±€ã€éæ¸è¿‘çš„æ”¶æ•›ç•Œï¼Œå°†å­¦ä¹ å·®è·åˆ†è§£ä¸º**æ‰°åŠ¨è¯¯å·®**ï¼ˆperturbation errorï¼‰å’Œ**æ”¶æ•›è¯¯å·®**ï¼ˆconvergence errorï¼‰ï¼Œåˆ†åˆ«é‡åŒ–äº†ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **å…¼å…·æ•ˆç‡ä¸å¯è§£é‡Šæ€§**ï¼šä¸ä»…æå‡äº†æ€§èƒ½ï¼Œè¿˜æä¾›äº†è®­ç»ƒè¿‡ç¨‹ä¸­ç‰¹å¾é‡è¦æ€§çš„å®æ—¶æ´å¯Ÿã€‚
- **ç¨³å®šæ€§å¼º**ï¼šé€šè¿‡RKHS-SHAPå’Œæ ¸åŒ–è®¾è®¡ï¼Œåœ¨å™ªå£°ç¯å¢ƒä¸‹è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚
- **ç†è®ºä¿éšœ**ï¼šæä¾›äº†åœ¨çŠ¶æ€æ‰°åŠ¨ä¸‹çš„éæ¸è¿‘æ”¶æ•›è¯æ˜ï¼Œå¡«è¡¥äº†ç›¸å…³ç†è®ºç©ºç™½ã€‚
- **è½»é‡çº§**ï¼šç›¸æ¯”æ·±åº¦RLæ–¹æ³•ï¼Œè®¡ç®—å¼€é”€æ˜¾è‘—æ›´ä½ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†ï¼ˆç¯å¢ƒï¼‰
å®éªŒåœ¨ä¸‰ä¸ªæ ‡å‡†çš„è¿ç»­æ§åˆ¶ç¯å¢ƒä¸Šè¿›è¡Œï¼š
- **Pendulum-v1**ï¼š1ç»´åŠ¨ä½œç©ºé—´ï¼Œè§‚æµ‹ä¸º `(cosÎ¸, sinÎ¸, Î¸_dot)`ã€‚
- **BipedalWalker-v3**ï¼š4ç»´åŠ¨ä½œç©ºé—´ï¼Œ24ç»´æœ¬ä½“æ„Ÿå— + 10ç»´æ¿€å…‰é›·è¾¾ã€‚
- **Ant-v5**ï¼š8ç»´åŠ¨ä½œç©ºé—´ï¼Œ105ç»´é«˜ç»´çŠ¶æ€ï¼ˆåŒ…å«å…³èŠ‚è§’åº¦ã€é€Ÿåº¦ã€æ¥è§¦ç­‰ï¼‰ã€‚

### å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡
- **è¯„ä¼°ç»´åº¦**ï¼š
  - **æ•ˆç‡ï¼ˆEfficiencyï¼‰**ï¼šæœ€ç»ˆå¹³å‡å›æŠ¥ï¼ˆaverage returnï¼‰ã€æ”¶æ•›é€Ÿåº¦ã€‚
  - **ç¨³å®šæ€§ï¼ˆStabilityï¼‰**ï¼šåœ¨æ³¨å…¥é›¶å‡å€¼é«˜æ–¯å™ªå£°ï¼ˆä¸åŒæ–¹å·®ï¼‰ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼ˆå‡å€¼ä¸æ ‡å‡†å·®ï¼‰ã€‚
  - **å¯è§£é‡Šæ€§ï¼ˆInterpretabilityï¼‰**ï¼šé€šè¿‡beeswarmå›¾å’Œheatmapå¯è§†åŒ–SHAPå€¼ï¼Œåˆ†æç‰¹å¾é‡è¦æ€§éšè®­ç»ƒçš„æ¼”åŒ–ã€‚
  - **è®¡ç®—å¼€é”€**ï¼šFLOPså’Œæ¯è½®æ›´æ–°çš„è¿è¡Œæ—¶é—´ï¼ˆruntimeï¼‰ã€‚
- **æ¶ˆèç ”ç©¶**ï¼šæ¯”è¾ƒäº†ä¸åŒå˜ä½“ï¼ŒåŒ…æ‹¬ï¼š
  - `RSA2C-KME`ï¼šä½¿ç”¨RKHS-SHAPçš„**on-manifold**æœŸæœ›ï¼ˆåŸºäºKMEï¼‰ã€‚
  - `RSA2C-CME`ï¼šä½¿ç”¨RKHS-SHAPçš„**off-manifold**æœŸæœ›ï¼ˆåŸºäºCMEï¼‰ã€‚
  - `Advanced AC`ï¼šæ— SHAPçš„åŸºçº¿ç‰ˆæœ¬ã€‚
  - `RKHS-AC`ï¼šä¼ ç»Ÿæ ¸ACæ–¹æ³•ï¼ˆLever & Stafford, 2015ï¼‰ã€‚
- **åŸºçº¿æ–¹æ³•**ï¼šä¸ä¸»æµæ·±åº¦RLç®—æ³•å¯¹æ¯”ï¼š
  - **SAC (Soft Actor-Critic)**
  - **PPO (Proximal Policy Optimization)**

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ

#### Pendulum-v1 ç¯å¢ƒ
- **æœ€ç»ˆå›æŠ¥æå‡**ï¼š
  - `RSA2C-CME` å’Œ `RSA2C-KME` ç›¸æ¯” `Advanced AC` åˆ†åˆ«æå‡äº†çº¦ **47.6%** å’Œ **49.2%**ã€‚
- **ä¸æ·±åº¦RLå¯¹æ¯”**ï¼š
  - å°½ç®¡ä¸ä½¿ç”¨ç¥ç»ç½‘ç»œï¼Œ`RSA2C` å˜ä½“åœ¨æ€§èƒ½ä¸Šä¸SACå’ŒPPOå…·æœ‰ç«äº‰åŠ›ã€‚
  - SACåˆæœŸå›æŠ¥é«˜ä½†å¾ˆå¿«é¥±å’Œï¼ŒPPOæ”¹è¿›æ…¢ä¸”æ³¢åŠ¨å¤§ã€‚
- **è®¡ç®—å¼€é”€**ï¼š
  - è™½ç„¶FLOPså¢åŠ ï¼ˆ`RSA2C-CME`: 14.960 MFLOPsï¼‰ï¼Œä½†å®é™…è¿è¡Œæ—¶é—´ä»…æ¯”`Advanced AC`å¢åŠ çº¦ **12.58%**ï¼Œè¿œä½äºSACå’ŒPPOï¼ˆå³ä½¿åœ¨GPUä¸Šï¼‰ã€‚

#### BipedalWalker-v3 ç¯å¢ƒ
- **ç¨³å®šæ€§**ï¼š
  - åœ¨å™ªå£°ç¯å¢ƒä¸‹ï¼Œ`RSA2C-CME` çš„æ ‡å‡†å·®ï¼ˆ24.75â€“43.06ï¼‰æ˜¾è‘—ä½äº `RSA2C-KME`ï¼ˆ46.74â€“56.03ï¼‰ï¼Œè¡¨æ˜CMEæ›´ç¨³å®šã€‚
  - `RSA2C-CME` åœ¨é«˜å™ªå£°ä¸‹ï¼ˆ0.01æ–¹å·®ï¼‰çš„å¹³å‡å›æŠ¥ï¼ˆ254.85ï¼‰ä¼˜äº `RSA2C-KME`ï¼ˆ242.05ï¼‰ã€‚
- **è®¡ç®—å¼€é”€**ï¼š
  - è¿è¡Œæ—¶é—´å¢é•¿çº¦ **6.9%**ï¼Œä»è¿œä½äºSACå’ŒPPOã€‚

#### Ant-v5 ç¯å¢ƒï¼ˆé«˜ç»´æŒ‘æˆ˜ï¼‰
- **æ€§èƒ½ç“¶é¢ˆ**ï¼š
  - æ‰€æœ‰RKHSæ–¹æ³•ï¼ˆåŒ…æ‹¬`RSA2C`ï¼‰éƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œæœ€ç»ˆå›æŠ¥çº¦1000ã€‚
  - SACæŒç»­æ”¹è¿›å¹¶è¶…è¿‡4000ï¼Œæ˜¾ç¤ºå‡ºæ·±åº¦RLåœ¨é«˜ç»´éçº¿æ€§ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚
- **ç¨³å®šæ€§**ï¼š
  - `RSA2C` å˜ä½“åœ¨æ‰€æœ‰å™ªå£°æ°´å¹³ä¸‹ä¿æŒæé«˜çš„ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·® < 45ï¼‰ã€‚
  - SACå’ŒPPOåœ¨å™ªå£°ä¸‹è¡¨ç°æä¸ç¨³å®šï¼ŒPPOç”šè‡³å´©æºƒè‡³è´Ÿå›æŠ¥ã€‚
- **è®¡ç®—å¼€é”€**ï¼š
  - `RSA2C-CME` éœ€è¦3.853 GFLOPsï¼Œä»æ˜¾è‘—ä½äºSACï¼ˆ29.28 GFLOPsï¼‰ã€‚

### æ¶ˆèå®éªŒç»“æœ
- **SHAPçš„æœ‰æ•ˆæ€§**ï¼šå¼•å…¥SHAPçš„`RSA2C`æ˜¾è‘—ä¼˜äºæ— SHAPçš„`Advanced AC`ï¼ŒéªŒè¯äº†ç‰¹å¾é‡è¦æ€§å¼•å¯¼è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚
- **CME vs KME**ï¼š
  - `RSA2C-CME` æ”¶æ•›æ›´å¹³æ»‘ã€æ›´ç¨³å®šï¼Œå°¤å…¶åœ¨å™ªå£°ç¯å¢ƒä¸‹ã€‚
  - `RSA2C-KME` åˆæœŸæ”¶æ•›å¿«ä½†æ³¢åŠ¨å¤§ã€‚
  - åŸå› ï¼šCMEæ˜¾å¼å»ºæ¨¡ç‰¹å¾ä¾èµ–å…³ç³»ï¼Œèƒ½è‡ªé€‚åº”é‡åˆ†é…é‡è¦æ€§ï¼›KMEå¿½ç•¥ç›¸å…³æ€§ï¼Œæ˜“å—ä¸»å¯¼ç‰¹å¾å™ªå£°å½±å“ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **å¯è§£é‡Šæ€§å¯é©±åŠ¨ä¼˜åŒ–**ï¼šå°†SHAPç­‰å½’å› æ–¹æ³•ä»â€œäº‹åè§£é‡Šâ€è½¬å˜ä¸ºâ€œè®­ç»ƒä¿¡å·â€ï¼Œèƒ½æœ‰æ•ˆæå‡ACç®—æ³•çš„æ•ˆç‡å’Œç¨³å®šæ€§ã€‚
2. **CMEä¼˜äºKME**ï¼šåœ¨off-manifoldè®¾ç½®ä¸‹è®¡ç®—çš„SHAPï¼ˆCMEï¼‰ç”±äºè€ƒè™‘äº†ç‰¹å¾é—´çš„æ¡ä»¶ä¾èµ–ï¼Œæ¯”on-manifoldçš„KMEæ›´å…·é²æ£’æ€§ã€‚
3. **è½»é‡é«˜æ•ˆ**ï¼š`RSA2C` åœ¨ä¸ä¾èµ–æ·±åº¦ç¥ç»ç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸æ·±åº¦RLç›¸å½“çš„æ€§èƒ½ï¼Œä¸”è®¡ç®—å¼€é”€ä½ä¸€ä¸ªæ•°é‡çº§ã€‚
4. **å¼ºé²æ£’æ€§**ï¼š`RSA2C` å¯¹çŠ¶æ€è§‚æµ‹å™ªå£°è¡¨ç°å‡ºæå¼ºçš„æŠµæŠ—åŠ›ï¼Œè€Œæ·±åº¦RLæ–¹æ³•ï¼ˆå¦‚SACã€PPOï¼‰åˆ™éå¸¸æ•æ„Ÿã€‚
5. **ç†è®ºä¸å®è·µä¸€è‡´**ï¼šåœ¨LQRç¯å¢ƒä¸­çš„å®éªŒéªŒè¯äº†ç†è®ºæ¨å¯¼çš„éæ¸è¿‘æ”¶æ•›ç‡ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- **é«˜ç»´è¡¨å¾èƒ½åŠ›æœ‰é™**ï¼šåœ¨Ant-v5ç­‰é«˜ç»´å¤æ‚ç¯å¢ƒä¸­ï¼Œçº¿æ€§RKHSå‡½æ•°é€¼è¿‘ç»“åˆRBFæ ¸éš¾ä»¥æ•æ‰è¶³å¤Ÿå¤æ‚çš„ç»“æ„ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚
- **è®¡ç®—å¤æ‚åº¦**ï¼šå°½ç®¡è¿›è¡Œäº†ç¨€ç–åŒ–ï¼Œä½†åœ¨é«˜ç»´çŠ¶æ€ä¸‹ç»´æŠ¤RKHSå­—å…¸å’Œè®¡ç®—SHAPä»æœ‰ä¸€å®šå¼€é”€ã€‚
- **é€‚ç”¨èŒƒå›´**ï¼šç›®å‰ä¸»è¦é’ˆå¯¹è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼Œå¯¹ç¦»æ•£æˆ–éƒ¨åˆ†å¯è§‚æµ‹ä»»åŠ¡çš„æ‰©å±•éœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•åˆ°é«˜ç»´è§‚æµ‹**ï¼šç»“åˆ**Random Fourier Features (RFFs)** æˆ–å…¶ä»–å¯æ‰©å±•æ ¸è¿‘ä¼¼æŠ€æœ¯ï¼Œå¤„ç†å›¾åƒç­‰é«˜ç»´è¾“å…¥ã€‚
2. **æ··åˆæ¶æ„**ï¼šå°†æ ¸åŒ–å½’å› æ–¹æ³•ä¸æ·±åº¦è¡¨ç¤ºå­¦ä¹ ç»“åˆï¼Œä»¥å…¼é¡¾è¡¨è¾¾èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚
3. **å¤šæ™ºèƒ½ä½“RL**ï¼šæ¢ç´¢è¯¥æ–¹æ³•åœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
4. **ç†è®ºæ·±åŒ–**ï¼šç ”ç©¶åœ¨æ›´ä¸€èˆ¬åŒ–MDPæˆ–å¯¹æŠ—æ€§ç¯å¢ƒä¸‹çš„æ”¶æ•›æ€§è´¨ã€‚

</details>

---

### 3. [Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)](https://arxiv.org/abs/2512.05306)

**Authors**: Y. Sungtaek Ju  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.05306v1  

#### Abstract
Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Ga...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿ **Kolmogorov-Arnold Network (KAN)** è™½ç„¶åœ¨å¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºæ ‡å‡† MLPï¼ˆé€šè¿‡è¾¹ä¸Šçš„å¯å­¦ä¹ å•å˜é‡å‡½æ•°å®ç°ï¼‰ï¼Œä½†å…¶**ç¡®å®šæ€§æ¶æ„ç¼ºä¹ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›**ï¼Œé™åˆ¶äº†å…¶åœ¨ç§‘å­¦æœºå™¨å­¦ä¹ ï¼ˆSciMLï¼‰ç­‰å¯¹ç½®ä¿¡åº¦è¦æ±‚é«˜çš„åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

ç°æœ‰è´å¶æ–¯æ–¹æ³•å¦‚ **Bayesian KAN** æˆ– **Gaussian Process KAN (GP-KAN)** å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- **è®¡ç®—å¤æ‚åº¦é«˜**ï¼šç²¾ç¡® GP æ¨æ–­éœ€ $O(N^3)$ æ—¶é—´ï¼Œéš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡æ•°æ®ï¼›
- **ä¸ç¡®å®šæ€§å»ºæ¨¡ä¸å®Œæ•´**ï¼šä»…å»ºæ¨¡æƒé‡ç©ºé—´ä¸ç¡®å®šæ€§ï¼Œè€Œéå‡½æ•°ç©ºé—´ä¸ç¡®å®šæ€§ï¼›
- **æ— æ³•åŒºåˆ† aleatoric ä¸ epistemic ä¸ç¡®å®šæ€§**ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šSVGP-KAN
ä½œè€…æå‡º **Sparse Variational Gaussian Process Kolmogorov-Arnold Network (SVGP-KAN)**ï¼Œå°†ç¨€ç–å˜åˆ†æ¨æ–­ï¼ˆSparse Variational Inferenceï¼‰å¼•å…¥ KAN æ¶æ„ä¸­ï¼Œæ„å»ºä¸€ä¸ªå…¼å…·**å¯è§£é‡Šæ€§ã€ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§**çš„æ–°å‹ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚

#### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š
1. **å‡½æ•°ç©ºé—´ä¸ç¡®å®šæ€§å»ºæ¨¡**  
   å°†æ¯æ¡è¾¹ä¸Šçš„å•å˜é‡å‡½æ•° $\phi_{j,i}$ å»ºæ¨¡ä¸º **Gaussian Process (GP)**ï¼Œä»è€Œå®ç°å¯¹å‡½æ•°æœ¬èº«çš„æ¦‚ç‡å»ºæ¨¡ï¼Œè€Œéå‚æ•°åˆ†å¸ƒã€‚

2. **ç¨€ç–å˜åˆ†è¿‘ä¼¼æå‡å¯æ‰©å±•æ€§**  
   å¼•å…¥ **Inducing Points** å’Œ **Variational Inference**ï¼Œå°†è®¡ç®—å¤æ‚åº¦ä» $O(N^3)$ é™ä½è‡³ $O(NM^2)$ï¼Œå…¶ä¸­ $M \ll N$ ä¸ºè¯±å¯¼ç‚¹æ•°é‡ï¼Œå®ç°å‡†çº¿æ€§æ‰©å±•ã€‚

3. **è§£æçŸ©åŒ¹é…ï¼ˆAnalytic Moment Matchingï¼‰è¿›è¡Œä¸ç¡®å®šæ€§ä¼ æ’­**  
   åˆ©ç”¨ GP è¾“å‡ºçš„é—­å¼å‡å€¼ä¸æ–¹å·®è¡¨è¾¾ï¼Œåœ¨æ·±åº¦åŠ æ€§ç»“æ„ä¸­é€å±‚ä¼ æ’­è¾“å…¥ä¸ç¡®å®šæ€§ï¼Œæ”¯æŒ **epistemic** ä¸ **aleatoric** ä¸ç¡®å®šæ€§çš„åˆ†ç¦»ã€‚

4. **å¼‚æ–¹å·®å™ªå£°å»ºæ¨¡ï¼ˆHeteroscedastic Noise Modelingï¼‰**  
   å¼•å…¥ç¬¬äºŒä¸ª GP æ¥å»ºæ¨¡è§‚æµ‹å™ªå£°çš„å¯¹æ•°æ–¹å·® $\log\sigma^2_{\text{noise}}(x)$ï¼Œä½¿æ¨¡å‹èƒ½è‡ªåŠ¨è¯†åˆ«ä¸åŒåŒºåŸŸçš„æµ‹é‡ç²¾åº¦å·®å¼‚ã€‚

5. **çµæ´»çš„ KL æ­£åˆ™åŒ–æ§åˆ¶æœºåˆ¶**  
   é€šè¿‡è°ƒèŠ‚ ELBO ä¸­çš„ KL æƒé‡ $\lambda$ï¼Œå¯åœ¨ä¸åŒä»»åŠ¡é—´å¹³è¡¡â€œæ‹Ÿåˆèƒ½åŠ›â€ä¸â€œä¸ç¡®å®šæ€§æ ¡å‡†â€ï¼Œä¾‹å¦‚ï¼š
   - $\lambda = 0.01$ï¼šç”¨äºæ ¡å‡† in-distribution é¢„æµ‹ï¼›
   - $\lambda = 0$ï¼šä¿ç•™ OOD è¾“å…¥æ—¶çš„è‡ªç„¶æ–¹å·®æ”¾å¤§æ•ˆåº”ï¼Œå¢å¼ºå¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿

| æ–¹æ³• | å¯è§£é‡Šæ€§ | ä¸ç¡®å®šæ€§é‡åŒ– | æ‰©å±•æ€§ | åŒºåˆ† aleatoric/epistemic |
|------|----------|----------------|--------|----------------------------|
| Deterministic KAN | âœ… å¼º | âŒ æ—  | âœ… å¥½ | âŒ å¦ |
| Bayesian KAN | â­• ä¸­ç­‰ | â­• å‚æ•°ç©ºé—´ | â­• ä¸€èˆ¬ | â­• æœ‰é™ |
| Exact GP-KAN | âœ… å‡½æ•°ç©ºé—´ | âœ… æ˜¯ | âŒ $O(N^3)$ | âœ… æ˜¯ |
| **SVGP-KAN (æœ¬æ–‡)** | âœ… è¾¹å‡½æ•°å¯è§†åŒ– | âœ… å‡½æ•°ç©ºé—´ + åˆ†ç¦» | âœ… $O(NM^2)$ | âœ… æ˜¾å¼åˆ†ç¦» |

> âœ… è¡¨ç¤ºä¼˜åŠ¿æ˜æ˜¾ï¼Œâ­• è¡¨ç¤ºéƒ¨åˆ†æ”¯æŒï¼ŒâŒ è¡¨ç¤ºä¸æ”¯æŒæˆ–è¾ƒå·®

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ§ª å®éªŒè®¾è®¡æ¦‚è¿°
è®ºæ–‡é€šè¿‡ä¸‰ä¸ªå…¸å‹ SciML åœºæ™¯éªŒè¯ SVGP-KAN çš„æœ‰æ•ˆæ€§ï¼š

| Study | ä»»åŠ¡ç±»å‹ | æ•°æ®æ¥æº | ä¸»è¦ç›®æ ‡ |
|-------|---------|----------|----------|
| **Study A** | æµä½“é€Ÿåº¦åœºé‡å»º | åˆæˆäºŒç»´ä¸å¯å‹ç¼©æµåœº | æ ¡å‡†ç©ºé—´å˜åŒ–çš„å¼‚æ–¹å·®æµ‹é‡å™ªå£°ï¼ˆAleatoric Uncertaintyï¼‰ |
| **Study B** | å¤šæ­¥é¢„æµ‹ | åˆæˆ advection-diffusion æ–¹ç¨‹è½¨è¿¹ | é‡åŒ–æ¨¡å‹è¯¯å·®éšæ—¶é—´ç´¯ç§¯å¯¼è‡´çš„ Epistemic Uncertainty å¢é•¿ |
| **Study C** | å¼‚å¸¸æ£€æµ‹ | MNISTï¼ˆè®­ç»ƒä»…â€œ0â€ï¼Œæµ‹è¯•å«â€œ7â€ï¼‰ | éªŒè¯ OOD æ£€æµ‹èƒ½åŠ›ï¼Œåˆ©ç”¨ç“¶é¢ˆå±‚ä¸ç¡®å®šæ€§ä½œä¸ºåˆ¤æ® |

---

### ğŸ“Š å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### Study A: Heteroscedastic Noise Calibration
- **æ•°æ®ç”Ÿæˆ**ï¼š
  - ä½¿ç”¨æµå‡½æ•° $\psi(x,y)$ æ„é€ çœŸå®é€Ÿåº¦åœº $(v_x, v_y)$ï¼›
  - æ·»åŠ ç©ºé—´ç›¸å…³å™ªå£°ï¼š$\sigma^2_{\text{true}}(x,y) = \sigma_{\text{base}}^2 \left(1 + A_{\text{noise}} \exp(-\frac{(x-x_c)^2 + (y-y_c)^2}{r^2})\right)$
- **æ¨¡å‹ç»“æ„**ï¼š3 å±‚å…¨è¿æ¥ GPKANLayerï¼Œæ¯è¾¹ä½¿ç”¨ 20 ä¸ª inducing points
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - Pearson ç›¸å…³ç³»æ•°ï¼ˆé¢„æµ‹ä¸ç¡®å®šæ€§ vs å®é™…ç»å¯¹è¯¯å·®ï¼‰
  - é¢„æµ‹åŒºé—´è¦†ç›–ç‡ï¼ˆÂ±1Ïƒ, Â±2Ïƒ, Â±3Ïƒï¼‰
  - RMSEã€æ ‡å‡†åŒ–é¢„æµ‹è¯¯å·®ï¼ˆmean/std of $(y-\mu)/\sigma$ï¼‰

#### Study B: Multi-step Forecasting
- **ç‰©ç†ç³»ç»Ÿ**ï¼š2D advection-diffusion PDEï¼š
  $$
  \frac{\partial T}{\partial t} = -\mathbf{u} \cdot \nabla T + \kappa \nabla^2 T + S(x,t)
  $$
- **Pe æ•° â‰ˆ 1571**ï¼ˆå¯¹æµä¸»å¯¼ï¼‰ï¼Œç¡®ä¿æŒ‘æˆ˜æ€§
- **è®­ç»ƒæ–¹å¼**ï¼šä»…å­¦ä¹ å•æ­¥è½¬ç§» $T_n \to T_{n+1}$
- **æµ‹è¯•æ–¹å¼**ï¼šEnsemble Rolloutï¼ˆ10 ä¸ªåˆå§‹æ‰°åŠ¨æ ·æœ¬å‰å‘ä¼ æ’­ 15 æ­¥ï¼‰
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - Ensemble spreadï¼ˆepistemic uncertaintyï¼‰
  - æ—¶é—´ç»´åº¦ä¸Šçš„ uncertainty growth factor
  - ç©ºé—´ uncertainty variation ratio

#### Study C: Out-of-Distribution Detection
- **æ•°æ®é›†**ï¼šMNISTï¼Œè®­ç»ƒé›†ä»…åŒ…å« digit "0"
- **æ¨¡å‹ç»“æ„**ï¼šConvolutional Autoencoder + SVGP-KAN Bottleneckï¼ˆlatent dim=6ï¼‰
- **è®­ç»ƒç­–ç•¥**ï¼š$\lambda = 0$ï¼ˆæ—  KL æ­£åˆ™åŒ–ï¼‰ï¼Œä»¥ä¿ç•™ OOD æ”¾å¤§æœºåˆ¶
- **è¯„ä¼°æŒ‡æ ‡**ï¼š
  - Anomaly Score Ratioï¼ˆOOD / ID çš„ bottleneck uncertainty æ¯”å€¼ï¼‰
  - ROC-AUC
  - Reconstruction MSE å¯¹æ¯”

---

### âš–ï¸ åŸºçº¿æ–¹æ³•å¯¹æ¯”
æ–‡ä¸­æœªç›´æ¥åˆ—å‡ºä¸å…¶ä»–æ¨¡å‹ï¼ˆå¦‚ BNNã€Deep Ensembleã€Standard GPï¼‰çš„å®šé‡æ¯”è¾ƒï¼Œè€Œæ˜¯å¼ºè°ƒï¼š
- ç›¸è¾ƒäº exact GP-KANï¼š**è®¡ç®—æ•ˆç‡æ˜¾è‘—æå‡**ï¼ˆ$O(NM^2)$ vs $O(N^3)$ï¼‰
- ç›¸è¾ƒäº deterministic KANï¼š**æä¾› principled uncertainty quantification**
- ç›¸è¾ƒäºæ™®é€š BNNï¼š**ä¿æŒè¾¹å‡½æ•°å¯è§£é‡Šæ€§**

> æ³¨ï¼šä½œè€…å¼€æºä»£ç ï¼ˆGitHub: `sungjuGit/svgp-kan`ï¼‰ï¼Œä¾¿äºåç»­å¤ç°ä¸æ¨ªå‘å¯¹æ¯”ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### âœ… Study A: Heteroscedastic Noise Calibration
- **Pearson ç›¸å…³æ€§**ï¼š$\rho = 0.55$ï¼ˆé¢„æµ‹ä¸ç¡®å®šæ€§ä¸å®é™…è¯¯å·®ä¹‹é—´ï¼‰
  - è§£é‡Šï¼šé€‚åº¦ç›¸å…³åˆç†ï¼Œå›  aleatoric noise æœ¬è´¨éšæœºï¼Œä¸åº”å®Œå…¨ç¡®å®š
- **è¦†ç›–ç‡è¯¯å·® < 1%**ï¼š
  - Â±1Ïƒ è¦†ç›–ç‡æ¥è¿‘ç†è®ºå€¼ï¼ˆ68.3%ï¼‰
  - Â±2Ïƒ å’Œ Â±3Ïƒ ä¹Ÿé«˜åº¦ä¸€è‡´ï¼Œè¡¨æ˜ä¸ç¡®å®šæ€§**è‰¯å¥½æ ¡å‡†**
- **æ ‡å‡†åŒ–é¢„æµ‹è¯¯å·®**ï¼šmean = -0.09ï¼Œstd = 1.01 â†’ æ¥è¿‘ç†æƒ³ $N(0,1)$
- **RMSE = 0.085** â†’ å‡å€¼é¢„æµ‹å‡†ç¡®

> âœ”ï¸ æˆåŠŸè¯†åˆ«é«˜å™ªå£°åŒºåŸŸå¹¶åŠ¨æ€è°ƒæ•´ç½®ä¿¡åŒºé—´å®½åº¦

---

### âœ… Study B: Epistemic Uncertainty Growth
- **Ensemble Spread å¢é•¿å› å­**ï¼š**2.4 å€**ï¼ˆä» t=0 åˆ° t=14ï¼‰
  - ç‰©ç†ç³»ç»Ÿç»éªŒè¯æ˜¯ç¨³å®šçš„ï¼ˆéæ··æ²Œï¼‰ï¼Œå› æ­¤å¢é•¿æºäº**æ¨¡å‹è¿‘ä¼¼è¯¯å·®ç§¯ç´¯**
- **Spatial Variation Ratio**ï¼šé«˜è¾¾ **12.7 å€**
  - ä¸ç¡®å®šæ€§é›†ä¸­åœ¨æ¢¯åº¦å‰§çƒˆåŒºåŸŸï¼ˆå¦‚çƒ­å†·ç•Œé¢ï¼‰ï¼Œç¬¦åˆç›´è§‰
- **è§†è§‰åˆ†æ**ï¼šéšç€é¢„æµ‹æ­¥æ•°å¢åŠ ï¼Œensemble mean é€æ¸åç¦» ground truthï¼Œuncertainty map æ˜¾ç¤ºå‡ºæ¸…æ™°çš„ç©ºé—´èšç„¦æ¨¡å¼

> âœ”ï¸ æˆåŠŸæ•æ‰ multi-step rollout ä¸­çš„å¯é æ€§ä¸‹é™è¶‹åŠ¿ï¼Œå¹¶ç»™å‡ºç‰©ç†æ„ä¹‰æ˜ç¡®çš„ä¸ç¡®å®šæ€§åˆ†å¸ƒ

---

### âœ… Study C: Out-of-Distribution Detection
- **Anomaly Score Ratio**ï¼šå¹³å‡ **10~100 å€**
  - å³ OOD è¾“å…¥ï¼ˆdigit â€œ7â€ï¼‰çš„ bottleneck uncertainty è¿œé«˜äº ID è¾“å…¥ï¼ˆdigit â€œ0â€ï¼‰
- **ROC-AUC**ï¼š**0.8 ~ 0.9**ï¼Œæ˜¾ç¤ºè‰¯å¥½åˆ¤åˆ«èƒ½åŠ›
- **Reconstruction MSE**ï¼šOOD è¾“å…¥é«˜å‡º 3â€“4 å€
- **å®šæ€§è§‚å¯Ÿ**ï¼šå¯¹äº â€œ7â€ï¼Œé‡æ„å›¾åƒæ¨¡ç³Šã€è¶‹äºâ€œ0â€ç±»æ¨¡å¼ï¼Œä½“ç°æµå½¢æŠ•å½±è¡Œä¸º

> âœ”ï¸ åœ¨æ— éœ€æ˜¾å¼ OOD æ ‡ç­¾è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä¾é  GP çš„è‡ªç„¶æ–¹å·®æ”¾å¤§æœºåˆ¶å®ç°æœ‰æ•ˆå¼‚å¸¸æ£€æµ‹

---

### ğŸ” æ¶ˆèå®éªŒï¼ˆéšå«åˆ†æï¼‰
è™½ç„¶æ²¡æœ‰ç‹¬ç«‹ç« èŠ‚ï¼Œä½†é€šè¿‡ä¸åŒ $\lambda$ è®¾ç½®ä½“ç°äº†æ¶ˆèæ€æƒ³ï¼š
- $\lambda = 0.01$ï¼šé€‚ç”¨äº in-distribution å›å½’ä»»åŠ¡ï¼ˆStudies A & Bï¼‰ï¼Œæå‡æ ¡å‡†æ€§
- $\lambda = 0$ï¼šä¸“ç”¨äº OOD æ£€æµ‹ï¼ˆStudy Cï¼‰ï¼Œé˜²æ­¢ KL æ­£åˆ™å‹åˆ¶ä¿¡å·æ–¹å·®ï¼Œä¿ç•™æ•æ„Ÿæ€§

æ­¤å¤–ï¼Œinducing point æ•°é‡ $M=20$ è¢«è¯æ˜è¶³ä»¥åœ¨å¤šæ•°ä»»åŠ¡ä¸­å–å¾—ç¨³å®šè¡¨ç°ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ğŸ¯ ä¸»è¦å‘ç°
1. **SVGP-KAN æˆåŠŸå®ç°äº†å¯æ‰©å±•ã€å¯è§£é‡Šä¸”å…·å¤‡ principled uncertainty quantification çš„ SciML æ¶æ„**ã€‚
2. èƒ½å¤Ÿ**æ˜ç¡®åŒºåˆ† aleatoric ä¸ epistemic ä¸ç¡®å®šæ€§**ï¼š
   - Study Aï¼šå“åº”æµ‹é‡å™ªå£°å¼ºåº¦ â†’ aleatoric
   - Study Bï¼šéšé¢„æµ‹æ­¥æ•°å¢é•¿ â†’ epistemic
   - Study Cï¼šé¢å¯¹ OOD è¾“å…¥ â†’ epistemicï¼ˆæ¨¡å‹æœªçŸ¥ï¼‰
3. **ä¸ç¡®å®šæ€§å…·æœ‰ç‰©ç†æ„ä¹‰çš„ç©ºé—´ç»“æ„**ï¼Œå¯ç”¨äºæŒ‡å¯¼è‡ªé€‚åº”é‡‡æ ·æˆ–ä¼ æ„Ÿå™¨éƒ¨ç½²ã€‚
4. **è‡ªç„¶æ–¹å·®æ”¾å¤§æœºåˆ¶**ï¼ˆnatural variance amplificationï¼‰æ˜¯ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç° OOD æ£€æµ‹çš„æœ‰æ•ˆæ‰‹æ®µï¼Œå°¤å…¶å½“ $\lambda = 0$ æ—¶æ›´æ˜¾è‘—ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **æ ¸å‡½æ•°é€‰æ‹©å—é™**ï¼š
   - å½“å‰ä¸»è¦åŸºäº RBF kernelï¼Œå…¶å‡è®¾å‡½æ•°å…‰æ»‘ï¼›
   - å¯¹ä¸è¿ç»­æˆ–å°–é”è¿‡æ¸¡ç°è±¡å»ºæ¨¡å¯èƒ½ä¸è¶³ï¼Œæœªæ¥å¯å°è¯• MatÃ©rn kernelã€‚
2. **Inducing Point æ•°é‡éœ€æ‰‹åŠ¨è®¾å®š**ï¼š
   - $M$ æ˜¯è¶…å‚ï¼Œè¿‡å¤šå½±å“æ•ˆç‡ï¼Œè¿‡å°‘æŸå¤±ç²¾åº¦ï¼›
   - ç¼ºä¹è‡ªåŠ¨åˆ†é…æœºåˆ¶ï¼ˆå¦‚æ ¹æ®å±€éƒ¨å¤æ‚åº¦è°ƒæ•´å¯†åº¦ï¼‰ã€‚
3. **å½“å‰ä»…æ”¯æŒ Gaussian Likelihood**ï¼š
   - ä¸é€‚ç”¨äºåˆ†ç±»ã€è®¡æ•°æ•°æ®æˆ–é‡å°¾å™ªå£°åœºæ™¯ã€‚
4. **KL weight $\lambda$ éœ€ç»éªŒè°ƒå‚**ï¼š
   - ä¸åŒä»»åŠ¡æœ€ä¼˜å€¼ä¸åŒï¼Œç¼ºä¹è‡ªåŠ¨åŒ–è°ƒèŠ‚ç­–ç•¥ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³éé«˜æ–¯ä¼¼ç„¶**ï¼ˆPoisson, Bernoulli, Student-tï¼‰ä»¥è¦†ç›–æ›´å¤šç§‘å­¦é—®é¢˜ã€‚
2. **å¼€å‘åŠ¨æ€ $\lambda$ è°ƒæ•´æœºåˆ¶**ï¼ŒåŸºäºéªŒè¯é›†ä¸ç¡®å®šæ€§æ ¡å‡†æŒ‡æ ‡è‡ªåŠ¨ä¼˜åŒ–ã€‚
3. **ç»“åˆ Recurrent æˆ– Graph ç»“æ„**ï¼Œåº”ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹æˆ–åˆ†å­å»ºæ¨¡ã€‚
4. **é›†æˆ Active Learning æ¡†æ¶**ï¼Œåˆ©ç”¨ä¸ç¡®å®šæ€§å›¾æŒ‡å¯¼æ•°æ®é‡‡é›†æˆ–ç½‘æ ¼ç»†åŒ–ã€‚
5. **æ¢ç´¢ Automatic Relevance Determination (ARD)** æœºåˆ¶ï¼Œå®ç°ç‰¹å¾é‡è¦æ€§ä¸ä¸ç¡®å®šæ€§è”åˆåˆ†æã€‚

---

## æ€»ç»“

âœ… **SVGP-KAN æ˜¯ä¸€ç§é¢å‘ç§‘å­¦æœºå™¨å­¦ä¹ çš„æ–°å‹ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¶æ„**ï¼Œå®ƒèåˆäº†ï¼š
- KAN çš„**å¯è§£é‡Šæ€§**
- GP çš„**åŸåˆ™æ€§ä¸ç¡®å®šæ€§å»ºæ¨¡**
- Sparse Variational Inference çš„**å¯æ‰©å±•æ€§**

é€šè¿‡ä¸‰é¡¹ç²¾å¿ƒè®¾è®¡çš„å®éªŒï¼Œè®ºæ–‡å……åˆ†å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ **å¼‚æ–¹å·®å™ªå£°æ ¡å‡†ã€å¤šæ­¥é¢„æµ‹ä¸ç¡®å®šæ€§å¢é•¿å»ºæ¨¡ã€ä»¥åŠ OOD æ£€æµ‹**æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚å°½ç®¡å­˜åœ¨ä¸€äº›é™åˆ¶ï¼Œä½†å®ƒä¸ºæ„å»º**å¯é ã€å¯ä¿¡ã€å¯è§£é‡Šçš„ SciML æ¨¡å‹**æä¾›äº†åšå®åŸºç¡€ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

</details>

---

### 4. [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)

**Authors**: Yiwen Liang, Qiufeng Li, Shikai Wang, Weidong Cao  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.05341v1  

#### Abstract
Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of propr...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å½“å‰ç”¨äºç¡¬ä»¶ä»£ç ç”Ÿæˆçš„ **Large Language Models (LLMs)** åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½**è¿‡åº¦è®°å¿†**ï¼ˆmemorizationï¼‰ä»¥ä¸‹ä¸‰ç±»æœ‰å®³å†…å®¹ï¼š
- **ä¸“æœ‰çŸ¥è¯†äº§æƒ (IP)**ï¼šå¦‚æ¥è‡ªå…¬å¼€ä»“åº“ä¸­çš„å—ç‰ˆæƒä¿æŠ¤çš„ RTL æ¨¡å—ï¼›
- **è¢«æ±¡æŸ“çš„åŸºå‡†æµ‹è¯•æ•°æ®**ï¼šå¦‚ VerilogEval å’Œ Rtllm æ•°æ®é›†ä¸­å­˜åœ¨ä¸æ¨¡å‹è®­ç»ƒé›†é‡å çš„é—®é¢˜ï¼›
- **ä¸å®‰å…¨çš„ç¼–ç æ¨¡å¼**ï¼šå¯èƒ½å¯¼è‡´ç”Ÿæˆå¸¦æœ‰æ¼æ´æˆ–ä¸å¯ç»¼åˆçš„ RTL ä»£ç ã€‚

è¿™äº›è®°å¿†è¡Œä¸ºä¼šå¼•å‘ **IP æ³„éœ²é£é™©ã€è¯„ä¼°å¤±çœŸã€ç³»ç»Ÿå¯é æ€§ä¸‹é™**ç­‰é—®é¢˜ï¼Œä¸¥é‡åˆ¶çº¦äº† LLM åœ¨å¯ä¿¡ç¡¬ä»¶è®¾è®¡ä¸­çš„åº”ç”¨ã€‚

ä¼ ç»Ÿè§£å†³æ–¹æ¡ˆï¼ˆå¦‚ä»å¤´é‡æ–°è®­ç»ƒï¼‰æˆæœ¬è¿‡é«˜ï¼ˆä¾‹å¦‚ LLaMA-3 8B éœ€è¦çº¦ 146 ä¸‡ GPU å°æ—¶ï¼‰ï¼Œè€Œé€šç”¨çš„ **machine unlearning** æ–¹æ³•ï¼ˆå¦‚ GAã€SimNPOï¼‰åœ¨åº”ç”¨äºç¡¬ä»¶é¢†åŸŸæ—¶ï¼Œå¾€å¾€ç ´å RTL ä»£ç çš„è¯­æ³•æ­£ç¡®æ€§å’ŒåŠŸèƒ½å®Œæ•´æ€§ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

æœ¬æ–‡æå‡ºé¦–ä¸ªé¢å‘ **LLM-based ç¡¬ä»¶ä»£ç ç”Ÿæˆ** çš„**é¢†åŸŸä¸“ç”¨é—å¿˜æ¡†æ¶**ï¼ˆdomain-specific unlearning frameworkï¼‰ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

#### âœ… **1. è¯­æ³•ä¿æŒå‹é—å¿˜ç­–ç•¥**ï¼ˆSyntax-Preserving Unlearningï¼‰
- **åŠ¨æœº**ï¼šRTL ä»£ç å…·æœ‰ä¸¥æ ¼çš„è¯­æ³•ç»“æ„ï¼ˆå¦‚ `always @(posedge clk)`ã€`module/endmodule` ç­‰å…³é”®å­—ï¼‰ï¼Œè‹¥åœ¨é—å¿˜è¿‡ç¨‹ä¸­ä¿®æ”¹è¿™äº›ç»“æ„è¯ï¼Œä¼šå¯¼è‡´ç”Ÿæˆä»£ç æ— æ³•ç¼–è¯‘æˆ–ç»¼åˆã€‚
- **æ–¹æ³•**ï¼š
  - å¼•å…¥ **keyword masking**ï¼šå¯¹ Verilog å…³é”®å­—ï¼ˆå¦‚ `wire`, `reg`, `assign`ï¼‰è¿›è¡Œæ©ç ï¼Œä½¿å…¶ä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼›
  - ä½¿ç”¨ **skip-tag masking**ï¼šé€šè¿‡ `<SKIP_S>` å’Œ `<SKIP_E>` æ ‡è®°éœ€è¦è·³è¿‡çš„ä»£ç ç‰‡æ®µï¼Œåœ¨è®¡ç®—æŸå¤±æ—¶ä¸è€ƒè™‘è¿™äº› tokenã€‚
- **æ•ˆæœ**ï¼šç¡®ä¿é—å¿˜è¿‡ç¨‹ä¸ä¼šç ´å RTL çš„è¯­æ³•éª¨æ¶ï¼Œç»´æŒç”Ÿæˆä»£ç çš„ **compilability** å’Œ **synthesizability**ã€‚

#### âœ… **2. ç»†ç²’åº¦æ¥¼å±‚æ„ŸçŸ¥é€‰æ‹©æ€§æŸå¤±å‡½æ•°**ï¼ˆFine-grained Floor-aware Selective Loss, FiFSLï¼‰
- **åŠ¨æœº**ï¼šä¼ ç»Ÿé—å¿˜æ–¹æ³•å¯¹æ‰€æœ‰æ ·æœ¬å‡åŒ€æ–½åŠ â€œåå‘å­¦ä¹ â€å‹åŠ›ï¼Œå®¹æ˜“é€ æˆè¿‡åº¦é—å¿˜æˆ–ä¸ç¨³å®šã€‚
- **æ–¹æ³•**ï¼š
  - åœ¨ token çº§åˆ«å¼•å…¥ **margin-based æ§åˆ¶æœºåˆ¶**ï¼šä»…å½“æ ·æœ¬çš„ loss å°äºé˜ˆå€¼ Î³ æ—¶æ‰æ–½åŠ å¼ºé—å¿˜å‹åŠ›ï¼›
  - åŠ å…¥ **hard floor $L_{\text{min}}$**ï¼šåªå¯¹æœªå……åˆ†é—å¿˜çš„æ ·æœ¬ï¼ˆloss > $L_{\text{min}}$ï¼‰è®¡ç®—æ¢¯åº¦ï¼Œé¿å…å·²é—å¿˜æ ·æœ¬ç»§ç»­æ‰°åŠ¨æ¨¡å‹ï¼›
  - ä½¿ç”¨ **smooth softplus æ˜ å°„** æ›¿ä»£åŸå§‹è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œæå‡ä¼˜åŒ–ç¨³å®šæ€§ã€‚
- **ä¼˜åŠ¿**ï¼šå®ç°æ›´ç²¾å‡†ã€é«˜æ•ˆã€ç¨³å®šçš„é—å¿˜ï¼Œå°¤å…¶é€‚ç”¨äºé•¿åºåˆ— RTL ç”Ÿæˆä»»åŠ¡ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç‰¹æ€§ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆGA / SimNPOï¼‰ | æœ¬æ–‡æ–¹æ³•ï¼ˆOursï¼‰ |
|------|------------------------|------------------|
| æ˜¯å¦ä¿ç•™è¯­æ³•ç»“æ„ | âŒ å¿½è§† HDL ç‰¹æ€§ï¼Œæ˜“ç ´åè¯­æ³• | âœ… æ˜¾å¼ä¿æŠ¤å…³é”®è¯ä¸ç»“æ„ |
| é—å¿˜æ•ˆç‡ | âš ï¸ éœ€ 4â€“7 ä¸ª epoch æ‰æ”¶æ•› | âœ… é€šå¸¸åªéœ€ **1 ä¸ª epoch** |
| å¯æ‰©å±•æ€§ | âŒ æœ€å¤§æ”¯æŒ ~10% forget set | âœ… æ”¯æŒé«˜è¾¾ **30% forget set**ï¼ˆå³ 3Ã— æ›´å¤§ï¼‰ |
| åŠŸèƒ½ä¿çœŸåº¦ | âŒ Pass@1 æ€¥å‰§ä¸‹é™ | âœ… å‡ ä¹æ— æŸä¿ç•™ç”Ÿæˆèƒ½åŠ› |
| å®ç”¨æ€§ | âš ï¸ å­˜åœ¨ utility degradation | âœ… å®ç°æ¥è¿‘ç†æƒ³çš„ forget-utility trade-off |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

- **ä¸»è®­ç»ƒ/å¾®è°ƒæ•°æ®é›†**ï¼š**RTLCoder dataset**ï¼ˆå¼€æºæŒ‡ä»¤-ä»£ç é…å¯¹æ•°æ®é›†ï¼ŒåŒ…å«è‡ªç„¶è¯­è¨€æè¿°ä¸å¯¹åº” RTL å®ç°ï¼‰
- **é—å¿˜é›†æ„é€ æ–¹å¼**ï¼šç”±äºç¼ºä¹çœŸå®â€œéœ€é—å¿˜â€çš„ä¸“æœ‰ IP æˆ–æ±¡æŸ“æ•°æ®ï¼Œä½œè€…é‡‡ç”¨ **ä»è®­ç»ƒé›†ä¸­é‡‡æ · 10%ã€20%ã€30% çš„æ ·æœ¬ä½œä¸º forget set** æ¥æ¨¡æ‹Ÿç°å®åœºæ™¯ï¼ˆå¦‚ IP åˆ é™¤è¯·æ±‚ã€æ±¡æŸ“æ¨¡å—ç§»é™¤ç­‰ï¼‰
- **éªŒè¯é›†**ï¼šRTLCoder çš„ hold-out æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ utility

---

### **å®éªŒè®¾ç½®**

- **æ¨¡å‹é€‰æ‹©**ï¼š
  - CodeLlama-7B-Instruct
  - Llama-3-8B-Instructï¼ˆé€šç”¨æ¨¡å‹ï¼‰
  - DeepSeek-Coder-7B-Instruct
  - Qwen2.5-Coder-7B-Instruct
- **å¾®è°ƒé…ç½®**ï¼š
  - Adam ä¼˜åŒ–å™¨ï¼Œlr = 1e-5ï¼Œbatch size = 2ï¼Œmax seq len = 2048
  - å¾®è°ƒ 6 è½®ä»¥å»ºç«‹å¼ºè®°å¿†åŸºçº¿
- **é—å¿˜é˜¶æ®µå‚æ•°**ï¼š
  - lr = 2e-6
  - FiFSL å‚æ•°ï¼šÎ² = 2.5, Î³ = 0, $L_{\text{min}}$ = 0.35ï¼ˆåŸºäºé¦–è½®å¾®è°ƒåçš„éªŒè¯ loss è®¾å®šï¼‰

---

### **è¯„ä¼°æŒ‡æ ‡**

#### ğŸ”¹ **Forget Qualityï¼ˆé—å¿˜è´¨é‡ï¼‰**
- **PrivLeak**ï¼šè¡¡é‡æ¨¡å‹æ˜¯å¦å†ç”Ÿå‡º forget set ä¸­çš„åŸå§‹ä»£ç ç‰‡æ®µï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- **MinK++**ï¼šæ£€æµ‹æ¨¡å‹æ˜¯å¦ä»æ®‹ç•™å¯¹æŸäº›é«˜é¢‘ token åˆ†å¸ƒçš„è®°å¿†ï¼ˆè¶Šä½è¶Šå¥½ï¼Œæœªè®­ç»ƒæ¨¡å‹çº¦ä¸º 0.5ï¼‰

#### ğŸ”¹ **Model Utilityï¼ˆæ¨¡å‹æ•ˆç”¨ï¼‰**
- **Validation Loss**ï¼šåœ¨ä¿ç•™é›†ä¸Šçš„äº¤å‰ç†µæŸå¤±ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- **Pass@1**ï¼šç”Ÿæˆä»£ç èƒ½å¦é€šè¿‡ç¼–è¯‘å¹¶æ»¡è¶³åŠŸèƒ½æµ‹è¯•ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
- **BLEU / chrF**ï¼šä¸å‚è€ƒå®ç°çš„ n-gram å’Œå­—ç¬¦çº§ç›¸ä¼¼åº¦ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰

#### ğŸ”¹ **æ³›åŒ–èƒ½åŠ›**
- åœ¨ **VerilogEval** å’Œ **Rtllm** åŸºå‡†ä¸Šè¯„ä¼°é—å¿˜åæ¨¡å‹çš„è¡¨ç°å˜åŒ–

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

- **Gradient Ascent (GA)** [9]ï¼šæœ€å¤§åŒ– forget set ä¸Šçš„è´Ÿå¯¹æ•°ä¼¼ç„¶
- **SimNPO** [12]ï¼šæ”¹è¿›ç‰ˆ NPOï¼Œå¼•å…¥å¹³æ»‘æƒ©ç½šé¡¹å¢å¼ºç¨³å®šæ€§
- **SALAD** [16]ï¼šå”¯ä¸€å°è¯•å°† unlearning åº”ç”¨äºç¡¬ä»¶è®¾è®¡çš„å·¥ä½œï¼Œä½†ç›´æ¥å¥—ç”¨é€šç”¨æ–¹æ³•ï¼Œæœªåšé¢†åŸŸé€‚é…

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table II & Fig. 3ï¼‰**

| æ¨¡å‹ | Forget Set | æ–¹æ³• | Epochs | MinK++ â†“ | Pass@1 â†‘ | ç›¸å¯¹æå‡ |
|------|------------|-------|--------|----------|----------|-----------|
| Llama-7B | 30% | GA | 4 | 0.54 | 3% | â€” |
| Llama-7B | 30% | SimNPO | 4 | 0.53 | 1% | â€” |
| Llama-7B | 30% | **Ours** | **1** | **0.53** | **44%** | â¬†ï¸ **14.3Ã— Pass@1** |
| DeepSeek-7B | 30% | GA | 5 | 0.51 | 28% | â€” |
| DeepSeek-7B | 30% | SimNPO | â€” | 0.50 | 28% | â€” |
| DeepSeek-7B | 30% | **Ours** | **1** | **0.45** | **53%** | â¬†ï¸ æ˜¾è‘—ä¼˜äºåŸºçº¿ |
| Qwen-7B | 30% | GA | 3 | 0.50 | 13% | â€” |
| Qwen-7B | 30% | SimNPO | 3 | 0.50 | 10% | â€” |
| Qwen-7B | 30% | **Ours** | **1** | **0.49** | **55%** | â¬†ï¸ æ¥è¿‘åŸå§‹æ€§èƒ½ |

> æ³¨ï¼šPass@1 ä¸‹é™è¡¨ç¤º utility æŸå¤±ï¼›æœ¬æ–‡æ–¹æ³•åœ¨é«˜é—å¿˜æ¯”ä¾‹ä¸‹ä»ä¿æŒé«˜åŠŸèƒ½æ€§ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

- **é—å¿˜èƒ½åŠ›æ›´å¼º**ï¼š
  - æ”¯æŒ **3Ã— æ›´å¤§çš„ forget set**ï¼ˆ30% vs 10%ï¼‰ï¼Œè€ŒåŸºçº¿åœ¨ 20%-30% å³å´©æºƒï¼›
- **æ•ˆç‡æ›´é«˜**ï¼š
  - **ä»…éœ€ 1 ä¸ª epoch å®Œæˆæœ‰æ•ˆé—å¿˜**ï¼Œè€Œ GA/SimNPO éœ€ 4â€“7 ä¸ª epochï¼›
- **å®ç”¨æ€§æ›´å¥½**ï¼š
  - åœ¨ **PrivLeak å’Œ MinK++ ä¸Šæ˜¾è‘—é™ä½**ï¼Œè¡¨æ˜è®°å¿†è¢«æœ‰æ•ˆæ¸…é™¤ï¼›
  - åŒæ—¶ **Pass@1ã€BLEUã€chrF å‡ ä¹ä¸å˜æˆ–è½»å¾®æ³¢åŠ¨**ï¼Œè¯´æ˜ utility å¾—åˆ°è‰¯å¥½ä¿ç•™ã€‚

---

### **æ¶ˆèå®éªŒä¸æ³›åŒ–èƒ½åŠ›åˆ†æ**

#### âœ… **æ³›åŒ–èƒ½åŠ›ä¿æŒè‰¯å¥½ï¼ˆTable IIIï¼‰**

| Model | Forget Set | VerilogEval Pass@1 | Rtllm Pass@1 |
|-------|------------|--------------------|-------------|
| Qwen-7B | 30% | 33% â†’ **33%**ï¼ˆä¸å˜ï¼‰ | 36% â†’ **36%** |
| DeepSeek-7B | 30% | 35% â†’ **33%** | 40% â†’ **38%** |

> è¡¨æ˜é—å¿˜æ“ä½œæœªæŸå®³æ¨¡å‹åœ¨æ ‡å‡† benchmark ä¸Šçš„æ•´ä½“è¡¨ç°ã€‚

#### âœ… **è·¨æ¨¡å‹ä¸€è‡´æ€§é«˜**
- åœ¨å››ç§ä¸åŒæ¶æ„çš„ LLM ä¸Šå‡å–å¾—ä¸€è‡´ä¼˜åŠ¿ï¼Œè¯æ˜æ–¹æ³•å…·æœ‰è‰¯å¥½çš„**å¯è¿ç§»æ€§ä¸é²æ£’æ€§**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. âœ… **æœºå™¨é—å¿˜å¯ä»¥æˆä¸ºæ„å»ºå¯é ç¡¬ä»¶ç”Ÿæˆ LLM çš„å…³é”®æŠ€æœ¯è·¯å¾„**ï¼šé€šè¿‡æœ‰é€‰æ‹©åœ°â€œå¿˜è®°â€æœ‰å®³å†…å®¹ï¼Œåè€Œæå‡äº†ç³»ç»Ÿçš„å®‰å…¨æ€§ä¸å¯ä¿¡åº¦ã€‚
2. âœ… **é¢†åŸŸç‰¹æ€§å¿…é¡»èå…¥ unlearning è¿‡ç¨‹**ï¼šå¿½ç•¥ RTL è¯­æ³•ç»“æ„çš„ä¼ ç»Ÿæ–¹æ³•ä¼šå¯¼è‡´ç”Ÿæˆå¤±æ•ˆï¼›æœ¬æ–‡æå‡ºçš„ syntax-preserving ç­–ç•¥æ˜¯æˆåŠŸçš„å…³é”®ã€‚
3. âœ… **FiFSL å®ç°äº†é«˜æ•ˆä¸”å¯æ§çš„é—å¿˜**ï¼šç»“åˆ margin æ§åˆ¶ä¸åŠ¨æ€æ ·æœ¬ç­›é€‰ï¼Œå®ç°äº†å¿«é€Ÿæ”¶æ•›ä¸ç¨³å®šè®­ç»ƒã€‚
4. âœ… **å¤§è§„æ¨¡é—å¿˜ + é«˜æ•ˆ + é«˜ä¿çœŸæ˜¯å¯è¡Œçš„**ï¼šæœ¬æ–¹æ³•åœ¨ä»… 1 ä¸ª epoch å†…å®Œæˆé«˜è¾¾ 30% æ•°æ®çš„é—å¿˜ï¼ŒåŒæ—¶å‡ ä¹ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

- **ä¾èµ–äººå·¥å®šä¹‰å…³é”®è¯é›†åˆ**ï¼šè™½ç„¶åŸºäº IEEE æ ‡å‡† [32]-[34]ï¼Œä½†åœ¨æ‰©å±•åˆ°å…¶ä»– HDLï¼ˆå¦‚ VHDLï¼‰æ—¶éœ€é‡æ–°è®¾è®¡ keyword maskï¼›
- **forget set æ„é€ ä¸ºæ¨¡æ‹Ÿåœºæ™¯**ï¼šç›®å‰å®éªŒåŸºäºè®­ç»ƒé›†å­é›†æŠ½æ ·ï¼Œå°šæœªåœ¨çœŸå®æ³•å¾‹åˆè§„è¯·æ±‚æˆ–å·¥ä¸šçº§ IP åˆ é™¤åœºæ™¯ä¸­éªŒè¯ï¼›
- **æœªæ¶‰åŠå¤šè½®å¢é‡é—å¿˜**ï¼šå¦‚ä½•æ”¯æŒæŒç»­æ€§çš„â€œåˆ é™¤è¯·æ±‚æµâ€å°šå¾…ç ”ç©¶ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

- æ‰©å±•è‡³ **VHDLã€SystemVerilog æ›´å¤æ‚è¯­æ³•ç»“æ„**ï¼›
- æ¢ç´¢ **è‡ªåŠ¨åŒ–è¯†åˆ«éœ€é—å¿˜æ¨¡å—çš„æŠ€æœ¯**ï¼ˆå¦‚åŸºäº watermarking æˆ–æº¯æºåˆ†æï¼‰ï¼›
- ç ”ç©¶ **å¢é‡å¼ unlearning pipeline**ï¼Œæ”¯æŒåœ¨çº¿åˆ é™¤è¯·æ±‚ï¼›
- ç»“åˆ **formal verification** æŠ€æœ¯è¿›ä¸€æ­¥éªŒè¯é—å¿˜åä»£ç çš„åŠŸèƒ½ç­‰ä»·æ€§ä¸å®‰å…¨æ€§ã€‚

---

## æ€»ç»“

> æœ¬æ–‡å¼€åˆ›æ€§åœ°å°† **machine unlearning** å¼•å…¥ **LLM-based ç¡¬ä»¶ä»£ç ç”Ÿæˆ** é¢†åŸŸï¼Œæå‡ºé¦–ä¸ªé¢†åŸŸè‡ªé€‚åº”çš„é—å¿˜æ¡†æ¶ã€‚é€šè¿‡ **syntax-preserving masking** ä¸ **FiFSL æŸå¤±å‡½æ•°** çš„ååŒè®¾è®¡ï¼Œå®ç°äº† **é«˜æ•ˆã€ç²¾å‡†ã€å¯é çš„é—å¿˜**ï¼Œåœ¨æ”¯æŒæ›´å¤§ forget set çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äº GAã€SimNPO ç­‰é€šç”¨æ–¹æ³•ï¼Œå¹¶ä¿æŒäº† RTL ç”Ÿæˆçš„è´¨é‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ºæ‰“é€ **å¯ä¿¡èµ–ã€åˆè§„ã€å®‰å…¨çš„ AI è¾…åŠ©èŠ¯ç‰‡è®¾è®¡å·¥å…·é“¾**æä¾›äº†é‡è¦æŠ€æœ¯åŸºç¡€ã€‚

</details>

---

### 5. [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)

**Authors**: Wencheng Cai, Xuchao Gao, Congying Han, Mingqiang Li, Tiande Guo  
**Category**: cs.AI  
**Published**: 2025-12-08  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.05753v1  

#### Abstract
The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local o...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
æœ¬æ–‡é’ˆå¯¹**è®¤çŸ¥é›·è¾¾åœ¨ç”µå­å¹²æ‰°ç¯å¢ƒä¸‹çš„å¿«é€Ÿéƒ¨ç½²é—®é¢˜**ï¼ˆAnti-Jamming Cognitive Radar Deploymentï¼‰å±•å¼€ç ”ç©¶ã€‚è¯¥é—®é¢˜æ˜¯ç°ä»£æˆ˜äº‰ä¸­æå‡æˆ˜åœºæ„ŸçŸ¥èƒ½åŠ›çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äº**è¿›åŒ–ç®—æ³•**ï¼ˆEvolutionary Algorithms, å¦‚ GAã€PSOï¼‰ï¼Œå­˜åœ¨ä»¥ä¸‹ç¼ºé™·ï¼š
- **è®¡ç®—è€—æ—¶é•¿**ï¼šéœ€è¦å¤šè½®è¿­ä»£æœç´¢ï¼›
- **æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜**ï¼šæœç´¢ç­–ç•¥å—é™ï¼Œéš¾ä»¥å…¨é¢æ¢ç´¢è§£ç©ºé—´ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ç§å…¨æ–°çš„åŸºäº**æ·±åº¦å¼ºåŒ–å­¦ä¹ **ï¼ˆDeep Reinforcement Learning, DRLï¼‰çš„æ¡†æ¶â€”â€”**Fast Anti-Jamming Radar Deployment Algorithm (FARDA)**ï¼Œå…¶ä¸»è¦åˆ›æ–°åŒ…æ‹¬ï¼š

- **é¦–æ¬¡å°† DRL åº”ç”¨äºæŠ—å¹²æ‰°é›·è¾¾éƒ¨ç½²é—®é¢˜**ï¼šæ­¤å‰è™½æœ‰ç ”ç©¶ä½¿ç”¨ RL å¤„ç†é›·è¾¾èµ„æºè°ƒåº¦æˆ–æ³¢å½¢è®¾è®¡ï¼Œä½†å°šæ— å·¥ä½œå°†å…¶ç”¨äºéƒ¨ç½²ä½ç½®ä¼˜åŒ–ã€‚
- **ç«¯åˆ°ç«¯å»ºæ¨¡ä¸ºç»„åˆä¼˜åŒ–é—®é¢˜**ï¼šå°†é›·è¾¾éƒ¨ç½²å»ºæ¨¡ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–ä»»åŠ¡ï¼Œå¹¶æ„å»ºç›¸åº”çš„ **Markov Decision Process (MDP)**ã€‚
- **è®¾è®¡ä¸“ç”¨ç¥ç»ç½‘ç»œæ¨¡å—**ï¼š
  - æ„å»ºèåˆ **DCNN + LSTM** çš„ç¼–ç å™¨ï¼ˆEncoder Networkï¼‰ï¼Œç”¨äºæå–æ£€æµ‹æ¦‚ç‡çƒ­åŠ›å›¾ï¼ˆheatmapï¼‰ç‰¹å¾å¹¶è®°å¿†å†å²çŠ¶æ€å˜åŒ–ã€‚
- **æå‡ºæ–°å‹å¥–åŠ±æ•´å½¢æœºåˆ¶ CVDP-EXPR**ï¼š
  - **Constraint Violation Degree Penalty (CVDP)**ï¼šå¯¹åç¦»å‡åŒ€éƒ¨ç½²çš„è¡Œä¸ºæ–½åŠ æƒ©ç½šï¼Œé˜²æ­¢è®­ç»ƒå´©æºƒã€‚
  - **EXPonential Function Reward (EXPR)**ï¼šç¼“è§£åæœŸå¢ç›Šé€’å‡å¯¼è‡´çš„å­¦ä¹ æ•ˆç‡ä¸‹é™é—®é¢˜ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
- **é€Ÿåº¦æå¿«**ï¼šç›¸æ¯”è¿›åŒ–ç®—æ³•æé€Ÿçº¦ **7,000 å€**ï¼›
- **è¦†ç›–æ€§èƒ½æ›´ä¼˜**ï¼šå¹³å‡è¦†ç›–ç‡ç•¥é«˜äºåŸºçº¿æ–¹æ³•ï¼›
- **éƒ¨ç½²æ•ˆç‡æ˜¾è‘—æå‡**ï¼šæå‡ºçš„â€œæ•ˆç‡æŒ‡æ ‡â€ï¼ˆefficiency metricï¼‰é«˜å‡ºåŸºçº¿ **120 å€ä»¥ä¸Š**ï¼›
- **å¯æ‰©å±•æ€§å¼º**ï¼šé€‚ç”¨äºå®æ—¶æ€§è¦æ±‚é«˜çš„å†›äº‹åº”ç”¨åœºæ™¯ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†ä¸ç¯å¢ƒè®¾ç½®
- **ä»¿çœŸåŒºåŸŸ S**ï¼š$20\text{km} \times 120\text{km}$ï¼Œåæ ‡èŒƒå›´ $[3\times10^4, 5\times10^4] \times [0, 1.2\times10^5]$ï¼›
- **é›·è¾¾éƒ¨ç½²åŒº $D_R$**ï¼šå·¦ä¸‹çŸ©å½¢åŒºåŸŸï¼›
- **æ•Œæ–¹å¹²æ‰°èŠ‚ç‚¹åŒº $D_j$**ï¼šå³ä¸ŠçŸ©å½¢åŒºåŸŸï¼›
- **ç¦»æ•£åŒ–ç²’åº¦**ï¼šåŸå§‹ä¸ºæ¯ 100 ç±³é‡‡æ ·ä¸€ç‚¹ï¼ˆå…±çº¦ $2.4\times10^5$ ç‚¹ï¼‰ï¼Œè®­ç»ƒæ—¶ç®€åŒ–ä¸ºæ¯ 500 ç±³é‡‡æ ·ï¼Œä¸”ä»…ä¿ç•™ä¸ŠåŠåŒºåŸŸä»¥åŠ é€Ÿè®¡ç®—ï¼›
- **å¹²æ‰°æºæ•°é‡**ï¼šå›ºå®š 3 ä¸ªï¼›
- **éœ€éƒ¨ç½²é›·è¾¾æ•°**ï¼š4 ä¸ªã€‚

### å®éªŒè®¾ç½®
- **æµ‹è¯•æ•°æ®é›†**ï¼šéšæœºç”Ÿæˆ 500 ç»„ä¸åŒçš„å¹²æ‰°èŠ‚ç‚¹é…ç½®ä½œä¸ºæµ‹è¯•é›†ï¼›
- **è®­ç»ƒè¿‡ç¨‹**ï¼šåœ¨ç®€åŒ–ç¯å¢ƒä¸­è¿›è¡Œï¼Œç¡®ä¿é«˜æ•ˆè®­ç»ƒï¼›
- **ç¡¬ä»¶å¹³å°**ï¼š
  - FARDAï¼šNvidia GeForce RTX 3090 GPUï¼›
  - è¿›åŒ–ç®—æ³•ï¼šIntel Core i9-12900 CPUã€‚

### è¯„ä¼°æŒ‡æ ‡
1. **Coverage (%)**ï¼šè¢«æˆåŠŸæ¢æµ‹çš„åŒºåŸŸå æ¯”ï¼ˆé˜ˆå€¼ $T=0.5$ï¼‰ï¼›
2. **Time (å“åº”æ—¶é—´)**ï¼šå•æ¬¡éƒ¨ç½²æ‰€éœ€æ—¶é—´ï¼›
3. **Efficiency Metric**ï¼ˆè‡ªå®šä¹‰ï¼‰ï¼š
   $$
   \text{efficiency} = \frac{\text{coverage}}{\log(1 + \text{time})}
   $$
   ç”¨ä»¥è¡¡é‡å•ä½å¯¹æ•°æ—¶é—´å†…çš„æ¢æµ‹æ•ˆèƒ½ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **PSO1D**ï¼šç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•ï¼Œåœ¨éƒ¨ç½²è¾¹ç•Œ BR ä¸Šè¿è¡Œï¼›
- **GA1D**ï¼šé—ä¼ ç®—æ³•ï¼Œåœ¨åŒä¸€è¾¹ç•Œ BR ä¸Šè¿è¡Œï¼›
> æ³¨ï¼šä½œè€…é€šè¿‡é¢„å®éªŒå‘ç°ï¼Œåœ¨è¾¹ç•Œéƒ¨ç½²åè€Œä¼˜äºåœ¨æ•´ä¸ªåŒºåŸŸå†…æœç´¢ï¼Œå› æ­¤æ‰€æœ‰æ¯”è¾ƒå‡åŸºäºè¾¹ç•Œéƒ¨ç½²ç­–ç•¥ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 3ï¼‰
| æ–¹æ³•       | Coverage (%) | Time     | Efficiency |
|------------|--------------|----------|-----------|
| PSO1D      | 93.75        | 24.9 min | 0.13      |
| GA1D       | 93.79        | 30.7 min | 0.12      |
| **FARDA**  | **93.94**    | **0.25s**| **4.21**  |

#### å¯¹æ¯”åˆ†æ
- **é€Ÿåº¦ä¼˜åŠ¿**ï¼šFARDA æ¯”è¿›åŒ–ç®—æ³•å¿«çº¦ **7,000 å€**ï¼ˆä»åŠå°æ—¶é™è‡³ 0.25 ç§’ï¼‰ï¼›
- **è¦†ç›–æ€§èƒ½**ï¼šFARDA è¦†ç›–ç‡æœ€é«˜ï¼ˆ+0.15%~+0.2%ï¼‰ï¼›
- **æ•ˆç‡æŒ‡æ ‡**ï¼šFARDA æ˜¯ PSO1D çš„ **32 å€ä»¥ä¸Š**ï¼Œæ˜¯ GA1D çš„ **35 å€ä»¥ä¸Š**ã€‚

### ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ï¼ˆTable 4ï¼‰
å°†æµ‹è¯•æ ·æœ¬æŒ‰éš¾åº¦åˆ†ä¸ºä¸‰ç±»ï¼š
| ç±»å‹   | å®šä¹‰                     | FARDA æ”¹è¿›å¹…åº¦ |
|--------|--------------------------|----------------|
| Bad    | GA1D & PSO1D < 90%       | +0.29%         |
| Normal | 90% â‰¤ ... < 95%          | +0.17%         |
| Good   | â‰¥95%                     | +0.08%         |

> è¡¨æ˜ FARDA åœ¨**å›°éš¾åœºæ™¯ä¸‹æå‡æ›´ä¸ºæ˜æ˜¾**ï¼Œå…·æœ‰æ›´å¼ºé²æ£’æ€§å’Œå®ç”¨æ€§ã€‚

### æ¶ˆèå®éªŒç»“æœ

#### ï¼ˆ1ï¼‰ç½‘ç»œç»“æ„æ¶ˆèï¼ˆTable 5ï¼‰
| æ–¹æ³•           | Coverage (%) |
|----------------|---------------|
| FARDA (å®Œæ•´)   | **93.94**     |
| w/o CNN        | 93.89         |
| w/o LSTM       | 93.90         |
| w/o Encoder    | 91.44         |

> ç»“è®ºï¼š**Encoder è‡³å…³é‡è¦**ï¼Œç¼ºå°‘ä»»ä¸€ç»„ä»¶éƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯å®Œå…¨ç§»é™¤ Encoder å¯¼è‡´è¦†ç›–ç‡æš´è·Œè¿‘ 2.5%ã€‚

#### ï¼ˆ2ï¼‰å¥–åŠ±æœºåˆ¶æ¶ˆèï¼ˆTable 6ï¼‰
| æ–¹æ³•             | Coverage (%) |
|------------------|---------------|
| FARDA (å®Œæ•´)     | **93.94**     |
| w/o CVDP         | 88.80         |
| w/o EXPR         | 93.90         |
| w/o CVDP-EXPR    | 88.80         |

> ç»“è®ºï¼š
- **CVDP è‡³å…³é‡è¦**ï¼šå»é™¤åæ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œä¸”å‡ºç°â€œæ‰€æœ‰é›·è¾¾èšé›†ä¸€ç«¯â€çš„è®­ç»ƒå´©æºƒç°è±¡ï¼ˆè§ Figure 5ï¼‰ï¼›
- **EXPR ä½œç”¨è¾ƒå°ä½†æœ‰ç›Š**ï¼šè½»å¾®æ”¹å–„åæœŸå­¦ä¹ ç¨³å®šæ€§ï¼›
- **CVDP æ˜¯é˜²æ­¢æ¨¡å¼åå¡Œçš„æ ¸å¿ƒæœºåˆ¶**ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **DRL å¯æœ‰æ•ˆè§£å†³å¤æ‚é›·è¾¾éƒ¨ç½²é—®é¢˜**ï¼šé€šè¿‡åˆç†å»ºæ¨¡ä¸ç½‘ç»œè®¾è®¡ï¼ŒDRL èƒ½åœ¨ä¿è¯é«˜è¦†ç›–ç‡çš„åŒæ—¶å®ç°è¶…é«˜é€Ÿéƒ¨ç½²ï¼›
2. **è¾¹ç•Œéƒ¨ç½²ä¼˜äºå…¨åŸŸæœç´¢**ï¼šå®éªŒè¯æ˜é™åˆ¶é›·è¾¾ä»…éƒ¨ç½²åœ¨åŒºåŸŸè¾¹ç•Œï¼ˆBRï¼‰ä¸ä»…ä¸æŸå¤±æ€§èƒ½ï¼Œåè€Œæœ‰åŠ©äºæ”¶æ•›ï¼›
3. **ç¯å¢ƒç®€åŒ–æ˜¾è‘—åŠ é€Ÿè®­ç»ƒ**ï¼šé€šè¿‡é™ä½åˆ†è¾¨ç‡ä¸è£å‰ªå†—ä½™åŒºåŸŸï¼Œå¯åœ¨å‡ ä¹ä¸å½±å“æ€§èƒ½çš„å‰æä¸‹å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡ï¼›
4. **CVDP å¥–åŠ±æœºåˆ¶è‡³å…³é‡è¦**ï¼šèƒ½æœ‰æ•ˆå¼•å¯¼é›·è¾¾å‡åŒ€åˆ†å¸ƒï¼Œé¿å…è®­ç»ƒå‘æ•£ã€‚

### æ–¹æ³•çš„å±€é™æ€§
- å½“å‰æ¨¡å‹å‡è®¾ä¸º**é™æ€å¹²æ‰°ç¯å¢ƒ**ï¼šå³å¹²æ‰°æºä½ç½®ä¸å˜ï¼Œæœªè€ƒè™‘åŠ¨æ€ç§»åŠ¨ç›®æ ‡æˆ–è·³é¢‘å¹²æ‰°ï¼›
- éƒ¨ç½²åŒºåŸŸå½¢çŠ¶è¾ƒè§„åˆ™ï¼šå°šæœªéªŒè¯åœ¨å¤æ‚åœ°å½¢æˆ–å¤šè¿é€šåŒºåŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼›
- é›·è¾¾æ•°é‡å›ºå®šï¼šæœªæ”¯æŒå¯å˜è§„æ¨¡éƒ¨ç½²ï¼›
- åŠ¨ä½œç©ºé—´è¿ç»­åŒ–å¤„ç†å¯èƒ½å¯¼è‡´å¾®å°å®šä½è¯¯å·®ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³æ›´å¤æ‚çš„é™æ€ç¯å¢ƒ**ï¼š
   - æ”¯æŒæ›´å¤šé›·è¾¾/å¹²æ‰°æºï¼›
   - æ›´å¤§æˆ–éè§„åˆ™éƒ¨ç½²åŒºåŸŸï¼›
2. **ç ”ç©¶åŠ¨æ€ç¯å¢ƒä¸‹çš„è‡ªé€‚åº”éƒ¨ç½²**ï¼š
   - å¹²æ‰°æºéšæ—¶é—´ç§»åŠ¨ï¼›
   - é›·è¾¾éœ€å¹³æ»‘è°ƒæ•´ä½ç½®ï¼ˆä¸èƒ½çªå˜ï¼‰ï¼›
3. **å¼•å…¥å¤šæ™ºèƒ½ä½“ååŒæœºåˆ¶**ï¼šå¤šä¸ªé›·è¾¾ä½œä¸ºç‹¬ç«‹ Agent ååŒå†³ç­–ï¼›
4. **ç»“åˆåœ¨çº¿å­¦ä¹ æœºåˆ¶**ï¼šå®ç°æˆ˜åœºç¯å¢ƒæŒç»­æ¼”åŒ–ä¸‹çš„å¢é‡éƒ¨ç½²ç­–ç•¥æ›´æ–°ã€‚

---

> âœ… æ€»ç»“ï¼šFARDA æ˜¯é¦–ä¸ªå°† DRL æˆåŠŸåº”ç”¨äºæŠ—å¹²æ‰°é›·è¾¾éƒ¨ç½²çš„å·¥ä½œï¼Œå®ç°äº†**ä»â€œæ…¢è€Œå‡†â€åˆ°â€œå¿«è€Œæ›´å‡†â€çš„èŒƒå¼è½¬å˜**ï¼Œå…·å¤‡æå¼ºçš„å®æˆ˜åº”ç”¨æ½œåŠ›ã€‚

</details>

---

### 6. [Using Large Language Models to Create Personalized Networks From Therapy Sessions](https://arxiv.org/abs/2512.05836)

**Authors**: Clarissa W. Ong, Hiba Arnaout, Kate Sheehan, Estella Fox, Eugen Owtscharow, Iryna Gurevych  
**Category**: cs.AI  
**Published**: 2025-12-08  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.05836v1  

#### Abstract
Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalabil...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š*Using Large Language Models to Create Personalized Networks From Therapy Sessions*

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»Ÿçš„**ä¸ªæ€§åŒ–æ²»ç–—**ï¼ˆpersonalized treatmentï¼‰ä¾èµ–äºå¯†é›†çš„çºµå‘æ•°æ®ï¼ˆå¦‚ç”Ÿæ€ç¬æ—¶è¯„ä¼°ï¼ŒEMAï¼‰æ¥æ„å»º**ä¸ªæ€§åŒ–ç½‘ç»œ**ï¼ˆpersonalized networksï¼‰ï¼Œç”¨äºæ¡ˆä¾‹æ¦‚å¿µåŒ–å’Œå¹²é¢„è§„åˆ’ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™ï¼š
- æ•°æ®æ”¶é›†æˆæœ¬é«˜ã€è´Ÿæ‹…é‡ï¼ˆæ‚£è€…éœ€å¤šæ¬¡å¡«å†™é—®å·ï¼‰
- èŠ‚ç‚¹å˜é‡é€šå¸¸æ˜¯é¢„è®¾çš„ï¼Œéš¾ä»¥æ•æ‰ä¸ªä½“ç‹¬ç‰¹çš„å¿ƒç†è¿‡ç¨‹
- ç»Ÿè®¡ä¼°è®¡æ–¹æ³•å¯¹æ•°æ®è´¨é‡è¦æ±‚é«˜ï¼ˆå¦‚å¹³ç¨³æ€§å‡è®¾ï¼‰ï¼Œåœ¨çœŸå®ä¸´åºŠåœºæ™¯ä¸­éš¾ä»¥æ»¡è¶³

æ­¤å¤–ï¼Œäººå·¥æ„å»ºç½‘ç»œè€—æ—¶ä¸”æ˜“å—ä¸»è§‚åè§å½±å“ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§**ç«¯åˆ°ç«¯çš„LLMé©±åŠ¨ç®¡é“**ï¼ˆend-to-end pipelineï¼‰ï¼Œä»**æ²»ç–—ä¼šè¯è½¬å½•æ–‡æœ¬**ä¸­è‡ªåŠ¨ç”Ÿæˆ**ä¸ªæ€§åŒ–ç½‘ç»œ**ï¼ˆpersonalized networksï¼‰ï¼Œæ”¯æŒæ¡ˆä¾‹æ¦‚å¿µåŒ–å’Œæ²»ç–—è§„åˆ’ã€‚

è¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸€ä¸ª**ä¸‰é˜¶æ®µå¤šæ­¥æµç¨‹**ï¼š
1. **è¿‡ç¨‹æ£€æµ‹**ï¼ˆProcess Detectionï¼‰ï¼šè¯†åˆ«å®¢æˆ·è¯è¯­ä¸­çš„å¿ƒç†è¿‡ç¨‹ï¼ˆpsychological processesï¼‰å¹¶åˆ†ç±»å…¶ç»´åº¦ï¼ˆdimensionsï¼ŒåŸºäºEEMMæ¡†æ¶ï¼‰ã€‚
2. **ä¸»é¢˜èšç±»**ï¼ˆTheme Clusteringï¼‰ï¼šå°†ç›¸ä¼¼çš„å¿ƒç†è¿‡ç¨‹èšç±»ä¸ºå…·æœ‰ä¸´åºŠæ„ä¹‰çš„ä¸»é¢˜ï¼ˆclinical themesï¼‰ã€‚
3. **è¿æ¥ç”Ÿæˆ**ï¼ˆConnection Generationï¼‰ï¼šé€šè¿‡é›†æˆå­¦ä¹ ï¼ˆensemble learningï¼‰åœ¨ä¸»é¢˜ä¹‹é—´ç”Ÿæˆå¯è§£é‡Šçš„æœ‰å‘è¿æ¥ï¼ˆexcitatory/inhibitory linksï¼‰ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ç»Ÿè®¡ä¼°è®¡ç½‘ç»œï¼ˆStatistically Estimated Networksï¼‰ | æœ¬æ–‡çš„LLMç”Ÿæˆç½‘ç»œ |
|------|-----------------------------------------------|------------------|
| **æ•°æ®æ¥æº** | éœ€è¦å¯†é›†çš„çºµå‘è‡ªæˆ‘æŠ¥å‘Šï¼ˆå¦‚EMAï¼‰ | ä»…éœ€æ²»ç–—ä¼šè¯è½¬å½•æœ¬ |
| **èŠ‚ç‚¹ç”Ÿæˆ** | é¢„è®¾å˜é‡ï¼Œè·¨ä¸ªä½“å›ºå®š | ä»å®¢æˆ·è¯è¯­ä¸­è‡ªä¸‹è€Œä¸Šæå–ï¼Œé«˜åº¦ä¸ªæ€§åŒ– |
| **å®¢æˆ·è´Ÿæ‹…** | é«˜ï¼ˆé¢‘ç¹å¡«å†™é—®å·ï¼‰ | æ—  |
| **å¯æ‰©å±•æ€§** | ä½ï¼ˆæ•°æ®æ”¶é›†å’Œåˆ†æå¤æ‚ï¼‰ | é«˜ï¼ˆè‡ªåŠ¨åŒ–å¤„ç†è½¬å½•æœ¬ï¼‰ |
| **æ´å¯Ÿæ·±åº¦** | åæ˜ å˜é‡é—´ç»Ÿè®¡å…³è” | å¯æ­ç¤ºæ½œåœ¨ä¸»é¢˜å’ŒåŠŸèƒ½å…³ç³» |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **æ¥æº**ï¼š77æ¬¡1å°æ—¶çš„æ²»ç–—ä¼šè¯è½¬å½•æœ¬ï¼Œæ¥è‡ª6åè¢«è¯Šæ–­ä¸º**é‡åº¦æŠ‘éƒéšœç¢**ï¼ˆMDD, n=1ï¼‰æˆ–**å¹¿æ³›æ€§ç„¦è™‘éšœç¢**ï¼ˆGAD, n=5ï¼‰çš„å‚ä¸è€…ã€‚
- **æ ‡æ³¨**ï¼šä¸“å®¶æ‰‹åŠ¨æ ‡æ³¨äº†3,364ä¸ªåŒ…å«å¿ƒç†è¿‡ç¨‹çš„å®¢æˆ·è¯è¯­ï¼ˆutterancesï¼‰ï¼Œæ¯ä¸ªè¯è¯­æ ‡æ³¨æ˜¯å¦åŒ…å«è¿‡ç¨‹åŠå¯¹åº”çš„EEMMç»´åº¦ã€‚
- **ç»´åº¦æ ‡ç­¾**ï¼šAffect, Cognition, Attention, Motivation, Sense of Self, Overt Behavior, Context/Moderators, Sociocultural, Biophysiologicalã€‚

### å®éªŒè®¾ç½®
#### æ¨¡å‹
- ä¸»è¦ä½¿ç”¨å¼€æºå¤§æ¨¡å‹ **LLaMA-3.1-70B-Instruct**ï¼ˆæœ¬åœ°è¿è¡Œï¼Œä¿éšœéšç§ï¼‰ã€‚
- åœ¨è¿æ¥ç”Ÿæˆé˜¶æ®µå¼•å…¥é—­æºæ¨¡å‹ï¼ˆå¦‚GPT-4o-miniï¼‰è¿›è¡Œé›†æˆå­¦ä¹ ã€‚

#### è¯„ä¼°ç­–ç•¥
é‡‡ç”¨åˆ†é˜¶æ®µè¯„ä¼°ï¼š
- **è¿‡ç¨‹æ£€æµ‹é˜¶æ®µ**ï¼šä½¿ç”¨æ ‡å‡†åˆ†ç±»æŒ‡æ ‡ï¼ˆF1, Precision, Recallï¼‰ä¸äººå·¥æ ‡æ³¨å¯¹æ¯”ã€‚
- **ä¸»é¢˜èšç±»ä¸è¿æ¥ç”Ÿæˆé˜¶æ®µ**ï¼šç”±ä¸‰ä½é¢†åŸŸä¸“å®¶è¿›è¡Œ**ç›²è¯„**ï¼ˆblind evaluationï¼‰ï¼Œè¯„ä¼°ç”Ÿæˆç½‘ç»œçš„è´¨é‡ã€‚

#### è¯„ä¼°æŒ‡æ ‡
- **è¿‡ç¨‹æ£€æµ‹**ï¼šF1 Score, Precision, Recallã€‚
- **ä¸»é¢˜è´¨é‡**ï¼ˆä¸“å®¶è¯„åˆ†ï¼‰ï¼š
  - **Insightfulness**ï¼šä¸´åºŠç›¸å…³æ€§ï¼ˆClinical Relevanceï¼‰ã€æ–°é¢–æ€§ï¼ˆNoveltyï¼‰ã€æœ‰ç”¨æ€§ï¼ˆUsefulnessï¼‰
  - **Trustworthiness**ï¼šç‰¹å¼‚æ€§ï¼ˆSpecificityï¼‰ã€è¦†ç›–ç‡ï¼ˆCoverageï¼‰ã€å®Œæ•´æ€§ï¼ˆCompletenessï¼‰ã€ä¾µå…¥æ€§ï¼ˆIntrusivenessï¼‰ã€å†—ä½™æ€§ï¼ˆRedundancyï¼‰
- **è¿æ¥è´¨é‡**ï¼šæ¸…æ™°åº¦ï¼ˆClarityï¼‰ã€è¿æ¥è´¨é‡ï¼ˆConnection Qualityï¼‰ã€æ²»ç–—æ´å¯ŸåŠ›ï¼ˆTherapeutic Insightï¼‰
- **æ€»ä½“æ¯”è¾ƒ**ï¼šç›´æ¥æç¤ºï¼ˆdirect promptingï¼‰ä½œä¸ºåŸºçº¿ï¼Œä¸å¤šæ­¥ç®¡é“å¯¹æ¯”ã€‚

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Baseline**ï¼šä½¿ç”¨â€œç›´æ¥æç¤ºâ€ï¼ˆdirect promptingï¼‰æ–¹æ³•ï¼Œå³ä¸€æ¬¡æ€§è®©LLMä»è½¬å½•æœ¬ç”Ÿæˆå®Œæ•´ç½‘ç»œï¼ˆä¸»é¢˜+è¿æ¥ï¼‰ï¼Œä¸ç»è¿‡ä¸­é—´æ­¥éª¤ã€‚
- **ç›®æ ‡**ï¼šéªŒè¯**å¤šæ­¥åˆ†è§£**ï¼ˆmulti-step decompositionï¼‰æ˜¯å¦ä¼˜äºç«¯åˆ°ç«¯ç”Ÿæˆã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### è¿‡ç¨‹æ£€æµ‹æ€§èƒ½
- åœ¨**few-shot**è®¾ç½®ä¸‹ï¼ˆK=100ï¼‰ï¼Œæ¨¡å‹åœ¨è¯†åˆ«å¿ƒç†è¿‡ç¨‹ä»»åŠ¡ä¸Šçš„**F1è¾¾åˆ°çº¦90%**ï¼Œè¡¨æ˜LLMèƒ½å‡†ç¡®ä»è¯è¯­ä¸­è¯†åˆ«ä¸´åºŠç›¸å…³å†…å®¹ã€‚
- å³ä½¿åœ¨å°‘é‡ç¤ºä¾‹ï¼ˆfew-shotï¼‰ä¸‹ï¼Œæ€§èƒ½ä¹Ÿæ˜¾è‘—ä¼˜äºzero-shotï¼Œè¯´æ˜**in-context learning**æœ‰æ•ˆã€‚

### ä¸»é¢˜èšç±»è´¨é‡ï¼ˆä¸“å®¶è¯„åˆ†ï¼‰
- å¤šæ­¥èšç±»æ–¹æ³•åœ¨æœ€ç»ˆè¿­ä»£ï¼ˆStage 3ï¼‰ä¸­å¾—åˆ†ï¼š
  - **ä¸´åºŠç›¸å…³æ€§**ï¼š72%
  - **æ–°é¢–æ€§**ï¼š75%
  - **æœ‰ç”¨æ€§**ï¼š74%
- æ˜¾è‘—ä¼˜äºå•æ­¥èšç±»æ–¹æ³•ï¼ˆåˆå§‹å¾—åˆ†åˆ†åˆ«ä¸º65%, 58%, 56%ï¼‰ã€‚

### è¿æ¥ç”Ÿæˆæ–¹æ³•æ¯”è¾ƒ
ä¸‰ç§é›†æˆç­–ç•¥æ¯”è¾ƒï¼ˆprompt-based, temperature-based, model-basedï¼‰ï¼š
- **Model-based ensemble**ï¼ˆä½¿ç”¨å¤šä¸ªä¸åŒLLMï¼‰è¡¨ç°æœ€ä½³ï¼Œåœ¨æ‰€æœ‰ä¸‰é¡¹æŒ‡æ ‡ï¼ˆClarity, Connection Quality, Therapeutic Insightï¼‰ä¸Šå‡è·å¾—æœ€å¤šä¸“å®¶åå¥½ï¼ˆçº¦77%ï¼‰ã€‚
- **Prompt-based ensemble**è¡¨ç°æœ€å·®ï¼Œè¯´æ˜ä»…é æç¤ºå˜ä½“å¤šæ ·æ€§ä¸è¶³ã€‚

### å¤šæ­¥ç®¡é“ vs. ç›´æ¥æç¤ºï¼ˆå…³é”®å¯¹æ¯”ï¼‰
| è¯„ä¼°ç»´åº¦ | å¤šæ­¥ç®¡é“åå¥½ç‡ | ç›´æ¥æç¤ºåå¥½ç‡ |
|---------|----------------|----------------|
| **æœ‰æ„ä¹‰çš„ä¸´åºŠä¸»é¢˜** | 89.4% | 10.6% |
| **ä¿¡æ¯ä¸°å¯Œçš„è¿æ¥** | 77.3% | 22.7% |
| **æ”¯æŒæ²»ç–—è§„åˆ’** | 92.4% | 7.6% |

> **ç»“è®º**ï¼šå¤šæ­¥ç®¡é“åœ¨æ‰€æœ‰æ–¹é¢æ˜¾è‘—ä¼˜äºç›´æ¥æç¤ºï¼Œä¸“å®¶é«˜è¾¾90%ä»¥ä¸Šåå¥½è¯¥æ–¹æ³•ã€‚

### æ¶ˆèå®éªŒï¼ˆé—´æ¥ä½“ç°ï¼‰
- **ä¸¤æ­¥èšç±» vs. å•æ­¥èšç±»**ï¼šä¸¤æ­¥æ³•æ›´å¯æ§ã€å¯è§£é‡Šï¼Œç”Ÿæˆä¸»é¢˜è´¨é‡æ›´é«˜ã€‚
- **é›†æˆå­¦ä¹  vs. å•ä¸€æ¨¡å‹**ï¼šmodel-based ensembleè¡¨ç°æœ€å¥½ï¼Œè¯´æ˜æ¨¡å‹å¤šæ ·æ€§æå‡å¯é æ€§ã€‚
- **æ¸©åº¦è®¾ç½®**ï¼šé™¤è¿æ¥ç”Ÿæˆå¤–ï¼Œå…¶ä½™é˜¶æ®µå‡è®¾temperature=0ä»¥ç¡®ä¿è¾“å‡ºç¡®å®šæ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **LLMå¯ç”¨äºä»æ²»ç–—è½¬å½•æœ¬ä¸­ç”Ÿæˆä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„ä¸ªæ€§åŒ–ç½‘ç»œ**ï¼Œæ— éœ€å¯†é›†çºµå‘æ•°æ®ã€‚
2. **å¤šæ­¥ç®¡é“è®¾è®¡è‡³å…³é‡è¦**ï¼šå°†ä»»åŠ¡åˆ†è§£ä¸ºè¿‡ç¨‹æ£€æµ‹ã€ä¸»é¢˜èšç±»ã€è¿æ¥ç”Ÿæˆä¸‰ä¸ªé˜¶æ®µï¼Œæ˜¾è‘—ä¼˜äºç«¯åˆ°ç«¯çš„ç›´æ¥æç¤ºæ–¹æ³•ã€‚
3. **ç”Ÿæˆçš„ç½‘ç»œå…·æœ‰é«˜ä¸´åºŠä»·å€¼**ï¼šä¸“å®¶è¯„ä»·å…¶åœ¨ä¸´åºŠç›¸å…³æ€§ã€æ–°é¢–æ€§å’Œæœ‰ç”¨æ€§æ–¹é¢å¾—åˆ†è¾¾72â€“75%ï¼Œå¯ç”¨äºæ¡ˆä¾‹æ¦‚å¿µåŒ–å’Œæ²»ç–—è§„åˆ’ã€‚
4. **é›†æˆå­¦ä¹ æå‡è¿æ¥è´¨é‡**ï¼šç‰¹åˆ«æ˜¯ä½¿ç”¨å¤šä¸ªä¸åŒLLMçš„model-based ensembleæ•ˆæœæœ€ä½³ã€‚
5. **æ–¹æ³•å…·å¤‡å¯æ‰©å±•æ€§ä¸å®ç”¨æ€§**ï¼šè¾“å…¥ä»…ä¸ºè½¬å½•æœ¬ï¼Œè¾“å‡ºä¸ºå¯è§†åŒ–ç½‘ç»œï¼Œé€‚åˆæ•´åˆåˆ°ä¸´åºŠå·¥ä½œæµä¸­ã€‚

### å±€é™æ€§
1. **æ¨¡å‹å•ä¸€æ€§**ï¼šä¸»è¦ä¾èµ–LLaMA-3.1ï¼Œæœªç³»ç»Ÿæ¯”è¾ƒå¤šç§LLMã€‚
2. **æ•°æ®è§„æ¨¡å°**ï¼šä»…6åå‚ä¸è€…ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰å¾…éªŒè¯ã€‚
3. **ç¼ºä¹éè¯­è¨€ä¿¡æ¯**ï¼šä»…åŸºäºæ–‡æœ¬ï¼Œå¿½ç•¥äº†æ²»ç–—ä¸­çš„éè¨€è¯­è¡Œä¸ºï¼ˆå¦‚è¯­è°ƒã€è¡¨æƒ…ï¼‰ã€‚
4. **äººç±»è¯„ä¼°ä¸»è§‚æ€§å¼º**ï¼šéƒ¨åˆ†æŒ‡æ ‡ï¼ˆå¦‚æ–°é¢–æ€§ã€æ²»ç–—æ´å¯Ÿï¼‰ä¸“å®¶é—´ä¸€è‡´æ€§è¾ƒä½ï¼ˆCohenâ€™s kappa < 0.3ï¼‰ã€‚
5. **æœªä¸äººç±»ç”Ÿæˆç½‘ç»œå¯¹æ¯”**ï¼šè™½å› è®¤çŸ¥è´Ÿè·é«˜è€Œæœªå®æ–½ï¼Œä½†ä»ç¼ºå°‘ä¸â€œé»„é‡‘æ ‡å‡†â€çš„ç›´æ¥æ¯”è¾ƒã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **åŠ¨æ€ç½‘ç»œæ„å»º**ï¼šæ•´åˆå¤šèŠ‚æ²»ç–—ä¼šè¯ï¼Œè¿½è¸ªå¿ƒç†å˜åŒ–è½¨è¿¹ã€‚
2. **æ²»ç–—å¸ˆç½‘ç»œç”Ÿæˆ**ï¼šåˆ†ææ²»ç–—å¸ˆå¹²é¢„ç­–ç•¥ï¼Œç”¨äºåŸ¹è®­ä¸ç£å¯¼ã€‚
3. **æ‚£è€…-æ²»ç–—å¸ˆç½‘ç»œèåˆ**ï¼šæ„å»ºäº’åŠ¨è§†è§’ä¸‹çš„è”åˆç½‘ç»œã€‚
4. **ç–—æ•ˆéªŒè¯ç ”ç©¶**ï¼šå¼€å±•ä¸´åºŠè¯•éªŒï¼Œæ£€éªŒLLMç”Ÿæˆç½‘ç»œæ˜¯å¦èƒ½çœŸæ­£æ”¹å–„æ²»ç–—ç»“å±€ï¼ˆtreatment utilityï¼‰ã€‚
5. **ä¸å…¶ä»–æ–¹æ³•å¯¹æ¯”**ï¼šå¦‚ä¸åŸºäºEMAæ•°æ®çš„ç»Ÿè®¡ç½‘ç»œæˆ–äººç±»å°è±¡ç”Ÿæˆçš„ç½‘ç»œè¿›è¡Œæ€§èƒ½æ¯”è¾ƒã€‚

---

> **æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ª**å¯è¡Œä¸”é«˜æ•ˆçš„LLMé©±åŠ¨æ¡†æ¶**ï¼Œå®ç°äº†ä»æ²»ç–—å¯¹è¯ä¸­è‡ªåŠ¨æ„å»ºä¸ªæ€§åŒ–å¿ƒç†ç½‘ç»œï¼Œä¸º**è¿‡ç¨‹å¯¼å‘ç–—æ³•**ï¼ˆPBTï¼‰å’Œ**ç²¾å‡†å¿ƒç†å¥åº·**ï¼ˆprecision mental healthï¼‰æä¾›äº†å¯æ‰©å±•çš„å·¥å…·ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨æ½œåŠ›ã€‚

</details>

---

### 7. [Learning from Self Critique and Refinement for Faithful LLM Summarization](https://arxiv.org/abs/2512.05387)

**Authors**: Ting-Yao Hu, Hema Swetha Koppula, Hadi Pouransari, Cem Koc, Oncel Tuzel, Raviteja Vemulapalli  
**Category**: cs.CL  
**Published**: 2025-12-08  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.05387v1  

#### Abstract
Large Language Models (LLMs) often suffer from hallucinations: output content that is not grounded in the input context, when performing long-form text generation tasks such as summarization. Prior works have shown that hallucinations can be reduced by iteratively critiquing and refining previously ...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šLearning from Self Critique and Refinement for Faithful LLM Summarization

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ‰§è¡Œé•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ç”Ÿæˆï¼‰æ—¶æ™®éå­˜åœ¨**å¹»è§‰ï¼ˆhallucinationsï¼‰**é—®é¢˜ï¼Œå³ç”Ÿæˆçš„å†…å®¹æ— æ³•åœ¨è¾“å…¥ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°æ”¯æŒã€‚å°½ç®¡å·²æœ‰ç ”ç©¶å°è¯•é€šè¿‡æ¨ç†é˜¶æ®µçš„è¿­ä»£æ‰¹è¯„ä¸ä¿®æ­£ï¼ˆcritique and refinementï¼‰æ¥ç¼“è§£è¯¥é—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–æ›´å¼ºçš„æ•™å¸ˆæ¨¡å‹æˆ–å¤šæ¨¡å‹åä½œæ¡†æ¶ï¼Œå¹¶å¼•å…¥é¢å¤–çš„**æ¨ç†æ—¶è®¡ç®—å¼€é”€ï¼ˆtest-time computeï¼‰**ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚

### æå‡ºçš„æ–°æ–¹æ³•ï¼šSCRPO
æœ¬æ–‡æå‡º **Self Critique and Refinement-based Preference Optimization (SCRPO)**ï¼Œä¸€ç§**è‡ªç›‘ç£è®­ç»ƒæ¡†æ¶**ï¼Œåˆ©ç”¨åŒä¸€ä¸ªLLMè‡ªèº«çš„æ‰¹è¯„ä¸ä¿®æ­£èƒ½åŠ›ï¼Œåœ¨è®­ç»ƒé˜¶æ®µæå‡å…¶æ‘˜è¦ç”Ÿæˆçš„å¿ å®åº¦ï¼ˆfaithfulnessï¼‰ï¼Œè€Œæ— éœ€å¤–éƒ¨ç›‘ç£æˆ–æ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ã€‚

#### æ–¹æ³•æµç¨‹ï¼š
1. å¯¹æ— æ ‡ç­¾æ–‡æ¡£ç”Ÿæˆåˆå§‹æ‘˜è¦ï¼ˆinitial summariesï¼‰ï¼›
2. åŒä¸€ä¸ªLLMå¯¹è¿™äº›æ‘˜è¦è¿›è¡Œ**è‡ªæˆ‘æ‰¹è¯„ï¼ˆself-critiqueï¼‰**ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨å¹»è§‰ï¼›
3. åŸºäºæ‰¹è¯„åé¦ˆï¼ŒåŒä¸€æ¨¡å‹å¯¹ä¸å¿ å®åœ°æ‘˜è¦è¿›è¡Œ**è‡ªæˆ‘ä¿®æ­£ï¼ˆself-refinementï¼‰**ï¼›
4. æ„å»ºåå¥½æ•°æ®é›†ï¼šå°†ä¿®æ­£åçš„æ‘˜è¦ä½œä¸º `chosen`ï¼ŒåŸå§‹ä¸å¿ å®åœ°æ‘˜è¦ä½œä¸º `rejected`ï¼›
5. ä½¿ç”¨ **Preference Learning**ï¼ˆåŸºäºDPO+NLLæ­£åˆ™åŒ–ï¼‰å¯¹è¯¥LLMè¿›è¡Œå¾®è°ƒã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚Test-Time Refinementï¼‰ | SCRPO |
|------|-------------------------------|--------|
| æ˜¯å¦éœ€è¦æ•™å¸ˆæ¨¡å‹ | æ˜¯ï¼ˆæˆ–ä¸“ç”¨åˆ¤åˆ«æ¨¡å‹ï¼‰ | å¦ï¼ˆä»…ç”¨è‡ªèº«ï¼‰ |
| æ¨ç†æ—¶è®¡ç®—æˆæœ¬ | é«˜ï¼ˆéœ€å¤šæ¬¡è°ƒç”¨ï¼‰ | æ— é¢å¤–å¼€é”€ï¼ˆè®­ç»ƒåå³å›ºåŒ–ï¼‰ |
| å¯æ‰©å±•æ€§ | ä½ï¼ˆä¾èµ–å¤šæ¨¡å‹/å¼ºæ¨¡å‹ï¼‰ | é«˜ï¼ˆå•æ¨¡å‹é—­ç¯ï¼‰ |
| æ€§èƒ½è¡¨ç° | ä¸­ç­‰ | æ›´ä¼˜ï¼ˆè§å®éªŒéƒ¨åˆ†ï¼‰ |

> âœ… **æ ¸å¿ƒåˆ›æ–°**ï¼šé¦–æ¬¡å°†â€œè‡ªæˆ‘æ‰¹è¯„+è‡ªæˆ‘ä¿®æ­£â€æœºåˆ¶ç”¨äºæ„å»ºåå¥½æ•°æ®ï¼Œå¹¶é€šè¿‡åå¥½å­¦ä¹ å®ç°**è®­ç»ƒæ—¶çŸ¥è¯†è’¸é¦**ï¼Œä½¿æ¨¡å‹å†…åŒ–å¿ å®æ€§åˆ¤æ–­èƒ½åŠ›ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
åœ¨ä¸‰ä¸ªæ ‡å‡†æ‘˜è¦åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼š
- **XSum**ï¼šæ–°é—»æ‘˜è¦ï¼Œå¼ºè°ƒç®€æ´æ€§å’Œäº‹å®å‡†ç¡®æ€§ï¼›
- **CNNDM**ï¼šæ–°é—»æ–‡ç« æ‘˜è¦ï¼›
- **SAMSum**ï¼šæ—¥å¸¸å¯¹è¯æ‘˜è¦ï¼Œæ›´å…·å£è¯­åŒ–æŒ‘æˆ˜ã€‚

ä»å„æ•°æ®é›†è®­ç»ƒé›†ä¸­é‡‡æ ·10,000ä¸ªæ–‡æ¡£ç”¨äºSCRPOè®­ç»ƒã€‚

### å®éªŒè®¾ç½®
- **åŸºç¡€æ¨¡å‹**ï¼šQwen2.5-7B-Instruct
- **é€‚é…å™¨æŠ€æœ¯**ï¼šLoRAï¼ˆrank=16, alpha=32ï¼‰
- **ç”Ÿæˆæ–¹å¼**ï¼šBeam Searchï¼ˆbeam size=5ï¼‰
- **æç¤ºå·¥ç¨‹**ï¼šè®¾è®¡äº†å¤šä¸ªpromptæ¨¡æ¿ç”¨äºsummarizationã€critiqueã€fact extractionã€NLIã€refinementç­‰æ­¥éª¤ï¼ˆè¯¦è§Appendix A.1ï¼‰

### è¯„ä¼°æŒ‡æ ‡
| ç±»å‹ | æŒ‡æ ‡ | æè¿° |
|------|------|------|
| **å¿ å®æ€§ï¼ˆFaithfulnessï¼‰** | MiniCheck | è‡ªåŠ¨åŒ–äº‹å®æ ¸æŸ¥å·¥å…·ï¼Œè¡¡é‡æ‘˜è¦æ˜¯å¦å¯è¢«æºæ–‡æ¡£æ”¯æŒ |
| | GPT4-Likert | GPT-4äººå·¥è¯„åˆ†æ¨¡æ‹Ÿï¼ŒLikerté‡è¡¨è¯„ä¼°å¿ å®æ€§ |
| **æ•´ä½“è´¨é‡** | GEvalç³»åˆ— | åŒ…æ‹¬Coherenceï¼ˆè¿è´¯æ€§ï¼‰ã€Consistencyï¼ˆä¸€è‡´æ€§ï¼‰ã€Fluencyï¼ˆæµç•…æ€§ï¼‰ã€Relevanceï¼ˆç›¸å…³æ€§ï¼‰å››ä¸ªç»´åº¦ï¼ŒåŸºäºGPT-4è¯„ä¼° |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **Pretrained LLM**ï¼šæœªç»å¾®è°ƒçš„åŸå§‹æ¨¡å‹
- **MPO (Choi et al., 2024)**ï¼šåŸºäºä¸åŒè§£ç ç­–ç•¥ç”Ÿæˆåå¥½å¯¹çš„æ–¹æ³•
- **SCOPE (Duong et al., 2025)**ï¼šä½¿ç”¨context-lessæ¨¡å‹ç”Ÿæˆä¸å¿ å®åœ°å“åº”æ„å»ºåå¥½æ•°æ®
- **SCRPO variants**ï¼š
  - SCRPO-Inference Timeï¼šç›´æ¥åœ¨æ¨ç†æ—¶æ‰§è¡Œcritique & refine
  - SCRPO-Critique Onlyï¼šä»…ç”¨æ‰¹è¯„æ‰“åˆ†æ„é€ åå¥½ï¼Œæ— ä¿®æ­£
  - SCRPO-SFTï¼šä»…å¯¹ä¿®æ­£åæ‘˜è¦åšSFTï¼Œéåå¥½å­¦ä¹ 

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆæ¥è‡ªTable 3ï¼‰

| æ–¹æ³• | XSum MiniCheck | XSum GPT4-Likert | CNNDM MiniCheck | SAMSum MiniCheck |
|------|----------------|------------------|------------------|------------------|
| Pretrained LLM | 0.701 | 4.16 | 0.715 | 0.437 |
| MPO | 0.694 | 4.13 | 0.712 | 0.456 |
| SCOPE | 0.713 | 4.14 | 0.721 | 0.440 |
| SCRPO-Inference Time | 0.722 | 4.23 | 0.746 | 0.470 |
| SCRPO (Ours) | **0.761** | **4.38** | **0.806** | **0.523** |

> ğŸ” æ‰€æœ‰æ•°æ®é›†ä¸Šï¼ŒSCRPOå‡æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
- **ç›¸æ¯”MPO/SCOPE**ï¼šä¸¤è€…æœªèƒ½ç¨³å®šè¶…è¶Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯´æ˜å…¶åå¥½æ„é€ æœªæœ‰æ•ˆæ•æ‰å¿ å®æ€§ä¿¡å·ï¼›è€ŒSCRPOé€šè¿‡æ˜¾å¼çš„critique+refinementæœºåˆ¶ï¼Œæ˜¾è‘—æå‡MiniCheckå’ŒGPT4-Likertå¾—åˆ†ã€‚
- **ç›¸æ¯”Inference-Time Refinement**ï¼šè™½ç„¶ä¹Ÿèƒ½æå‡å¿ å®æ€§ï¼Œä½†SCRPOä¸ä»…æ•ˆæœæ›´å¥½ï¼Œä¸”**ä¸å¢åŠ æ¨ç†å¼€é”€**ï¼Œæ•ˆç‡æ›´é«˜ã€‚
- **è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›**ï¼ˆTable 4ï¼‰ï¼š
  - å³ä½¿åœ¨ç›®æ ‡åŸŸå¤–è®­ç»ƒï¼ˆå¦‚ç”¨CNNDMè®­ç»ƒã€åœ¨XSumæµ‹è¯•ï¼‰ï¼ŒSCRPOä»ä¼˜äºé¢„è®­ç»ƒæ¨¡å‹ï¼›
  - åœ¨XSumâ†’SAMSumè¿™ç§å¤§é¢†åŸŸè¿ç§»ä¸‹ï¼Œä¾ç„¶ä¿æŒè½»å¾®ä¼˜åŠ¿ï¼Œæ˜¾ç¤ºè‰¯å¥½é²æ£’æ€§ã€‚

### æ¶ˆèå®éªŒç»“æœï¼ˆTable 3 & Table 5ï¼‰

#### ä¸åŒç»„ä»¶çš„å½±å“ï¼ˆSCRPOå˜ä½“ï¼‰ï¼š
| å˜ä½“ | MiniCheck (XSum) | GPT4-Likert (XSum) | ç»“è®º |
|------|------------------|--------------------|------|
| SCRPO-Critique Only | 0.738 | 4.33 | ä»…æœ‰æ‰¹è¯„ä¸è¶³ä»¥æœ€å¤§åŒ–æå‡ |
| SCRPO-SFT | 0.735 | 4.23 | ä»…SFTä¸å¦‚åå¥½å­¦ä¹ æœ‰æ•ˆ |
| SCRPO (å®Œæ•´) | **0.761** | **4.38** | **æ‰¹è¯„+ä¿®æ­£+åå¥½å­¦ä¹ ç¼ºä¸€ä¸å¯** |

#### æ‰¹è¯„ç­–ç•¥æ¯”è¾ƒï¼ˆTable 1ï¼‰
- **Binary Feedback** vs **Fine-grained Feedback**
  - åè€…ï¼ˆç»†ç²’åº¦åé¦ˆï¼‰æ€§èƒ½æ›´ä¼˜ï¼Œå› å…¶åˆ†è§£ä¸ºï¼š
    1. Atomic Fact Extraction
    2. NLI åˆ¤æ–­æ¯ä¸ªäº‹å®æ˜¯å¦entailed
    3. èšåˆéentailedäº‹å®ä½œä¸ºåé¦ˆ
  - æ›´ç²¾ç»†çš„é”™è¯¯å®šä½å¸¦æ¥æ›´é«˜è´¨é‡çš„ä¿®æ­£å’Œåå¥½æ•°æ®ã€‚

#### åå¥½ä¸‰å…ƒç»„é€‰æ‹©ç­–ç•¥ï¼ˆTable 2ï¼‰
- **Extreme Selection**ï¼ˆæœ€å·®åˆå§‹ + æœ€ä½³ä¿®æ­£ï¼‰æ•ˆæœæœ€ä½³ï¼Œå¹³è¡¡äº†å¿ å®æ€§ä¸æ•´ä½“è´¨é‡ï¼›
- Beam Searchè™½MiniChecké«˜ï¼Œä½†GEvalå¾—åˆ†ä¸‹é™æ˜æ˜¾ï¼Œå½±å“å®ç”¨æ€§ã€‚

#### äººç±»è¯„ä¼°ç»“æœï¼ˆTable 5ï¼‰
| è¯„ä¼°ç»´åº¦ | SCRPOèƒœå‡º | å¹³å±€ | Pretrainedèƒœå‡º |
|----------|-----------|-------|----------------|
| å¿ å®æ€§ï¼ˆFaithfulnessï¼‰ | **24%** | 75% | 1% |
| æ•´ä½“è´¨é‡ï¼ˆGeneral Qualityï¼‰ | 31% | 41% | 28% |

> ğŸ‘¥ äººç±»è¯„ä¼°è¯å®SCRPOæ˜¾è‘—æå‡å¿ å®æ€§ï¼ŒåŒæ—¶æ•´ä½“è´¨é‡ç›¸å½“ç”šè‡³ç•¥ä¼˜ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **SCRPOèƒ½æœ‰æ•ˆæå‡LLMæ‘˜è¦çš„å¿ å®æ€§**ï¼šåœ¨XSumã€CNNDMã€SAMSumä¸‰å¤§åŸºå‡†ä¸Šå…¨é¢è¶…è¶ŠSOTAè‡ªç›‘ç£æ–¹æ³•ã€‚
2. âœ… **è®­ç»ƒæ—¶è’¸é¦ä¼˜äºæ¨ç†æ—¶ä¿®æ­£**ï¼šSCRPOä¸ä»…æ€§èƒ½æ›´å¼ºï¼Œè€Œä¸”æ¨ç†é›¶å¼€é”€ï¼Œæ›´é€‚åˆéƒ¨ç½²ã€‚
3. âœ… **ç»†ç²’åº¦æ‰¹è¯„æœºåˆ¶æ›´æœ‰æ•ˆ**ï¼šåŸºäºatomic fact extraction + NLIçš„åé¦ˆæ¯”äºŒå€¼åˆ¤æ–­æ›´èƒ½å¼•å¯¼é«˜è´¨é‡ä¿®æ­£ã€‚
4. âœ… **åå¥½å­¦ä¹ ä¼˜äºSFT**ï¼šç›´æ¥å­¦ä¹ â€œä»€ä¹ˆæ˜¯æ›´å¥½çš„æ‘˜è¦â€æ¯”å•çº¯æ¨¡ä»¿ä¿®æ­£ç»“æœæ›´æœ‰æ•ˆã€‚
5. âœ… **å…·å¤‡è·¨åŸŸæ³›åŒ–èƒ½åŠ›**ï¼šå³ä½¿è®­ç»ƒä¸æµ‹è¯•é¢†åŸŸä¸åŒï¼Œä»èƒ½å¸¦æ¥å¢ç›Šã€‚

### æ–¹æ³•çš„å±€é™æ€§
- â— **æ¨¡å‹å®¹é‡è¦æ±‚è¾ƒé«˜**ï¼šå®éªŒè¡¨æ˜å°äº3Bå‚æ•°çš„æ¨¡å‹ï¼ˆå¦‚0.5Bã€1.5Bï¼‰åœ¨SCRPOä¸‹å‡ºç°æ€§èƒ½é€€åŒ–ï¼ˆFigure 3ï¼‰ï¼Œè¯´æ˜è¯¥æ–¹æ³•ä¾èµ–è¾ƒå¼ºçš„å†…éƒ¨æ¨ç†èƒ½åŠ›ã€‚
- â— **ä¾èµ–é«˜è´¨é‡promptè®¾è®¡**ï¼šcritiqueä¸refinementçš„æ•ˆæœé«˜åº¦ä¾èµ–promptçš„è´¨é‡ï¼Œå­˜åœ¨å·¥ç¨‹è°ƒå‚è´Ÿæ‹…ã€‚
- â— **æœªè§£å†³æ ¹æœ¬å¹»è§‰æˆå› **ï¼šä»æ˜¯åå¤„ç†å¼æ”¹è¿›ï¼Œæœªè§¦åŠLLMç”Ÿæˆæœºåˆ¶æœ¬èº«çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
- å°†SCRPOåº”ç”¨äºå…¶ä»–æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚é—®ç­”ã€å¯¹è¯ç”Ÿæˆï¼‰ï¼›
- æ¢ç´¢è½»é‡åŒ–ç‰ˆæœ¬ä»¥é€‚é…å°æ¨¡å‹ï¼›
- ç»“åˆè§£ç æ—¶çº¦æŸï¼ˆå¦‚constrained decodingï¼‰è¿›ä¸€æ­¥å¢å¼ºå¿ å®æ€§ï¼›
- ç ”ç©¶å¦‚ä½•è‡ªåŠ¨åŒ–ä¼˜åŒ–critiqueä¸refinementçš„promptæ¨¡æ¿ã€‚

---

> ğŸ“Œ **æ€»ç»“ä¸€å¥è¯**ï¼š  
> SCRPOé€šè¿‡è®©LLMâ€œè‡ªå·±æ‰¹è¯„è‡ªå·±å¹¶æ”¹è¿›â€ï¼Œåœ¨è®­ç»ƒä¸­æ„å»ºé«˜è´¨é‡åå¥½æ•°æ®ï¼Œå®ç°äº†**æ— éœ€æ•™å¸ˆæ¨¡å‹ã€æ— æ¨ç†å¼€é”€ã€é«˜å¿ å®åº¦**çš„æ‘˜è¦ç”Ÿæˆæ–°èŒƒå¼ï¼Œæ˜¯è¿ˆå‘å¯ä¿¡LLMçš„é‡è¦ä¸€æ­¥ã€‚

</details>

---

### 8. [IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection](https://arxiv.org/abs/2512.05531)

**Authors**: Yang Xu, Yixiao Ma, Kaifeng Zhang, Zuliang Yang, Kai Ming Ting  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.05531v1  

#### Abstract
Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathb...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šIDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡é’ˆå¯¹**æ•°æ®æµç¯å¢ƒä¸‹çš„å¼‚å¸¸æ£€æµ‹**ï¼ˆStreaming Anomaly Detectionï¼‰ä¸­å­˜åœ¨çš„ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š
- **æ¦‚å¿µæ¼‚ç§»**ï¼ˆConcept Driftï¼‰ï¼šæ•°æ®åˆ†å¸ƒéšæ—¶é—´å˜åŒ–ï¼Œä¼ ç»Ÿé™æ€æ¨¡å‹å¤±æ•ˆï¼›
- **è®¡ç®—æ•ˆç‡ç“¶é¢ˆ**ï¼šç°æœ‰æ–¹æ³•è‹¥é‡‡ç”¨å…¨é‡é‡è®­ç»ƒï¼ˆretrainingï¼‰ç­–ç•¥ï¼Œè®¡ç®—å¼€é”€å¤§ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚

è®¸å¤šç°æœ‰æµå¼å¼‚å¸¸æ£€æµ‹å™¨åŸºäºè¾ƒå¼±çš„ç¦»çº¿åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ iForestã€LOFï¼‰ï¼Œå…¶æ€§èƒ½ä¸Šé™å—é™äºè¿™äº›åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡º **IDK-S**ï¼ˆIncremental Distributional Kernel for Streaming anomaly detectionï¼‰ï¼Œä¸€ç§é¢å‘æ•°æ®æµçš„å¢é‡å¼åˆ†å¸ƒæ ¸æ–¹æ³•ï¼Œæ ¸å¿ƒåˆ›æ–°å¦‚ä¸‹ï¼š

#### ï¼ˆ1ï¼‰ç»§æ‰¿å¹¶æ”¹è¿›å¼ºå¤§çš„ç¦»çº¿æ£€æµ‹å™¨ IDK
- å°†è¡¨ç°ä¼˜å¼‚çš„ç¦»çº¿æ£€æµ‹å™¨ **Isolation Distributional Kernel (IDK)** æˆåŠŸé€‚é…åˆ°æµå¼åœºæ™¯ã€‚
- IDK åŸºäº **kernel mean embedding (KME)** æ¡†æ¶ï¼Œåˆ©ç”¨**æ•°æ®ä¾èµ–å‹æ ¸å‡½æ•°**ï¼ˆdata-dependent kernelï¼‰ï¼Œåœ¨å¯†åº¦ä¸å‡å’Œå±€éƒ¨å¼‚å¸¸æ£€æµ‹ä¸Šä¼˜äº iForestã€LOF ç­‰ç»å…¸æ–¹æ³•ã€‚

#### ï¼ˆ2ï¼‰è®¾è®¡è½»é‡çº§å¢é‡æ›´æ–°æœºåˆ¶
- é¿å…å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œæ˜‚è´µçš„å‘¨æœŸæ€§é‡è®­ç»ƒï¼›
- æå‡º**å¢é‡åˆ†åŒºæ›¿æ¢æœºåˆ¶**ï¼ˆincremental partition-update mechanismï¼‰ï¼š
  - å½“æ»‘åŠ¨çª—å£ç§»åŠ¨æ—¶ï¼Œä»…ç§»é™¤æ¥è‡ªè¿‡æœŸæ•°æ®æ‰¹æ¬¡çš„é‡‡æ ·ç‚¹åŠå…¶å¯¹åº”çš„ hypersphere åˆ†åŒºï¼›
  - ç”¨æ–°åˆ°è¾¾æ•°æ®ä¸­çš„æ ·æœ¬ç”Ÿæˆæ–°çš„åˆ†åŒºè¿›è¡Œæ›¿æ¢ï¼›
  - ç»´æŠ¤ç‰¹å¾æ˜ å°„æ€»å’Œï¼ˆrunning sum of feature vectorsï¼‰ä»¥é«˜æ•ˆæ›´æ–°å‡å€¼æ˜ å°„ã€‚

#### ï¼ˆ3ï¼‰ç†è®ºä¿éšœï¼šç»Ÿè®¡ç­‰ä»·æ€§
- **Proposition 1** è¯æ˜ï¼šIDK-S çš„å¢é‡æ›´æ–°ç­–ç•¥åœ¨ä»»æ„æ—¶åˆ»ç”Ÿæˆçš„æ ·æœ¬é›†åˆçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä¸å®Œå…¨é‡è®­ç»ƒç‰ˆæœ¬çš„ IDK å®Œå…¨ä¸€è‡´ã€‚
- è¿™æ„å‘³ç€å…¶æ•ˆç‡æå‡**æœªç‰ºç‰²æ¨¡å‹çš„ç»Ÿè®¡æœ‰æ•ˆæ€§**ï¼ŒåŒºåˆ«äºè®¸å¤šå¯å‘å¼è¿‘ä¼¼æ–¹æ³•ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | IDK-S çš„ä¼˜åŠ¿ |
|------|-------------|
| **å‡†ç¡®æ€§** | åŸºäºæ›´å¼ºçš„ IDK æ ¸å¿ƒï¼Œåœ¨å¤šæ•°æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºåŸºäº iForest/LOF/kNN çš„æµå¼æ–¹æ³• |
| **æ•ˆç‡** | æ—¶é—´å¤æ‚åº¦ä» $O(wbt)$ é™è‡³ $O(lwt)$ï¼Œå…¶ä¸­ $l \ll w$ï¼Œå®ç°æ•°é‡çº§åŠ é€Ÿ |
| **å†…å­˜å‹å¥½** | ç©ºé—´å¤æ‚åº¦ä¸º $O(wd + btd + wbt)$ï¼Œä»…ä¾èµ–çª—å£å¤§å°ï¼Œé€‚ç”¨äºæ— é™æµ |
| **é€‚åº”æ€§** | èƒ½æœ‰æ•ˆè·Ÿè¸ªéå¹³ç¨³åˆ†å¸ƒï¼ˆå°¤å…¶æ˜¯æ¸å˜å‹æ¦‚å¿µæ¼‚ç§»ï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
å…±ä½¿ç”¨ **13 ä¸ªåŸºå‡†æ•°æ®é›†**ï¼Œæ¶µç›–ï¼š
- **11 ä¸ªå¤§è§„æ¨¡é™æ€å¼‚å¸¸æ£€æµ‹æ•°æ®é›†**ï¼ˆç»éšæœºæ‰“ä¹±æ¨¡æ‹Ÿæµå¼è¾“å…¥ï¼‰ï¼š
  - åŒ…æ‹¬ `Donors`, `Http`, `ForestCover`, `Fraud`, `Shuttle` ç­‰ï¼›
  - æ•°æ®è§„æ¨¡ä»æ•°åƒåˆ°ç™¾ä¸‡çº§ï¼ˆæœ€å¤§è¾¾ 619k+ å®ä¾‹ï¼‰ï¼Œç»´åº¦ 3â€“48ï¼Œå¼‚å¸¸æ¯”ä¾‹ 0.03%â€“32%ã€‚
- **2 ä¸ªéå¹³ç¨³ï¼ˆnon-stationaryï¼‰çœŸå®æµæ•°æ®é›†**ï¼š
  - `INSECTS`ï¼šæ˜†è™«é£è¡Œè¡Œä¸ºå—æ¸©åº¦å½±å“å¯¼è‡´çªå˜å‹æ¦‚å¿µæ¼‚ç§»ï¼ˆabrupt driftï¼‰ï¼›
  - `TwoCluster`ï¼šåˆæˆæ•°æ®ï¼Œå±•ç¤ºæ¸è¿›å¼åˆ†å¸ƒæ¼‚ç§»ï¼ˆgradual driftï¼‰ã€‚

> æ•°æ®é›†è¯¦æƒ…è§ Table 2ã€‚

---

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

| è®¾ç½®é¡¹ | æè¿° |
|--------|------|
| **çª—å£å¤§å° $w$** | å›ºå®šä¸º 2048 |
| **æ›´æ–°æ­¥é•¿ $l$** | å›ºå®šä¸º 100 |
| **é›†æˆæ•° $t$** | æ‰€æœ‰é›†æˆæ–¹æ³•è®¾ä¸º 100ï¼ˆå« IDK-Sã€SiForestã€oIFORï¼‰ |
| **IDK-S å‚æ•° $\nu$** | åœ¨ $[2^1, ..., 2^6]$ èŒƒå›´å†…æœç´¢æœ€ä¼˜å€¼ |
| **è¿è¡Œæ¬¡æ•°** | æ¯ä¸ªæ–¹æ³•åœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šæ‰§è¡Œ 20 æ¬¡å–å¹³å‡ |
| **ç¡¬ä»¶å¹³å°** | AMD 128æ ¸ CPU @2GHzï¼Œ1TB RAMï¼ˆæ‰€æœ‰æ–¹æ³•ç»Ÿä¸€ï¼‰ |
| **GPU ä½¿ç”¨æƒ…å†µ** | ä»… Memstream ä½¿ç”¨ GPUï¼Œå…¶ä½™å‡ä¸º CPU è¿è¡Œ |

#### è¯„ä¼°æŒ‡æ ‡
- **æ£€æµ‹æ•ˆæœ**ï¼šROC AUC Scoreï¼ˆArea Under the Curveï¼‰
- **è¿è¡Œæ•ˆç‡**ï¼šæ€»è¿è¡Œæ—¶é—´ï¼ˆsecondsï¼‰
- **åŠ¨æ€æ€§èƒ½åˆ†æ**ï¼šåœ¨éå¹³ç¨³æ•°æ®ä¸Šç»˜åˆ¶æ»‘åŠ¨çª—å£å†…çš„ç¬æ—¶ AUC æ›²çº¿

---

### å¯¹æ¯”çš„åŸºçº¿æ–¹æ³•ï¼ˆBaseline Methodsï¼‰
ä»£è¡¨å½“å‰ä¸»æµç ”ç©¶æ–¹å‘ï¼Œæ‰©å±•ä¸åŒåŸºç¡€æ¨¡å‹è‡³æµå¼åœºæ™¯ï¼š

| æ–¹æ³• | ç±»å‹ | åŸºç¡€æ¨¡å‹ |
|------|------|---------|
| **SiForest** | æµå¼æ–¹æ³• | Isolation Forestï¼ˆåŸºäº reservoir samplingï¼‰ |
| **oIFOR** | æµå¼æ–¹æ³• | Isolation Forestï¼ˆåŸºäºå¤šåˆ†è¾¨ç‡ç›´æ–¹å›¾ï¼‰ |
| **DILOF** | æµå¼æ–¹æ³• | Local Outlier Factorï¼ˆLOFï¼‰ |
| **CPOD** | æµå¼æ–¹æ³• | Distance-based Outlier Detection |
| **Memstream** | æµå¼æ–¹æ³• | Deep Autoencoder + kNNï¼ˆå”¯ä¸€ä½¿ç”¨ GPUï¼‰ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 3ï¼‰

| æŒ‡æ ‡ | IDK-S è¡¨ç° |
|------|----------|
| **å¹³å‡ AUC** | **0.876**ï¼ˆæ‰€æœ‰æ–¹æ³•ä¸­æœ€é«˜ï¼‰ |
| **æ¬¡ä¼˜æ–¹æ³•** | SiForestï¼ˆ0.853ï¼‰ï¼Œå·®è·è¾¾ **2.2% ç»å¯¹æå‡** |
| **æœ€å·®æ–¹æ³•** | DILOF å’Œ CPODï¼ˆçº¦ 0.73ï¼‰ |
| **æœ€å¿«è¿è¡Œæ—¶é—´** | å¹³å‡ **27 ç§’**ï¼ˆè¿œä½äºå…¶ä»–æ–¹æ³•ï¼‰ |
| **æœ€æ…¢æ–¹æ³•** | CPODï¼ˆ1916 ç§’ï¼‰ã€DILOFï¼ˆ781 ç§’ï¼‰ |

---

### ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ

#### âœ… å‡†ç¡®æ€§æ–¹é¢
- åœ¨ **13 ä¸ªæ•°æ®é›†ä¸­æœ‰ 12 ä¸ªå–å¾—æœ€ä½³ AUC**ï¼›
- å”¯ä¸€ä¾‹å¤–æ˜¯ `Shuttle` æ•°æ®é›†ï¼ˆIDK-S: 0.976 vs SiForest: 0.996ï¼‰ï¼Œä½†è¯¥ç°è±¡å·²åœ¨ç¦»çº¿ IDK ç ”ç©¶ä¸­è¢«è§‚å¯Ÿåˆ°ï¼ˆiForest åœ¨æ­¤ç‰¹å®šæ•°æ®ä¸Šç•¥ä¼˜ï¼‰ï¼Œå°è¯äº†â€œæµå¼æ€§èƒ½ç”±åŸºç¡€æ¨¡å‹å†³å®šâ€çš„æ ¸å¿ƒè®ºç‚¹ã€‚

#### âš¡ æ•ˆç‡æ–¹é¢ï¼ˆé€Ÿåº¦ä¼˜åŠ¿æä¸ºæ˜¾è‘—ï¼‰
- åœ¨å…­ä¸ªæœ€å¤§æ•°æ®é›†ä¸Šï¼Œç›¸æ¯”ç¬¬äºŒå¿«çš„ oIFORï¼Œ**æé€Ÿè‡³å°‘ 5 å€ä»¥ä¸Š**ï¼›
- åœ¨å°å‹é«˜ç»´æ•°æ®ï¼ˆå¦‚ `Satellite`, `NYC_taxi_single`ï¼‰ä¸Šï¼Œ**è¿è¡Œæ—¶é—´å‡å°‘ä¸€ä¸ªæ•°é‡çº§**ï¼ˆ10x ä»¥ä¸Šï¼‰ï¼›
- æ€»ä½“å¹³å‡è¿è¡Œæ—¶é—´ä»…ä¸º SiForest çš„ **1/16**ï¼ŒoIFOR çš„ **1/8**ï¼ŒCPOD çš„ **1/71**ã€‚

> ç¤ºä¾‹ï¼š`TwoCluster` æ•°æ®é›†ï¼ˆ100ä¸‡å®ä¾‹ï¼‰
> - IDK-S: 124 ç§’
> - SiForest: 2194 ç§’ â†’ **17.7x æ›´æ…¢**

---

### æ¶ˆèå®éªŒä¸å‚æ•°åˆ†æï¼ˆParameter Analysisï¼‰

#### ï¼ˆ1ï¼‰æœ€ä¼˜é‡‡æ ·æ•° $\nu$ åˆ†æï¼ˆFigure 5ï¼‰
- **IDK-S æœ€ä¼˜ $\nu$ é›†ä¸­åœ¨ [2, 8]**ï¼›
- å¯¹æ¯”ï¼š
  - oIFORï¼ˆmax_leaf_samplesï¼‰ï¼š[32, 128]
  - SiForestï¼ˆreservoir_samplesï¼‰ï¼š[256, 512]
- ç»“è®ºï¼šIDK-S æ‰€éœ€æ ·æœ¬æå°‘å³å¯è¾¾åˆ°é«˜æ€§èƒ½ â†’ **æ›´é«˜çš„æ ·æœ¬æ•ˆç‡**ï¼ˆsample efficiencyï¼‰

#### ï¼ˆ2ï¼‰ä¸ IDK å˜ä½“æ¯”è¾ƒï¼ˆTable 4ï¼‰

| æ–¹æ³• | å¹³å‡ AUC | å¹³å‡æ—¶é—´ï¼ˆç§’ï¼‰ |
|------|----------|----------------|
| **Offline IDK**ï¼ˆæ‰¹å¤„ç†ï¼‰ | 0.898 | 8 |
| **Retraining-based IDK** | 0.879 | 352 |
| **IDK-S**ï¼ˆæœ¬æ–‡ï¼‰ | **0.877** | **14** |

- IDK-S ä¸ retraining ç‰ˆæœ¬æ€§èƒ½å‡ ä¹æŒå¹³ï¼ˆä»…å·® 0.002 AUCï¼‰ï¼Œä½†é€Ÿåº¦å¿« **25 å€ä»¥ä¸Š**ï¼›
- æ˜¾ç¤ºå¢é‡æœºåˆ¶åœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æå¤§æå‡äº†æ•ˆç‡ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **æµå¼å¼‚å¸¸æ£€æµ‹çš„æ€§èƒ½å¤©èŠ±æ¿å—é™äºåŸºç¡€ç¦»çº¿æ¨¡å‹**ï¼š
   - å½“å‰å¤šæ•°æµå¼æ–¹æ³•åŸºäº iForest æˆ– LOFï¼Œå¤©ç„¶ç»§æ‰¿å…¶å¼±ç‚¹ï¼ˆå¦‚å¯¹å¯†åº¦å˜åŒ–æ•æ„Ÿï¼‰ï¼›
   - æ”¹è¿›æ–¹å‘åº”ä¼˜å…ˆé€‰æ‹©æ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ IDKï¼‰ã€‚

2. **IDK-S å®ç°äº†ç²¾åº¦ä¸æ•ˆç‡çš„åŒé‡çªç ´**ï¼š
   - å‡­å€Ÿ IDK çš„æ•°æ®ä¾èµ–æ ¸æœºåˆ¶ï¼Œåœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹è¾¾åˆ° SOTA ç²¾åº¦ï¼›
   - å¢é‡æ›´æ–°æœºåˆ¶ä½¿å…¶æ¯” retraining å¿« 25 å€ï¼Œæ¯”ç°æœ‰æµå¼æ–¹æ³•å¿« 8â€“70 å€ã€‚

3. **å¢é‡ç­–ç•¥å…·æœ‰ç†è®ºä¿è¯**ï¼š
   - Proposition 1 è¯æ˜å…¶ä¸å…¨é‡é‡è®­ç»ƒåœ¨ç»Ÿè®¡ä¸Šç­‰ä»·ï¼Œä¸æ˜¯å¯å‘å¼è¿‘ä¼¼ã€‚

4. **å¯¹æ¸è¿›å¼æ¦‚å¿µæ¼‚ç§»é€‚åº”è‰¯å¥½**ï¼š
   - åœ¨ `TwoCluster` ä¸Šè¡¨ç°ç¨³å®šï¼›
   - ä½†åœ¨ `INSECTS` çš„çªå˜æ¼‚ç§»åä»å‡ºç°çŸ­æš‚æ€§èƒ½ä¸‹é™ï¼Œè¯´æ˜æ‰€æœ‰æ–¹æ³•å¯¹æ­¤ç±»å˜åŒ–å‡æœ‰æŒ‘æˆ˜ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
- **å¯¹çªå˜å‹æ¦‚å¿µæ¼‚ç§»å“åº”ä¸å¤Ÿè¿…é€Ÿ**ï¼šè™½ç„¶èƒ½æœ€ç»ˆæ¢å¤ï¼Œä½†å­˜åœ¨æ»åï¼›
- **åˆå§‹åŒ–ä¾èµ–ç¬¬ä¸€ä¸ªçª—å£çš„ä»£è¡¨æ€§**ï¼ˆcold-start problemï¼‰ï¼šè‹¥åˆå§‹çª—å£ç¼ºä¹æ­£å¸¸æ¨¡å¼å¤šæ ·æ€§ï¼Œå¯èƒ½å½±å“æ—©æœŸæ£€æµ‹è´¨é‡ï¼›
- å½“å‰æœªæ¢ç´¢è‡ªé€‚åº”å‚æ•°è°ƒæ•´æœºåˆ¶ï¼ˆå¦‚åŠ¨æ€è°ƒæ•´ $\nu$ æˆ– $t$ï¼‰ã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
- æå‡å¯¹**çªå‘å¼æ¦‚å¿µæ¼‚ç§»**çš„é²æ£’æ€§å’Œå“åº”é€Ÿåº¦ï¼›
- æ¢ç´¢ç»“åˆ**åœ¨çº¿å­¦ä¹ æœºåˆ¶**æˆ–**è®°å¿†å¢å¼ºç»“æ„**æ¥åŠ å¿«æ¨¡å‹é€‚åº”ï¼›
- æ‰©å±•è‡³**å¤šæ¨¡æ€æˆ–å›¾æ•°æ®æµ**ä¸­çš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ï¼›
- ç ”ç©¶æ›´é«˜æ•ˆçš„ç‰¹å¾æ˜ å°„å‹ç¼©æŠ€æœ¯ä»¥è¿›ä¸€æ­¥é™ä½å†…å­˜å ç”¨ã€‚

--- 

> **æ€»ç»“ä¸€å¥è¯**ï¼š  
> IDK-S é€šè¿‡å°†å¼ºå¤§çš„ç¦»çº¿æ£€æµ‹å™¨ IDK ä¸ç†è®ºå¯è¯çš„å¢é‡æ›´æ–°æœºåˆ¶ç›¸ç»“åˆï¼Œåœ¨ä¸æŸå¤±ç»Ÿè®¡æœ‰æ•ˆæ€§çš„å‰æä¸‹ï¼Œå®ç°äº†æµå¼å¼‚å¸¸æ£€æµ‹åœ¨**å‡†ç¡®æ€§**å’Œ**æ•ˆç‡**ä¸Šçš„åŒé‡é£è·ƒï¼Œä¸ºé«˜ååã€éå¹³ç¨³ç¯å¢ƒä¸‹çš„å®æ—¶å¼‚å¸¸ç›‘æ§æä¾›äº†æ–°èŒƒå¼ã€‚

</details>

---

### 9. [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916)

**Authors**: Damien Lesens, Beheshteh T. Rakhshan, Guillaume Rabusseau  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.05916v1  

#### Abstract
The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply lo...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity â€” æ ¸å¿ƒæ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
åœ¨åŸºäº Transformer çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œ**Key-Value (KV) Cache** æ˜¯åŠ é€Ÿè‡ªå›å½’ç”Ÿæˆçš„å…³é”®æœºåˆ¶ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ˜¾è‘—çš„å†…å­˜å¼€é”€â€”â€”å…¶å¤§å°éšåºåˆ—é•¿åº¦å’Œæ‰¹å¤„ç†è§„æ¨¡çº¿æ€§å¢é•¿ï¼Œæˆä¸ºé•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚

ç°æœ‰çš„ KV Cache å‹ç¼©æ–¹æ³•ï¼ˆå¦‚ K-SVDã€EigenAttentionï¼‰é€šå¸¸ä»…å¯¹ Keys è¿›è¡Œä½ç§©åˆ†è§£ï¼Œæˆ–ç®€å•æ‹¼æ¥ Queries å’Œ Keys åè¿›è¡Œ SVDï¼Œå¿½ç•¥äº†æ³¨æ„åŠ›æœºåˆ¶çš„æœ¬è´¨ä¾èµ–äº **query-key å†…ç§¯ï¼ˆQK^Tï¼‰**ã€‚è¿™äº›æ–¹æ³•æ— æ³•æœ€ä¼˜åœ°é€¼è¿‘å®é™…çš„ attention matrixï¼Œå¯¼è‡´å‹ç¼©å attention è¾“å‡ºå¤±çœŸè¾ƒå¤§ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ï¼šKQ-SVD
ä½œè€…æå‡º **KQ-SVD**ï¼Œä¸€ç§é’ˆå¯¹ attention matrix $ \mathbf{QK}^T $ çš„**é—­å¼æœ€ä¼˜ä½ç§©è¿‘ä¼¼æ–¹æ³•**ï¼Œç›´æ¥æœ€å°åŒ– attention è¾“å‡ºè¯¯å·®ä¸Šç•Œã€‚

#### åˆ›æ–°æ€è·¯ï¼š
- å°† KV Cache å‹ç¼©å»ºæ¨¡ä¸ºå¯¹ **$ \mathbf{QK}^T $** çš„ä½ç§©é€¼è¿‘é—®é¢˜ï¼Œè€Œéå•ç‹¬å‹ç¼© K æˆ–è”åˆåµŒå…¥ Q/Kã€‚
- æ¨å¯¼å‡ºè¯¥ä¼˜åŒ–é—®é¢˜çš„**è§£æè§£ï¼ˆclosed-form solutionï¼‰**ï¼Œé€šè¿‡è®¡ç®— $ \mathbf{KQ}^T $ çš„ SVD å¾—åˆ°æœ€ä¼˜æŠ•å½±çŸ©é˜µã€‚
- åŒæ—¶è€ƒè™‘ Value ä¸ Output Projection çš„äº¤äº’ï¼Œè¿›ä¸€æ­¥æå‡è¾“å‡ºä¿çœŸåº¦ã€‚

#### æ•°å­¦å½¢å¼åŒ–ï¼š
ç›®æ ‡æ˜¯æœ€å°åŒ–ï¼š
$$
\min_{\mathbf{A},\mathbf{B}} \| \mathbf{KAB}^\top \mathbf{Q}^\top - \mathbf{KQ}^\top \|_F
$$
å…¶æœ€ä¼˜è§£ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š
$$
\mathbf{A} = \mathbf{K}^+\mathbf{U}_R,\quad \mathbf{B} = \mathbf{K}^\top \mathbf{U}_R
$$
å…¶ä¸­ $ \mathbf{U}_R $ æ˜¯ $ \mathbf{KQ}^T $ çš„å‰ R ä¸ªå·¦å¥‡å¼‚å‘é‡æ„æˆçš„çŸ©é˜µã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹æ³• | ç¼ºé™· | KQ-SVD å¦‚ä½•æ”¹è¿› |
|------|------|----------------|
| **K-SVD** | åªå‹ç¼© Keyï¼Œå¿½ç•¥ Query åˆ†å¸ƒï¼›ä¸èƒ½ä¿è¯ QK^T é€¼è¿‘è´¨é‡ | æ˜¾å¼å»ºæ¨¡ QK^Tï¼Œè·å¾—æ›´ä¼˜çš„ attention fidelity |
| **Eigen / Concatenated QK-SVD** | å¯¹ Query å’Œ Key æ‹¼æ¥ååš SVDï¼Œå¯¹ norm ä¸å¹³è¡¡æ•æ„Ÿ | åœ¨ä»»æ„ç¼©æ”¾ä¸‹ä¿æŒç¨³å®šï¼Œç†è®ºè¯æ˜å…¶é€€åŒ–é£é™© |
| **LoRC / ASVD / Palue** | å‹ç¼©æƒé‡çŸ©é˜µè€Œéç¼“å­˜æœ¬èº«ï¼Œéœ€è®­ç»ƒæˆ–éƒ¨ç½²å¤æ‚ | **æ— éœ€å†è®­ç»ƒ**ï¼Œé€‚ç”¨äºç°æˆæ¨¡å‹çš„åè®­ç»ƒæ ¡å‡†ï¼ˆpost-training calibrationï¼‰ |

> âœ… **æ ¸å¿ƒä¼˜åŠ¿æ€»ç»“**ï¼š
> - **ç†è®ºä¸Šæœ€ä¼˜**ï¼šé¦–æ¬¡æä¾›å¯¹ attention matrix çš„æœ€ä¼˜ä½ç§©é€¼è¿‘ï¼›
> - **è®¡ç®—é«˜æ•ˆ**ï¼šå¯é€šè¿‡æ ‡å‡† SVD å¿«é€Ÿæ±‚è§£ï¼›
> - **å…¼å®¹æ€§å¼º**ï¼šæ”¯æŒ GQA æ¶æ„ï¼Œå¹¶å¯æ‰©å±•è‡³ Value-Output è·¯å¾„ï¼›
> - **é²æ£’æ€§å¥½**ï¼šä¸å— query/key norm å·®å¼‚å½±å“ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- ä½¿ç”¨ **C4 dataset**ï¼ˆColossal Clean Crawled Corpusï¼‰
  - è®­ç»ƒé›†ç”¨äºå­¦ä¹ ä½ç§©æŠ•å½±ï¼ˆcalibration setï¼‰
  - éªŒè¯é›†ç”¨äºè¯„ä¼°å‹ç¼©æ•ˆæœ

### âš™ï¸ å®éªŒè®¾ç½®
- **æ¨¡å‹èŒƒå›´**ï¼š
  - é GQA æ¨¡å‹ï¼š`Llama2-7B`, `Llama2-13B`
  - æ”¯æŒ GQA çš„æ¨¡å‹ï¼š`Llama3-8B`, `Mistral-7B-v0.3`
- **æ ¡å‡†é˜¶æ®µï¼ˆCalibrationï¼‰**ï¼š
  - ä½¿ç”¨ 128 æ¡è®­ç»ƒåºåˆ—ï¼ˆæ¯æ¡ 2048 tokensï¼‰ï¼Œæ”¶é›†å„å±‚æ¯ä¸ª attention head çš„ K/Q/V ç¼“å­˜å¹¶æ‹¼æ¥æˆå¤§çŸ©é˜µ
- **è¯„ä¼°é˜¶æ®µ**ï¼š
  - ä½¿ç”¨ 32 æ¡éªŒè¯åºåˆ—ï¼ˆæ¯æ¡ 2048 tokensï¼‰ï¼Œæ¨¡æ‹Ÿ attention è®¡ç®—è¿‡ç¨‹
- **Rank Selection**ï¼š
  - æŒ‰å±‚é€‰æ‹©å‹ç¼©ç§© Rï¼Œä¿ç•™è‡³å°‘ 90% çš„è°±èƒ½é‡ï¼ˆå³ç›¸å¯¹ Frobenius error â‰¤ 0.1ï¼‰

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡
æ‰€æœ‰è¯¯å·®å‡ä¸º **ç›¸å¯¹ Frobenius èŒƒæ•°è¯¯å·®**ï¼š
$$
\text{err} = \frac{\|\mathbf{M} - \hat{\mathbf{M}}\|_F}{\|\mathbf{M}\|_F}
$$
åˆ†åˆ«è¯„ä¼°ä»¥ä¸‹ç»„ä»¶ï¼š
- Key ($ \mathbf{K} $)ã€Query ($ \mathbf{Q} $)ã€Value ($ \mathbf{V} $) çŸ©é˜µ
- Attention Score Matrix ($ \mathbf{QK}^T $)
- æœ€ç»ˆ Multi-Head Attention è¾“å‡ºï¼ˆMHA(X)ï¼‰

### ğŸ” åŸºçº¿æ–¹æ³•å¯¹æ¯”
- **K-SVD**ï¼šä»…å¯¹ Key çŸ©é˜µåš SVD å‹ç¼©
- **Eigen**ï¼šå°† Query å’Œ Key æ‹¼æ¥ååš SVDï¼Œå®ç° joint embedding
- **KQ-SVD**ï¼ˆæœ¬æ–‡æ–¹æ³•ï¼‰ï¼šåŸºäº $ \mathbf{KQ}^T $ çš„æœ€ä¼˜ä½ç§©é€¼è¿‘

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“ˆ æ€»ä½“æ€§èƒ½å¯¹æ¯”ï¼ˆå›¾1ï¼‰
åœ¨æ‰€æœ‰æµ‹è¯•æ¨¡å‹ä¸Šï¼ŒKQ-SVD è¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼š

| æŒ‡æ ‡ | ç»“æœ |
|------|------|
| **Attention Score Matrix ($ \mathbf{QK}^T $) è¯¯å·®** | KQ-SVD æ˜¾è‘—ä½äº K-SVD å’Œ Eigenï¼ˆå¹³å‡ä½ 30â€“50%ï¼‰ |
| **MHA è¾“å‡ºè¯¯å·®** | KQ-SVD åœ¨æ‰€æœ‰æ¨¡å‹ä¸Šå‡å–å¾—æœ€ä½è¯¯å·®ï¼Œå°¤å…¶åœ¨ GQA æ¨¡å‹ä¸­ä¼˜åŠ¿æ›´æ˜æ˜¾ |
| **Key çŸ©é˜µé‡å»ºç²¾åº¦** | K-SVD æœ€ä½³ï¼ˆå› å…¶ç›´æ¥ä¼˜åŒ– Kï¼‰ï¼Œä½†è¿™æ˜¯â€œè™šå‡ä¼˜åŠ¿â€ï¼Œå› æœªåæ˜  attention fidelity |
| **Query ä¿æŒèƒ½åŠ›** | Eigen å’Œ KQ-SVD å‡ä¼˜äº K-SVDï¼Œè¯´æ˜ joint å¤„ç† Query æ›´åˆç† |

> ğŸ’¡ **å…³é”®è§‚å¯Ÿ**ï¼šå°½ç®¡ K-SVD èƒ½æœ€å¥½åœ°é‡å»º Keyï¼Œä½†å®ƒä¸¥é‡æŸå®³äº† $ \mathbf{QK}^T $ çš„å‡†ç¡®æ€§ï¼Œæœ€ç»ˆå¯¼è‡´æ›´é«˜çš„ MHA è¾“å‡ºè¯¯å·®ã€‚

---

### ğŸ”¬ ä¸å¹³è¡¡ Query-Key å®éªŒï¼ˆå›¾2ï¼‰
äººä¸ºå¼•å…¥ scale imbalanceï¼šä»¤ $ \mathbf{K}' = \beta \mathbf{K}, \mathbf{Q}' = \mathbf{Q}/\beta $

| æ–¹æ³• | å¯¹ scale å˜æ¢çš„é²æ£’æ€§ |
|------|------------------------|
| **K-SVD** | ä¸å˜ï¼ˆåªä¾èµ– Kï¼‰ |
| **KQ-SVD** | ä¸å˜ï¼ˆåŸºäº $ \mathbf{KQ}^T $ï¼‰ |
| **Eigen** | **ä¸¥é‡é€€åŒ–**ï¼å½“ $ \beta > 1 $ æ—¶è¿…é€Ÿè¶‹è¿‘äº K-SVD |

> âœ… å®éªŒéªŒè¯äº† Theorem 4ï¼š**Eigen æ–¹æ³•åœ¨ query/key norm ä¸å¹³è¡¡æ—¶ä¼šé€€åŒ–ä¸º K-SVD**ï¼Œè€Œ KQ-SVD å§‹ç»ˆä¿æŒç¨³å®šã€‚

ä¾‹å¦‚ï¼Œåœ¨ Llama2-7B ä¸Šï¼š
- å½“ $ \beta = 10 $ æ—¶ï¼ŒEigen çš„è¾“å‡ºè¯¯å·®å‡ ä¹ä¸ K-SVD å®Œå…¨é‡åˆ
- è€Œ KQ-SVD è¯¯å·®å§‹ç»ˆä¿æŒæœ€ä½ä¸”ä¸å˜

---

### âœ… æ¶ˆèå®éªŒä¸ç†è®ºéªŒè¯
- **Theorem 3 æˆç«‹æ€§éªŒè¯**ï¼šå®éªŒè¯æ˜ K-SVD ä¸ KQ-SVD çš„è¯¯å·®å·®å€¼ç¡®å®ç­‰äºè¢«é—æ¼çš„ top-R singular space åŒ¹é…ç¨‹åº¦ï¼ŒéªŒè¯äº†è§£æå…¬å¼çš„æ­£ç¡®æ€§ã€‚
- **GQA å…¼å®¹æ€§éªŒè¯**ï¼šTheorem 5 è¡¨æ˜åªéœ€å°†ç»„å†…å¤šä¸ª Query çŸ©é˜µå †å å³å¯åº”ç”¨ KQ-SVDï¼Œå®éªŒæ˜¾ç¤ºå…¶åœ¨ Llama3/Mistral ä¸Šä¾ç„¶æœ‰æ•ˆã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Attention fidelity çš„æ ¸å¿ƒæ˜¯ $ \mathbf{QK}^T $ çš„é€¼è¿‘è´¨é‡**ï¼Œè€Œä¸æ˜¯å•ç‹¬å‹ç¼© Key æˆ– Valueã€‚
2. **ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ K-SVDã€Eigenï¼‰æœ¬è´¨ä¸Šæ˜¯éæœ€ä¼˜çš„**ï¼Œå‰è€…å¿½ç•¥ Query åˆ†å¸ƒï¼Œåè€…å¯¹ norm æ•æ„Ÿã€‚
3. **KQ-SVD æä¾›äº†ç¬¬ä¸€ä¸ªå¯¹ attention matrix çš„é—­å¼æœ€ä¼˜ä½ç§©é€¼è¿‘æ–¹æ¡ˆ**ï¼Œå…·æœ‰ä¸¥æ ¼ç†è®ºä¿éšœã€‚
4. **å®éªŒå…¨é¢éªŒè¯äº† KQ-SVD åœ¨å¤šç§ä¸»æµ LLM ä¸Šçš„ä¸€è‡´ä¼˜è¶Šæ€§**ï¼Œå°¤å…¶æ˜¯åœ¨ GQA æ¶æ„ä¸‹è¡¨ç°çªå‡ºã€‚
5. **è¯¥æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒ**ï¼Œå¯åœ¨ post-training é˜¶æ®µå®Œæˆæ ¡å‡†ï¼Œé€‚åˆå®é™…éƒ¨ç½²ã€‚

---

### âš ï¸ å±€é™æ€§
- **ä¾èµ–æ ¡å‡†æ•°æ®é›†**ï¼šéœ€è¦é«˜è´¨é‡ã€ä»£è¡¨æ€§å¼ºçš„æ•°æ®æ¥æ„å»º calibration cacheã€‚
- **å‡è®¾ç¼“å­˜å…·æœ‰ä½ç§©ç»“æ„**ï¼šè™½ç„¶å®è¯æˆç«‹ï¼Œä½†åœ¨æŸäº›ä»»åŠ¡ï¼ˆå¦‚é«˜åº¦éšæœºæ–‡æœ¬ï¼‰å¯èƒ½å¤±æ•ˆã€‚
- **æœªç»“åˆé‡åŒ–æˆ–å…¶ä»–å‹ç¼©æŠ€æœ¯**ï¼šå½“å‰ä»…ä¸ºä½ç§©æŠ•å½±ï¼Œæœªæ¥å¯ä¸å…¶ä»–æ–¹æ³•ï¼ˆå¦‚é‡åŒ–ã€ç¨€ç–åŒ–ï¼‰ç»„åˆä½¿ç”¨ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **åŠ¨æ€ Rank Allocation**ï¼šæ ¹æ®ä¸åŒå±‚/å¤´çš„é‡è¦æ€§è‡ªé€‚åº”åˆ†é…å‹ç¼©ç§© Rã€‚
2. **åœ¨çº¿æ›´æ–°æœºåˆ¶**ï¼šå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´ä½ç§©å­ç©ºé—´ä»¥åº”å¯¹åˆ†å¸ƒæ¼‚ç§»ã€‚
3. **ä¸é‡åŒ–ç»“åˆ**ï¼šæ¢ç´¢ KQ-SVD + KV Cache Quantization çš„è”åˆå‹ç¼©ç­–ç•¥ã€‚
4. **æ‰©å±•åˆ° Longformer / FlashAttention æ¶æ„**ï¼šé€‚é…ç¨€ç–æ³¨æ„åŠ›æˆ–çº¿æ€§æ³¨æ„åŠ›åœºæ™¯ã€‚

---

## âœ… æ€»ç»“ä¸€å¥è¯
> **KQ-SVD ä» attention fidelity å‡ºå‘ï¼Œæå‡ºäº†é¦–ä¸ªå¯¹ $ \mathbf{QK}^T $ çš„æœ€ä¼˜ä½ç§©å‹ç¼©æ–¹æ³•ï¼Œåœ¨ç†è®ºä¸¥è°¨æ€§å’Œå®éªŒæ€§èƒ½ä¸Šå…¨é¢è¶…è¶Šç°æœ‰ KV Cache å‹ç¼©æ–¹æ¡ˆï¼Œæ˜¯é«˜æ•ˆ LLM æ¨ç†çš„é‡è¦è¿›å±•ã€‚**

</details>

---

### 10. [CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning](https://arxiv.org/abs/2512.05576)

**Authors**: Ting-Ting Xie, Yixin Zhang  
**Category**: cs.AI  
**Published**: 2025-12-08  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.05576v1  

#### Abstract
Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Fr...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼š**CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰åŸºäºå°è§„æ¨¡ LLM çš„ä¸´åºŠæ™ºèƒ½ä½“ï¼ˆå¦‚ **TxAgent**ï¼‰å­˜åœ¨ **Context Utilization Failure**ï¼ˆä¸Šä¸‹æ–‡åˆ©ç”¨å¤±è´¥ï¼‰é—®é¢˜ï¼š  
å°½ç®¡èƒ½å¤ŸæˆåŠŸæ£€ç´¢åˆ°ç›¸å…³çš„ç”Ÿç‰©åŒ»å­¦è¯æ®ï¼Œä½†åœ¨æœ€ç»ˆè¯Šæ–­æ¨ç†æ—¶æœªèƒ½æœ‰æ•ˆåˆ©ç”¨è¿™äº›ä¿¡æ¯ï¼Œå¯¼è‡´â€œå¹»è§‰â€ï¼ˆhallucinationï¼‰å’Œé”™è¯¯å†³ç­–ã€‚

æ­¤å¤–ï¼Œè¿˜å­˜åœ¨ä»¥ä¸‹ä¸‰ç±»å…¸å‹é”™è¯¯ï¼š
- **Reasoning & Retrieval Failure**ï¼ˆå å¤±è´¥æ¡ˆä¾‹çš„ 65.8%ï¼‰
- **Output Parsing Error**ï¼ˆ19.2%ï¼‰
- **Instruction Adherence Failure**ï¼ˆ12.3%ï¼‰

è¿™äº›é—®é¢˜æºäºæ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ã€å¤šæ­¥éª¤å·¥å…·è°ƒç”¨ä»»åŠ¡ä¸­éš¾ä»¥ç»´æŒé€»è¾‘ä¸€è‡´æ€§å’Œè¯­ä¹‰è¿è´¯æ€§ã€‚

---

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯

æå‡º **Executor-Analyst Framework** â€”â€”ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“åä½œæ¶æ„ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯**è§£è€¦ï¼ˆDecouplingï¼‰**ï¼š

| è§’è‰² | åŠŸèƒ½ |
|------|------|
| **Executorï¼ˆæ‰§è¡Œè€…ï¼‰** | ä½¿ç”¨ç»è¿‡å¾®è°ƒçš„å°æ¨¡å‹ï¼ˆå¦‚ TxAgent / Llama-3.1-8Bï¼‰ï¼Œä¸“æ³¨äºç²¾ç¡®è°ƒç”¨å·¥å…·ï¼ˆTool Callingï¼‰ï¼Œä» ToolUniverse ä¸­æ£€ç´¢åŸå§‹è¯æ® |
| **Analystï¼ˆåˆ†æè€…ï¼‰** | ä½¿ç”¨å¤§ä¸Šä¸‹æ–‡åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ Gemini 2.5ï¼‰ï¼Œè´Ÿè´£æ•´åˆå™ªå£°è¯æ®ã€è¿›è¡Œäº‹å®éªŒè¯ã€è¡¥å……ç¼ºå¤±ä¿¡æ¯å¹¶ç”Ÿæˆæœ€ç»ˆè¯Šæ–­ |

#### åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š
1. **æ¨¡å—åŒ–è§£è€¦è®¾è®¡**ï¼šå°†â€œæ‰‹â€ï¼ˆtool executionï¼‰ä¸â€œè„‘â€ï¼ˆclinical reasoningï¼‰åˆ†ç¦»ï¼Œå…¼é¡¾è¯­æ³•ç²¾åº¦ä¸è¯­ä¹‰é²æ£’æ€§ã€‚
2. **Stratified Ensembleï¼ˆåˆ†å±‚é›†æˆï¼‰ç­–ç•¥**ï¼šç›¸æ¯”ä¼ ç»Ÿçš„å…¨å±€èšåˆï¼ˆGlobal Poolingï¼‰ï¼Œé‡‡ç”¨ Late Fusion æ¶æ„ä¿ç•™å¤šæ ·åŒ–çš„æ£€ç´¢è·¯å¾„ï¼Œé¿å…æ—©æœŸä¿¡æ¯ç“¶é¢ˆã€‚
3. **Training-Free è®¾è®¡**ï¼šä¸ä¾èµ–æ˜‚è´µçš„ç«¯åˆ°ç«¯å¾®è°ƒï¼Œé€šè¿‡æ¶æ„å·¥ç¨‹å®ç°é«˜æ€§èƒ½ï¼Œå…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚
4. **å¼•å…¥æœç´¢å¢å¼ºæœºåˆ¶**ï¼šAnalyst å¯ä¸»åŠ¨å‘èµ·äº’è”ç½‘æœç´¢ä»¥è¡¥å……å·¥å…·æœªè¦†ç›–çš„ä¿¡æ¯ï¼Œæå‡ä¿¡æ¯å®Œæ•´æ€§ã€‚

---

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿è¯´æ˜ |
|------|----------|
| æ€§èƒ½ | åœ¨ CURE-Bench ä¸Šè¾¾åˆ° SOTA è¡¨ç°ï¼ˆ83.803% å‡†ç¡®ç‡ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå•æ¨¡å‹æˆ–å•ä¸€ä»£ç†æ–¹æ¡ˆ |
| é²æ£’æ€§ | åˆ†å±‚é›†æˆå‡å°‘é›†ä½“å¹»è§‰ï¼Œæé«˜å¤æ‚ç—…ä¾‹å¤„ç†èƒ½åŠ› |
| å¯ç»´æŠ¤æ€§ | å·¥å…·é›†æ›´æ–°æ— éœ€é‡æ–°è®­ç»ƒï¼Œæ”¯æŒå¿«é€Ÿè¿­ä»£ |
| æˆæœ¬æ•ˆç‡ | å…å»å¤§è§„æ¨¡å¾®è°ƒæˆæœ¬ï¼Œä»…éœ€ç»„åˆå·²æœ‰æ¨¡å‹å³å¯æ„å»ºé«˜é˜¶ç³»ç»Ÿ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“š æ•°æ®é›†
- **CURE-Bench@NeurIPS 2025**ï¼šä¸€ä¸ªé¢å‘å¤§è§„æ¨¡æ²»ç–—å†³ç­–æ¨ç†çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚
- åŒ…å«ä¸¤ä¸ªé˜¶æ®µçš„æ•°æ®é›†ï¼š
  - `testset_phase1`ï¼ˆå…¬å¼€æ¦œå•ï¼‰
  - `testset_phase2`ï¼ˆç§æœ‰æ¦œå•ï¼Œç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
- è¦†ç›–å¤šç§ä¸´åºŠåœºæ™¯ï¼Œè¦æ±‚æ¨¡å‹é€šè¿‡è°ƒç”¨è¶…è¿‡ 200 ä¸ªå·¥å…·ï¼ˆAPIsã€æ•°æ®åº“ç­‰ï¼‰å®Œæˆå¤šæ­¥æ¨ç†ã€‚

### âš™ï¸ å®éªŒè®¾ç½®
- **Executor**ï¼šä½¿ç”¨ **TxAgent**ï¼ˆåŸºäº Llama-3.1-8B å¾®è°ƒï¼‰ï¼Œè´Ÿè´£å¤šè½®å·¥å…·è°ƒç”¨ä¸è¯æ®æ”¶é›†ã€‚
- **Analyst**ï¼šä½¿ç”¨ **Gemini 2.5-Pro / Flash** ç­‰é—­æºå¤§æ¨¡å‹ï¼Œè´Ÿè´£ç»¼åˆåˆ†æä¸è¯Šæ–­è¾“å‡ºã€‚
- å¼•å…¥ **Self-Consistency (SC)** æœºåˆ¶äºä¸¤ä¸ªé˜¶æ®µï¼š
  - Executor å±‚ï¼šå¯¹å¤šä¸ªå¹¶è¡Œå®ä¾‹çš„ç»“æœè¿›è¡Œå¤šæ•°æŠ•ç¥¨ï¼Œæå‡æ£€ç´¢ç¨³å®šæ€§ã€‚
  - Analyst å±‚ï¼šå¤šä¸ªç‹¬ç«‹æ¨ç†é“¾èåˆï¼Œå¢å¼ºç»“è®ºä¸€è‡´æ€§ã€‚
- **Topological Configurations å¯¹æ¯”**ï¼š
  - **Config A**: Global Poolingï¼ˆEarly Fusionï¼‰
  - **Config B**: Stratified Ensembleï¼ˆLate Fusionï¼‰â€”â€”æœ¬æ–‡æå‡ºçš„æ–¹æ³•

### ğŸ¯ è¯„ä¼°æŒ‡æ ‡
- ä¸»è¦æŒ‡æ ‡ï¼š**å‡†ç¡®ç‡ï¼ˆAccuracy %ï¼‰**
- è¾…åŠ©åˆ†æï¼šå¤±è´¥æ¨¡å¼åˆ†ç±»ã€ä¸Šä¸‹æ–‡é•¿åº¦å½±å“ã€å·¥å…·æ•°é‡æ‰©å±•æ€§æµ‹è¯•

### ğŸ†š åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿æ¨¡å‹ | æ˜¯å¦ä½¿ç”¨å·¥å…· | ç‰¹ç‚¹ |
|--------|-------------|------|
| TxAgent (baseline) | æ˜¯ | ç»è¿‡å¾®è°ƒï¼Œæ“…é•¿å·¥å…·è°ƒç”¨ä½†æ¨ç†å¼± |
| Llama-3.1-8B, Qwen3-8B ç­‰å¼€æºæ¨¡å‹ | å¦ | é›¶æ ·æœ¬è¡¨ç°å·®ï¼ˆ<30%ï¼‰ |
| Gemini 2.5 ç³»åˆ— | å¦ / æ˜¯ï¼ˆgeneric searchï¼‰ | æ¨ç†å¼ºï¼Œä½†ç¼ºä¹ä¸“ç”¨å·¥å…·æ¥å£ |
| å•ä¸€æ¨¡å‹ + Self-Consistency | æ˜¯ | å¦‚ TxAgent(n=30)ï¼Œä½œä¸ºå¼ºåŸºçº¿ |

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### ğŸ“Š å…³é”®æ€§èƒ½æ•°æ®ï¼ˆå‡åœ¨ `phase2` æµ‹è¯•é›†ä¸ŠæŠ¥å‘Šï¼‰

| æ¨¡å‹é…ç½® | å‡†ç¡®ç‡ (%) |
|---------|-----------|
| TxAgent (single) | 69.325 |
| Gemini-2.5-flash (no tools) | 63.104 |
| TxAgent + SC (n=30 executors) | 73.508 |
| **Executor-Analyst (Config A, Global Pooling)** | **79.311 ~ 80.510** |
| **Executor-Analyst (Config B, Stratified Ensemble)** | **81.367** |
| **+ Gemini with search** | **83.803** âœ… (**SOTA**) |

> ğŸ’¡ æœ€ç»ˆæäº¤ç‰ˆæœ¬ä½¿ç”¨ï¼š**TxAgent (n=10) + Gemini-2.5-flash-with-search (n=3) + Stratified Ensemble**

---

### ğŸ”¬ æ¶ˆèå®éªŒç»“æœ

#### (1) æ¸©åº¦å‚æ•°è°ƒä¼˜ï¼ˆExecutorï¼‰
| Temperature | Phase1 Score |
|------------|------------|
| 0.6        | 58.950     |
| 0.7        | 59.248     |
| **0.8**    | **65.214** âœ… |
| 0.9        | 56.747     |

â†’ è¡¨æ˜é€‚åº¦å¢åŠ ç”Ÿæˆéšæœºæ€§æœ‰åŠ©äºæ¢ç´¢æ›´ä¼˜å·¥å…·è°ƒç”¨è·¯å¾„ã€‚

#### (2) Self-Consistency æ•ˆæœï¼ˆExecutor å±‚ï¼‰
- éšç€é‡‡æ ·è·¯å¾„æ•° $n$ å¢åŠ ï¼Œå‡†ç¡®ç‡æŒç»­ä¸Šå‡ï¼š
  - $n=1$: ~69.3%
  - $n=15$: ~73.3%
  - $n=60$: ~74.2%
- æ”¶ç›Šé€’å‡å‡ºç°åœ¨ $n > 20$

#### (3) æ¶æ„æ‹“æ‰‘å¯¹æ¯”ï¼ˆLate vs Early Fusionï¼‰
- **Global Pooling (A)**ï¼šè™½èƒ½é™å™ªï¼Œä½†æ˜“ä¸¢å¤±ç¨€æœ‰ä½†å…³é”®çš„è¯æ®ã€‚
- **Stratified Ensemble (B)**ï¼šé€šè¿‡ä¿æŒå¤šä¸ªç‹¬ç«‹æ¨ç†æµï¼Œåœ¨åæœŸèåˆç­”æ¡ˆï¼Œæ˜¾è‘—æå‡æ€§èƒ½ï¼ˆ**+0.866%**ï¼‰ï¼ŒéªŒè¯äº†å¤šæ ·æ€§ä¿ç•™çš„é‡è¦æ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **Decoupling is Key**ï¼šå°†å·¥å…·æ‰§è¡Œä¸ä¸´åºŠæ¨ç†åˆ†ç¦»å¯æ˜¾è‘—ç¼“è§£ Context Utilization Failureï¼Œæ˜¯æ€§èƒ½è·ƒå‡çš„æ ¸å¿ƒé©±åŠ¨åŠ›ã€‚
2. **Topology Matters**ï¼šLate Fusionï¼ˆStratified Ensembleï¼‰ä¼˜äº Early Fusionï¼Œè¯æ˜ä¿¡æ¯å¤šæ ·æ€§åœ¨å¤æ‚æ¨ç†ä¸­çš„ä»·å€¼ã€‚
3. **Search Enhances Robustness**ï¼šå³ä½¿å·²æœ‰ä¸“ç”¨å·¥å…·é›†ï¼Œé€šç”¨æœç´¢å¼•æ“ä»èƒ½è¡¥å……é—æ¼ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥æå‡å‡†ç¡®æ€§ã€‚
4. **Training-Free Architecture Engineering is Powerful**ï¼šæ— éœ€å¾®è°ƒå³å¯è¶…è¶Šå¾®è°ƒè¿‡çš„ä¸“ç”¨æ¨¡å‹ï¼Œå‡¸æ˜¾æ¶æ„è®¾è®¡çš„é‡è¦æ€§ã€‚

---

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
1. **Curse of Dimensionality in Action Spaces**ï¼š
   - å½“ ToolUniverse æ‰©å±•è‡³ 600+ å·¥å…·æ—¶ï¼ŒTxAgent å‡†ç¡®ç‡ä» 92.0% ä¸‹é™è‡³ 87.5%ã€‚
   - è¡¨æ˜æ‰å¹³åŒ–å·¥å…·æ£€ç´¢é¢ä¸´ç»´åº¦ç¾éš¾ï¼Œéœ€å¼•å…¥å±‚æ¬¡ç´¢å¼•æˆ–åˆ†ç±»æœºåˆ¶ã€‚

2. **Context-Performance Paradox**ï¼š
   - ä¸Šä¸‹æ–‡é•¿åº¦è¶…è¿‡ **12k tokens** åï¼Œæ€§èƒ½åè€Œä¸‹é™ï¼ˆä» 94% â†’ 87.93%ï¼‰ã€‚
   - è¿‡å¤šå™ªå£°ä¿¡æ¯å¹²æ‰°æ³¨æ„åŠ›æœºåˆ¶ï¼Œå½¢æˆâ€œä¿¡æ¯è¿‡è½½â€ã€‚

3. **ä¾èµ–é—­æºæ¨¡å‹ï¼ˆGeminiï¼‰**ï¼š
   - å½“å‰æ¡†æ¶ä¸¥é‡ä¾èµ– Gemini ç­‰é—­æºå¼ºæ¨ç†æ¨¡å‹ï¼Œé™åˆ¶äº†å®Œå…¨å¼€æºéƒ¨ç½²çš„å¯èƒ½æ€§ã€‚

---

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **Hierarchical Tool Indexing**ï¼šæŒ‰ä¸´åºŠé¢†åŸŸç»„ç»‡å·¥å…·åº“ï¼Œå®ç°ç²—ç²’åº¦â†’ç»†ç²’åº¦æ£€ç´¢ã€‚
2. **Information Compression & Early Rejection**ï¼šåŸºäºç½®ä¿¡åº¦è¿‡æ»¤æ— å…³å·¥å…·è¾“å‡ºï¼Œæå‡ä¸Šä¸‹æ–‡æ•ˆç‡ã€‚
3. **Training-Free Adaptation via ICL/RAG**ï¼šåˆ©ç”¨ In-Context Learning æˆ– RAG è‡ªåŠ¨å¼•å¯¼æ¨¡å‹ä½¿ç”¨æ–°å·¥å…·ï¼Œé¿å…é¢‘ç¹å¾®è°ƒã€‚
4. **Curriculum Learning for Executors**ï¼šé€æ­¥æš´éœ²æ›´å¤§å·¥å…·é›†ï¼Œæå‡æŠ—å¹²æ‰°èƒ½åŠ›ã€‚
5. **å‘ Zero-Shot Tool Use æ¼”è¿›**ï¼šéšç€ Foundation Model èƒ½åŠ›å¢å¼ºï¼ŒExecutor å¯ç®€åŒ–ä¸º Prompt-Based æ¨¡å—ï¼Œè¿›ä¸€æ­¥é™ä½ç³»ç»Ÿå¤æ‚åº¦ã€‚

---

## âœ… æ€»ç»“

**CureAgent** æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„ **Training-Free Executor-Analyst Framework**ï¼Œé€šè¿‡è§£è€¦å·¥å…·æ‰§è¡Œä¸ä¸´åºŠæ¨ç†ï¼Œåœ¨ **CURE-Bench** ä¸Šå®ç°äº† **83.803%** çš„ SOTA å‡†ç¡®ç‡ã€‚è¯¥å·¥ä½œä¸ä»…å±•ç¤ºäº†æ¶æ„åˆ›æ–°çš„å·¨å¤§æ½œåŠ›ï¼Œä¹Ÿæ­ç¤ºäº†å½“å‰ä¸´åºŠ AI é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼š**ä¿¡æ¯å¤šæ ·æ€§ä¿æŠ¤ã€ä¸Šä¸‹æ–‡æ•ˆç‡ä¼˜åŒ–ã€ä»¥åŠå¤§è§„æ¨¡åŠ¨ä½œç©ºé—´ä¸‹çš„å¯æ‰©å±•æ€§**ã€‚å…¶æå‡ºçš„ **Stratified Ensemble** å’Œ **late fusion** ç­–ç•¥ä¸ºä¸‹ä¸€ä»£å¯ä¿¡åŒ»ç–— AI ç³»ç»Ÿæä¾›äº†é‡è¦èŒƒå¼å‚è€ƒã€‚

</details>

---

### 11. [Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning](https://arxiv.org/abs/2512.05747)

**Authors**: Jinlong Liu, Mohammed Bahja, Venelin Kovatchev, Mark Lee  
**Category**: cs.CL  
**Published**: 2025-12-08  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.05747v1  

#### Abstract
Recent advances in large language models (LLMs) show impressive performance in open-ended story generation, but fine-grained stylistic control remains limited. Existing methods often rely on shallow cues (e.g., names or topics) to simulate authorial style, without robust evaluation. In this work, we...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³**å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿ç¯‡æ•…äº‹ç”Ÿæˆä¸­ç¼ºä¹ç²¾ç»†é£æ ¼æ§åˆ¶èƒ½åŠ›**çš„é—®é¢˜ã€‚å°½ç®¡å½“å‰çš„ LLM å¦‚ GPT-4o å’Œ Claude åœ¨å¼€æ”¾æ€§å†™ä½œä»»åŠ¡ä¸Šè¡¨ç°æµç•…ï¼Œä½†å®ƒä»¬å¯¹â€œç»å…¸ä½œè€…é£æ ¼â€çš„æ¨¡ä»¿å¾€å¾€åœç•™åœ¨è¡¨é¢çº¿ç´¢ï¼ˆå¦‚è§’è‰²åå­—ã€ä¸»é¢˜å…ƒç´ ï¼‰ï¼Œè€ŒéçœŸæ­£æ•æ‰æ–‡å­¦è¯­è°ƒã€å™äº‹èŠ‚å¥ç­‰æ·±å±‚é£æ ¼ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯é çš„è¯„ä¼°æœºåˆ¶æ¥è¡¡é‡é£æ ¼ä¸€è‡´æ€§ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ€è·¯
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº **Group Relative Policy Optimization (GRPO)** çš„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå®ç°**ä»¥ä½œè€…é£æ ¼ä¸ºæ¡ä»¶çš„é•¿ç¯‡æ•…äº‹ç”Ÿæˆ**ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

- **å°†ä½œè€…èº«ä»½éªŒè¯ï¼ˆAuthorship Verification, AVï¼‰ä¿¡å·èå…¥å¥–åŠ±å»ºæ¨¡**ï¼šé€šè¿‡å¾®è°ƒä¸€ä¸ª sentence transformer æ¨¡å‹ä½œä¸ºé£æ ¼å¥–åŠ±å‡½æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿé‡åŒ–ç”Ÿæˆæ–‡æœ¬ä¸ç›®æ ‡ä½œè€…ï¼ˆå¦‚ Mark Twainï¼‰ä¹‹é—´çš„é£æ ¼ç›¸ä¼¼åº¦ã€‚
- **å¤šå¥–åŠ±ç³»ç»Ÿè®¾è®¡**ï¼š
  - **Style Reward**ï¼šåŸºäº AV å¾—åˆ†ï¼Œå¼•å¯¼æ¨¡å‹å‘ç‰¹å®šå†™ä½œé£æ ¼é æ‹¢ï¼›
  - **Content Reward**ï¼šè¯„ä¼°å™äº‹è´¨é‡ï¼ˆè¿è´¯æ€§ã€äººç‰©å‘å±•ç­‰ï¼‰ï¼›
  - **Completeness Reward**ï¼šç¡®ä¿è¾“å‡ºè¾¾åˆ°ç›®æ ‡é•¿åº¦å¹¶æœ‰å®Œæ•´ç»“å°¾ã€‚
- **åŠ¨æ€å¯è§£é‡Šçš„é£æ ¼è¯„åˆ†æœºåˆ¶**ï¼šåˆ©ç”¨å¥–åŠ±æ¨¡å‹è¾“å‡ºä½œä¸ºç”Ÿæˆè¿‡ç¨‹ä¸­çš„å®æ—¶é£æ ¼å¯¹é½åº¦é‡ï¼Œæä¾›å¯æµ‹é‡ã€å¯æ§çš„é£æ ¼ç”Ÿæˆç®¡é“ã€‚

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³• | æœ¬å·¥ä½œ |
|------|--------|-------|
| é£æ ¼å»ºæ¨¡æ–¹å¼ | æµ…å±‚æç¤ºï¼ˆprompt engineeringï¼‰ã€å…³é”®è¯å¤åˆ¶ | åŸºäº AV çš„æ·±åº¦é£æ ¼åµŒå…¥å­¦ä¹  |
| å¥–åŠ±æœºåˆ¶ | å•ä¸€å¥–åŠ±æˆ–äººå·¥æ ‡æ³¨åå¥½ï¼ˆå¦‚ DPOï¼‰ | å¤šç»´åº¦ã€è‡ªåŠ¨åŒ–çš„å¥–åŠ±ç»„åˆï¼ˆGRPO + è‡ªå®šä¹‰ rewardï¼‰ |
| è¾“å‡ºå¤šæ ·æ€§ | å—é™äºå›ºå®š accept/reject å¯¹ï¼ˆDPO ç¼ºé™·ï¼‰ | GRPO æ”¯æŒæ›´ä¸°å¯Œçš„æ¢ç´¢è·¯å¾„ |
| è®­ç»ƒæ•ˆç‡ | PPO éœ€è¦ critic ç½‘ç»œï¼Œè®¡ç®—å¼€é”€é«˜ | GRPO ä¸éœ€ç‹¬ç«‹ criticï¼Œé™ä½èµ„æºæ¶ˆè€— |

> âœ… **ä¼˜åŠ¿æ€»ç»“**ï¼šè¯¥æ–¹æ³•å®ç°äº†**åœ¨è¾ƒå°è§„æ¨¡æ¨¡å‹ï¼ˆ8Bï¼‰ä¸‹è¶…è¶Šæ›´å¤§æ¨¡å‹ï¼ˆå¦‚ GPT-4oï¼‰çš„é£æ ¼æ¨¡ä»¿èƒ½åŠ›**ï¼ŒåŒæ—¶ä¿æŒåˆç†çš„å™äº‹è´¨é‡å’Œç»“æ„å®Œæ•´æ€§ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### æ•°æ®é›†
- **åŸºç¡€æ•°æ®æ¥æº**ï¼šProject Gutenberg å…¬å…±é¢†åŸŸæ–‡æœ¬ï¼Œæ¶µç›– 84 ä½ä½œå®¶çš„ 161 æœ¬ä¹¦ç±ã€‚
- **ç›®æ ‡ä½œè€…èšç„¦**ï¼šä»¥ **Mark Twain** ä¸ºæ ¸å¿ƒç ”ç©¶å¯¹è±¡ï¼Œä½¿ç”¨ *The Adventures of Huckleberry Finn* ä½œä¸ºé£æ ¼å‚è€ƒèŒƒä¾‹ã€‚
- **æ•°æ®é¢„å¤„ç†**ï¼š
  - æ–‡æœ¬åˆ‡åˆ†ä¸º 1500 å­—å·¦å³çš„æ®µè½ï¼ˆchunkï¼‰ï¼›
  - æ„é€ ä¸‰ç§é…å¯¹ç±»å‹ç”¨äºè®­ç»ƒé£æ ¼åˆ¤åˆ«å™¨ï¼š
    - æ­£æ ·æœ¬ï¼ˆPositiveï¼‰ï¼šåŒä¸€ä½œè€…ä¸åŒä¹¦ç±çš„å†…å®¹ç»æ©ç é‡å»ºåå¯¹æ¯”ï¼›
    - è´Ÿæ ·æœ¬ï¼ˆNegativeï¼‰ï¼šä¸åŒä½œè€…ä¹‹é—´å¯¹æ¯”ï¼›
    - Refilled-refilled å¯¹æ¯”ï¼šæ§åˆ¶å†…å®¹ä¸€è‡´ä½†é£æ ¼å˜åŒ–ç¨‹åº¦ã€‚

### å®éªŒè®¾ç½®
- **æ¨¡å‹æ¶æ„**ï¼šé‡‡ç”¨ 8B å‚æ•°çº§åˆ«çš„ LLM è¿›è¡Œ GRPO å¾®è°ƒã€‚
- **è®­ç»ƒæµç¨‹**ï¼š
  1. **SFTï¼ˆSupervised Fine-Tuningï¼‰é˜¶æ®µ**ï¼šå…ˆç”¨é€šç”¨æ•…äº‹æç¤ºè®­ç»ƒåŸºç¡€å™äº‹èƒ½åŠ›ï¼›
  2. **GRPO é˜¶æ®µ**ï¼šå¼•å…¥é£æ ¼åŒ–æç¤ºï¼ˆå¦‚ â€œin the style of Mark Twainâ€ï¼‰ï¼Œç»“åˆä¸‰é‡å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚
- **è¶…å‚æ•°é€‰æ‹©**ï¼š
  - å­¦ä¹ ç‡ï¼š3Ã—10â»â¶ï¼›
  - Î²ï¼ˆKL æ•£åº¦ç³»æ•°ï¼‰ï¼š0.035ï¼ˆç»è°ƒå‚ç¡®å®šæœ€ä¼˜ç¨³å®šæ€§ï¼‰ï¼›
  - æ‰¹å¤§å°ï¼šæ¯ prompt è‡³å°‘é‡‡æ · 16 æ¡å“åº”ã€‚

### è¯„ä¼°æŒ‡æ ‡
| æŒ‡æ ‡ | å®šä¹‰ | ç”¨é€” |
|------|-----|-----|
| **Style Score** | åŸºäº fine-tuned sentence transformer è¾“å‡ºçš„ AV ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆç» sigmoid å½’ä¸€åŒ–ï¼‰ | è¡¡é‡é£æ ¼å¯¹é½ç¨‹åº¦ |
| **Content Score** | ä½¿ç”¨ LLM-as-a-judge æ–¹æ³•ï¼ŒæŒ‰å››çº§è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼ˆ1â€“4 åˆ†ï¼‰ï¼Œå½’ä¸€åŒ–è‡³ [0.25, 1] | è¯„ä¼°å™äº‹è´¨é‡ |
| **Completeness Score** | æ˜¯å¦å®Œæˆå¥å­ + è¾¾åˆ°çº¦ 1500 å­—ï¼Œæœªå®Œæˆåˆ™ä¹˜ä»¥ 0.5 æƒ©ç½šå› å­ | æ§åˆ¶è¾“å‡ºå®Œæ•´æ€§ |
| **Weighted Avg Score** | æœ€ç»ˆç»¼åˆå¾—åˆ† = `0.6Ã—Style + 0.3Ã—Content + 0.1Ã—Completeness` | ç»¼åˆè¯„ä»·æ¨¡å‹æ€§èƒ½ |

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
å‚ä¸æ¯”è¾ƒçš„ä¸»æµ LLM åŒ…æ‹¬ï¼š
- GPT-4o
- Claude Sonnet 4
- Gemini 2.5 Flash
- Qwen2.5-32B
- Gemma-3-27B

æ‰€æœ‰æ¨¡å‹å‡åœ¨åŒä¸€ç»„ 10 ä¸ª prompt ä¸Šç”Ÿæˆæ•…äº‹ï¼Œå¹¶ç”±ç»Ÿä¸€çš„ reward model æ‰“åˆ†ï¼Œä¿è¯å…¬å¹³æ€§ã€‚

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ï¼ˆè§ Table 3ï¼‰
| Model | Style Â± SD | Content Â± SD | Completeness Â± SD | Weighted Avg Â± SD |
|-------|------------|--------------|--------------------|-------------------|
| **FT-Agentic (8B)** | **0.628 Â± 0.336** | 0.750 Â± 0.000 | 0.744 Â± 0.227 | **0.676 Â± 0.203** |
| GPT-4o | 0.510 Â± 0.194 | **0.800 Â± 0.100** | 0.873 Â± 0.136 | 0.633 Â± 0.117 |
| Claude Sonnet 4 | 0.450 Â± 0.203 | 0.750 Â± 0.000 | 0.999 Â± 0.002 | 0.595 Â± 0.122 |
| Qwen2.5-32B | 0.328 Â± 0.218 | 0.750 Â± 0.000 | 0.911 Â± 0.013 | 0.513 Â± 0.131 |

> ğŸ” **å…³é”®å‘ç°**ï¼š
> - å°½ç®¡æˆ‘ä»¬çš„ 8B æ¨¡å‹åœ¨ **Content** ä¸Šç•¥é€Šäº GPT-4oï¼Œåœ¨ **Completeness** ä¸Šä¸å¦‚ Claudeï¼Œä½†åœ¨æœ€å…³é”®çš„ **Style Score ä¸Šæ˜¾è‘—é¢†å…ˆï¼ˆ0.628 vs. 0.510ï¼‰**ï¼›
> - ç»¼åˆå¾—åˆ†ä¹Ÿæœ€é«˜ï¼ˆ**0.676 > 0.633**ï¼‰ï¼Œè¯´æ˜è¯¥æ–¹æ³•åœ¨é£æ ¼æ§åˆ¶æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

---

### æ¶ˆèå®éªŒä¸åˆ†æï¼ˆéšå«åœ¨æ–‡ä¸­ï¼‰
è™½ç„¶æ²¡æœ‰æ˜ç¡®åˆ—å‡ºæ¶ˆèè¡¨ï¼Œä½†ä»ä»¥ä¸‹å‡ ç‚¹å¯æ¨æ–­å…³é”®ç»„ä»¶ä½œç”¨ï¼š

- **é£æ ¼å¥–åŠ±æ¨¡å‹çš„æœ‰æ•ˆæ€§**ï¼š
  - è¡¨ 2 æ˜¾ç¤ºï¼Œç»è¿‡ fine-tuning åï¼Œå„ sentence transformer æ¨¡å‹çš„ same-author ä¸ cross-author åˆ†å¸ƒå®Œå…¨åˆ†ç¦»ï¼ˆIQR Overlap = 0%ï¼‰ï¼Œè¡¨æ˜ AV æ¨¡å‹å…·å¤‡å¼ºå¤§åˆ¤åˆ«åŠ›ã€‚
  - ç‰¹åˆ«æ˜¯ **FT-all-distilroberta-v1** å®ç°æœ€å¤§ Î”ï¼ˆ+0.628ï¼‰ï¼Œæˆä¸ºæœ€å¼ºé£æ ¼åˆ¤åˆ«å™¨ã€‚

- **GRPO ä¸­ Î² å‚æ•°çš„å½±å“**ï¼ˆå›¾ 3ï¼‰ï¼š
  - Î² < 0.04 å¯¼è‡´ KL æ•£åº¦å‰§çƒˆæ³¢åŠ¨ï¼Œè®­ç»ƒä¸ç¨³å®šï¼›
  - æœ€ç»ˆé€‰å®š Î² = 0.035 å¹¶é…åˆæ¢¯åº¦å½’ä¸€åŒ–ï¼Œå®ç°ç¨³å®šæ”¶æ•›ã€‚

- **ä¸ºä½•ä¸é€‰æœ€å¼ºåˆ¤åˆ«å™¨ï¼Ÿ**
  - å°½ç®¡ FT-all-distilroberta-v1 åˆ¤åˆ«æœ€å¼ºï¼Œä½†å®ƒå¯¼è‡´æ—©æœŸ RL æ¢ç´¢å›°éš¾ï¼ˆreward è¿‡ä½ï¼‰ï¼›
  - å› æ­¤é€‰ç”¨ **FT-all-mpnet-base-v2**ï¼Œå› å…¶æä¾›æ›´å¹³æ»‘çš„å¥–åŠ±æ¢¯åº¦ï¼Œåˆ©äºç­–ç•¥è¿­ä»£ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. âœ… **GRPO + AV-based reward æ˜¯æœ‰æ•ˆçš„é£æ ¼æ§åˆ¶æ–¹æ¡ˆ**ï¼šå³ä½¿åœ¨ 8B è§„æ¨¡æ¨¡å‹ä¸Šï¼Œä¹Ÿèƒ½ç”Ÿæˆé«˜åº¦é£æ ¼å¯¹é½çš„é•¿ç¯‡æ•…äº‹ï¼Œä¸”ä¼˜äº GPT-4o ç­‰å•†ä¸šå¤§æ¨¡å‹ã€‚
2. âœ… **é£æ ¼ä¸å™äº‹å®Œæ•´æ€§å­˜åœ¨æƒè¡¡**ï¼šæå‡é£æ ¼ç›¸ä¼¼æ€§å¯èƒ½å¯¼è‡´å™äº‹å®Œæ•´æ€§ä¸‹é™ï¼ˆå¦‚ç»“å°¾ä»“ä¿ƒã€é€»è¾‘æ–­è£‚ï¼‰ï¼Œè¿™æ˜¯å½“å‰ RL æ¡†æ¶ä¸‹çš„å›ºæœ‰æŒ‘æˆ˜ã€‚
3. âœ… **é£æ ¼å¯ä»¥è¢«é‡åŒ–å¹¶ç”¨äºæŒ‡å¯¼ç”Ÿæˆ**ï¼šé€šè¿‡æ„å»ºå¯æ§çš„æ•°æ®å¢å¼ºä¸ fine-tuned reward modelï¼Œå®ç°äº†é£æ ¼å¯¹é½çš„å¯æµ‹é‡ã€å¯ä¼˜åŒ–ã€‚

---

### å±€é™æ€§
1. **è®­ç»ƒæ•°æ®å•ä¸€**ï¼šä»…åŸºäº Mark Twain çš„ä¸€éƒ¨ä½œå“ï¼ˆ*Huckleberry Finn*ï¼‰è¿›è¡Œé£æ ¼å»ºæ¨¡ï¼Œæ³›åŒ–èƒ½åŠ›å—é™ï¼›
2. **å†…å®¹å¥–åŠ±æ¨¡å‹ä¸å¯é **ï¼šä¾èµ– LLM-as-a-judge æ–¹å¼æ‰“åˆ†ï¼Œå­˜åœ¨è¯¯åˆ¤é£é™©ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§ï¼›
3. **ç¼ºä¹äººç±»è¯„ä¼°**ï¼šç›®å‰æ‰€æœ‰è¯„ä¼°å‡ä¸ºè‡ªåŠ¨åŒ–æŒ‡æ ‡é©±åŠ¨ï¼Œæ— æ³•åˆ¤æ–­é£æ ¼æ˜¯å¦â€œçœŸå®å¯ä¿¡â€ï¼›
4. **å™äº‹å®Œæ•´æ€§ä»ä¸è¶³**ï¼šéƒ¨åˆ†ç”Ÿæˆæ•…äº‹è™½é£æ ¼é²œæ˜ï¼Œä½†æƒ…èŠ‚æ”¶æŸä¸ä½³ï¼Œå½±å“é˜…è¯»ä½“éªŒã€‚

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•å¤šä½œè€…ã€å¤šæ–‡ä½“è®­ç»ƒé›†**ï¼šçº³å…¥æ›´å¤šç»å…¸ä½œå®¶ï¼ˆå¦‚ Jane Austen, Hemingwayï¼‰ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼›
2. **æ”¹è¿›å†…å®¹å¥–åŠ±æ¨¡å‹**ï¼šå¼•å…¥æ›´å¼ºçš„å› æœæ¨ç†ä¸æƒ…èŠ‚ä¸€è‡´æ€§æ£€æµ‹æ¨¡å—ï¼›
3. **åŠ å…¥äººç±»åé¦ˆï¼ˆHuman-in-the-loopï¼‰**ï¼šå¼€å±•ç”¨æˆ·ç ”ç©¶ï¼Œæ”¶é›†å¯¹é£æ ¼çœŸå®æ€§ä¸å™äº‹è´¨é‡çš„äººå·¥è¯„åˆ†ï¼›
4. **æ¢ç´¢å…¨å±€å™äº‹è§„åˆ’æœºåˆ¶**ï¼šåœ¨ç”Ÿæˆå‰å¼•å…¥å¤§çº²ç”Ÿæˆå™¨æˆ– plot controllerï¼Œæå‡é•¿ç¨‹ä¸€è‡´æ€§ã€‚

---

> ğŸ“Œ **æ€»ä½“è¯„ä»·**ï¼š  
> æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ **GRPO + å®šåˆ¶åŒ– reward modeling** å®ç°é«˜è´¨é‡çš„ç»å…¸ä½œè€…é£æ ¼æ•…äº‹ç”Ÿæˆã€‚å®ƒä¸ä»…æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„æŠ€æœ¯è·¯å¾„ï¼Œä¹Ÿä¸ºâ€œé£æ ¼å¯æ§åˆ›ä½œâ€è¿™ä¸€åˆ›é€ æ€§ AI ä»»åŠ¡æä¾›äº†å¯å¤ç°ã€å¯è¯„ä¼°çš„ç ”ç©¶èŒƒå¼ã€‚å°½ç®¡åœ¨å™äº‹å®Œæ•´æ€§æ–¹é¢ä»æœ‰æŒ‘æˆ˜ï¼Œä½†å…¶åœ¨å°æ¨¡å‹ä¸Šè¶…è¶Šå¤§æ¨¡å‹çš„è¡¨ç°ï¼Œè¯æ˜äº†**ä»»åŠ¡ä¸“ç”¨è®­ç»ƒçš„ä»·å€¼**ã€‚

</details>

---

### 12. [Bridging quantum and classical computing for partial differential equations through multifidelity machine learning](https://arxiv.org/abs/2512.05241)

**Authors**: Bruno Jacob, Amanda A. Howard, Panos Stinis  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.05241v1  

#### Abstract
Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum P...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Bridging Quantum and Classical Computing for Partial Differential Equations through Multifidelity Machine Learning  
**æ ¸å¿ƒç»“è®ºä¸å®éªŒç»“æœæ€»ç»“**

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### âœ… è§£å†³çš„é—®é¢˜
å½“å‰é‡å­è®¡ç®—åœ¨æ±‚è§£åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰æ–¹é¢è™½æœ‰ç†è®ºåŠ é€Ÿæ½œåŠ›ï¼Œä½†åœ¨**è¿‘ä¸­æœŸç¡¬ä»¶ï¼ˆNISQ devicesï¼‰ä¸Šé¢ä¸´ä¸¥é‡é™åˆ¶**ï¼š
- **qubit æ•°é‡æœ‰é™** â†’ ç©ºé—´åˆ†è¾¨ç‡ä½ï¼ˆç²—ç½‘æ ¼ï¼‰
- **ç”µè·¯æ·±åº¦å—é™** â†’ æ—¶é—´ç§¯åˆ†ç²¾åº¦å·®ï¼Œéš¾ä»¥è¿›è¡Œé•¿æ—¶é—´æ¨¡æ‹Ÿ

è¿™å¯¼è‡´é‡å­ PDE æ±‚è§£å™¨åªèƒ½äº§ç”Ÿ**ä½ä¿çœŸåº¦ï¼ˆlow-fidelityï¼‰ç»“æœ**ï¼Œæ— æ³•æ»¡è¶³å®é™…ç§‘å­¦è®¡ç®—éœ€æ±‚ã€‚

### ğŸš€ æå‡ºçš„æ–°æ–¹æ³•
ä½œè€…æå‡ºä¸€ç§**æ··åˆé‡å­-ç»å…¸å¤šä¿çœŸåº¦æœºå™¨å­¦ä¹ æ¡†æ¶ï¼ˆhybrid quantum-classical multifidelity learning frameworkï¼‰**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- åˆ©ç”¨**é‡å­ç®—æ³•ç”Ÿæˆå¤§é‡ä½ä¿çœŸæ•°æ®**ï¼ˆcoarse-grid, short-timeï¼‰
- ç»“åˆ**å°‘é‡é«˜ä¿çœŸç»å…¸ä»¿çœŸæ•°æ®**ï¼ˆfine-grid, long-timeï¼‰
- é€šè¿‡**å¤šä¿çœŸåº¦ç¥ç»ç½‘ç»œæ¶æ„**ï¼ˆMultifidelity KANï¼‰å­¦ä¹ ä» LF åˆ° HF çš„æ˜ å°„ï¼Œå®ç°è¯¯å·®æ ¡æ­£ä¸æ—¶é—´å¤–æ¨

è¯¥æ¡†æ¶çš„å…³é”®ç»„æˆéƒ¨åˆ†åŒ…æ‹¬ï¼š
- **é‡å­æ±‚è§£å™¨**ï¼šé‡‡ç”¨ QLBM-frugal ç®—æ³•ï¼ˆQuantum Lattice Boltzmann Methodï¼‰ï¼Œèµ„æºé«˜æ•ˆï¼Œé€‚åˆ NISQ è®¾å¤‡
- **ç»å…¸æ±‚è§£å™¨**ï¼šæä¾›ç¨€ç–çš„é«˜ä¿çœŸå‚è€ƒæ•°æ®
- **ä¿®æ­£æ¨¡å‹**ï¼šåŸºäº Kolmogorov-Arnold Networks (KAN)ï¼Œåˆ†ç¦»çº¿æ€§å’Œéçº¿æ€§ä¿®æ­£åˆ†æ”¯ï¼Œå¹¶é€šè¿‡å¯å­¦ä¹ å‚æ•° $\alpha$ åŠ¨æ€åŠ æƒèåˆ

### ğŸ” ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **å®ç”¨æ€§æå‡** | é¦–æ¬¡å°†çœŸå®é‡å­ç®—æ³•è¾“å‡ºä¸å¤šä¿çœŸå­¦ä¹ ç»“åˆï¼Œä½¿å½“å‰ç¡¬ä»¶ä¸Šçš„é‡å­è®¾å¤‡å…·å¤‡â€œå®ç”¨ä»·å€¼â€ï¼ˆpractical quantum utilityï¼‰ |
| **æ•°æ®æ•ˆç‡é«˜** | ä»…éœ€**ç¨€ç–é«˜ä¿çœŸè®­ç»ƒæ•°æ®**å³å¯å®Œæˆæœ‰æ•ˆæ ¡æ­£ï¼Œå¤§å¹…å‡å°‘æ˜‚è´µçš„ç»å…¸é«˜åˆ†è¾¨ç‡æ¨¡æ‹Ÿéœ€æ±‚ |
| **æ³›åŒ–èƒ½åŠ›å¼º** | èƒ½å¤Ÿå®ç°**æ—¶é—´åŸŸå¤–æ¨ï¼ˆtemporal extrapolationï¼‰**ï¼Œé¢„æµ‹è¿œè¶…é«˜ä¿çœŸè®­ç»ƒçª—å£çš„æ—¶é—´æ¼”åŒ– |
| **æ¶æ„è®¾è®¡å…ˆè¿›** | ä½¿ç”¨ KAN æ›¿ä»£ä¼ ç»Ÿ MLPï¼Œå…·æœ‰æ›´å¼ºçš„å‡½æ•°é€¼è¿‘èƒ½åŠ›å’Œå‚æ•°æ•ˆç‡ï¼›å¤åˆçº¿æ€§/éçº¿æ€§è·¯å¾„å¢å¼ºé²æ£’æ€§ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†ä¸ç‰©ç†é—®é¢˜
å®éªŒéªŒè¯äº†ä¸¤ä¸ªå…¸å‹éçº¿æ€§ PDE é—®é¢˜ï¼š

| é—®é¢˜ | æè¿° |
|------|------|
| **Viscous Burgers Equation (1D)** | éçº¿æ€§å¯¹æµ-æ‰©æ•£æ–¹ç¨‹ï¼Œåˆå§‹ä¸ºé«˜æ–¯è„‰å†²ï¼Œå½¢æˆæ¿€æ³¢ä¼ æ’­ï¼Œæµ‹è¯•ç©ºé—´é”åˆ©æ¢¯åº¦æ•æ‰èƒ½åŠ› |
| **2D Lid-Driven Cavity Flow (Navier-Stokes)** | ä¸å¯å‹æµä½“æµåŠ¨ï¼ŒRe=100ï¼Œå­˜åœ¨ä¸»æ¶¡ä¸è§’æ¶¡ï¼Œæµ‹è¯•å¤æ‚æµåœºç»“æ„æ¢å¤èƒ½åŠ› |

#### æ•°æ®æ¥æº
- **ä½ä¿çœŸæ•°æ®ï¼ˆLFï¼‰**ï¼šç”± QLBM é‡å­ç”µè·¯æ¨¡æ‹Ÿç”Ÿæˆï¼ˆstatevector emulationï¼‰
  - Burgers: $N=16$ ç½‘æ ¼ vs ç»å…¸ $N=256$
  - Cavity: $16\times16$ vs $64\times64$
- **é«˜ä¿çœŸæ•°æ®ï¼ˆHFï¼‰**ï¼šç»å…¸æœ‰é™å·®åˆ†/æœ‰é™ä½“ç§¯æ³•æ±‚è§£è·å¾—
- **è®­ç»ƒç­–ç•¥**ï¼š
  - LF ç½‘ç»œè®­ç»ƒä½¿ç”¨å…¨æ—¶é—´æ®µæ•°æ®ï¼ˆå¦‚ $t \in [0, 0.5]$ï¼‰
  - HF ä¿®æ­£ç½‘ç»œä»…ä½¿ç”¨å‰åŠæ®µæ•°æ®è®­ç»ƒï¼ˆå¦‚ $t \leq 0.25$ï¼‰ï¼ŒååŠæ®µç”¨äºæµ‹è¯•**å¤–æ¨èƒ½åŠ›**

### âš™ï¸ å®éªŒè®¾ç½®
- **é‡å­æ¨¡æ‹Ÿå¹³å°**ï¼šQiskit statevector simulatorï¼ˆéçœŸå®ç¡¬ä»¶ï¼Œä½†æ¨¡æ‹ŸçœŸå®ç”µè·¯è¡Œä¸ºï¼‰
- **ç¡¬ä»¶é…ç½®**ï¼šNVIDIA A100 GPU
- **ç½‘ç»œç»“æ„**ï¼š
  - ä½ä¿çœŸç½‘ç»œ $K_{LF}$ï¼šè¾“å…¥ $(x,t)$ï¼Œè¾“å‡º $q_{LF}$
  - å¤šä¿çœŸç½‘ç»œï¼š$q_{MF} = \alpha \cdot K_{nl}(x,t,q_{LF}) + (1-\alpha)\cdot K_{lin}(x,t,q_{LF})$
    - $K_{lin}, K_{nl}$ åˆ†åˆ«ä¸ºçº¿æ€§å’Œéçº¿æ€§ KAN å­ç½‘
    - $\alpha$ å¯å­¦ä¹ ï¼Œåˆå§‹åŒ–ä¸º 0.5ï¼Œå—æ­£åˆ™é¡¹ $\lambda \alpha^4$ æ§åˆ¶ä»¥åå¥½çº¿æ€§æ¨¡å‹
- **ä¼˜åŒ–å™¨**ï¼šAdam ($lr=10^{-3}$)
- **è®­ç»ƒæµç¨‹**ï¼š
  1. å…ˆè®­ç»ƒ $K_{LF}$ åŒ¹é…é‡å­è¾“å‡º
  2. å†»ç»“ $K_{LF}$ æƒé‡ï¼Œè”åˆè®­ç»ƒ $K_{lin}, K_{nl}, \alpha$

### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡
- **ç›¸å¯¹ $L_2$ è¯¯å·®**ï¼š
  $$
  L^2 = \frac{\|q_{pred} - q_{true}\|_2}{\|q_{true}\|_2}
  $$
- åˆ†åŒºåŸŸæŠ¥å‘Šï¼š
  - è®­ç»ƒåŒºï¼ˆ$t \leq t_{cut}$ï¼‰
  - å¤–æ¨åŒºï¼ˆ$t > t_{cut}$ï¼‰
  - å…¨åŸŸå¹³å‡
- å¯¹æ¯”å¯¹è±¡ï¼š
  - Raw quantum LF solverï¼ˆæœªä¿®æ­£ï¼‰
  - Classical HF solverï¼ˆé»„é‡‘æ ‡å‡†ï¼‰

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### âœ… å…³é”®æ€§èƒ½æ•°æ®æ±‡æ€»

#### âœ… **Viscous Burgers æ–¹ç¨‹ï¼ˆTable 1ï¼‰**
| æ–¹æ³• | å…¨åŸŸ $L^2$ é”™è¯¯ | å¤–æ¨åŒºé”™è¯¯ | æå‡å¹…åº¦ |
|------|------------------|------------|----------|
| Raw LF Solver | 0.335 | â€” | â€” |
| Best MF (B4) | **0.0615** | 0.0914 | â†“ **82%** |

> æœ€ä¼˜é…ç½®ï¼ˆ$\lambda=10^{-4}, G=5$ï¼‰åœ¨å¤–æ¨åŒºåŸŸä¹Ÿæ˜¾è‘—ä¼˜äºåŸå§‹ LFï¼ˆ0.1159 â†’ 0.0914ï¼‰

#### âœ… **Lid-Driven Cavity æµåŠ¨ï¼ˆTable 2ï¼‰**
| æ–¹æ³• | æ°´å¹³é€Ÿåº¦ $L^2$ | å‚ç›´é€Ÿåº¦ $L^2$ | æå‡å¹…åº¦ |
|------|------------------|------------------|----------|
| Raw LF Solver | 0.285 | 0.412 | â€” |
| Best MF (C3) | **0.079** | **0.167** | â†“ **72% (u), 60% (v)** |

> åœ¨ $t=3.0$ï¼ˆå®Œå…¨å¤–æ¨ï¼‰ä»èƒ½å‡†ç¡®é‡å»ºä¸»æ¶¡ç»“æ„

### ğŸ” ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”
| åŸºçº¿ | è¡¨ç° |
|------|------|
| **çº¯éçº¿æ€§ä¿®æ­£ï¼ˆ$\alpha=1$ å›ºå®šï¼‰** | å¤–æ¨æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼ˆBurgers: $L^2=0.7173$ï¼‰ï¼Œè¯´æ˜è¿‡åº¦ä¾èµ–éçº¿æ€§æ˜“è¿‡æ‹Ÿåˆ |
| **æ— æ­£åˆ™åŒ–ï¼ˆ$\lambda=0$ï¼‰** | è®­ç»ƒè¯¯å·®ä½ä½†å¤–æ¨å·®ï¼Œ$\alpha \approx 0.7$ï¼Œåå‘å¤æ‚æ¨¡å‹ |
| **è¿‡å¤§çš„ B-spline ç½‘æ ¼ï¼ˆG=7ï¼‰** | å‡ºç°æ˜æ˜¾è¿‡æ‹Ÿåˆï¼ˆextrap error â†‘ï¼‰ |
| **è¿‡çª„ç½‘ç»œå®½åº¦ï¼ˆwidth=5ï¼‰** | æ¬ æ‹Ÿåˆï¼Œè¯¯å·®æœ€é«˜ |

### ğŸ” æ¶ˆèå®éªŒå…³é”®å‘ç°
| å®éªŒå˜é‡ | å‘ç° |
|---------|------|
| **Blending Regularization ($\lambda$)** | ä¸­ç­‰å¼ºåº¦æ­£åˆ™åŒ–ï¼ˆå¦‚ $10^{-4} \sim 10^{-5}$ï¼‰æœ€ä¼˜ï¼Œä¿ƒä½¿æ¨¡å‹é€‰æ‹©æ›´ç®€å•çš„çº¿æ€§ä¸»å¯¼ä¿®æ­£ï¼Œæå‡æ³›åŒ– |
| **B-spline Grid Resolution (G)** | $G=5$ å¹³è¡¡è¡¨è¾¾åŠ›ä¸æ³›åŒ–ï¼Œ$G=7$ è¿‡æ‹Ÿåˆï¼Œ$G=3$ æ¬ æ‹Ÿåˆ |
| **Network Width** | å®½åº¦å¢åŠ ä¸å¿…ç„¶æå‡æ€§èƒ½ï¼›é€‚åº¦å®¹é‡ + æ­£åˆ™åŒ– > å•çº¯å¢å¤§æ¨¡å‹ |
| **Temporal Extrapolation** | æ‰€æœ‰æ¡ˆä¾‹ä¸­ï¼ŒMF æ¨¡å‹åœ¨æ—  HF æ•°æ®çš„åæœŸä»ä¿æŒé«˜ç²¾åº¦ï¼Œè¯æ˜å…¶çœŸæ­£â€œå­¦åˆ°â€äº†ä¿®æ­£è§„å¾‹ |

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### âœ… ä¸»è¦å‘ç°
1. **é‡å­-ç»å…¸ååŒå¯è¡Œ**ï¼šå³ä½¿é‡å­æ±‚è§£å™¨å› ç¡¬ä»¶é™åˆ¶åªèƒ½è¿è¡Œåœ¨ç²—ç½‘æ ¼ã€çŸ­æ—¶é—´å°ºåº¦ï¼Œå…¶è¾“å‡ºä»å¯ä½œä¸ºæœ‰æ•ˆçš„**ä½ä¿çœŸæº**ç”¨äºæ„å»ºé«˜æ€§èƒ½æ··åˆæ¨¡å‹ã€‚
2. **å¤šä¿çœŸå­¦ä¹ æ˜¾è‘—ææ•ˆ**ï¼šé€šè¿‡å¼•å…¥ä»…å«å°‘é‡ HF æ•°æ®çš„ KAN ä¿®æ­£ç½‘ç»œï¼Œå¯å°†é‡å­è§£çš„è¯¯å·®é™ä½ **70â€“80%**ï¼Œè¾¾åˆ°æ¥è¿‘ç»å…¸é«˜ä¿çœŸè§£çš„ç²¾åº¦ã€‚
3. **å…·å¤‡å¼ºå¤–æ¨èƒ½åŠ›**ï¼šæ¨¡å‹èƒ½åœ¨**å®Œå…¨æ²¡æœ‰é«˜ä¿çœŸè®­ç»ƒæ•°æ®çš„æ—¶é—´åŒºé—´å†…åšå‡ºå‡†ç¡®é¢„æµ‹**ï¼Œè¿™å¯¹é•¿æœŸåŠ¨åŠ›å­¦æ¨¡æ‹Ÿæå…·æ„ä¹‰ã€‚
4. **çº¿æ€§ä¿®æ­£è‡³å…³é‡è¦**ï¼šæ¶ˆèå®éªŒè¯æ˜ï¼Œå¼ºåˆ¶ä½¿ç”¨çº¯éçº¿æ€§ä¿®æ­£ä¼šå¯¼è‡´ç¾éš¾æ€§å¤–æ¨å¤±è´¥ï¼Œè€Œå…è®¸çº¿æ€§è·¯å¾„å­˜åœ¨å¹¶åŠ ä»¥æ­£åˆ™åŒ–ï¼Œèƒ½è‡ªåŠ¨å¹³è¡¡æ¨¡å‹å¤æ‚åº¦ï¼Œæå‡ç¨³å®šæ€§ã€‚

### âš ï¸ æ–¹æ³•çš„å±€é™æ€§
- å½“å‰å®éªŒåŸºäº **statevector æ¨¡æ‹Ÿ**ï¼Œå°šæœªåœ¨çœŸå®é‡å­ç¡¬ä»¶ä¸Šè¿è¡Œï¼ˆå™ªå£°ã€æµ‹é‡è¯¯å·®æœªè€ƒè™‘ï¼‰
- æ‰€æœ‰ HF æ•°æ®ä»éœ€æ¥è‡ªç»å…¸æ±‚è§£å™¨ï¼Œè‹¥ HF è·å–æˆæœ¬æé«˜ï¼Œåˆ™æ•´ä½“æ”¶ç›Šå—é™
- æ¶æ„ä¾èµ–äºè‰¯å¥½çš„ LF-HF ç›¸å…³æ€§ï¼Œå¯¹äºé«˜åº¦æ··æ²Œç³»ç»Ÿå¯èƒ½æŒ‘æˆ˜æ›´å¤§

### ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘
1. **æ‰©å±•è‡³æ›´å¤š PDE ç±»å‹**ï¼šå¦‚ååº”-æ‰©æ•£ç³»ç»Ÿã€å¼¹æ€§åŠ›å­¦ã€Maxwell æ–¹ç¨‹ç­‰
2. **é›†æˆ Physics-Informed Learning**ï¼šåŠ å…¥æ®‹å·®çº¦æŸï¼Œè¿›ä¸€æ­¥å‡å°‘å¯¹ HF æ•°æ®çš„ä¾èµ–
3. **éƒ¨ç½²åˆ°çœŸå®é‡å­è®¾å¤‡**ï¼šç ”ç©¶å™ªå£°é²æ£’æ€§åŠé‡‡æ ·è¯¯å·®å½±å“
4. **åŠ¨æ€è‡ªé€‚åº”ä¿çœŸåº¦è°ƒåº¦**ï¼šæ ¹æ®è¯¯å·®ä¼°è®¡åŠ¨æ€å†³å®šä½•æ—¶è°ƒç”¨ HF æ±‚è§£
5. **Operator Learning æ‰©å±•**ï¼šä½¿ç”¨ DeepONet æˆ– Fourier Neural Operators å®ç°è·¨å‚æ•°åŒ–çš„å¤šä¿çœŸå»ºæ¨¡

---

## æ€»ç»“
è¯¥è®ºæ–‡æˆåŠŸå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ **Multifidelity Machine Learning** æ¡¥æ¥å½“å‰å—é™çš„é‡å­è®¡ç®—èƒ½åŠ›ä¸å®é™…ç§‘å­¦è®¡ç®—çš„éœ€æ±‚ã€‚å®ƒä¸ä»…æå‡ºäº†ä¸€ä¸ª**å®ç”¨çš„æ··åˆé‡å­-ç»å…¸æ¡†æ¶**ï¼Œè¿˜é€šè¿‡ä¸¥è°¨å®éªŒéªŒè¯äº†å…¶åœ¨éçº¿æ€§ PDE ä¸Šçš„**é«˜ç²¾åº¦æ ¡æ­£ä¸æ—¶é—´å¤–æ¨èƒ½åŠ›**ï¼Œä¸ºè¿‘ä¸­æœŸé‡å­è®¾å¤‡åœ¨è®¡ç®—ç‰©ç†ä¸­çš„â€œæœ‰ç”¨æ€§â€å¼€è¾Ÿäº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚

</details>

---

### 13. [Bounded Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2512.05623)

**Authors**: Kibidi Neocosmos, Diego Baptista, Nicole Ludwig  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.05623v1  

#### Abstract
In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šBounded Graph Clustering with Graph Neural Networks

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
ä¼ ç»ŸåŸºäº **Graph Neural Networks (GNNs)** çš„å›¾èšç±»ï¼ˆcommunity detectionï¼‰æ–¹æ³•å­˜åœ¨ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼šå³ä½¿ç”¨æˆ·æŒ‡å®šäº†æœŸæœ›çš„èšç±»æ•°é‡ $ c $ï¼Œæ¨¡å‹ä¹Ÿå¸¸å¸¸æ— æ³•è¿”å›**æ°å¥½** $ c $ ä¸ªç¤¾åŒºã€‚è¿™æ˜¯ç”±äºï¼š
- èšç±»è¿‡ç¨‹æ˜¯é€šè¿‡ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼ˆå¦‚ modularityï¼‰å®ç°çš„ï¼›
- èŠ‚ç‚¹ä»¥â€œè½¯åˆ†é…â€æ–¹å¼è¢«åˆ†é…åˆ°ç¤¾åŒºï¼ˆsoft cluster assignmentï¼‰ï¼Œæœ€ç»ˆé€šè¿‡ `argmax` è½¬æ¢ä¸ºç¡¬åˆ’åˆ†ï¼›
- åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼ŒæŸäº›ç¤¾åŒºå¯èƒ½æ²¡æœ‰èŠ‚ç‚¹è¢«åˆ†é…ï¼Œå¯¼è‡´å®é™…è¾“å‡ºçš„ç¤¾åŒºæ•°å°‘äºæŒ‡å®šå€¼ã€‚

æ­¤å¤–ï¼Œå¤§å¤šæ•° GNN æ–¹æ³•è¦æ±‚é¢„å…ˆçŸ¥é“ç¡®åˆ‡çš„ç¤¾åŒºæ•°é‡ï¼Œè€Œç°å®ä¸­è¯¥æ•°å€¼é€šå¸¸æ˜¯æœªçŸ¥çš„ã€‚

---

### æå‡ºçš„æ–°æ–¹æ³•ä¸åˆ›æ–°æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ç§**çµæ´»ä¸”åŸåˆ™æ€§çš„çº¦æŸæœºåˆ¶**ï¼Œç”¨äºæ§åˆ¶ GNN è¾“å‡ºçš„ç¤¾åŒºæ•°é‡ï¼š

#### ï¼ˆ1ï¼‰å¼•å…¥å¯è°ƒèŠ‚çš„ä¸Šä¸‹ç•Œçº¦æŸ
- ç”¨æˆ·å¯ä»¥æŒ‡å®šä¸€ä¸ª**èŒƒå›´** $[l, c]$ï¼Œå…¶ä¸­ï¼š
  - $ c $ï¼šæœ€å¤§ç¤¾åŒºæ•°ï¼ˆä¸Šç•Œï¼Œç”± cluster assignment matrix åˆ—æ•°å†³å®šï¼‰
  - $ l $ï¼šæœ€å°ç¤¾åŒºæ•°ï¼ˆä¸‹ç•Œï¼Œç”±æ–°å¢çš„ **constraint term** æ§åˆ¶ï¼‰
- å½“ $ l = c $ æ—¶ï¼Œå¼ºåˆ¶æ¨¡å‹è¾“å‡º**ç²¾ç¡®æ•°é‡**çš„ç¤¾åŒºã€‚

#### ï¼ˆ2ï¼‰è®¾è®¡æ–°çš„æŸå¤±å‡½æ•°é¡¹
æå‡ºäº†ä¸€ä¸ªæ–°çš„å¯å¾®åˆ†çš„çº¦æŸé¡¹ï¼ˆconstraint termï¼‰ï¼š
$$
\text{constraint}(S, l) = l - \sum_{k=1}^{l} \max_{1 \leq i \leq n} \frac{s_{ik}}{\max_k s_{ik}}
$$
è¯¥çº¦æŸä½œç”¨äº cluster assignment matrix $ S $ï¼Œå¯¹æ¯ä¸€è¡Œè¿›è¡Œå½’ä¸€åŒ–åï¼Œç»Ÿè®¡æœ€æœ‰å¯èƒ½æˆä¸ºä¸»å¯¼ç¤¾åŒºçš„å‰ $ l $ ä¸ªåˆ—çš„æœ€å¤§å€¼ä¹‹å’Œï¼Œé¼“åŠ±è‡³å°‘ $ l $ ä¸ªç¤¾åŒºéç©ºã€‚

#### ï¼ˆ3ï¼‰ç»“åˆå¹³è¡¡æ­£åˆ™åŒ–é¡¹
å¼•å…¥æ”¹è¿›ç‰ˆçš„ balance regularization termï¼š
$$
\text{balance}(S) = \frac{\|\text{diag}(S^T S)\|_2^2}{n \|e_c\|_2^2}
$$
å®ƒé¼“åŠ±å„ç¤¾åŒºå¤§å°å‡è¡¡ï¼Œé¿å… singleton clustersï¼ˆå•èŠ‚ç‚¹ç¤¾åŒºï¼‰ï¼Œä»è€Œæå‡èšç±»è´¨é‡ã€‚

æœ€ç»ˆæŸå¤±å‡½æ•°ä¸ºï¼š
$$
\mathcal{L} = \text{modularity}(S) + \mu \cdot \text{constraint}(S, l) + \lambda \cdot \text{balance}(S)
$$

---

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **å¯æ§æ€§æ›´å¼º** | æ”¯æŒæŒ‡å®šç¤¾åŒºæ•°é‡èŒƒå›´æˆ–ç²¾ç¡®å€¼ï¼Œè§£å†³äº† GNN æ— æ³•å¯é è¿”å›æŒ‡å®šæ•°é‡ç¤¾åŒºçš„é—®é¢˜ |
| **çµæ´»æ€§æ›´é«˜** | å¯ä½œä¸ºæœç´¢æœ€ä¼˜ç¤¾åŒºæ•°çš„åŸºç¡€æ¡†æ¶ï¼ˆä¾‹å¦‚é€æ­¥æ”¶ç´§åŒºé—´ï¼‰ |
| **å…¼å®¹æ€§å¼º** | å¯åµŒå…¥ä»»ä½•åŸºäº modularity æˆ– min-cut çš„ GNN èšç±»æ¨¡å‹ä¸­ |
| **æ•ˆæœæ›´ç¨³å®š** | å®éªŒè¡¨æ˜èƒ½æ˜¾è‘—æé«˜è¾“å‡ºç¤¾åŒºæ•°çš„å‡†ç¡®æ€§ï¼Œå¹¶æ”¹å–„èšç±»è´¨é‡ï¼ˆå¦‚ ARIï¼‰ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†

#### åˆæˆæ•°æ®ï¼ˆSynthetic Dataï¼‰
- åŸºäº **Stochastic Block Model (SBM)** ç”Ÿæˆ
- å‚æ•°å˜åŒ–åŒ…æ‹¬ï¼š
  - ç½‘ç»œè§„æ¨¡ï¼šSmall ($10^2$), Medium ($10^3$), Large ($10^4$)
  - ç¤¾åŒºæ•°é‡ï¼š5, 10, 20
  - å¯†åº¦ï¼šLow (~0.01â€“0.05), Medium (~0.07â€“0.1), High (~0.15)
- æ¯ç§é…ç½®ç”Ÿæˆ 10 ä¸ªç½‘ç»œï¼Œå…±æ•°åç»„å®éªŒ
- èŠ‚ç‚¹ç‰¹å¾ä½¿ç”¨é‚»æ¥çŸ©é˜µçš„è¡Œå‘é‡ï¼ˆæä¾›æ‹“æ‰‘ä¿¡æ¯ï¼‰

#### çœŸå®ä¸–ç•Œæ•°æ®ï¼ˆReal-world Datasetsï¼‰
| æ•°æ®é›† | èŠ‚ç‚¹æ•° | è¾¹æ•° | æè¿° |
|--------|-------|-----|------|
| **Cora** | 2,708 | 5,429 | å¼•ç”¨ç½‘ç»œï¼Œ7 ç±»ä¸»é¢˜ |
| **Citeseer** | 3,312 | 4,715 | å¼•ç”¨ç½‘ç»œï¼Œ6 ç±»ä¸»é¢˜ |
| **PubMed** | 19,717 | 44,338 | åŒ»å­¦æ–‡çŒ®å¼•ç”¨ï¼Œ3 ç±» |
| **Actor** | 7,600 | 30,019 | æ¼”å‘˜å…±ç°ç½‘ç»œï¼Œç±»åˆ«æ¥è‡ªè§’è‰²å±æ€§ |

> æ³¨ï¼šçœŸå®æ•°æ®ä¸­çš„æ ‡ç­¾ä»…ä½œä¸ºå‚è€ƒè¾¹ç•Œï¼Œä¸è§†ä¸ºâ€œçœŸå®ç¤¾åŒºâ€ã€‚

---

### å®éªŒè®¾ç½®ä¸è¯„ä¼°æŒ‡æ ‡

#### æ¨¡å‹æ¶æ„
- GNN backboneï¼š**GraphSAGE**ï¼ˆmean aggregationï¼‰
- åæ¥ä¸¤å±‚ MLP è¾“å‡º cluster assignment matrix $ S $
- æ‰€æœ‰æ¨¡å‹å…±äº«ç›¸åŒç»“æ„ï¼Œä»… loss å‡½æ•°ä¸åŒ

#### å¯¹æ¯”æ¨¡å‹ï¼ˆLoss Function å·®å¼‚ï¼‰
| æ¨¡å‹åç§° | Loss ç»„æˆ |
|---------|----------|
| **GNN** | ä»… modularity |
| **GNN+REG** | modularity + balance regularization |
| **GNN+CONSTRAINT** | modularity + constraint($l$) |
| **GNN+REG+CONSTRAINT** | modularity + constraint($l$) + balance |

> æ‰€æœ‰å®éªŒè¿è¡Œ 3 æ¬¡ï¼ˆä¸åŒéšæœºç§å­ï¼‰ï¼ŒæŠ¥å‘Šå‡å€¼ä¸æ ‡å‡†å·®

#### è¶…å‚æ•°
- $\mu = \lambda = 1$
- å­¦ä¹ ç‡ï¼š$10^{-3}$
- ä¼˜åŒ–å™¨ï¼šAdam
- è®­ç»ƒè½®æ¬¡ï¼š3000 epochs
- GNN å±‚æ•°ï¼šåˆæˆæ•°æ®ç”¨ 3 å±‚ï¼ŒçœŸå®æ•°æ®ç”¨ 4 å±‚

#### è¯„ä¼°æŒ‡æ ‡
- **è¾“å‡ºç¤¾åŒºæ•°é‡**ï¼šæ˜¯å¦è½åœ¨ $[l, c]$ å†…ï¼Œæ˜¯å¦æ¥è¿‘ ground truth
- **Adjusted Rand Index (ARI)**ï¼šè¡¡é‡é¢„æµ‹ç¤¾åŒºä¸çœŸå®æ ‡ç­¾çš„ä¸€è‡´æ€§ï¼ˆå·²ä¿®æ­£éšæœºå› ç´ ï¼‰
  $$
  \text{ARI} \in [-1, 1], \quad \text{è¶Šé«˜è¶Šå¥½}
  $$

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### å…³é”®æ€§èƒ½æ•°æ®ä¸å¯¹æ¯”ç»“æœ

#### âœ… æˆåŠŸå¼ºåˆ¶æ»¡è¶³ç¤¾åŒºæ•°é‡çº¦æŸ
- åœ¨å‡ ä¹æ‰€æœ‰å®éªŒä¸­ï¼ˆé™¤æå°‘æ•°è¾¹ç•Œæƒ…å†µï¼‰ï¼Œ**GNN+CONSTRAINT** å’Œ **GNN+REG+CONSTRAINT** éƒ½æˆåŠŸå°†è¾“å‡ºç¤¾åŒºæ•°é™åˆ¶åœ¨ $[l, c]$ èŒƒå›´å†…ã€‚
- å›¾ 1(a) æ˜¾ç¤ºï¼šå½“ $ l \leq 5 $ï¼ˆground truthï¼‰æ—¶ï¼Œè¾“å‡ºå¤šæ ·ï¼›å½“ $ l > 5 $ï¼Œè¾“å‡ºé›†ä¸­åœ¨ $ l $ ä¸Šï¼Œè¯´æ˜çº¦æŸç”Ÿæ•ˆã€‚
- å›¾ A.3 è¡¨æ˜ï¼šå½“ $ l = c $ æ—¶ï¼Œæ¨¡å‹èƒ½å‡†ç¡®è¾“å‡ºæŒ‡å®šæ•°é‡çš„ç¤¾åŒºã€‚

#### ğŸ”º ARI è¡¨ç°ä¼˜å¼‚
- å›¾ 1(b) å’Œ å›¾ 4 æ˜¾ç¤ºï¼Œåœ¨ $ l = 4,5 $ é™„è¿‘ ARI æœ€é«˜ï¼Œç¬¦åˆé¢„æœŸï¼ˆground truth ä¸º 5ï¼‰
- å½“ $ l > 5 $ï¼ŒARI ä¸‹é™ï¼Œè¯´æ˜å¼ºè¡Œæ‹†åˆ†ä¼šç ´åè‡ªç„¶ç¤¾åŒºç»“æ„
- **GNN+REG+CONSTRAINT** çš„ ARI æ˜¾è‘—é«˜äºå…¶ä»–æ–¹æ³•ï¼ˆå°¤å…¶åœ¨ 20 ç¤¾åŒºåœºæ™¯ä¸‹ï¼‰

#### ğŸ“Š ç¤¾åŒºæ•°é‡é¢„æµ‹å¯¹æ¯”ï¼ˆå›¾ 2ï¼‰
| åœºæ™¯ | GNN / GNN+REG | GNN+CONSTRAINT / GNN+REG+CONSTRAINT |
|------|----------------|------------------------------------|
| ç¤¾åŒºæ•°å¢åŠ ï¼ˆ5â†’20ï¼‰ | è¾“å‡ºåŸºæœ¬ä¸å˜ï¼ˆçº¦ 4â€“8ï¼‰ | è¾“å‡ºç´§è´´ lower bound $ l $ |
| ç»“è®º | ä¸¥é‡ä½ä¼°çœŸå®ç¤¾åŒºæ•° | æ›´è´´è¿‘è®¾å®šä¸‹é™ï¼Œç¼“è§£ modularity åˆ†è¾¨ç‡é™åˆ¶é—®é¢˜ |

> ç‰¹åˆ«æ˜¯åœ¨ 20 ç¤¾åŒºä»»åŠ¡ä¸­ï¼š
> - GNN å’Œ GNN+REG è¾“å‡ºä»… 1â€“4 ä¸ªç¤¾åŒº
> - GNN+REG+CONSTRAINT è¾“å‡º 16 ä¸ªï¼Œä¸”éƒ¨åˆ†åŒ¹é… ground truthï¼ˆè§å›¾ 3ï¼‰

#### ğŸ’¡ æ¶ˆèå®éªŒç»“æœ
| è§‚å¯Ÿ | å‘ç° |
|------|------|
| åŠ å…¥ **constraint** å•ç‹¬æœ‰æ•ˆ | GNN+CONSTRAINT èƒ½ä¿è¯è¾“å‡º â‰¥ $ l $ ä¸ªç¤¾åŒºï¼ŒéªŒè¯å…¶ç‹¬ç«‹æœ‰æ•ˆæ€§ |
| åŠ å…¥ **balance regularization** æå‡è´¨é‡ | GNN+REG+CONSTRAINT æ¯” GNN+CONSTRAINT å¾—åˆ°æ›´é«˜çš„ ARIï¼Œå› ä¸ºåè€…å…è®¸ singleton clusters |
| ä¸¤è€…ååŒæœ€ä½³ | constraint ä¿è¯æ•°é‡ï¼Œbalance ä¿è¯è´¨é‡ â†’ æ¨èè”åˆä½¿ç”¨ |

#### â±ï¸ è¿è¡Œæ—¶é—´å¯¹æ¯”ï¼ˆè¡¨ 2 & è¡¨ 4ï¼‰
- æ‰€æœ‰æ¨¡å‹è¿è¡Œæ—¶é—´ç›¸è¿‘
- æ·»åŠ  constraint å’Œ balance é¡¹å¯¹è®¡ç®—å¼€é”€å½±å“æå°
- å¤§è§„æ¨¡ç½‘ç»œï¼ˆå¦‚ Large SBMï¼‰è€—æ—¶çº¦ 9 å°æ—¶ï¼Œä»å…·å¯è¡Œæ€§

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **GNN é»˜è®¤ä¸èƒ½å¯é è¿”å›æŒ‡å®šæ•°é‡çš„ç¤¾åŒº**ï¼Œæ™®éå­˜åœ¨â€œè¾“å‡ºç¤¾åŒºæ•° < è¾“å…¥ $ c $â€çš„ç°è±¡ã€‚
2. **æå‡ºçš„ constraint term èƒ½æœ‰æ•ˆæ–½åŠ ç¤¾åŒºæ•°é‡ä¸‹ç•Œ**ï¼Œç¡®ä¿è¾“å‡ºåœ¨ $[l, c]$ èŒƒå›´å†…ã€‚
3. å½“ $ l = c $ æ—¶ï¼Œå¯å¼ºåˆ¶è¾“å‡º**ç²¾ç¡®æ•°é‡**çš„ç¤¾åŒºï¼Œå¡«è¡¥äº† GNN åœ¨æ­¤åŠŸèƒ½ä¸Šçš„ç©ºç™½ã€‚
4. **ç»“åˆ balance regularization å¯è¿›ä¸€æ­¥æå‡èšç±»è´¨é‡**ï¼ˆå¦‚ ARIï¼‰ï¼Œé˜²æ­¢å‡ºç°å¤§é‡å•èŠ‚ç‚¹ç¤¾åŒºã€‚
5. æ–¹æ³•åœ¨å¤šç§ç½‘ç»œè§„æ¨¡ã€å¯†åº¦ã€çœŸå®/åˆæˆæ•°æ®ä¸Šå‡è¡¨ç°ç¨³å¥ã€‚

---

### æ–¹æ³•çš„å±€é™æ€§
1. **ä¾èµ– cluster assignment matrix çš„è´¨é‡**ï¼š
   - è‹¥åˆå§‹åˆ†é…æ¨¡ç³Šï¼ˆå¦‚æ‰€æœ‰æ¦‚ç‡æ¥è¿‘ï¼‰ï¼Œconstraint éš¾ä»¥å‘æŒ¥ä½œç”¨
   - å»ºè®®é€‚å½“å¢å¤§ $\mu$ æƒé‡ä»¥å¢å¼ºçº¦æŸåŠ›åº¦ï¼ˆè§ Appendix A.3ï¼‰
2. **æœªè§£å†³è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ $ k $ çš„é—®é¢˜**ï¼š
   - è™½ç„¶æ”¯æŒèŒƒå›´è¾“å…¥ï¼Œä½†ä»éœ€äººå·¥è®¾å®š $ l $ å’Œ $ c $
   - è‡ªåŠ¨æœç´¢æœ€ä¼˜ç¤¾åŒºæ•°ä»ä¸ºå¼€æ”¾é—®é¢˜
3. **æµ‹è¯•ç½‘ç»œå…·æœ‰è¾ƒå¼ºç¤¾åŒºç»“æ„**ï¼š
   - åœ¨å¼±ç¤¾åŒºç»“æ„ï¼ˆweak community signalï¼‰ä¸‹çš„è¡¨ç°å°šæœªç³»ç»Ÿè¯„ä¼°
   - å¯èƒ½é¢ä¸´ç›¸å˜é˜ˆå€¼æŒ‘æˆ˜ï¼ˆphase transitionï¼‰

---

### æœªæ¥å·¥ä½œæ–¹å‘
1. **å¼€å‘åŸºäº constraint çš„è‡ªåŠ¨æœç´¢ç­–ç•¥**ï¼š
   - å¦‚é€æ­¥ç¼©å° $[l,c]$ åŒºé—´ï¼Œå¯»æ‰¾ modularity æœ€é«˜çš„è¾“å‡º
2. **æ¢ç´¢æ›´é²æ£’çš„ constraint å½¢å¼**ï¼š
   - æ”¯æŒæœ€å°ç¤¾åŒºå¤§å°ï¼ˆminimum cluster sizeï¼‰çº¦æŸï¼ˆå·²åœ¨ Appendix A.2 æå‡ºåˆæ­¥æ–¹æ¡ˆï¼‰
3. **æ‰©å±•è‡³åŠ¨æ€å›¾æˆ–å¼‚æ„å›¾åœºæ™¯**
4. **ç†è®ºåˆ†æ constraint çš„æ”¶æ•›æ€§ä¸ç¨³å®šæ€§**

---

## æ€»ç»“

æœ¬æ–‡é’ˆå¯¹ GNN åœ¨å›¾èšç±»ä¸­**æ— æ³•å¯é æ§åˆ¶è¾“å‡ºç¤¾åŒºæ•°é‡**çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ã€å¯å¾®åˆ†çš„çº¦æŸæœºåˆ¶ï¼Œä½¿ç”¨æˆ·èƒ½å¤ŸæŒ‡å®šç¤¾åŒºæ•°é‡çš„**èŒƒå›´æˆ–ç²¾ç¡®å€¼**ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®é›†ä¸Šå‡èƒ½æœ‰æ•ˆæ‰§è¡Œçº¦æŸï¼Œå¹¶ç»“åˆ balance regularization æ˜¾è‘—æå‡äº†èšç±»è´¨é‡ã€‚è¿™ä¸€å·¥ä½œå¢å¼ºäº† GNN åœ¨æ— ç›‘ç£ç¤¾åŒºæ£€æµ‹ä»»åŠ¡ä¸­çš„å®ç”¨æ€§å’Œå¯æ§æ€§ï¼Œä¸ºåç»­è‡ªåŠ¨åŒ–ç¤¾åŒºå‘ç°æä¾›äº†é‡è¦åŸºç¡€ã€‚

</details>

---

### 14. [Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws](https://arxiv.org/abs/2512.05817)

**Authors**: Zhengquan Luo, Zhiqiang Xu  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.05817v1  

#### Abstract
Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradi...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws è®ºæ–‡æ€»ç»“

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### è§£å†³çš„é—®é¢˜
- **ç†è®ºç¢ç‰‡åŒ–**ï¼šç°æœ‰çš„ Dataset Distillation (DD) æ–¹æ³•ï¼ˆå¦‚ Gradient Matching, Distribution Matching, Trajectory Matchingï¼‰åŸºäºä¸åŒçš„ä»£ç†ç›®æ ‡ï¼ˆsurrogate objectivesï¼‰å’Œä¼˜åŒ–å‡è®¾ï¼Œç¼ºä¹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶æ¥è§£é‡Šå…¶å…±æ€§ã€‚
- **é…ç½®é²æ£’æ€§ç¼ºå¤±**ï¼šå½“å‰ DD æ–¹æ³•åœ¨è®­ç»ƒé…ç½®ï¼ˆå¦‚ optimizerã€architectureã€augmentationï¼‰å‘ç”Ÿå˜åŒ–æ—¶è¡¨ç°ä¸ç¨³å®šï¼Œç¼ºä¹å¯¹â€œé…ç½®å˜åŒ–ä¸‹ä»èƒ½ä¿æŒæ€§èƒ½â€çš„ç†è®ºåˆ»ç”»ã€‚
- **ç»éªŒç°è±¡ç¼ºä¹è§£é‡Š**ï¼šä¾‹å¦‚ï¼Œéšç€ distilled sample æ•°é‡å¢åŠ ï¼Œæ€§èƒ½æå‡å‡ºç°é¥±å’Œï¼ˆIPC saturationï¼‰ï¼Œä½†ç¼ºä¹ç†è®ºè§£é‡Šã€‚

### æå‡ºçš„æ–°æ–¹æ³•ä¸æ–°æ€è·¯
ä½œè€…æå‡ºäº†ä¸€ä¸ª**ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶**â€”â€”**Configuration-Dynamics-Error Framework**ï¼Œå°†ä¸»æµ DD æ–¹æ³•çº³å…¥åŒä¸€ Generalization Error åˆ†æè§†è§’ï¼Œå¹¶å¾—å‡ºä¸¤ä¸ªæ ¸å¿ƒå®šå¾‹ï¼š

#### ï¼ˆ1ï¼‰Scaling Lawï¼ˆå•é…ç½®ç¼©æ”¾å¾‹ï¼‰
- å½“é…ç½®å›ºå®šæ—¶ï¼Œæ³›åŒ–è¯¯å·®éšè’¸é¦æ ·æœ¬æ•° $k$ çš„å¢é•¿è€Œä¸‹é™ï¼Œéµå¾ªï¼š
  $$
  \Delta \leq O\left(\frac{1}{\sqrt{k}}\right) + \epsilon_{\text{bound}}
  $$
- $\epsilon_{\text{bound}}$ æ˜¯ç”±é…ç½®å†³å®šçš„ä¸å¯çº¦è¯¯å·®ä¸‹é™ï¼Œè§£é‡Šäº†ä¸ºä½•å¢å¤§ $k$ åˆ°ä¸€å®šç¨‹åº¦åæ€§èƒ½ä¸å†æå‡ï¼ˆå³ IPC saturationï¼‰ã€‚

#### ï¼ˆ2ï¼‰Coverage Lawï¼ˆé…ç½®è¦†ç›–å¾‹ï¼‰
- å½“è€ƒè™‘å¤šä¸ªç›®æ ‡é…ç½®æ„æˆçš„é…ç½®æ— $\mathcal{A}$ æ—¶ï¼Œæ‰€éœ€è’¸é¦æ ·æœ¬æ•° $k$ å¿…é¡»ä¸é…ç½®å¤šæ ·æ€§ $H_{\text{cov}}(\mathcal{A}, r)$ æˆ**çº¿æ€§å…³ç³»**æ‰èƒ½ç»´æŒæ³›åŒ–æ€§èƒ½ï¼š
  $$
  k = \Omega(H_{\text{cov}}(\mathcal{A}, r))
  $$
- è¿™æ˜¯é¦–æ¬¡å½¢å¼åŒ–å®šä¹‰äº† DD çš„â€œæ•ˆç”¨è¾¹ç•Œâ€ï¼ˆutility boundaryï¼‰ï¼Œå³ $k$ ä¸é…ç½®å¤šæ ·æ€§çš„æƒè¡¡å…³ç³»ã€‚

#### ç»Ÿä¸€è§†è§’ï¼šSurrogates Are Interchangeable
- è¯æ˜ GMã€DMã€TM æœ¬è´¨ä¸Šéƒ½æ˜¯é€šè¿‡åŒå±‚ä¼˜åŒ–æœºåˆ¶å‡å°‘åŒä¸€ä¸ªâ€œåŒ¹é…å·®å¼‚â€ï¼ˆalignment discrepancy $\Delta_a$ï¼‰çš„ä¸åŒä»£ç†æ–¹å¼ã€‚
- å®ƒä»¬åœ¨ç†è®ºä¸Šæ˜¯å¯äº’æ¢çš„ï¼ˆexchangeableï¼‰ï¼Œåªæ˜¯åœ¨æ ·æœ¬æ•ˆç‡å’Œé²æ£’æ€§ä¸Šæœ‰æ‰€æƒè¡¡ã€‚

### ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿
| æ–¹é¢ | ä¼˜åŠ¿ |
|------|------|
| **ç†è®ºç»Ÿä¸€æ€§** | é¦–æ¬¡å°† GM/DM/TM ç»Ÿä¸€åœ¨ä¸€ä¸ª generalization error bound æ¡†æ¶ä¸‹ï¼Œæ­ç¤ºå…¶æœ¬è´¨ä¸€è‡´æ€§ |
| **å¯è§£é‡Šæ€§å¢å¼º** | è§£é‡Šäº† IPC saturation å’Œè·¨é…ç½®æ€§èƒ½é€€åŒ–ç­‰ç»éªŒç°è±¡ |
| **è®¾è®¡æŒ‡å¯¼æ„ä¹‰** | æä¾›äº†å¦‚ä½•é€‰æ‹© $k$ å’Œ surrogate çš„åŸåˆ™ï¼šå‡å°‘ $\epsilon_{\text{bound}}$ æ¯”ç›²ç›®å¢å¤§æ•°æ®æ›´æœ‰æ•ˆï¼›$k$ åº”éšé…ç½®å¤šæ ·æ€§çº¿æ€§å¢é•¿ |
| **æ™®é€‚æ€§å¼º** | ä¸ä¾èµ–ç‰¹å®šæ¨¡å‹æˆ–ä¼˜åŒ–å™¨ï¼Œé€‚ç”¨äºå¤šç§ DD èŒƒå¼ |

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### ä½¿ç”¨çš„æ•°æ®é›†
- **MNIST**
- **CIFAR-10 / CIFAR-100**
- **ImageNette**ï¼ˆImageNet çš„ 10 ç±»å­é›†ï¼‰

### å®éªŒè®¾ç½®
- **è’¸é¦æ–¹æ³•**ï¼š
  - Gradient Matching (GM): DC, DSA
  - Distribution Matching (DM): DM
  - Trajectory Matching (TM): MTT
  - æ‰©æ•£æ¨¡å‹æ–¹æ³•ï¼šMGD3ï¼ˆç”¨äº ImageNetteï¼‰
- **æºé…ç½®ï¼ˆsource configurationï¼‰**ï¼šå§‹ç»ˆä¸º `ConvNet + SGD`ï¼ˆå¯ç”¨ DSA æ—¶ä¹Ÿä¸€è‡´ï¼‰
- **ç›®æ ‡é…ç½®ï¼ˆtarget configurationï¼‰**ï¼šå¤šæ ·åŒ–ç»„åˆ
  - æ¶æ„ï¼šConvNet, LeNet, ResNet-18, AlexNet, MLP, VGG11
  - ä¼˜åŒ–å™¨ï¼šSGD, Adam
  - æ•°æ®å¢å¼ºï¼šå¯ç”¨/ç¦ç”¨ DSA
- **è’¸é¦æ ·æœ¬æ•° $k$**ï¼šæŒ‰ IPCï¼ˆimages per classï¼‰ä» 2 åˆ° 200 ä¸ç­‰ï¼Œ$k = \text{IPC} \times \#\text{classes}$

### è¯„ä¼°æŒ‡æ ‡
- **Generalization Error Gap**ï¼š
  $$
  \Delta_a = |R(\theta_s^{(a)}) - R(\theta_T^{(a)})|
  $$
  å³åœ¨ç›¸åŒç›®æ ‡é…ç½® $a$ ä¸‹ï¼Œä½¿ç”¨è’¸é¦æ•°æ®è®­ç»ƒçš„æ¨¡å‹ä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ä¹‹é—´çš„æµ‹è¯•è¯¯å·®å·®ã€‚
- **Scaling Law éªŒè¯**ï¼šç»˜åˆ¶ $\Delta_a$ vs. $1/\sqrt{k}$ï¼Œè¿›è¡Œçº¿æ€§å›å½’ï¼ŒæŠ¥å‘Šæ–œç‡ã€æˆªè·ã€$R^2$
- **Coverage Law éªŒè¯**ï¼šè¿‘ä¼¼é…ç½®å¤æ‚åº¦ $H_{\text{cov}} \approx \log M$ï¼Œå…¶ä¸­ $M$ æ˜¯é‡‡æ ·é…ç½®æ•°ã€‚ç»˜åˆ¶ $\Delta(k, M)$ vs. $\sqrt{\log M}/\sqrt{k}$ï¼ŒéªŒè¯çº¿æ€§å…³ç³»

### åŸºçº¿æ–¹æ³•å¯¹æ¯”
- å¯¹æ¯”äº†å››ç§ä»£è¡¨æ€§ DD æ–¹æ³•ï¼šDC (GM), DSA (GM), DM (DM), MTT (TM), MGD3 (diffusion)
- æ‰€æœ‰æ–¹æ³•å‡ä½¿ç”¨å®˜æ–¹å¼€æºå®ç°ï¼Œé»˜è®¤è¶…å‚ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### Scaling Law å®éªŒç»“æœï¼ˆå•é…ç½®ï¼‰
- åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šï¼Œ$\Delta_a$ ä¸ $1/\sqrt{k}$ å‘ˆç°å‡ºé«˜åº¦çº¿æ€§çš„è´Ÿç›¸å…³å…³ç³»ï¼Œ$R^2$ å¤šæ•° > 0.85ï¼ŒéªŒè¯äº†ç†è®ºé¢„æµ‹ã€‚
- **å…¸å‹ç»“æœ**ï¼š
  - **MNIST**ï¼šDC/DSA/DM æ‹Ÿåˆæä½³ ($R^2 = 0.90â€“0.99$)ï¼ŒMTT æ–œç‡æ›´å¤§ä¸”æ–¹å·®é«˜ï¼Œè¡¨æ˜è½¨è¿¹ä¸ç¨³å®šæ€§ã€‚
  - **CIFAR-10**ï¼šæ‰€æœ‰æ–¹æ³•å‡ç¬¦åˆè§„å¾‹ï¼ŒDC/DSA æœ€ç¨³å®šã€‚
  - **CIFAR-100**ï¼šDC/DSA ä»ä¿æŒé«˜çº¿æ€§ ($R^2 > 0.92$)ï¼ŒDM å’Œ MTT åç¦»æ›´æ˜æ˜¾ï¼Œè¯´æ˜å¤æ‚æ•°æ®æ”¾å¤§è½¨è¿¹è¯¯å·®ã€‚
  - **ImageNette**ï¼šæ‰©æ•£æ–¹æ³• MGD3 ä¹Ÿç¬¦åˆ $1/\sqrt{k}$ è§„å¾‹ï¼Œè¡¨æ˜è¯¥å¾‹è¶…è¶Šä¼ ç»ŸåŒ¹é…èŒƒå¼ã€‚

### Coverage Law å®éªŒç»“æœï¼ˆè·¨é…ç½®ï¼‰
- ç»˜åˆ¶ $\Delta(k, M)$ vs. $\sqrt{\log M}/\sqrt{k}$ï¼Œè§‚å¯Ÿåˆ°è¿‘ä¼¼çº¿æ€§è¶‹åŠ¿ï¼Œæ”¯æŒ Coverage Lawã€‚
- **ä¸åŒæ–¹æ³•çš„æ•æ„Ÿæ€§æ’åº**ï¼ˆæ–œç‡è¶Šå¤§è¶Šæ•æ„Ÿï¼‰ï¼š
  - **MNIST**ï¼šMTT ($\beta_1=0.91$) >> DM ($0.14$) > DC/DSA ($\sim 0.20$)
  - **CIFAR-10**ï¼šé¡ºåºä¸º MTT < DC < DSA < DMï¼Œåæ˜  DM æ›´é²æ£’äºé…ç½®å˜åŒ–
  - **CIFAR-100**ï¼šæ‹Ÿåˆåº¦é™ä½ï¼ˆå› ç±»å†…å·®å¼‚å¤§å¯¼è‡´å™ªå£°å¤šï¼‰ï¼Œä½†ä»å¯è§è¶‹åŠ¿
- **ç»“è®º**ï¼š
  - GM æ–¹æ³•åœ¨å•ä¸€é…ç½®ä¸‹é«˜æ•ˆï¼Œä½†åœ¨è·¨é…ç½®æ—¶é€€åŒ–å¿«
  - DM æ›´é²æ£’äºé…ç½®å¤šæ ·æ€§
  - TM æœ€ä¸ç¨³å®šï¼Œå°¤å…¶åœ¨å¤æ‚æ•°æ®é›†ä¸Š

### æ¶ˆèå®éªŒä¸åˆ†æ
- **æ— æ˜¾å¼æ¶ˆèå®éªŒ**ï¼Œä½†é€šè¿‡ä¸åŒæ–¹æ³•çš„è¡Œä¸ºå·®å¼‚é—´æ¥éªŒè¯äº†ç†è®ºï¼š
  - Surrogate choice å½±å“æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ï¼Œä½†æœ€ç»ˆéƒ½å—ç›¸åŒ scaling law æ”¯é…
  - $k$ å¢åŠ åˆæœŸè¯¯å·®å¿«é€Ÿä¸‹é™ï¼ŒåæœŸè¶‹äºå¹³ç¼“ï¼ˆæ¥è¿‘ $\epsilon_{\text{bound}}$ï¼‰ï¼ŒéªŒè¯ saturation ç°è±¡
  - ä½¿ç”¨ $\log M$ ä½œä¸º $H_{\text{cov}}$ çš„ä»£ç†å¯è¡Œï¼Œé¿å…æ˜‚è´µçš„è·ç¦»è®¡ç®—

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### ä¸»è¦å‘ç°
1. **DD æ–¹æ³•æœ¬è´¨ç»Ÿä¸€**ï¼šGMã€DMã€TM å¹¶éç‹¬ç«‹å¯å‘å¼ï¼Œè€Œæ˜¯é€šè¿‡ä¸åŒä»£ç†å‡å°‘åŒä¸€â€œåŒ¹é…å·®å¼‚â€çš„æ‰‹æ®µï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹å…·æœ‰ç›¸åŒçš„æ³›åŒ–è¡Œä¸ºã€‚
2. **Scaling Law å­˜åœ¨**ï¼šæ³›åŒ–è¯¯å·®éš $1/\sqrt{k}$ ä¸‹é™å¹¶è¶‹äºé¥±å’Œï¼Œè§£é‡Šäº† IPC saturation ç°è±¡ã€‚
3. **Coverage Law æˆç«‹**ï¼šè’¸é¦æ ·æœ¬æ•° $k$ å¿…é¡»ä¸é…ç½®å¤šæ ·æ€§ $H_{\text{cov}}$ æˆçº¿æ€§å¢é•¿ï¼Œå¦åˆ™æ— æ³•ä¿è¯è·¨é…ç½®æ€§èƒ½ã€‚
4. **è®¾è®¡å¯ç¤º**ï¼š
   - å‡å°‘ä¸å¯çº¦è¯¯å·® $\epsilon_{\text{bound}}$ æ¯”å•çº¯å¢åŠ  $k$ æ›´é‡è¦
   - é¢å‘å¤šæ ·åŒ–éƒ¨ç½²åœºæ™¯æ—¶ï¼Œåº”æ ¹æ®é…ç½®å¤šæ ·æ€§è§„åˆ’ $k$

### æ–¹æ³•çš„å±€é™æ€§
1. **ä¼˜åŒ–åŠ¨åŠ›å­¦å‡è®¾è¾ƒå¼º**ï¼šä¾èµ– PL æ¡ä»¶å’Œ Lipschitz è¿ç»­æ€§ï¼Œå¯èƒ½ä¸é€‚ç”¨äº AdamWã€å¤§æ‰¹æ¬¡è®­ç»ƒç­‰éå‡¸æˆ–è‡ªé€‚åº”åœºæ™¯ã€‚
2. **é…ç½®è·ç¦»ä¼°è®¡å›°éš¾**ï¼š$d_A$ å’Œ $H_{\text{cov}}$ çš„ç²¾ç¡®è®¡ç®—æ˜¯ NP-hardï¼Œå®éªŒä¸­ä½¿ç”¨ $\log M$ ä»…ä¸ºä»£ç†ï¼Œå¯èƒ½ä½ä¼°å®é™…å¤šæ ·æ€§ã€‚
3. **æœªæ¶µç›–æ•°æ®åˆ†å¸ƒåç§»**ï¼šå½“å‰æ¡†æ¶èšç„¦ç®—æ³•é…ç½®å˜åŒ–ï¼Œæœªè€ƒè™‘åŸŸè¿ç§»ã€ç±»åˆ«ä¸å¹³è¡¡ç­‰è¯­ä¹‰å±‚é¢çš„å˜åŒ–ã€‚

### æœªæ¥å·¥ä½œæ–¹å‘
1. **æ”¾å®½ä¼˜åŒ–å‡è®¾**ï¼šæ‰©å±•è‡³æ›´å¼±æ¡ä»¶ï¼ˆå¦‚ one-point convexity æˆ– uniform stabilityï¼‰ä¸‹çš„ç†è®ºåˆ†æã€‚
2. **æ”¹è¿› $H_{\text{cov}}$ ä¼°è®¡**ï¼šå¼€å‘åŸºäºèšç±»æˆ–åµŒå…¥ç©ºé—´çš„å¯å­¦ä¹ é…ç½®å¤æ‚åº¦ä¼°è®¡å™¨ã€‚
3. **æ‰©å±•é…ç½®å®šä¹‰**ï¼šå°†æ•°æ®åˆ†å¸ƒå·®å¼‚ï¼ˆå¦‚ Wasserstein è·ç¦»ï¼‰çº³å…¥é…ç½®ç©ºé—´ï¼Œç ”ç©¶è·¨åŸŸåœºæ™¯ä¸‹çš„ coverage lawã€‚
4. **æŒ‡å¯¼é²æ£’ DD è®¾è®¡**ï¼šåŸºäº coverage law å¼€å‘è‡ªé€‚åº”è’¸é¦ç­–ç•¥ï¼ŒåŠ¨æ€åˆ†é…åŸå‹ä»¥è¦†ç›–é«˜å¤šæ ·æ€§é…ç½®ã€‚

--- 

> **æ€»ç»“**ï¼šæœ¬æ–‡æå‡ºäº†é¦–ä¸ªç»Ÿä¸€çš„ Dataset Distillation ç†è®ºæ¡†æ¶ï¼Œæ­ç¤ºäº† **Scaling Law** ä¸ **Coverage Law** ä¸¤å¤§åŸºæœ¬è§„å¾‹ï¼Œä¸ä»…è§£é‡Šäº†é•¿æœŸå­˜åœ¨çš„ç»éªŒç°è±¡ï¼Œè¿˜ä¸ºæ„å»º**é«˜æ•ˆä¸”é…ç½®é²æ£’**çš„è’¸é¦æ–¹æ³•æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€å’Œè®¾è®¡æŒ‡å—ã€‚

</details>

---

### 15. [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)

**Authors**: Chuang Yu, Jinmiao Zhao, Mingxuan Zhao, Yunpeng Liu, Xiujun Shu, Yuanhao Feng, Bo Wang, Xiangyu Yue  
**Category**: cs.AI  
**Published**: 2025-12-08  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.05530v1  

#### Abstract
Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-...

<details>
<summary><strong>ğŸ¤– AI Summary (by qwen-long)</strong> - Click to expand</summary>

# è®ºæ–‡æ€»ç»“ï¼šMIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models

---

## 1. è®ºæ–‡çš„ä¸»è¦è´¡çŒ®å’Œåˆ›æ–°ç‚¹

### **è§£å†³äº†ä»€ä¹ˆé—®é¢˜**

å½“å‰çš„ **Multimodal Large Language Models (MLLMs)** åœ¨æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨ä»¥ä¸‹å…³é”®ç¼ºé™·ï¼š

- **å•ç†ç”±ç›‘ç£ï¼ˆSingle-rationale supervisionï¼‰** å¯¼è‡´æ¨¡å‹ä»…å­¦ä¹ è¡¨é¢æ˜ å°„å…³ç³»ï¼Œæ— æ³•æ•æ‰äººç±»æ¨ç†çš„å¤šæ ·æ€§ä¸å¤æ‚æ€§ã€‚
- ç¼ºä¹å¯¹é”™è¯¯æˆ–è¯¯å¯¼æ€§è§£é‡Šçš„è¯†åˆ«ä¸çº æ­£èƒ½åŠ›ï¼Œé€»è¾‘é²æ£’æ€§å¼±ã€‚
- é¢å¯¹æ¨¡ç³Šæˆ–å¤šä¹‰åœºæ™¯æ—¶ï¼Œå®¹æ˜“äº§ç”Ÿâ€œé€»è¾‘æ¼‚ç§»â€æˆ–â€œå¹»è§‰â€ï¼Œç¼ºä¹ä¸»åŠ¨åˆ¤åˆ«ä¸è‡ªæˆ‘ä¿®æ­£æœºåˆ¶ã€‚

è¿™äº›é—®é¢˜é™åˆ¶äº† MLLMs å‘çœŸæ­£è®¤çŸ¥æ™ºèƒ½çš„æ¼”è¿›ã€‚

---

### **æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•æˆ–æ–°æ€è·¯**

ä½œè€…æå‡º **MIND**ï¼ˆMulti-rationale INtegrated Discriminativeï¼‰æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨èµ‹äºˆ MLLMs ç±»ä¼¼äººç±»çš„â€œç†è§£ â†’ åæ€ â†’ çº æ­£â€ï¼ˆ*Understand â†’ Rethink â†’ Correct*ï¼‰çš„è®¤çŸ¥èƒ½åŠ›ï¼Œå®ç°ä»è¢«åŠ¨æ¨¡ä»¿åˆ°ä¸»åŠ¨åˆ¤åˆ«å¼æ¨ç†çš„èŒƒå¼è½¬å˜ã€‚

#### æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š

1. **Rationale Augmentation and Discrimination (RAD) èŒƒå¼**
   - è‡ªåŠ¨é«˜æ•ˆåœ°ç”Ÿæˆå¤šæ ·åŒ–çš„**æ­£å‘ç†ç”±ï¼ˆpositive rationalesï¼‰** å’Œè¯­ä¹‰åè½¬çš„**è´Ÿå‘ç†ç”±ï¼ˆnegative rationalesï¼‰**ã€‚
   - æ„å»ºå¤šç†ç”±è®­ç»ƒæ ·æœ¬ï¼Œæä¾›ç»Ÿä¸€ä¸”å¯æ‰©å±•çš„æ•°æ®åŸºç¡€ã€‚
   - é€šè¿‡æ‰¹é‡ç”Ÿæˆæœºåˆ¶æå‡æ•ˆç‡ï¼Œå¹¶è®¾è®¡æ­£/è´Ÿæç¤ºæ¨¡æ¿æ§åˆ¶è¯­ä¹‰è´¨é‡ã€‚

2. **Progressive Two-stage Correction Learning (P2CL) ç­–ç•¥**
   - **é˜¶æ®µ I (P2CL-I)**ï¼šå¤šç†ç”±æ­£å‘å­¦ä¹ ï¼ˆMulti-rationale positive learningï¼‰ï¼Œå¢å¼ºè¯­ä¹‰ç†è§£ä¸é€»è¾‘å»ºæ¨¡ã€‚
   - **é˜¶æ®µ II (P2CL-II)**ï¼šä¸»åŠ¨é€»è¾‘åˆ¤åˆ«ä¸çº æ­£ï¼ˆActive logic discrimination and correctionï¼‰ï¼Œè¾“å…¥å¯èƒ½é”™è¯¯çš„ç†ç”±å¹¶è¦æ±‚æ¨¡å‹è¾“å‡ºæ­£ç¡®ç­”æ¡ˆä¸ä¿®æ­£åçš„ç†ç”±ï¼Œå½¢æˆè‡ªçœä¸ä¿®å¤æœºåˆ¶ã€‚

3. **Multi-rationale Contrastive Alignment (MCA) ä¼˜åŒ–ç­–ç•¥**
   - åœ¨åµŒå…¥ç©ºé—´ä¸­æ‹‰è¿‘æ­£ç¡®ç†ç”±ï¼ˆhard positivesï¼‰ï¼Œæ¨å¼€é”™è¯¯ç†ç”±ï¼ˆhard negativesï¼‰ã€‚
   - é‡‡ç”¨â€œéš¾ä¾‹æŒ–æ˜â€æœºåˆ¶ï¼ˆBottom-k æ­£ä¾‹ / Top-k è´Ÿä¾‹ï¼‰å¢å¼ºåˆ¤åˆ«è¾¹ç•Œã€‚
   - æ˜¾è‘—æå‡è¯­ä¹‰ä¸€è‡´æ€§ä¸é€»è¾‘æ•æ„Ÿåº¦ã€‚

---

### **ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿**

| ç»´åº¦ | ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ Multimodal-CoTï¼‰ | MIND |
|------|-------------------------------|------|
| ç›‘ç£ä¿¡å· | å•ä¸€æ ‡å‡†ç†ç”± | å¤šæ ·æ­£å‘ + æŒ‘æˆ˜æ€§è´Ÿå‘ç†ç”± |
| æ¨ç†æ¨¡å¼ | è¢«åŠ¨æ¨¡ä»¿ | ä¸»åŠ¨åˆ¤åˆ«ä¸è‡ªæˆ‘çº æ­£ |
| è¯­ä¹‰å»ºæ¨¡ | è¡¨é¢æ˜ å°„ | å¤šç†ç”±è¯­ä¹‰èšåˆä¸å†²çªåˆ†ç¦» |
| æ•°æ®æ„å»º | æ‰‹å·¥æ ‡æ³¨ | è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„ RAD èŒƒå¼ |
| æ³›åŒ–ä¸é²æ£’æ€§ | å¼± | å¼ºï¼Œå°¤å…¶åœ¨å¤æ‚/æ¨¡ç³Šåœºæ™¯ |

MIND å®ç°äº†ä»â€œ**Imitation-based Reasoning**â€ åˆ° â€œ**Discriminative Reasoning**â€ çš„èŒƒå¼è·ƒè¿ã€‚

---

## 2. æ ¸å¿ƒå®éªŒæ–¹æ³•å’Œè®¾ç½®

### **ä½¿ç”¨çš„æ•°æ®é›†**

å®éªŒè¦†ç›–ç§‘å­¦ã€å¸¸è¯†ã€æ•°å­¦ä¸‰å¤§é¢†åŸŸï¼Œä½¿ç”¨ä»¥ä¸‹ä¸‰ä¸ªå…¬å¼€ VQA æ•°æ®é›†åŠå…¶ RAD æ‰©å±•ç‰ˆæœ¬ï¼š

| æ•°æ®é›† | é¢†åŸŸ | åŸå§‹æ ·æœ¬æ•° | RAD æ‰©å±•å€æ•° |
|--------|------|------------|--------------|
| **ScienceQA** | ç§‘å­¦ | 21,208 | Ã—1000 |
| **A-OKVQA** | å¸¸è¯† | 24,903 | Ã—1000 |
| **MÂ³CoT** | ç§‘å­¦+å¸¸è¯†+æ•°å­¦ | 11,328 | Ã—500 |

æ‰€æœ‰æ•°æ®åˆ’åˆ†ä¿æŒåŸå§‹è®¾å®šä»¥ç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚

---

### **å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡**

- **æ¨¡å‹æ¶æ„**ï¼šåŸºäº T5 çš„ encoder-decoder æ¶æ„ï¼Œæ¢ç´¢ Base (223M) å’Œ Large (738M) ä¸¤ç§è§„æ¨¡ã€‚
- **è§†è§‰ç¼–ç å™¨**ï¼šå†»ç»“çš„ BLIP2-flan-t5-xxl æå–å›¾åƒç‰¹å¾ã€‚
- **å›¾åƒæè¿°ç”Ÿæˆ**ï¼šQwen2.5-VL-72Bã€‚
- **è®­ç»ƒå‚æ•°**ï¼š
  - å­¦ä¹ ç‡ï¼š8e-5
  - Batch sizeï¼š8
  - åºåˆ—é•¿åº¦ï¼š512
  - Epochsï¼šScienceQA-RAD ä¸º 200ï¼Œå…¶ä½™ä¸º 400
- **ç¡¬ä»¶**ï¼š8Ã—NVIDIA H20 GPU (96GB)
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ï¼ˆAccuracyï¼‰

---

### **åŸºçº¿æ–¹æ³•å¯¹æ¯”**

æ¶µç›–å¤šç§ä¸»æµæ–¹æ³•ç±»åˆ«ï¼š

- **Few-shot CoT æ–¹æ³•**ï¼šGPT-3.5, ChatGPT, GPT-4, IPVR
- **Fine-tuning æ–¹æ³•**ï¼šMCAN, ViLT, VisualBERT, LLaVA, LaVIN
- **Multimodal-CoT ç±»æ–¹æ³•**ï¼šMultimodal-CoT, MC-CoT, DPMM-CoT, T-SciQ
- **å·¥å…·å¢å¼ºæ–¹æ³•**ï¼šHuggingGPT, VoT
- **é›¶æ ·æœ¬å¤§æ¨¡å‹**ï¼šGPT-4V, Gemini

---

## 3. ä¸»è¦å®éªŒç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

### **å…³é”®æ€§èƒ½æ•°æ®**

| æ•°æ®é›† | MIND-base å‡†ç¡®ç‡ | å½“å‰ SOTAï¼ˆæ­¤å‰ï¼‰ | æå‡å¹…åº¦ |
|--------|------------------|------------------|----------|
| **ScienceQA** | **92.29%** | 91.97% (T-SciQ) | +0.32% |
| **A-OKVQA** | **70.6%** | ~58.7% (IPVR+GPT-3) | +11.9% |
| **MÂ³CoT** | **57.38%** | 44.85% (Multimodal-CoT) | **+12.53%** |

> æ³¨ï¼šMIND-base å‚æ•°é‡ä»…ä¸º 223Mï¼Œè¿œå°äºå¤šæ•°å¯¹æ¯”æ¨¡å‹ï¼ˆå¦‚ GPT-4, LLaVA-13Bï¼‰ã€‚

---

### **ä¸åŸºçº¿æ–¹æ³•çš„å¯¹æ¯”ç»“æœ**

#### âœ… ScienceQA ç»“æœï¼ˆè§ Table 1ï¼‰
- ç›¸æ¯” **Multimodal-CoTbase**ï¼ˆåŒå‚æ•°é‡ï¼‰ï¼š**+6.98%**ï¼ˆ85.31% â†’ 92.29%ï¼‰
- è¶…è¿‡ **LLaVA**ï¼ˆ13Bï¼‰ï¼š+1.37%
- è¶…è¿‡ **GPT-4 w/CoT**ï¼š+8.3%

#### âœ… A-OKVQA ç»“æœï¼ˆè§ Table 2ï¼‰
- ç›¸æ¯” **Multimodal-CoTbase**ï¼š**+20.0%**ï¼ˆ50.6% â†’ 70.6%ï¼‰
- è¶…è¿‡æœ€ä½³ Few-shot æ–¹æ³•ï¼ˆIPVR+GPT-3ï¼‰ï¼š+11.9%

#### âœ… MÂ³CoT ç»“æœï¼ˆè§ Table 3ï¼‰
- ç›¸æ¯” **Multimodal-CoTbase**ï¼š**+12.53%**
- ç›¸æ¯” **MC-CoTbase**ï¼š+3.87%
- **MIND-large** è¿›ä¸€æ­¥æå‡è‡³ **61.56%**ï¼Œè¶…è¶Š GPT-4Vï¼ˆ56.95%ï¼‰è¿‘ **4.61%**ï¼Œä¸”å‚æ•°æ›´å°ã€‚

---

### **æ¶ˆèå®éªŒç»“æœï¼ˆAblation Studiesï¼‰**

#### ğŸ”¹ åˆ†é¡¹æ¶ˆèï¼ˆTable 4 & S1ï¼‰

| é…ç½® | ScienceQA Acc. | æå‡ |
|------|----------------|------|
| Baseline | 90.29% | â€” |
| + MCA only | 90.36% | +0.07% |
| + P2CL only | 92.15% | +1.86% |
| **å®Œæ•´ MIND (P2CL + MCA)** | **92.29%** | **+2.00%** |

> è¡¨æ˜ P2CL æ˜¯ä¸»é©±åŠ¨åŠ›ï¼ŒMCA æä¾›äº’è¡¥å¢ç›Šï¼ŒäºŒè€…ååŒå®ç°â€œ1+1 > 2â€ã€‚

#### ğŸ”¹ ç†ç”±è´¨é‡ä¸æ•°é‡å½±å“ï¼ˆTable 5 & 6ï¼‰

- **ç†ç”±ç”Ÿæˆæ¨¡å‹è¶Šå¼ºï¼Œæ€§èƒ½è¶Šé«˜**ï¼š
  - DeepSeek-R1-Qwen8B â†’ 91.49%
  - Qwen3-235B-22A â†’ **91.61%**
  - â€œFinal Mixâ€ï¼ˆå¤šæºæ··åˆï¼‰â†’ **92.29%**
- **ç†ç”±æ•°é‡å¢åŠ å¸¦æ¥æŒç»­æå‡ï¼Œä½†åœ¨ Ã—1000 åè¶‹äºé¥±å’Œ**ï¼š
  - åŸå§‹ 21K â†’ Ã—1000 (21M)ï¼š**+2.00%**

#### ğŸ”¹ P2CL å„é˜¶æ®µä½œç”¨åˆ†æï¼ˆTable 8ï¼‰

- ç§»é™¤ P2CL-Iï¼ˆæ­£å‘å­¦ä¹ ï¼‰ï¼šæ€§èƒ½ä¸‹é™ **0.66%**
- ç§»é™¤ P2CL-IIï¼ˆçº é”™æœºåˆ¶ï¼‰ï¼šæ€§èƒ½ä¸‹é™ **1.93%**
- è¾“å…¥è´Ÿç†ç”±ä½†ç›‘ç£æ­£ç†ç”±ï¼ˆNeg â†’ Posï¼‰ï¼šæ˜¾è‘—æå‡è‡³ 91.72%ï¼ŒéªŒè¯çº é”™æœ‰æ•ˆæ€§ã€‚

---

## 4. å…³é”®ç»“è®ºå’Œå‘ç°

### **ä¸»è¦å‘ç°**

1. **å¤šç†ç”±è®­ç»ƒæ˜¯æå‡æ¨ç†é²æ£’æ€§çš„å…³é”®**ï¼š
   - æ¨¡å‹éœ€åŒæ—¶å­¦ä¹ â€œä»€ä¹ˆæ˜¯æ­£ç¡®çš„â€å’Œâ€œä»€ä¹ˆæ˜¯é”™è¯¯çš„â€ï¼Œæ‰èƒ½å»ºç«‹åˆ¤åˆ«è¾¹ç•Œã€‚

2. **â€œç†è§£ â†’ åæ€ â†’ çº æ­£â€è®¤çŸ¥å¾ªç¯æœ‰æ•ˆ**ï¼š
   - P2CL ç­–ç•¥æˆåŠŸæ¨¡æ‹Ÿäººç±»åæ€è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹å…·å¤‡ä¸»åŠ¨è¯†åˆ«å¹¶ä¿®æ­£é”™è¯¯é€»è¾‘çš„èƒ½åŠ›ã€‚

3. **MCA æ˜¾è‘—å¢å¼ºè¯­ä¹‰åˆ¤åˆ«åŠ›**ï¼š
   - å¯¹æ¯”å­¦ä¹ æœºåˆ¶å¸®åŠ©æ¨¡å‹åœ¨é«˜ç»´ç©ºé—´ä¸­æ¸…æ™°åˆ†ç¦»æ­£ç¡®ä¸é”™è¯¯æ¨ç†è·¯å¾„ã€‚

4. **è‡ªåŠ¨åŒ–æ•°æ®å¢å¼ºï¼ˆRADï¼‰å¯è¡Œä¸”é«˜æ•ˆ**ï¼š
   - åˆ©ç”¨å¤§æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡æ­£/è´Ÿç†ç”±ï¼Œå¤§å¹…é™ä½äººå·¥æ ‡æ³¨æˆæœ¬ï¼Œæ”¯æŒå¯æ‰©å±•è®­ç»ƒã€‚

5. **å°æ¨¡å‹ä¹Ÿèƒ½å®ç° SOTA æ€§èƒ½**ï¼š
   - MIND-baseï¼ˆ223Mï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šæ•°åäº¿å‚æ•°çš„å¤§æ¨¡å‹ï¼Œè¯æ˜å…¶æ¨ç†æ•ˆç‡ä¼˜åŠ¿ã€‚

---

### **æ–¹æ³•çš„å±€é™æ€§**

1. **ä¾èµ–é«˜è´¨é‡ç”Ÿæˆæ¨¡å‹è¿›è¡Œ RAD**ï¼š
   - è‹¥åˆå§‹ç†ç”±ç”Ÿæˆæ¨¡å‹èƒ½åŠ›ä¸è¶³ï¼Œå¯èƒ½å¯¼è‡´å™ªå£°ä¼ æ’­ã€‚
2. **è®¡ç®—å¼€é”€è¾ƒé«˜**ï¼š
   - å°½ç®¡ RAD æ‰¹é‡ç”Ÿæˆæå‡äº†æ•ˆç‡ï¼Œä½†å¤šç†ç”±å­˜å‚¨ä¸è®­ç»ƒä»éœ€å¤§é‡æ˜¾å­˜ã€‚
3. **è´Ÿç†ç”±æ„é€ ä¾èµ–è¯­ä¹‰åè½¬æç¤º**ï¼š
   - å¹¶éæ‰€æœ‰é”™è¯¯ç±»å‹éƒ½èƒ½è¢«æœ‰æ•ˆæ¨¡æ‹Ÿï¼ˆå¦‚é€»è¾‘è·³è·ƒã€äº‹å®é”™è¯¯ç­‰ï¼‰ã€‚
4. **ç›®å‰é›†ä¸­åœ¨ VQA åœºæ™¯**ï¼š
   - æ˜¯å¦é€‚ç”¨äºé•¿æ–‡æœ¬ç”Ÿæˆã€è§„åˆ’ç­‰æ›´å¤æ‚ä»»åŠ¡å°šå¾…éªŒè¯ã€‚

---

### **æœªæ¥å·¥ä½œæ–¹å‘**

1. **åŠ¨æ€éš¾åº¦è°ƒèŠ‚çš„è´Ÿç†ç”±ç”Ÿæˆ**ï¼š
   - æ ¹æ®æ¨¡å‹å½“å‰èƒ½åŠ›è‡ªé€‚åº”ç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§çš„é”™è¯¯ç†ç”±ã€‚
2. **è½»é‡åŒ– MIND æ¶æ„è®¾è®¡**ï¼š
   - æ¢ç´¢æ›´é«˜æ•ˆçš„å¤šç†ç”±èåˆä¸å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œé™ä½éƒ¨ç½²æˆæœ¬ã€‚
3. **æ‰©å±•è‡³æ›´å¤šæ¨¡æ€ä¸ä»»åŠ¡**ï¼š
   - å¦‚è§†é¢‘æ¨ç†ã€æœºå™¨äººå†³ç­–ã€è·¨æ¨¡æ€å¯¹è¯ç­‰ã€‚
4. **ç»“åˆå¼ºåŒ–å­¦ä¹ è¿›è¡Œè‡ªæˆ‘è¿›åŒ–**ï¼š
   - è®©æ¨¡å‹åœ¨äº¤äº’ä¸­ä¸æ–­å‘ç°æ–°ç±»å‹çš„æ¨ç†é™·é˜±å¹¶è‡ªä¸»æ„å»ºè®­ç»ƒæ ·æœ¬ã€‚

---

> **æ€»ç»“**ï¼š  
> MIND æå‡ºäº†ä¸€ç§å…¨æ–°çš„ **multi-rationale discriminative reasoning** èŒƒå¼ï¼Œé€šè¿‡ **RAD + P2CL + MCA** ä¸‰é‡æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº† MLLMs çš„æ¨ç†å‡†ç¡®æ€§ã€é²æ£’æ€§ä¸å¯è§£é‡Šæ€§ã€‚å…¶å®éªŒå……åˆ†éªŒè¯äº†â€œä¸»åŠ¨åˆ¤åˆ«â€ä¼˜äºâ€œè¢«åŠ¨æ¨¡ä»¿â€çš„ç†å¿µï¼Œä¸ºè¿ˆå‘æ›´é«˜å±‚æ¬¡çš„è®¤çŸ¥æ™ºèƒ½æä¾›äº†é‡è¦è·¯å¾„ã€‚

</details>

---

### 16. [Interleaved Latent Visual Reasoning with Selective Perceptual Modeling](https://arxiv.org/abs/2512.05665)

**Authors**: Shuai Dong, Siyuan Wang, Xingyu Liu, Zhongyu Wei  
**Category**: cs.CL  
**Published**: 2025-12-08  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.05665v1  

#### Abstract
Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently force...

---

### 17. [Sepsis Prediction Using Graph Convolutional Networks over Patient-Feature-Value Triplets](https://arxiv.org/abs/2512.05416)

**Authors**: Bozhi Dan, Di Wu, Ji Xu, Xiang Liu, Yiziting Zhu, Xin Shu, Yujie Li, Bin Yi  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.05416v1  

#### Abstract
In the intensive care setting, sepsis continues to be a major contributor to patient illness and death; however, its timely detection is hindered by the complex, sparse, and heterogeneous nature of electronic health record (EHR) data. We propose Triplet-GCN, a single-branch graph convolutional model...

---

### 18. [Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs](https://arxiv.org/abs/2512.05648)

**Authors**: Igor Shilov, Alex Cloud, Aryo Pradipta Gema, Jacob Goldman-Wetzler, Nina Panickssery, Henry Sleight, Erik Jones, Cem Anil  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.05648v1  

#### Abstract
Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, eve...

---

### 19. [BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language](https://arxiv.org/abs/2512.05721)

**Authors**: Nitin Priyadarshini Shankar, Vaibhav Singh, Sheetal Kalyani, Christian Maciocco  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.05721v1  

#### Abstract
We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off betwee...

---

### 20. [LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction](https://arxiv.org/abs/2512.05915)

**Authors**: Marius F. R. Juston, Ramavarapu S. Sreenivas, Dustin Nottage, Ahmet Soylemezoglu  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.05915v1  

#### Abstract
Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enh...

---

### 21. [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)

**Authors**: Tarun Suresh, Nalin Wadhwa, Debangshu Banerjee, Gagandeep Singh  
**Category**: cs.AI  
**Published**: 2025-12-08  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.05439v1  

#### Abstract
As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We pr...

---

### 22. [Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change](https://arxiv.org/abs/2512.05364)

**Authors**: Ananth Hariharan, David Mortensen  
**Category**: cs.CL  
**Published**: 2025-12-08  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.05364v1  

#### Abstract
This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demo...

---

### 23. [MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation](https://arxiv.org/abs/2512.05671)

**Authors**: Zhitao He, Haolin Yang, Zeyu Qin, Yi R Fung  
**Category**: cs.CL  
**Published**: 2025-12-08  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.05671v1  

#### Abstract
The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current rese...

---

### 24. [FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity](https://arxiv.org/abs/2512.05372)

**Authors**: Chengjie Ma, Seungeun Oh, Jihong Park, Seong-Lyun Kim  
**Category**: cs.DC  
**Published**: 2025-12-08  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.05372v1  

#### Abstract
Federated learning (FL) holds strong potential for distributed machine learning, but in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often struggle to participate effectively due to limited communication capacity. Their small sub-models learn quickly at first but become under-par...

---

### 25. [When unlearning is free: leveraging low influence points to reduce computational costs](https://arxiv.org/abs/2512.05254)

**Authors**: Anat Kleiman, Robert Fisher, Ben Deaner, Udi Wieder  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.05254v1  

#### Abstract
As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this...

---

### 26. [Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models](https://arxiv.org/abs/2512.05339)

**Authors**: Mahesh Kumar Nandwana, Youngwan Lim, Joseph Liu, Alex Yang, Varun Notibala, Nishchaie Khanna  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.05339v1  

#### Abstract
Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs...

---

### 27. [NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process](https://arxiv.org/abs/2512.05893)

**Authors**: Neha Gupta, Aditya Maheshwari  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.05893v1  

#### Abstract
In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $\mu >0$ and $\b...

---

### 28. [On the Bayes Inconsistency of Disagreement Discrepancy Surrogates](https://arxiv.org/abs/2512.05931)

**Authors**: Neil G. Marchant, Andrew C. Cullen, Feng Liu, Sarah M. Erfani  
**Category**: cs.LG  
**Published**: 2025-12-08  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.05931v1  

#### Abstract
Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models cha...

---

### 29. [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)

**Authors**: Igor Halperin  
**Category**: cs.AI  
**Published**: 2025-12-08  
**Score**: 3.5  
**Type**: new  
**ArXiv ID**: 2512.05156v1  

#### Abstract
Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden lay...

---

### 30. [MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare](https://arxiv.org/abs/2512.05365)

**Authors**: Zag ElSayed, Craig Erickson, Ernest Pedapati  
**Category**: cs.AI  
**Published**: 2025-12-08  
**Score**: 3.5  
**Type**: new  
**ArXiv ID**: 2512.05365v1  

#### Abstract
Healthcare AI systems have historically faced challenges in merging contextual reasoning, long-term state management, and human-verifiable workflows into a cohesive framework. This paper introduces a completely innovative architecture and concept: combining the Model Context Protocol (MCP) with a sp...

---

## ğŸ”§ Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## ğŸ“… Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## ğŸš€ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## ğŸ“ Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## ğŸ” Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
