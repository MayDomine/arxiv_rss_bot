# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-10-10 12:51:21 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)

**Authors**: Shuqing Luo, Yilin Guan, Pingzhi Li, Hanrui Wang, Tianlong Chen  
**Category**: cs.CL  
**Published**: 2025-10-10  
**Score**: 12.5

arXiv:2510.07486v1 Announce Type: new 
Abstract: Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT), but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM decoding. Query-aware page-level sparse decoding can achieve state-of-the-art performance under c...

---

### 2. [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)

**Authors**: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji  
**Category**: cs.CL  
**Published**: 2025-10-10  
**Score**: 12.5

arXiv:2508.19740v4 Announce Type: replace 
Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identi...

---

### 3. [Locality-Sensitive Hashing-Based Efficient Point Transformer for Charged Particle Reconstruction](https://arxiv.org/abs/2510.07594)

**Authors**: Shitij Govil, Jack P. Rodgers, Yuan-Tang Chou, Siqi Miao, Amit Saha, Advaith Anand, Kilian Lieret, Gage DeZoort, Mia Liu, Javier Duarte, Pan Li, Shih-Chieh Hsu  
**Category**: cs.LG  
**Published**: 2025-10-10  
**Score**: 12.5

arXiv:2510.07594v1 Announce Type: cross 
Abstract: Charged particle track reconstruction is a foundational task in collider experiments and the main computational bottleneck in particle reconstruction. Graph neural networks (GNNs) have shown strong performance for this problem, but costly graph cons...

---

### 4. [Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization](https://arxiv.org/abs/2510.08256)

**Authors**: Jason Bohne, Pawel Polak, David Rosenberg, Brian Bloniarz, Gary Kazantsev  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 11.5

arXiv:2510.08256v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and effective alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with user preferences. However, existing DPO formulations ...

---

### 5. [LLMs on a Budget? Say HOLA](https://arxiv.org/abs/2506.18952)

**Authors**: Zohaib Hasan Siddiqui, Jiechao Gao, Ebad Shabbir, Mohammad Anas Azeez, Rafiq Ali, Gautam Siddharth Kashyap, Usman Naseem  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 10.5

arXiv:2506.18952v2 Announce Type: replace-cross 
Abstract: Running Large Language Models (LLMs) on edge devices is constrained by high compute and memory demands posing a barrier for real-time applications in sectors like healthcare, education, and embedded systems. Current solutions such as quantiz...

---

### 6. [DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed Machine Learning Systems](https://arxiv.org/abs/2510.08522)

**Authors**: Yuanjun Dai, Keqiang He, An Wang  
**Category**: cs.DC  
**Published**: 2025-10-10  
**Score**: 10.5

arXiv:2510.08522v1 Announce Type: cross 
Abstract: Existing batch size selection approaches in dis- tributed machine learning rely on static allocation or simplistic heuristics that fail to adapt to heterogeneous, dynamic computing environments. We present DYNAMIX, a reinforcement learning framework...

---

### 7. [SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference](https://arxiv.org/abs/2510.08544)

**Authors**: Hengrui Zhang, Pratyush Patel, August Ning, David Wentzlaff  
**Category**: cs.DC  
**Published**: 2025-10-10  
**Score**: 10.5

arXiv:2510.08544v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have gained popularity in recent years, driving up the demand for inference. LLM inference is composed of two phases with distinct characteristics: a compute-bound prefill phase followed by a memory-bound decode phase. T...

---

### 8. [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/abs/2502.13685)

**Authors**: Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 10.0

arXiv:2502.13685v3 Announce Type: replace-cross 
Abstract: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the ...

---

### 9. [From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill](https://arxiv.org/abs/2510.08055)

**Authors**: Gunjun Lee, Jiwon Kim, Jaiyoung Park, Younjoo Lee, Jung Ho Ahn  
**Category**: cs.DC  
**Published**: 2025-10-10  
**Score**: 10.0

arXiv:2510.08055v1 Announce Type: cross 
Abstract: Large Language Model (LLM) inference in production must meet stringent service-level objectives for both time-to-first-token (TTFT) and time-between-token (TBT) while maximizing throughput under fixed compute, memory, and interconnect budgets. Moder...

---

### 10. [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)

**Authors**: Shuang Cheng, Yihan Bian, Dawei Liu, Yuhua Jiang, Yihao Liu, Linfeng Zhang, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, Bowen Zhou  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 9.5

arXiv:2510.06303v2 Announce Type: replace-cross 
Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies the training efficiency of autoregressive models with the parallel inference capability of diffusion. Instead of costly end-to-end diffusion training, SDAR perform...

---

### 11. [A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training](https://arxiv.org/abs/2510.05112)

**Authors**: Lijuan Jiang, Xingjian Qian, Zhenxiang Ma, Zan Zong, Hengjie Li, Chao Yang, Jidong Zhai  
**Category**: cs.DC  
**Published**: 2025-10-10  
**Score**: 9.5

arXiv:2510.05112v2 Announce Type: replace 
Abstract: Pipeline parallelism is an essential distributed parallelism method. Increasingly complex and diverse DNN models necessitate meticulously customized pipeline schedules for performance. However, existing practices typically rely on predefined sched...

---

### 12. [Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers](https://arxiv.org/abs/2509.06493)

**Authors**: Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, Xia Xiao  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 9.0

arXiv:2509.06493v2 Announce Type: replace 
Abstract: The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This ...

---

### 13. [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)

**Authors**: Pei-Shuo Wang, Jian-Jia Chen, Chun-Che Yang, Chi-Chih Chang, Ning-Chi Huang, Mohamed S. Abdelfattah, Kai-Chiang Wu  
**Category**: cs.CL  
**Published**: 2025-10-10  
**Score**: 9.0

arXiv:2509.18344v2 Announce Type: replace 
Abstract: The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality,...

---

### 14. [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)

**Authors**: Leigang Qu, Ziyang Wang, Na Zheng, Wenjie Wang, Liqiang Nie, Tat-Seng Chua  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 8.5

arXiv:2510.07940v1 Announce Type: cross 
Abstract: Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a tra...

---

### 15. [Can Large Language Models Be Trusted as Evolutionary Optimizers for Network-Structured Combinatorial Problems?](https://arxiv.org/abs/2501.15081)

**Authors**: Jie Zhao, Tao Wen, Kang Hao Cheong  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 8.5

arXiv:2501.15081v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown strong capabilities in language understanding and reasoning across diverse domains. Recently, there has been increasing interest in utilizing LLMs not merely as assistants in optimization tasks, but as...

---

### 16. [Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling](https://arxiv.org/abs/2503.02233)

**Authors**: Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, Kai Yu  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 8.5

arXiv:2503.02233v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are prone to hallucination stemming from misaligned self-awareness, particularly when processing queries exceeding their knowledge boundaries. While existing mitigation strategies employ uncertainty estimation or...

---

### 17. [Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding](https://arxiv.org/abs/2509.18085)

**Authors**: Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee, Christopher Lott, Fatih Porikli  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 8.5

arXiv:2509.18085v2 Announce Type: replace-cross 
Abstract: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often genera...

---

### 18. [Climate Surrogates for Scalable Multi-Agent Reinforcement Learning: A Case Study with CICERO-SCM](https://arxiv.org/abs/2510.07971)

**Authors**: Oskar Bohn Lassen, Serio Angelo Maria Agriesti, Filipe Rodrigues, Francisco Camara Pereira  
**Category**: cs.LG  
**Published**: 2025-10-10  
**Score**: 8.5

arXiv:2510.07971v1 Announce Type: new 
Abstract: Climate policy studies require models that capture the combined effects of multiple greenhouse gases on global temperature, but these models are computationally expensive and difficult to embed in reinforcement learning. We present a multi-agent reinf...

---

### 19. [Dynamic Features Adaptation in Networking: Toward Flexible training and Explainable inference](https://arxiv.org/abs/2510.08303)

**Authors**: Yannis Belkhiter, Seshu Tirupathi, Giulio Zizzo, Merim Dzaferagic, John D. Kelleher  
**Category**: cs.LG  
**Published**: 2025-10-10  
**Score**: 8.5

arXiv:2510.08303v1 Announce Type: new 
Abstract: As AI becomes a native component of 6G network control, AI models must adapt to continuously changing conditions, including the introduction of new features and measurements driven by multi-vendor deployments, hardware upgrades, and evolving service r...

---

### 20. [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)

**Authors**: Makram Chahine, Philipp Nazari, Daniela Rus, T. Konstantin Rusch  
**Category**: cs.LG  
**Published**: 2025-10-10  
**Score**: 8.5

arXiv:2510.02823v2 Announce Type: replace 
Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks efficiently, offer both parallelizable training and fast inference. At their core are recurrent dynamical systems that maintain a hidden state, with update costs scaling w...

---

### 21. [An LLM-Powered Cooperative Framework for Large-Scale Multi-Vehicle Navigation](https://arxiv.org/abs/2510.07825)

**Authors**: Yuping Zhou, Siqi Lai, Jindong Han, Hao Liu  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 8.0

arXiv:2510.07825v1 Announce Type: new 
Abstract: The rise of Internet of Vehicles (IoV) technologies is transforming traffic management from isolated control to a collective, multi-vehicle process. At the heart of this shift is multi-vehicle dynamic navigation, which requires simultaneously routing ...

---

### 22. [Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models](https://arxiv.org/abs/2510.07799)

**Authors**: Eric Hanchen Jiang, Guancheng Wan, Sophia Yin, Mengting Li, Yuchen Wu, Xiao Liang, Xinfeng Li, Yizhou Sun, Wei Wang, Kai-Wei Chang, Ying Nian Wu  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 8.0

arXiv:2510.07799v1 Announce Type: cross 
Abstract: The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as tas...

---

### 23. [DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation](https://arxiv.org/abs/2510.07865)

**Authors**: Guowei Zou, Haitao Wang, Hejun Wu, Yukun Qian, Yuhang Wang, Weibing Li  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 8.0

arXiv:2510.07865v1 Announce Type: cross 
Abstract: The ability to learn multi-modal action distributions is indispensable for robotic manipulation policies to perform precise and robust control. Flow-based generative models have recently emerged as a promising solution to learning distributions of a...

---

### 24. [Expert-Token Resonance MoE: Bidirectional Routing with Efficiency Affinity-Driven Active Selection](https://arxiv.org/abs/2406.00023)

**Authors**: Jing Li, Zhijie Sun, Dachao Lin, Xuan He, Binfan Zheng, Yi Lin, Rongqian Zhao, Xin Chen  
**Category**: cs.CL  
**Published**: 2025-10-10  
**Score**: 8.0

arXiv:2406.00023v4 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) architectures enable efficient scaling of large language models by activating only a subset of parameters per input. However, existing MoE models suffer from two critical limitations: (1) inefficient token-to-expert routin...

---

### 25. [Phantora: Maximizing Code Reuse in Simulation-based Machine Learning System Performance Estimation](https://arxiv.org/abs/2505.01616)

**Authors**: Jianxing Qin, Jingrong Chen, Xinhao Kong, Yongji Wu, Tianjun Yuan, Liang Luo, Zhaodong Wang, Ying Zhang, Tingjun Chen, Alvin R. Lebeck, Danyang Zhuo  
**Category**: cs.DC  
**Published**: 2025-10-10  
**Score**: 8.0

arXiv:2505.01616v3 Announce Type: replace 
Abstract: Modern machine learning (ML) training workloads place substantial demands on both computational and communication resources. Consequently, accurate performance estimation has become increasingly critical for guiding system design decisions, such a...

---

### 26. [Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization](https://arxiv.org/abs/2510.08233)

**Authors**: Yuchen Zhu, Wei Guo, Jaemoo Choi, Petr Molodyk, Bo Yuan, Molei Tao, Yongxin Chen  
**Category**: cs.LG  
**Published**: 2025-10-10  
**Score**: 8.0

arXiv:2510.08233v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) are promising alternatives to autoregressive large language models (AR-LLMs), as they potentially allow higher inference throughput. Reinforcement learning (RL) is a crucial component for dLLMs to achieve compar...

---

### 27. [TaoSR-SHE: Stepwise Hybrid Examination Reinforcement Learning Framework for E-commerce Search Relevance](https://arxiv.org/abs/2510.07972)

**Authors**: Pengkun Jiao, Yiming Jin, Jianhui Yang, Chenhe Dong, Zerui Huang, Shaowei Yao, Xiaojiang Zhou, Dan Ou, Haihong Tang  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 7.5

arXiv:2510.07972v1 Announce Type: new 
Abstract: Query-product relevance analysis is a foundational technology in e-commerce search engines and has become increasingly important in AI-driven e-commerce. The recent emergence of large language models (LLMs), particularly their chain-of-thought (CoT) r...

---

### 28. [OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2510.07651)

**Authors**: Yuzhe Gu, Xiyu Liang, Jiaojiao Zhao, Enmao Diao  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 7.5

arXiv:2510.07651v1 Announce Type: cross 
Abstract: Large language models (LLMs) with extended context windows enable powerful downstream applications but impose significant memory overhead, as caching all key-value (KV) states scales linearly with sequence length and batch size. Existing cache evict...

---

### 29. [TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection](https://arxiv.org/abs/2411.02886)

**Authors**: Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 7.5

arXiv:2411.02886v4 Announce Type: replace-cross 
Abstract: Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-di...

---

### 30. [Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning](https://arxiv.org/abs/2509.26383)

**Authors**: Jinyeop Song, Song Wang, Julian Shun, Yada Zhu  
**Category**: cs.AI  
**Published**: 2025-10-10  
**Score**: 7.5

arXiv:2509.26383v3 Announce Type: replace-cross 
Abstract: Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multip...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
