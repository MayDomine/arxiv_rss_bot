# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2026-01-26 05:57:24 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers](https://arxiv.org/abs/2601.16956)

**Authors**: Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae  
**Category**: cs.DC  
**Published**: 2026-01-26  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2601.16956v1  

#### Abstract
The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this mas...

---

### 2. [W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs](https://arxiv.org/abs/2601.16536)

**Authors**: Yuanhong He, Peiyu Niu, Jun Chen, Chenchen Zhang, Chao Yang  
**Category**: cs.DC  
**Published**: 2026-01-26  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2601.16536v1  

#### Abstract
As Large Language Models (LLMs) scale, weight-only quantization (W4A16: 4-bit weights, 16-bit activations) becomes critical for reducing memory footprint with minimal accuracy loss. However, its efficient deployment on Huawei's Ascend 910 Neural Processing Unit (NPU) is challenging due to limited na...

---

### 3. [When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability in Multi-Agent LLM Systems](https://arxiv.org/abs/2601.16280)

**Authors**: Donghao Huang, Gauri Malwe, Zhaoxia Wang  
**Category**: cs.AI  
**Published**: 2026-01-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.16280v1  

#### Abstract
Multi-agent systems powered by large language models (LLMs) are transforming enterprise automation, yet systematic evaluation methodologies for assessing tool-use reliability remain underdeveloped. We introduce a comprehensive diagnostic framework that leverages big data analytics to evaluate proced...

---

### 4. [LongCat-Flash-Thinking-2601 Technical Report](https://arxiv.org/abs/2601.16725)

**Authors**: Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chao Zhang, Chen Gao, Chen Zhang, Chengcheng Han, Chenhui Yang, Chuyu Zhang, Cong Chen, Cunguang Wang, Daoru Pan, Defei Bu, Dengchang Zhao, Di Xiu, Dishan Liu, Dongyu Ru, Dunwei Tu, Fan Wu, Fengcheng Yuan, Fengcun Li, Gang Xu, Guanyu Wu, Guoyuan Lin, Haibin Wang, Hansi Yang, Hao Yang, Haonan Yan, Haoxiang Ma, Haoxing Wen, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiacheng Zhang, Jiahong Zhou, Jiahuan Li, Jiaming Wang, Jian Yang, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiapeng Zhu, Jiaqi Sun, Jiarong Shi, Jiarui Zhao, Jingang Wang, Jinluan Yang, Jinrui Ding, Jinwei Xiao, Jiyuan He, Juncan Xu, Kefeng Zhang, Keheng Wang, Li Wei, Lianhui Ma, Lin Qiu, Lingbing Kong, Lingchuan Liu, Linsen Guo, Mengshen Zhu, Mengxia Shen, Mingyang Zhu, Peiguang Li, Peng Pei, Pengcheng Jia, Pengtao Zhang, Peng Zhao, Qi Gu, Qiong Huang, Qiyuan Duan, Quanchi Weng, Rongxiang Weng, Rongzhi Zhang, Rumei Li, Shanglin Lei, Shengnan An, Shijun Dai, Shuaikang Liu, Shuang Zhou, Shuo Wang, Songyuan Zhao, Tao Liang, Tianhao Hu, Tianze Chen, Wei Liu, Wei Shi, Wei Wang, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Wentao Chen, Wentao Shi, Xi Su, Xiangcheng Liu, Xiandi Ma, Xiangyu Xi, Xiangyuan Liu, Xiangzhou Huang, Xiao Liu, Xiaodong Cai, Xiaolong Chen, Xiaowei Shi, Xiaoyu Li, Xin Chen, Xingchen Liu, Xuan Huang, Xuezhi Cao, Xunliang Cai, Yan Chen, Yang Bai, Yang Liu, Yang Yang, Yang Zheng, Yaoming Wang, Yaoming Zhu, Yaqi Huo, Yanyu Chen, Yaorui Shi, Yerui Sun, Yi Zhang, Yihao Chen, Yi-Kai Zhang, Yifan Lu, Yifan Zhao, Yitao Zhai, Yongjing Yin, Yongwei Zhou, Youshao Xiao, Yuchuan Dai, Yuchen Xie, Yuchen Yu, Yufei Zhang, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunke Zhao, Yuwei Jiang, Yuxin Bian, Yuxin Chen, Yuxin Liu, Yue Xu, Yueqing Sun, Zeyang Yu, Zhao Yang, Zhengsheng Huang, Zhengyu Chen, Zhijian Liu, Zhikang Xia, Zhimin Lin, Zhiyuan Yao, Zhuofan Chen, Zhuowen Han, Zijian Zhang, Ziran Li, Ziwen Wang, Ziyuan Zhuang  
**Category**: cs.AI  
**Published**: 2026-01-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.16725v1  

#### Abstract
We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, includi...

---

### 5. [Auto-Regressive Masked Diffusion Models](https://arxiv.org/abs/2601.16971)

**Authors**: Mahdi Karami, Ali Ghodsi  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2601.16971v1  

#### Abstract
Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture design...

---

### 6. [Curate-Train-Refine: A Closed-Loop Agentic Framework for Zero Shot Classification](https://arxiv.org/abs/2601.16530)

**Authors**: Gaurav Maheshwari, Kevin El Haddad  
**Category**: cs.CL  
**Published**: 2026-01-26  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.16530v1  

#### Abstract
Large language models (LLMs) and high-capacity encoders have advanced zero and few-shot classification, but their inference cost and latency limit practical deployment. We propose training lightweight text classifiers using dynamically generated supervision from an LLM. Our method employs an iterati...

---

### 7. [AuroraEdge-V-2B: A Faster And Stronger Edge Visual Large Language Model](https://arxiv.org/abs/2601.16615)

**Authors**: Xiang Chen  
**Category**: cs.CL  
**Published**: 2026-01-26  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.16615v1  

#### Abstract
Recently, due to the advancement of multimodal technology, people are attempting to use visual large language models (VLLMs) in industrial production. Many deep learning models (DLMs) deployed in the production environment are gradually being replaced by VLLMs. Compared with DLMs, VLLMs have some ad...

---

### 8. [Better Generalizing to Unseen Concepts: An Evaluation Framework and An LLM-Based Auto-Labeled Pipeline for Biomedical Concept Recognition](https://arxiv.org/abs/2601.16711)

**Authors**: Shanshan Liu, Noriki Nishida, Fei Cheng, Narumi Tokunaga, Rumana Ferdous Munne, Yuki Yamagata, Kouji Kozaki, Takehito Utsuro, Yuji Matsumoto  
**Category**: cs.CL  
**Published**: 2026-01-26  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.16711v1  

#### Abstract
Generalization to unseen concepts is a central challenge due to the scarcity of human annotations in Mention-agnostic Biomedical Concept Recognition (MA-BCR). This work makes two key contributions to systematically address this issue. First, we propose an evaluation framework built on hierarchical c...

---

### 9. [Efficient Gaussian process learning via subspace projections](https://arxiv.org/abs/2601.16332)

**Authors**: Felipe Tobar, Elsa Cazelles  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2601.16332v1  

#### Abstract
We propose a novel training objective for GPs constructed using lower-dimensional linear projections of the data, referred to as \emph{projected likelihood} (PL). We provide a closed-form expression for the information loss related to the PL and empirically show that it can be reduced with random pr...

---

### 10. [Towards a Theoretical Understanding to the Generalization of RLHF](https://arxiv.org/abs/2601.16403)

**Authors**: Zhaochun Li (Beijing Institute of Technolegy, Zhongguancun Academy), Mingyang Yi (Renmin University of China), Yue Wang (Zhongguancun Academy), Shisheng Cui (Beijing Institute of Technolegy), Yong Liu (Renmin University of China)  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.16403v1  

#### Abstract
Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored....

---

### 11. [The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics](https://arxiv.org/abs/2601.16849)

**Authors**: Henri Nikoleit, Ankit Anand, Anurag Murty Naredla, Heiko R\"oglin  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2601.16849v1  

#### Abstract
We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. ...

---

### 12. [Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs](https://arxiv.org/abs/2601.16479)

**Authors**: Hongjia Wu, Shuai Zhou, Hongxin Zhang, Wei Chen  
**Category**: cs.AI  
**Published**: 2026-01-26  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.16479v1  

#### Abstract
While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hier...

---

### 13. [LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification](https://arxiv.org/abs/2601.16549)

**Authors**: Meet Raval, Tejul Pandit, Dhvani Upadhyay  
**Category**: cs.AI  
**Published**: 2026-01-26  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.16549v1  

#### Abstract
The combination of multimodal Vision-Language Models (VLMs) and Large Language Models (LLMs) opens up new possibilities for medical classification. This work offers a rigorous, unified benchmark by using four publicly available datasets covering text and image modalities (binary and multiclass compl...

---

### 14. [Limits of n-gram Style Control for LLMs via Logit-Space Injection](https://arxiv.org/abs/2601.16224)

**Authors**: Sami-ul Ahmed  
**Category**: cs.CL  
**Published**: 2026-01-26  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.16224v1  

#### Abstract
Large language models (LLMs) are typically personalized via prompt engineering or parameter-efficient fine-tuning such as LoRA. However, writing style can be difficult to distill into a single prompt, and LoRA fine-tuning requires computationally intensive training and infrastructure. We investigate...

---

### 15. [Robust Categorical Data Clustering Guided by Multi-Granular Competitive Learning](https://arxiv.org/abs/2601.16491)

**Authors**: Shenghong Cai, Yiqun Zhang, Xiaopeng Luo, Yiu-Ming Cheung, Hong Jia, Peng Liu  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.16491v1  

#### Abstract
Data set composed of categorical features is very common in big data analysis tasks. Since categorical features are usually with a limited number of qualitative possible values, the nested granular cluster effect is prevalent in the implicit discrete distance space of categorical data. That is, data...

---

### 16. [Multigrade Neural Network Approximation](https://arxiv.org/abs/2601.16884)

**Authors**: Shijun Zhang, Zuowei Shen, Yuesheng Xu  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2601.16884v1  

#### Abstract
We study multigrade deep learning (MGDL) as a principled framework for structured error refinement in deep neural networks. While the approximation power of neural networks is now relatively well understood, training very deep architectures remains challenging due to highly non-convex and often ill-...

---

### 17. [DSGym: A Holistic Framework for Evaluating and Training Data Science Agents](https://arxiv.org/abs/2601.16344)

**Authors**: Fan Nie, Junlin Wang, Harper Hua, Federico Bianchi, Yongchan Kwon, Zhenting Qi, Owen Queen, Shang Zhu, James Zou  
**Category**: cs.AI  
**Published**: 2026-01-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.16344v1  

#### Abstract
Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of...

---

### 18. [Persona Jailbreaking in Large Language Models](https://arxiv.org/abs/2601.16466)

**Authors**: Jivnesh Sandhan, Fei Cheng, Tushar Sandhan, Yugo Murawaki  
**Category**: cs.CL  
**Published**: 2026-01-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.16466v1  

#### Abstract
Large Language Models (LLMs) are increasingly deployed in domains such as education, mental health and customer support, where stable and consistent personas are critical for reliability. Yet, existing studies focus on narrative or role-playing tasks and overlook how adversarial conversational histo...

---

### 19. [Select or Project? Evaluating Lower-dimensional Vectors for LLM Training Data Explanations](https://arxiv.org/abs/2601.16651)

**Authors**: Lukas Hinterleitner, Loris Schoenegger, Benjamin Roth  
**Category**: cs.CL  
**Published**: 2026-01-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.16651v1  

#### Abstract
Gradient-based methods for instance-based explanation for large language models (LLMs) are hindered by the immense dimensionality of model gradients. In practice, influence estimation is restricted to a subset of model parameters to make computation tractable, but this subset is often chosen ad hoc ...

---

### 20. [Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis](https://arxiv.org/abs/2601.16800)

**Authors**: Gaurav Negi, MA Waskow, Paul Buitelaar  
**Category**: cs.CL  
**Published**: 2026-01-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.16800v1  

#### Abstract
Fine-grained opinion analysis of text provides a detailed understanding of expressed sentiments, including the addressed entity. Although this level of detail is sound, it requires considerable human effort and substantial cost to annotate opinions in datasets for training models, especially across ...

---

### 21. [A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning](https://arxiv.org/abs/2601.16399)

**Authors**: Sihan Zeng, Sujay Bhatt, Sumitra Ganesh, Alec Koppel  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.16399v1  

#### Abstract
We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objec...

---

### 22. [Rethinking Large Language Models For Irregular Time Series Classification In Critical Care](https://arxiv.org/abs/2601.16516)

**Authors**: Feixiang Zheng, Yu Wu, Cecilia Mascolo, Ting Dang  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.16516v1  

#### Abstract
Time series data from the Intensive Care Unit (ICU) provides critical information for patient monitoring. While recent advancements in applying Large Language Models (LLMs) to time series modeling (TSM) have shown great promise, their effectiveness on the irregular ICU data, characterized by particu...

---

### 23. [Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach](https://arxiv.org/abs/2601.16568)

**Authors**: Abdurahman Maarouf, Alket Bakiaj, Stefan Feuerriegel  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.16568v1  

#### Abstract
Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they...

---

### 24. [FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization](https://arxiv.org/abs/2601.16897)

**Authors**: Antesh Upadhyay, Sang Bin Moon, Abolfazl Hashemi  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.16897v1  

#### Abstract
We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provi...

---

### 25. [Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection](https://arxiv.org/abs/2601.16976)

**Authors**: Estela S\'anchez-Carballo, Francisco M. Melgarejo-Meseguer, Jos\'e Luis Rojo-\'Alvarez  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2601.16976v1  

#### Abstract
Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely e...

---

### 26. [AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems](https://arxiv.org/abs/2601.16964)

**Authors**: Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah  
**Category**: cs.AI  
**Published**: 2026-01-26  
**Score**: 3.5  
**Type**: new  
**ArXiv ID**: 2601.16964v1  

#### Abstract
The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, ...

---

### 27. [Persuasion Tokens for Editing Factual Knowledge in LLMs](https://arxiv.org/abs/2601.16781)

**Authors**: Paul Youssef, J\"org Schl\"otterer, Christin Seifert  
**Category**: cs.CL  
**Published**: 2026-01-26  
**Score**: 3.5  
**Type**: new  
**ArXiv ID**: 2601.16781v1  

#### Abstract
In-context knowledge editing (IKE) is a promising technique for updating Large Language Models (LLMs) with new information. However, IKE relies on lengthy, fact-specific demonstrations which are costly to create and consume significant context window space. In this paper, we introduce persuasion tok...

---

### 28. [Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles](https://arxiv.org/abs/2601.16936)

**Authors**: Anton Zamyatin, Patrick Indri, Sagar Malhotra, Thomas G\"artner  
**Category**: cs.LG  
**Published**: 2026-01-26  
**Score**: 3.5  
**Type**: new  
**ArXiv ID**: 2601.16936v1  

#### Abstract
In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by appl...

---

### 29. [SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems](https://arxiv.org/abs/2601.16286)

**Authors**: Varun Chillara, Dylan Kline, Christopher Alvares, Evan Wooten, Huan Yang, Shlok Khetan, Cade Bauer, Tr\'e Guillory, Tanishka Shah, Yashodhara Dhariwal, Volodymyr Pavlov, George Popstefanov  
**Category**: cs.AI  
**Published**: 2026-01-26  
**Score**: 3.0  
**Type**: new  
**ArXiv ID**: 2601.16286v1  

#### Abstract
Agentic AI pipelines suffer from a hidden inefficiency: they frequently reconstruct identical intermediate logic, such as metric normalization or chart scaffolding, even when the user's natural language phrasing is entirely novel. Conventional boundary caching fails to capture this inefficiency beca...

---

### 30. [SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care](https://arxiv.org/abs/2601.16529)

**Authors**: Dongshen Peng, Yi Wang, Carl Preiksaitis, Christian Rose  
**Category**: cs.AI  
**Published**: 2026-01-26  
**Score**: 3.0  
**Type**: new  
**ArXiv ID**: 2601.16529v1  

#### Abstract
Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs ...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
