# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## üìä Statistics

- **Last Updated**: 2025-08-14 15:43:24 UTC
- **Total Papers Found**: {{PAPER_COUNT}}
- **Categories Monitored**: {{CATEGORIES}}

## üìö Recent Papers

### 1. [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)

**Authors**: Muneeza Azmat, Momin Abbas, Maysa Malfiza Garcia de Macedo, Marcelo Carpinette Grave, Luan Soares de Souza, Tiago Machado, Rogerio A de Paula, Raya Horesh, Yixin Chen, Heloisa Caroline de Souza Pereira Candello, Rebecka Nordenlow, Aminat Adebiyi  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 4.5

arXiv:2508.09937v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values and safety standards has become critical. The field has developed diverse alignment approaches including trad...

---

### 2. [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)

**Authors**: Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, Zhijie Deng  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 4.0

arXiv:2508.09192v1 Announce Type: cross 
Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs for text generation, with the potential to decode multiple tokens in a single iteration. However, none of the existing open-source dLLMs have...

---

### 3. [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)

**Authors**: Zhifan Luo, Shuo Shao, Su Zhang, Lijing Zhou, Yuke Hu, Chenxu Zhao, Zhihao Liu, Zhan Qin  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 4.0

arXiv:2508.09442v1 Announce Type: cross 
Abstract: The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization...

---

### 4. [Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion](https://arxiv.org/abs/2508.09537)

**Authors**: Yanzhou Li, Tianlin Li, Yiran Zhang, Shangqing Liu, Aishan Liu, Yang Liu  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 4.0

arXiv:2508.09537v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used for function completion in repository-scale codebases. Prior studies demonstrate that when explicit instructions--such as docstrings--are provided, these models can generate highly accurate implemen...

---

### 5. [Beyond Memorization: Assessing Semantic Generalization in Large Language Models Using Phrasal Constructions](https://arxiv.org/abs/2501.04661)

**Authors**: Wesley Scivetti, Melissa Torgbi, Austin Blodgett, Mollie Shichman, Taylor Hudson, Claire Bonial, Harish Tayyar Madabushi  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 4.0

arXiv:2501.04661v2 Announce Type: replace-cross 
Abstract: The web-scale of pretraining data has created an important evaluation challenge: to disentangle linguistic competence on cases well-represented in pretraining data from generalization to out-of-domain language, specifically the dynamic, real...

---

### 6. [EvoP: Robust LLM Inference via Evolutionary Pruning](https://arxiv.org/abs/2502.14910)

**Authors**: Shangyu Wu, Hongchao Du, Ying Xiong, Shuai Chen, Tei-Wei Kuo, Nan Guan, Chun Jason Xue  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 4.0

arXiv:2502.14910v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, but their massive size and computational demands hinder their deployment in resource-constrained environments. Existing model pruning methods...

---

### 7. [Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based Mobile Agents](https://arxiv.org/abs/2506.13205)

**Authors**: Xuan Wang, Siyuan Liang, Zhe Liu, Yi Yu, Aishan Liu, Yuliang Lu, Xitong Gao, Ee-Chien Chang  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 4.0

arXiv:2506.13205v5 Announce Type: replace-cross 
Abstract: Mobile agents powered by vision-language models (VLMs) are increasingly adopted for tasks such as UI automation and camera-based assistance. These agents are typically fine-tuned using small-scale, user-collected data, making them susceptibl...

---

### 8. [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)

**Authors**: Haoyang Li, Jiaqing Li, Jialun Cao, Zongyuan Yang, Yongping Xiong  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.5

arXiv:2508.09220v1 Announce Type: cross 
Abstract: Large foundation models have achieved significant performance gains through scalable training on massive datasets. However, the field of \textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression \textbf{R}ecognition (HMER) has been impeded by t...

---

### 9. [Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference](https://arxiv.org/abs/2508.09229)

**Authors**: Danil Sivtsov, Aleksandr Katrutsa, Ivan Oseledets  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.5

arXiv:2508.09229v1 Announce Type: cross 
Abstract: Efficient deployment of a pre-trained LLM to a cluster with multiple servers is a critical step for providing fast responses to users' queries. The recent success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy them efficiently...

---

### 10. [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)

**Authors**: Aayush Gupta  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.5

arXiv:2508.09288v1 Announce Type: cross 
Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection and related jailbreak attacks; heuristic guardrails (rules, filters, LLM judges) are routinely bypassed. We present Contextual Integrity Verification (CIV), an inference-time...

---

### 11. [Beyond Na\"ive Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)

**Authors**: Arjun Ashok, Andrew Robert Williams, Vincent Zhihao Zheng, Irina Rish, Nicolas Chapados, \'Etienne Marcotte, Valentina Zantedeschi, Alexandre Drouin  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.5

arXiv:2508.09904v1 Announce Type: cross 
Abstract: Forecasting in real-world settings requires models to integrate not only historical data but also relevant contextual information, often available in textual form. While recent work has shown that large language models (LLMs) can be effective contex...

---

### 12. [GenAI Confessions: Black-box Membership Inference for Generative Image Models](https://arxiv.org/abs/2501.06399)

**Authors**: Matyas Bohacek, Hany Farid  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.5

arXiv:2501.06399v2 Announce Type: replace-cross 
Abstract: From a simple text prompt, generative-AI image models can create stunningly realistic and creative images bounded, it seems, by only our imagination. These models have achieved this remarkable feat thanks, in part, to the ingestion of billio...

---

### 13. [FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention](https://arxiv.org/abs/2504.02211)

**Authors**: Huangliang Dai, Shixun Wu, Jiajun Huang, Zizhe Jian, Yue Zhu, Haiyang Hu, Zizhong Chen  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.5

arXiv:2504.02211v2 Announce Type: replace-cross 
Abstract: Transformer models rely on High-Performance Computing (HPC) resources for inference, where soft errors are inevitable in large-scale systems, making the reliability of the model particularly critical. Existing fault tolerance frameworks for ...

---

### 14. [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)

**Authors**: Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, Yi Wu  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.5

arXiv:2508.07976v2 Announce Type: replace-cross 
Abstract: Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vas...

---

### 15. [MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis](https://arxiv.org/abs/2508.08275)

**Authors**: Haiyun Guo, ZhiYan Hou, Yu Chen, Jinghan He, Yandu Sun, Yuzhe Zhou, Shujing Guo, Kuan Zhu, Jinqiao Wang  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.5

arXiv:2508.08275v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) require continual instruction tuning during their post-training phase to adapt to the dynamic real-world demands. However, the absence of rigorous and systematic benchmarks has hindered progress in th...

---

### 16. [Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer](https://arxiv.org/abs/2508.09131)

**Authors**: Zixin Yin, Xili Dai, Ling-Hao Chen, Deyu Zhou, Jianan Wang, Duomin Wang, Gang Yu, Lionel M. Ni, Lei Zhang, Heung-Yeung Shum  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.5

arXiv:2508.09131v2 Announce Type: replace-cross 
Abstract: Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency ...

---

### 17. [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)

**Authors**: Liping Huang, Yicheng Zhang, Yifang Yin, Sheng Zhang, Yi Zhang  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09144v1 Announce Type: cross 
Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial for arrival management in aviation, particularly for runway sequencing. Given the rapidly changing airspace context, the ETA prediction efficiency is as important as its a...

---

### 18. [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)

**Authors**: Shugang Hao, Hongbo Li, Lingjie Duan  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09146v1 Announce Type: cross 
Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still incurs poor throughput performance under dynamic channel environments. Recent model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply optimize backoff stra...

---

### 19. [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)

**Authors**: Junghwan Lim, Sungmin Lee, Dongseok Kim, Eunhwan Park, Hyunbyung Park, Junhyeok Lee, Wai Ting Cheung, Dahye Choi, Jaeheui Her, Jaeyeon Huh, Hanbin Jung, Changjin Kang, Beomgyu Kim, Jihwan Kim, Minjae Kim, Taehwan Kim, Youngrok Kim, Haesol Lee, Jeesoo Lee, Kungyu Lee, Dongpin Oh, Yeongjae Park, Bokki Ryu, Daewon Suh, Dongjoo Weon  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09148v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized artificial intelligence, yet developing an effective foundational LLM that balances high performance with computational efficiency remains challenging, especially for emerging r...

---

### 20. [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)

**Authors**: Jan Tauberschmidt, Sophie Fellenz, Sebastian J. Vollmer, Andrew B. Duncan  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09156v1 Announce Type: cross 
Abstract: We present a framework for fine-tuning flow-matching generative models to enforce physical constraints and solve inverse problems in scientific systems. Starting from a model trained on low-fidelity or observational data, we apply a differentiable p...

---

### 21. [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)

**Authors**: Hazem Hesham Yousef Shalby, Fabrizio Pittorino, Francesca Palermo, Diana Trojaniello, Manuel Roveri  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09176v1 Announce Type: cross 
Abstract: The deployment of deep neural networks on resource-constrained devices relies on quantization. While static, uniform quantization applies a fixed bit-width to all inputs, it fails to adapt to their varying complexity. Dynamic, instance-based mixed-p...

---

### 22. [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)

**Authors**: Bing Han, Feifei Zhao, Dongcheng Zhao, Guobin Shen, Ping Wu, Yu Shi, Yi Zeng  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09190v1 Announce Type: cross 
Abstract: Fine-tuning as service injects domain-specific knowledge into large language models (LLMs), while challenging the original alignment mechanisms and introducing safety risks. A series of defense strategies have been proposed for the alignment, fine-t...

---

### 23. [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)

**Authors**: Yun Wang, Long Zhang, Jingren Liu, Jiaqi Yan, Zhanjie Zhang, Jiahao Zheng, Xun Yang, Dapeng Wu, Xiangyu Chen, Xuelong Li  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09486v1 Announce Type: cross 
Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding but struggle with long-form videos due to context window limits. Consequently, recent approaches focus on keyframe retrieval, condensing lengthy videos into a small set of...

---

### 24. [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)

**Authors**: Jessy Lin, Vincent-Pierre Berges, Xilun Chen, Wen-Tau Yih, Gargi Ghosh, Barlas O\u{g}uz  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09494v1 Announce Type: cross 
Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory. However, learning and recalling facts from this memory is known to be unreliable, depending largely on the prevalence of particular facts in the training data and other fa...

---

### 25. [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)

**Authors**: Yifei Sun, Junming Liu, Ding Wang, Yirong Chen, Xuefeng Yan  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09630v1 Announce Type: cross 
Abstract: Multivariate time series data typically comprises two distinct modalities: variable semantics and sampled numerical observations. Traditional time series models treat variables as anonymous statistical signals, overlooking the rich semantic informat...

---

### 26. [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)

**Authors**: Gerardo Loza, Junlei Hu, Dominic Jones, Sharib Ali, Pietro Valdastri  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09681v1 Announce Type: cross 
Abstract: We proposed a novel test-time optimisation (TTO) approach framed by a NeRF-based architecture for long-term 3D point tracking. Most current methods in point tracking struggle to obtain consistent motion or are limited to 2D motion. TTO approaches fr...

---

### 27. [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)

**Authors**: Jiaqi Cao, Jiarui Wang, Rubin Wei, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09874v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers...

---

### 28. [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)

**Authors**: Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09945v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified ...

---

### 29. [Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model](https://arxiv.org/abs/2508.09971)

**Authors**: Zihan Wang, Nina Mahmoudian  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2508.09971v1 Announce Type: cross 
Abstract: Vision-driven autonomous river following by Unmanned Aerial Vehicles is critical for applications such as rescue, surveillance, and environmental monitoring, particularly in dense riverine environments where GPS signals are unreliable. We formalize ...

---

### 30. [Probing Mechanical Reasoning in Large Vision Language Models](https://arxiv.org/abs/2410.00318)

**Authors**: Haoran Sun, Qingying Gao, Haiyun Lyu, Dezhi Luo, Yijiang Li, Hokin Deng  
**Category**: cs.AI  
**Published**: 2025-08-14  
**Score**: 3.0

arXiv:2410.00318v4 Announce Type: replace 
Abstract: Mechanical reasoning is a hallmark of human intelligence, defined by its ubiquitous yet irreplaceable role in human activities ranging from routine tasks to civil engineering. Embedding machines with mechanical reasoning is therefore an important ...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- {{KEYWORDS}}

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 