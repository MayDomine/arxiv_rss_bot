# ICLR 2026 Papers ðŸ“š

- **Last Updated**: 2025-11-12 06:56:27 UTC
- **Total Filtered Papers**: 18533
- **Papers with Ratings**: 81
- **Total Submissions**: (cached)

Click any title to view the full discussion on OpenReview.

**Configuration**: Filtered by keywords ['LLM', 'RL', 'RLHF', 'Inference', 'Training', 'Attention', 'Pipeline', 'MOE', 'Sparse', 'Quantization', 'Speculative', 'Efficient', 'Efficiency', 'Framework', 'Parallel', 'Distributed', 'Kernel', 'Decode', 'Decoding', 'Prefill', 'Throughput', 'Fast', 'Network', 'Hardware', 'Cluster', 'FP8', 'FP4', 'Optimization', 'Scalable', 'Communication']

> Showing top 100 papers, sorted by keyword relevance score. 81 papers have review ratings.

| # | Title | Avg Rating | Reviews | Decision | OpenReview |
| --- | --- | --- | --- | --- | --- |
| 1 | [Prima.cpp: Fast 30-70B LLM Inference on Heterogeneous and Low-Resource Home Clusters](https://openreview.net/forum?id=h0LjpOG1jq) | 4.67 | 3 | Pending | [Link](https://openreview.net/forum?id=h0LjpOG1jq) |
| 2 | [Sandbox-RL: Scalable Multi-LLMs Optimization through Sandbox-Based Reinforcement Learning](https://openreview.net/forum?id=0pFcKF2li1) | 2.67 | 3 | Pending | [Link](https://openreview.net/forum?id=0pFcKF2li1) |
| 3 | [SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling](https://openreview.net/forum?id=yTeDQeuKKz) | 4.00 | 3 | Pending | [Link](https://openreview.net/forum?id=yTeDQeuKKz) |
| 4 | [Sem-MoE: Semantic-aware Model-Data Collaborative Scheduling for Efficient MoE Inference](https://openreview.net/forum?id=MSHPrMpIHZ) | 5.33 | 3 | Pending | [Link](https://openreview.net/forum?id=MSHPrMpIHZ) |
| 5 | [Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism](https://openreview.net/forum?id=W7sVYFJAEp) | 6.00 | 3 | Pending | [Link](https://openreview.net/forum?id=W7sVYFJAEp) |
| 6 | [Pretraining a Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm](https://openreview.net/forum?id=RHJVkaIYYa) | 3.00 | 4 | Pending | [Link](https://openreview.net/forum?id=RHJVkaIYYa) |
| 7 | [SVL: Empowering Spiking Neural Networks for Efficient 3D Open-World Understanding](https://openreview.net/forum?id=JYke7CIuIa) | 4.00 | 3 | Pending | [Link](https://openreview.net/forum?id=JYke7CIuIa) |
| 8 | [GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference](https://openreview.net/forum?id=3Gre3i1tSD) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=3Gre3i1tSD) |
| 9 | [LONGSHIELD: SCALABLE DISTRIBUTED DIFFERENTIALLY PRIVATE TRAINING FOR LONG-CONTEXT LLMS](https://openreview.net/forum?id=1Q2NVxcSuS) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=1Q2NVxcSuS) |
| 10 | [STAR: Speculative Decoding with Searchable Drafting and Target-Aware Refinement for Multimodal Generation](https://openreview.net/forum?id=pMdKnxkFRw) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=pMdKnxkFRw) |
| 11 | [SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks](https://openreview.net/forum?id=AtmupAPZ17) | 3.50 | 4 | Pending | [Link](https://openreview.net/forum?id=AtmupAPZ17) |
| 12 | [Breaking the Efficiency-Accuracy: Fusion of Rotation Quantization and N:M Sparsity for LLMs Inference](https://openreview.net/forum?id=h17M5TP0Sg) | 3.33 | 3 | Pending | [Link](https://openreview.net/forum?id=h17M5TP0Sg) |
| 13 | [PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse Attention for Vision-Language Large Models](https://openreview.net/forum?id=XtpVQ21bcY) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=XtpVQ21bcY) |
| 14 | [Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill](https://openreview.net/forum?id=uN5cHI3kpt) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=uN5cHI3kpt) |
| 15 | [ESSA: Evolutionary Strategies for Scalable Alignment](https://openreview.net/forum?id=1MX3QC0bSH) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=1MX3QC0bSH) |
| 16 | [READER: Retrieval-Assisted Drafter for Efficient LLM Inference](https://openreview.net/forum?id=4GxLFqsIwo) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=4GxLFqsIwo) |
| 17 | [CSAttention: Centroid-Scoring Attention for Accelerating LLM Inference](https://openreview.net/forum?id=CEpNboUJyw) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=CEpNboUJyw) |
| 18 | [Real-time Routing under Partial Observability: Information-Efficient Policies for Connected Vehicles](https://openreview.net/forum?id=UqjoMDJCmM) | 4.40 | 5 | Pending | [Link](https://openreview.net/forum?id=UqjoMDJCmM) |
| 19 | [Mixture-of-Channels: Exploiting Sparse FFNs for Efficient LLMs Pre-Training and Inference](https://openreview.net/forum?id=8DrZ0PHd94) | 4.00 | 3 | Pending | [Link](https://openreview.net/forum?id=8DrZ0PHd94) |
| 20 | [Fast-dLLM v2: Efficient Block-Diffusion LLM](https://openreview.net/forum?id=1NZ3DHF9nT) | 6.00 | 4 | Pending | [Link](https://openreview.net/forum?id=1NZ3DHF9nT) |
| 21 | [SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models](https://openreview.net/forum?id=98IxaUQtMY) | 6.67 | 3 | Pending | [Link](https://openreview.net/forum?id=98IxaUQtMY) |
| 22 | [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://openreview.net/forum?id=VCtqg9aFaC) | 4.40 | 5 | Pending | [Link](https://openreview.net/forum?id=VCtqg9aFaC) |
| 23 | [Parallel Prompting: Fast LLM Inference for Shared-Context, Short-to-Moderate Output](https://openreview.net/forum?id=T5KBO4IeM2) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=T5KBO4IeM2) |
| 24 | [TAH-QUANT: Effective Activation Quantization in Pipeline Parallelism over Slow Network](https://openreview.net/forum?id=cEkVJeMwSd) | 4.50 | 4 | Pending | [Link](https://openreview.net/forum?id=cEkVJeMwSd) |
| 25 | [Unlocking the Power of Layer By Layer Training For LLM](https://openreview.net/forum?id=Wqbi2pUcvb) | 3.50 | 4 | Pending | [Link](https://openreview.net/forum?id=Wqbi2pUcvb) |
| 26 | [PSPO: Trainable Potential-Based Reward Shaping with Internal Model Signals for Post-Training Policy Optimization of Large Language Models](https://openreview.net/forum?id=UXt9ul6pLJ) | 3.50 | 4 | Pending | [Link](https://openreview.net/forum?id=UXt9ul6pLJ) |
| 27 | [Optimizing LLM Inference Offloading with Hierarchical Scheduling and Dynamic Sparsification](https://openreview.net/forum?id=upV1K9g7JQ) | 3.00 | 4 | Pending | [Link](https://openreview.net/forum?id=upV1K9g7JQ) |
| 28 | [One Stone Three Birds: Training-free Core-context-aware Attention for Efficient LLM Prefilling, Decoding, and KV Caching](https://openreview.net/forum?id=0lGVMSAazo) | 3.50 | 4 | Pending | [Link](https://openreview.net/forum?id=0lGVMSAazo) |
| 29 | [Fed-Energy: Federated Reinforcement Learning for Scalable and Energy-Efficient Large-Scale Code Optimization](https://openreview.net/forum?id=APawIJjJlP) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=APawIJjJlP) |
| 30 | [Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs](https://openreview.net/forum?id=ZQ9uqllSts) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=ZQ9uqllSts) |
| 31 | [Hierarchy Decoding: A Training-free Parallel Decoding Strategy  for Diffusion Large Language Models](https://openreview.net/forum?id=ZsIQUjQtdW) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=ZsIQUjQtdW) |
| 32 | [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://openreview.net/forum?id=CwQzoZ1WxH) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=CwQzoZ1WxH) |
| 33 | [LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding](https://openreview.net/forum?id=YWCHLdNGVU) | 5.20 | 5 | Pending | [Link](https://openreview.net/forum?id=YWCHLdNGVU) |
| 34 | [Group Think: Collaborative Parallel Reasoning Model](https://openreview.net/forum?id=OZA4zivTFq) | 3.50 | 4 | Pending | [Link](https://openreview.net/forum?id=OZA4zivTFq) |
| 35 | [FOLD: Fast Correct Speculative Decoding](https://openreview.net/forum?id=zm35dmBdok) | 3.33 | 3 | Pending | [Link](https://openreview.net/forum?id=zm35dmBdok) |
| 36 | [AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism](https://openreview.net/forum?id=hzikvjtIj4) | 3.33 | 3 | Pending | [Link](https://openreview.net/forum?id=hzikvjtIj4) |
| 37 | [EfficientLLM: Evaluating Large Language Models Efficiency](https://openreview.net/forum?id=DKAUzhGiDa) | 4.50 | 4 | Pending | [Link](https://openreview.net/forum?id=DKAUzhGiDa) |
| 38 | [Adaptive Curriculum Learning for RLHF with Influence-Based Cluster Bandits](https://openreview.net/forum?id=8HvWBamUkS) | 5.00 | 4 | Pending | [Link](https://openreview.net/forum?id=8HvWBamUkS) |
| 39 | [Hierarchical Speculative Decoding through Training-Free Slim-Verifier](https://openreview.net/forum?id=9UsWULey9J) | 2.67 | 3 | Pending | [Link](https://openreview.net/forum?id=9UsWULey9J) |
| 40 | [AURA: Augmented Representation for Unified Accuracy-aware Quantization](https://openreview.net/forum?id=Mc4AIAvS4R) | 3.00 | 4 | Pending | [Link](https://openreview.net/forum?id=Mc4AIAvS4R) |
| 41 | [Optimized Early-Exit Based Speculative Decoding via Pipeline Parallelism](https://openreview.net/forum?id=6ezbdRe90k) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=6ezbdRe90k) |
| 42 | [Sparsity Forcing: Reinforcing Token Sparsity of MLLMs](https://openreview.net/forum?id=gxNTP2eER3) | 5.00 | 4 | Pending | [Link](https://openreview.net/forum?id=gxNTP2eER3) |
| 43 | [OPPO: Accelerating PPO-based RLHF via Pipeline Overlap](https://openreview.net/forum?id=31Mr6wLBeF) | 6.50 | 4 | Pending | [Link](https://openreview.net/forum?id=31Mr6wLBeF) |
| 44 | [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://openreview.net/forum?id=y2QHVETUqJ) | 3.50 | 4 | Pending | [Link](https://openreview.net/forum?id=y2QHVETUqJ) |
| 45 | [Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception](https://openreview.net/forum?id=Cox6AaRyan) | 5.50 | 4 | Pending | [Link](https://openreview.net/forum?id=Cox6AaRyan) |
| 46 | [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://openreview.net/forum?id=170GODIkgT) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=170GODIkgT) |
| 47 | [Scaling Overhead Matters: Saliency-Aware Graph-Based Efficient Post-Training Quantization for LLMs](https://openreview.net/forum?id=MxFZ57x7Gl) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=MxFZ57x7Gl) |
| 48 | [Cluster Topologyâ€‘Driven Placement of Experts Reduces Network Traffic in MoE Inference](https://openreview.net/forum?id=YW4RkDi5eH) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=YW4RkDi5eH) |
| 49 | [Bridging the Gap Between Promise and Performance for FP4 Quantization](https://openreview.net/forum?id=zCBGe9AqJZ) | 6.50 | 4 | Pending | [Link](https://openreview.net/forum?id=zCBGe9AqJZ) |
| 50 | [Configuring Parallel Training of Neural Networks using Bayesian Optimization](https://openreview.net/forum?id=p2xCepZMyW) | 4.67 | 3 | Pending | [Link](https://openreview.net/forum?id=p2xCepZMyW) |
| 51 | [WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity](https://openreview.net/forum?id=I4IaRjKQHy) | 3.33 | 3 | Pending | [Link](https://openreview.net/forum?id=I4IaRjKQHy) |
| 52 | [Tactic: Adaptive Sparse Attention with Clustering and Distribution Fitting for Long-Context LLMs](https://openreview.net/forum?id=tJod11fK1A) | 5.00 | 4 | Pending | [Link](https://openreview.net/forum?id=tJod11fK1A) |
| 53 | [AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs](https://openreview.net/forum?id=XPIEkFdEDi) | 6.00 | 4 | Pending | [Link](https://openreview.net/forum?id=XPIEkFdEDi) |
| 54 | [OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training](https://openreview.net/forum?id=0A2rXt5SAy) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=0A2rXt5SAy) |
| 55 | [Dynamic-dLLM: Dynamic Cache-Budget and Adaptive Parallel Decoding for Training-Free Acceleration of Diffusion LLM](https://openreview.net/forum?id=SdnkB5pGbq) | 6.00 | 4 | Pending | [Link](https://openreview.net/forum?id=SdnkB5pGbq) |
| 56 | [PAML: MoE-Based Partitioning and Merging Framework for Solving Large-scale Multi-task VRPs](https://openreview.net/forum?id=Y74tGjpsjq) | 2.00 | 4 | Pending | [Link](https://openreview.net/forum?id=Y74tGjpsjq) |
| 57 | [FSA: An Alternative Efficient Implementation of Native Sparse Attention Kernel](https://openreview.net/forum?id=c5mdo1hWrs) | 7.33 | 3 | Pending | [Link](https://openreview.net/forum?id=c5mdo1hWrs) |
| 58 | [S4NN: Scalable Self-Supervised Spiking Neural Networks](https://openreview.net/forum?id=P1KhPUTceF) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=P1KhPUTceF) |
| 59 | [TINY BUT MIGHTY: A SOFTWARE-HARDWARE CO- DESIGN APPROACH FOR EFFICIENT MULTIMODAL IN- FERENCE ON BATTERY-POWERED SMALL DEVICES](https://openreview.net/forum?id=ql30VWGyda) | 5.00 | 4 | Pending | [Link](https://openreview.net/forum?id=ql30VWGyda) |
| 60 | [Disentangling Token Dependencies for Efficient Decoding in Diffusion Language Models](https://openreview.net/forum?id=0ZSLZWAmWo) | 2.50 | 4 | Pending | [Link](https://openreview.net/forum?id=0ZSLZWAmWo) |
| 61 | [Accelerating Discrete Diffusion Decoding with Parallel Scan](https://openreview.net/forum?id=rQM3oU9cyg) | 4.80 | 5 | Pending | [Link](https://openreview.net/forum?id=rQM3oU9cyg) |
| 62 | [QeRL: Beyond Efficiency - Quantization-enhanced Reinforcement Learning for LLMs](https://openreview.net/forum?id=zw8zxMJJlm) | 6.00 | 4 | Pending | [Link](https://openreview.net/forum?id=zw8zxMJJlm) |
| 63 | [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://openreview.net/forum?id=7QZanjCD6M) | 4.50 | 4 | Pending | [Link](https://openreview.net/forum?id=7QZanjCD6M) |
| 64 | [Contrastive-Online-Meta (COM): A Dynamic Adaptation Mechanism for Instruction-Tuned CodeLLMs](https://openreview.net/forum?id=TjF9WLcu8o) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=TjF9WLcu8o) |
| 65 | [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://openreview.net/forum?id=J7xDwZSyI4) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=J7xDwZSyI4) |
| 66 | [SecP-Tuning: Efficient Privacy-Preserving Prompt Tuning for Large Language Models via MPC](https://openreview.net/forum?id=iJNM7KY8FD) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=iJNM7KY8FD) |
| 67 | [Towards Multiplier-Free Transformers with Stochastic Attention](https://openreview.net/forum?id=CaOaBlq6Bv) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=CaOaBlq6Bv) |
| 68 | [Speculative Actions: A Lossless Framework for Faster AI Agents](https://openreview.net/forum?id=P0GOk5wslg) | N/A | 0 | Pending | [Link](https://openreview.net/forum?id=P0GOk5wslg) |
| 69 | [In-Place Test-Time Training](https://openreview.net/forum?id=dTWfCLSoyl) | 7.33 | 3 | Pending | [Link](https://openreview.net/forum?id=dTWfCLSoyl) |
| 70 | [Training-Free Native Sparse Attention for KV Cache Compression](https://openreview.net/forum?id=sQjYtFSEuZ) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=sQjYtFSEuZ) |
| 71 | [Communication-Efficient Multi-Device Inference Acceleration for Transformer Models](https://openreview.net/forum?id=rpblsD3eXG) | 5.00 | 4 | Pending | [Link](https://openreview.net/forum?id=rpblsD3eXG) |
| 72 | [Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding](https://openreview.net/forum?id=GkMxEQlWyU) | 4.67 | 3 | Pending | [Link](https://openreview.net/forum?id=GkMxEQlWyU) |
| 73 | [SLAKE: Softmax-Approximated Training-Free Linear Attention with KV-Cache Eviction for Long-Sequence LLMs](https://openreview.net/forum?id=w70fv82Vft) | 4.50 | 4 | Pending | [Link](https://openreview.net/forum?id=w70fv82Vft) |
| 74 | [SCORE: Similarity-Aware Contextual Overlap-Redundancy Eviction for Efficient KV Cache Compression in LLMs](https://openreview.net/forum?id=HTlkxdEDWo) | 3.00 | 4 | Pending | [Link](https://openreview.net/forum?id=HTlkxdEDWo) |
| 75 | [How to Get Spiking LLMs? A Dual ANN-to-SNN Conversion with Layer-Wise Calibration](https://openreview.net/forum?id=nzkObUsajY) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=nzkObUsajY) |
| 76 | [PARD: Accelerating LLM Inference with Lowâ€‘Cost PARallel Draft Model Adaptation](https://openreview.net/forum?id=XbOyv7iVGL) | 4.50 | 4 | Pending | [Link](https://openreview.net/forum?id=XbOyv7iVGL) |
| 77 | [DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning](https://openreview.net/forum?id=ckUU5XySLn) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=ckUU5XySLn) |
| 78 | [QuoKA: Query-Oriented KV Selection for Efficient LLM Prefill](https://openreview.net/forum?id=YS4N1YxXSM) | 6.00 | 4 | Pending | [Link](https://openreview.net/forum?id=YS4N1YxXSM) |
| 79 | [Efficient and Stable Grouped RL Training for Large Language Models](https://openreview.net/forum?id=deSF7BrNli) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=deSF7BrNli) |
| 80 | [Parallel Training in Spiking Neural Networks](https://openreview.net/forum?id=RGxDhp3m0I) | 3.50 | 4 | Pending | [Link](https://openreview.net/forum?id=RGxDhp3m0I) |
| 81 | [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://openreview.net/forum?id=LUttHOTlYz) | 3.50 | 4 | Pending | [Link](https://openreview.net/forum?id=LUttHOTlYz) |
| 82 | [TopGQ: Fast GNN Post-Training Quantization Leveraging Topology Information](https://openreview.net/forum?id=IFw1tVHhHM) | 3.33 | 3 | Pending | [Link](https://openreview.net/forum?id=IFw1tVHhHM) |
| 83 | [KVCompose: Efficient Structured KV Cache Compression with Composite Tokens](https://openreview.net/forum?id=GNKIV7oSl2) | 3.50 | 4 | Pending | [Link](https://openreview.net/forum?id=GNKIV7oSl2) |
| 84 | [Evolving Sparsity: Leveraging Token Importance Dynamics for Efficient LLM Decoding with Sparse Attention](https://openreview.net/forum?id=YPjcyMzhE2) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=YPjcyMzhE2) |
| 85 | [Sparse Topology Pairwise Scoring for Large-Scale Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=5IqvMNQaUg) | 5.00 | 4 | Pending | [Link](https://openreview.net/forum?id=5IqvMNQaUg) |
| 86 | [Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation](https://openreview.net/forum?id=k6AJ1N7BA2) | 5.33 | 3 | Pending | [Link](https://openreview.net/forum?id=k6AJ1N7BA2) |
| 87 | [Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression](https://openreview.net/forum?id=KKVCWwurQQ) | 4.00 | 3 | Pending | [Link](https://openreview.net/forum?id=KKVCWwurQQ) |
| 88 | [HiSpec: Hierarchical Speculative Decoding for LLMs](https://openreview.net/forum?id=CYGI23WQjI) | 4.00 | 3 | Pending | [Link](https://openreview.net/forum?id=CYGI23WQjI) |
| 89 | [HoVer: Holistic Verification for Semantic-Aware Speculative Generation](https://openreview.net/forum?id=GQlF9F8HAs) | 3.33 | 3 | Pending | [Link](https://openreview.net/forum?id=GQlF9F8HAs) |
| 90 | [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://openreview.net/forum?id=t5uLZSRjhF) | 6.00 | 4 | Pending | [Link](https://openreview.net/forum?id=t5uLZSRjhF) |
| 91 | [Layer-wise Sensitivity-aware Sparsity Allocation for Efficient LLM Inference](https://openreview.net/forum?id=erGq3kjCmy) | 5.33 | 3 | Pending | [Link](https://openreview.net/forum?id=erGq3kjCmy) |
| 92 | [Towards Efficient Post-Training Quantization For Large Vision-Language Models Via Token-Wise Redundancy Elimination](https://openreview.net/forum?id=CXVf8Vx2E2) | 4.67 | 3 | Pending | [Link](https://openreview.net/forum?id=CXVf8Vx2E2) |
| 93 | [RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs](https://openreview.net/forum?id=MPMyROJvJV) | 5.00 | 4 | Pending | [Link](https://openreview.net/forum?id=MPMyROJvJV) |
| 94 | [LLMBoost: Make Large Language Models Stronger with Boosting](https://openreview.net/forum?id=giTyBPDKT9) | 4.67 | 3 | Pending | [Link](https://openreview.net/forum?id=giTyBPDKT9) |
| 95 | [AMS-Quant: Adaptive Mantissa Sharing for Floating-Point Quantization](https://openreview.net/forum?id=4vXNp4wujY) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=4vXNp4wujY) |
| 96 | [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://openreview.net/forum?id=lkMh48jItD) | 4.00 | 4 | Pending | [Link](https://openreview.net/forum?id=lkMh48jItD) |
| 97 | [TAKE: Task-Aware Chunked KV Cache Eviction for Efficient Long-Context LLM Prefill](https://openreview.net/forum?id=kMLfUshPwo) | 4.00 | 3 | Pending | [Link](https://openreview.net/forum?id=kMLfUshPwo) |
| 98 | [A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration](https://openreview.net/forum?id=8dLexnao2h) | 6.50 | 4 | Pending | [Link](https://openreview.net/forum?id=8dLexnao2h) |
| 99 | [Less Is More: Fast and Accurate Reasoning with Cross-Head Unified Sparse Attention](https://openreview.net/forum?id=3iwDzfIk60) | 5.00 | 4 | Pending | [Link](https://openreview.net/forum?id=3iwDzfIk60) |
| 100 | [Ban&Pick: Ehancing Performance and Efficiency of MoE-LLMs via Smarter Routing](https://openreview.net/forum?id=KWMR2YfC55) | 5.33 | 3 | Pending | [Link](https://openreview.net/forum?id=KWMR2YfC55) |